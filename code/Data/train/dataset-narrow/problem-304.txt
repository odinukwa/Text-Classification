My Goals Essentially, I want to be able to achieve optimal routing between POPs as we deploy them. But right now I am not able to achieve any level of control over how iBGP is choosing it's paths. My current design 

We have a number of Cisco 891 routers configured with dual WAN links. All of these routers are running Cisco IOS 15.x In some cases, we use policy based routing to force particular traffic down one link. I use PBR a lot with Dual WAN setups, as the customer generally wants certain traffic to go over a particular link. For example, I will often need to have real-time traffic such as VOIP go over one link, then general internet to go over another, and typically will use PBR to route the traffic according to source/destination IP, VLAN, or whatever fits. Obviously, it is best to still offer a degraded service if the dedicated voice link should go offline. I tend to use with an IP SLA tracking object in order to allow the traffic to still fail-over to the other WAN, should it need to. My question is: Is it at all possible to do the same config (wth a tracking object to verify availability) when using PBR to set an interface, instead of a next-hop? There's a couple of reasons for this: 

I thought this would work pretty perfectly, be a pretty elegant solution, and is exactly the type of solution I am hoping to get to. I wrote up the configs in a couple of hours and deployed. But scratched my head until I realised that iBGP does not support AS path prepending. 

Even if I could get this working, it seems like it would never be a supported solution. What I'm considering 

We have a similar setup, with a single /24 public range. At the time, I was not aware that the 2950 switches could support PVLANs (which coincidentally is what we've also been running). Instead, to conserve IP space, I used the following configuration: 

We've found this to be a pretty flexible configuration. As long as you're not planning to scale to a much larger size, I think it works well. I'm not saying that this would necessarily be a better solution over PVLANs, but simply offering it as an alternative that has worked for us so far. To be honest with you, though, if I had've realised at the time I could get a PVLAN configuration working on the 2950s, then I probably would have done that instead. Edit - Virtual Machines Again, I'm not entirely sure how this might compare with PVLANs, but one of the major benefits we've experienced in our setup, is that it works seamlessly on our virtual platform. For each virtual server host, it is configured with a trunk port, and we simply define virtual networks per VLAN, which each VM can belong to, as needed. This means that we can have customers with multiple physical servers, and VMs across multiple hosts, and span their specific VLAN across all of them. Meaning they can talk to all of their servers/VMs "locally" but still remain isolated from all other customers on our infrastructure. I would consider this a major benefit of our configuration. I'm not sure you'd be able to bring PVLANs down to your ESX/Xen/Virtual Hosts, which would limit your options. 

The first thing that jumps out at me is that you appear to be shaping everything to 4 Mbps. I imagine that the rate changes on routers/sites with different circuit speeds, but you generally want to avoid shaping when dealing with latency-sensitive applications like VoIP and RDP as it can cause excessive buffering and jitter during periods of congestion. Also, the command is a bit tricky: Each instance actually allocates n% of the remaining available bandwidth, not n% of the total. This graphic from an article by Arden Packeer should help illustrate the idea: 

You could create a second match condition in the class-map matching all source IP networks you want to block (with an ACL). Any requests to youtube.com from a source IP not matched by this ACL will not be dropped. 

Designate one Internet circuit as the primary and the other as failover Implement "NAT outside" (public space) routing between the sites with the firewalls 

This looks like a simple layer two domain. MPLS is typically used only as backbone or distribution transit, that is, among routers. To implement MPLS, you would need to push the VLAN interfaces down to the access switches (and thus split Host5 into a new network or move it to switch 2) and run MPLS among the multilayer switches. Although, this wouldn't bring much benefit to the topology shown. 

Both traceroutes above should be following the exact same path, and there are no filtering mechanisms in place along it. The same thing happens in the reverse direction as well. What am I missing? What's the difference between BGP routes learned by direct connection versus static configuration with regard to MPLS/label forwarding? 

Check that you actually have valid internal routes for these networks to override the routes to null0. If there are no more-specific or more-preferable routes for these networks, the routers will of course discard traffic destined for them. 

I have two routers, A (Cat6500 w/SUP720-3BXL, IOS 12.2(33)SXH4) and B (Nexus 7K w/SUP1, NX-OS 5.2(4)), separated by several hops across an MPLS core, each with VRF ABC. Router A has two directly connected routes and four static routes within this VRF. 

It's important to note that any classifications you define need to match what your WAN provider supports. Most providers offer only a handful of pre-configured QoS profiles from which you must choose which best suits your needs. You can classify however you'd like on incoming traffic at the WAN edge, but the provider's QoS controls are what control the the treatment of traffic during transit across the WAN. 

STP does not operate differently whether the network has three, four, or a dozen switches. Each switch applies the same logic locally, and don't even actually know how many switches exist within the network. Legacy and rapid STP behave differently (you'll need to research the differences) but both operate on the same basic principle: calculate the best path to the root bridge and disable all redundant paths. If you know the topology of and the costs associated with each link in the network, it's entirely possible to calculate the best path on paper alone. 

Hoping some one knows what PBR config I can use to achieve this when setting interfaces. For reference, here's how I currently do it, with : EDIT: Included more detailed example. 

What I've tried I tried adding an OSPF link cost of 15,000 which I calculated as a safe figure based on my ref bandwidth of 100 Gbps as always being least preferred OSPF cost. I thought this would count as "IGP cost" and yet BGP is still preferring the Melbourne path for some reason. After this didn't seem to have any impact, my main plan was to use AS PATH prepending on iBGP. The plan was that I would have peer-groups per POP. And in my templating I would designate how many prepends to do, based on how far apart the two POPs were. I had thought this would be a fairly common type of goal. For example: 

I'm looking to implement a new network design with Juniper EX Series switches. Part of this involves implementing vPC between my core and ToR switches (and also between ToR and host). The goal is to be able to split 4x1GE links into 2x2GE port groups, each one split between my two core switches for redundancy, and still obtain near 4 Gbps bandwidth to each rack. 

I'm fairly new to the concept of vPC but I don't seem to be able to find a Juniper equivalent for this technology - other than stacking the ToR switches with Virtual Chassis technology. I'll probably end up using Virtual Chassis if I can't do it any other way, but I'd like to try and keep the ToR switches un-stacked, if I can. The only other option I know of is to use MSTP to balance traffic per VLAN - which is probably OK given that we use 1xVLAN per customer and each host can do 2 Gbps max, but a port channel seems much simpler to configure and maintain. Am I missing something? 

(As an aside, we're currently using a Linux router - 8 cores + 8 GB RAM, and it handles just under 1000 individual IPTables rules without breaking a sweat - though the throughput is < 50 Mbps). 

EDIT: 30/01: I think that I am wrong about how IGP cost is calculated and perhaps they are currently the same? All my OSPF routes are type E2. If the IGP costs are the same then I suppose it makes sense best path selection happens based on RID, which in this case the RID of the MEL BDR would be lower than SYD. I have set the OSPF link cost between Sydney to 15,000 much higher than default. I've calculated this to work reliably with our 100 Gbps reference bandwidth. In terms of OSPF link costs - this is OSPFs preferences of each next-hop of the BGP routes: 

There are tools like WANem which allow you to simulate WAN links by artificially causing arbitrary delay and loss rates on a link. WANem works at a relatively high-level; you won't see physical errors on the link but packets will be dropped. It can be deployed on commodity hardware. I know there are a few other tools which serve similar purposes but I can't recall the names of any at the moment. 

Is there a way to filter the control plane of the firewall so that these packets are dropped at the interface edge rather than processed and rejected? This would be done with a simple interface ACL on a Cisco router or ASA, but I'm not sure how to go about this on ScreenOS. 

Per-prefix labeling is used for this VRF on both routers. Notice that the two directly connected routes receive a shared aggregate label (95) whereas the four static routes each receive a unique label. Router B agrees on the VPN labels to use: 

I have two sites, A and B, in BGP AS 65000, and a third site, C, in AS 65001. All three sites have connectivity via carrier MPLS and there is internal connectivity between sites A and B. I am trying to influence BGP so that traffic from site A to site C will be routed via site A's MPLS link, and traffic from site B to site C will route via site B's MPLS link. The topology is similar to what is described in this example. 

Some vendors support IEEE 802.1ae/MACsec for encryption layer two between the access switch and its endpoints. I'm afraid I can't be any more help as I have no experience with it (and frankly it sounds like a nightmare to administer). 

Does anyone have any clever tricks around this? We could certainly use a separate user account instead of root, but that still leaves open the possibility that someone will log in as root during a maintenance action and forget to log out. 

The edge routers at sites A and B will both see their own MPLS link as the best path, because EBGP routes are preferred over IBGP routes. However, routers further inside AS 65000 will all prefer either one link or the other. My goal is to force all the routers at either site to prefer the closest link. (Unfortunately, I'm not able to split the two sites into separate ASes at this time.) Is there a sane way to accomplish this while still allowing failover connectivity to site C between the site A and B links? Edit: I should have noted that there is no IGP in use here. In fact, the networks at each site exist within a VRF as part of a much, much larger network. As such, any solution needs to be rely entirely on BGP.