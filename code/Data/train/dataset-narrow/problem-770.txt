The method should work regardless of version, but you could to use the major/minor property when available: 

Exclusive locks associated with those data modifications will be held until the transaction is complete. See the Deadlock Troubleshooting posts by Bart Duncan. 

You might find that backing up the TLog in 10 minute intervals will allow for less growth in the TLog file. Re-creating the TLog file to reduce/resize VLFs can help too. Please read Important change to VLF creation algorithm in SQL Server 2014 (Sqlskills.com) on how to accomplish this. Backing up the TLog every 10 minutes may not help if the index rebuild process is taking an hour. And with an AG you also have to take into account hardening on the secondary so that log can clear on the primary: When you rebuild indexes there ends up being a significant number of changes that have to flow to the AG secondary. Until all of those changes (transactions) are hardened on the secondary replica, SQL cannot clear the transaction log on the primary (safety feature so that you do not lose transactions). It doesn't matter which replica you run the log backups on, you will not be able to clear those logs until everything is hardened. This can create significant volume in your TLogs. It's a side effect of running AGs and doing index maintenance. You need more disk space. 

To recover the data, instead of vacuuming, make a copy of the database as it is now, look for a hex editor, and see what you can salvage by hand. See also Undelete accidentally deleted records in Sqlite3 on Stack Overflow. 

I am in the process of now setting all of the bits in the table using a user-defined function (see below). It is in progress and taking about 1 minute per every 1 million rows in the ~68 million row table. 

Community wiki answer: A given athlete can play in multiple sports, so you should probably separate that. Also I do not understand the ranking part attached to countries. This should be at best a view, not a table. As a general advice, do not store ages for individuals, but date of birth, because this does not change while the age changes each year. You should just store the result (position) number for each athlete and competition, not medals that will be derived from it. You also need to take into account ex-aequos. Also, even for a given competition, a given athlete may participate in multiple steps (semi-finals, final, etc.) so you may need to cater for that. 

Sargability wouldn't be a problem if the query is written to use , since SQL Server will include a helper covering seek predicate. See Understanding SARGability (to make your queries run faster) by Rob Farley (recorded at SQLBits VII in 2010) which demoed this. Note however that the conversion might still affect the accuracy of cardinality estimates. 

Community wiki answer: You need to be careful with drives getting full due to log file growth. When this happens, the transaction which caused this cannot even rollback as there is no space in the log to write the rollback information. Adding a data file to another drive and allowing the log to grow should be a safer option, but that cannot be done on a secondary replica, so I believe you are left with option you took. 

Community wiki answer originally left by the question asker as a comment: The fix was removing the timestamp formatting from 

Community Wiki answer containing answers originally added to the question or left in comments by the question author 

Community wiki answer: For the first case, see the Q & A: How SQL server generates a Query plan with Auto Create Statistics set to OFF. You use a variable so it's considered unknown input and is estimated as 30% of table cardinality (30% of 1000 rows). In the second case it uses statistics histograms. You can learn about them in SQL Server: Part 2 : All About SQL Server Statistics :Histogram by Nelson John. You'll also get the 700 row estimate for your parameterized query if you force a new plan to be compiled based on the current value of . For example: 

Community wiki answer: Have you done a full search on all the drives attached to the SQL Server? Not just where you expect the files to be, but anywhere on the drive? It is also possible that they were connected via UNC path (), have you looked for them there? If you cannot find the primary file and do not have backups, there isn't anything we can do to help you. My guess is that either the MDF files are not where you expected them to be, are not named as you expected them to be, or got deleted after you detached the database. 

Community wiki answer: I would try installing the SQL Server 2005 Native Client in the Feature Pack for Microsoft SQL Server 2005 - April 2006. Please note also that post says Windows 2000 Service Pack 4 is required to be supported. 

or with . Regarding the idea of creating a new table and populating in batches, see Restructure 100 Million Row (or more) Tables in Seconds. SRSLY! on SQL Server Central. 

Community wiki answer: Works as intended: You created a constraint that ensures the uniqueness of and together. Then you inserted a second row with the same , but different value. 

Answer originally provided by the question author in a comment: So, somehow, I neglected to mention that I was pulling the insert statements from a file...and including the part where it turns the index off before the inserts, and on again after (the code looked like it was commented out - I just didn't understand the syntax). Apparently, turning the index off and on like that ALWAYS causes the table to reindex from scratch...removing the or switch on the fixed the issue, and my inserts take a couple seconds now. 

Community wiki answer: Yes, you should always back the system databases up. This is an essential part of any disaster recovery procedure, which every production server should have documented and tested. That said, some folks find it a bit less painful to script out the objects they need from system databases and then run the scripts where they'll be going, rather than restoring over system databases. 

Thanks for the hint. The number of VLFs was the root of my problem. This particular database had CDC issues which meant the transaction log wasn't being cleared out on backups. It had grown to over 300 GB. Once I disabled CDC, I was able to flush the log and give it a better growth plan. The compressed backup works again. 

You could use SSMS for this. Right-click on the database -> tasks -> generate scripts. Generate Scripts can script out permissions, but the method is pretty hidden. When you get to the screen where you choose the location to script things out to, click the Advanced button. Scroll through there until you see the "Script Object-level Permissions" option, and set it to "True". The script will then include permissions for whatever you scripted out. 

Community wiki answer: There are loads of options/possibilities. See Datetime Format Models in the Database SQL Language Reference. For example, if you have it set to , you should use . lets you store 20th century dates in the 21st century using only two digits, as described in The RR Datetime Format Element: 

Community wiki answer: Z should be the virtual server name of the cluster. I don't use Server Manager so I have no idea what "current host server" means and whether or not it is supposed to represent the active node for the SQL Server clustered instance (which is different from the Windows cluster). If you had two separate FCIs and one was currently active on each node, what would you expect "current host server" to say? Does Server Manager represent each SQL Server cluster separately or just the windows cluster? I understand these are physical machines - this is a completely different concept. "Virtual Server Name" is the external name of the SQL Server failover cluster instance - it's how SSMS and your applications connect to SQL Server regardless of which physical node the clustered instance is currently using. I suspect "current host server" is something completely different and has to do with the Windows cluster, not the SQL Server failover clustered instance sitting on top of Windows. is going to give you the physical name of the currently active node vs will give you the virtual name of the cluster. 

Community wiki answer: If the query is now parameterized, you should be able to ignore all the ones that still show constant literals - those are in the plan cache now, but they'll age out over time. In addition to the outer join that's not really an outer join anymore, is required, or is it possible there is a skinnier pair of indexes that could cover the actual output requirements (usually some subset of )? You should also encourage your developers to always use the schema prefix e.g. . Simple parameterization only applies to trivial plans. Also consider optimize for ad hoc workloads. Kimberly Tripp has some good guidance on cleaning the cache. Think of forced parameterization as a last resort, because it will affect all your queries, with a high risk of performance regressions. 

Answer originally left as a comment by the question author: It worked after adding the new 12c parameters instead of using the deprecated parameters in . 

You can also read the error log using every few hours and save those records in a table. You can pass start and end times to this procedure, that way you do not have read what you already processed. See Reading All Available SQL Error logs by Taiob Ali. 

Community wiki answer: I don't know if you can prove that any observed behavior is always guaranteed, one way or the other, unless you can manufacture a counter-example. In the absence of that, the way to fix the order that results are returned, of course, is to add an . I don't know if there is a "fix", or that there exists a need for a fix, if you can demonstrate that in some scenarios the queries are processed in a different order. The lack of any explicit, official documentation suggests to me that you should not depend on this. This is exactly the kind of thing that got people into trouble with in a view, and without , 8 years ago when SQL Server 2005's optimizer was released. With all of the new features in the newer versions of SQL Server (with more coming), even if you think you can guarantee a specific behavior today, I wouldn't expect it to hold true (until it is documented to do so). Even if you're not depending on this behavior, what are you going to do with the results? Anyway, I wouldn't call a Simple Talk article by an outsider official. For all we know this is just a guess based on observation. Microsoft is never going to publish official documentation saying 'x' is not guaranteed to do 'y'. This is one of the reasons we still, almost a decade later, have trouble convincing people that they can't rely on observed ordering without - there is no documentation that states "it is not guaranteed." 

Community wiki answer: are binary data files used by the WiredTiger storage engine. Individual files are not usable as a standalone backup. If you want to take a file copy backup of a MongoDB you need to include all of the files using a Supported File Copy Backup Method. If you have a valid file backup you can use it as the for another instance. Aside from copying files, there is no restore special restore process for a file copy backup. Can you clarify what files you have in your backup? 

Community wiki answer: Did you really read chapter 23, where vertical/horizontal decomposition is described? I don't see any 'n/a' or 'd/k' anywhere there, even more in numeric columns. Figure 8 on page no 384 shows is what is "obtained" from the database, so a result set. Not what is being stored in the tables. As the previous pages describe, the suggestion is to decompose the table into smaller ones (fewer columns and/or rows) and each table stores the different info, i.e. one table for unknown, another for not applicable, etc.) The only place where 'n/a', 'd/k' are stored if you use a table with a column, and combine multiple missing-information tables into one. 

Community wiki answer: No, this is not possible. I'd add this as a code quality item to their code reviews. That said, the root cause of the multiple plans is not the different call syntax. That could also happen with different options, etc. Instead look at query plans and the parameterization behavior, and the Query Store (if on 2016+) to see why you are getting different plans. See also: Slow in the Application, Fast in SSMS? by Erland Sommarskog. 

Those pages in the documentation are an overview. The left-side navigation links to further information, and there are links in the page as well to more details. 

You are not creating a linked server just because you are using The official Microsoft documentation shows you how to create a linked server. If you are having difficulty with this, please edit your question with a specific problem. See also: How to Add a Linked Server 

Community wiki answer: Yes of course the other transaction will wait. (Nearly) Every DDL statement takes an exclusive lock on the object being modified (dropped, created). See Explicit Locking in the documentation. 

Community wiki answer: Checking compression You can check if your backup is compressed or not. There are a few methods in How can I tell if a SQL Server backup is compressed?. You can also check the value of and of the table in . For example: 

Community wiki answer: You already sound like you know the right answer. The best approach is the one that has the trigger code run the fastest with the least amount of code. If it were me, I'd have one trigger per action. Will there be duplicate code - absolutely - but it saves you for when anything needs to be changed and can be customized per situation. You could try both and then simulate load to see which you like/performs the best. Aaron Bertrand found there was observable additional overhead with multiple triggers performing the same operations, but YMMV. However, a couple dozen transactions per minute doesn't sound like a tremendous amount of activity. Maintainability 'might' be able to justify a small amount of overhead. There are numerous advantages to using separate 'single-function' triggers. As you indicated, they have a single purpose and can be individually tested (by different people), enabled, disabled. Isolating the processing into separate 'single-function' triggers 'might' reduce the risk involved in future maintenance. Of course, final testing of the individual triggers cannot be by different people, as you need to ensure the triggers work properly in unison. And, while future maintenance of smaller chunks of code is logically easier, you cannot ignore the other triggers when making future changes, as something that causes no issues in trigger A could completely mess up what trigger C is doing. 

Community wiki answer: A table variable is an instance of a table. Tables are called relations in relational algebra - hence relational variable (or relvar for short). Base table means that they are not views which are derived from base tables. 

Community wiki answer please edit to improve if you can From an indexing and execution standpoint, you're introducing the likelihood for execution plans that cannot use the seeking capability of indexes. That means poor performance on updates and selects. Seems like a lot of extra work for no payoff. 

Other suggestions left in comments in case they help others: It's not normal behaviour to have to restart except during configuration changes. You should disable the in process option if you care about your core database stability - otherwise errors in the linked server driver can cause your engine to crash. It isn't trivial to get working and has a slew of knock-on effects, for example in how and what it uses to present security credentials across the network. Expect to spend a day trying to disentangle that if you go down that path. 

Community wiki answer: If SQL Server is crashing, that is a defect and you need to open a support incident with Microsoft so that they can fix it - or if you're lucky, they already have a fix for it. With Axapta, you may be able to disable or drop numerous indexes. The product comes with a number of indexes, but if certain components of the application are not being used, some indexes may not be used. Fewer indexes = faster reindexing. Another option is to add logic to the job so that it waits until the other one is finished before it starts. Or, simply skip the index maintenance and just update statistics. is far more important. Reindexing on flash storage can be rather pointless from a technical perspective, but it removes a barrier when people who don't understand it are complaining about a performance issue. Developers and level 1 tech support people, for example, often want to blame fragmented indexes for performance issues, and if you can show them that your indexes aren't fragmented, you can quickly move the troubleshooting process forward. Otherwise, you have to educate them why those 10,000 blog posts about fragmented indexes causing performance issues are not applicable. normally runs on a database snapshot copy of the database that is transactionally consistent. This means your two workloads (checkdb and rebuilds) shouldn't affect each other, except to compete for physical resources. If is returning an error, that's a problem. Generally, if hits really bad errors a memory dump will occur. Your error is an IO error () so I would check your disk subsystem and not look at the maintenance. If your disk subsystem is going to throw IO errors when it gets hit hard - that's a pretty big problem that needs to be fixed or you're going to run into this without doing checkdb/index maintenance.