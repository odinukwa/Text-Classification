Since it sounds like you're fitting a model, there's the additional consideration that certain important combinations of features could be relatively rare in the population. This is not an issue for generalizability, but it bears heavily on your considerations for sample size. For instance, I'm working on a project now with (non-big) data that was originally collected to understand the experiences of minorities in college. As such, it was critically important to ensure that statistical power was high specifically in the minority subpopulation. For this reason, blacks and Latinos were deliberately oversampled. However, the proportion by which they were oversampled was also recorded. These are used to compute survey weights. These can be used to re-weight the sample so as to reflect the estimated population proportions, in the event that a representative sample is required. An additional consideration arises if your model is hierarchical. A canonical use for a hierarchical model is one of children's behavior in schools. Children are "grouped" by school and share school-level traits. Therefore a representative sample of schools is required, and within each school a representative sample of children is required. This leads to stratified sampling. This and some other sampling designs are reviewed in surprising depth on Wikipedia. 

Shiny is a framework for generating HTML-based apps that execute R code dynamically. Shiny apps can stand alone or be built into Markdown documents with , and Shiny development is fully integrated into RStudio. There's even a free service called shinyapps.io for hosting Shiny apps, the package has functions for deploying Shiny apps directly from R, and RStudio has a GUI interface for calling those functions. There's plenty more info in the Tutorial section of the site. Since it essentially "compiles" the whole thing to JavaScript and HTML, you can use CSS to freely change the formatting and layout, although Shiny has decent wrapper functionality for this. But it just so happens that their default color scheme is similar to the one in the screenshot you posted. edit: I just realized you don't need them to be dynamic. Shiny still makes very nice-looking webpages out of the box, with lots of options for rearranging elements. There's also functionality for downloading plots, so you can generate your dashboard every month by just updating your data files in the app, and then saving the resulting image to PDF. 

The sample must be representative. In expectation, at least, the distribution of features in your sample must match the distribution of features in the population. When you are fitting a model with a response variable, this includes features that you do not observe, but that affect any response variables in your model. Since it is, in many cases, impossible to know what you do not observe, random sampling is used. The idea with randomization is that a random sample, up to sampling error, must accurately reflect the distribution of all features in the population, observed and otherwise. This is why randomization is the "gold standard," but if sample control is available by some other technique, or it is defensible to argue that there are no omitted features, then it isn't always necessary. Your sample must be large enough that the effect of sampling error on the feature distribution is relatively small. This is, again, to ensure representativeness. But deciding who to sample is different from deciding how many people to sample. 

There is an excellent comparison of the common inner-product-based similarity metrics here. In particular, Cosine Similarity is normalized to lie within [0,1], unlike the dot product which can be any real number, but, as everyone else is saying, that will require ignoring the magnitude of the vectors. Personally, I think that's a good thing. I think of magnitude as an internal (within-vector) structure, and angle between vectors as external (between vector) structure. They are different things and (in my opinion) are often best analyzed separately. I can't imagine a situation where I would rather compute inner products than compute cosine similarities and just compare the magnitudes afterward. 

I have an understanding problem. I am a beginner in machine learning and have also a little experience in modelling NNs but not for time series. But I cannot imagine how to use Neiural Networks for time series. So if I want to train a Multilayer NN what is the input? I read several papers about such ANN to predict time series. But they did not explain how actually. Can I imagine it as follows: Is every input node a specific time stamp? If I want to predict the value for time t+1, and I use 15 input neurons, so I use the values at each time stamp from t-15 upt to t? How do I train such a NN? I am a little confused. 

I want to get tips and hints from you how I can handle the customer IDs in a time series of data of an online store to forecast revenue. So I have data like customer ID, date of order, quantity, price, retail price, etc. So it is a time seire and I am constraint to the customer ID. I wanted to train a neural network But How do I handle the customer ID in the neural network? Do I ignore the indivual behaviour and the fact I have a time series? What are the simplest way to forecast revenues of such a typical time series of order from an store? 

I f have a time series of long time horizon (like a stock price), how do I train in the simplest way the echo state network? If I chose the training set to be 2 years e.g. then I could chose every day in those 2 years as input. Can I train the network for 1 month? So i divide the 2 years in parts of 1 month. And for every month in the past I train the network and do average of all neworks? So, waht are common methods for time series analysis with echo state networks for training? Do I rather train the whole time series or do I divide it supparts to train? 

Im am interested in solving the task from the data mining cup 17 for an online-shop. It is simliar to the the kaggle competition of Ro√ümann. You have user IDs, and product attributes like the price of the competitor. Plus those attributes you have the information if the user X clicked, or even ordered the product or only assigned the product to the shopping basket. My response variable is the revenue of purchased product. So if only click on a product it gives no revenue of course. So, I would solve this task with a random forest If you have only the product features to classify revenue. But, of course the time history of how often e.g. a product is clicked in the last 5 days or assigned to the basket will influences the likelihood og beeing purchased. How I use such an time history together with a random forest forecasting? So I have a interaction between the product features and a time dependent user interaction which might influences revenue. I would be pleased about logical advices how I have to perform such an task 

How is a time series like the Rossmann Kaggle competition used to forecast sales? The simplest solution I saw is a random forest. There the data for every time point is used to feed the random forests. So there is no time dependence because I use every data at each time as independent. Thats also good. But what I a doing if I want to solve this with a neural network? So my input layer corresponds to every time point (for the same time independence)? In the data mining Cup 2017 there is a similiar competition. there are over 5000 products with attributes like if it was clicked or drawn to the basket or ordered. Since there are intreractions between the user actions like clicking at the product, history come important. Do I have to train a neural network for each product to forecast revenue? 

Below is a template I use for LDA data compression. It assumes that you've split your data into a training and test set, the feature space has been properly scaled, and there are three classes in your label vector (you can adjust accordingly). It plots the individual and cumulative "discriminability" of each linear discriminant and then relies on the package in to transform the feature space using the number of discriminants you intend on using (here I chose to use the first 2). It also scales the within class scatter matrices by default. LINEAR DISCRIMINANT ANALYSIS calculate mean vectors 

will perform the encoding for you (The categorical_features attribute gives the index of the feature you want encoded). More to your point though, if your feature has many levels, one-hot encoding can leave you with a sparse and inefficient X array. In this case it may be beneficial to do a count transform, possibly replacing each level with its corresponding log-odds ratio. Count Transforms The link above does a more thorough job explaining it, but, in layman's terms, for each categorical feature you: 

I hope that answers most of what you're asking. I'm not sure why you would want to concatenate two very different variables like cities and urls, but if you encode them or do a count transform your X array should be just fine and you can leave them separated. Also, you shouldn't have to worry about the length of a given variable - python is object oriented (if you're getting error messages complaining about 'length', you've likely just confused your indices). 

Unfortunately scikit-learn does not handle categorical features well - they must be encoded. One-hot-encoding is great and can be implemented in sklearn very easily. So if you have an array of feature columns, X, and a class label vector, y 

count the number of times a level(category) belongs to class0 or class1 add 0.5 to each count (this takes care of instances when a level belongs strictly to one class) calculate the probability that it belongs to either class. calculate the log of the odds ratio replace each level with its corresponding log odds ratio 

The LDA crashes for the exact reason you suspected. You have complex eigenvalues. If you use , which was designed to decompose Hermetian matrices, you will always get real eigenvalues. can decompose nonsymetric square matrices, but, as you've suspected, it can produce complex eigenvalues. In short, is more stable, and I would suggest using it for both PCA and LDA. Dropping the complex part of the eigenvalues may have been acceptable in your specific example, but in practice it should be avoided. Depending on the size of the complex part of the number, it can significantly change the result. For example think of the multiplication of two complex conjugates. $(3+ .1i)*(3-.1i)=9-.01=9.01$ comared to $9$ when droping the complex part is a relatively safe, but $(3-2i)*(3+2i)=13$ compared to $9$ is a significant miscalculation. Using the above method for the eigen-decomposition will prevent this situation from arising. Remember that one of the assumptions of LDA is that the features are normally distributed and independent of each other. Try running . If the counts are not close to being equal, you've violated the first asumption of LDA, and the within-class scatter matrix must be scaled, in short divide each count by the number of class samples $N_i$. By doing this it should be obvious that computing the normalized scatter matrix is the same as computing the covariance matrix $\Sigma_i$. $$\Sigma_i=\frac{1}{N_i}S_W=\frac{1}{N_i}(x-m_i)(x-m_i)^T $$ Make sure your scaling your features before you do your PCA/LDA. If the above doesn't fix your eigenvector verification step I suspect the problem is that the eigenvectors are scalled differently. Remember from your linear algebra class that a single eigenvalue, $\lambda_i$, has infinitely many eigenvectors, each being a scalar multiple of the others. $v_i=[1,2,3]$ and $v_i=[2,4,6]$ can both be the eigenvector of $\lambda_i$. So while you may get different values when calculating values at any given step after the decomposition, the end result should be the same.