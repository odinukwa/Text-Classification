Yuval Peres gave the answer in terms of the Kullback-Leibler divergence. Another way is to recall that the sample complexity will be captured by the inverse of the squared Hellinger distance between the two coins. Now, letting $D_p$ and $D_{p+\varepsilon}$ be the distributions of a Bernoulli random variable with parameter $p$ and $p+\varepsilon$ respectively, $$\begin{align} d_H(D_p,D_{p+\varepsilon})^2 &= \frac{1}{2}\lVert D_p-D_{p+\varepsilon}\rVert^2_2 = \frac{1}{{2}}\left((\sqrt{p}-\sqrt{p+\varepsilon})^2+(\sqrt{1-p}-\sqrt{1-p-\varepsilon})^2\right) \\ &= \frac{1}{2}\left({p(1-\sqrt{1+\varepsilon/p})^2+(1-p)(\sqrt{1}-\sqrt{1-\varepsilon/(1-p)})^2}\right) \end{align}$$ Assuming wlog $p\leq 1/2$, we can see easily by a Taylor expansion that this is $$\begin{align} d_H(D_p,D_{p+\varepsilon})^2 &= \Theta\left(\frac{\varepsilon^2}{p}\right) \end{align}$$ leading to the same answer as Yuval Peres' (from a different method). Interestingly, this also shows the usual observation, that the quadratic relation between TV and Hellinger distance can matter a lot: for $p=1/2$, the bound $1/TV$ (i.e., $\Omega(1/\varepsilon^2)$ here) is tight; but for $p=O(\varepsilon)$, then it is quadratically worse than the optimal, which is $1/d_H^2$ (that is, $\Omega(1/\varepsilon)$). 

While this doesn't answer your exact question, CFG parsing is a decision problem that was reduced from matrix multiplication (so it is as hard as matrix multiplication in a sense). Specifically, in [1] it was shown that CFG parsing is as hard as boolean matrix multiplication. In particular, if CFG parsing (a decision problem) can be solved in $O(gn^{3-\epsilon})$ time, boolean matrix multiplication can be solved in $O(n^{3-\epsilon/3})$ time. An interesting aspect is that matrix multiplication can also be used for fast CFG algorithms, so the problems are computationally equivalent in a sense. The reduction has some unusual aspects because boolean matrix multiplication requires $n^2$ output bits, whereas CFG parsing only requires one. To deal with this, the paper assumes that the CFG parser solves certain subproblems when parsing the string (and argues that this is a reasonable assumption to make). The reduction makes $n^2$ queries to these subproblems to obtain the product matrix. Thus CFG parsing is a decision problem that is computationally as hard (in a sense) as matrix multiplication. However, this is not specifically a decision version of matrix multiplication, and furthermore, the reduction relies on the idea that CFG parsing is actually made up of $n^2$ decision subproblems. 

I realize the problem itself would need a more thorough and precise formulation to be tackled, as in the above we deal with full real numbers (so "hiding" information in a single real number, say the weight of a node in the middle layer, would give a very easy way out). But with this dealt with appropriately, hopefully there are non-trivial statements to be made with regard to what compression can be achieved, with respect to some distribution over the inputs? Has this type of question been looked at from a theoretical viewpoint, in our community or another? 

A somewhat more elementary, and slightly messier proof (or at least it feels so to me). For convenience, write $\varepsilon = \frac{\gamma}{\sqrt{n}}$, with $\gamma\in [0,1)$ by assumption. We explicitly lower bound the expression of $\operatorname{d}_{\rm TV}{(P,U)}$: \begin{align*} 2\operatorname{d}_{\rm TV}{(P,U)} &= \sum_{x\in\{0,1\}^n} \left\lvert{ \left( \frac{1}{2} + \frac{\gamma }{\sqrt{n}} \right)^{\lvert{x}\rvert}\left( \frac{1}{2} - \frac{\gamma }{\sqrt{n}} \right)^{n-\lvert{x}\rvert} - \frac{1}{2^n} }\right\rvert \\ &= \frac{1}{2^n}\sum_{k=0}^n \binom{n}{k}\left\lvert{ \left( 1 + \frac{2\gamma }{\sqrt{n}} \right)^{k}\left( 1 - \frac{2\gamma }{\sqrt{n}} \right)^{n-k} - 1 }\right\rvert \\ &\geq \frac{1}{2^n}\sum_{k=\frac{n}{2}+\sqrt{n}}^{\frac{n}{2}+2\sqrt{n}} \binom{n}{k}\left\lvert{ \left( 1 + \frac{2\gamma }{\sqrt{n}} \right)^{k}\left( 1 - \frac{2\gamma }{\sqrt{n}} \right)^{n-k} - 1 }\right\rvert \\ &\geq \frac{C}{\sqrt{n}}\sum_{k=\frac{n}{2}+\sqrt{n}}^{\frac{n}{2}+2\sqrt{n}} \left\lvert{ \left( 1 + \frac{2\gamma }{\sqrt{n}} \right)^{k}\left( 1 - \frac{2\gamma }{\sqrt{n}} \right)^{n-k} - 1 } \right\rvert \end{align*} where $C>0$ is an absolute constant. We lower bound each summand separately: fixing $k$, and writing $\ell = k-\frac{n}{2} \in [\sqrt{n},2\sqrt{n}]$, \begin{align*} \left( 1 + \frac{2\gamma }{\sqrt{n}} \right)^{k}\left( 1 - \frac{2\gamma }{\sqrt{n}} \right)^{n-k} &= \left( 1 - \frac{4\gamma ^2}{n} \right)^{n/2}\left( \frac{1 + \frac{2\gamma }{\sqrt{n}}}{1 - \frac{2\gamma }{\sqrt{n}}}\right)^\ell \\ &\geq \left( 1 - \frac{4\gamma ^2}{n} \right)^{n/2}\left( \frac{1 + \frac{2\gamma }{\sqrt{n}}}{1 - \frac{2\gamma }{\sqrt{n}}}\right)^{\sqrt{n}} \xrightarrow[n\to\infty]{} e^{4\gamma -2\gamma ^2} \end{align*} so that each summand is lower bounded by a quantity that converges (when $n\to \infty$) to $e^{4\gamma -2\gamma ^2}-1 > 4\gamma -2\gamma ^2 > 2\gamma $; implying that each is $\Omega(\gamma )$. Summing up, this yields \begin{align*} 2\operatorname{d}_{\rm TV}{(P,U)} &\geq \frac{C}{\sqrt{n}}\sum_{k=\frac{n}{2}+\sqrt{n}}^{\frac{n}{2}+2\sqrt{n}} \Omega(\gamma ) = \Omega(\gamma) = \Omega(\varepsilon\sqrt{n}) \end{align*} as claimed. 

Chess endgame techniques have been greatly enhanced by the advent of endgame tablebases. Endgame tablebases are lookup tables that solve chess when there are no more than (currently) seven pieces on the board. Here is an online tablebase I've used in the past that works for up to six pieces. Algorithmically, these tablebases are not very interesting; they are generated mostly by brute force. However, they have contributed to several aspects of endgame theory. Wikipedia has a nice summary of some interesting points here. These discoveries also had implications for the "fifty move rule," which states that after fifty moves without a capture or pawn advance, either player can claim a draw. Even before computer analysis, several endgames were thought to take more than fifty moves, and the rule was slightly extended in those circumstances (probably the most famous is the rook and bishop vs rook endgame). As the number of positions requiring these moves became larger, these extensions were dropped and the normal 50-move rule was reinstated in all cases. Modern analysis has shown that some endgames take several hundred moves . This is another interesting article, summarizing some effects of seven-piece tablebases on endgame theory. I particularly like the mutual zugzwang shown in the last position. 

On Universal Learning Algorithms, Oded Goldreich and Dana Ron (1997), Information Processing Letters, Volume 63, Issue 3, pp. 131-136. (see also the updated version) Adapting Levin's argument for the existence of an optimal algorithm for NP, the authors show that there exists a universal learning algorithm (in several learning settings, including PAC): "if a concept class is learnable, this algorithm will learn it, optimally." Beyond the result itself, and perhaps more strikingly, this is also (as pointed out and discussed in the paper) a great illustration of the dangers of abusing $O(\cdot)$ notations and asymptotics. 

As answered there, the general one-sided version is solved in the following paper of Egor Klenin and Alexander Kozachinsky [1], who show that (with your notations) $\tilde{\Theta}\!\left(\frac{t}{(1-\alpha)^2}\right)$ bits are necessary and sufficient. [1] One-sided error communication complexity of Gap Hamming Distance, Egor Klenin and Alexander Kozachinsky, 2016. ECCC TR16-173. 

Papadimitriou, Christos H., and Mihalis Yannakakis. "A note on succinct representations of graphs." Information and Control 71.3 (1986): 181-185. 

I assume you're talking about maintaining keys, in order, in an $O(N)$-sized array, with $O(\log^2 N)$ amortized worst-case update time. 

The first thing to notice is that an $n$-vertex polygon polygon $B$ is inside circle $A$ if and only if all of the vertices of $B$ are inside $A$. (1) So the solution to your problem is exactly the area of $n$ arbitrary intersecting circles, each of radius $r$ and centered at a vertex of $B$. (You then divide by $S$ to get the final probability). There's an interesting blog about this problem here which includes some heuristic solutions which may be sufficient for your use case. From a theoretical point of view, the furthest-point Voronoi diagram gives us a good starting point. Let's look at a point $p$ inside a cell of the furthest-point Voronoi diagram. We want to know if the circle $A$ centered at $p$ contains $B$. By the above, this happens if and only if all vertices of $B$ are within distance $r$ of $p$. Now, since we're inside a cell of the furthest-point Voronoi diagram, we can just test whether or not $p$ is within distance $r$ of that furthest point. In particular, the area we want within a given Voronoi cell is the intersection of that Voronoi cell with a single circle of radius $r$. This can be easily calculated in time linear in the number of edges of the cell. Since the furthest-point Voronoi diagram has $O(n)$ cells with $O(n)$ total edges, this takes $O(n)$ time in total. Then the total running time is dominated by the time to calculate the furthest-point Voronoi diagram, which is $O(n \log n)$. A similar strategy gives you an explicit region $C$ in the same amount of time. (1): The "only if" direction is obvious. To see the "if" direction, we can first note that by convexity if all vertices of $B$ are inside $A$, then all edges of $B$ must be as well. The same argument then extends to all interior points of $B$ since they must lie on a line between two vertices or edges. 

My apologies if the question is a tad vague—I did try to search the literature for more, but didn't find anything (the similarity between the keywords "Takens" and "taken" on Google may be partly to blame). If I understood correctly the Wikipedia page on Takens' theorem, and some discussions I had with people in applied (physics-related) fields that mentioned it to me, Taken's theorem essentially states that 

(Sorry, this is a bit biased towards papers I have co-authored, mostly due to my familiarity with those.) 

Here is a detailed outline, not entirely made rigorous. Setting $b_n \stackrel{\rm def}{=} \frac{1}{6}-Y_n$, with $b_0 = 1/6$, we have $$ b_{n+1} = \frac{4}{3}\left( \sqrt{1+\frac{3}{2} b_n - \frac{9}{2} b_n^2} - 1\right)\tag{1} $$ (I like to set things near zero.) By induction, $b_n \geq 0$ for every $n$, and a simple computation shows that $b_{n+1} - b_n \leq 0$ for all $n$. By monotone convergence, $(b_n)_n$ converges, and the fixed point being zero we have $$\lim_{n\to \infty}b_n = 0\,.\tag{2}$$ We can do a Taylor expansion of (1) to get $$ b_{n+1} = b_n - \frac{27}{8}b_n^2 + o(b_n^2)\tag{3} $$ i.e., summing, $$b_{n} = b_0 - \frac{27}{8}\sum_{k=0}^{n-1} \left(b_k^2 + o(b_k^2)\right)\,\tag{4}$$ Solving the recurrence $$ \tilde{b}_{n} = \frac{1}{6} - \frac{27}{8}\sum_{k=0}^{n-1} \tilde{b}_{k}^2$$ yields $\tilde{b}_{n} = \frac{1+o(1)}{6+\frac{27}{8}n} \displaystyle\operatorname*{\sim}_{n\to\infty} \frac{8}{27n}$. (For instance, solving the continuous version $h' = -\frac{27}{8}h^2$ with $h(0)=1/6$.) "Therefore", $b_n \displaystyle\operatorname*{\sim}_{n\to\infty} \tilde{b}_{n}$ and $$ Y_n = \frac{1}{6} - \frac{8}{27n} + o\left(\frac{1}{n}\right)\,.\tag{5} $$ 

Lee, Lillian. "Learning of context-free languages: A survey of the literature." Techn. Rep. TR-12-96, Harvard University (1996). 

I think this paper may be of some help. Obviously it's not the same problem--in fact it's the reverse problem, covering a rectangle with polygons--but some of the ideas could be a starting point. In particular, this reverse problem is NP-hard and I suspect yours may be as well (though there's no obvious extension of the reduction as far as I can tell). E. Arkin, A. Efrat, G. Hart, I. Kostitsyna, A. Kroller, J. Mitchell, and V. Polishchuk. Scandinavian Thins on Top of Cake: On the Smallest One-Size-Fits-All Box. Fun with Algorithms. pg.16-27. 2012 

A paper by Abboud et al. recently accepted to SODA 2016 shows that subtree isomorphism cannot be solved in $O(n^{2-\epsilon})$ time unless the strong exponential time hypothesis is false. Of course, we can verify an isomorphism in linear time. In other words, the SETH gives us a natural problem in $\sf{P}$ with an $\Omega(n^{1-\epsilon})$ gap between finding and verifying. In particular, a $O(n^2/\log n)$ algorithm is known for rooted, constant-degree trees (for which Abboud et al.'s lower bound results still apply). So under SETH, the almost linear find-verify gap is essentially tight for this problem. 

[BCOST15] Eric Blais, Clément L. Canonne, Igor Carboni Oliveira, Rocco A. Servedio, Li-Yang Tan. Learning Circuits with few Negations. APPROX-RANDOM 2015: 512-527 

(dominated version) $y \preceq x$, i.e. $y_i \leq x_i$ for all $i\in[n]$; or (one-sided version) $\operatorname{d}_H(x,y) = \frac{n}{2}$; or $\operatorname{d}_H(x,y) \geq \frac{n}{2}+\sqrt{n}$ 

On the other hand, Blais [2] showed that non-adaptive testing of $k$-juntas had query complexity $\tilde{O}(k^{3/2})$, hence the separation. 

As mentioned in a comment above, the Boolean Hidden Matching Problem introduced and studied in [BJK04,KR06] seems to (almost) meet your requirement. The input size is roughly $n\log n$ (as an input is of the form $(x,M,w)\in\{0,1\}^{2n}\times\{0,1\}^{n\times 2n}\times \{0,1\}^{2n}$, where $M$ is a very sparse matrix that can be encoded with $n\log n$ bits); and $\textsf{yes}$- and $\textsf{no}$-instances of the promise problem have distance $\Theta(n)$, The one-way randomized communication complexity of $\textsf{BHM}_n$ is $\Omega(\sqrt{n})$, as shown in [KR06]. 

A modified Bentley-Ottmann sweep algorithm (here, for example) will find all line segment intersections in $O((n+k)\log n + k)$ time, where $k$ is the number of intersections. It's fairly straightforward to extend the algorithm to handle circles. A good walkthrough is here under "finding intersections". $k$ can be $\Omega(n^2)$ in theory (in which case pairwise checking is faster), but it seems difficult to avoid the $k$ term, and in any case you seem to say $k$ will be small in your question. Granted, this recomputes the intersections from scratch each time you run the algorithm, which may be inefficient for your purposes. You may be able to get around this a bit by maintaining the list of circles sorted by $x$ coordinate, and modifying this list online as the circles move. This won't help your asymptotic running time, but it avoids sorting each circle every time. If the circles stay relatively sparse (few cross the same vertical line), the binary search tree on the $y$-coordinates will be cheap to maintain. The other issue is that knowing two circles intersect already does not improve the running time at all. Once two circles attach to each other, you will calculate their intersection every single time, which is a waste. 

[1] Erik Waingarten and Amit Levi. Lower Bounds for Tolerant Junta and Unateness Testing via Rejection Sampling of Graphs, 2018. arXiv:1805.01074 [2] Eric Blais. Improved Bounds for Testing Juntas. APPROX-RANDOM, 2008. [3] Roksana Baleshzar, Deeparnab Chakrabarty, Ramesh Krishnan S. Pallavoor, Sofya Raskhodnikova, C. Seshadhri. Optimal Unateness Testers for Real-Valued Functions: Adaptivity Helps. ICALP, 2017. [4] Xi Chen, Erik Waingarten, Jinyu Xie. Boolean Unateness Testing with $\tilde{O}(n^{3/4})$ Adaptive Queries. FOCS, 2017. 

Let $f\colon 2^{[n]} \to \mathbb{R}$ be a submodular function (one can assume $f$ is bounded, if this helps). We are given noisy oracle access to $f$: on any $S$ and for any $\tau > 0$, one can obtain an additive $\tau$-approximation of $f(S)$ (at cost $\operatorname{poly}(1/\tau)$). I am interested in what is known in minimizing (up to some arbitrary (additive) accuracy $\alpha>0$) $f$, given this sort of access: If we had exact oracle access, then this could be done in polynomial time, exactly; what about robustness to approximate queries? Is there any algorithm (or, on the other side of the spectrum, lower bounds) for it?