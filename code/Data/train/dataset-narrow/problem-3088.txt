I want to choose the "optimal" hyperparameters for gbm. So I run the following code using the package 

I also get the whole data table as output in my console, which floods my console. Any ideas how to fix it ? (It might also be a setting in R, but I am not aware of it. 

I have 2 matrices, A (1000x21) and B (1000x7). Matrix A has individuals(=1000) in the rows and their consumption in 21 days at the columns. Matrix B has the SAME individuals(=1000) in the rows and some weights for each day of the week(=7) in the columns. What I would like to have in the end is 1000 (with the dimension of 2x21) matrices (one for each individual), lets call them $X_{i}$. In the first row of each $X_{i}$ I would like to have the consumption of the individual $i$ each of the 21 days (this will come from matrix A), and at the second row of the $X_{i}$ I would like to have the respective weight of that day (this will come from matrix B). So matrix A looks like $[cons_{1,1} \ cons_{1,2} \ ... \ cons_{1,21} \\ \ cons_{2,1} \ cons_{2,2} \ ... \ cons_{2,21} \\ . \\ . \\ . \\ \ cons_{1000,1} \ cons_{1000,2} \ ... \ cons_{1000,21}] $ Matrix B looks like $[weight_{1,1} \ weight_{1,2} \ ... \ weight_{1,7} \\ \ weight_{2,1} \ weight_{2,2} \ ... \ weight_{2,7} \\ . \\ . \\ . \\ \ weight_{1000,1} \ weight_{1000,2} \ ... \ weight_{1000,7}]$ And I would like the matrix $X_{i}$ to be like $[cons_{i,1} \ cons_{i,2} \ ... \ cons_{i,21} \\ \ weight_{i,1} \ weight_{i,2} \ ... weight_{i,k}]$ Any ideas how to do this in R in a loop ? 

I recall I was struggling for some time deriving the second equation. That constant keeps many of your missing elements. Let's break it down using your $(a-b)^{2}$ notation. We will have $a^{2}$, $b^{2}$, and $2ab$: 

On top of what Wikipedia says I would add: Hyperparameter is a parameter that concerns the numerical optimization problem at hand. The hyperparameter won't appear in the machine learning model you build at the end. Simply put it is to control the process of defining your model. For example like in many machine learning algorithms we have learning rate in gradient descent (that need to be set before the learning process begins as Wikipedia defines it), that is a value that concerns how fast we want the gradient descent to take the next step during the optimization. Similarly as in Linear Regression, hyperparameter is for instance the learning rate. If it is a regularized Regression like LASSO or Ridge, the regularization term is the hyperparameter as well. Number of features: I would not regard "Number of features" as hyperparameter. You may ask yourself whether it is a parameter you can simply define during the model optimization? How you set the Number of features beforehand? To me "Number of features" is part of feature selection i.e. feature engineering that goes before you run your optimization! Think of image preprocessing before building a deep neural network. Whatever image preprocessing is done is never considered hyperparameter, it is rather a feature engineering step before feeding it to your model. 

I have a that contain around 3000 observations. Every observation falls in one of the five categories (these are pre-defined). I am using and from the package for classification. Before I run the algorithms i optimize the of each algorithm using the . For model validation I am having a look at the for both algorithms, for both and . The issue is that in sample the algorithms perform relatively well for all 5 categories but out of sample they they only perform well for one category. I guess that is very common. I have 2 explanations: the 5 categories are not equally divided, this means that for a couple categories I have a few observations. Second, is that the algorithms are overfitting. The "1m dollar question" is how can someone avoid overfitting ? I thought of running a preliminary analysis on my data set, to see which variables (in my case most of them dummy variables) in my data set are the "most important" for the classification, and then use those, instead of the whole data set. This could be tested by taking the means for the variables per category and which variable has the highest difference across the categories. Could this be a good idea ? I understand that the subject is too broad, but I think some ideas from experienced people on this topic could be useful for everyone ! 

While it is always preferred to have more informative variables for building a model, in reality though, perhaps like your case, we have to live with what we have. Although, if I were you I would think of digging other external data relevant to water level like geographical data, climate (there should be many things out there that you can add to your current feature space). Anyhow your current situation, my educated guess is that a simple regression is not a good choice of model anyway. Simply it won't capture any nonlinear correlation between your independent and dependent variables suitable for prediction i.e. water levels of rivers. I would strongly suggest, if you want to build a better predictive model hopefully performing better than a simple regression and still fast and easy, to go with Gradient Boosting Decision Trees (GBDT) either using LightGBM, XGBoost, or recently my favorite Catboost implementations (otherwise you could think of Neural Network as well, but depends how much data you have etc). Each has its own pros and con, please check out this nice video by Mateusz Susikin in PyData Conference 2017 going through some of their differences. Please note when building a GBDT model, you need to be careful how to encode your categorical variables, and not least, how to include your independent continuous variables if they exist. Please go through these stackexchange post1, post2 where I discuss ways to handle them, or at least they may give you some hints. 

I have a panel data set and I have created a model and finally I have obtained some density forecasts. That is, I run my model for the $y_{it}$ and i obtain predictions for $\hat{y_{i,t+1}}$ , $\hat{y_{i,t+2}}$ ,..., $\hat{y_{i,t+h}}$ etc. The $\hat{y_{i,t+1}}$ , $\hat{y_{i,t+2}}$ etc, all of them follow a normal distribution, but with different mean and variance. so $\hat{y_{i,t+1}} \sim N(\mu_{1},\sigma_{1}^{2})$ , $\hat{y_{i,t+2}} \sim N(\mu_{2},\sigma_{2}^{2})$, ..., $\hat{y_{i,t+h}} \sim N(\mu_{h},\sigma_{h}^{2})$. My question is, what is their joint distribution ? And how can I obtain it ? Edit: The $y_{it}$'s are the daily consumption of a product of individual $i$ at time $t$, so yes, they are highly correlated. Thank you for your help :) 

What I would like to do now, is to plot a 3D contour plot (so that I can actually see the "mountain" that is created after plotting the histogram of $y_{1}$ against the histogram of $y_{2}$). So in the z-axis I would like to have the frequencies of the values. Any suggestions ? 

I am not aware about Instagram, or if there is a ready-to-use dataset for your purposes, but in general in order to get data from (almost) any online source you should a method called "web scraping" (This might help you while you google). It might also be useful to you, if you decide what kind of tools you are going to use for your analysis. I have tried something simple, but it was for Twitter though Here is a rather simple piece of code that gets twitter tweets concerning using 

QUICK update (a good solution): I found a Python implementation of KLIEP algorithm of that research paper (last point) to find those weights. It rather seems easy to use! Basically it resamples the training by putting weights (via the KLIEP algorithm) so that the assumption of having a similar distribution of train and test holds true as much as possible. 

There is a very cool active Python package called pandas-profiling, is exactly what you want. With a simple it returns a lot of important statistical information about your data, the official documentation says: 

One of the critical assumption one would make to build a machine learning model for future prediction is that unseen data (test) comes from the same distribution as training data! However, in reality this rather simple assumption breaks easily and upcoming data (its distribution) changes over time for many reasons. For those who may not be familiar with this very important problem, I encourage looking here or post! To me, your question falls into the same category. Although I do not have the perfect solution (an implementation to offer), but I think you may look: 

Well it was not an impossible task. CelebA dataset is large, well not super large compared to many other image datasets (>200K RGB images, totally 1.4GB in size, each image ~ 8 KB). YET surprisingly it takes the hell of the time to convert these images to numpy arrays and even stuck during the run of a small CNN model. My computer specs: MacBook Pro (2015), Memory: 8GB, Harddisk: 128 GB. Even with almost more than 4GM free memory, and 20 GB free hard-disk I could not manage this on my local machine. UPDATE: It seems quite possible and way more efficient via method of in Keras. While it is not that option for this multi label classification at hand, I simply made some dummy subclasses and it worked and the model runs much faster!! To My Questions (finally!!): 

Here is the code I wrote to answer my question. It might not be the most efficient one but it works. Sharing is caring :) 

The , , and can be obtained as it is explained here in Also in that site you can see a guide of how to web scrap data from Twitter using Python. You might also find useful this (Python), this (Python) and this (R) 

I have 2 vectors $1000 \times 1$, lets call them $y_{1}$ and $y_{2}$. Each vector represents a normal distribution with certain mean and variance. I plot the contour plot using the following R code: 

Assuming that we have a dataset with the respondents on each row ( respondents) and their respective characteristics as columns ( characteristics). Each respondent has also a . In case of high number of respondents, is it a good idea to remove the duplicate respondents and sum their s ? Will this lead to different results ? So my initial data would look like this 

I know that i use as argument , but still for instance the optimal combination for gbm using is the "50th optimal combination" for the gbm using , and the "2nd optimal combination" for gbm using , is the "14th optimal combination" for gbm using 

Well, that is not true that Boxplot only gives hard stops! Violin plots are rather contemporary version of Boxplots I would say, and easy for the eyes to see the distribution of data. Boxplots also can reveal how data is distributed. For example, here you see a Boxplot of a normally distributed data that is symmetrical with the mean and median in the center (top), as well as a non-normal data (bottom). The source is here. 

Please note you do not need a model with probabilistic nature. See this post, or this answer or this one. 

Have you tried regularization (L1, L2)? Have confident you are about the independency of your independent variables? In simple regression you are assuming this holds, but may it is not which is easily the case that your independent variables are correlated. Maybe you should analyze this first using confusion matrix (see here for a brief tutorial) Have you tried gradient boosting decision trees (GBDT) for regression? Note1: If you want to go with GBDTs, do not leave that other 1 independent variable alone. You said you have 6 independent variables out of 7. I assume the other one is continuous numerical values? If that so, you have to handle that properly in GBDTs like binarizing (put in bins). Note2: You may try either XGBoost, or Catboost, and a nice tutorial using Catboost. Each has its pros and cons. For the former hyperparameter tuning could be challenging and there is no ways to automatically account for categorical variables but have larger community. While the latter is recently released (mid last year), it is much easier in terms of hyperparameter tuning (often defaults works great), and there are ways to automatically account for categorical features without explicitly encode them, but less support and community is smaller. Note3: The good thing about GBDT is that you do not need to worry about the correlation of features per se. Algorithm automatically ignore the coupling of strongly correlated features. Note4: You may easily overfit, although there are built-in regularization. Catboost for example have a interactive learning curve for training and validation to control overfitting (see the last tutorial).