but I think work is happening right now to improve that in the doc. Another issue that might be worth pointing out is that there are multiple ICU major versions out there, and the list of pre-defined collations that instantiates differs significantly between these versions, especially ICU pre-53 versus post-53. This has been put in evidence in another discussion: Crash report for some ICU-52 (debian8) COLLATE and work_mem values EDIT: in the next beta or RC, the list of ICU collations created by initdb will change thoroughly , as mentioned in this discussion. The list will made by iterating on ICU locales rather than collations, and as a result the question "What are the possible ICU collation names?" will have to be reconsidered with the new values. 

Maybe you want to limit the clients to those which present a given certificate, as described in Using client certificates: 

This error generally means that the server is not running. Based on output and the thread of comments, it was due to the main package being somehow uninstalled. Since the uninstall hasn't been called with the option to , the data and configuration files are still there, so can fix the problem. 

Assuming the locale of the database is both in Ubuntu (master) and FreeBSD (slave), I believe the differences in sort semantics alone may account for the fact that the index is unusable on the slave. Here's an example of how they sort differently: On Ubuntu 12.04: 

It's just a case folding problem. You're trying to connect with but due to case folding rules, you created in lowercase, as the output shows. Per documentation: 

This bug has been seen to set password expiry dates far in the past, such as 1/1/1970. In this case the error message when trying to connect is no different than with a wrong password. You can check these expiry dates with: 

To clear the WAL files, see pg_resetxlog. The data directory on Ubuntu 12 should be Note that is located in , not in , so it's not necessarily in $PATH. Also it should be run as the user. To clear and recreate the entire cluster if you don't care about the data, run: 

Even if extracting fields from a date would always produce results that could fit in an integer, according to the doc, doesn't directly work on a type: 

so it does not fit both in the return type and in the argument types. A trigger function does not take arguments but can access the values inserted or changed in the row variables and . They get automatically defined in plpgsql, see Trigger procedures in the plpgsql chapter for details and examples. Concerning the error message: 

doesn't provide a command line option for the password to avoid disclosing it to other local users. However it accepts the environment variable, so your equivalent of mysql's is, in shell syntax: 

It's a temporary table that wasn't removed automatically when it should have. The fix from the point of view of a DBA is to simply drop the table manually. There are several references to this problem on mailing lists. See for instance [bug fix] Suppress "autovacuum: found orphan temp table" message for a discussion on this subject. 

To save bytea to the disk server-side, it is necessary to be a database superuser. Normal users are not allowed to write to the filesystem. Assuming superuser rights, the simplest way is to implement it as a function in one of the "untrusted" languages. Example in pl/perlu: 

Since you mention a salt, I assume you're looking for short strings that are hard to guess or reproduce by an outsider, despite being originated with a database sequence. The permuteseq extension essentially does that. The function that produces a short string from an integer has to be provided, for instance: 

That's a typical case of case misuse. When creating the view, notice that is written with a uppercase plus double quotes. This implies that any future reference to this object must include the uppercase . Yet in the error message it's a lowercase so that name does not refer to the view, it refers to nothing, hence the error. The solution is to either not use double quotes at all, or use them consistently. 

That invocation outside of SQL might be handy in the rare cases when these expressions may have to be computed in the middle of a failed transaction, or when disconnected from the database. 

knowing that is the system pseudo-column that indicates the physical location of the row version within its table. Alternatively, the primary key can be used if there is one. In your case, the transformation is not obvious because your subquery is meant to produce only one row with its at the end. It should be redesigned to produce all the target rows in a single resultset, presumably where and are determined per . 

I believe the settings you mentioned already ensure that geqo will be used, except that the values for and are so low that the results won't be good. Apart from that, to get different plans, you should play with . From the doc's section Generating Possible Plans with GEQO: 

Someone recently submitted a formal bug report that looks identical to your question. Without any follow-up so far. If the problem is a mismatch between pgsql_fdw implementation and the way the plpgsql interpreter prepares its queries, using EXECUTE might help: 

It's done with See ALTER USER in the doc. To drop a database, either you're superuser (which can be granted with too) or you must own the database. 

Personally I had success with when faced with this problem with a MySQL client app, but don't take this as a recommendation, my understanding of TCP being superficial at best. 

The first subquery extracts the lines to draw between dimensions 1 and 2, the next subquery between dimensions 2 and 3, and so on. eliminates duplicates, so the client side doesn't have to do it. The concatenates the results without any further processing. However it's a heavy query and it's dubious that it would be any faster than what you're already doing. The subquery is likely to gets slowly materialized on disk, so it might be interesting to compare the execution time with this other form repeating the same condition: 

Usage: the same as pl/perlu version. You still need to be superuser, unless the superuser privileges for this function only are granted to other users (which in the present case would be only OK if 100% trusting the users, otherwise it's disastrous): 

and the would evaluate to the same value for both columns of the same output row, as mandated by the standard. But PostgreSQL does not have this construct. 

Whether you're using a rule or a trigger to divert the INSERTs into the child tables, immediately after the INSERT you may use: 

You want to do an incremental backup of the archive folder to remote storage. Should you need to restore from the backup, the basic scenario is that you'd need your base backup as the starting point, and the entire contents of the archive folder to replay the transactional activity that happened between the starting point and the crash. Also to avoid having the files in the archive folder piling up forever, you want to do a new base backup from time to time and delete the files that were archived before the new base backup. 

The command needs an exclusive lock, which affects concurrent readers in a way you may want to anticipate: 

takes a file path as argument and imports the file contents. As you want to avoid the round-trip to the file system, that function won't do. I haven't use Python/psycopg2 personally, but from a glance at the doc it appears the class you're looking for is lobject. It provides the usual read/write/close methods wrapping the libpq API for large objects. When creating a new large object, the OID that postgres assigned to it seems to be available through . 

You don't have to specify and the port number, so it connects through Unix domain socket instead of a TCP/IP connection, which differs in these two ways: 

Maybe it's just index bloat. does not help with index bloat, on the contrary, as said in the doc for 8.4: 

(emphasis mine). This seems to support what you wrote in the question: a specific column of a specific row. 

If no delete or update happened, autovacuum shouldn't process the table, per . See Automatic Vacuuming for the relevant configuration parameters. You may set to zero to help figure out what's being autovacuumed. In any case, it is possible to completely disable autovacuum for a particular table, with: 

If PostgreSQL had a standard-compliant NEXTVAL, you could do without the subquery and just use something like: 

The mentioned demo would need statement pooling in a multi-statement transaction. So in order to make it work, you'd need to hack pgBouncer to bypass that check. That's technically possible, but in the end it would demonstrate a setup that never exists in practice. In this sense, it has more potential to confuse people rather than to help them understanding how isolation works. 

Aside from invisible characters, the possibility of an OS patch changing the results of such an query exists through a change in locales. An index involving text types is only valid if the underlying collation never changes. Postgres relies on the OS for comparing strings, through the libc, so a change in an OS collation may have the effect of an index becoming unusable, as if it was corrupted. If there is an index on the column, you can test this hypothesis by issuing before the offending query and see if it gives the same results. This check may also reveal an index corruption independantly of any problem with the collation. 

EDIT: a web search on the error message reveals a blog post from Robert Berry suggesting a method that seems less fastidious than the above. Essentially, create this function: 

The simplest way in SQL is to query the view with a WHERE clause on and matching yours. All the properties you want (and more) are in the output columns of this single view. This view is part of the Information Schema whose purpose is to provide standard ways to do database introspection. 

You may also split a suspicious string character by character to look at each codepoint individually, with the character, the Unicode codepoint, and the UTF-8 representation 

I don't know if that applies to Amazon RDS, but a possible reason for getting an empty list of queries with is when using a version older than 3.7.0 (released in August 2013) against a 9.2 or newer PostgreSQL instance. I just noticed this with the ptop 3.6.2 package that currently ships with Ubuntu 12.04 LTS. I guess this may be also not fixed in other distributions. The empty list problem occurs with older versions of because they expect the postgres process ID in , and this column was renamed to in PG 9.2. This is combined with being unhelpful by not reporting any error when its SQL command fails. Instead it just displays a blank list of processes. The problematic query can be spotted in the server's log. 

It must be executed by another superuser. The question says there are two accounts that are superuser and you do have their passwords, so you should be able to log in with one of them and just issue that SQL command. If you didn't know these passwords, you could still log in bypassing the password by temporarily changing . How to do that is a frequently asked question, see for instance: remove password requirement for user postgres . 

So again it's being taken care of automatically, albeit differently than with 8.1's pg_dump, but this dump is going to be reloadable directly in any version of postgres>=8.1, without tweaking anything. 

Anyway, if can contain problematic characters such as separators or quotes, either double or single or both, none of the above is good enough. As your SQL statements need the table's name both as a literal string and as an identifier, the problem is not as easy as it would seem. The ultimate solution, to cover for any syntactic hazard in the table's name, could be to define and call a temporary plpgsql function that uses dynamic SQL functionalities.