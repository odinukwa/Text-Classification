are you sure you are not re-intersecting the same surface when checking for light occluder ? this is a classical precision issue. There are many way to tackle it: 

Don't forget that at the end this is to be integrated in some cone (e.g. pixel footprint). Then the visible cross section of differential surface dN is also be $(N\cdot V)$. 

I know that by now it might be considered as a kind of sad recurrent joke, but by chance, does anyone here has the least information about the colorspace of RGB values in the MERL BRDF measurement database ? 

the arbitrary line can be represented as $P = (x,y) = P_0 + \lambda. \vec{dir}$ (works in n dimensions, no special case). If your other line is $x=x_1$ simply inject this in to solve for $\lambda$ and get $y$: $y=y_0+(x_1-x_0).\frac{{dir}_y}{{dir}_x}$. 

The opacity acts linearly through the colors, i.e. it affects R G B similarily, so it should affects the value V the same way. You just want to have an intensity factor decay as $0.95^n$. 

When you use linewidth or line antialiasing or pointwidth or pointsprites, OpenGL creates for you a small rectangle instead of the line or point, with texture coordinates. Nowadays you can even program this yourself using geometry shaders ot even the tesselator. A totally different approach is to use defered shading, using one geometric pass just to store information in the RGBAZ buffer, then a second pass than you run on all the pixels of the screen to do some process. (to act on all pixels, simply draw one full-screen rectangle). Nowadays you can even do the first pass as one or several "render to texture", then MIPmap these textures, so that the final pass can easily access to less local values. 

Difficulty depends on what you really want to classify, i.e. what exactly your data base of images is made ok. For simple hand-drawn or computer art, I would think of just looking the color histogram (but for complex art with gradients; but still these gradients are likely to induce a very smooth colormap). At the other extreme, distinguishing realistic computer graphics image from reality is probably impossible. 

You can get fancy with individual weighting on the blur maps if you want to adjust the look, but an equally weighted mix (and yes, it should be additive) will work too. I’m not sure about whether you need to gamma-correct the blurred image before adding it to the source one, but you should definitely get it into linear space before doing the blur; this article has some good illustrations as to why. 

As you’ve found, it’s hard to use a two-dimensional format, even from multiple viewpoints, to represent a 3D structure that’s not axis-aligned. I’m not sure I have a useful suggestion there—there’s a reason most 3D file formats support specifying translation/rotation/scaling of individual parts. A different direction you might try would be using the existing work in the LDraw project; their file format is technically ASCII, and though it’s closer to a standard 3D format in that the fundamental parts (like studs) are built out of lists of vertices / triangles / etc., the higher-level pieces (bricks, plates, etc.) are built out of references to the fundamental ones, which sounds like it’s roughly along the lines of what you’re after. There’s an assortment of third-party design tools that provide GUIs on top of the LDraw parts library; of those, I’m partial to Bricksmith. 

It doesn’t matter that it’s a point light; the 1/π normalization factor is what’s causing your surface to be 1/π as bright as you’d like it to be. You can either up your light intensity so the maximum brightness is π or just get rid of the normalization factor. There’s a good article with some notes about this and how it interacts with more complex lighting techniques here. 

The left one is all quads; the right one has the same overall shape, but one corner is made of triangles instead of quads. Now see what happens when the boxes get subdivided: 

For 3D modeling, the usual reason to prefer quads is that subdivision surface algorithms work better with them—if your mesh is getting subdivided, triangles can cause problems in the curvature of the resulting surface. For an example, take a look at these two boxes: 

It speed does not matter, I suggest to use a truncated sinc or a Lanczos isotropic kernel: to compute a target pixel, you back-rotate the filter and convolve it with the image. Since it is isotropic, it is separable and you can even use a square filter parallel to the axis of the source image. 

with $C_{vox},T_{vox}$ the color and transparency of the current voxel, and $C_{pix},T_{pix}$ the cumulated color and transparency of the pixel. 

why not building a bounding box (or spheres) hierarchy ? (but for a shadertoy implementation, the lack of dynamic loop length might spoil the gain ). 

Color spaces issues are such a pain, especially when you create color data at the middle of the chain, and a pile of libs and apps separate this to the display... Here I only worry about gamma transform. Shadertoy addicts are coding in this web site webGLSL shaders, which in my case are rendered through google chrome on linux ubuntu and displayed on a calibrated monitor. Is one layer already doing the gamma transform, or should I do pow(color,1./2.2) at the end of my rendering ? For me (linux,etc), it seems I must do it. Is it also true on windows, mac OS, whether Angle or Native OpenGL is used (windows), whatever the browser ? The point is that often shaders look ok on windows and very dark on linux, for instance, both people being sure of their settings and qualibration. 

Volume rendering is not like ray-tracing, it is like "scene rendering". i.e. there exists several algorithms to render volumes. One close to ray-tracing is ray-marching, and has may variants. The simplest: for every pixel in the screen plane, trace a ray starting from the eye point to the screen pixel location, and advance along the ray by constant steps. At each voxel calculate the pixel color and transparency, blend it to the pixel value, and so on. Blend = 

Someone points me that this article describes the spectral response of the camera the MERL paper used, and even provides colour transform matrices at the end. Unfortunately, using those transforms result in even worse results than assuming sRGB primaries. 

…which is a 4×4 square, centered on (0,0), with the points specified in clockwise order (which won’t draw correctly with a triangle strip, as you’re seeing in the above screenshot). It looks like you’re expecting values outside of the [-1, 1] range to not be drawn at all, but all that’s doing is drawing triangles that are larger than the viewport. 

There’s also the assortment of techniques in the GPU Gems article PaulHK linked to. One thing I’d add is about the light-falloff-texture approach mentioned at the beginning of it, which Valve used for a couple of interesting effects in Source games in addition to fake SSS—see their documentation for some information about that. 

See the weird pinching in the highlight on the right box? That’s caused by the messy subdivision. This particular one is a pretty benign case, but you can get way more messed-up-looking results with more complex meshes with higher subdivision levels (like the ones you’d usually use in film). 

Short answer: yes. Longer answer: yes, because the vectors you’re using are meant to represent directions, not directions-and-distances. Think of it in terms of light: it doesn’t matter how far a photon’s traveled, whether it’s from the sun or from a lamp on your desk—once it arrives at a surface, it’s going to get reflected in exactly the same way. From a mathematical standpoint, you’re making lighting calculations based on the angles between the incoming light, the view point, and the surface normal. The dot product is defined as the length of the two input vectors multiplied by each other and by the cosine of the angle between them. To get just the angle—which you need for the lighting model—you need to factor out the length of the two vectors, i.e. normalize them. 

You might find this paper useful—its authors describe a way to make reasonably plausible caustic effects just by modulating their shadows with the surface normal and refractive index. It won’t get you the more complex/interesting hot-spot shapes that you’d get with photon mapping, but it could work well depending on your scene. 

You can use this formula on each edge where one point is < isovalue and the other is > . Then you link together the points belonging to the same square to form the edges of your iso-mesh. Then you make polygons by linking edges sharing a vertex. In case of more than 3 sides (could be up to 6), it's up to you to prefer spliting the polygon into triangles. 

The first things I would try is to see if smartphone pixels are visible (if the photo is high res) or if strange aliasing occurs (your example image shows both). Another classical test (but not adapted to any scenes) is to detect perspective bias: if the camera is not exactly parallel to the smartphone screen, then 3D objects projected on the image on screen are projected again to the camera captor through a slightly different perspective. If the scene contains spheres, discs (physical or as reflects, for instance), perpendicular angles, or any similar invariant, then you can detect this. (this kind of trick is also used to detect images photoshoped by composition of several images). 

No, because the underlying physics is not the same, nor the lobe shape - not to speak of their parameters such as color and Fresnel term. Specular is really true surface interaction with the interface material/air, so it has Fresnel modulation and the internal medium has no influence on colors. But the surface condition strongly influence the reflectance, of course. Think of it as the reflections on ocean surface. Diffuse is due to the subsurface scattering, light entering the medium, and thus gains the color characteristic and loose directionality. Think of it as the green color of the water turbidity. If depth of penetration is less than pixel size CG people don't call it "subsurface" but the physics is the same. Of course diffuse input is what specular let pass, which is angle (and polarization) dependant. Beside, there exist transparent medium with internal interfaces (e.g. thin shells), so case exists were light goes inside but still acts specularely (or even wavely) and not diffusely. Oppositely, most plastics and paints/varnishs and biological medium are intrinsically transparent, but contain pigments (often based on a metal atom) that cause the opacity (diffusion or specular on these "objects in the object"). Think of them as the fishes in clear water :-) .