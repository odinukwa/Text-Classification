Window: [1, 5, 2] Mean: 2.66 STDev: 2.081 Result[-0.800, 1.120, -0.320] Window: [10, 50, 20] Mean 26.66 STDev: 20.81 Result[-0.800, 1.120, -0320] 

I have a feature that is boolean and I would like to feed it to a neural net as one of the inputs. I think in theory the best is to encode as false->0 and true->1 because 0 as an input will deactivate weights of a neuron. Is this correct? 

Where (i) is the item in Window(n) and (n) represents the Nth sliding window This results an output that keeps the magnitude: 

If I apply a min-max or the Z-score on the entire dataset then the small values will be too close to 0 due to having only a few very high values. Any ideas about how to overcome these problems? 

I use a neural net to generate predictions based on a time series of signals. I use a sliding window to feed the data to an LSTM model. The input signals have a random frequency that - I believe - is a valuable input. Time measure is restricted to seconds. By random frequency I mean the time between the signals varies between 0 (delta not measurable in seconds but order is known) and an extreme of 1-2 hours of rough maximum. I struggle with the following questions: 

This output keeps the Z-score benefits while preserves the magnitude. Can you help me out if this a fair approach? 

How I understand your data is a single feature of temperature. LSTM works with RNN time series like . In your case it is . Forget about the first one, it's technical. 

This is not desired. The magnitude of the input carries information that is useful and I need to preserve it while applying normalization benefits. My best idea is: 

If I apply Z-score exclusively on each sliding window scope then the overall magnitude is eliminated. It would be interesting to keep an indicator of the overall input signal frequency measured on the entire dataset not only on the given window. Shall I create a second feature for compensating this? If I apply a min-max approach on the given window then the small values will be too close to 0 due to having only a few very high values. 

How to convert the frequency to an input feature for a neural net? My best idea is to measure the time elapsed since the last signal. This gives me a feature for each new signal in seconds. Approximate range is 0 through 7200 seconds. Is there any better way? Depending on the answer on point 1 how should I normalize the data? I can have anything between 0 seconds and 1-2 hours. Providing my assumption on point 1 is right then I can add: the median of the entire sliding window is sometimes well below 30 seconds, sometimes well over 300 seconds. Problems I have: 

It doesn't mean the value has to be fixed. It means that we can choose any $z_n$ value, and have the other's adjusted to represent any valid target probabilities. So for argument's sake, we can pick any fixed value. 

Predicting $0.5$ for all items in your case would also give you MSE of $0.25$. That is because independently of whether the true label is $0$ or $1$, the squared error for each example will be $0.5^2 = 0.25$ Your model is performing badly under an MSE measure, when such a simple model that does not take input data into account can get the same score. However, it is debatable whether MSE gives you a useful metric here. 

Depends on the nature of the data. There might be an element of "Scissor/Paper/Stone" in the competition you are scoring, where different strengths and weaknesses of competitors can combine such that Player A beats Player B, Player B beats Player C, but Player C beats Player A. In that case, you cannot produce reliable ranking between players by considering each entrant separately, and a network that rates each player individually will perform less well than one that can compare players. If players are in more of a race-to-finish or score max points separately in a competition, then separately rating each player in each competition should be more reliable. And it is definitely easier to build and train a neural network to predict that. An alternative, if your events are more like tournaments where entrants oppose each other (even if within some larger free-for-all), is to predict relative rank between pairs of players. This may not be consistent, so you will need to use a pairwise ranking method to resolve that for the final winner. If it really is a knockout tournament, and you know how the initial draw and team combinations will work, then you could maybe make a prediction by simulating the possible games. There is nothing preventing you from combining these approaches in some way either. Whichever method you use, you will want to think a little about what your metric is going to be to select the best approach. If you only care about predicting the winner, then accuracy of that prediction might be enough. If you care about where the eventual winner is placed, perhaps mean reciprocal rank would be better (score 1 for correct prediction, 1/2 for predicting winner as ranked second, 1/3 if third etc). 

Most people will refer to mapreduce jobs while talking about hadoop. A mapreduce job splits big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset. 

now hadoop would be done with everything. You can now load the result into the HDFS (hadoop distributed file system) or into any DBMS or file. Thats just one very basic and simple example of what hadoop can do. You can run much more complicated tasks in hadoop. As you already mentioned in your question, hadoop and noSQL are complementary. I know a few setups where i.e. billions of datasets from sensors are stored in HBase and get then through hadoop to finally be stored in a DBMS. 

After the mapping it will reduce the values of each key to a new value (in this example the average over the value set of each key)(figure 3) figure 3 

Let's assume you load into hadoop a set of with the population of some neighborhoods within a city and you want to get the average population over the whole neighborhoods of each city(figure 1). figure 1 

A few gigabytes is not very "big". It's more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don't get TB's of data a day). Most professionals working in a big data environment consider > ~5TB as the beginning of the term big data. But even then it's not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem. i.e. if you do alot of searches in you database it would probably be better to run a solr instance/cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it into solr instead of just moving the data from sql to nosql in term of persistence and performance. 

For my part I can say that I use click frequency on i.e. eCommerce products. When you combine it with the days of the year it can even bring you great suggestions. i.e.: We have historical data from 1 year over 2 products (Snowboots[], Sandalettes[]) 

This problem is not complex enough to justify a large convolutional network. However, if you are determined to use a CNN, then you could, just try to keep the architecture very simple. Just one convolutional layer (probably SAME/padded), no pooling or dropout, only a few feature maps (e.g. no more than 4, maybe just 2 will do) - and a softmax fully-connected layer for the output. Bear in mind that it can take more epochs, and perhaps more tuning of hyper-params, in order to get a more complex model to fit a simple problem. If you follow the plan in your earlier problem, and train against the whole population of valid states and moves, then you don't need to worry about over-fitting. You should bear in mind that tic-tac-toe is simple enough that you can use tabular methods (i.e. approaches that simply enumerate and score all possible game states) to find an optimal policy. The network is being used in your case as a policy function approximation, and is a bit like using a sledgehammer to crack a nut. This makes sense if you are learning a technique used on more sophisticated grid-based games, using a toy problem. However, if your goal was more directly to learn a policy for a tic-tac-toe playing bot, then you would be better off not using any supervised learning model for the policy. 

It doesn't usually matter, provided your NN framework has separate gradient calculation stage from parameter update stage. The gradients should be collected with all weights and parameters at their current values (during back-propagation). Then the parameters are updated, using those gradients. There should be no interaction between the parameters during the update step. However, if you have implemented the training code yourself from scratch, and have interleaved the gradient calculations and weight updates, then this only works in a pure online situation with weights updated as the gradients are calculated. In that scenario, you should probably update the weights for a layer before updating the PReLU parameter for the same layer, to prevent changes in the activation function altering the gradients during your calculations (this is not a concern with usual weight updates). Alternatively, separate your gradient calculations from the weight update steps. This is more flexible and allows you to use other more advanced optimisations and layer designs.