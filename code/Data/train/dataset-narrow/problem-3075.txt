Ok, I was looking for an answer and now I have it clearer: Scipy documentation does not elaborate too much on the explanation, buy wikipedia article is much more clear. For those who are looking for an answer, there are two major groups of sparse matrices: a) Sparse types used to construct the matrices: 

I have a text dataset which I vectorize using a tfidf technique and now in order to make a cluster analysis I am measuring distances between these vector representations. I have found that a common technique is to measure distance using cosine similarity, and when I ask why euclidean distance is not used, the common answer is that cosine similarity works better when vectors have different magnitude. Since my text vectorized representation is normalized I wonder which is the advantage of using cosine similarity over euclidean distance in order to cluster my data? 

You can also choose the method used to calculate the correlation between this: -pearson -kendall -spearman 

I have a pipeline built which at the end outputs a bunch (thousands to tens of thousands or more) of named entities. I'd like to do aggregates on those named entities (to see, e.g. how many times a specific named entity is mentioned in my corpus). A problem that I am arriving at; however, is that the named entities often don't match up with each other even though they are the same entity. For example, one instance of the named entity might be "Dr. John Smith" while another instance is "John Smith" or one instance might be "Google" while another might be "Google Inc.". This makes aggregating quite hard to do. In order to deal with this issue and set "Dr. John Smith" to be the same entity as "John Smith", I was thinking of doing word matching between my named entities. I.e. I would check if named entity A has a word in common with named entity B and if they do set them to be the same entity. This approach is obviously seriously flawed. I will be equating "John Nguyen" and "John Smith" as the same entity even though they are obviously not. What's potentially even worse with this method though is I might run into similarity chains where I have "John Smith" linked with "Richard Smith" linked with "Richard Sporting Goods Inc." linked with "Google Inc." etc etc... While I may be willing to allow issues arising from former problem through, the latter problem appears to be catastrophic. Are there any accepted techniques in the NLP community for dealing with this issue? 

Contrary to what others are suggesting, trying to extract data on square footage and number of rooms from apartment ads is not a problem for machine learning - especially not when you don't have any training data. You will spend 10 times as long trying to build a machine learning system for this task than you would manually extracting the data yourself. If you really want an automated system, consider building some simple regex rules based on ads you have seen. There are only so many ways people describe these features, and you should be able to get most of them when they are present in the ad. You won't get 100% accuracy, but you're not going to do any better another way, in my opinion. You can start with some simple rules, and improve them as you look at more and more ads. In Python, for example: 

Unfortunately none of these three variables can go directly into linear regression. looks like a numerical variable, but it is actually categorical. For example is probably orthogonal to , and should not be interpreted as twice as significant. The correct way to handle this is to create a boolean dummy indicator variable for each possible site code. You can also use this method for some of your other variables which also appear to be categorical. and will not work well in a linear regression because their relationship is highly non-linear. For example two points can have the same latitude/longitude but be very far apart. One typical method is to convert pairs into predefined zones, and treat the zone as a categorical variable. The final type of variable you have is a . You could convert this directly to a categorical variable, but it might be better to take only the month to reduce the number of categories and generalize seasonal effects better. 

Once the matrices are build using one of the a) types, to perform manipulations such as multiplication or inversion, we should convert the matrix to either CSC or CSR format. 

At first sight, the total acumulated energy consumption seems to have a linear relation with time, so I suggest to try a linear regression at first. There are several libraries you can use to code it. I recommend you do it with pandas and sklearn, here is an answer related to this: answer. If the relation is not linear, so I could try with a more complex model (but I suggest to keep simplicity at first). Since you are trying to predict a temporal serie, I would try with an LSTM model. Here is a tutorial to implement an LSTM neural network with keras. 

I am trying to select the best scipy sparse matrix type to use in my algorithm. In that, I should initialize data in a vij way, then I should use it to perform matrix vector multiplication. Eventually I have to add rows and cols. Trying to select the best for my problem, I want to understand which are the best cases to use each of this types: lil_matrix, coo_matrix, csr_matrix, csc_matrix, dok_matrix. Can someone explain me? Its not necessary to show examples of all the types in the same answer. 

Encode each as the number of times it appears in the training set Encode each as the mean of the target value in the training set. See "Target-based encoding" here. If the target is categorical you can use a vector of frequencies for each target category, eg `[0.4, 0.1, 0.3, 0.2]. 

Convolutional layers are different in that they have a fixed number of weights governed by the choice of filter size and number of filters, but independent of the input size. Each filter has a separate weight in each position of its shape. So if you use two 3x3x3 filters then you will have 54 weights, again not counting bias. This is illustrated in a second diagram from CS231n: 

As mentioned, these and other methods will only be useful if each user tends to appear multiple times in the training set. Also see the categorical-encoding library for more ideas. 

You can replace with and use . This will work if you don't already have rows with entries that you want to keep. 

I am ploting a figure using matplotlib. In this figure I have a simple interactive feature: when you hover the mouse over some data, some annotation appears. How can I plot this and continue with my script? I am recomputing the data in a for loop (so I need to update the plot once per 5 minutes, for example). This is a minimum temple of my code: 

If you are trying to add weights to rare or infrequent terms, which appear only in few texts, definetly you should use the tf-idf technique, which computes the frequency of each word on all the data set and after that computes a weight of each word in each text. Another case, if you want to add weights to specific words, you just can modify the tf-idf technique. 

Using text documents in different languages you are going to have different vector representations, unless you translate the documents previously. For example, house and maison are going to be related to different features. So a cluster algorithm is not going to recognize them as synonymous. You should try a previous translation of your reviews. The quality of that translation is going to affect the clustering algorithm depending on the algorithms you are using. If you tell me the steps you are performing in your cluster I could help you better. 

Say I'm building a neural network to fit some classifier of some sort. To make things concrete, let's take the example of predicting housing prices using features of houses. What should I do if one or two of my features consist of many more numbers than the other features, or even all other features combined? For example, say I have a few housing features: size in sqft, age, median income of location. These are 3 numbers. And then I have another feature, height of the roof for each square foot of the house (it's a bit contrived for this example of course) for which I would have actually "size in sqft"-numbers for this feature. So now my feature vector looks like this: X = [1500sqft, 34 years, $54,000, 10ft, 10.1ft, 10.3ft...1497 more numbers here...] It seems that if I just naively put this into a neural net that the first 3 features would essentially be ignored since they only account for 3/1503 features. But they might actually be important. One try might be to simply average the "height of roof" feature over all of its elements to get an "average height of the roof" feature. That makes sense for this example, but what if sometimes I don't want to take this average? Are there any industry practices on what I might try if I ran into a problem like this? 

This is exactly why. If you use your test dataset to select the best architecture that can predict, then you are not using it as a test data, you are using your test data as a validation data. And, in the end, you run out of data to test. To make it clear: 

b) Sparse types that support efficient access, arithmetic operations, column or row slicing, and matrix-vector products: 

Have you found a good approach? I am envolved in the same work right now. My approach is the following: 1) Make a vector respresentation of all texts in the data set, for example with tfidf technique. 2) Take the first vector and put it in a pile. 3) Enter in the following loop: 3a) take the next vector and compute the cosine similarity between this vector and the centroid of each built pile. 3b) if one of this cosine similarity falls below a predefined threshold, stack this document representation in the corresponding pile. Another case, build a new pile with this vector. 3c) recompute the centroid of each modified pile. This algorithm is going to find similar tweets, which we suppose that are related with same topic. 

Many neural network examples you see in the literature are doing classification problems, E.g. LeNet, AlexNet, Inception, etc are all image classification problems. In this domain, it's useful for the neural network to give outputs between 0 and 1 because an output between 0 and 1 can be interpreted, in some sense, as a probability. The reason these networks output numbers between 0 and 1 is in the layer activations of the network. The last layer in these networks is usually a softmax layer (or, if you're doing just binary classification, a sigmoid layer). Softmax and sigmoid functions have the nice property that they give outputs between 0 and 1 (softmax has the added nice property that it gives outputs which sum to 1). If you want your neural net to be able to output numbers that aren't between 0 and 1, simply change your activation function. Instead of a softmax last layer, you could use a linear one. In this case, it also makes sense to change the loss function you are using to perhaps something like Mean Squared Error (binary cross entropy, for example, won't work too well on negative numbers). There's nothing stopping you from using a deep neural network to perform regression rather than classification. 

The class probabilities are the normalized weighted average of indicators for the k-nearest classes, weighted by the inverse distance. For example: Say we have 6 classes, and the 5 nearest examples to our test input have class labels 'F', 'B', 'D', 'A', and 'B', with distances 2, 3, 4, 5, and 6, respectively. Then the unnormalized class probabilities can by computed by: 

This is not correct. Every connection between neurons has its own weight. In a fully connected network each neuron will be associated with many different weights. If there are inputs (i.e. neurons in the previous layer) to a layer with neurons in a fully connected network, that layer will have weights, not counting any bias term. You should be able to see this clearly in this diagram of a fully connected network from CS231n. Every edge you see represents a different trainable weight: 

Seperate 12-convolutional-block policy and value networks (as in AlphaGo) A single 12-convolutional-block network with dual policy and value outputs Separate 20-residual-block networks A single 20-residual-block network with dual policy and value outputs A single 40-residual-block network with dual policy and value outputs