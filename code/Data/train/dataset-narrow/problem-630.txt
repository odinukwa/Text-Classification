Your code for the query is using two servers, identified by IP address as shown below. Where is the server [123.456.7.890]? And where is the server [098.765.4.321]? My guess is that [098.765.4.321] is the [RemoteServer] and [123.456.7.890] is the [LocalServer]. First of all, the slowness of the code is not SQL Agent's fault. Code run from SQL Agent is pretty much as efficient or inefficient as code run directly from SSMS. The problem is that you are moving much more data across the linked server than you might think. Here is the outline of your code: 

No, Logs cannot be disabled. Even if you set the database option for SIMPLE recovery model, the log will still exist since it is necessary for transactionally consistent activities, which are a mainstay of SQL Server. Nonetheless, running in the SIMPLE recovery model should reduce the conflicts that you are facing. Since this is SQL Server 2008, why not try getting rid of the database prior to rebuilding with something like the following: EDIT: Minor update to script to show how to keep control of the database. 

There is a TechNet blog that discusses some merge replication issues with blobs on a SQL Server 2008 or higher server. $URL$ Note that the author cautions about which settings to use when there are SQL Server 2005 clients, such as you have. 

AD Group = a set of logins that share the same database access needs. Role = a set of permissions that define a set of rights to be shared by logins. 

This script does include the job schedules that were set up on the origin server. From the scripted output you can pick out the SQL Agent jobs that you want to duplicate and create them on the other server. 

The reference in the SQL Server 2012 documentation is at: $URL$ Therefore you can control where the space is used, in-row or stored out of row. 

Any serious development process should include some Source Control software and a plan for using the tool. Since Git, Mercurial, SVN, etc are freely available choose one. Git is commonly used by some of the largest organizations so you could choose to use that. There are, of course, many other source control tools. If you use this approach, regularly checking in the new versions of objects (schemas and code), you will have a trail of previous versions to compare against. Then you can look for changes that affect your code. Of course, Microsoft's SQL Server Data Tools (SSDT) includes Schema Compare, as do many tools such as Red Gate, Idera, and so forth. This should give you the tools you need to track the changes in your schemas and changes in the code as well. 

Consider dropping the unused indexes. If you have a yearly job that would really use the index, consider creating it just for that job. Feature-Phobic Indexes is suggesting that a filtered index may be more valuable to you than a standard index. A filtered index would only cover the rows that meet your search criteria. This is a little more speculative, but the advice is to try some filtered indexes. If they do not work for you, you can drop them again. 

You should not need to DENY CONTROL to a login. Simply do not GRANT the right to that user. Regarding the side effects of DENY CONTROL see the post here: $URL$ Comment from that link: "The only option you have is to pay careful attention to not grant any permission to the principal that you did not indeed intend to include. You can consider using techniques like auditing to help you with this, but you cannot enforce it." So REVOKE your DENY and just make sure that only the needed rights are granted. 

No log files to backup during the large FULL backup. Switching to FULL before beginning the DIFF backup will give you the needed log to start with and its longest growth is probably during the DIFF backup. 

The Momentus XT is an "Advanced Format Drive" and I see that you are running Windows 7 SP1. If the drive runs 512e mode this can cause you a problem with SQL Server. This is because it can create sectors in some multiple of 512 byte boundaries, instead of always at 4096 bytes. (Your error was caused by a 6 * 512 byte sector.) If that is the case, then you will need to take some corrective action. Microsoft has a blog post and a KB article for Windows 7 and Windows 2009 R2 on this issue which points on to other links. $URL$ $URL$ If this is your problem you may want to download the latest FSUTIL to investigate. $URL$ This has not been the fix for everybody. In the MSDN forums I found a lengthy set of responses with various fixes. $URL$ From this discussion you will see that some finally resolved their problems by updating drivers. 

So, it seems that the actual contents of the NVARCHAR(MAX) is what matters to the CREATE INDEX. EDIT: Jon Seigel identified that the TRY_CAST triggers the failure on create index when the string is longer than nvarchar(4000). 

No, an Active Directory group cannot be the schema, though I suppose you could name a schema with the same text as your AD group, but that does not get you more auditability. If individual logins are all members of the AD group, they still appear in the database by default as their login name, not the AD group name. (You can, of course, rename the database users to different names, but I do not see how that would really help you.) However, using 'multiple users' (i.e one login and user per person) does not mean that they will all create their own schema. You have considerable control over this behavior. When you create the user's you can assign them to a default schema, such as . Likewise, if you need to set that up after the fact you can: 

As you can see, you are being given 3 possible fixes, the best if which is: Restore master from a good Full backup. You can, of course, try to repair or rebuild the master database, but these can cause you more secondary work to get everything back in order. UPDATE WITH EXAMPLE: Looking for concrete examples, I did find this post by Glenn Berry. $URL$ He outlined that his problem was caused by "cleaning out the SQL Server Install directory, specifically "C:\Program Files\Microsoft SQL Server\MSSQL10.MSSQLSERVER\MSSQL\Install‚Äù. " This is what lead to his problem with the new install. What he did to get back on track was: 

Recommendation: Drop and Recreate the Linked Server 'SRAPP'. You can help yourself by scripting out the existing definition. Drop the existing linked server and recreate it using the new server name. As of SQL Server 2016 can only be defined for: 

In the dbo.sysschedules table you will see all of the data needed to calculate the schedules. The columns named delimit the periods in which the jobs can run. The columns named define the frequency of the job runs. You will notice a , which defines a type of frequency, which drives how the and other columns are interpreted. Please read the definitions at: $URL$ EDIT: You can calculate the next_run_datetime by using one of the following: 

Since replication can be going out to many subscribing servers, the mirror in this case may keep the data flowing to many other servers. In a replication environment that can save a lot of extra work and downtime in case of a failure. 

But please note that this only answers formatting a phone number for the 10-digit answers. If you are using internationally dialed numbers, you need to investigate the various forms of phone numbers you might need to handle. See: $URL$ 

If you want to (1) maintain your log shipping environment and (2) want to have a read/write copy occassionally you will need to maintain 2 databases, TargetDB and Target2DB. Question, could you just restore the backup from the Server A SourceDB to Server B Target2DB? That might take a little longer, but there are fewer moving parts running all the time. If read/only access would be sufficient, you can create a database snapshot for reading purposes. This creates a point in time snapshot of the database, supported by the .SNP file that maintains the needed data to support the snapshot, despite other changes coming in. 

Yes, it is possible, but it requires some agreement between both parties. If the "sysadmin" (it does not need to be a by the way) wants his own backups, does he want all the transaction logs and differentials in addition to a full backup? If the "sysadmin" only needs full backups, he can do his own backups and use them without disrupting the DBA's scheduled coverage of backups. This could be used for populating a test server or creating a test database. However, the DBA team is normally guarding the recoverability of all the data on the server, which means that the DBA backups () could provide any point-in-time restore that the backups cover. That is why you do not want the other backups to interfere. 

Neal Hambly in his post at SQL Server SOS_SCHEDULER_YIELD Wait Type includes this comment (highlights are mine): 

What fits well with your current skills? (That might make it 'easier' to use.) Is your focus on getting something working soon, or on developing new skills? What is the cost of your choices (money, time, effort, unfamiliarity, etc)? 

Schedule a delay on executing the query on the second server of n seconds to give the data time to usually make it across. (Call it computational overhead, if you wish.) If you keep a transactional time and date (as a column) on every row, then you can calculate which rows you will return by using date math to exclude the final n seconds of data. If use the point 2 philosophy on both the primary and secondary server, then the results on both servers will look synchronous most of the time. 

The following link provides you with the details: $URL$ Once you have everything set up on the restored database, you can (of course) turn off TDE and let the server eventually de-encrypt the database, if that is your intention. There is a Very Important Note on removing encryption from a database that is to be restored from SQL Server 2008 or 2008 R2 to another server at: $URL$ (This may not matter to you, depending on the source of the restored database.) 

SQL Server can support a single Distributor on the local server. If you are trying to reduce the load on the main server you can instead create the Distributor on a remote server. If the load is heavy, an additional step that would help manage the distribution is to create a distribution database for each database that is being published. This could reduce the contention within each database that might be present in a single distribution database. Both a local server and a remote server can support multiple distribution databases. David Poole's article from 2010, Scaling Out the Distribution Database. may provide you more insight. It can be found here: $URL$ Both articles are helpful and I have not detailed every step in the setup. 

I use Ola Hallengren's scripts these days to support some 50 SQL Servers. See: $URL$ and download his MaintenanceSolution.sql. These scripts have won several awards and are quite widely used. The documentation is quite good, for guidance on backups and other database maintenance feature. You can choose frequency of backups, types of backups, retention of backups, and on and on. (In the past we used SQL Server's Maintenance Plans, then some of our own code, and now Ola's scripts.) I have a one month retention of backups, some have more and some less. The backup retention period is specified in hours, not in days, so you do have a lot of flexibility. One comment on deleting files is that you ideally should do each backup (or perhaps each day's backups) to an individual file. That way when you delete a backup, you only delete one backup (or one day's worth of backups.) 

This preserves your security settings at the role level, so you should not need to repeat those grants. Just add or remove a user from a role, when needed. 

Regarding Asynchronous and Synchronous connections, this is not about speed of the connection, but about the results of the synchronization. 

Future changes to the OS or the format of the command could easily disrupt the script. Still, hope it helps you for the time being. This is more like something that you might want to find or develop a bit of CLR code to get the directory information without so much maneuvering. 

If I understand you correctly you are looking to see who is blocking and why, not just gathering overall statistics. (Both approaches have real value, of course.) Since every event is transient, so it is not surprising that some values would not return what you expect. We have been using the approach outlined by Tony Rogerson quite a few years ago that uses Event Notifications. $URL$ This approach will capture the state of blocking based on a timing trigger. In the sample code Tony has configured to receive a notification when a block has lasted 10 seconds. (And every 10 seconds after until the block is removed.) This approach will provide a capture of the XML describing the blocking state at the moment of the event. This includes the blocked and the blocking process. And if none of the blocks are long enough to capture, then your event table will be very quiet. You can monitor for shorter or longer blocks, depending on your need. I generally run at 25 seconds, to cut out a lot of chatter from shorter blocking periods. New software releases can be a good reason to monitor blocking more closely. 

In this case the OR of candidate tokens provides the full list of non-wildcarded strings to Full Text Search. The performance is based on the ability of the function to return those values. Note: This is stretching the purpose of Full Text Search, but within its limitations it may at times be useful. Also, you should review the behaviors of numeric values in SQL Server Full Text Indexing.