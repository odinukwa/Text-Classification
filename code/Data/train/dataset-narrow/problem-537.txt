You can use Azure SSIS integration with Azure Data Factory as explained here. On this resource you will find how to run packages that use Windows Authentication to connect to on-premises resources like a folder. Hope this helps. 

My suggestion is to use Microsoft Data Migration Assistant. It will perform an assessment of your on-premises databases first, then it allows you to migrate the database to Azure SQL Database. This tool is very user friendly. It's very easy to migrate a database to Azure with this tool. 

You can use CREATE DATABASE AS COPY to create a snapshot of production database, and then use the same CREATE DATABASE AS COPY option to create databases for development environments using the snapshot database. All this can be automated using an Azure Runbook. The runbook can run with a SQL login with elevated privileges to accomplish all copy activities. 

I have a Sharded Cluster and I am trying to access a specific member of a replica set within a shard. When connecting to the mongos it is fine, but when using the same credentials on a member of the replica set I am unable to gain access even though my user has the role of root. The error is: 

MySQL dump the databases, then MySQL import Convert them to MyISAM across the board, make a copy of the file system, then move them into the new database directory, then change back to InnoDB if needed. 

I feel like there is something obvious that I am missing but I can not figure out what, any help would be greatly appreciated. 

** Please note, do not copy the mysql db if you have it within your backup into the new MySQL directory because this will cause all sorts of issues. 

If Telnet is successful, the window will change to a completely blank screen. Try connecting with that IP using SSMS again. If telenet is not successful, try tracert to identify at what step it fails to reach the Azure SQL Database server. 

It seems like the workstation that DMA is running on doesnâ€™t have enough disk space to do schema extract. The following is the explanation the Microsoft engineer shared with us about why DMA requires disk space: 

Those BKRK are related system processes and back end connections and do not count upon the limit of connections associated with every service tier. They can safely be ignored. BKRK processes are related to service broker on on-premises SQL Server instances and Azure SQL Database may be using it as part of the service it provides. With Azure SQL Database you will always see many connections and processes related to the automated features the PaaS provides to the customer. Learn more about back end connections here. Instead of focus on system processes, please try to know if blocking is the culprit. 

I recommend looking at the documentation on how to do a rolling upgrade across the board, but one step which caught me out when doing the rolling upgrade was the addition of the parameter which I used, or you can use parameter that is in the upgrade documentation. You need to use one of these parameters for MongoDB 3.6, this is required for configs, mongos and mongods. Would be good to see and example of the errors you are getting. 

Then you will want to create a statefulset binding to the persistent volumes which means that when the pods go down and come back up they attach to the correct persistent volume, the yaml will look something like this: 

Your service can remain the same apart from changing it to look at . Once they are all up you will need to issue the command in order to tell the pods they are now working as one replica set, details of this can be found in the mongodb documentation: $URL$ ** Note, I have copied the example yaml's from my sharded cluster that has been deployed through kubernetes which means I have removed some parameters so there could potentially be a typo, also you might want to scale down the persistent volumes and memory limits before proceeding. 

For more ways how to capture specific query waits, please read here. Look at the query plan, you may also find no indexes are participating on the join. Can you use SQL Data Sync to keep a copy of the table on the on-premise SQL Server and then create the join with the local copy of the table? 

You can use Get-AzureRmSqlDatabaseGeoBackup Powershell cmdlet to get all available backups on a specified server. 

I shared the case scenario with the Azure SQL Database team at Microsoft via an Exchange Mailing list for Azure SQL Databases contributors/MVPs. The answer provided by one of the managers of the team is the following: "Yes, transactional replication will be the best way at this point if they want to keep the replication running after the migration to Managed Instances." To validate I have a communication link with Azure SQL Database team to provide you the answer, please visit these two links. Link1 and Link2. Hope this helps. 

I believe this is because you have more than one pod trying to use the same using the same data directory which you can not do if the pod is running mongo, one data directory has to be exclusive to one mongo instance. If the goal is to create a group of pods with the same data then you are looking at creating a replica set. In order to do this first you will need to create multiple persistent volumes as so: 

Before I start, I will say that I am not familiar with wampserver but I am familiar with MySQL and LAMP. First things first, are your tables within your databases MyISAM, InnoDB or a mixture of both? - You can tell on the file level by looking at the suffixes. MyISAM tables have the table name with the following suffixes: 

Instead of creating a maintenance plan on a local SQL Server instance to perform maintenance tasks on an Azure SQL database, consider creating a runbook using Azure Automation. The runbook should run a PowerShell script like the following: 

Let start by making sure you are using the current IP of your Azure SQL Database server. Let's ping the name of the Azure SQL server. The ping will fail but it should also return the current IP of the SQL Azure Server if DNS resolution is working. 

Index maintenance is an I/O intensive workload. Under these circumstances please consider to scale to premium tiers prior to run index maintenance tasks, and when the workload finishes scale down to previous tier. You can automate that using T-SQL. That should reduce maintenance time. Additionally, compacting large objects data (image, text, ntext, varchar(max), nvarchar(max), varbinary(max), and xml) can save disk space but it can increase the time it takes to maintain indexes. If you have a restricted time window for reindexing you can set LOB_COMPACTION to OFF. Hope this helps. 

If they are MyISAM, copy the tables straight into the new MySQL database directory and this will work as long as the files have the correct read write permissions and ownership (mysql). If you have InnoDB tables then this will be an issue because you can't simply copy them into a location, in this case what you would have wanted to do is: 

Now try pinging and seeing if you have connection, if so then MySQL should work, although depending on setup you may have to comment out the from the config. 

First of all are the containers visible to one another i.e. can you ping the mysql box from the webserver? If not it is a visibility issue, try creating a docker network to put the two containers within. First create a docker network: