You can also run this adhoc this way without SQL Agent scheduling specifying the correct DateTime variable when you want it to stop. Once the process has run, you just go to the file and open it as usual. See script logic at bottom with comments on how to confirm this isn't running any longer, etc. -- any traces for that matter and how to stop them from running if they are. NOTE: In my case we scheduled this job start with SQL Agent job at 7 AM and then put the DateTime variable value for the @stoptime argument passed to the sp_trace_create object so it quit running at 8 AM on this day. Once in the office and reviewing this, we were able to sift through and determine the issue and correct. We filtered our trace criteria down as much as we could though beforehand when we built the script file that saves to disk to give the TSQL for scheduling. 

Members of this role in the database you create it on should only have access to the applicable metadata as long as they don't have other permissions to database objects such as explicit SELECT access to the a table or a member of the db_datareader fixed database role. 

Small starting clarification: the article you linked to on InnoDB text/blob storage is a little out of date with MySQL 5.5, and the barracuda row format. This article is more up to date. On to your question: In the schema you have, each row will be less than ~8K, so you can guarantee across both antelope and barracuda row formats that all data will be stored in-line - not requiring external/overflow pages. If you were to require overflow pages, they are never de-duplicated (which is what I would probably describe your 'pooling' mechanism as). Even worse than they are never de-duplicated, they are never shared... If you could have a record too big to fit inline (~8K limit), each text/blob that needs to be moved out will take a minimum of a 16K page to itself. 

In newer MySQL versions it gets a bit more complicated to explain because there is also an in-place fast ALTER TABLE, but for 5.1 the answer to your question is simply "in the table's directory". 

It would seem the explanation would be that your your committed transaction sizes with the operations are HUGE. Simply make the commit\batch size parameters of your logic in SSIS or your execute TSQL in the package of a smaller size. Try testing with 100, 1000, 10000, 100000, and so on to see what gives you the best result and prevents the issue from occurring. If the transactions are smaller, once the committed transactions are committed in recovery mode, then the log space of the committed transactions can be reused by subsequent (or other) transactions. 

Give this a shot since you state brings up a NULL value, despite restarting the server. 1. Run the below with the local as the second argument. . . TSQL 

The people or security contexts running the expensive queries on the wrong instance should be identified and then notified to change their processes on the instance and DB to start running their stuff on the instance you want those run on. Since the secondary is read-only/standby, we would assume these expensive queries are SELECT statements only where the primary DB that's not in standby would have to run queries that update data or modify objects regardless of the expense. You could also look into why some of these queries are so expensive in the first place, see if adding indexes would help, rewriting queries for performance tuning, etc. I think that'd be a good root cause type solution. It seems like it'd be tricky to do this with one of more AD groups though as the changes are replicated from primary to secondary and secondary would be in standby to accept transaction log changes from primary when those are applied so I'm not sure how you would accomplish this in that sort of configuration with log shipping. I'm also not certain how often your tran logs are being restored to secondary as I thought when LSRestore jobs run, it disconnects all session on secondary until those transactions are committed, so I assume it's once a day or not too often or people would be screaming about their queries, etc. getting disconnected during normal hours. If once-a-day restores are occurring for LSRestore jobs on secondary, then this would mean the data gotten from that server is 24 hours old or however long between your LSBackup, LSCopy, and LSRestore jobs. So who can use which DB with this regard may depend on how fresh their query results need to be from the business side, etc. Lots of factors to consider here but getting to the root cause and having bad performing queries tuned, adding indexes, etc. may be the best solution as well as having the people with access take responsibility with their processes to not hose up the performance for others when they run their stuff. 

I find talking about storage engines using cores can be misleading for beginners. Provided that a program is sufficiently multi-threaded, the operating system will schedule it across as many cores as possible. The specific problem that limits cpu-scaling is when internal locking code (mutexes) have contention and block threads from running concurrently. All storage engines will require mutexes, but certainly there are some hot ones in MyISAM. If we ignore mutex contention for a second and get back to your main question: how important is it to have many cores? - I like having lots of cores for workloads that serve user facing requests. Having many can reduce variance between query times. Think of this as like lining up at the super market with 12 aisles open versus just 2. Update: I wrote a blog post on why vertical scalability (multi-cores) is important. 

The first thing that I would point out is that MySQL data files are quite portable. You don't have to worry about endianess/bitness/operating system etc. The next thing to mention, is that in MySQL versions do not always paint a picture of what has changed: 

Consider granting VIEW DEFINITION permission as defined in the below quoted reference; this still applies to SQL Server 2012 as well. 

From the MySQL Connections screen click on the little wrench icon to the right to bring up Manage Server Connections window. 

According to this StackOverflow post, if you have the server-level settings (referenced below) configured with ForceEncryption set to Yes, then that will enforce an encrypted connection regardless of the SSMS GUI option being checked or not prior to making connections to that server. This may be a sufficient workaround for people where the "encrypted connection" is of more importance than the actual option being checked within the SSMS GUI. 

Can you point the 'full path' to the UNC path i.e. instead and see if that works? Just like when you map the "X" drive to just use that in the full path of one of your packages and run to see if it'll work. If one of your jobs work like that (assuming most all are setup the same and this way, etc), you can probably script out the SQL Agent jobs through SSMS by pressing F7 (once SQL Agent jobs is highlighted), selecting them all from the right pane window, right click, then create to new query window, then do a mass CTRL+H and do a find and replace to replace with , and then run that. Just be sure the SSIS proxy account or the SQL Server Agent account has appropriate NTFS and SHARE permissions where the SSIS packages reside to read them. 

Yes, your kill statement is the likely cause. The command will only show you what is happening in foreground threads. InnoDB is likely rolling back the transaction in the background. Some diagnostics you might find useful: 

Shameless plug: I wrote about this in a blog post. A couple of jobs ago I was using this to log/serve ~1 billion ads/month. 

I can't vouch for their credibility, but I did manage to find 4.0 RPMs with a Google search for "mysql 4.0 rpm". The obligatory disclaimer of course is that MySQL 5.5 is now the minimum supported version of MySQL. But that will be the same case with using Red Hat Linux 9 (Shrike). 

I believe the background threads are fixed in number (the actual count will depend on some configuration settings such as and the number of etc.) The FOREGROUND threads are one per connection + a potential of on top of that. 

Since the granularity of caching is done at a page level (both in InnoDB and de-facto in MyISAM due to filesystem block), having large 42-column rows means that you will fit fewer rows per page on average and there is no split where the hot sub-set of columns are kept in memory while the inactive ones can not. This results in a sort-of cache dilution, where you may require more memory than if you were to normalize the schema and split into a few different tables. (Note: InnoDB does overflow large text/varchar/blob columns to separate pages. I agree with Rick's comment that InnoDB is the way to go.) 

Due to the fact that you have a configuration of in an of , I can only assume this is the ROOT cause of the issue (see notes and resource link below). There could be latency issues with committed transactions in this configuration due to the primary replica waiting for acknowledgement from the secondary replica that it's hardened its transaction logs before it commits its transaction on the primary. So, not knowing all the business and infrastructure detail on your side, you may want to consider or perhaps test changing the to since it commits its transactions without waiting for acknowledgment from the secondary replica that it has hardened its transaction logs. You may want to check and confirm if there are issues with transaction log hardening on the secondary replica server when this occurs. If you're going over a slower WAN or MAN link perhaps, confirm no issues at network level hops, or any general server issues, etc. If there is an issue found at one of these other levels, then fixing that should fix your original issue I would think since the primary could acknowledge quicker that the secondary replica hardened its logs and then it'd commit its transaction. 

I have a blog post explaining why this is here. The short version: The query cache causes scalability issues on multi-core machines. So it is now disabled by default. 

I agree with @Remus' last point - most people use Hadoop for crunching and store the result in MySQL. You can also have apache write a custom log for you with only the fields you require, and export environment variables from the application for Apache to save (if required). For prior art, I would recommend taking a look at how OpenX (advertising server) has solved this problem through various versions: 

You most likely have a file in your home directory, which is specifying a password. The clue here is that when you did not specify a password (), the error message is still saying that you did. 

Fan in (multi-source replication) will be supported from MySQL 5.7. A labs release is available here: $URL$ 

You've got to put it in context - InnoDB only verifies the checksums when it reads a page from the block storage device, and updates it before flushing it back to the storage device. While in memory, the checksum is not maintained. For many years, an IO to a disk has taken something on the order of 5-10ms (1ms = 1/1000th of a second). Computing a checksum probably takes somewhere around 50us (1us = 1/1000000th of a second). I don't have the actual data what it is in InnoDB's case, but if you Google "Numbers everyone should know", you'll hopefully agree I'm correct within an order of magnitude. Enter an era now where we have things like Fusion-io flash devices, which on paper have a ~20-30us access time, and you can see that reevaluating the checksum makes sense. My general advice: Don't ruin your backwards compatibility with MySQL releases unless you really need it. Most people do not need it yet. 

Give something like the below a try... You'll obviously need to plug in your variables for your environment, check the data types (may need to add logic to keep leading zeros?), change from the final temp tables to your regular table(s), etc. Works fine for me for import from XML files to temp tables without deleting the files afterwards but adding logic to delete files from the UNC path shouldn't be too difficult with another xp_cmdshell command. 

Continued Trouble If you continue to have trouble you might set the Use SSL field to a value of No and also ensure all the SSL fields below that are blank and then test the connection again. 

Below is a script of the T-SQL part of this process which I run after doing the above 4 steps, and it always works in my environment to accomplish what seems to be similar to what you've explained that you're trying to accomplish. I also have to ensure the AD account has a strong/complex password, and set it to never expire. 

The minimal package was designed for use by the official docker images for MySQL. It cuts out some of the non-essential pieces of MySQL, but is otherwise the same product. 

So it looks like the time increased because there has to be a comparison to confirm that the value itself has not been modified, which in the case of a 1G longtext takes time (because it is split across many pages). But the modification itself does not seem to churn through the redo log. I suspect that if values are regular columns that are in-page the comparison adds only a little overhead. And assuming the same optimization applies, these are no-ops when it comes to the update. Longer Answer I actually think the ORM should not eliminate columns which have been modified (but not changed), as this optimization has strange side-effects. Consider the following in pseudo code: 

Short answer: MySQL does not support this feature. Longer version: The minimum granularity for replication filters is at the table level. If using only MySQL (and no script in between) you can achieve similar by having a shadow table on the master which you apply triggers to insert into. The resulting trigger changes can then be replicated to the slave in the desired format. Noting that this would require row-based-replication enabled to work, otherwise triggers fire on slaves. If using a script in between, you can quite easily watch for changes in the replication stream and apply any transformations. I have an example of how to do this here. 

Just a quick thought on something to look into, in a domain type environment, some operating systems allow you to logon to the server before full network connectivity is established. You may want to check for either local or domain level group policy settings to not allow logon or OS startup until full network connectivity is established. Just in case you notice this when you log onto the server after reboots, it actually logs onto the OS with the cached credential before it can reach the domain controllers to authenticate (network connectivity not fully established) with the login credential if it's a domain credential the SQLExpress service account is running as. Not sure if that's exactly applicable in your case but this is something to at least simply investigate and try to test at least just in case. I found this in some article I saved long ago when I had a similar issue with an AD home directory (not via login script) to map home directory for a workstation PC: The policy value for Computer Configuration -> Administrative Templates -> System -> Logon “Always Wait for the Network at Computer Startup and Logon” will be set to “Enabled”. If the following registry value doesn’t exist or its value is not set to 1, then this is a finding: Registry Hive: HKEY_LOCAL_MACHINE Subkey: \Software\Policies\Microsoft\Windows NT\CurrentVersion\Winlogon\ Value Name: SyncForegroundPolicy Type: REG_DWORD Value: 1