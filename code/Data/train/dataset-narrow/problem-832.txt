I've written some code to parse the names and phone numbers from craigslist. It starts from the link in "m_url" then goes one layer deep to parse the name and then again another layer deep to parse the phone number. Note that it goes 2 layer deep only when it sees "show contact button" on that page so that it can unveil the phone number from that link to scrape. It only prints the result when it sees the button on that page. That's because there are around 120 names on that page but it prints only those containing that specific button. Sometimes when I come across such "show contact button" link within a page from where I am supposed to harvest data, I get frightened. That's why I tried to work on it. It works smoothly now. Any improvement on this script will be very helpful. 

You don't have to use x as the parameter name for the lambda. You can call it model with a small m for instance. 

Notice that I replaced your setter with an method. It's ususally good practice not to replace an entire encapsulated collection using a setter. Although you can validate the entries in the setter, a property should not do much logic other than returning or setting privates. If you have a look at the collections in the BCL, they all expose an method and a method to empty the collection. The method also gives you the option of adding multiple sets in turn. If you need different logic when stuff is added etc, have a look at the template method pattern: $URL$ If you need multiple base classes with differing logic, you could combine the inheritance with the extension methods, or another base class. :) 

I've written a script in VBA which is able to scrape images from a webpage and save it to a customized folder successfully. Firstly, it scrapes the image link then downloads the image and rename it according to it's identity. It takes 2/3 seconds to accomplish the task. I tried to do the whole thing specklessly. Here is the script I tried with: 

After a long try I've been able to create a script in vba which can successfully handle webpages with lazy-load. It can reach the bottom of a slow loading webpage if the hardcoded number of the loop is set accurately. I tried with few such pages and found it working flawlessly. The one I'm pasting below is created using site. It can parse the title of different news after going down to a certain level of that page according to the loop I've defined. Now, what I wanna expect to have is do the same thing without using hardcoded delay what I've already used in my script. Thanks in advance for any guidance to the improvement. Here is what I've written: 

AFAIK, you won't get client-side validation of VG2, but otherwise I can't see that it shouldn't work. (There is probably a client-side event and a method you could call to duplicate the behavior for the client) 

Makes sense, but you should DRY up your concrete factory at least. Forgive me my rusty Java, but something like this: 

Not entirely sure I got your schema right, but something like this should be possible. (omitted a bit of ordering, and I assume you only have one current year) 

I don't immediately see any efficiency issues with your code. It's simple and concise. But I do notice that you don't follow all the best practices for plugin authoring described here: $URL$ I guess you'll do something else than alerting ten times in production? ;) 

I had a desire to make a recursive web crawler in vba. As I don't have much knowledge on vba programming, so it took me a while to understand how the pattern might be. Finally, I've created one. The crawler I've created is doing just awesome. It starts from the first page of a torrent site then tracking the site's next page link it moves on while extracting names until all links are exhausted. Any input on this to make it more robust will be a great help. Thanks in advance. Here is what I've written: 

I have written some code in python in combination with selenium to parse all the names from facebook friend list. It was hard to manage the pop up notification and the process of scrolling to the end of that page. However, my scraper can do that successfully. I tried to do the whole thing very carefully. There are always rooms for improvement, though. here is the working code: 

Property will expose the current value on postback by it self. You could also expose an event on your control that you refire in the ValueChanged event of the hidden field. 

There's a couple of nice alternatives for you. Extensions If you don't need to encapsulate the instances, you could make your methods extensions for or whatever you've extended: 

As a sidenote, I'd look for another name than "BaseClass". It doesn't say anything about what it does. 

This might be off topic, but if you use switch statements for more complex functionality, you should read this before going further. $URL$ 

I just re-read some comments and realize you need paging too. Add a private int page, and a private int pagesize, then add the multiplication of those to foods[i] in CreateButtons, and swap foods.Length with pagesize. You can have as big an array as you want then. 

I've written some code in vba for the purpose of making twofold "POST" requests to get to the destination page and harvest name and address from there. There are two types of structures within which the desired results lie. One type of structure holds name and address in a single "th" storage and the other holds name in one "td" and address in another "td". So, to handle this I had to use error handler to get the most out of it. By using xmlhttp I could not get any result so I used WinHttpRequest in my script to get the result by enabling redirection. My script is running errorlessly at this moment. However, any suggestion to improve my code specially by handling error more efficiently will be highly appreciated. Here is the full working code: 

I would refactor it into a plugin that takes the content as an option. Then you will have two instances of "dialog controllers" each with its own events. Have a look at the "plugin with data" example in the jQuery docs. $URL$ You'd set it up like 

I wouldn't presume the metadata information gathered from reflection as mentioned by w0lf will be handled completely if you reference the Model property directly from the lambdas. 

Entity Framework implementation assembly References Model and the Entity Framework / System.Data.Entity assemblies 

Of course you can expand on that by employing the template pattern for the extra parameter settings. I would also look into extracting the parameters to a "parameter object" instead of several parameters. 

I've written a script in python using requests module in combination with selenium along with regex to parse email address (if any exists) from any website. I tried to create it in such a way so that it can traverse javascript enabled sites as well. My crawler is supposed to track any website link (given in it's list storage) then find or etc keywords from that page and parsing the matching link it will go to the target page and using regular expression it will finally parse the email address from that page. It scrapes the email address along with the link address where it parses the email from. I tried with several links and most of the cases it succeeds. I know it's very hard to create a full-fledged one but I tried and it is not despairing at all. Any suggestion to improve this crawler will be vastly appreciated. Here is what I have written: 

If you do this, you'll probably see some pattern emerge. I can already see two classes, or rather instances of a- instead of one. Maybe something like Strategy? 

I'd use enums. My suggestion presumes you are able to refactor your existing code a bit. If your control can be modified to keep the position value in one property of the following enum type, this will work. 

You aren't renaming it either, you're naming the parameter for the anonymous function the lambda expression represents. But if you don't like the lambda syntax, there's also overloads taking the property name as a string. 

Or better yet, build the initial list based upon content in a dictionary. That way you don't run the risk of having typos in the switch or the initial setup: 

I've written some code for the purpose of scraping names and urls from several links found in the left sided bar in a webpage and populate the data in several sheets [also giving each sheet a new name taking a customized portion from url] in a workbook so that things do not get messy and the data can be located separately. I tried to do the whole thing accurately. Here is what I did: 

I have written a crawler in python with the combination of class and function. Few days back I saw a scraper in a tutorial more or less similar to what I did here. I found it hard the necessity of using class here. However, I decided to create one. My scraper is able to traverse all the next pages and print the collected results errorlesly. If there is any suggestion or input to give this scraper a better look, I'm ready to comply with that. Thanks in advance. Here is what I've written: