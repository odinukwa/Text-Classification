The problem, I think, is that you are comparing two different kinds of depth values. For want of better terms, the rasterizer's depth buffer stores 'parallel' depth, whereas raymarching calculates 'true' depth - that is, the rasterizer stores the distance to the plane parallel to the view plane which contains the point, but the raymarcher calculates the actual 3D distance from camera to point. 

This seems to be caused by specular term returning >1 around the highlights, probably caused by Unity's hack of multiplying specular by pi (instead of dividing diffuse) as shown in the code snippet below. 

I've been writing some custom shaders based around Unity's Standard BRDF, which is uses GGX distribution term and Smith visibility term. It looks great at either end of the smoothness scale (1 and 2 in the pic below). However, with the smoothness set to about 50%, things start to get a little ugly (3). Increasing the metalness makes it worse (4), and strongly coloured specular gets worse still, as different colour channels blow out at different rates(5). 

A half-edge is an edge split along its length, and having a directional component, that is, a beginning vertex and an end vertex. Where two polygons share an edge, each polygon gets a single half-edge between the same two vertices, which will have opposite directions if the winding order is consistent. These half-edges will have references to one another as two halves of a pair. The full half edge data structure stores for each half-edge: 

The first 3 parameters to are image unit, texture, and mip level. The next two deal with binding arrays of textures, and should just be set to when binding a single texture. The final two are access and format (for stores). Note that although I've used the GL 4.5-specific functions for creating textures and binding the texture unit, the works with every version that supports load/store - 4.2 and above. 

At first i thought it was the dreaded SIMD alignment issue, but I'm compiling 64-bit so that should be taken care of. After a bit more tinkering i found out the ortho and world matrices were fine, it was just the projection matrix that was causing problems. Assigning to this matrix with the other XMMatrix... functions also works ok. So: 

Yes. If you're compounding operations to make a matrix, then the inverse matrix will be the compound of the inverse operations, in the reverse order. So if $C = AB$ then $C^{-1} = B^{-1}A^{-1}$ Think of it geometrically. Taking a 2D example, if you have an object at the origin, and you want to move it +2 units in X then rotate around the origin by +45 degrees. To undo this transformation, you need to first rotate by -45 degrees then translate by -2 units in X. 

While making a GPU Marching Cubes implementation a while back, I ran into the issue of OpenGL holding on to memory I had asked it to release. There are a lot of threads on this over at Stack Overflow, and the gist seems to be that you simply can't rely on the API to release VRAM in a timely manner. In my app, I was trying to be smart, releasing all temp buffers used in mesh creation once I was done with them, and allocating an exact-sized vertex buffer each time the mesh changed. This was counterproductive, and rapidly led to out-of-memory crashes. In my case, since it was just a toy app for a uni project, I solved it by allocating worst-case sized buffers and leaving them allocated, recycling buffer space wherever possible for different passes during the mesh extraction process. This was also a bit of an edge-case, since buffers were pretty huge (sometimes hundreds of MB) so out of memory errors appeared quickly. This is fine for a strictly limited use case like mine, but my question is, how is VRAM handled in environments like games, where large numbers of meshes and textures need to be loaded and unloaded dynamically? I'm thinking particularly of open-world games like GTA where the world is constantly being streamed-in as the player moves around, but even in more linear games, loading a new level would need a lot of unloading and reloading assets. Are assets generally loaded to sections of large fixed buffers and space usage tracked somehow? Or if using a large number of smallish buffers, can the APIs keep up with a few of them being created and destroyed regularly? Can anyone with experience in engine design shed some light on how this is done in real world usage? 

I wrote a simple Phong shader with two directional lights for a project, and noticed an unpleasant artifact in the lighting. Where both lights are illuminating the same region, dark bands appear at the light terminators where N.L reaches zero. At first I thought it must be a bug in my code, but a quick test revealed the Unity standard shader has the exact same problem. 

I've been getting interested in SIMD programming on CPU using SSE recently, but I'm a complete noob on the subject. I found this article describing how to make an efficient float3 type using the recent __vectorcall convention, and it made me wonder how best to deal with the wasted space inherent in using a 128-bit type to represent vectors smaller than 4. For a typical use case, you might want to transform a bunch of positions and normals on the CPU in order to do a batched draw call. When transmitting the data over to the GPU, though, ideally you'd want this as tightly packed arrays of float3, either interleaved or in separate buffers. It seems like there are two options here: A - store the data tightly packed, and unpack and repack to and from SSE-friendly format when you need to manipulate it, taking the CPU performance hit, or B - store the data loosely packed on 16-byte boundaries, SSE it directly, and take the bandwidth and VRAM hit when sending to GPU. Which of these options would be preferred in a real-world 3D engine, or under what circumstances would you prefer one to the other? 

When you scale along the X-axis, the X-coordinate (parallel to the axis) gets stretched, while the Y-coordinate (perpendicular to the axis) remains the same. You can think of scaling along an arbitrary axis as stretching along some diagonal. Here's a pic of a square being scaled along the main diagonal (the axis pointing to <1, 1> ) by factors of 2 and 0.5. 

The projection matrix distorts the view frustum (the volume the camera can see) into a unit cube. So everything with all coordinates in the range -1 to 1 after projection is potentially visible, and everything else can be clipped. To make things further away look smaller, we need to divide by their Z distance. We can't put this value directly into the matrix since it is different for each point we want to project, so instead we place each point's Z into its projected W, giving (X, Y ,Z ,1) -> (X', Y', Z', Z), known as clip coordinates. This is then divided by the new W to give (X'/Z, Y'/Z, Z'/Z, 1), known as normalized device coordinates. On the GPU this division happens automatically after the vertex shader and before clipping and rasterization. The general form of a projection matrix is 

In a linear transformation system, your origin is always a fixed point, since 0*anything = 0. So imagine you have a cinema screen, and the origin is at the centre of the screen. Using linear transformations, you can rotate, scale or shear the image, what you can't do is move it, since you have a fixed point in the middle. Now add a dimension, and move your origin out of the screen through this new dimension. Your origin is now at the projector, at <0, 0, 0>, and the centre of your screen is at <0, 0, 1>. So as well as the image transforms you had available before, you can now effectively move your image, by shearing through the space in between the projector and the screen. Add another dimension and you have the 4D -> 3D case. 

I found an answer in the form of the Todd-Coxeter Algorithm from group theory. This still involves iterating over permutations of the group, and removing duplicates. However, the group can be defined using symbols to represent different sequences of operations, and duplicate-matching performed on these symbols rather than elements of the group itself. For example, if a pair of mirrors A and B have an angle of (PI/3) then they form a cycle of order 3 (since a reflection through a pair of mirrors equals a rotation through twice the angle between them). So the sequence ABABAB maps any element back onto itself. By constructing a path like this for each pair of mirrors, it's possible to define new elements for each step along a path, then eliminate duplicates either through string-matching operations or by constructing a graph and eliminating equivalent nodes. For large sets, this should be much more efficient than multiplying and comparing matrices directly at each step. 

The texture is laid out linearly in VRAM, and the texture unit merges transactions on different areas when loading to cache. The texture gets swizzled when passed to the GPU, and is stored in VRAM in Morton-coded order (or another space-filling curve). 

The A and B terms relate to the field of view, higher values give a narrower view. Specifically, B is the cotangent of half the vertical view angle, and A is B/aspect ratio. The C and D terms are bias and scale terms which map the near plane to -1 (0 in DirectX), and the far plane to 1. These are used for writes to the depth buffer to determine which objects occlude others. The depth mapping is actually a function of 1/Z, rather than Z, since this gives better precision close to the near plane, and allows perspective-correct interpolation. Also note that in OpenGL, W' is -Z rather than Z, since we are using a right-handed coordinate system, with negative Z pointing into the screen. 

A perfect Lambert reflector actually reflects light in a cosine distribution - that is, the amount of light per unit area reflected in any given direction $R$ is proportional to $N.R$. The reason the radiance appears constant for all angles is that as the view direction moves away from the normal, the reflected light per unit area decreases, but the surface area per projected beam area (and hence per pixel) increases by the same amount - the cosine factors cancel out. Given this distribution, to be energy conserving, the probability density for the reflected light in all possible directions cannot sum to more than 1. If you integrate the cosine function over the whole hemisphere around the normal, you get $\pi$, so this is the normalization factor you need. 

I've been digging a bit into what actually happens at a hardware level on the GPU, and found NVidia's Life of a triangle which explains the pipeline pretty well, at least for green boxes. One thing I wasn't clear on is what happens with indexed meshes, when the same vertex is used for a bunch of different triangles. Since data is generally not persisted for any longer than necessary in a stream processor, I'm guessing the vertex is simply destroyed after being rasterized, then fetched and run through the vertex shader again whenever it appears in a new triangle. Can anyone confirm this? Also, what happens in line-strip or triangle-strip modes? Does the GPU persist the transformed vertex data somewhere until the 2 or 3 relevant primitives have been rasterized in these cases? 

There shouldn't be any performance penalty inherent in using a 3D group over a 2D one, the dimensions are just an aid for programmers to map invocations on to memory locations (since you can read or write wherever you like). The group size affects which invocations can access the same block of groupshared memory, so it forces all warps from the same group to run on the same SMP. If you have a shader that needs to move stuff around a lot in its local memory block (e.g. a bitonic sorter), a large group size is preferable, but in your case it shouldn't make much difference. I suspect your problem is with the idle threads within your warps. If I understand your question properly, you're using a thread per input texel or sample to do the texture reads, but then only a subset of these threads to do the processing? If so, you're sacrificing a lot of compute power within each warp. Remember, texture reads are done in minimum 2x2 blocks, and the results get cached in a way optimized for locality for the dimension of the texture (see this question). So although your general purpose shader might seem slow with all the texture reads, a lot of them will be effectively almost free. Also, the GPU does latency-hiding on slow memory reads, swapping in other warps until the result of the read is available. So your general purpose shader actually seems like the smart option here, as it maximizes compute power.