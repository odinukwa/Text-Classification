"By design" doesn't necessarily mean that there is any specific gain in the choice. It simply means that a choice was made, intentionally. That said, I think the system is easier for developers to handle when you don't have to implement progress tracking. It also likely simplifies any TRCs around achievements (what is or is not acceptable in terms of "tracking progress" for an achievement that is unlocked by reaching a particular point in the game's plot, for example?) EDIT: In the general case, I think it could be argued that exposing the progress towards an achievement could expose too much of the underlying "spreadsheet" of the game. Whether or not that matters depends on the style of the game, I guess -- if you really want to encourage suspension of disbelief, maybe it's better to hide more of these mechanics that to show them. 

You can certainly do all of this by simply storing the move list for a game in a text file. That would be the simplest way to store the data, but at the expense of forcing you to build all of the query mechanisms yourself. Something more structured like an XML or JSON file will probably make querying slightly easier (since you are on iOS, you may want to consider the APIs for reading files, which are effectively the same thing). It's unclear to me what you are actually using Parse for; your description here makes it sound like it's just a file storage system that is redundant with local storage (do you want to also collect the stats for the opposing player in a peer-to-peer game?). A SQLite DB would serve your described needs just as well and avoid the latency involved with round-tripping this file over the network. If your aversion to using 'a database' stems from not wanting to have something like a SQL Server instance up and running to deal with this data, SQLite (perhaps via CoreData, which will then give you all the -based queries you probably want) is worth looking into. But barring that, your described implementation using text files seems workable. 

Some developers do, some developers don't (in games and elsewhere). It depends on what the needs/requirements of those developers are, and what existing technology they have to leverage. C++'s standard library is often given the same treatment, and people often wonder the same thing you are wondering about it, too. Most of the reasons are similar, for example: 

EDIT: Also, here is a tutorial I dug up via a Google search for "XNA relative mouse input" (which is what you're asking about). It's a bit old, I think, so some of the methods may have had their names changed, but the concepts should still apply. EDIT 2: Meant to say "handling this in Win32" and not "handling this is Win32," updated accordingly. 

A cursory bit of searching with Google suggests that you probably want to use JPanel for this, although it's possible you could make use of any control since it looks like the technique you'll want to use is to subclass the control and override a method (the method, specifically), which will give you a instance you can use for rendering. JPanel appears to be a very basic empty control and so is probably the ideal choice for this. 

Most "shader effects" like filtering and normal mapping have very little in common that could be considered building blocks between effects. 2D filtering and normal mapping aren't any more difficult or advanced than each other, for example, and the same is true of many effects with only a few notable exceptions (the various flavors of shadow mapping come to mind). They're just different. That's why you'll see so many cookbook-style books when it comes to discussing actual graphical effects programming (as opposed to graphics programming at a fundamental level). Most in-world effects (i.e., not post-processing ones) are rooted at some level in the physical properties of light and the simulation or approximation thereof. To that end, reading books that focus on ray-tracing as a rendering method may help you better understand the core theories involved (Physically Based Rendering is also an excellent read). Once you understand that, the "scattered theory" present in books like GPU Gems will relate primarily to the specific ways in which the GPU can be manipulated to approximate the real-world physical scenarios desired, and will seem less disjointed. Similarly, post-processing shaders tend to draw from signal and image processing theory. I think I recall having this book as a textbook in a class on the subject, but don't recall too much about it. In general, though, you aren't going to find too much that caters to that kind of approach because the theory that binds together all the various shader effects that are in vogue right now is structured like a very, very shallow tree rather than a very deep one -- there are very few dependencies that cater to "bottom up" approaches once you get beyond the fundamentals of graphics programming theory itself (how to structure the scene, the transformation pipeline and associated linear algebra, rasterization, et cetera). 

There isn't really a universally standard coordinate system. I have worked on games and with tools, across all of them, have used almost every possible combination of systems. I think the only ones I have no worked with in practice are "+X is up" and "-X is up." 

Google provides several links that may be starting points: here, here and here for example. There isn't likely to be an actual "fastest" or "best" algorithm -- there rarely is. Rather you'll find some are more suited for particular scenarios. You have not provided enough information about your scenario or what you have tried yourself in order for us to provide really directed answers. Also, have you profiled your code and identified the bottleneck(s)? 

Framerate independent motion is when objects in your game move based on some criteria other than which rendering frame you are on. Typically this alternate criteria is the delta time since the last update. A game loop that is framerate-dependant might look like this: 

This is very possible. Most graphics APIs support a concept known as alpha blending, a process of combining two images in such a way as to make it look like one of them is transparent. This is generally accomplished by adding an additional color channel to the standard red, green and blue channels: alpha, which represents how opaque a pixel is. Zero indicates a fully transparent pixel, and the maximum value (usually 1 or 255 depending on how the color channels are represented indicates full opacity). Thus, to accomplish your goal of a game with a background other than solid black, you need: 

is a method that exists on the game object itself, or some system at a similar layer of abstraction, and it's sole job is to take an entity, extract useful bits of information about it, and turn that into a (or multiple render objects, if that's needed). An entity's position data is certainly useful for things beyond rendering, so it can remain in the entity. Things like texture references (as in, just strings or IDs) aren't actually render-API objects and so they could remain in the entity, particularly if you are using a component-based approach to entity construction that is so popular these days: an "aspect" or "visual" component can be added that holds all the render-only data like texture or shader references. If you don't have a component-based system, you can still offload some of that information to external databases. For example, an entity could have a "race" and "gender" like in most RPGs, and the you can actually look up the texture ID reference in some catalog of race/gender combinations you load from one of your data files. This approach scales naturally to user interface: user interface is another system that you fit an adapter in front of to create render objects for all the appropriate screen / widget elements in the current scene. 

The Windows Phone class can be used to read the current orientation (, which will be a motion reading) a yaw/pitch/roll attitude. The value of each measure will be in radians. The yaw/pitch/roll measurements are the same as those in traditional flight dynamics (so it's pitch and roll you want). If you give your ball a velocity vector, you can then adjust that vector in your game's update logic for the ball based on the current values of the phone's attitude. For example, you might do something like: 

Since Steamworks is a C++ API, you'd need to write a wrapper. You can use the Java Native Interface (JNI) for this, or possible the Java Native Access API instead. The JNI is the "classical" method, the JNA thing seems to be newer and I know less about it. The Steamworks SDK is not public, so I can't link you directly to the interface you'd want to wrap, but all the ones you'd want to wrap are probably located in . Fortunately, Steamworks is so popular that there are already wrappers other people have written for you, such as steamworks4j. Steamworks services support an offline mode themselves, so you should not need to do anything specific yourself to deal with tracking and recording achievements when the user is offline.