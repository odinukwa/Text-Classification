Begin transaction (serializable) Get unprocessed messages (using the flag) Set their to true (in RAM) and update their status Commit transaction Do the business Set their to false (in RAM) Begin transaction (serializable) Update their status Commit transaction 

I look at my code's logs, and see that a query to delete something which occurs like 200 times a day, sometimes times-out. I have no clue to how to debug this. Because it's a very random behavior. Nothing is changed accept that the size of database is growing under operation. This is a pseudo-log: 

And one hundred times it worked. Now I'm stuck. I've done all the steps, and client can't connect to the engine. It simply times out. Here's what I've done: 

This works fine when runs sequentially. But when I run multiple instances of my application (concurrency), I see that some messages are processed twice or thrice. Here's what happens: 

Can someone explain or direct me how execution on indexes happen with different constants at intervals in Mysql. I notice only for the first execution on the table it takes time, after that with different constants it executes the query very quickly. I would like to know how to execute the query in such a way that it should take same amount of time every time it executes with different constants, is there a way to set some parameter off / on? Query executed time : 9 mins. 

Kill all processes for mysql in ps -ef | grep -i mysql If you see any any ibdata file generated under your default data directory you may remove them also iblogfile1,2 since its a fresh installation no need to safe backup them. chown -R mysql:root /var/lib/mysql chmod -R 775 /var/lib/mysql also the same permissions to /etc/my.cnf. Now try to start the instance by issuing /etc/init.d/mysql start and tail -f /var/log/mysqld.log. It should start without any issues if you get any error message . 

As soon as I start this query, it finishes. Thus, I get no chance to see to find out what type of lock this query has applied on database objects. I can inflate this query with transaction statements: 

But frankly, that doesn't seem the proper way for natural locking detection, because I've manipulated the default behavior using transaction statements. Do we have a tool, like to show us some information about locking of a given query? If not, how can we find out what type of lock a given query can apply on a database, because they get executed TOO fast to be detected. 

I have an application in which some reports time out before getting data. We started increasing timeout througout our code in different places, and as our database is growing bigger and bigger, we see that we're repeating/duplicating more code. So we decided to increase database timeout for queries globally. But I have this feeling that this is wrong. Yet I'm not able to bring forward any reasons. Is it bad to increase timeout everywhere? Why shouldn't we do this? We're using SQL Server and Entity Framework. 

If usage means you can only connect to the DB Server and not anything else. Whereas in this case REPLICATION SLAVE will require to read and pull binlog events from Master. To Fix this: On Master Execute: 

[difference this time is you are granting with the password for REPLICATION SLAVE GRANT] Show Grants: If you do show grants; after you give GRANT REPLICATION command. You should get below grants from same slave host via CLI. 

Rollback: If it doesn't start for some reason put back the iblogfiles and start. Please see link $URL$ for detailed explanation 

Now this is your final data set : /backup/2017-06-01_18-01-52/2017-06-01_18-22-29/* , replace data dir to this data set -> Ensure every 1 to 4 steps had no errors and completed OK is found on all outputs. Test the result incremental backup data on a Test DB and Restore it to see if the server starts without any error messages 

Learner can declare that he/she knows a given word Learner can memorize the given word (to be managed by the application later) Learner can choose to ignore the given word, so that it won't be given to him/her later Admin can ignore a word in general, so that it won't be given to any learner A list of words exist in database, as the reference Next word to be given to the student, should be new to him/her 

And it has non-clustered indexes on and fields. I want to find what a has sent us in a specified period. Thus I run this query: 

Get unprocessed messages (using the flag) Set their to true (in RAM) and update their status Do the business Set their to false (in RAM) and update their status 

As you can see, this is the only information that I have. That a delete operation is timed-out at time X. I'm stuck at how to debug this and find out the cause. I don't know even from where to start. I'm using SQL Server 2014. 

What had happened?: Your backup would have just started and there was a query that was going on running on a particular table for a longer period of time before even FLUSH TABLES command and didn't release the lock on the table and must have been waiting for that thread to get completed or keep trying to flush until the revision_version of that table is same as all tables. Thus you get other threads blocked for other tables as well. As this is entire DBs*.Tables* level lock while flush tables was going on. Finally it would have got accumulated every new connection in processlist and piled up until and not allowing anyone to login. Let's say if you have managed to login to the terminal and tried to kill flush tables, I don't think there is a way to pull back or rollback the flushed tables that has been done and release its own thread connection. So it might in a for longer time. And thus you might have reached the last option that is to restart the server. How to fix it?: At the time of issue, when admin managed to login to mysql prompt. Instead of issuing kill command on FLUSH TABLES thread, if kill was given to the thread running for long SELECT.There are chances that SELECT would have dropped and table is left FLUSH TABLES to acquire and update revision_version and release the lock for new queries. And backup would have continued. Since I don't think anyone expecting an answer on the other end waiting for the result for query running long hrs. What is the long term solution?: 

I'm trying to find out the position of each given record of a query in 0-100 scale. I use ranking function this way: 

Last night all of our systems failed and connections to the database kept dropping. We watched the log today and found out that this was the message: 

Instance A gets some unprocessed messages While instance A is setting the to true in RAM, instance B gets some messages, and chances are that it fetches one or more of the messages which are already fetched by instance A 

And this query works lightening-speed fast for past 2 years. But when I change the part to a closer date, it freezes out and takes more than 2 minutes to complete. In other words, based on different inputs, it behaves differently, sometimes even hanging out and not returning for more than 10 minutes. I expected a consistent behavior. What do I miss about indexing? What can cause this inconsistent performance? Update: I changed names of columns and table, so I can't attach execution plan as a picture. But here's the issue. Thanks for guiding me. when I change value of date parameter, SQL changes index seek from to . I never thought that SQL creates execution plan based on the value of parameters. How that could be? 

You may give it a try using percona backup alpha version for Windows - Download_Link . Below are the steps after installation in Windows Bash. 

Now the sync is broken on Slave. Your SQL_SLAVE_THREAD stops. Or anyother conflicting statements tries to append it will get broken, above is a better example for that. But if you get updates like below on Master, for sure your Slave data is consistent with Master. 

Both the servers are ntp synched with GST. And they remain same after Master reboot too. It took almost 2 hrs to bring up Master Server. How come only slave can have the data that Master is not aware of? Please have someone come accross such a thing? 

innodb_thread_concurrency innodb_flush_neighbors -> This shouldn't be enabled if SSD innodb_log_file_size -> Configure as recommended innodb_io_capacity -> Worth to verify the disk IO, see if IO waits for disk seeks are fine. Try to see if its bursting IOPs on Mysql end for capacity given on server. innodb_flush_log_at_trx_commit -> You can set it to 0/2 if consistency is a trade-off. But if you need consistency then you need to ensure the disk IOPs is given to burst fsync at every commit. Recommended is 1. 

SQL Server database which is very large (4TB) is stuck in recovery state. Reason: Data center had a planned downtime and we had to turn the physical machine off and when it turned on, as I read in error logs, SQL Server couldn't access the files, because files were on a SAN machine, and probably things didn't went smooth. 

I don't know why, but during steps 1 to 4, other instances can still perform read queries. This is not desired. I want to exclusively prevent anything, even read queries from being executed on messages table during step 1 to 4. How can I do that? What am I missing in my design? The goal is to make sure that while a message is queued for processing, no other instance would process it again. 

I'm stuck at setting transaction isolation level. Here's my scenario that happens in the application: 

As only you are aware of the word you may keep that word as a seperator. And at your application end split this word, if you get any other data after seperating 'clops' from the return type then treat it as a real return data. By this way you can ignore/suppress the warning that you get. 

If you ask me there is no better way to fix this kind of issue with handlers or exceptional handling declarative statements. But you may try to give a constant value that will return even if your answer should be NULL. Let that constant value be a wierdest one that you can expect from the fucntion that could return. Add that constant value to your return function. Disclaimer: I haven't tested it, but you may give it a try. Let me know on your results. In this case I'm mentioning 'clops' as the constant. Try below changes to your function only under select clause and see. 

This of course results in tremendous problems, because I'm working on a text-processing application and data comes almost from everywhere and I need to normalize text before processing it. If I know the reason of difference, I might find a solution to handle it. Thank you. 

Of course it can be solved in a minute of Googling around, via some tricks and scripts. However, the more we search the less we find out about the reason behind this. Why it has happened in the first place at all? What possible reasons caused it, and might cause it again? Was it because of an attack? We have no clue, and any help is appreciated. Update: in this post it's been argued that a policy check can make this happen. We haven't set a policy so far, and we do not even know where are the policies. Update2: We realized that some of the threads in some applications kept working, even though other threads and other applications were encountering message. Could it be that doesn't require re-authentication? How is that possible?