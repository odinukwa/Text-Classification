I think Andersson and Petersson's "Approximate Indexed Lists" are what I was looking for. If I am reading it correctly, they provide the updates I was looking for in constant amortized time and the queries I was looking for in constant worst-case time. 

Let $S \neq T$ be two sets of w-bit integers. If their fingerprints are the same, then the fingerprint of $S \triangle T$, the symmetric difference of $S$ and $T$, is 0, which happens with probability $2^{-w}$. 

In Kurt Mehlhorn's monograph "Data Structures and Algorithms 1: Sorting and Searching", he poses the following question (III.9.22): 

Hopefully, huge proofs are checked by a small proof checker that can be read and understood by humans in a reasonable amount of time. This is sometimes called the "de Bruijn criterion". If you believe that the proof checker will not certify an incorrect proof, you need only check the checker. 

This is the first link on Google for the search "chernoff limited independence": "Chernoff-Hoeffding Bounds for Applications with Limited Independence" by Schmidt et al. 

I think "A High-Performance Algorithm for Identifying Frequent Items in Data Streams" by Anderson, et al. shows an answer, though the weights are integral, not real. 

Use a forest of 2-4 trees in which the keys are at the leaves. Every internal node contains a copy of the minimum key that descends from it as well as a positive integer indicating the number of values of this key descending from it. Additionally, maintain a van Emde Boas tree containing as keys each positive integer $i$ where $e_i$ is the left-most element in one of the 2-4 trees. In the vEB tree, each integer $i$ has satellite data: a pointer to the root of the tree the key associated with $e_i$ is in. 

following "Space-Efficient Online Computation of Quantile Summaries", by Greenwald and Khanna. I'm also interested in whether this is possible with non-integer, non-consecutive moments, so sets of values like $\sum x_i^{-1/2}, \sum x_i^\pi$ are also of interest. I am more interested in deterministic algorithms than randomized ones. 

Orthogonal 2-dimensional range reporting is the problem of storing a set of values from $U \times V$, where $U$ and $V$ are totally ordered universes, subject to queries of the form "Return all stored points in the four-sided box $[a,b] \times [c,d]$", where $a,b \in U$ and $c,d \in V$ are parameters of the query. Nekrich, in "Orthogonal range searching in linear and almost-linear space ", summarizes the best known results for linear space: $$ \begin{array}{|l|l|l|} \hline \text{Reference} & \text{Query Time} & \text{Modification Time} \\ \hline \text{Kreveld and Overmars} & O(\sqrt{n \lg n} + k) & O(\lg n)\\ \hline \text{Chazelle} & O((k+1)\lg^2 n) & O(\lg^2 n)\\ \hline \text{Nekrich} & O(\lg n + k \lg^\varepsilon n) & O(\lg^{3+\varepsilon} n) \\ \hline \end{array} $$ 

In Willard's Searching Unindexed and Nonuniformly Generated Files in $\log \log N$ Time, he references a preliminary version (of the linked paper) entitled "Surprisingly efficient search algorithms for nonuniformly generated files" appearing at the 21st Allerton Conference on Communication Control and Computing in 1983, pp. 656-662. I can't find this paper on the web, but in the later (linked) version above, he says that the older paper shows that the synergy between binary and interpolation search can reduce search time to $o(\log n)$ for certain non-uniform distributions of record keys. Specifically, call a PDF $\mu$ regular if there are $b_1, b_2, b_3, b_4$ such that $\mu(x) = 0$ for $x < b_1$ or $x > b_2$ and $\mu(x) \geq b_3 > 0$ and $|\mu\prime(x)| < b_4$ for $b_1 \leq x \leq b_2$. For data produced by regular PDFs, interpolation search takes $\Omega(\log n)$ expected time, while binary search takes $\Theta(\log n)$ expected time. Interleaving them, however, takes $O(\sqrt{\log n})$ expected time. You may also be interested in Willard and Reif's "Parallel processing can be harmful: The unusual behavior of interpolation search", which shows that "parallel processing before the literally last iteration in the search of an unindexed ordered file has nearly no usefulness". 

Is there a simple and cache-oblivious data structure that solves the dynamic predecessor problem for strings of length exactly $k$ over an alphabet $A$ in worst-case $O((k\log A)/B + \log n)$ memory transfers and $O(k\log A)$ letter comparisons, where $B$ is the block size? By predecessor problem I mean the problem of maintaining a set under insertion, deletion, and predecessor, where predecessor(k) finds the largest key in the set that is $\leq k$. Ternary search trees achieve the letter comparison bound and are very simple, but can require $\Theta(k\log A)$ memory transfers. String B-trees do better than the I/O bound I'm looking for, but they are not simple, and the cache-oblivious ones are even more complex and do not achieve the I/O bound in the worst-case. (String B-trees might achieve the comparison bound I'm looking for; I'm not sure.) My intuition is that by fixing the size of the keys and relaxing the I/O bound, a simpler structure might be possible. 

Even without the time bound, it is impossible to "avoid memory segmentation" unless you can move the allocated objects around, like in a compacting garbage collector. See Robson's "Bounds for Some Functions Concerning Dynamic Storage Allocation", which shows that allocating $m$ bytes in blocks of size between $n$ and $N$ requires $\Omega(m \log (N/n))$ bytes of memory. Additionally, the buddy system achieves this bound and can be done in logarithmic time. 

Inductive types in programming languages that have dependent types allow the return types of the constructors to depend on the values (not just the types) of the arguments. 

The the tree made by has height 2, the tree made by has height 2, and the tree made by has height 3. However, the tree made by has height 5. Here is the code I used to find this error. 

Use linear time selection to find the $k$th largest element, then do a partition step from quicksort using the $k$th largest element as the pivot. 

GHC's 's function builds a fingertree in $O(\lg n)$ time and space, but this is enabled by knowing the elements that go on the right spine of the finger tree from the get-go. This library was written by the authors of the original paper on 2-3 finger trees. If you want to build a finger tree by repeated concatenation, you might be able to reduce the transient space usage while building by changing the representation of the spines. The spines on 2-3 finger trees are cleverly stored as synchronized singly-linked lists. If, instead, you store the spines as deques, it may be possible to save space when concatenating trees. The idea is that concatenating two trees of the same height takes $O(1)$ space by reusing the spines of the trees. When concatenating 2-3 finger trees as originally described, the spines that are internal to the new tree can no longer be used as-is. Kaplan and Tarjan's "Purely Functional Representations of Catenable Sorted Lists" describes a more complicated finger tree structure. This paper (in section 4) also discusses a construction similar to deque suggestion I made above. I believe the structure they describe can concatenate two trees of equal height in $O(1)$ time and space. For building finger trees, is this enough space saving for you? NB: Their use of the word "bootstrapping" means something a bit different than your use above. They mean storing part of a data structure using a simpler version of the same structure. 

Is it possible to estimate within $\epsilon$ the quantiles of a set of integers $\{x_1, x_2, \dots, x_n\}$ given only the values $\sum x_i^0,\sum x_i^1, \sum x_i^2, \dots, \sum x_i^{f(n)}$ where $f \in o(n)$? If so, what is the most slowly growing $f$ that will suffice? By "estimate the quantiles", I mean: 

What you're looking for is called a "succinct" or "implicit" dictionary. The best solution I know of is Backyard cuckoo hashing, by Arbitman et al from FOCS 2010, which "guarantees constant-time [insert, delete, lookup] operations in the worst case with high probability" while using $B + o(B)$ bits, where $B$ is the lower bound you mention. If you need updates, I don't know if you'll be able to beat $O\left(\sqrt{B}\right)$ redundancy, given the lower bounds in Brodnik et al.'s "Resizable Arrays in Optimal Time and Space". 

Perhaps one must consider the parent of the node to be inserted as having weight 2. Then the "find the lowest in-density ancestor" is equivalent to "find the least amount of rebuilding needed to maintain the invariants". edit: In the physical world, density is defined as mass divided by volume. This corresponds to the density notion in these cache-oblivious B-trees if we view each "item" as having mass 1 and each location in $\mathcal{T'}$ (the static binary tree) as having volume 1. Before inserting an item, we want to make sure that once it is in the tree it is in a subtree that is within its prescribed density bounds. We therefore imagine that it is already there in the tree, but taking up no space yet. The reason why we might imagine that it presently takes up no space is that, well, there might not be any space for it to take up right now, as in your example. To make sure that the tree the new item will be in is within bounds, we need to find its closest ancestor that we actually can rebalance (without increasing the height) and insert the item into while still remaining within the density boundaries. To do so, we find the nearest ancestor that will be within the density boundaries, which means using its current "volume" and its soon-to-be-correct "mass". To get the latter, we must include an extra unit of mass for the item we are about to insert. 

A side note: GHC has a mechanism for treating value constructors as type constructors. This is not the same as the dependent inductive types that Coq has, but it lessens the syntactic burden of GADTs somewhat, and it can lead to better error messages. 

You might store $\Theta(B)$ children in every node of a trie with alphabet size $\Theta(B)$ by treating your binary sequences as strings over an alphabet each letter of which takes $\Theta(\lg B)$ bits. This will waste space if your string set is sparse, but I think inserts and prefix searches require $(n/\lg B)$ I/Os in the worst case where $n$ is the length of the longest unique prefix (in the case of insert) or the length of the prefix searched for. To find the longest sequence with a certain prefix, store with each child pointer the length of the longest descendant string. If the number of strings you plan to store is $o(2^n)$, you might also be interested in the string B-tree. Insert takes $O(n/B + \log_B k)$ I/Os in the worst-case for a string of length $n$ in a tree containing $k$ strings. Prefix search takes $O((p + occ)/B + \log_B k)$ I/Os in the worst-case, where $p$ is the length of the prefix to search and $occ$ is the number of results of the search. These are both larger than your requested bound ($O(p/B)$ or $O(n/B)$ in this notation), and prefix search isn't exactly what you're looking for, it sounds like, but I think storing the longest descendant length will work in this case. 

Furthermore, once a key $k$ has been subject to a , all further inserts are $> k$. Related work: Bose et al.'s "Fast Local Searches and Updates in Bounded Universes", which is faster than I need for but slower than I need for . Brodnik et al.'s "Worst case constant time priority queue", which uses the exotic "Yggdrasil memory". For the purposes of this question, I'm interested in more standard integer RAM models. Brodnik and Karlsson's "Multiprocess Time Queue", which limits insert to elements with keys in $(k_{\min}, k_{\min} + \delta_{\min}]$, where $k_{\min}$ is the value of the minimum key. Note that this is pretty simple with a hash table, but that uses amortization and randomness: 

Mihai Pătraşcu explained on his blog how to strengthen the variance bound of Chebyshev by looking at higher moments. He references "Chernoff-Hoeffding Bounds for Applications with Limited Independence" by Schmidt et al. You also might be interested in "Concentration of Measure for the Analysis of Randomized Algorithms" by Dubhashi and Panconesi. 

What are the best known results for a data structure offering the following operations on sets of points in 2-dimensional euclidean space: 

Can order be maintained in a list in $O(1)$ amortized time without using any arithmetic operations not in $AC^0$? 

Most balanced trees that have logarithmic-time insert and delete also have logarithmic time $merge$. This includes AVL trees, red-black trees, and B trees. You can see this question on Stack Overflow for several references results. They include a paper on CGAL (red-black), a reference to the OCaml standard library (AVL), as well as the red-black tree chapter of CLRS's Introduction to Algorithms. When I was learning about this topic, I was happy to find that $merge$ is actually usually a simpler operation than $insert$ and $delete$. 

In this particular case, I'm not particularly interested in approximate nearest neighbor, Monte Carlo algorithms, or algorithms that assume the data is well-formed in some way. I am not as prejudiced against Las Vegas algorithms, algorithms that assume the coordinates of the point have $O(\lg n)$ bits, or algorithms with running time depending on $k$. 

This is similar to a log-structured merge trees or cache oblivious lookahead arrays (or COLAs). The simplest COLA is an array of size $2^k-1$ with virtual "levels" of length $2^i$ for $0 \leq i < k$ laid out in order in the array. Each level is totally full or totally empty, and each level is sorted. To insert an item, place it it level 0 (with $2^0$ length) if level 0 is empty. Otherwise, merge it with the item at level 0 and fill level 1 (with $2^1$ length) with the resulting list of length 2. If level 1 is already full, merge again; repeat. Insert takes $O(\lg n)$ amortized / $O(n)$ worst-case time. Lookup takes $O(\lg^2 n)$ time. More advanced versions can reduce and partially deamortize these. 

Yes, this can be done through bucketing. The representation in Briggs and Torczon can be used to store not just bits, but values associated with any set bits, by storing them next to the values in the dense array. For $k$-bit values, this leads to a total storage cost of $(2n+1)\lceil \lg n \rceil + kn$. Now pick $k=\lceil \lg n \rceil$. Each $k$-bit value will represent a set of $\lceil \lg n \rceil$ contiguous bits in the bit array. We thus only need to track $\lceil n/\lceil \lg n \rceil \rceil$ of them with the dense and sparse arrays. This leads to a total cost of $\Theta(n)$ bits of storage, with $\Theta(\lg n)$ initialized bits. Like the basic solution, this requires $O(1)$ operations on words of length $\Theta(\lg n)$ to set a single bit. 

Each $Split$ uses $O(\log \log n)$ time for the vEB operation and $O(\log n)$ key comparisons. Each $FindMin$ uses $O(\log \log n)$ time for the vEB operation and $O(l)$ key comparisons. Each $DecreaseKey$ uses $O(\log n)$ key comparisons. 

The best known lower bounds are discussed in Thorup's "Mihai Pǎtraşcu: Obituary and Open Problems", and none preclude deterministic amortized constant-time operations. 

Okasaki's Functional Pearl, "Even Higher-Order Functions for Parsing or Why Would Anyone Ever Want To Use a Sixth-Order Function?" answers this question with a type of order 6. 

I know more about purely-functional data structures than external-memory data structures, but I'll give this a go. A B-tree can be written in a purely-functional way by path copying. The path will be short ($O(\log_B n)$), but copying each block takes $O(B)$ writes in $O(1)$ blocks. If you're willing to let the structure be just fully persistent, rather than purely functional, I think you can reduce the number of writes in a node copy to $O(\lg w)$ expected amortized, where $w$ is the word width, using the fully persistent arrays in "Confluently Persistent Tries for Efficient Version Control" You might want to watch this presentation about RethinkDB, which uses purely-functional data structures because of the cost of writes in SSDs. 

Goodrich's "Randomized Shellsort: A Simple Oblivious Sorting Algorithm" has a discussion of data-oblivious sorting. Sorting networks are data-oblivious, but impractical in general, as I understand it. 

If there is a constant $k$ such that any $k$-independent family suffices for #3, then it the polynomial construction of $k$-independent families would meet #1 and #2. What bound do we have for the heaviest loaded bin with $k$-independent hashing? Using Theorem 4.III of "Chernoff-Hoeffding bounds ..." and the union bound, I think I can get a bound of $O(n^{2/k})$ on the weight of the heaviest loaded bin w.h.p. Can this be brought down to $O(\lg ^c n)$ using other techniques? 

What are the best known extensions of these in external memory? The result from Arge et al.'s "On two-dimensional indexability and optimal range search indexing" uses superlinear space. The only generalizations I know have bounds $$ \begin{array}{|l|l|l|} \hline \text{Reference} & \text{Query Time} & \text{Modification Time} \\ \hline \text{Kanth and Singh} & O(\sqrt{n/B} + k/B) & O(\lg_B n)\\ \hline \text{Procopiuc et al.} & O(\sqrt{n/B} + k/B) & O\left(\frac{1}{B} \left( \lg_{M/B} \frac{n}{B}\right) \left(\lg \frac{n}{M}\right)\right)\\ \hline \end{array} $$ 

This answer is a bit hand-wavey. Select a function $h$ uniformly at random from the set of all functions from w-bit words to w-bit words. For each set, maintain a w-bit fingerprint as follows: