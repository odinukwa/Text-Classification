tables that are created in a child scope (Procedure, Trigger, -ed SQL) are automatically dropped when the scope ends. But ones at of 0 are only automatically dropped when the session ends. These hang around for multiple batches. So for such adhoc queries it is essential to drop them (especially if they might be run multiple times or a later batch will create one of the same name). Pasting the following into a new query window in SSMS 

If the value passed for is a variable or parameter (rather than a constant) you may find you get better plans if you use with this. Particularly if the cardinality of and is very different (likely even without the hint only the relevant one will be accessed as the table access will be under a filter with a startup expression predicate). 

The one affected by using two part names is If you try the following under the credentials of a user with default schema 

gives the closest estimated subtree cost to at . Either way it is clear that it is basing the costing on the assumption that each scan will only have to process a tiny proportion of the table, in the order of hundreds of rows rather than millions. I'm not sure exactly what maths it bases this assumption on and it doesn't really add up with the row count estimates in the rest of the plan (The 236 estimated rows coming out of the nested loops join would imply that there were 236 cases where no matching row was found at all and a full scan was required). I assume this is just a case where the modelling assumptions made fall down somewhat and leave the nested loops plan significantly under costed. 

It is the parameter that has been implicitly converted, not the column. The query has been subject to Simple Parameterization by SQL Server. You have no control over the datatypes used in this process. It uses the smallest datatype that can hold the literal value ( can fit into a ). The implicit cast of a parameter to an won't cause any problems. However to avoid having multiple plans in cache for , , and get rid of the implicit cast you could explicitly parameterize the query yourself - with a parameter of datatype rather than having it be parameterized automatically. 

A similar documentation bug was filed by Aaron about . Another annoying aspect of that stored procedure is that the must be passed as 'bigint' and it doesn't accept a literal integer. I assume that this is also because it is an extended stored procedure. 

Then extract a month at a time from and merge into (with a when matched then increment, when not matched insert). The leading column means that as long as you write the query sargably the extraction of each month can be done efficiently and the extracted rows for a month will be ordered by making a merge join against possible without a sort. Whilst that could benefit this particular query you'd need to assess the utility of this index against your overall workload. Then calculate the totals from (can leverage the PK order to avoid a sort) and store those somewhere and move onto the next month. (Or possibly instead of you could use a "temporary" permanent table and create an indexed view on that to avoid the separate explicit aggregation step and just copy the values straight from that before moving on) 

2) Otherwise you could loop through all databases (and query in each) with something like this code for a more reliable and flexible sp_MSforeachdb 

Logical fragmentation occurs when the logically next page is different from the physically next page. In the case of the leaf level of an index with a monotonically increasing key this can happen if extent allocations for the index become interleaved with the extent allocations for other objects. Even without this a small amount of fragmentation will ensue due to the first page allocations coming from mixed extents and pages for different index levels being interleaved in the same extents. (Edit: Plus of course effect of updates and page splits as per @gbn's answer) 

Because you have an equality predicate on . With the reversed order it is able to seek into exactly the rows matching the status and these can then feed into a stream aggregate to do the grouping. Your original index supports the but not the - meaning the whole index will need to be scanned and only rows matching the retained. 

In this circumstance I would say yes. I'd probably also add an to let it "sniff" the variable values. The optimal plan will likely vary dependant on the proportion of rows in the larger table that match this range. It provides a potentially useful extra path to the optimiser and it is not something that the query optimiser ever does by itself as far as I know. The closest thing to it is that with a merge join it will stop processing an input when either one is finished. Thus meaning that it potentially avoids a full scan. The only downside that springs to mind would be if the calculation of the min/max range values itself might be expensive (but this should be very cheap if the table you are using as a filter is indexed on that column). I created two test tables 

It depends on the nature of the . A lot of times this is in fact implemented by creating a new column, copying the data across and marking the old column as dropped. Example: 

The definitive article on the topic is Arrays and Lists in SQL Server 2005 and Beyond which contains code and performance test results for a variety of split functions. You say these are column names? You should probably read another of Erland's articles on The Curse and Blessings of Dynamic SQL where he discusses both SQL injection and when dynamic SQL is a good idea. 

I'm not aware of any documentation. I did look into this and make some observations however that are too long for a comment. The 10% estimate is not always a degradation. Take the following example. 

Because this is valid SQL. First of all consider that it is valid to run a against the table that doesn't reference any columns from that table. 

You need to give the views a different owner than the tables so that the ownership chain is broken and the permissions are checked on the base objects. 

I noticed a relatively long running (20 min+) auto update statistics operation in a daily datawarehouse build. The table involved is 

BTW: There is a persistent belief that putting the most selective column first is required. This doesn't make any difference here. Either way SQL Server will be able to seek on both columns. Contrary to the StackOverflow answer linked in the comments there is no big difference in efficiency either way (I guess there might be a minor benefit in CPU time by checking the column most likely to differ first and thus potentially avoiding the need to do a comparison for the second key column in the composite key but even if true this would be a micro optimisation) Sometimes this will make sense for other queries in your workload though. E.g if there are queries against both A and B individually and the index wasn't covering for either of them but one of the columns was selective enough to make a plan with lookups worthwhile then putting the most selective first would make sense in that instance. 

No, this feature was put in to record the last time that ran successfully. It would be extremely unlikely that a feature would be added that allows this value to be edited at will and effectively lie about the history. The only way to update this that does not actually involve hacking the database boot page is to execute . So you should customise these scripts to not report that issue or ignore this result if it is entirely expected that this database isn't being checked due to alternate arrangements. 

And as there is an equi join predicate the plan also shows an equality predicate . As a result the cardinality estimate of rows is likely to be pretty accurate. For the / version when and are the same value then it simplifies OK to the same equality expression and gives the same plan but when and it only goes as far as 

The are lookups in to the index. The original scan or seek will have been on another non covering index. For example 

This isn't generally possible but is in the specific case in your question. Windowed functions are allowed only in steps 5.1 and 6 in the Logical Query Processing flow chart here (The and ). cannot be used to reduce the returned rows. In SQL Server 2008 can only do so in conjunction with (For 2012 can filter on the basis of ). As you are only interested in ones where the expression evaluates to in this case you can use 

Just addressing the isolation level aspect. Yes this will work but with deadlock risk. Two transactions will both be able to read the row concurrently. They will not block each other as they will either take an object lock or index locks dependant on table structure and these locks are compatible. But they will block each other when attempting to acquire the locks needed for the update (object lock or index respectively) which will lead to deadlock. The use of an explicit hint instead will serialize the reads thus avoiding the deadlock risk. 

SQL Server maintains statistics on substrings in string columns in the form of tries that are usable by the query but not by the . See the String Summary Statistics section for more about this. A couple of important caveats are that any escaping of wildcards must be done with the proprietary square bracketing technique rather than the keyword and that for strings longer than 80 characters only the first and last 40 characters are used. 

The above returns if supplied an invalid input for the datatype (including -ve numbers) and I would be minded to leave it that way. But if you must change these to you can wrap it in an 

A repro script is here. It simply creates a table with a clustered PK and a columnstore index and tries to update the PK stats with a low sample size. This does not use partitioning - showing that the partitioning aspect is not required. However the use of partitioning described above does make matters worse as switching out the partition and then switching it back in (even without any other changes) will increase the modification_counter by double the number of rows in the partition thus practically guaranteeing that the statistics will be considered stale and auto updated. I've tried adding a non clustered index to the table as indicated in KB2986627 (both filtered with no rows and then, when that failed, a non filtered NCI also with no effect). The repro did not show the problematic behaviour on build 11.0.6020.0 and after upgrading to SP3 the issue is now fixed. 

No SQL Server doesn't do any kind of automatic maintenance. Running multiple times will have no effect after the first successful run (ignoring pages modified since being reorganized or those it could not lock and so skipped on the first attempt). It swaps out of order pages into the correct order so once it has done that and they are all in the correct order additional runs will have no effect. It cannot remove fragmentation that arises due to non contiguous allocations. For that you need to rebuild. 

It only does that if you try and insert the new column between already existing ones. If you add it to the end of the table it will just do a simple 

Partition by Thing and find the minimum (alphabetically first) event in each partition. If it is rather than this Thing has no end event. 

Is costed at on my machine for my example table. If SQL Server was aware that the zoo had no black swans and that it would need to do a full scan rather than just reading 100,000 rows this plan would not be chosen. I've tried multi column stats on and and filtered stats on but none of these help convince it that black swans don't exist and the scan will need to process more than 100,000 rows. This is due to the "inclusion assumption" where SQL Server essentially assumes that if you are searching for something it probably exists. On 2008+ there is a documented trace flag 4138 that turns off row goals. The effect of this is that the plan is costed without the assumption that the will allow the child operators to terminate early without reading all matching rows. With this trace flag in place I naturally get the more optimal index intersection plan. 

But you should still keep the check constraint as a last line of defence so however the invalid value appears it is stopped. 

which means proprietary Access extensions like work when accessing the page from Firefox (but this will fail in Chrome) 

You have an index on that can be seeked into. This satisfies the range predicate on and also supplies the required ordering for your row numbering. However it does not cover the query and lookups are needed to return the missing columns. The plan estimates that there will be 30,000 such lookups. There may in fact be more as the predicate on may mean some rows are discarded after being looked up (though not in this case as you say COLUMNF always has a value of 1). If the row numbering plan was to use a clustered index scan it would need to be a full scan followed by a sort of all rows matching the predicate. is considerably cheaper than the so the plan with lookups is chosen. You say that the plan with lookups is in fact 5 times faster than the clustered index scan. Likely a much greater proportion of the clustered index needed to be read to find 30,000 matching rows than was assumed in the costings. You are on SQL Server 2014 SP1 CU5. On SQL Server 2014 SP2 the actual execution plan now has a new attribute Actual Rows Read which would tell you how many rows it did actually read. On previous versions you can use to see the same information.