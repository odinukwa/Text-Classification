It will take some time to run this, depending how many rows you move in each batch, but it keeps your table (which is now part of the view) online. You will generate a lot of logs, and it will require a fair amount of disk space, but that's how I'd do it. 

This is not true. Transaction log backups are incremental, that is, they only store incremental changes since the previous transaction log backup. You are confusing the differential log backup with "backups are not incremental", because differential backups only keep a record of the changes since the last full backup. (Differential backups work in all recovery models, namely Simple, Bulk-Logged, and Full.) If you want incremental backups, take transaction log backups regularly. 

With this structure, when you add a new company, you add it to the table, and then add a reference to it in the link table. So if you had 100 countries, you would have several hundred rows in this table. I can assure you that querying this will be far more efficient than a very wide table. When you want to query the data, you'll need to write an additional , but this makes managing the data so much easier, and less error-prone. Notice that I've added a column, which I would use for a two- or three-digit ISO standard (e.g. "USA", "CAN"). This means you could even use the code as a foreign key instead of the . 

The big thing about NoSQL is the concept of "eventual consistency" or "optimistic replication". Assuming nothing in the database is really dependent on the order of inserts, modifications or deletes, it vastly improves performance. After all, performance is a feature. 

Depends on who you ask. Is it in terms of new features, etc.? For example, a 32-bit RDBMS system could be considered obsolete nowadays, because 64-bit is common, and most engines run safely on a 64-bit platform, with access to more memory. Is it in terms of scaling? For example, an old design that doesn't scale well, might be considered obsolete. A database with queries that use old-style joins, could be considered obsolete, and in this case would prevent upgrades to newer technology without changing the code. Some wonks like to say that SQL itself is obsolete, that NoSQL technology is better suited to certain problem areas like unstructured data. I personally wouldn't say this is the right use of "obsolete", but that could be an example too. 

A PIVOT statement, given all its hard-coding, is a glorified CASE statement. It was easier to build out a UNION query with all the counters, and run CASE statements against the values. You can definitely convert this into dynamic SQL for future reference, but this is a good start. 

You will need to have the Full Recovery Model enabled on all four databases. Now you will issue a full backup on all four databases, let's say ten minutes before the point in time you require. Now you will do a transaction log backup of each database, at a point in time immediately after the exact moment you need. On the other end, you will restore each full backup , and then transaction log backup, with the switch to specify the exact moment in time you need, as a DATETIME. Remember to at the end to bring them into active use. 

I do not discuss typical maintenance tasks you should also be running on your instance, which includes , index and statistic maintenance, etc. Those are outside of the scope of this answer. 

Aside: if you're seeing high CPU as a result of a new OR/M, I'd look at something called implicit conversion. What happens is that on string data types (, , , ), you might be seeing Unicode vs ASCII conversions between the database and your application code. The best way to look for this is to make sure your ORM matches the string data types correctly. If it's in the database, make sure the ORM doesn't submit parameters as . More reading here: Convert Implicit and the related performance issues with SQL Server. 

You can safely attach any database from a prior build. As soon as you do this, the internal database structure will be automatically upgraded if there are any changes. This includes system databases, but they will not be considered system databases, using this method. You can still retrieve SQL Server Agent jobs from the old database if you need them, as well as anything you wanted from , including things like logins. 

In my output, the object was called and had an of . Now we can use a script similar to Ryan Cooper's answer, by plugging that into the clause, as the : 

At this point, I would need to store the textual old/new value of the company name, as well as the id of the recently added user.You might see that I'm already headed off in the wrong direction, and this is where I ask for help. I have 2 questions: Should I just use -datatype, or if this is considered poor design then what would be a sensible way to store these log-events? Thanks in advance. 

For the two former events, I'd store the type of event, the old value and the new value. This works fine as long as I stop here. However, if I want to log the 2 former-events, I would need to be able to store different data-types in the same column. The changes would look something like this: 

We have several tables which we "refresh" frequently by rebuilding them in a staging-table, then performing a metadata-switch to the production-table using the statement. These operations are causing a huge amount of logging, and we're not sure how to handle it. For now we just moved these tables and operations to a new database using the recovery model, but I would love to hear alternative solutions to this problem. How do you suggest we handle the massive log generation as a result of frequent table rebuilds? 

This example does not check whether any values has actually changed. If we ignore -checking and sane fallbacks for a moment, this could be checked in one of the following ways: 

Yet, the severity of the failed test is, as mentioned, considered high. Octopus Deploy While configuring the Octopus Deploy Server, the setup fails with a FATAL error during the initialization of the OctopusServer-instance. The article related to the error-message does not explain why this is a requirement, but simply states that it will be a requirement for future deployments, from and including Octopus version 3.8. As a side-note, RedGate's CI-tool package, the DLM Automation Suite, supports deployments with varying collations without complaints. The recommendation of keeping all column collations to the database default seems more like guidelines or best practices to me. Why is it considered such a serious error by some? 

I am attempting to construct TSQL queries to substitute various GUI tools provided by SQL Server Management Studio. One of these tools is the , accessible through the window. 

Our current database environment includes a cluster with 3 nodes(one primary and two read-only replicas), as well as a single, independent server standing next to the cluster. For brevity, I'll call the nodes N1, N2 and N3, and the independent server S1. Recently, we configured for our servers, using S1 as the MSX (master) server, and N1, N2 and N3 as TSX (target) servers. This means that SQL Agent Jobs that operate on the cluster-nodes are created and managed from S1. TSX servers report their state, outcome etc to the MSX server, which can be accessed using said . From the , the job-history of each enlisted server (TSX) can be accessed by selecting : 

There are two reasons that prompts me to ask this question: tSQLt The T-SQL testing framework tSQLt considers it an issue of "High Severity" when there exists columns with a non-default collation. The author of the test states the following: 

Question: In SQL Server 2016, does updating a column to the same value (e.g. updating a column from to ) produce the same amount of transaction-logging as when updating a column to a different value? Read below for more details. We have several SQL Agent jobs running on a schedule. These jobs select data from a source-table (replicated data, linked servers), transform it, then insert/update/deletes the rows of the local target-table accordingly. We have been through various strategies while trying to find the best way to achieve this. We've tried 

In Example #1, the SET-operation will update all columns, even if only 1 column has changed. In Example #2, the SET-operation will update all columns for all rows, falling back to the old value if the value is unchanged. In both examples, all columns are hit by the SET-operation, and, according to my seniors, this creates an unnecessary/problematic amount of transaction-logging when done frequently. The same applies for the -statement. Even if you check a matched row for changes, all columns are hit by the update. 

Now, say I want to log different types of events on the -object. I'd make a log-table called and and store events there. These tables would look like this: 

It feels like there must be a smarter way around this, and I would appreciate any clarification and corrections to the statements made in this post. Like mentioned earlier, I'm just trying my best to understand why it has to be this way. Apologies for the lengthy post and thanks in advance. 

I don't think you're doing what you want to do. It looks like you're striping your backups across two paths, which means your backup is evenly split between the two locations. I suspect that your actual backup size is not 3.5MB, but rather 7MB. The difference in size is most likely due to the local storage being quicker. Per Ola Hallengren's documentation: 

Once you've done all that and monitored it, start focusing on performance issues using an index strategy. There are DMVs built into SQL Server you can use to query missing indexes (or sp_BlitzIndex from Brent Ozar Unlimited). Using appropriate human judgment (i.e. don't madly create every index it recommends, look for ways to create covering indexes), you should find some improvements. Once again, monitor performance against your current baseline. The reason we are going to all this trouble before answering the cloud question, is that a poorly performing system on-premises will also perform poorly in the cloud. This will result in you paying more than you need to. Remote access Your main motivation for moving to the cloud is to have remote access to your database. SQL Server running on a Windows (or Linux) VM By all means, check out SQL Server running on a virtual machine. Bear in mind that you'll pay for Windows (where applicable), SQL Server, and any data drives you assign to the VM. With a 1 TB database, you'll need at least two P30 premium storage drives, which isn't cheap. Then the size of the VM must also be taken into account, because SQL Server is licensed by number of CPU cores. Platform as a Service (Azure SQL Database) 

To force the behaviour you're trying to compare, we have to do this construction (notice the additional and extra brackets): 

Short answer: yes it can help, because it's theoretically instantaneous. You would insert your data into a staging table with the same definition as your main partitioned table, and then switch it into the partitioned table, which is a metadata operation (schema lock). Long answer: it might make performance suffer over the long run to have such a large table, and unless you move to later versions of SQL Server, statistics updates are quite difficult to manage. If this is a read-heavy table, I'd consider using view partitioning instead. Each month (for example) would get its own table, with check constraints on a date column to help the query optimizer know where the data is physically stored, and a view over the top of that: 

Assuming the values for your initial query result are 1 and 2, you have two values returned back, which you can now plug into a clause. You would use this to search for the number of times T3ID appears where it has two matches. This code is written in T-SQL, but I'm sure you can convert it quite easily to Access, as it's for demonstration purposes anyway. 

I'd stick with . An takes up 4 bytes of space. On 500,000 rows, that works out to 2 million bytes, or just under 2MB per day of overhead (which is practically nothing these days). Double that for , which is 8 bytes in size, so 500,000 rows will need under 4MB of space. We generally recommend if you're going to add (or change) over 2 billion rows over the lifetime of the table or index. While supports a range of 4 billion values from roughly -2 billion to 2 billion, people generally start their value at 0 or 1, cutting out half of the available range. In a case where you're doing 500,000 a day, a might be better for you. It's a good trade-off between storage and data churn in the medium to long term. 

To normalize this properly, you need what is sometimes called a cross-reference or link table, to avoid making that country table so wide. For example (this is for demonstration purposes only): 

While this is possible to do with the Dedicated Admin Connection and a modified version of , it is unsupported. I would rather spend this time to restore properly on a new instance (on Developer Edition under a VM makes the most sense), and then access it from there. 

This comprises an ordered set of backup media, which can be tape, disk files, Azure Blob Storage, etc., but (and this is important) not a combination of two or more of these. So if you create a media set, your backup media in that media set must be of the same kind. On to your question about the name of the media set. According to the same article (emphasis added): 

Ultimately, this all depends on what the business needs to satisfy service level agreements internally and externally. If a reporting / OLAP style database can be regenerated in two hours from an OLTP source, then you don't need to worry as much about it as the OLTP database. I hope this helps you come up with an appropriate strategy for your business. That's what should drive these decisions.