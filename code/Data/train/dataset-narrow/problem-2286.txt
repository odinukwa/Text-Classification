If S is part of the input, then the problem of finding a maximal element already becomes ``NP-hard'' (if we think of the lattice such that its elements are n bit long strings), e.g. you can say $x<y$ if CNF(x) is not true and CNF(y) is true for some fixed CNF. Also, there might be many maximal elements satisfying P, so even to output all of them might take a long time, so I think there is only hope to find one maximal. In general, binary search works if you can recursively select elements such that after you are left either with the above elements, or the above elements are deleted, and in every such sets a fixed ratio of the elements are deleted. Eg. if S is a fixed dimensional grid, then there is a fast algorithm: Always halve one coordinate while keeping the others minimal, so ask e.g. in the first step (n/2,0,...,0). One important related theorem is the Tarski fixed point theorem, where instead of P you have a monotone mapping from a lattice to itself. The theorem says that the fixed points form a lattice. We proved with Jaroslaw Byrka and Paul Duetting that in this setting if the lattice is a d-dimensional grid, then you can find a fixed point in about $n^d$ time where the algorithm is a simple generalization of binary search. 

There is no algorithm that always beats all others in an IPD tournament. Proof: if your algorithm cooperates in round 1, then it gets last place in a tournament where all other bots always defect; if your algorithm defects in round 1, then it gets last place in a tournament where all other bots play unforgiving tit-for-tat (cooperate until the opponent defects, then defect forever). If there are a finite number of rounds in each face-off of an IPD tournament, the sole Nash Equilibrium - evolutionarily stable or otherwise - is that all bots always defect. If instead each face-off has a constant probability of ending after each round (for example), then there might be many evolutionarily stable NEs, depending on how the payoff structure is organized. One such ESNE is that all bots play tit-for-tat. Evolutionary Stability is a pretty important game theory concept, though, and it sounds like that's the magic phrase you want to google. My back-of-the-envelope intuition for it is this: a ball rolling along $f(x) = x^3$ is unlikely to come to a rest at $x = 0$, even though the first derivative is zero. Nash Equilibrium is a first-order equilibrium concept that lists $x=0$ as an equilibrium of this system, but Evolutionary Stability is a second-order equilibrium concept that throws out $x = 0$ because if you poke the system, the equilibrium breaks. 

I am wondering what the exact (including hidden constants) worst case number of subexpressions in an arbitrary boolean expression with $n$ literals (not necessarily distinct) is. For simplicity it is probably ok to consider boolean expressions with just AND and OR gates, since extending it to, for example, XOR will not affect this number I assume. To clarify what I mean, here is a small example: $$(A \land B) \lor (C \land D)$$ has the subexpressions $(A \land B)$, $(C \land D)$, $(A \land B) \lor (C \land D)$. I purposely ignored subexpressions, which are just composed of bare literals. 

Assuming certain the hardness of certain computational problems, I think you should be able to do this in a more or less constant amount of space when you consider multisets. You can simply use a homomorphic collision resistant hash function and keep one hash digest for each stream. Everytime you see a new element, you compute its hash value, and add it to the digest for that stream (using the homomorphic properties). If the streams have been the same, so will the hash values. If the streams have not been the same, then the hash values will be different with an overwhelming probability due to the collision resistance of the hash function, since otherwise the concatenation of both streams would constitute a pair of strings that provides a collision for the hash function. Such homomorphic hash functions can be instantiated from different widely believed computational hardness assumptions. For example, see Streaming Authenticated Data Structures (Section 3), for a lattice-based homomorphic hash function or Chameleon Hashing and Signatures for constructions based on the discrete logarithm and the factoring problem. The only thing that slowly grows here is the hash functions output length, which depends on the length of the stream, but if your streams are poly, then you can fix it to some security/confidence parameter like 80, that is, for any two polynomially (possibly adversarially) chosen streams, the probability of a false positive is negligible in that parameter. 

If a node has fewer than $n^{1/3}$ neighbors, then add all edges incident on that node. From your remaining nodes, greedily select the remaining node with the most neighbors. Declare this to be a "cluster center," and then temporarily set aside this node and all its neighbors. Repeat until you're out of nodes. Let $C$ be the final set of cluster centers. There is a proof in the paper mentioned above that guarantees that $C$ is only $O(n^{2/3})$ big. Add to $H$ the edge from each cluster center to each of its neighbors. For each $c, c' \in C$, add to $H$ a single weighted edge from $c$ to $c'$ of weight $\delta_G(c, c')$. 

This is intended as a comment, but it's too long to post as a comment. You might also be interested in graph spanners or emulators for your purposes. A spanner of a graph $G = (V, E)$ is a subgraph $H = (V, E')$ with few edges, but approximately preserved distances. An emulator is a graph $H = (V, E', w)$ whose edges are allowed to be weighted. The best result for spanners is $O(n^{4/3})$ edges and an additive error of +6 on distance estimates in the graph. The best result for emulators is $O(n^{4/3})$ edges and an additive error of +4. It is not known for either if we can beat $O(n^{4/3})$, even if the error is allowed to be polylogarithmic. If this sounds useful, I can try and dig up the relevant constructions for you. 

You can do $O(\log \frac n\epsilon)$ space if you only want an approximation. The main idea is that you use the random hash function $h$ to do the same protocol as in the Goldwasser-Sipser Set Lowerbound (which you can find, e.g., in the Arora-Barak book). So you pick a target $y$ and observe whether $h(x_i)$ holds for any $i$. If the sequence is diverse, then whp. you can find such a $i$, otherwise you cannot. To increase the probability of success, you can pick $O(\log \frac 1\epsilon)$ different $y$'s of length $O(\log n)$ and do the protocol for each. If you want an exact answer, then more precise calculations are needed. I think $O(polylog \frac n\epsilon)$ space might be still enough, but I might be wrong. 

Any natural number can be regarded as a bit sequence, so inputting a natural number is the same as inputting a 0-1 sequence, so NP-complete problems with natural inputs obviously exist. But are there any natural problems, i.e. ones that do not use some encoding and special interpretation of the digits? For example "Is n a prime?" is such a natural problem, but this one is in P. Or "Who wins the Nim game with heaps of size 3, 5, n, n?" is another problem that I consider natural, but we also know this to be in P. I am also interested in other complexity classes instead of NP. Update: As pointed out by Emil Jeřábek, given $a,b,c\in \mathbb N,$ to determine whether $ax^2+by-c=0$ has a solution over the naturals is NP-complete. This is exactly what I had in mind as natural, except that here the input is three numbers instead of just one. Update 2: And after more than four years waiting, Dan Brumleve has given a "better" solution - note that it's still not complete because of the randomized reduction. 

I'm told that, based on some implementation details of the proof of this theorem, this can be viewed as a continuous analogue of the claim that $\exists k \, \, \, P \subset SIZE(n^k)$. Sorry I'm not qualified to be more precise than this -- if anyone else has heard this idea, maybe they could help me out. 

I'm nowhere near as knowledgeable about this topic as Stasys, but I heard a different justification for this conjecture which I might as well share. I heard that the conjecture was based on the positive solution to Hilbert's Thirteenth Problem, which was jointly resolved by Komolgorov and his student Arnold. The theorem (which is much more general than Hilbert's stated problem) says: 

Surprisingly, for the +2 and +6 spanners, the relevant algorithm is simply while $H$ is not a $+k$ spanner, find a pair of nodes that violates the condition and add its path to $H$. The algorithm is simple, but the argument behind it is somewhat nasty. If you're interested, read Knudsen. Hope that helps! 

I am not sure I understand your question. Let $L_M(k)=\{(x,i,t)$ for which on input $x$ we have that the $k$-th bit of $z_{i,t}$ is $1$. This language is in EXP, thus also in P/poly. Putting together these circuits for different values of $k$, we get the multi-output circuit. 

As far as I know, this is still open. A very recent paper that mentions these quantities and some bounds is Aaronson et al: Weak parity (see $URL$ You can also see chapter 14 of Jukna: Boolean funcions and the 1999 (still beats 1998!) survey by Buhrman and de Wolf. Another very recent paper about randomized decision tree complexity is Magniez et al: $URL$ Finally, a short summary I made for myself last month (without defs): R2<=R0<=D<=n D<=N0*N1<=C^2<=R0^2 s<=bs<=C<=s*bs<=bs^2 (new: [Gilmer-Saks-Srinivasan]: there is f s.t. bs^2(f)=O(C(f))) D<=N1*bs<=bs^3<=(3R2)^3 deg<=D<=bs*deg<=deg^3 (new: [Tal]: bs<=deg^2) D<=N1*deg C<=bs*deg^2<=deg^4 Sensitivity conjecture is that s is also polynomially related to other parameters. 

If I understand your question correctly, by path you mean that we build up a tree starting from s by always taking an edge adjacent to our current tree. This problem is NP-complete, the reduction is from SAT. The main part of the graph will consist of n cycles of length four attached to each other at vertices s=v0, v1, v2, .., vn. So from s we have two paths of length two to v1, from v1 two paths to v2 and so on. From vn there will be a path of length N+1 to t. We will have only enough weights to use one of each pair of paths and depending on which path we use, the value of the variable xi will be true or false. The weight in s is N+2n, where N is some big number. Each clause is represented by an additional vertex that is connected by a path of length N to the center vertex of the path that corresponds to its literals and it has value N+epsilon (I know we need integers, but we can multiply up and replace each edge by 1/epsilon edges), where 1/epsilon is the number of clauses. So we have to visit every clause-vertex to get to t. 

The class $PR$ (primitive recursive functions) strictly contains the class $ELEMENTARY = TIME(2^n) \cup TIME(2^{2^n}) \cup TIME(2^{2^{2^n}}) \cup \dots$. You say "Perhaps NP-hard problems are solvable with primitive recursion but not as efficiently." This is true. Even the total recursive function model of computation does not carry a notion of time complexity that agrees with that of Turing Machines. To see a simple example, how do you define the predecessor function in primitive recursive terms? You'd have to do something like: use the unbounded search operator to find the smallest $y$ such that $S(y) = x$. That is analogous to computing $f(x) = x-1$ with the function: $i = 0$ $While \, (i + 1 \ne x) \, \, \, \, i++$ Which takes $2^n$ time. Obviously, there is a better way! 

Some of the very early work on complexity theory used a sequential time model -- that is, rather than studying the worst-case runtime of the TM that can produce the correct output on an arbitrary input, they studied machines that would run infinitely and enumerate the correct output for each input in lexicographic order. The complexity of the machine was then based on the worst-case time gap ("delay") between the enumeration of consecutive outputs. This model can be used to study the problem of taking an input $1^n$ and producing on output the $2^n$-sized truth table of a language on all inputs of length $n$ , while trying to minimize the average computation time required per input (so $2^n poly(n)$ is considered "efficient" in this model). This seems pretty similar to the question you're asking. Here is a paper that uses that model. Here is a blog post that is only somewhat related, but includes some references that you might find interesting. One note about this model is that for some NP problems, including SAT, you can print their truth table in polynomial time per bit by exploiting the self-reducibility of the problem. For example, with SAT, you can always efficiently find the next bit of the truth table by fixing one of the variables, computing the reduced version of the problem under this variable fix, and then looking up the solution to the reduced version of the problem in the truth table that you have computed so far. 

I'm trying to understand a basic randomized response mechanism for differential privacy (concrete definition not relevant for the question), but I have some trouble understanding the last step in the calculations. The randomized response mechanism works as follows: Let's call $q(x) = \frac{1}{n} \sum_{i=1}^{n}q(x_i)$ a counting query for a database $x$ with $n$ entries $x_i$. Each $q(x_i)$ maps into ${0,1}$. Now define a mechanism $M$ that works as follows: $M(x_1, \dots, x_n) = (y_1, \dots, y_n)$, where each $y_i$ is computed as follows: $ y_i = \begin{array}{cc} \{ & \begin{array}{cc} q(x_i) & \textrm{with probability } \frac{1+\epsilon}{2} \\ \neg q(x_i) & \textrm{with probability } \frac{1-\epsilon}{2} \end{array} \end{array} $ Now we can use the value of $M$ to estimate the value of the counting query $q(x)$. The expected value of $y_i$ is $\mathbb{E}[y_i] = \epsilon q(x_i) + \frac{1-\epsilon}{2}$. Now the bit that I do not understand By Chernoff bound, $$ \left| \frac{1}{n}\sum_i\frac{1}{\epsilon} \cdot (y_i - \frac{(1-\epsilon)}{2}) - q(x) \right| \lt \mathcal{O}\left(\frac{1}{\sqrt{n}\cdot\epsilon}\right) $$ I assume they used the additive Chernoff bound $ \Pr[X \gt \mu + \epsilon] \leq e^{-2n\epsilon^2}$. It's not clear to me how they got the last part, since it's also not exactly clear to me how they apply the Chernoff bound. In particular how they use the expected value of $y_i$ in combination with $x_i$ and the bound. Could somebody maybe give me a more detailed calculation of how the last statement was calculated? Thanks in advance!