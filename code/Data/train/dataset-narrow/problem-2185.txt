I would recommend investigating the field of Finite Model Theory and more particularly its sub-field Descriptive Complexity. It can be used to model such sorts of problems. 

It is is worth thinking about WHY intuistionistic logic is the natural logic for computation, since all too often people get lost in the technical details and fail to grasp the essence of the issue. Very simply, classical logic is a logic of perfect information: all statements within the system are assumed to be known or knowable as unambiguously true or false. Intuistionistic logic, on the other hand, has room for statements with unknown and unknowable truth values. This is essential for computation, since, thanks to the undecidability of termination in the general case, it will not always be certain what the truth value of some statements will be, or even whether or not a truth value can ever be assigned to certain statements. Beyond this, it turns out that even in strongly normalizing environments, where termination is always guaranteed, classical logic is still problematic, since double negation elimination $\neg\neg P \implies P$ ultimately boils down to being able to pull a value "out of thin air" rather than directly computing it. In my opinion, these "semantic" reasons are a much more important motivation for the use of intuistionistic logic for computation than any other technical reasons one could marshal. 

CoC is most likely the way to go. Just dive into Coq and work through a nice tutorial like Software Foundations (which Pierce of TaPL and ATTaPL is involved in). Once you get a feel for the practical aspects of the dependent typing, go back to the theoretical sources: they'll make a lot more sense then. Your list of features sounds basically correct, but seeing how they play out in practice is worth a thousand feature points. (Another, slightly more advanced tutorial is Adam Chlipala's Certified Programming with Dependent Types) 

I'm wondering if anyone knows of a formalization (even limited) of any part of finite model theory in any of the major proof assistants. (I'm most familiar with Coq, but Isabelle, Agda, etc. would acceptable.) Especially of interest would be any of the results in descriptive complexity. 

I think this a misanalysis of the "co" prefix in this case. "Coroutine" is "co" in the sense of "co-worker"; something that works together with another. The term precedes by a long way the gross overuse for programming concepts of the prefix "co" in the Category Theoretic sense of a dual of another concept. (Yes, there is editorial content there. ;-) ) 

Just because you can always embed a lower-order type into the higher-order universe doesn't imply that the reverse is true. The higher-order universe is always strictly larger than the lower-order one, so there is no problem with respect to predicativity. 

I don't know of any work that pursues this line, but a few moments thought about it led me to this hypothesis: wouldn't the "root" of the exponential type just be the codomain, and the "logarithm" of the exponential just the domain? 

A fairly comprehensive discussion of this stuff can be found in this book: Lectures on the Curry-Howard Isomorphism. This is based on the freely available older version: Lectures on the Curry-Howard Isomorphism. 

That definition implies that there is no required unique bottom in a predomain, i.e. each down set (reverse chain) could have a distinct least element, but that there is no necessary least element for the whole structure. You could form a domain by appending a bottom under all other least elements, or by identifying all least elements (i.e. say they are all the same element) if that is semantically viable in your structure. 

${\Sigma_2^P}^{NP}$ is the set of language decided by an alternating turing machine in existential, and then universal state, with an oracle in NP. Both the universal and the existantial part can querye NP. Hence, in this case you decided to write this as $(NP^{NP})^{A}$ then the way you should think of it is as $(NP^{NP^A\cup A})$ (by $\cup$ I mean an oracle either to $A$ or to an $NP^A$ language). Hence ${\Sigma_2^P}^{NP}$ is equal to $(NP^{(NP^{NP})})^{NP}$ which is certainly equal to $(NP^{NP^{NP}})$ since every query you could make to the $NP$ oracle, you could make it to the $NP^{NP}$ oracle. 

NSPACE(0)P=RE wich I guess is tad bit absurd. Indeed, let L be a language recursively enumerable, M a TM who recognise L and M′ a TM that read an input and a number n of "1" and then simulates M for this input on n steps. Then without using any space I could copy the input on the oracle tape, guess the number of 1 needed and query M′. Then, M' will accept iff M accept and have an input big enough to be polynomial. 

Usually I give the factoring problem as example; I first ask for the number that divide 15; usually people can answer 3, 5, and have fun wondering if 1 and 15 are correct answer. Then I give a huge number (more than 10 digits) and ask if they can tell me what are the dividers; and I explain that, even for computer scientist, this is a really hard question. Then if I have time, I try to explain that the question is either to figure out how to solve this problem, or to prove that it will always take a lot of time( a notion that we precisely know how to define). And then a little word of cryptography, to explain why it is usedd, and a word about how many time it take team of scientist to break the key of number with hundreds of digit (I avoid to speak of bits because people seems to better know what a digit is) 

Finally, I will introduce the real structure I'm interested in, which is more complicated and probably less usual. A partial preorder $P$ can be seen as partial function from $[1,r]^2$ to $\{<,>,=\}$, where $i=_P j$ if $i<j$ and $j<i$ in $P$ and $f(i,j)$ is undefined if neither $i<j$ nor $i>j$ in $P$. We can associate to $P$ a subset of $\mathbb N^r$ called $\mathbb N_P$ defined by $(x_1,\dots,x_r)\in \mathbb N_P$ if for all $i<_Pj$ $x_i<x_j$ and for all $i=_Pj$, $x_i=x_j$. The real data structure I must study is the structure of partial function from $[1,r]^2$ to $\{<,\le,>,\ge,=,\not=\}$. I will call this structure "extended preorder". Let $P$ be an extended preorder, where $(x_1,\dots,x_r)\in \mathbb N_P$ if for all $i<_Pj$ $x_i<x_j$ and for all $i=_Pj$, $x_i=x_j$, for all $i\le j$, $x_i\le x_j$, and for all $i\not_P=j$, $x_i\not=x_j$. I say that a total preorder $T$ is included in an extended preorder $P$ if $i\le_Pj$ implies $i<_Tj$ or $i=_Tj$, and $i\not=_Pj$ imply $i<_Tj$ or $j<_Ti$. The operation I must have are still the same, having a set $S$ of extended preorder, verify if an extended preorder is incompatible with every extended preorder of the set, and verify that every total preorder is included in an extended preorder of the set. Or to say it another way, $(\mathbb N_P)_{P\in S}$ is a partition of $\mathbb N^r$. 

To answer to your comment, I guess I should make another answer, speaking only on Krom and Horn (May be I should ask a question about those to CSTheory) I suggest that you read section 5.3 page 34 of my paper about the problem I met on Horn and Krom in High Order logic. You will meet the same problem in Variable Order (which is clearly a superset of High Order). I don't know if you did pay attention to it, but SO(krom) is equal to P when the first order is universal; indeed you can express NP-complete problem if you add existantial first order variable. (I don't remember the example I had before, I can try to search it if you want it) I don't know what this syntactical resctriction would become for high order or variable order logic... my point is just that you should also think of a good way to restrain quantifiers, because restraining the quantifier-free part alone is not usefull (at least for Krom formulae) 

If you are having trouble with the concept of least fixed point, I would recommend spending some time getting a background in more general order theory. Davey and Priestley, Introduction to Lattices and Order is a good intro. To see why the transitive closure is the least fixed point, imagine building up the closure from an empty set, applying the logical formula one step at a time. The least fixed point arrives when you can't add any new edges using the formula. The requirement that the formula be positive ensures that the process is monotonic, i.e. that it grows at each step. If you had a negative subformula, you could have the case where on some steps the set of edges would decrease, and this could lead to a non-terminating oscillation up and down, rather than a convergence to the LFP. 

I find that the most "natural" way to get an intuition of complexity classes is to revert to Turing's starting point and try to solve the problem "manually". Start with a sorting task. From a jumble of, say, five words have the class order them alphabetically. This should be easy. Then double the number of words, and repeat the exercise. It will be obvious that, though the second problem is harder, it isn't that much harder. Next try a traveling salesman task. Start with a grid of say three cities with distances between them. The class will probably be able to solve this in short order. Now double the number of cities to six, and continue with the exercise until everyone's head is spinning. An experience like this is very likely to leave a lasting visceral impression that a purely technical introduction may not. 

Let me offer the simple, intuitive way that I think about this. If you restrict yourself to closed lambda expressions, you have an equivalent of the combinatory logic. In fact with just a few simple closed lambda expressions you can generate all the others. Closed lambda expressions give you the equivalent of implications where any conclusion/output you reach is either something you put in as an input, or something that you built by combining your inputs (in the general case, possibly recursively). This means that you can't pull a result "out of thin air" the way you can with non-constructive logics / mathematics. The only tricky bit left is how you handle negation / non-termination, which is a whole area by itself, but hopefully I've already given you the simple, but deep, correspondence between the three that you are asking for. 

This started as a comment under Andrej Bauer's answer, but it got too big. I think an obvious definition of ambiguity from a Finite Model Theory point of view would be: $ambiguous(\phi) \implies \exists M_1,M_2 | M_1 \vDash \phi \wedge M_2 \vDash \phi \wedge M_1 \vDash \psi \wedge M_2 \nvDash \psi$ In words, there exist distinct models of your grammar encoded as a formula $\phi$ that can be distinguished by some formula $\psi$, perhaps a sub-formula of $\phi$. You can connect this to Andrej's response about proofs through Descriptive Complexity. The combination of the existence of an encoding of a particular model plus its acceptance by an appropriate TM as a model of a given formula IS a proof that the axioms and inferences (and hence an equivalent grammar) encoded in that formula are consistent. To make this fully compatible with Andrej's answer, you would have to say that the model is "generated" by the formula acting as a filter on the space of all possible finite models (or something like that), with the encoding and action of filtering on the input model as the "proof". The distinct proofs then witness the ambiguity. This may not be a popular sentiment, but I tend to think of finite model theory and proof theory as the same thing seen from different angles. ;-) 

If I understand correctly, you just want a definition of the logic; then you can find a definition of MSO$_j$ (where 2 is a special case) by example in ''Elements of Finite Model Theory'' of Leonid Libkin. Even if you are interested in infinite model, the definition of the logic is the same. Quickly is just the set of formulae with existantial quantification over sets of the elements of the universe, then existantial quantifications over those sets; then a first-order formula. 

I have a question that seems to me really natural and have probably already been studied. But keyword search on this site or google does not seems to help me to find any relevent paper. I have got a finite non deterministic automaton $A$ over an alphabet $\alpha$ without epsilon-transition. What can I tell about the number of different path the automaton could take for accepting a word ? In particular, I want to know if this number is bounded, or if for every $c$ I can find a word $w_c$ that is accepted in at least $c$ different way by the automaton. Right now, I can find some necessary, and some sufficient condition, but not any necessary and sufficient condition, for the number to be unbounded By clarity, I'll define the way I cound the number of accepting path. Let $w\in\alpha^*$ and $q$ a state, I can define the number of path to $q$ by inuction on $|w|$ by $N(\epsilon,q)=1$ if $q\in I$ else $0$, where $I$ is the set of initial state and $F$ of final state. $N(ws,q)=\sum_{q'\in Q\atop \delta(q',s)=q}N(w,q')$. Then the number of path is $\sum_{q \in F}N(w,q)$. 

I guess that notions I describe are already well known, may be by combinatorician, but I do not know their name or any book/article about them. So if you have a link/title I would love to read it. Let $r$ be an integer, let $P_r$ be the set of partial (pre)order over $[1,r]$ and $T_r$ the set of total (pre)order. I say that $P\in P_r$ is included in $T\in T_r$ if it is included as subset of $[1,r]^2$, or to state it another way, if for all $i,j$ with $i$ less than $j$ for $P$ then it is also less for $T$. Finally I say that $P$ and $P'$ are incompatible if there is $i,j$ with $i<j$ for $P$ and $i>j$ for $P'$ (or if $i=j$ for $P$ and $i<j$ for $P'$). Let $P\in P_r$ with $i<j$ and no $k$ such that $i<k<j$, then $P_{i,j}$ is the same partial (pre)order, except that $i$ and $j$ are incomparable. I would like to find an efficient data structure to store a subset of $P_r$. I can't imagine something better than a trie of depth $r(r-1)/2$, with one level for every pair $(i,j)$ with $i<j$. If possible I would want to be able to efficiently add and remove elements from the set, or at least to easily transform $P$ into $P_{i,j}$ as defined above. I need to know if for a given subset $S$ of $P_r$ and $P\in P_r$, $P$ is incompatible with every $P'\in S$. Or an equivalent problem would be to figure out if a set $S$ is such that its element are one to one incomparable. I also need to know if for every total (pre)order $T$ it is a superset of a partial order $P\in S$. Intuitively, to each $P\in P_r$ is associate its set $T_P=\{T\in T_r\mid T\supseteq P\}$ and I need to know if $(T_P)_{P\in S}$ form a partition of $T_r$. Then let $T_S=\bigcup_{P\in S} T_P$, then I would also be interested by having a way to efficiently describe $T_S$. (That is, efficiently checking for a given $T\in T_r$ if $T\in T_S$.