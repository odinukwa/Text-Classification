I think you'll find that the exact same proof of Lemma 3 in [1] (the proof itself appears in [2]) concerning $\rightarrow^\infty_\beta$ also holds for $\rightarrow^\infty_{\beta\bot}$: indeed, they are defined in the same way from $\rightarrow^*_\beta$ and $\rightarrow^*_{\beta\bot}$ respectively, which are transitive by definition! The lemma holds for an arbitrary reflexive-transitive congruence relation in place of $\rightarrow^*_{\beta\bot}$, as you can verify in [2] without needing to go into the detail of the coinductive proofs. In particular, the proofs of lemmas 4.1-5 in [2] are unchanged (with extra induction cases in 4.4). 

To add tho the already good answers, there is a huge field of research called Termination Analysis which deals with methods to analyze the termination of various computation formalisms. Of particular interest are Term Rewrite Systems which seem to capture quite well the termination problems of functional programs. I can't even begin to outline the various lines of work involved in this vast field. My personal work has been related to size types (see e.g. Barthe et al.), which integrates quite well with the STLC. 

The beauty is that you can iterate this as many times as you want if your function takes more arguments. For example $\mathrm{rem\_mod}$ that computes the remainder of a number $n$ divided by $m$ modulo $r$ of type 

Now it's pretty clear that this theorem is a consequence of the fix-point theorem in the $\lambda$-calculus. We can use this to prove a variant of the logical-fixed point theorem: 

The answer can be understood categorically through the lens of F-algebras. The categorical representation of a recursive type $I$ in a category $\cal C$ can roughly be specified using a functor $F:{\cal C}\rightarrow{\cal C}$. One then works in the category of $F$-algebras with 

Applying higher homotopy-theoretic ideas to CS is still a very nascent field! My understanding is that it's not even that old as a mathematical field. Certainly HoTT is the central impetus for such ideas. Even there though, there only have been few applications of category theory of "dimension" higher than 2. One nice "computer science-y" one is Homotopical Patch Theory by Anguili et al. They show that some common operations and properties inherent like version control systems can be best understood using homotopy type theory. Another rather unrelated train of thought is some interesting work on the relationship between (2-)Homology theory and confluence of term rewrite systems (or more complex structures such as higher algebras). Some examples are Y. Guiraud Confluence of linear rewriting and homology of algebras . Y. Lafont & A. Proute Church-Rosser property and homology of monoids. 

In general, people always care about floating point errors. However I disagree with Andrej, and I do not think that floats are preferred to arbitrary precision reals (for the most part) because of sociological reasons. I believe the main argument against exact computation of reals is one of performance. So the short answer is, whenever performance is more important than precision, you'll want to use floating point numbers. The application that springs to mind is the use of computational fluid dynamics to design the aerodynamics of cars or planes, where small errors in computation are easily made up with the astronomical gains of using dedicated floating point units found in many widespread processors. In particular, the problem of representing a wide range of real numbers using a fixed number of bits is not as trivial as it may seem at first glance. In numerical simulation, values may vary widely (e.g. when there is turbulence), so fixed-point computations aren't appropriate. Even when precision is not fixed by the hardware, using arbitrary precision numbers can be several orders of magnitude slower than using floating point numbers. In fact, even in the nice case were all the numbers are rational, simple operations like inverting a matrix can result in large, hard to control denominators (see here for an example). Many large linear optimization packages use floating points with appropriate rounding modes to find approximate solutions because of this exact issue (see for example, the majority of programs found here). 

As it just so happens, there have been recent development in the theory of dependent types, in which a types, which traditionally represent a static invariant for a computer program, can be interpreted to be a topological space, or rather an equivalence class of such spaces (a homotopy type). This has been the subject of intense research over the last few years, which culminated in a book. Older work has attempted to give a description of models of computation systems, like the pure $\lambda$-calculus, in terms of certain topological spaces called domains. The Wikipedia article gives a good overview. 

The function is sound, in the sense that it returns only for functions which are $\beta\eta$ equal to the $\bf{zero}$ function. The function is only complete modulo termination: there is no finite method to determine for an arbitrary whether is going to return , or simply run forever. This is where undecidability rears its head. However if it returns false, then (by Böhm's theorem) is not $\beta\eta$ equal to $\bf{zero}$. 

Taking $m=2$, this seems to give at least an upper bound on what you want, and I suspect it's not far from tight, as you have almost full Presburger atomic formulas "at the root". 

In general, what we usually call the logical relations argument isn't really linked to impredicativity: the main idea is simply to interpret terms in some abstract algebra $\cal A$, and to represent types as a ($n$-ary) relation $R \subseteq \cal A^n$. This works perfectly fine for all kinds of type theories, including dependently typed theories, see e.g. Shürmann and Sarnat: Structural Logical Relations where a predicative logic (that of Twelf) is used to prove a certain property (decidability of equality) for a predicative calculus (simply typed $\lambda$-calculus) using logical relations. As you may have suspected, however, it is not possible to prove normalization of system F in Agda (if Agda isn't secretly stronger than expected, i.e. about the strength of Martin-Löf type theory with a bunch of universes). This is because normalization of system F implies consistency of 2nd order arithmetic ($\mathrm{PA}_2$) which is stronger than M-L type theory with any number of (predicative) universes. It's instructive to work out exactly where the proof goes wrong in Agda, though. It indeed occurs when you try to define the logical relations interpretation of the impredicative quantification. The interpretation of the non-impredicative connectives though (including "dependent" quantification) is kosher in a theory like Agda. 

Regarding question 1): the variable $Y$ must not appear in $Y$, indeed it needs to be unconstrained. If you want to have any hope of the lhs being equal to the rhs, certainly you should have the same number of free variables on each side, which is impossible if $Y$ gets captured in $T$. The intuition is that $\exists T$ should be equal to the infinite conjunction $$ ((\forall x. T\rightarrow U_0)\rightarrow U_0)\wedge((\forall x. T\rightarrow U_1)\rightarrow U_1)\wedge\ldots $$ for every possible type $U_i$. Indeed, it is not hard to show for any specific type $U$ that $$\exists x.T \rightarrow ((\forall x. T\rightarrow U)\rightarrow U)$$ In words: 

There is a vast literature on how to do this properly. The main approach on how to handle $\alpha$-equivalence that I know of is Nominal Logic which instead of re-naming, advocates "swapping" which is better behaved. The other (non)-solution is to do away with variable names altogether and simply have pointers from variable positions to $\lambda$s, as seen in the variants of the de Bruijn index method. 

This is an interesting question! As Anthony's answer suggests, one can use the usual approaches to compiling a non-dependent functional language, provided you already have an interpreter to evaluate terms for type-checking. This is the approach taken by Edwin Brady. Now this is conceptually simpler, but it does lose the speed advantages of compilation when performing type checking. This has been addressed in several manners. First, one can implement a virtual machine which compiles terms to byte-code on the fly to perform the conversion check. This is the idea behind implemented in Coq by Benjamin Gregoire. Apparently there is also this thesis by Dirk Kleeblatt on this exact subject, but down actual machine code rather than a virtual machine. Second, one may generate code in a more conventional language which, upon execution, checks all the conversions necessary to type-check a dependently typed program. This means we can use Haskell, say, to type-check an Agda module. The code can be compiled and run, and if it accepts, then the code in the dependently-type language can be assumed to be well-typed (barring implementation and compiler errors). I've first heard this approach suggested by Mathieu Boesflug. Finally, one may require that the terms appearing in types and the terms intended to be run be part of two distinct languages. If the terms appearing at the type level do not themselves have dependent types, then one may compile in two stages: first, compile the "type-level" code and then you can execute this when checking the types of the "term-level" code. I'm not aware of any system that proceeds in this manner, but it is potentially possible for many systems, like Microsoft's F$^*$ language which has distinct type-level and program-level terms. 

Inferring reasonable statements, i.e. $1+\pi$ should be easy to recognize as a real number Acceptable performance, i.e. $1+1$ in $\mathbb{N}$ should be recognized nigh-instantaneously, even as part of a much larger expression Understandable error messages, i.e. $\pi+\mathbb{Z}$ should fail because $\mathbb{Z}$ is not (coercible to) a real number, rather than some crazy failure involving additive structure on quotient sets. 

Of course, the "right" framework in which to think about these maters, which can be seen as the minimal requirements for this proof to go through, is Lawvere's fixed point theorem which states the the theorem holds in every Cartesian Closed Category (so in particular, in any reasonable type theory). Andrej Bauer writes beautifully about this theorem in the paper On fixed-point theorems in synthetic computability, and I suspect might have some interesting things to add to this answer. 

Barendregts Lambda Calculi with Types is more advanced, but it covers some important topics in the "classical" theory of types. 

I strongly disagree with the "find a list of open problems" approach. Usually open problems are quite hard to make progress on, and I'm thoroughly unconvinced that good research is done by tackling some hard but uninteresting problem in a technical area. That being said, of course solving an open problem is really good for academic credentials. But that's not what you are asking. Research is a process designed to generate understanding at a high level. Solving technical problems is a means to that end: often the problem and its solution illuminate the structure or behavior of some scientific phenomenon (a mathematical structure, a programing language practice, etc). So my first suggestion is: find a problem that you want to understand. Research is fundamentally about confusion. Are there some specific topics you are interested in, but that you feel you have a fundamentally incomplete comprehension of, or that seem technically clear, but that you lack any good intuition for? Those are good starting points. Follow Terry Tao's advice ask yourself dumb questions! A lot of good research comes out of these considerations. In fact, this whole page contains a lot of good advice. Note that if you are looking at a well-explored problem or field, it's unlikely you'll get original insights right away, so it's important to read up on literature concurrently with your own explorations. Second, don't discount communicating with your Professors. Ask them about their own research, not necessarily about projects they want to give you. Engage in a conversation! This helps you find out what you are interested in, but also what the research landscape looks like in their field. Research doesn't happen in a vacuum, so you should speak to your fellow students, PhDs in your department, go to talks and workshops at your university, etc. You'll find that being immersed in a research environment helps you do research a lot more than finding a list or specific problem and locking yourself in your office. Finally, I would suggest working on something small. Research is bottom-up much more than it is top down, and it's rare that a very simple task (writing a proof or a program) turns out to be as simple as you expected it to. Doing several small projects that are not research-scale (expanding on homework, writing up an explanation of something you learned) often build up into genuine research level stuff. It's common to try to "go big" at the beginning, but that's just now how our brains work. 

I'm afraid I don't have a clear answer to your question. As I imagine you know, the basic definition of being a manifold cannot even be formulated in HoTT, since it crucially relies on the topology of the space in question, which does not exist for every type! The primitive notion in HoTT is that of the path space $I_X$ of a type $X$. But, as far as I know, this does not in general define a topology over $X$, which is required in the usual definition of manifolds. Another point is the property vs structure problem that seems to underly your question. It is certainly possible to define a manifold structure over a type: a manifold is a tuple $(X, T, A)$ where $X$ is a type, $T$ is a topology over $X$, and $A$ is an atlas, etc. Given such a structure (which may include proofs of the various properties that underlie the structure), you can of course ask whether it is homotopy equivalent to another $Y$, as a homotopy type. This means that being a manifold is first defined as being a structure, and only then can you ask the question whether another type has the property of being homotopy equivalent to some manifold. This is just what you do in "ordinary" math though, and doesn't seem to nicely use the synthetic properties of HoTT. As far as I can tell, the machinery required to talk about manifolds in a synthetic way is still being worked out, and certainly hasn't been well explored yet in any of the current (semi-)implementations of HoTT. This is an exiting avenue of research though. It seems to be structural, and thus requires adding constructs to "ordinary" HoTT. The most notable proponent of this approach is Urs Shreiber (though Mike Shulman certainly is a close second, and might be able to give a more informed answer), and some pointers can be found here and here (n-Cat Lab). 

With a little thought, you can probably strengthen this argument to give you the full theorem directly without the internalization. 

I'll compliment Neel's (excellent, as usual) answer with a bit more exposition on why levels are used in practice. The first important limitation of CoC is that it is trivial! A surprising observation is that there is no type for which you can prove that it has more than one element, much less an infinite number of them. Adding just 2 universes gives you the natural numbers with provably infinitely many elements, and all "simple" datatypes. The second limitation is the computation rules: CoC only supports iteration, i.e. the recusive functions do not have access to the sub-terms of their arguments. For this reason, it is more convenient to add inductive types as a primitive construction, giving rise to the CIC. But now another problem arises: the most natural induction rule (called elimination in this context) is inconsistent with the Excluded Middle! These problems don't appear if you restrict the induction rule to predicative types with universes. In conclusion, it appears that CoC has neither the expressiveness nor the robustness wrt consistency that you would like in a foundational system. Adding universes solves many of these problems.