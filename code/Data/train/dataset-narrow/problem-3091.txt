Build a dictionary of common words that frequently appear in these documents (e.g., MEDICAL, SEX, AGE, etc). Then, for each word in the output from OCR, check whether it is similar to a word in your dictionary; if so, then replace it with the dictionary word. "Similar" might be defined as "edit distance <= 1". For example, your sample output has the word "sax". If your dictionary contains the word "SEX", "sax" would be detected as a misspelling of "SEX" (edit distance = 1). This post-processing will help you detect some of the common words and help you identify the fields. What it won't do is clean up errors in the OCR of codes, as those can be anything. There's probably no good way to handle that. Also, do a Google search on how to use Tesseract. There are some best practices that seem to improve its output (e.g., convert to greyscale TIFF format, deskew text, binarize, and more). Finally, some commercial OCR software is significantly better than Tesseract or any other free OCR. e.g., Abbyy seems to be well-regarded. 

We can't tell you what loss function to use. That is based on business needs. In particular, what is the cost of being wrong? Is the cost of being wrong proportional to relative error? absolute error? Something else? That will drive the choice of loss function. You should try to choose a loss function where the value of the loss function is proportional to the cost of the error (e.g., the monetary cost to your company). Using ReLU as the activation function in the final layer would be sensible given that your data is censored to replace all negative outputs with 0. 

Your parameter $\alpha$ has fairly low dimension. Therefore, I recommend that you apply optimization methods directly to try to find the best $\alpha$ (without trying to use convolutional neural networks and regression for this purpose). Define a distance measure on images, $\|I-J\|$, to represent how dissimilar images $I,J$ are. You might use the squared $L_2$ norm for this, for instance. Now, the loss for a particular parameter choice $\alpha$ is $$L(\alpha) = \sum_{k=1}^N \|F_\alpha(I_k)-J_k\|.$$ We can now formulate your problem as follows: given a training set of images $(I_k,J_k)$, find the parameter $\alpha$ that minimizes the loss $L(\alpha)$. A reasonable approach is to use some optimization procedure to solve this problem. You might use stochastic gradient descent, for instance. Because there might be multiple local minima, I would suggest that you start multiple instances of gradient descent from different starting points: use a grid search over the starting point. Since your $\alpha$ is only three-dimensional, it's not difficult to do a grid search over this three-dimensional space and then start gradient descent from each point in the grid. Stochastic gradient descent will allow you to deal with fairly large values of $N$. This does require you to be able to compute gradients for $L(\alpha)$. Depending on the filter $\alpha$, it might be possible to symbolically calculate the gradients (perhaps with the help of a framework for this, such as Tensorflow); if that's too hard, you can use black-box methods to estimate the gradient by evaluating $L(\cdot)$ at multiple points. If $L_2$ distance doesn't capture similarity in your domain, you could consider other distance measures as well. I expect this is likely to be a more promising approach than what you sketched in the question, using convolutional networks and a regression model. (For one thing, there's no reason to expect the mapping from "features of $I_k$" to "features of $J_k$" to be linear, so there's no reason to expect linear regression to be effective here.) 

Remove input data to test for leakage This is very generalized question, so without knowing the types and provenance of the input data, this can be hard to answer. But, in general, to check for leakage, you can use the model on some subsets of the input variables while removing other input variables. If you get data from multiple sources, then try removing all input variables from a single source, then re-run your models. You may be able to identify the source of the data leakage. Alternately, if computational power allows, you can brute force it by running the model with each of the 150 input variables removed, or all sets of two variables, etc. Use customer-centered time data Regarding model meta-data, again I would investigate data provenance. Are you predicting churn using the complete patterns of customers who stopped using the service? What I mean to say is, instead of looking backwards from a fixed real-time period, like today, to all customers who did or did not stop using the service, try looking from a fixed customer-time. Use only data from the first year that each customer used the service, and attempt to predict whether each customer will remain with the service for another year. The warning signs of a customer dropping the service may be obvious in the months leading up to that customer dropping the service, but by then, the predictive power of your model may be too late to stop that customer from leaving. Instead, index the time component of each customer's history to zero when the first start using the service, and run your model on this data. 

Why not use all non-code indicators as 'handprints?' For example, many IDE's will add specific comments to documents when they are started. Also, and this is one that would be easy to detect my work, I have a tendency in python to copy entire import blocks from one script to another, whether or not I actually need all the imports or not. If someone uses a copied import or include statement, then their imports will all be in the same order. If you track the order of imports, you may be able to find patterns. There are also issues of whether you use spaces or tabs for indentation, preferred number of spaces or tabs, etc. For languages with braces, you can see if braces are used following an if statement, or at the start of the next line. I believe this will give you enough 'handprints' to identify people distinctly. Once you do this, you can use multilevel clustering to attempt to assign documents first to a group of possible originators (i.e. all originators who use a certain IDE or text editor). Then you can go through and look for certain patterns within each group, clustering again within each cluster. 

Synthetic gradients make training faster, not by reducing the number of epochs needed or by speeding up the convergence of gradient descent, but rather by making each epoch faster to compute. The synthetic gradient is faster to compute than the real gradient (computing the synthetic gradient is faster than the backpropagation), so each iteration of gradient descent can be computed more rapidly. 

"Premature optimization is the root of all evil". --Knuth You could do this, but why? You definitely don't want to do this, if you're going to feed the result into a classifier: most classifiers will perform worse after this transformation. There's no point to do this, if you're trying to save space in a database: hard disks can store hundreds of gigabytes, so a measly 15 columns probably won't make any noticeable difference. 15 additional columns is tiny. Just save the additional 15 columns and spare yourself headaches. The cost of your time to program this up and troubleshoot all of the problems it causes downstream will almost surely exceed the miniscule cost of computation or storage to store the data in the natural format. And if you seriously have big data where you truly need to do some optimization, the very first step is to measure: measure how much space/time you're actually consuming, and what the dominant contributors to that total cost is, and then focus on optimizing those dominant factors. Optimizations should be guided by data; otherwise you risk implementing complicated optimizations that make little difference overall but add needless complexity and epicycles to your data processing workflow. 

You might enjoy looking into the literature on "adversarial examples". Given an instance $x$ with a label $y$, an adversarial example is a (typically carefully constructed) instance $x'$ that is very similar to $x$, but whose label differs from $y$. The research literature suggests that it is often possible to find adversarial examples that are very close to the original $x$. You could use the distance $d(x,x')$ as a measure of sharpness of the decision boundary near $x$; or you could average this over many $x$ to get a global measure of sharpness. There are many methods for finding adversarial examples. A standard simple one is the gradient sign method, originally described in the following paper: Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy. Explaining and harnessing adversarial examples. arXiv:1412.6572, 2014. Since then there have been numerous improvements that find even closer adversarial examples, e.g., iterative gradient sign (arxiv:1607.02533), Deepfool (arxiv:1511.04599), and others. You might also be interested in Cleverhans, a software library to assist with finding adversarial examples. 

Parse using regular expressions I work a project where we get thousands of data files per day from on the order of 10 different systems. The filenames are all a jumble, and have a tendency to change over time. This is a job for regular expressions. The primary thing I use to organize is functional groupings. I simply use a small set of indicators common to each file set. In my case, the server that aggregates and sends the data files to me appends its name to the filename (zs2101, or something like that). So then I search file names for this limited set of regular expressions (I am currently using 20). In your situation, it seems like the client name in the file headers is something you could search for an use for the base of your organization Then, I divide files by date of generation of the data. Each file comes with a timestamp. When I read files into my archive, I find the date field in every file and convert it to a standard format, and then change the filename to reflect that standard format. Now I build a directory tree for each server name, organized by date (a folder for each year, and a sub-folder for each month, in my case). My recommendation is that you use regular expressions to organize your data. These are deterministic in the sense that if you receive a strange file header or malformed filename, you know where it will end up (in my case, there is an 'Undetermined' folder that accepts everything that doesn't match a server or date regex). The problem with k-means is that if you get something you didn't plan for, it can be pretty hard to tell where the clustering algorithm will put it, leading to lost data. Good luck, hope this helps. 

There could be significant problems from using one time chunk in a time series to predict the next chunk. For example, if you were using data from September and October to predict retail shopping expenditure in November and December, you would be very wrong. Instead, you need to note the annual trend of increased shopping between Thanksgiving and Christmas to make correct predictions. So, while this sort of skirts the questions you asked, I think that if you are having difficulty using one month to predict the next month, you need to start looking at longer-term trends, especially annual ones. I would investigate this first before looking into population instability. 

To handle class imbalance, do nothing -- use the ordinary cross-entropy loss, which handles class imbalance about as well as can be done. Make sure you have enough instances of each class in the training set, otherwise the neural network might not be able to learn: neural networks often need a lot of data. Assuming you care about global accuracy (rather than the average of the accuracy on each individual class, say), I wouldn't bother with a weighted cross-entropy loss or duplicating images. Your training sounds rather small. To deal with that, you might try starting from an existing pre-trained model and fine-tune the last few layers. Also use image augmentation. 

Suppose we want to fit a function $f(x)$. We can either try to learn a neural network model $F(\cdot)$ so that $F(x) \approx f(x)$. Or, in the residual network approach, we try to learn a neural network model $R(\cdot)$ so that $x+R(x) \approx f(x)$. Why is the latter easier to learn? There's no fundamental reason why it should necessarily be so, in general. It depends on the specific data set. However, one possible intuition is that we might expect that in some settings, $f(\cdot)$ might be approximately linear: i.e., as a first-order approximation, $f(x) \approx x$ might be a reasonable first-order approximation. Then we want to model the error term: i.e., suppose $f(x)=x+r(x)$; then we want to model $r(x)$. In some settings, the error term $r(x)$ might be simpler to model or smaller than $f(x)$. In that case, a residual network architecture might work better. If you like, you can think of this as being akin to a Taylor-series approximation. The Taylor series for $f(x)$ is $$f(x) = c_0 + c_1 x + c_2 x^2 + \cdots$$ Suppose $x$ is small (i.e., $|x| \ll 1$). Then $f(x) = c_0$ is a zero-th order approximation; $f(x) = c_0 + c_1 x$ is a first-order approximation; and so on. At each step, we expect that the residual/error term is probably smaller than the approximation. You can think of a residual network as learning a Taylor series for $f(x)$, in the special case where $c_0 = 0$ and $c_1 = 1$. There are other more sophisticated explanations / intuitions, but hopefully this gives one possible perspective. 

Don't try all possible thresholds. Instead, pick a random sample of 1000 candidate thresholds (chosen uniformly at random out of the set of $n-1$ candidate thresholds), calculate the information gain for each, and choose the best one. Use dynamic programming to efficiently compute the information gain of all $n-1$ splits, in total of $O(n)$ time, by reusing computation. The algorithm is pretty straightforward to derive.