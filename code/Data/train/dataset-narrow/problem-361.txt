trunk port with allowed vlan (theirs) and native vlan encapsulate. mimics an access port but lets you throw multiple vlans at a later stage if you want. Though it depends again on what they've bought from you as described above. 

Been discussing this with people and reading up, would like people's thoughts on whether or not its worth running IPv6 in a VRF or just let it loose in the global routing table. Even though presently we use a VRF topology for various reasons, for IPv6 I am strongly on the side of running it natively - resource consumption (thinking of full internet routing tables in IPv6 in a VRF, yech) - no more multiple routing processes per router - lots of our present HW that is not going to go EOL in the next year or two struggles with IPv6 in a VRF (3750, 3560) For purposes of 'mimicking' VRF separation for areas that require it going to bite the bullet and put firewalls where I need, the only alternative (short of mirroring the present VRF topology) is to go to a nightmare of tunnels and PBRS I guess the penny dropped when reading about IPv6 on MPLS 6PE. Thoughts appreciated (especially the ones telling me I'm wrong! its how I learn lol) 

If it is TCP, it will be allowed regardless of its source. This is a rather moot entry since the next one will allow any source IP address over any protocol. Change it to: 

This will allow all traffic except incoming traffic on FastEthernet8 with a source address of 208.73.210.0/23. 

(for each port. I did it one by one instead of a range to specify). Each port from D11-D17 shows the MAC address of the Alcatel stack as well as the port to which it is connected in a fashing like: 

I have a 4-switch stack of Alcatel Omniswitches connected to an HP 5412 core via 8 uplinks; these are ports D11-D18. Unfortunately, the client did not yet give me access to the Alcatels, but a request has been sent out. Meanwhile, I am seeing a high amount of TX/RX traffic on several ports in IMC regarding the core. An example: 

and so on. On port D18, however, the last port in the LAG, I see every single WAP. This is the case with both CDP and LLDP: 

The example will assume that you will want to NAT-overload (a.k.a. "hiding NAT" or "IP masquerading") to the current (public) IP address of the Dialer interface. It will use a named access list - please note that ACLs use wildcard masks, not subnet masks (binary inversion). I think here was the major config error in your example. 

8. other clever bits to have There's some clever config bits here and there, and I'll comment them here 

Disclaimer: I am not suggesting to add any form of redistribution of internal routing information to an eBGP peering to an ISP. Please make sure that you know exactly what you are doing (and why), if you consider doing this. To answer your concerns about the eBGP peers not being directly connected: In that case, the stability of the adjacency is your concern, not BGP route dampening. There's ways to tune or help BGP a bit to allow for faster detection of connectivity issues between peers: $URL$ $URL$ , Section "BGP Fast Peering Session Deactivation" The best option might be to talk to your ISP and ask them if BFD based BGP peering is available, or (big style) what kind of redundancy options they have on offer. 

With Cisco ACL's, there is an implicit deny ip any any at the end of every list. You need to explicitly state the traffic that you want to allow/deny. Note that it short-circuits on the first ACL entry it hits that it applies to, so if you're sending a packet from a host on the 208.73.210.0/23 network, it will first hit this ACL entry: 

UDP is obviously a send-and-forget protocol. For example, during an NMap UDP scan, the only way to definitively prove that a UDP port is open is if you receive a response from that port. Keep in mind that many services may not reply to arbitrary data and require protocol or application-specific requets in order to warrant a response. Certain ICMP codes can gurantee that the port is closed, however. RFC 792 and RFC 1122 give us some good information as to what to expect when a port is closed. For example, an ICMP type 3 code 3 "Destination Port Unreachable" is, for all intents and purposes, almost guaranteed to be a closed port. A full list of codes can be found here: $URL$ 

Yes you're right. Looks like you'll have to deal with having separate IPs unless aggregating via a VPN concentrator. TBH the concentrator is probably the best bet, you'll just need 1 extra IP for all VPN connections, but its outside interface will need to reside in a different subnet/VLAN from the public IP hosts. I would go with that otherwise you're configuring IPSEC VPN directly on each individual server, what a nightmare 

It depends. I would view the issue in two aspects. - risk of wireless being compromised - what access the wireless network has if it is compromised or a compromised host runs on it. The former aspect has already been talked about extensively, so I haven't a lot to add, other than if it is a guest network then a lot of the more secure options e.g. dot1x is out of the question. If you are paranoid you could use a guest controller that offers one time passwords e.g. via a ticket printer (hotels etc. use these). The latter aspect is relatively straightforwards from a design POV: just make sure whatever VLAN or network segment it goes on has no access to your corporate network whatsoever. All of below would work, in decreasing order of security - Completely separate network including physically separate equipment/internet links - Virtually separate network via VRF that only has internet egress (combine with firewall or not) - Firewalled or ACLed VLAN that only has permission to egress to internet (but runs in same VRF or routing context as normal) - this is the easiest solution and is probably good enough unless you are military/banking etc. 

In past experience, this is generally indicative of a loop of some sort. (Note: This is a school, and students have been known to intentionally plug in an Ethernet cord from one wall to another. We have plans to re-mediate this, believe me.) The setup is rather simplistic in that there are a total of 324 Cisco WAPs, each connected to one of the four switches in the Alcatel stack with a 1 Gbit uplink. Now, on the core, when I run: 

I have a physical host server connected to an HP (say, 2650 series?) switch. The host is running multiple Hyper-V virtualized OSes, each on a different VLAN. If I make the switch port dot1q tagged for each VLAN and untagged on the main host, will that allow access for the OSes to function and connect to the network? In pseudo-Cisco terms, if I make it a trunk port for VLAN 2&3 and an access port for VLAN 1 (you can do that in HP), will this solve my problem? This is very time sensitive so I'm sorry for the short post. More information is available to me if needed. Thanks! 

The ppp ipcp route default from the dialer0 interface will install a route with administrative distance 1, overriding the above one while the PPPoE session is up. 6. LAN interface, DNS and DHCP The 881 has an integrated 4-port switch which you need to treat as such. You cannot assign IP adresses to its individual interfaces, but you need a VLAN and an SVI ("interface vlan") to go along with it. In this case, we'll use 192.168.10.0 and VLAN10 as internal network. Adapt accordingly to fit your network situation. 

As long as you go down the VLAN/SVI path with that switching module, AND as long as you can get along with the restrictions they impose, you can use them for most purposes of routing. A few things that don't work well (or at all) with the VLAN/Switchport/SVI combination, in no particular order: 

The Catalyst 2960-X supports what is called netflow lite, not full netflow, and for that, it needs at least the LANBASE license. See "Prerequisites" on $URL$ (publically available Cisco Doc). See the outputs of or to check the license on the given 2960-X. We have seen cases where a Lan Lite switch would accept commands for unsupported features without returning an error - and the feature would just not work. That being said, I don't see where the error in the config might be - we have 

I recently had two switches from different vendors, one was an HP 5412 Chassis running ProCurve and the other is an Alcatel 4-Switch stack, have a LACP misconfiguration which brought down a portion of the network via a switching loop. The LACP configurations were correct at first, but one of the switches (Alcatel stack) was restarted for maintenance, and the startup configuration was not saved correctly. This meant that the traffic was not handling 802.1q tagged packets on the correct VLANs and were all on the native VLAN (in this case, VLAN 2). The other side, however, attempted to negotiate LACP and, since there is no spannign tree within the network (not my design, I assure you), it created a switching loop. My question is why LACP does not detect this mis-match and automatically disable the ports? Or rather why is it allowed to forward traffic if it has not negotiated a LACP LAG? 

The left most 18 bits represent the network, while the 1's represent the hosts on the network. To find the broadcast address, just maintain whatever the first 18 bits are in the original 10.0.0.0 address, and replace the following 14 bits with 1's. In order to get the next subnet, simply increment the left hand side: 

(Disclaimer: I am not familiar with HP switches, so I wont' be able to give proper command syntax for show/display commands). For a first, let's leave all routing discussion aside. AS5500 an AS3100 are on a common subnet (172.16.1.0/24) on VLAN1, so they must be able talk to each other directly. Accessing AS3100 from "elsewhere" is for later, we first have to solve a switching problem - or confirm that we don't have one. Here's two test procedures to see if VLAN1 is actually "going through", i.o.w. if it is one single broadcast domain spannig both switches 

Caution: NVI NAT can be VERY taxing on the CPU of low-end routers like the 800 series. Where my old 881 used to be able to deliver 50-60Mbit/s with classic NAT, switching over to NVI caused the throughput to drop to 20-30Mbit/s and would have the CPU glowing red when under load. That was also the case when the to-be-hairpinned translation was not actually in use, just with traffic matching the normal "interface ... overload" outbound NAT rule. 

Note that the 208.73.210.0/23 network will still be able to communicate to other networks over different protocols (UDP, ICMP, etc). To block all outgoing traffic, try: 

A coworker and I recently migrated a core from old Dell switches to two running Comware 7 and IRF. What we have noticed is that every 5 days, the user accounts no longer accept the passwords. Even if no unsuccessful attempts were made whatsoever, all of the accounts lock every 5 days. We only have HTTPS and SSH protocols enabled for remote access over the VTY lines. What we find is that, when we enter the wrong password, it asks us to re-enter the correct one. However, once we do, it simply terminates the connection. The only way that I have found to fix it is by getting physical, console access, deleting the accounts, and re-creating them. Is there a way to fix this? Unfortunately, this is a sensitive client, so I will not be able to post the entire configuration, but I can post certain parts upon request: VTY Line configuration: 

To me, this looks perfectly normal. I don't think that these captures are identical. Looking at the TCP headers of the the first frame (TCP SYN), you can see that - although all other properties are identical - the sequence numbers are different. EDIT: Oh.. there is also MSS clamping going on - ASA rewrites the MSS field from 1460 to 1380, on both the SYN segment from the initiator and the SYN-ACK segment from the responder. I think this is ASA's TCP sequence number randomization at work. EDIT: And the ASA performs some TCP MSS manipulation, too. Other than that, I would sincerely hope that all other properties of the frame/packet are unchanged when it is incoming through one of the port channel's subifs and exiting through the other (give or take NAT rewriting things a bit here and there, but NAT is quite obviously not in use here). 

There's a few catches to avoid (like making sure that the crosslink is never used as transit by other sites or even considered as backbone link), but overall, we've seen improvements in resilience and failover times over our previous FHRP-only based designs.