I am trying to create a New Cluster for Always On Availability Groups. On when I try to add the cluster nodes I get the following error: . After some research on the internet I found out that it could be because the Nodes I am trying to add were previously members of another cluster, even though the cluster doesn't exist anymore but the nodes are still acting as node to an existing cluster. One solution that I saw on several places was to use powershell to forcibly remove the node; But when I execute the command it just sits there and doesn't do anything. 

This reduced the number of VLFs from to . The end-users has seen a sudden improvement in the application performance. Now one thing I am not sure about is, if 20 VLFs is the right number of VLFs for a 36GB log file. But it seems to have a good impact on the performance of the database. 

The user must be a member of server role for user to have enough permissions to create a database. You can execute the following statement to make a user member of server role. 

I have setup from ServerA to ServerB = = = = . I have used a Domain Account for Snapshot Agent and Log Reader Agent, called . The initial snapshot is initialised successfully, from there its all down hill. Error Log: 

Keep indexing minimum, Primary key Clustered index and a Datetime index and keeping the inserts very fast and get a performance hit on selects. 

Is there any wisdom in having multiple .mdf files for a database? Is there any drawbacks of having multiple .mdf files? 

Further investigation and "Googling" solved the issue, The issue was with drive, It was compressed and SQL Server did not like that. Solution Removed the compression from Drive and this time backup completed without any errors and much faster. 

Note You are doing an Insert here, would have nothing in it and it will always return null. Delete is only useful when you are deleting or updating data, when deleting the delete values can be returned using delete.xxx and when updating the old values can be returned using the deleted.xxx. But you have no use for deleted in your case since you are only inserting, you should be using . 

I would create One stored procedure with optional parameters. Also instead of using use dynamic sql to build your query. This kind of query where clause results in a very unefficient execution plan. Also Sql server doesn't do any shortcircuting when evaluating OR conditions in where clause, for example even if @Var is null evaluates true sql server may still go ahead with the other Or condition and starts evaluating the whole column against a null variable. Also with these optional parameters, if you pass a specific set of parameters and there is no execution plan for this procedure in the proc cache sql server will create an execution plan best for these specific set of parameters and uses it, but what if you pass a very different set of parameters on the next execution of this procedure, sql server looks into the proc cache and if it find an execution plan there it just goes ahead and uses that execution plan, even though it was a good execution plan for the first set of params passed but a very bad execution plan for these very different set of params (known as Parameter sniffing). Using dynamic sql also eliminates this parameter sniffing since it stores parameteised execution plans. I would use one procedure with dynamic sql something like this... 

I understand sql server has disabled this feature by default as a security measure, however I executed the following statements to enable it. Configuration 

Replication Monitor: : OK : for both Agents and Status shows it is Running..... and Property says As it is obvious from the error message the Login is failing to get access to SQL Server, So just for test purpose I gave this SQLAgent account sysadmin role. Yet the error persist. All the material I can find online, suggests Giving more permissions to login will solve the issue. But if sysadmin role hasnt solved it, I have kind of ran out of ideas now. Can someone please suggest what is that I am missing? how can I fix it? Any suggestions or pointer in the right direction is much appreciated. Thank you. 

How can I remove this orphaned node of an old cluster? Any ideas any pointers are much appreciated, Thank you. 

I have a table where 15,000 to 20,000 rows are being inserted every hour. Table Schema is something like 

I used the SQLIO tool to measure the disk Read/Write performance and it seems as fast as any other disk. I know for a fact I suspect that it is more likely that there is something wrong with the log write process, but I am running out of places to look for, can someone please advise me where else I should look for issues. What can be the possible causes of slow log writes? or any advice or pointers in the right direction are much appreciated. Thank you. 

for me details about the error message read this article I used the method suggested by but installed and wala it worked. Had to do a lot of reading and tried a lot of different methods to fix it, thought it will be nice to share it with you guys. cheers thank you. 

But the person claims that initially he installed SQL Server 2008 R2 Standard Version (licensed copy) on the server, but later on someone by mistake has installed Report Builder 3.0 enterprise evaluation version on that machine and this has caused the SQL server license to expire. I have never heard of such a thing. I thought Report Builder was a client application and as long as the server you are connecting to is licensed you should be able to use it, but it would never affect the licensing on the server it is connecting to. Anyway I connected to the server itself and started going through the windows log to check when the SQL Server was installed. In window's log almost a year ago when the SQL Server was installed , all the MsiInstaller events during SQL Server installation points to a directory 

I have been given a server (SQL Server 2008R2 64 bit) where our application is running. There has been a gradual decrease in the performance of the application. Now it has come to a point where performance has become a serious concern. I started to investigate and went through the following steps. 

No row is ever update nor deleted. As you can imagine table grows huge in very short time. The table is queried as well time to time. SELECT statements with filtering on DATETIME column and sometimes other filtering on INT column too. To my knowledge, I have two options 

I have been looking for any information online but couldn't find anything.... Any suggestions any solutions any pointers in the direct direction is much appreciated....... 

SQL Server cluster is a fault tolerance solution it has nothing to do with performance. If you want your load on sql server to be shared , you should be looking at Replication or maybe log shipping, direct reporting calls to that replicated /log shipped server and keep application calls to the main server. If your data gets too large that no matter what you do, performance is still a problem, consider archiving data to a data warehouse, in your application database (OLTP database) only keep recent records (last 6 months). Anything older than 6 months gets shipped to your data warehouse (OLAP database). The moral of the story is clustering your server isnâ€™t going to give you any performance boost, consider an alternative option. 

Another option that was suggested to me, was to create another table, Populate that table from this current table and index the living hell out of it to help all the possible select queries. Read data from this duplicated table. Keep the original table as it is with minimum indexing, for quick inserts. Since I am duplicating the data, I know this option violates the basic rules of normilazation, but this sounds like a good option for keeping inserts and reads as fast as possible. The problem is how can I maintain this near real time copy of this table inside the same database. I do not want to use any After Insert triggers as this will end up firing 15,000 to 20,000 times an hour. What other options I have to keep this near real time copy of table in the same database??? Or maybe another approach altogether, any suggestion or pointers in the right direction are much appreciated. 

Error Message Yet every time I execute the select statement to get data from Excel sheet I get the following error. 

to connect to local server and default instance just use any of these , , or . If you are trying to connect to a named instance on your local machine just use any of the above followed by . i.e , , or . Note If you want to connect to the default instance you just use the machine name, if you are trying to connect to a Named Instance then you have to use the machine name and the Instance Name. 

Important Note The SQL Server has no external IP, it can only be seen from the reporting server. Could this be the reason that the connection is failing? I mean does the report builder try to obtain a direct connection to the SQL Server when launched? Or is there anything else that I have missed out? Any pointer any suggestions are much appreciated , Thank you. 

indexing columns (Datetime, some int columns) to get better performance on select statements and get a performance hit on my INSERTS 

Hi guys I have question about how two different Hard disk setups can possibly affect performance of my database. I have two Options. Option 1: One Physical Harddisk and then Multiple Virtual hard drives for SQL Server's Database files (.mdf , .ndf),Log files, TempDB, Backups etc Option 2: One Physical Harddisk and then One huge Virtual hard drive for SQL Server's Database files (.mdf , .ndf),Log files, TempDB, Backups etc 

Alternatively, if you always look for a specific value in the column in question, you can make use of the Computed columns, something like... Test Table 

Let's say you are only interested in the rows where it has a value of and this the only value you always check for, then you can add another column (Computed) to the table, something like... 

I have searched online and it appears that some other people has also encountered the same issue but no one seems to have suggested a proven method to fix it. I have tried registering the dts.dll because some of the posts online suggested this should fix the issue but no joy. Any pointers or suggestion in the right direction are much appreciated. Thanks 

I am trying to take a Full Backup of a database size about 75GB. It is version. I have plenty of disk space where the backup is being created. So disk space isn't an issue. The backup process starts fine but half way through it errors out just saying . 

Question 1 am I correct in concluding that even if the person thinks he installed a standard version of sql server, it was actually an enterprise evaluation version of sql server ?? Question 2 Is it possible that an Enterprise Evaluation version of Report Builder can affect License of already installed SQL Server standard and cause it to expire? 

To my knowledge, If these were all physical hard drives, having multiple drives for all these files (tempDB, Log file, backups, database files), would definitely benefit from parallel processing. But is there any drawbacks of having one massive Virtual Hard drive vs multiple Virtual Hard drives as in reality, when Sql Server is reading or writing , it will be all done from one Physical Hard disk? Is one method preferred over other? Any suggestions or pointers in the right direction is much appreciated. Thanks 

SQL server always append some random number in the end of a temp table name (behind the scenes), when the concurrent users create temp tables in their sessions with the same name, sql server will create multiple temp tables in the tempdb. I created 3 temp tables called in three different sessions in my SSMS, now if I go to the tempdb I can see the temp tables created there with a random (unique) string appended to each temp table's name. 

The above query returned the databasename and number of VLFs (1600) To reduce the number of VLFs this is what I did: 

Even though you are looking for records with a string anywhere in there but you will benefit from the Index on column. 

As mentioned by Aaron in comments section that it can cause your insert statements to fail if someone passes a values longer than your column length (I would call this a bug in your code). Anyway, from performance's perspective, execution plan may look the same but SQL Server is having to do slightly more work when dealing with bigger data types as compare to smaller data types. If you switched on IO and Time statistics on before running these two inserts, you will notice that the insert with bigger variables are taking slightly longer than the smaller/correct size variables. Insert with large variables 

2) This will open a pop-up window enter your in Server Name section or select a server name from the Drop down list. 3) Enter the . 

I have search online but not found what I am looking for. The problem/query is to my knowledge a database in SQL Server should have One .mdf file and maybe some .ndf files and One .log file. I have seen many database with one mdf and multiple ndf files. But recently I came across some database on a server where every database had multiple mdf files. I took a backup of some database moved them to another server and restored the Primary file as .mdf and all the other files as .ndf obviously except the log file it was restored as .log. Now my questions are: 

Inside your merge statement in the output .. into clause Explicitly mention the column names for table. This is the reason you are getting the error. 

A colleague of mine rang me today to told me that on one of our servers the SQL Server has stopped working. If he tries to run the management studio it throws and error saying 

I have a production Server say I have setup log shipping to which is left in read-only mode. The purpose of this log shipping is to lower the load on production server for some expensive queries (painful reports). Now if I have to create some logins using our domain accounts. I cannot do this because the secondary database is in . I thought if I create these logins on Primary server it will be copied over to secondary server then the logs are restored there but this isnt the case. I have done a lot of research online finding a way around to this. I found the following resources for this. I tried every method suggested in this articles but none of them seems to work. 1) 2) 3) Has someone experienced the same issue? what did you do? Is there any way around for this issue? Any suggestions any pointer please.