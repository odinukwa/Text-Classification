Also, you can call the split method directly, without the statement, as it will return a single item list with the full string inside if the character is not in the string: 

The name is taken from the approach being somewhat similar to the sieve of Erathostenes algorithm. In your case you can take things one step further, and rather than storing the full array, store only the indices of the entries: 

which of course returns the same result as your code, but uses a more efficient algorithm. How much more efficient? Well, if \$n\$ is the length of the phone number, and \$m\$ is the maximum number of possible next digits for any digit, your algorithm will have exponential behavior, \$O(m^n)\$, while the DP approach brings that down to \$O(m n)\$. That is, by the way, a huge improvement. Try e.g. to run your code to calculate the number of 100 digit-long phone numbers, and I bet you will run out of memory before it finishes, while the DP approach spits the result in a blink of the eye: 

You algorithm is \$O(n^2)\$, as you are scanning the full list for every item of the list. A more efficient algorithm can be put together by first sorting the list: 

A properly implemented Mergesort takes \$O(n \log n)\$ time and uses \$O(n)\$ extra memory. And while I think your code complies with those limits, the constants involved, especially when it comes to memory, are larger than needed You typically want to write your algorithm to sort in-place (and precede it with a copy if you don't want to alter the input). This spares you lots of data copying ,which now only needs to happen in the merge step, and you actually only need to copy to a separate buffer the first half of the array. This also means that, once the merging is finished, any remaining items on the right side are already in the correct location and do not need to be copied back to the output. I'm not sure if the written description is helping or just making things more confusing, so let me explain it in code: 

Slicing a Python list creates a copy of the list. This is not what you typically want in a merge sort. To avoid this, you can leave your original list unsliced, and pass around indices to the intervals being sorted and merged. Such an in-place merge sort could be written as: 

Notice how the left part is the only one copied out into the buffer, as that is all that is needed. Notice also the loop structure, which is substantially different from yours. It is a matter of taste, but the single while loop for the merging, with an extra loop to copy items that could be left in the buffer (notice that any item left on the right side is already in the right place) seems to be preferred in most production implementations I've seen, and I find it more clear, but YMMV. The above code is creating a new buffer for every merge operation, which makes the code a little simpler. A possible way of reusing a buffer is to have it be a static variable of . Static variables are a little funny in Python, but it could look something like this: 

You would have to do the timings, but as I said originally, there is a good chance that using Python built-in methods, e.g. , ends up being faster than what I have outlined here, even though the approach is algorithmically less efficient. But that's a whole different story... 

This multiplies all of the "columns" first, then divides by each "column" to get the result of multiplying all but that column, which may be problematic depending on your data due to potential overflows or precision losses. And it breaks completely if there are any zeros in the input array. If you can get rid of the division (this is a typical interview question, by the way), then all of those problems dissappear. And you can do it with relative ease, by cleverly storing accumulated products from both ends of the array. then multiplying them together: 

Note that I'm keeping duplicate accounting on scheduled_nodes, both in and . This is to have the ordering of the FIFO queue and the fast membership check of a hash table. You could get rid of by checking against in , but you will again get quadratic performance, not a good thing. 

This is missing the empty lists for items not present in the original lists. You can create those by doing: 

Also, note that I am computing as , not . This is not relevant in Python, because integers never overflow. But in other languages, the form I have used will never overflow if and areboth positive values. This is a more or less famous bug, that was shipped with standard libraries for years, until someone figured it out. 

It is probably not worth the effort, as the board is small, and 4 is itself a smallish number, but efficiency wise, you want to minimize duplicate checks. Consider this alternative approach to checking a single row for a win: 

In Python that doesn't seem to pay off (performance wise) until is in the many hundreds, but I find lovely that, the way that code goes, is always exactly divisible by . Figuring out why is left as an exercise for the reader... 

You are doing what is often called a three-way binary search, as in each iteration you have three possible results: smaller, larger or equal. This may seem advantageous, because if you find the item early, you may be done with as little as one recursive call. The alternative is to never check for equality, and keep dividing the array in half until you are left with a single item. This has the advantage that, in each iteration, you only do one comparison instead of two, but you will always have to do the full \$\log n\$ recursive calls. On average you will come out ahead if you do the full \$\log n\$ iterations every time. The way I learned to implement this, is to use an index to the first item to search, and an index past the last item to search for: 

Notice that, since generating all primes below can be done in \$O(n \log \log n)\$ time, it is now the prime generation that dominates the total time, i.e. using this faster algorithm makes finding the pairs virtually free (when compared to finding the primes themselves). A sample run: 

Sorting algorithms are typically defined to sort their input in-place, and the canonical definition of mergesort is not different. This means that, as a general rule, you don't get to pop or append items from the lists you are merging. Also, in the merging step, rather than doing the merging into an auxiliary list, you copy into auxiliary storage data that could be overwritten, and merge directly into the original list: 

This type of problems lend themselves very well to recursion. A possible implementation, either in list or generator form could be: 

You are doing an \$O(n^2)\$ search over a sorted list, which very often implies an affirmative answer to the infamous "can we do better?" question. For this particular problem, you can get \$O(n)\$ performance by simultaneously scanning from the front and back of the list of primes: 

I would personally leave that check entirely out. Your other checks are also pretty much unnecessary, as Python has very well defined rules for iteration over empty iterables (does nothing), and adding a special path for size-one iterables could easily be dismissed as premature optimization (the root of all evil!). Even if you ignore this advice, PEP8 prefers over . If you reorganize your code a little, you can get rid of the second loop entirely: