2) I need to skip some of the rows based on the text in it. For example I don't want to read in the row in which the word 'Total' appears (as in). How do I do that ? Although, it is not extremely important as I can get rid off it later, but skipping these rows while reading the data is ideally what I need. I know it is a long post, but if someone can help me with it, i would greatly appreciate it. Please let me know if more information is needed. Thanks much. 

I have a dataset with large (4500 variables) and 3000 observations (n < p case). I am using LASSO (from the glmnet package in R) to reduce dimensionality of the problem. I have couple of questions. 1) After I ran LASSO the first time, the number of variables with non-zero coefficients goes down to ~80 (which seems reasonable). But the correlation matrix shows that some of these variables are very highly correlated themselves (FORMS distinct groups if you plot it using the package. So I ran LASSO exclusively on these variables whose coefficients were non-zero during the 1st run. This time, the algorithm picked up even less number of variables.I was expecting that they would remain more or less similar. I want to know if running LASSO multiple times is advisable or it introduces bias of any form in anyway that interferes with interpretation/prediction ? 2) My second question is about the magnitude of the coefficients. Some of the coefficients, in absolute value, turns out to be very close to 0 (~1E-14) while some are larger. I mean the spread is very large. Does it indicate that the variables with this small coefficients are making the model worse in any way. Is it advisable to get rid off those variables and rerun LASSO ? Thoughts/comments are most welcome. Thanks 

There's a related example to your problem in the Spark repo here. The strategy is to represent the documents as a RowMatrix and then use its columnSimilarities() method. That will get you a matrix of all the cosine similarities. Extract the row which corresponds to your query document and sort. That will give the indices of the most-similar documents. Depending on your application, all of this work can be done pre-query. 

The advantage of the first approach is you can use the same module to process your real-time data. The advantage of the second approach is that it's very fast and usually easier to implement. Since you're already in a Spark Dataset, here's the strategy: 

The visNetwork R package is the best I've worked with. It renders with the vis.js Javascript package right in your RStudio window. Nodes and edge visualization is fully customizable and may be data-driven. They implement click-functionality on your graphs, so you can rearrange them or highlight a node and its neighbors. I've found people really enjoy this in presentations, and the highlight functionality is great for making static PowerPoint presentations or figures. 

I am working in R. I have two vectors; A and B of lengths 5913 and 3733 respectively (with entries are repeated). I want to extract those values (with repetitions) that are present in both A and B. I have done (A %in% B) (lets call it C) and (B %in% A) (lets call it D). The length of C is 3906 and that of D is 3607 (so 2007 elements in A are not in B and 126 elements in B are not in A). But How do I find the common values ? I don't think I can use intersect() method as it expects vectors with no duplicate values. I have many many repetitions. Note: Due to the length of these vectors, I couldn't mention them here. Any help would be greatly appreciated. Thanks 

I am new to word/paragraph embedding and I am still trying to understand it. My question may be trivial. So please bear with me. I have a collection of short documents (each documents contain few sentences and lots of noise). I want to cluster these documents according to the most similar documents into one cluster (soft cluster is fine for now). Now there are several techniques available (and noted tutorials such as in scikit-learn) but I would like to see if I can successfully use doc2vec (gensim implementation). To this extent, I have ran the doc2vec on the collection and I have the "paragraph vector"s for each document. Obviously, I can cluster these vectors using something like K-Means. I am just wondering if this is the right approach or there is something else is needed. Thanks 

You can search "reinforcement learning matlab" and "markov decision process" for some available projects and tutorials. 

If you are using R, the ROCR package is excellent for constructing ROC curves (and many other metrics, too). 

The idea is to do this check (surgery/surgery canceled) many times, thereby growing a set of outcomes for each policy you wish to explore. If you don't want to roll your own, there are packages for many languages such as SimPy for Python and Simmer for R. The nice thing about discrete event simulation is that you don't need to make any simplifying assumptions about probabilities, and the business logic can be arbitrarily complex. For example, if the surgeon and one of the nurses are having an affair, the chance that one cancels on the same day as the other can be increased. 

You and others are correct to question these "rules". Truth is, choosing a test/training split is a simple question with a complicated answer. See this paper which discusses different cross-validation methodologies, where 

I am trying to see if my data is multimodal (in fact, I am more interested in bimodality of the data). I performed dip test and it does evidence against unmodal data. However, I want to see, in particular, if it is bimodal. I believe silver man's test can be used. However, I couldn't find the implementation of it in either r or in python. (The one in R is old and not working with the current version of R). Also, assuming that I have a bimodal data and that I am able to get the two components (using mixtools in R), how do I figure out how to find the point of intersection of the two components. For example, here is the histogram (overlaid with its density estimation) of the entire data. Here are the two components: I want to get the the x value where the curves intersected. I could have uploaded the data, but the length of the vector is rather long. Any general thought and idea is welcome including the R and/or Python packages are welcome. Thanks 

2018 Update! You can create an embedding (dense vector) space for your categorical variables. Many of you are familiar with word2vec and fastext, which embed words in a meaningful dense vector space. Same idea here-- your categorical variables will map to a vector with some meaning. From the Guo/Berkhahn paper: 

Here's what the output Dataset looks like (I didn't remove the bad rows so you can see the calculation): 

This is actually a general problem with time-series data: you have some logic to implement based on one or more values in the series. You always have two choices: 

How can we efficiently (least code) generate an interactive (zoom/pan) scatterplot where the annotation appears when mousing-over or clicking on an individual point? This is very helpful when evaluating clusterings. 

Check out this paper. It also addresses question of how many clusters to use. The R package mclust has a routine which will try different cluster models/number of clusters and plot the Bayesian inference criterion (BIC). (great vignette here). It's a general method, meaning, something you can do without being domain/data specific. (It's always good to be domain-specific if you have the time and data.) The chart is from the vignette by Lucca Scrucca. MClust tries 14 different clustering algorithms (represented by the different symbols), increasing the number of clusters from 1 to some default value. It's finds the BIC each time. Highest BIC is usually the best choice. You could apply this methodology to your own stable of clustering algorithms. 

The body of the table begins with . Each is a row of the table.Within each row, that is within each pair of , each column is given by . Here are my questions: 1) How do I scrape it ? I am using and for this purpose as well as module. I tried the following: 

I am trying to build a long string of the form...... How do I do it ? I am trying to do it using R ? By hand is impossible as there are currently over 200 such names and in future I may have more. So I need to automate the process. However, every time I tried to use R's paste0 function I am getting a different result. Here is what I did: 

I am looking at a problem where I have two documents, both are textual documents. The first one is a long (few pages long) textual document, while the second one contains about 10 short texts, each being a one liner. The problem is to automatically understand the 1st document and "map" key ideas from here to one or more short texts in the second document. I am not being able to get a good grip on the problem as to how to proceed. I tried the following approach: first collected as much similar documents as possible which are relevant; I have obtained 136 such documents, each few pages long. Then build a document level embedding using each sentence as a document (doc2vec). Then for each short sentence in the second document, inferred an embedding based on the document model I built. Then tried to find the most similar sentences for each of these short texts from the 2nd document. However the results are not good. Only about 30% match is what I am getting and the corresponding sentences are not much related to the target sentence. I am wondering if this is the right approach or are there any other approach available which I am not aware of. Your advice would be greatly appreciated. It may be the case that my explanation above is not adequate. Please let me know and I will try to improve it. Your input would be greatly appreciated. Thanks 

In particular, they found the signal-to-noise ratio of the data had a lot to do with how the model should be trained and tested. You can also find empirical papers (here's my dataset, here's my goal, here's what worked best) such as this one which found 10-fold cross-validation was best for model selection with that particular data set. While I don't claim to have read everything, I haven't yet stumbled upon some who found a simple 80/20 or 70/30 split was the best way to do anything. 

The folks at fastai have implemented categorical embeddings and created a very nice blog post with companion demo notebook. 

If you really need to do that (I will argue it is not a good idea), with Postgres you can store an array type and write a stored procedure for new item insertion. This stored procedure can do whatever distance checking you wish, such as checking the distance of the new vector against all others in the database, before storage. I would argue against this design because I suspect the criteria for uniqueness* could easily change over time. I think it'd be a better idea to store all of the vectors except exact matches. Then, create another table which uses the definition of uniqueness. Creating this table would be handled on the application side. If your definition of uniqueness changes, no problem, just make a new table. You could even compare several different definitions to see how your results are sensitive to it. If you do it this way, Cassandra is a great database choice. It's specifically designed for denormalized data storage (you have the same data stored in different forms or variations, so that your application gets exactly what it needs without further computation). *In your post you stated that a similarity of < 0.9 would result in storing a new vector. That's what I mean by criteria for uniqueness. 

I need to scrape a table off of a webpage and put it into a pandas data frame. But I am not being able to do it. Let me first give you a hint of how the table is encoded into html document. 

I am working on a NLP related task. I have about 150 documents, each few pages long (5/6 pages long on average). After removing stopwords and other unnecessary symbols and digits, I have about 104,000 unique words. The task at hand probably require some kind of word embedding (such as word2vec) as simple bag-of-words type approach aren't working properly. However, I am concerned about the size of the data I have. I have looked at pre-trained word embedding (GloVec), however, due to the narrow focus of the domain (manufacturing) of our texts, I am hesitating to use these pre-trained vectors. That leaves me with training our own. However, the size of our data set concerns me. Hence I am just throwing this question out there: What should be the lower bound on the size of the vocabulary that we need in order to train a word embedding model (word2vec) that will be reasonable. Any response would be greatly appreciated. Thanks