Also realize that not all consumers of data use R, but many still interface their platform of choice with data using SQL. 

I'm not sure that anything is wrong here. With a binary target, chance is 50%, and 75% is half way to perfect accuracy of 100%. Kappa, which ranges from 0 at chance to 1 at perfect separation, is 0.4, which is again a little less than half way to perfect prediction. So it seems to be corroborating the performance of your model. Kappa is defined as: (observed accuracy - expected accuracy)/(1 - expected accuracy) Since your classes appear balanced, you have (0.75 - 0.5) / (1 - 0.5) = 0.5 

I generally advocate for cross validation in addition to a hold-out sample. As for the number of folds, that depends heavily on your data. Generally you start to approach diminishing returns after some point, but you should try, and evaluate, several regimes. This is very much an empirical question with no hard and fast best answer. 

Data is plural, as in 'These data are depicted below'. Singular is 'datum' but it's rare one would have occasion to use the term. 

Yes, the issue is certainly relevant, since your ability to fit the model will depend on the amount of data you have, but more importantly, it depends on the quality of the predictors. A 10-times rule might be a rule of thumb (and there are many others), but it really depends on the predictive utility of your features. E.g., the iris dataset is fairly small but easily solved, because the features yield good separation of the targets. Conversely, you could have 10 million examples and fail to fit if the features are weak. 

Many will tell you that normality tests are overly sensitive, especially given that most statistical tests are robust to even gross departures from normality. If you are very concerned, conduct a parametric test anyway along with a nonparametric, and if the end results agree, stick with the parametric. 

In your example what differentiates the clusters is not the raw value but rapid departure from previous points. I might look into change-point detection. Nonparametic, but still requires some fiddling with tuning parameters. 

As with many data science tasks, there are many ways to attack this; the above is but one of many possibilities. Hope that helps. 

One problem with this is that dark images simply contain less information. Anyone with a background in photography will tell you it’s easier to decrease exposure on a bright image than increase exposure on a dark one (you can’t create what’s not there but you can throw away information you have). If you want all images on the same playing field, perhaps do the reverse. Or, try to extract only edges or high contrast regions from all images. Or, model them differently. 

In most individual differences research, subjects are additionally coded along some dimension, such as young/old, low/high working memory, normal/cognitively impaired. Without this, you’re simply looking at individual variation, which doesn’t seem to address any question (what is your research question?). If you simply want to look at variation in overall RT on a subject level, I’d construct CDFs of mean RT collapsing over color, or perhaps for each color separately. This will simply demonstrate that people differ, and you’ll find the distributions adhere to roughly ex-Gaussian. If you mean to test differences between colors, you can use ANOVA or compare CDFs using K-S tests, but again there isn’t a clear research question here. 

These aren't even comparable, really. SQL is a language meant for accessing data, R is a language meant for working with data. SQL isn't an effective tool for munging because it it's difficult to see intermediate steps and when it throws errors, it isn't likely to address the form/quality/structure of your data. My workflow is typically: 

I don't know that I'd call this a machine learning problem per se...though others may disagree, this sounds like an optimization problem. Your description is vague, but let's pretend it's a guitar that you're trying to tune and you have 6 strings. The trick is to define an objective function that quantifies, with a single number, how 'good' the tuning is. (The program can independently manipulate each string's tension). Perhaps it's some deviance in power from an ideal FFT, and you sum the deviance from each string. Optimization routines such as simplex, sub plex, Nelder-Mead, GA, etc., are ways of manipulating the tension in 'smart' ways that obviate the need for a brute force (or 'grid search') solution. So long as you can define an objective function, you can start trying it out, though the local max/min problem will need to be dealt with. I'd look into some tutorials on minimization algorithms 

Both approaches are 'lossy'. For PCA, assuming you retain fewer components than variables, you necessarily throw away information. If you don't have a few components that capture most of the variance, you could throw a lot away. For feature selection, (I assume you mean an automated approach like lasso), you again throw away information, hopefully only incremental so. But, depending on how you structure the selection routine, you let an algorithm make some level of design decisions which result in less data. Whether this is bad or not depends on intent. Fire pure classification it's less an issue. For interpretability it can be essential. 

You'll never actually know if you've made a Type-II (or type-I) error in practice. As you surmise, during the hypothesis testing stage you either do or do not reject H0. Type-I and type-II errors are more useful in terms of error rate. This is a function of your alpha level and statistical power. So for instance you can set alpha to 0.05, 0.01, 0.001, etc based on your tolerance for such errors. But again you can't know after the fact. So it's called an 'error' from a pedagogical standpoint. 

It's not really feasible to label points as coming from distribution A or B if there is be overlap amongst the distributions. You can't know whether the point fell near the mean of A or the tail of B. In the simplest case, consider two normally distributed datasets with overlap - how can you know which generated it? You could estimate the probability of each point falling under A or B, but that doesn't really solve the problem. A different approach, but one I'm not sure will help, would be to fit a new distribution defined as the convolution of A and B. The ex-gaussian distribution is one such example. But you'd have to do the maths to figure out the PDF equation, and that still won't necessarily label your points. 

Install R Install some packages that will help you along ('tm' for text mining,'dplyr' for cleaning/organizing your data, and perhaps also 'lubridate' for working with dates/times) Read in your data from your source, be it a text file, spreadsheet, or some database (if a database, you'll have to conquer connecting R to said database too) You want to do a word frequency analysis for each month. How you accomplish this will depend on how the data is organized, which I do not know, but it would involve first rounding all your dates to month (using lubridate's 'floor_date()' function is one way), then parsing the text for each month into a corpus that can be analyzed (using package tm). Finally, for each month I would make a table counting words, sorting by frequency. That would give you, for each month, the top 'trending' words. To discount words like 'the' and 'a', I might also use some of the tools in the 'tm' package to clean things up. Note that in #5 I said 'words', not 'terms'. If you want to account for terms consisting of > 1 words, you'll have to 'tokenize' them, but that's beyond the scope of this very brief intro.