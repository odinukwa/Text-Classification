EDIT: forgot to mention - if you need to change winding order - just reverse the test in that "if" (== to !=). 

Why so object-oriented? What's wrong with a flag? You can define flags simply by creating a new variable: where and etc. This would require adding a parameter to the Log function like this: and having all irrelevant messages disabled at startup, this way: . Assuming that you do all your logging from a single thread (each thread should have its own logging state anyway), you can at some point just store the old flags, assign new flags and execute some code, after which you set the flags back to the old ones. P.S. Object-oriented = less reusable on the scale of all reusable things. Functions/data are above OO and algorithms are above both. 

Looks like this is the issue (buffer pointer mismatch - geomVertexShaderBlob vs geomPixelShaderBlob): 

Now you can just invert the normal velocity to make the object bounce off the collision and recombine both into the new linear velocity vector. 

What do you use as the W component for vertex positions in CPU-based calculations? To apply the translation part of the matrix, you have to use 1 for W. GPU adds that W=1 for you while converting less than 4-component vertex position data streams to 4-component vectors. I assume that all of the other math is correct, though. There are many ways to mess things up with multiple non-commutative operations (like matrix-matrix/matrix-vector multiplication), it could also be possible that there might be wrong order of operands somewhere... 

Looks like the index count you pass to a draw call could be wrong, that would easily cause buffer overflow, which could lead to "looping". Also, if your vertex data consists only of a position vector, how come the vertices are colored and the position is wrong? Either way, there are serious memory usage issues with your code that have nothing to do with the API or the book. I can only suggest you to read more carefully. 

If you don't bind a texture to the slot, the result of sampling is basically undefined (though it's often ). To fix your issue, create a white 1x1 texture and assign it to when you're using this shader without a texture. 

Since I've posted something controversial here, I'm adding some explanations (which I shouldn't really but I guess there's no harm in being explicit about things). Irrlicht: scene nodes are renderables. As I've noted above, this leads to poor batching possibilities, let alone obvious problems with having a component system built with the engine (like not having a place for such things - but hey, it's supposed to be a -game- engine...). There's also a problem with logical grouping of code in the sense that all rendering code is scattered all over the source code. Ogre: it's so overdesigned (?) and so heavy that the code is unreadable and compiles slowly. Whatever problem they had come upon, they just applied an STL container on it. Well, it's not Boost but it's still bad. Such a design is guaranteed to be slow because of the insane amount of memory allocations required. Multimap? Seriously? Just use a sorted array (vector)... And the in-code documentation really doesn't help anyone understand what happens under the hood, which is why people actually refer to source code of any API. If this is not enough, just ask me in the comments. 

Since there can be seen standalone "edges" of pixels, it is clearly not a post-processing method. When looking at screenshots in different resolutions, the edge appears to be always one pixel wide. This leads me to a conclusion that they're probably using textures with bilinear filtering disabled and they're doing the filtering manually in a shader, by taking multiple samples from the texture within the screen pixel distance. The way I would implement this is by taking screen-space derivatives of texture coordinates and using multiple blur offsets. In HLSL: 

Lua wouldn't be the most efficient language to use. It's got lots of things you will find it hard to get used to (like array indices that start from 1 and the requirement to handle many different argument setups in each exposed function, as well as lack of IDE function-name-guessing support). It might work well if you can memorize things easily. The reason people say things like "using Lua is more efficient than using only C++" is because Lua is a scripting language so you don't have to wait a lot of time for it to recompile when you change something. Well, the same is true for a C++ project if you don't use too much of STL (limit yourself to string, vector and map and/or unordered_map) and don't use Boost at all and don't use any engines which have lots of those headers included. You can most probably enable a header dump in compile options to see what is included. This is how you can do it for MSVC. Now for the C++ vs. C# problem: people often pick C# as if it's going to save their life but unfinished projects and all sorts of other problems happen either way. For this choice, it's all about what you like to do - whether you like to be closer to metal and fight memory allocation correctness (C++) or have all sorts of things handled for you, if you can endure the extra typing, annoying exceptions and compiler errors appearing in place of simple features like using an integer like a boolean (C#). If it's a project you need to finish quickly, I'd recommend looking at the toolsets available, picking the best (no matter what programming language it works on) and forgetting about the language dilemma. 

Pixel arrays with/without tiling. You should use tiling if you intend to do efficient scaling / interpolation / software rendering since it improves the chances of finding the next accessed pixel still in cache. 32 bits per pixel (with the appropriate alignment) are preferred since there are some CPU-specific optimizations for accessing correctly aligned data. JPEG is lossy, PNG with max. compression is slow. You'll probably need to add some implementation specific compression step, if there is one. 

Find yourself a good terrain generator for the base terrain. Find or create something that would generate a mesh from a heightmap. Find a good modelling application with sculpting tools that you like. If necessary, create converters between yours and whatever the modelling application can import/export. Render the mesh in your game and use your editor for nothing more than selecting a mesh. 

Once you have the collision normal (not the normal of the face because corner collisions won't work right that way), you can just split the velocity (which should really be a vector, not a quaternion) into two: normal velocity and tangent velocity. 

Texture contains dark & almost transparent pixels. Fog is internally controlled by alpha to support alpha blending with fog enabled. You should remove alpha from the texture (or set it to 100% everywhere) if you intend to use the fixed function pipeline. If you have shaders in mesh effects, you can alternatively implement fog differently in those. 

From what I've found out while working with canvas, Firefox is indeed a bit slower than Chrome. The most important problem that I also experience and haven't found a way around is the stuttering (which can sometimes add up to one second)... but that most probably comes from their JS engine, not canvas. Either way, if it works well in Chrome without any special hacks, it's a bug that needs to be reported. 

This would allow you to manage these objects from your components and keep them far enough to allow you render them any way you want. 

The general approach is to replace models with broken versions of those. There are two ways to go from here: manually building the broken versions or generating them. If you work with an artist, the first way is much easier than the second. Some of the modelling apps may even have tools that generate broken versions of meshes. Generation is usually done by adding an extra material, slicing the mesh using planes, triangulating the created hole edge loops and applying the new material on those triangles. However, this sounds much easier than it actually is. For the right effect, the input mesh must have no visible holes (which means that, for example, bottoms of buildings must be closed). The difficulties in designing the algorithms and data layouts may arise in multimaterial setups where each part of mesh isn't closed while everything put together is. Since generation is such an insane thing to attempt, it should only be done on procedurally generated meshes. For everything else, you will need an artist and there's no sane way around that. Physics for everything is done as usual: generate convex hulls (libraries usually have something that will help with this) for each mesh and create the bodies. 

There are no techniques beyond keeping the differences in mind and always looking out for more (testing often on various hardware/software configurations). As for the tearing problem, it's all about VSync. The driver update probably had it turned on by default. Double buffering is just the act of swapping buffers to avoid waiting while the display is redrawn. Tearing occurs exactly because the buffer swap occurs in the middle of the screen repaint operation. To enable VSync in OpenGL, you need to use this extension in Windows, specifically the "wglSwapIntervalEXT" function, interval=1 would mean that buffer swap will wait for repainting of one frame, interval=2 means 2 frames etc. And interval=0 is the default "do not wait unless driver thinks differently" setting. Frame count per second equals your monitor's refresh rate setting. X Window system (Linux, Mac) needs this extension. P.S. You can also try SDL_GL_SetAttribute(SDL_GL_SWAP_CONTROL, 1); Some say it doesn't work in windowed mode though. 

Not true. It's mainly used for programming convenience and hierarchial animation. There is no way culling and collision checks benefit from scene graphs. Quite the opposite, actually, since it's required to calculate and cache the world space data before doing everything else. Bounding volume hierarchies are what you need for culling and collisions, if anything. 

This appears to be a "do my homework for me" type of question. There is no special algorithm or a clear answer. Still, it's a somewhat interesting piece of homework. :) And since there is no special algorithm, I'd implement it using the equivalent of constraint relaxation in physics. All the things you have stated as requirements are rules, or constraints. Which basically means that you can insert any new item anywhere, and then adjust the positions of all items to fit the rules: 

Pass a callback to the function that can check other things from inside the loop and optionally stop it in the middle. Create two new coroutines - one for movement, one for doing things while moving. Yield main thread until either of the two has finished, abort the remaining one afterwards. Turn movement into state, resolve it in parallel (possibly from outside Lua). , then do the additional work in the same thread. 

What's wrong with if-else blocks? If you don't have dynamic VBO types (the post didn't suggest anything like it), it's essentially the best solution. Don't fall in temptation to overdesign the solution - if it works, it's good. Everything else will just equal more code (more work / time spent) with the same results - and I don't think that is what you want. 

Start with the hook. Make the subject appear as exciting as possible. The game should have some kind of narrative that uses the subject-to-teach in the background, while leaving the foreground to the genuinely exciting parts. Use progression and variety to maintain interest. Rewards in the commonly understood sense are too narrow and often, bland. Plot twists, new "weapons" for puzzles, new scenery and interactions - that's where the excitement is at. Similarly, doing the same puzzle (but slightly more difficult) over and over should be avoided at all costs. Repetition is also necessary. Switching between entirely different kinds of tasks and sets of information is mentally tiring. While the content is full of variety, the subject shouldn't be. Associative transitions. Whenever the information taught is changed, it must be done gradually, to help the brain form links between things. Sudden jumps in context are hard to deal with and make things seem pointless - what's the point in remembering something if you're going to move on soon? There can be no time limits. If the player wants to understand something better, let it happen. In a typical classroom setting, everyone is forced to learn in sync with others, which is boring for some and stressful for other students. A computer is not constantly in a hurry, so it's an opportunity to do things right. Subject tree traversal: 

This might not look like an answer but from my own experience - no, I haven't seen any tutorial or book on the subject that is worth mentioning. I started learning it by making a game. When I built my second engine, I started by just picking any book on the subject and trying to reimplement things. I found lots of problems with the ways things were done according to the book on the way and if you want to follow any book, I suspect you will find just as many. So if possible, I'd suggest you avoid all of them as pure examples of design and let your work shape your engine as you improve it while making a game. This doesn't mean you shouldn't read them and think about the things written. It's just that no book will be even 50% right about things and it's what you should keep in mind while you read them. 

Z inversion simply means that the data somewhere is wrong. It is completely unnecessary in lighting calculations and can only do good when all vertex/light positions or normals happen to be scaled by the vector (1,1,-1) somehow. 

Both methods will require writing manual load/save functions for each serialized object. All the usual rules of inheritance and composition apply here (can reuse functions to serialize included objects) so it won't seem that hard in the end. To me, it would most certainly beat writing obscure wrappers and configuration files for a complex system. But I suppose you've got to try both things to see what you like more. 

Fake HDR, exponent, alpha channel. It only matters for a specific HDR format where, as mentioned above, .. 

It is a good enough data structure. For any problems you're having with it, I'd suggest you just run your app in debug mode. If you're doing some byte-fu (copying/modifications) with vector's data, you should check that before everything else. Otherwise, vector has enough assertions placed to let you know where the problem is. 

Problems like these are usually solved with something called a "context" variable - an "umbrella" structure that keeps all session-local state (variables that could also be global) inside it. Whenever you need something from it, you pass the context or "make it current" (set a global variable that is a pointer to a context). That way, you have at most one global variable instead of many. However, since this method is widely accepted and used despite globals being hated all around the world, this is somewhat close to hypocrisy. So until there's a real need for such workarounds (platform limits or the requirement to have multiple sessions/states), using globals is just fine. Using too many of them can make code hard to understand, that's about the only problem you could bump into so far.