You have to install management tools from sql server 2005/2008 or 2008R2 to manage sql server 2000. Profiler from sql server 2014 wont work. There were problems from 2014 using profiler to 2005 - this is fixed in 2014 CU2. 

Since you have not brought the other file group online, when you try to query TAB1_lamb1 table residing on lamb1 filegroup, the query processor will throw an error. 

This normally occurs when the log-shipping is not removed cleanly. To remove the orphan entries you have to delete them from msdb logshipping tables. 

Brent Ozar folks already have got you 50% - Inserting sp_Blitz® Output Into a Table My steps would be : 

and then use tsql Note: If you want HTML email to be send out, I have a script for that to help you out. 

If logshipping paths are same on the primary and secondary servers then adding an will be seamless. If the paths are different on primary and secondary servers then 

Also, look at Generate a SQL Server Instance Inventory. It has all tools available for scanning sql servers on network. 

Suggest you not to remove people but disable their login or just revoke access. Then after a full business cycle, if no one complains then remove those users. 

The same might be selected for source tables. Check the package, you will find the incorrect setting. 

Since you want to use BPA, this PowerShell script will do the work for you. Another thing to be aware is that you can use below freely available excellent tools for checking health/best practices for SQL Server. 

You can do concurrent full and log backups BUT the log is not cleared when the log backup ends. The clearing of the inactive portion of the log is delayed until the full backup completes. Also, the first log backup restored after a full or diff needs to have the data backup’s lastLSN + 1 contained within its LSN range. Refer : backup myths 

The second step is to run the SSIS package only if the above condition is FALSE. If the above step is true meaning if it is a holiday as per your HOLIDAY Table, then it should silently fail. 

Note: If you are going to use any formula or online calculator to calculate SQL Server memory configuration then best is to read Beaware of Wow… An online calculator to misconfigure your SQL Server memory ! - by Jonathan Kehayias first. Some good references : How much memory does my SQL Server actually need? - by Jonathan Kehayias 

If you go into details of how that will work - the log reader agent's default behavior is that it will not process any log records until those records have been hardened across all AG replicas. will tell replication that if it is not able to connect to the original publisher, connect to the AG Listener allowing the subscribers to reconnect to the publisher after an AG failover. Run at the distributor : 

Total Server Memory: Amount of memory currently allocated to Buffer Pool and not the total amount of memory to SQL Server Target Server memory: Ideal Size of the buffer pool corresponding to max memory for the instance. Note: If Total Server Memory > Target Server memory, then it suggests memory pressure. Below script will help you find LOW or HIGH Memory notifications from - system health session : 

I normally set database in single_user and then waitfor delay and then set the database back in multiuser as below : 

You have to have sql agent log output to a file as below or use T-SQL to find out from jobhistory table: 

It would be difficult to do that using Native tools or you will have to look for third party tools. Natively: 

As @RLF mentioned to initialize replication from backup. There is one another approach as below. Kendal Van Dyke blogged about the second approach at Reduce Transactional Replication Synchronization Time By Applying Snapshots Manually which is excellent and is a time and life saver. Its a bit of manual process, but I have dealt with it and its a win-win approach. 

If your company is willing to pay, I would suggest you to have a subscription for Pluralsight. Its worth for your company or as an individual as well its worth taking it. (I am speaking from my experience, since I have the subscription and its worth quality investment). It has courses from SQLSkills team and other people which teaches you a lot about SQLServer - internals, XEvents, Query plans, troubleshooting methodology, powershell, etc. As a side note, certification will give you a level of confidence to you and your employer, what matters is hands on experience. 

On the activity on the primary server which will be the Publisher server. Also, if the distribution agent on the same or different server matters a lot as it will be doing the heavy work of delivering the transactions of the articles that are published by the publisher server. On the geographic location of the Publisher - subscriber servers due to the limitation of the connectivity between the 2 regions (if geographically distributed). On how many articles and the the amount of data you are publishing. Remember if for some reasons, the articles subscription expires, then you have to reinitialize the subscribers by a snapshot and the amount of data will affect it. Only replicate what you need. Dont replicate the entire database. 

2) How can I find out the last command of the sleeping process? You can use Adam Machanic's sp_whoisActive tool to find the lead blocker. You can also use DMV's (starting sql 2005 and up) to find out what the spid was executing : 

Again completely baseless or you might have misunderstood it. Stored procedures can have their plan complied and stored in procedure cache and the plan can be reused. I don't think you can have priority of the kind you have mentioned. 

There is no direct way to set/change the ownership as a part of CREATE or ALTER DATABASE statement. If you want to enforce that, you can do this by adding a post deployment script which calls sp_changedbowner. This article describes that in detail. Similar approach can be applied to set the initial file size and growth Rate as well. Also, as a note: sp_changedbowner is now depreciated, so it is better to use ALTER AUTHORIZATION 

You can still see (atleast in SQL Server 2008R2) using , and you can set the database option as well 

Based on my notes from SQL Server internals (by Kalen Delaney).... below is my understanding Query Governor Cost limit is the max no. of seconds that a query can run. So, essentially : 

If you are not comfortable using Powershell, then you can go for native service broker functionality. 

I use to esp. avoid prolonged blocking. Refer to this gold mine blog post - Break large delete operations into chunks by Aaron Bertrand. Also, Take Care When Scripting Batches is very useful blog post. 

There is another way that you will find really useful as described excellently by Chad Miller on SSC - Database Space Capacity Planning. He also focuses on which is very useful. 

You can use sp_whoisactive to find out what is holding lock on the database or what SPID is rolling back. You can issue : 

Note: I have highlighted in the image what each column means and what you should look for. Basically, you have to monitor your autogrowth events for a duration of time e.g. during high activity or for you entire business cycle and then averaging it will give you a some what exact value that you can choose for autogrowth settings. Now, for Log file, you also have to consider factors like Index maintenance, CHECKDB running, etc. So size the log file to support the volume of data changes occurring in the database and take log backups frequently so as to allow rapid reuse of space within the log file. Also, worth to mention that you should enable Instant File Initialization as well. Works only for Data files ! Refer to Importance of data file size management esp the Data file growth and Data file shrinking by Paul Randal. Note: Do not shrink your database, unless you do a massive purge of your data and you are sure that the database is not going to grow that big again. It causes fragmentation and databases are meant to grow ! 

As I show , I have server database version-10.50.1600 and local database version-10.0.1600 . How can I copy database (with all tables and data(s)) from server to local which has different version ? Since you are trying to load data from higher version to lower version, this is not supported. As others have pointed out to script out database objects + data, depending on the database data size, I would take a decision. If the database size is few MB then, that is a good approach. If the database size is huge or in GB, then the generated script wont be either handled by SSMS nor SQLCMD. Below is my suggestion : 

Best is to use SSIS (data Import/export) to do this. It has the functionality to just transfer selected tables from source to destination. Optionally, you can even save the package so that it can be reused for future/repeated transfers. EDIT: Adding more screenshots for clarity. 

Your best bet is to use Redgate's data compare to compare and insert data in the database your application is pointing to. I have used this method in similiar situation and trust me ... this tool is a life saver. You can dowload a 30 day full trial version and then purchase it if it suits your needs. 

Above answer covers this question. A failover to 2016 will break replica sync status. Once you upgrade to higher version, the boot page gets upgraded which prevents tlog restoration of higher version to lower version. 

A word of Caution: You should not use as it is already deprecated. Instead use . Check out BOL reference here. 

Instead of carving out your own solution which clearly is not smart e.g. exclude offline db, take log backups etc is not good .. use Ola's backup solution. Also, make sure you test your restore plan by actually restoring the dbs and trying a point-in-time recovery where possible. 

Since you now have an automated mechanism in place that will fire up when a new database is created, you can use ALTER AVAILABILITY GROUP and ALTER DATABASE - SET HADR basically you have to just include : 

Configure Database Mail Create a sql agent job to run below t-sql (you dont need a stored procedure for what you are asking, but below code can be easily converted into a stored procedure --> I will leave that up to you - hint: just look for ) 

I would first concentrate on the Autogrowth settings for the database as too frequent and incorrect autogrowths can cause massive internal log fragmentation affecting any processes that reads transaction log file (e.g. replication, CDC, backups, etc) and there-by soaring the number of VLF's. Remember that the log file has to grow as it cannot be ZERO initialized and hence if the autogrowth settings are not appropriate, then SQL Server has to wait till the auto growth event finishes. Now, if you are using T-Rep, the log reader agent will read the log when replicating changes from the publisher to distributor and then the distributor agent will distribute the changes to the subscribers which are called "pending transactions". Once the changes are replicated, the log entry is marked as "replicated". The problem arises when the log reader becomes slow or it gets delayed leading to accumulating "pending replication" commands for long periods and they will remain part of active log and hence VLF cannot be truncated. You can check the log_reuse_wait_desc column in sys.databases which will be showing "Replication" as the problem. Note: Merge replication and snapshot replication do not affect transaction log size. If a database includes one or more transactional publications, the log is not truncated until all transactions relevant to the publications have been delivered to the distribution database. If the log file grows out of control, a common technique for T-Rep is to use sp_repldone command to mark all the log records currently waiting on the log reader as replicated and then you have to re-initialize the subscribers. Note: Even if you try to switch to simple recovery model thinking that it will truncate the log, it wont work as replication is supported in SIMPLE recover as well, and the log wont be able to get truncated as the log reader agent will need them to process. This brings us to doing Proper Log Management - size the log file to support the volume of data changes occuring in the database and take log backups frequently so as to allow rapid reuse of space within the log file. Best is to monitor your Transaction log along with other monitoring that you might be normally doing using PERFMON and DMV's. Just and FYI, sys.dm_db_log_space_usage is a new DMV in SQL Server 2012 that will get basic log size and space info. References : 

Refer to BOL for more details. You can Lists the accounts associated with one or more Database Mail profiles using sysmail_help_profileaccount_sp Below is the t-sql that will give you all the details : 

Ideally, it should not take 60 mins , but again it depends on what you are doing. If you are pausing and then stopping sql server then it probably might. If your entire goal is to stop sql server, then I would suggest you to (in maintenance window) : 

The best way is to install SSMS plugin - SSMSBoost add-in and script data in excel note: the plugin is free - community edition and has a prof. license as well. I have used the community edition and it works just fine - only thing is that you have to re-register the community edition after 30 days. 

Now for creating memory optimized tables you have to create table with Getting Started with SQL Server 2014 In-Memory OLTP will give you all the detailed steps to get you started. For more details, check this BOL article : In-Memory OLTP (In-Memory Optimization) 

Note: I have not tried the SQLDump, as I would personally prefer SSIS or T-SQL along with BCP and BULK INSERT - to move data out and in of sql server. 

-- since you want to restore the database to 1st backup and you already restored the sequence, its time to bring the database back online with 2 records. restore database [FGTest] with recovery 

Why rebuild all indexes on a table.. instead choose the index fragmentation threshold and only touch the indexes that have high fragmentation count with page count > 5K (for e.g.) ? Use Ola's index maintenance e.g. and parameters or Minion Reindex (both are free to use - I have use Ola's Index but not minion reindex solution). See my answer for more details. 

You should stripe your backup along with compression. Copy-dbaDatabase does it for you. It stripes into 3 backup files which is sufficient in most cases. Also, you will need to restore the database 

DBCC CHECKDB is vital for SQL Server databases to be 100% sure that there is no corruption. However, due to databases growing massive in size, its very hard to find a maintenance window when you claim to be 24x7 up. Over the years, SQL Server team has implemented various mechanisms that will detect most common forms of corruptions especially related to Physical corruption caused by hardware. SQL Server 2005 and up has PAGE_VERIFY = CHECKSUM which can help you proactively detect physical corruption in database pages thereby adding a checksum to each page as it is written to the I/O system and validates the checksum as it is read from the disk. Also, backup (full or differential) with CHECKSUM will guarantee to detect any I/O corruption caused by hardware. Hence, from the hardware side of corruption, SQL Server does a good job of detecting it and reporting it. (Make sure to set important corruption related Alerts as well). That being said, still logical corruption, scribbler induced errors - where in-memory pages are corrupted either by third-party code running inside the SQL Server process or by drivers or other software with sufficient privileges executing in Windows kernel mode and/or SQL Server Bugs, etc are undetectable using above methods and hence CHECKDB comes into picture. DBCC CHECKDB performs a more thorough checks that includes checking page headers for possible corruption that are not detectable by any other means. 

Yes the restore will work just fine. Every edition of SQL Server 2008 and later can restore a compressed backup. Refer : Backup Compression (SQL Server) for more details. 

System databases cannot be mirrored or logshipping. The best way is to script and sync up desired objects e.g. logins, jobs, etc. The best tool that I have used so far for migrating databases, jobs, agent schedules, operators, logins, etc is powershell based dbatools.io 

You don't have to write a cursor tsql script to check for new database created and schedule it to run for e.g. every minute. Instead use EVENTDATA() function in conjunction with server level trigger. 

Use fast load option on OLE DB destination and experiment with batchsize option and find your optimum number. Consider dropping the indexes on target table and recreate them after loading the data. For large amount of data, dropping and recreating indexes is faster than inserting with indexes. If the package is running on destination server itself, use SQL Server destination transform. Its considerably faster than anything else. Since Integration Services is all about moving large amounts of data, you want to minimize the network overhead. This means that the value 32K (32767) is the fastest option. DO NOT DO IT AT Server Level using sp_configure, instead adjust in the connection manager properties as described in the best practice article (Tuning your network part).