DevOps fundamentally is an arbitrage play: it exploits the idea that if the things you are developing and operating are known to be stateless and immutable and ephemeral you can move very quickly, because there are a bunch of things you just don't need to do. And this is true, you can, and you don't. But "traditional" IT is all about managing statefulness and controlling the transitions between states (or "mutation" if you prefer). A database server, a file server. A database server for example: once you have made changes to a schema and data has gone in, you can no longer "roll back" because you probably have nowhere you can put that data that satisfies the constraints of all the stakeholders. If the database server goes down you can't just spin up a new one, it has to be exactly the same to the greatest extent possible, and you need to have a plan for resolving the deltas. DevOps doesn't really address this and even worse, the typical DevOps pitch includes some jabs at "traditional" IT for being slow and obsolete, without any acknowledgement that maybe there were reasons for things to be done the "old fashioned" way other than the practitioners being dinosaurs. What that sounds like to a stakeholder is that DevOps people are willing to take risks with the company's most precious assets - customer records, accounts, inventory control, all the other things that typically reside in the stateful parts of the technology infrastructure. Therefore to work with and bring on board skeptics the very first thing you must do is convince them that you have mitigated all the risks of operating the stateful parts of the infrastructure in a DevOps fashion. I will wager that some variation on this theme is the root cause of TSB's woes at the time of writing. They failed to exercise sufficient care with the stateful parts of their system. 

resulting in the following output; obviously there is a misplacement for the username field in the command sequence. 

Imagine that a Maven POM configures a set of integration tests for web apps (could be a functional test as well). 

As you can see, in the first RUN statement I can list tables I have created in a new database, but in the second RUN statement the database is gone. Note. Normally, you would not transport your application data in production containers (immutability principle) but for demo and test purposes this can be nevertheless useful. 

Eventually, using the setting for the client I have found the log outputs (apparently currently there are no debug level messages implemented as reported. 

I have found this picture in a comment to this Reddit thread and would like to better understand its message. I do not know the image's original source, maybe it will come up later. 

I consider, the design is that a custom network is not automatically mapped to host network. What could be the problem with the SLES system? How to debug it? UPDATE Firewall does not seem to be a problem here: output for shows everywhere Ingress mode is active (no difference with one host); shows a live container listening on the proper tcp port. 

Mostly for devs but knowledgeable to others with the "disaster girl" meme: "Works on my machine.. Ops problem now!" This illustrates that lack of responsibility can endanger the whole company, and the value of software working only in a specific environment is not absolute. 

Docker daemon allows, according to documentatio, using memory storage driver for its images. Has anybody any daily business experiences with that? 

In this example Microsoft refers to a ".NET-nanoserver" image. The base image is there a ".NET runtime" - I consider this is something else than an OS base image. 

Let us say I do and within the section I edit the JSON to add . I save this and verify that it has taken effect by issuing , and I see it there. How would I then use this value from within a recipe? I would have expected to just be able to say when running the recipe on but that just returns nothing. The usual Ohai attributes such as obviously work as expected. 

Not impossible, very easy in fact with read-only routing. Clients intending to be read-only specify it in their connect string, so this is per-session rather than per-statement. You can have up to 9 replicas in an availability group. With modern hardware and storage, that goes a long way to being “scalable” - 10’s of thousands of concurrent connections easily. 

The author of that article has misappropriated the term "systems thinking" which has its origins in biology and sociology with Ludwig von Bertalanffy and his General Systems Theory (1968), and was later applied to cybernetics (in its original meaning of feedback loops in living and nonliving contexts - cells, machines, organisations, before "cybersecurity" and similar buzzwords hijacked it). In short you cannot learn about it because it is not a thing that exists in the way he uses it, just something he made up to sound clever - but if you are interested in real systems thinking, I'd say start from GST and work your way forwards. (Source: I have a Master's in Systems Analysis, before that term too was purloined to mean "IT support") 

I know about the resource obv, but in the interests of good system hygiene I would like to avoid as much as possible duplication, so I would like to share Gems between the embedded Ruby within Chef, and the system-wide one. Additionally I have become aware of a troubling situation: the Gems I have in the system Ruby come from the known-good local Yum repo and are installed via RPM over the LAN whereas goes off to the public Internet, and recent events in the Python community have made me very concerned about this. I have tried setting in my recipe both before and inside a but this appears to have no effect, I get a failure on . What am I missing? 

Source code version control system Build agents Automation and packaging scheduling system to encapsulate artefact composition logic A binary repository (different classes of binaries: libraries, distros, container images) A configuration repository for different environments Package and configuration distribution system to encapsulate deployment logic 

Every CI tool defines more or less "their" view on how integration and delivery can work, and every DevOps team (re)invents their specific way to support the business using these tools. But, how could you to define and execute high-level business logic? Here are two examples from daily work: 

Imagine you have decided to abstract from exact service location. Normally, you have a cody world with the service name=DNS. Say, you have prototyped something with S3 compatible API like exoscale/pithos. But then you think okay this was actually a mockup and now you go for real S3 in production. Would you rewrite the stack, trick around with or is there some more elegant way to proceed? 

These components have their bugs as well! Acceptable answer: at least one example of a GitHub project with working code going in this direction which is more than 6 months old but with updates within this timeframe (Maven or Gradle both ok). 

To paraphrase - what can a DevOps Engineer do himself/herself to feel less like a lone wolf? Lack of culture and management support is only one part of the equation. The other part is in my opinion that the detail DevOps knowledge often refers to complex contexts and it is important to have advice and reference to working examples. Therefore - do not feel like a lone wolf; participate in DevOps communities like this one here or tool-specific groups and GitHub - the feeling is at least then you are not the only lone wolf ;-) 

Do a thing on node A capturing the output Do another thing on node B with that output Back to node A now for some operation Now on node B again ... 

The tool you need is Packer using Docker as the "builder" and Chef as the "provisioner". Then you can add the resulting image to your repo and reuse it without having to pack again, until your recipes change. 

I have an application I would like to configure using Chef that spans multiple nodes. Let's say that the process of doing it consists of 

Microsoft in collaboration with EdX is running a comprehensive DevOps course. It covers Chef/Puppet for configuration management, Selenium for testing, Docker, Nagios, Loggly etc. Obviously being Microsoft they use Azure, but all this tooling is Open Source and so the skills are very transferrable. All the courses are free, you only pay if you want the Certificate. 

One way to do this would be to write a recipe that stored the stage it was on on each node and just keep running it on both nodes repeatedly until eventually it had a chance to do all operations on all nodes. But this is clumsy and won't scale if there's node C, D, etc. I obv know about notify to sequence dependencies within a single node, but that won't work across multiple nodes. I can't be the only person who needs this, so is there a mechanism or a design pattern/best practice/TTP for this style of activity? 

Use a Git post-receive hook to issue a in the directory you want to keep updated. You might want to mount the directory you want to keep updated locally to the Git server, or execute that command remotely with into Git Bash on Windows, or Powershell. Alternatively have the hook call a webhook and have the webhook execute the . The webhook method will be easiest with BitBucket, I don't know about the others. 

Two example integrations of Docker and Kubernetes are OpenShift and Rancher Labs AFAIK. Sadly but in fact we have not completely escaped us from the dependency hell. Question: is there an established source of information which distributions here package which versions (like Ubutu/Debian version chronicles on Wikipedia)? Background. Not-so-obvious facts for newbies (judged by my learnings so far) are (defining acceptable technical usability to a level that you do no need hours to debug even 101 tutorials): 

In Atlassian Bamboo, in script tasks you can use special Bamboo environment variables e.g. for current build number 

Back in the Java world.. Maven and Gradle are quite advanced tools offering sophisticated configuration capabilities. For example, you might want to have a fast-fail build stage in your CI to find compilation errors but then be able to run an extensive series of acceptance tests, maybe not on every commit. How to model this in Maven and are there maybe already available resources? Requirements: 

Honestly, I have even no idea how to interpret this message. Any ideas?.. UPD. nmap results for ports 2377,4789,7946 (hint from Felipe in a comment below) - no idea what are they good for but now it is clear that something is wrong with host C. Host A (Primary manager) 

That is okay for a quick test in a secured and trusted environment but it is a potential security breach. Do not try this at home. The real gotcha comes then in if I would like to use the Atom editor. Somehow I cannot find an option for the git executable. 

Both Docker Registry and Docker Engine have API interfaces. I consider it should be possible to implement a Docker plugin for this integration to have images scanned for example as somebody pushes them. (side note - Docker registry is an image registry, not container registry). This scanner functionality is also possible through the commercial Docker Enterprise edition. $URL$ UPD here an example with clair/docker, looks quite simple $URL$ 

Where does not exist yet because it is installed by . This will fail at recipe compile time for that reason, but will obviously work at convergence time providing the package installs successfully (and if it doesn't then obviously the recipe has failed anyway). This also fails if the package is installed be a previous recipe in the runlist since they are all compiled together upfront. How do I include things in a Chef recipe that that recipe or runlist installs itself? 

Firstly, because leadership is not very much to do with any particular technology, but about having a deep understanding of the problem that the team is writing software to solve, and about having great interpersonal skills: mentoring, conflict resolution, self-awareness... A person who thinks it is about technical ability alone lacks at least one of those. Secondly, let's be honest - the vast majority of "full stack" developers really only know JavaScript, and even then, only in the context of applying a pre-written framework to a well known problem, like an intranet or a shopping cart. A person who is seasoned in C++, or the major RDBMS's, or similar traditional skills will not find this impressive and will not take leadership from someone who believes that they are a master of all skills in the stack - unless that person swallows their ego and adopts a more realistic title. 

The answer is: a bit of both. To satisfy the constraints of "use git" and "manage a vast codebase" Microsoft developed a new filesystem (previously they were using a variant of Perforce called SourceDepot). It's open source but I have no personal experience of using it. Why would you want a monorepo? The most obvious reason is that you can modify an API and all the callers of that API in an atomic commit. Also there are advantages to being able to do a search across the entire codebase...