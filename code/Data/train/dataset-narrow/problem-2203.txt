Total cost: The total number of edges visited by the semi-tour, counting multiplicities. Width cost: The number of times the most-visited edge is visited. 

In coding theory, the quantity you are looking for is called $A_q(n, d)$, where $n$ is the length of vectors, $d$ is the minimum distance between them, and $q$ is the alphabet size (omitted when $q=2$). Characterizing $A_q(n,d)$ is a challenging open problem (with many basic questions remaining unanswered) but various asymptotic and non-asymptotic upper and lower bounds are known. See Chapter 17 of the book "The Theory of Error-Correcting Codes" by MacWilliams and Sloane for a summary of the most important ones. 

This is a beautiful research question with several facets to it, and there are different ways of formalizing the question depending on whether by extractor you mean seeded extractor or seedless extractor and whether by PRG you mean PRG for Boolean circuits or a more specialized family (e.g., epsilon-biased spaces). Here's a few informal thoughts off the top of my head (but not a full answer): 

Of course if you show a lower bound of $\Omega(n \log n)$ for your question, it would imply the same lower bound for the unrestricted model, which is not known. On the other hand, you are asking whether it is possible to compute the Fourier transform where each $x_i$ is known within $b$ bits of precision using $\ll n \log n$ arithmetic operations. If true, by the fact that the Fourier transform is a unitary operation (and thus, perfectly conditioned), we would be able to compute Fourier transform of any vector within $b$ bits of precision much faster than FFT, which is certainly not known either. 

One reason we see Max-CSPs more is that they often capture convex optimization problems for which minimization is easy (e.g., Max-CUT vs Min-CUT). A Min-CSP is equivalent to a Max-CSP when you negate all the constraints. So minimization and maximization are equivalent if you're asking for the optimum solution. But this reduction is not approximation preserving in the sense that, say, a constant factor approximation of the negated CSP does not necessarily imply a constant factor approximation of the original CSP. Approximation of Max-CSPs and Min-CSPs can however be studied under the unified framework of "generalized CSPs", when constraints have weights and can be fractionally satisfied. Prasad Raghavendra's thesis is a good resource about this. The specific problem of Vertex-Cover is not really a CSP in the standard sense. However the approximation of the problem is captured by the so-called "free-bit complexity" of PCPs. See "Free bits, PCPs and non-approximability -- towards tight results" by Bellare, Goldreich, and Sudan. 

In 1996, a long-standing open problem was solved by a computer; namely, that Robbins algebra and Boolean algebra are the same. The proof was found by an automated theorem prover. Moreover, the known proof of Four color theorem contains computer-generated components. The aim of this question is to list proofs that are (completely or partially) found by computer (whether the only known proof or the one discovered for the first time). 

There are known worst-case hardness results about ML decoding (for general and specific families of codes such as Reed-Solomon), computing or approximating minimum distance of codes, and so on. However there is a great room for improvement in these directions and several seemingly intractable problems are not analyzed yet. There are coding theoretic problems that are conjectured to be intractable, such as problems related to decoding random ensembles of codes. Such intractability assumptions are related to hardness of learning noisy parities and also form the basis of coding-theoretic public key cryptography (which is a big area of research by itself). For a good account of the latter, see Lorenz Minder's thesis. Finally, there's a survey on tractable algorithmic problems in coding, including recent developments in list decoding: Algorithmic Results in List Decoding. This is a good resource to quickly learn about basics of coding theory. Also for learning the basics, Luca Trevisan's article "Some Applications of Coding Theory in Computational Complexity" may be useful. 

One possible definition for a given point and ϵ would be the following: perturb each coordinate of the input independently with a Gaussian with mean 0 and variance $\epsilon^2/n$ and then look at the variance of the function's value with respect to this input distribution. It's also possible to perturb a specific coordinate this way and keep the rest fixed, depending on your application. You may also find this paper of interest: "Noise stability of functions with low influences: invariance and optimality" by E. Mossel, R. O'Donnell, K. Oleszkiewicz. 

If the optimization problem you have in mind is MAX-3SAT, it's not only APX-hard but also approximation resistant, in the sense that whenever all clauses have exactly 3 literals, it is hard to satisfy much more than 7/8 of the clauses (even if the instance is satisfiable), something that is trivially achieved by a random assignment. When some clauses may have less than 3 literals, there is still an algorithm that satisfies a 7/8 fraction of the optimum and that is the best you can do assuming P $\neq$ NP. See the paper "Some optimal inapproximability results" by Johan Håstad. 

The precise answer to your question is given by Oded Goldreich in his article "Bravely, Moderately: A Common Theme in Four Recent Works". Here is the link: $URL$ 

I think you can't beat the $\exp(n^{\Omega(1/d-1)})$ lower bound. Here's a construction that should work for computing parities: In order to compute parity of $n$ variables, partition them into groups of size $t \approx n^{1/d}$, compute parity of each group and recursively compute the parity of parities. That is, start with a parity tree of fan-out $t$. The depth of this tree would be $d$. Then, replace each parity gate with a CNF or DNF or size $\approx \exp(t)$. This way you get a circuit of depth $2d$. It is possible to alternate between CNFs and DNFs at different levels so the circuit collapses to depth $d$. This way you get a depth $d$ circuit of size $\approx \exp(n^{1/d})$ that computes the parity of $n$ bits. Now, since switching lemma is tight for the parity function, you shouldn't hope to get a better lower bound for more sophisticated functions just relying on that. 

Since you are accessing $f$ in a black box manner, you have to query $f(x)$ at all $2^n$ points. Since otherwise, you can design two functions $f_1$ and $f_2$ that are consistent at all query points yet have different number of zeros. 

"Long code" and "Dictator code" are two different names for the same code. Here's why: Let's start with the natural definition of the long code: The message is an $n$-bit string $i$ (the reason I'm calling this $i$ and not $x$ becomes clear soon) and the encoding is a vector $y$ of length $2^{2^n}$ where the positions of $y$ are indexed by all possible Boolean functions that map $n$ bits to one bit. Moreover, $y$ at position $f$ is simply $f(i)$. Now you can alternatively identify $f$ by its truth table. So you can also assume that the coordinate positions of $y$ are indexed by all possible strings of length $N := 2^n$ (i.e., all possible truth tables) and then $y$ at position $f=(f_1, \ldots, f_N)$ just encodes $f_i$. So the $i$th codeword is the truth table of the $i$th dictator function (among the dictator functions mapping $N$ bits to one bit). So the long code and the dictator code are the same. In the long code interpretation, codewords correspond to $n$-bit points and codeword positions correspond to $n$-bit predicates. In the dictator code interpretation, codewords correspond to dictator functions on $2^n$ bits and codeword positions correspond to evaluation points. 

If we have combinatorial expansion, then the second eigenvalue (without taking absolute value) is not too large (or, equivalently, second eigenvalue of the Laplacian is not too small) no matter what (that is, regardless of bipartiteness or having self-loops or even all vertices having the same degree, if notions are defined properly). See the proof of Cheeger's inequality, for example here: Spielman's lecture notes on Cheeger's inequality. 

Well, if any constraint is loose you can just decrease the corresponding $\xi_i$ until the constraint becomes tight, in which case you are only decreasing the objective value and get a better solution. 

We use Cauchy's Residue Theorem from complex analysis as the main technical tool in our paper "Approximating Linear Threshold Predicates". 

This problem is sometimes called CSP(Maj), and is a constraint satisfaction problem where each constraint is a Majority predicate on 3 variables. The problem is in P, as a special case of the following result: T. Schaefer. The complexity of satisability problems. In Conference record of the Tenth annual ACM Symposium on Theory of Computing, pages 216--226, 1978. However, PCP theory shows that the optimization version of the problem Max-CSP(Maj) is inapproximable within some constant factor. 

Set $\epsilon'_1 := 1/2 - \epsilon_1$. Run $A$ independently $T$ times. If the fraction of accepts exceeds $\epsilon'_1$ (falls below $\epsilon_2$), then accept (reject). Otherwise output a random guess. Use Chernoff bounds to show that the probability of ending up with a wrong value is exponentially small in $T$, more precisely the error probability can be upper bounded by $\exp(-T |\epsilon'_1 - \epsilon_2| )$. Therefore as long as $|\epsilon'_1 - \epsilon_2| \geq 1/\mathrm{poly}(n)$ (in other words, $\epsilon_1 + \epsilon_2 \leq 1/2 - 1/\mathrm{poly}(n)$), you can choose a polynomially bounded $T$ to bring the error probability below $1/3$. 

This is really a false belief in math, but comes up often in TCS contexts: If random variables $X$ and $Y$ are independent, then conditioned on $Z$ they remain independent. (false even if $Z$ is independent of both $X$ and $Y$.) 

It's somewhat like shopping for shoes, before you go shopping and try your luck it's hard to tell what you will end up with. That said, there are some nice blog posts about this. For example I found this useful back in grad school: Finding Problems to Work On. 

This answer is based on the idea of Dana in her answer above. I think you can construct such a matrix using two-source lossy condensers. Fix $\delta = 0.001$ and say $N=2^n$. Suppose you have an explicit function $f(x,y)$ that takes any two independent random sources $(X, Y)$, each of length $n$ and having min-entropy at least $k = n(1/2 - \delta)$ and outputs a sequence of $n' = n/2$ bits that is $\epsilon$-close to a distribution with min-entropy at least $k'=n(1/2-3\delta)$. I think you can use standard probabilistic arguments to show that a random function satisfies these properties (with overwhelming probability) if $2k > k'+\log(1/\epsilon)+O(1)$. To probabilistic argument should be similar to what used in the following paper for lossless condensers and more general conductors: M. Capalbo, O. Reingold, S. Vadhan, A. Wigderson. Randomness Conductors and Constant-Degree Expansion Beyond the Degree/2 Barrier In our case, we set $\epsilon = 2^{-k'}$, so we are sure about the existence of the function that we need. Now, an averaging argument shows that there is an $n'$-bit string $z$ such that the number of $(x,y)$ with $f(x,y)=z$ is at least $2^{1.5 n}$. Suppose you know such a $z$ and fix it (you can pick any arbitrary $z$ if you additionally know that your function maps the fully uniform distribution to a distribution that is $O(2^{-n/2})$-close to uniform). Now identify the entries of your $N \times N$ matrix by the possibilities of $(x,y)$ and put a $1$ at position $(x,y)$ iff $f(x,y)=z$. By our choice of $z$, this matrix has at least $2^{1.5n}$ ones. Now take any $2^k \times 2^k$ submatrix and let $X, Y$ be uniform distributions on the picked rows and columns, respectively. By the choice of $f$, we know that $f(X,Y)$ is $\epsilon$-close to having min-entropy $k'$. Therefore, if we pick a uniformly random entry of the submatrix, the probability of having a $1$ is at most $2^{-k'}+\epsilon\leq 2^{-k'+1}$. This means that you have at most $2^{2k-k'+1} = O(2^{n/2 + \delta})$ ones in the submatrix, as desired. Of course coming up with an explicit $f$ with the desired parameters (in particular, nearly optimal output length) is a very challenging task and no such function in known so far. 

I suppose you can easily achieve that for virtually any problem of the type you describe. Trivial example: Suppose objects are strings, and any $x$ is equivalent to only itself. Determining whether two elements are equivalent is always easy (it is simply equality). However, you can define $f()$ as your favorite injective hard function. 

The gap version of the closest vector in lattice problem is the following: Given a basis for an $n$-dimensional lattice and a vector $v$, distinguish between the two cases where there is a lattice vector at distance at most one $1$ from $v$ or when every lattice vector is $\beta$-far from $v$, for some fixed gap parameter $\beta > 1$. When $\beta = \sqrt{n}$, the problem is in $\mathsf{NP} \cap \mathsf{coNP}$ and thus unlikely to be $\mathsf{NP}$-complete (and is conjectured to be outside $\mathsf{P}$). This case is of central attention for lattice-based cryptography and the related dihedral hidden subgroup problem in quentum computing. When $\beta$ is much smaller, say $\beta = n^{o(1/\log \log n)}$, the problem becomes $\mathsf{NP}$-hard. 

Usually the main idea of a paper is simple. However it is often the case that the translation from the simple idea to mathematically rigorous proofs adds quite a bit of minor technical details, which contribute to the difficulty of understanding the paper. If you ask an author to explain their result in person, they are often able to do so pretty efficiently on a black board or even over a quick lunch. Ideally a paper should explain every mathematical proof in simple and plain English in addition to the formal terms. Alas few researchers put that into practice for several reasons, most notably time constraints and space limitations for the conference version and the fact that our journal papers, if they exist, are typically minor revisions of the conference submission (basically, with the omitted proofs back in the text). There are other factors mentioned by others, for example the unfortunate fact that technically difficult papers look more appealing to some reviewers. 

The best known algorithm for computing the edit (a.k.a. Levenshtein) distance between two strings of length $n$ takes $O((n/\log n)^2)$ time: William J. Masek, Mike Paterson: A Faster Algorithm Computing String Edit Distances. J. Comput. Syst. Sci. 20(1): 18-31 (1980). 

As far as I know, people conjecture that for Polar codes and any fixed DMBSC (discrete memoryless binary symmetric channel), $\log M \leq nC - O(n^{1-c})$ for some absolute constant $c > 0$ and vanishing error probability should be possible (or in other words, in order to be $\epsilon$ close to capacity, only a polynomially large $n$ in $1/\epsilon$ would suffice. However, there is no rigorous published proof available at this point. There are rumors about an upcoming proof, but that's not official yet. 

Well, you just need to consider the finite automaton corresponding to the language as a directed graph and output the BFS tree of length $n$ starting from the initial state. This algorithm should take linear time in the output length. 

Under believable hardness assumptions, it is possible to construct pseudo-random generators that are strong enough to deduce all of the assertions P=BPP, P=RP, P=co-RP (and thus, P=ZPP). You can find the details in Madhu Sudan, Luca Trevisan, Salil Vadhan. Pseudorandom generators without the XOR Lemma. 

Usually the question is interesting for constant alphabet sizes, since otherwise Reed-Solomon codes obviously achieve the Singleton bound. For constant (but still large) alphabet sizes, there are explicit codes that "approximately" achieve the Singleton bound, and are thus also asymptotically good. See Venkatesan Guruswami and Piotr Indyk. Linear time encodable and list decodable codes Proceedings of STOC 2003. Also there are several flavors of Expander and Concatenated Codes (rather than the famous constructions) that are asymptotically good. For example see "Construction of asymptotically good low-rate error-correcting codes through pseudo-random graphs" by Alon et al. Also there are more sophisticated "concatenated-like" constructions that achieve much more than asymptotic-goodness, for example: V. Guruswami and A. Smith. Codes for Computationally Simple Channels: Explicit Constructions with Optimal Rate, FOCS 2010. Finally, any construction of codes for stochastic errors (that is, the channel BSC(p)) that achieves constant rate and exponentially small error must be asymptotically good. However, I'm not aware of any such explicit constructions other than concatenated codes (Forney/Justesen and similar variations). 

I'm pretty sure this has a trivial answer but it's always faster to ask the community :-) I understand that, relative to a random oracle, P=BPP. But this is sometimes phrased via the shorthand "Almost-P=BPP", which I find rather confusing. Complexity zoo defines Almost-P as "The class of problems that are in $P^A$ with probability 1, where A is an oracle chosen uniformly at random." Logically, L belonging to Almost-P would then mean that: With probability 1 over a random instantiation of an oracle tape A, there is a polynomial time Turing machine T(A) that decides L. Now, the poor BPP machine trying to decide L would have no idea which T(A) to simulate. The choice of T(A) may wildly vary depending on A. Should Almost-P (similarly, Almost-anything) be rather defined as the set of languages L for which there is a polynomial time Turing machine T with an oracle tape such that, with probability 1 over the instantiation of the oracle tape, the language decided by T ends up being L? Under neither of the above definitions does the statement BPP=Almost-P seems to be logically equivalent to "relative to a random oracle A, $P^A=BPP^A$". My question is now, what is the precise definition of the class Almost-P (or, for that matter, Almost-anything)? And/or, what am I missing above?