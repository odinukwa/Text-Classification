You need to create variables that determine the position of your image, make your render function use those variables as the position, and change those variables when you want to move your object. Here's how you can do it: 

The best way to increase your chances of being employed as a game developer is to get your hands dirty and actually work on the development of some small games. You say that you are "interested in programming games", which makes me believe that you haven't developed any games, yet. I advice you to stop thinking and start doing. However, the first order of business will not be to specialize in any of those fields. Let me elaborate. When I interview you for a job, if you tell me you are interested in programming games and your specialty is game AI, I will immediately ask you which games you worked on. If the answer is "none", I don't care about how many books you read or how you believe you are an expert in game AI. You won't be hired, period. To obtain game development experience in a specific field such as AI, you need to seek out others that are developing games and try to get into their teams. You want specialization, remember? A generalist can develop the whole game, but a specialist needs to work in a team. However, you will again be asked for previous work for someone to trust you with the AI of their game, let's say. Even if you will work free of charge. So, it's a chicken and egg problem right there. Basically, you can't just specialize in anything with no prior game development experience. Here's what you need to do: you need to forget about specializing and start contributing to the development of a game in any way you can. You can develop a simple game using a game engine such as Unity, or ask someone else developing a game and they will tell you what they need done. Then you'll figure it out and that will give you experience. Once you have something to show, not only you will have a better chance of joining a team, but also you'll have a better idea on fields that you can specialize in or that you don't like to work on. Trying to make a decision right now without that experience is like marrying someone without getting to know them. You cannot know if you will hate coding game AI unless you have some relevant experience. 

If you need to, you could use the Assimp file import library's 3ds export function to create the 3ds files directly. The procedure would be as follows: 

I would recommend either leaving this system as-is and worry about performance later or use Assimp, which not only does this all for you, it supports more formats with consistency, and will save you some headaches later with its tangent generation and other useful preprocessing. Some pseudocode for aligning as asked for: 

I believe your error arises from rounding errors. I believe the problem could be solved by using for pixel-coordinate storage. This way you will always get the exact increment needed for the given delta time, which is usually very small and thus the resulting per update increment is less than 1, and truncated to 0. Don't forget to use proper rounding when converting back for rendering or comparison. For rendering I would use: Which will yield 'proper' rounding: up from 0.5 and down with anything less. 

Lets start with general rule of optimizing things, especially when it comes to games: Profile it. It's not worth working on something that works already, because especially with games, you will get stuck optimizing and refactoring what you have, instead of doing actual work. When a player is playing a game, they usually are doing just that, because a game requires most attention, especially as this will only get called when the player is moving, so the CPU cycles you save are not needed for anything else. But lets put that aside, this is a learning project, so you want to try what can you do. The idea you describe has two big negatives: 

Answering your comment: By rendering only quarter tiles, the possibilities decrease dramatically. You shouldn't have to handle tiles on tiles, as they should be on a different layers alphablended or keycolored on each other. Also, I don't see why would you need built-in neighbors, but that could be handled separately in the neighbor calculation, perhaps using some sort of maps(list of pairs that blend). The map is usually static, and the images are low-res, so no need to worry about performance. 

Below shows what is happening with my current code, if we had a perfect 60 FPS the position should = 1, but as we can see it does not because the FPS varies slightly from frame to frame. 

Alpha and beta testing are two of the stages that a software must undergo testing. Alpha testing occurs first and when the software passes that, beta testing can then be undertaken. If a software fails alpha testing, changes are done and it repeats the tests until the software passes. So to answer your question, an Alpha and Beta release can be considered the 'testable deployed artifact' that you are currently developing. Read more: Difference Between Alpha and Beta Testing | Difference Between | Alpha vs Beta Testing $URL$ 

Using the normal LibGDX approach, create a project using File -> Import -> Gradle (gradle build). Highlight all of the project folders and right click -> team -> share project Note: SVN does not support adding multiple folders at once, make sure to click 'Share Project' option. Enter repo URL: (I just clicked Next) Enter folder name: (I used 'Use project name as folder name') Click finish Repeat for each project folder 

Below is the code for my player object's update method. As we can see it is operating solely on the delta time for now. In a perfect world of 60 FPS, this would work perfectly but unfortunatly FPS dips will always happen (especially on mobile platforms). How can I add a calculation on top of the delta time to move the player 1 unit in real time regardless of framerate? 

Disclaimer: I'm not sure if this is the best way to do this but it accomplished what I am trying to do. 

It is generally assumed that the mass does not matter and they bounce up to the same height. This is because the coefficient of restitution, which lets you calculate the velocity change after the collision, does not depend on mass. The velocity right after the collision determines the height that the ball will move up to, independent of the mass (just like how mass does not affect free fall duration). So, for games, it's safe to assume that they will end up at the same height. However, in real life, it can be hard to make the two have the same contact properties and balls with different masses may end up bouncing to slightly different heights. This paper can provide further insight. 

I know exactly what's going on and it's a tricky one:) In time, since one orbits after the other (the update functions do not happen at the same time), their distance increases or decreases little by little. Right when the planet orbits a bit, you want your moon to do exactly the same motion so that their distance does not change. Otherwise you'll make an orbit around a slightly different radius. Your planet, before doing its orbit step, can remember where it was and where it went to, and tell the moon to move in the same direction with the same amount. To show you how to do that, I would need access to your planet code as well. However, it's simple subtractions and additions of transform.position values, you can also figure it out. In the meantime, below is a hacky fix that should remedy the situation if both the planet and the moon are orbiting around Vector3.up. I wasn't sure about how you use the orbit angle, so I changed that a bit. This works for me here: 

I base all this on personal experience from implementing an animated mesh with assimp and glm myself.(which now works) But it might not be the correct way of doing things. 

Probably your problem is due to normals not being per-face but smoothed. You can apply Smooth with AutoSmooth after import with 30 degrees threshold to your model or maybe use a better exchange format. You can use 30 degrees in most cases for half-smoothed objects. If you need blockiness just decrease it. (Or if you need smoothness increase it) 

First off, I think using the GPU is not suitable. Your usage defeats what the GPUs are made for: One program running on a lot of threads. Also GPUs don't support intercommunication and conditionals very well. But your expectations are quite low. 20Hz for lets say 10 000 computers. Nowadays you can count with a 3GHz CPU, with multiple cores. For simplicity lets assume you only have a single thread for this virtual simulation. I am also going to assume that the Lua code(or the one of your choice) is 10 times slower, than compiled C/++ code(Can be improved with LuaJIT, but will be worse due to constant switching between computers and cache misses, etc.). 20 * 10 000 * 10 = 2 000 000 Hz needed to simulate your virtual computers. or 2 000kHz. or 2 MHz. And you can safely assume a computer with a ~3GHz core. Please bear in mind that due to cache misses, memory operations, and because you probably underestimated your instruction needs you will need much more than 2MHz, but you are safe as long as you don't need more than ~1500 times more power. I think this idea is viable performance-wise. I would recommend using Lua, so you don't have to reinvent the wheel, and have a solid base to build on. Its also easier to program for starters(if you want your players to program them). This is pure speculation. If you want a real answer, profile it. As to whether you should research further or not, you should create a simple prototype(throwaway), which will test both if your game is fun and if its viable performance-wise. 

Unity animations do not animate changes to mesh details. Animations are only pos-rot-scale of nodes. Unity's skinned mesh renderer uses animations of nodes to deform meshes. How Unity deforms your mesh can be slightly different than Blender's deformation. Unity is trying to do the right thing given the armature animation and the mesh weights for the nodes of the armature. How are you animating the normals in Blender anyway? The armature in Blender isn't supposed to deform normals like the first screenshot you gave. 

Try placing this code in FixedUpdate() instead of Update(). Physics motions happen in FixedUpdate() and Update() is synched with draw calls. The two don't always happen at the same frequency. Most of the time this is the cause of the jittery motion. So, if you are going to affect the motion of a physically-simulated object, you need to do it in FixedUpdate(). Now, why does it happen only when you do the normalization? Probably it's because the extra processing time required by Normalize() causes the Update calls to lag more behind FixedUpdate. The fact that it acts differently in different computers also supports this idea. How much out-of-sync Update and FixedUpdate get depends a lot on the available CPU cycles, which tends to be nondeterministic. 

1) Since speed is a concern, you may want to take a look at approximate nearest neighbor algorithms. I've used ANN in the past and it performed very well for around 12 dimensions. It lets you adjust desired precision so that you can have a trade off between speed and precision and find what works best. 2) Since your visual occlusion is a black-box one (I'm assuming unpredictable moving obstacles), I'm not sure if you have much of a choice other than doing occlusion tests on the points that the NN algorithm returned. 3) I don't believe ANN supports points changing, but I'm not sure since I didn't need that. It seems Cgal and Pastel support dynamic sets, but in terms of insertion/removal of points. Perhaps the papers here would also provide some insight. I don't know if you need this advice, but I found that reusing libraries for such problems almost always is a better idea. There are so many pitfalls one can fall into while implementing the details. Good luck! 

I was curious to see what waitDrawingComplete actually does, but found its just an empty syncrhonized method! I guess it does nothing, am I missing something here? GameRenderer.java line 328 

Below is an image of my gameworld. As we can see, the player starts at position 0. each cell represents 1 game world unit. My goal is to get the player to move 1 unit (the red cell) in 1 second in 'real world' time using the LIBGDX framework. 

I decided to give LibGDX a try and am planning to use SVN for my code repository. I created my LibGDX project via the Gradle build as outlined in the book "Learning LibGDX Game Development". I'm not quite sure how to add the project to my repository. I have tried to upload the root folder (Project below) into svn, which does contain all the sub-project, but when I check out my workspace only contains the Project folder. Below is the structure of the project in Eclipse after the gradle build. 

I am studying the gameloop used in Replica Island to understand a bit about Android game development. The main loop has the below logic... GameThread.java (note the variable names in sample code dont match exact source code) 

I am using LibGDX to make a very simple game. I am going to hire someone to create a splash screen logo since I'm artistically challenged. I want to order a vector graphic so I can scale it later if needed. What would be a decent size to use as a baseline (in pixels) that would scale relatively well on desktop/android/ios/etc? 

Has anyone used SVN with LibGDX, if yes can you please guide on how to add a project to repo and checkout correctly?