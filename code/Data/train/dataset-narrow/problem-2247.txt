$NC$ algorithms for perfect matchings in general graphs is still open but there has been some progress. Here are a few that I am aware of: For general graphs, Agrawal-Hoang-Thierauf showed that given the promise that the number of perfect matchings is small, there is an $NC^2$ algorithm to enumerate all of them. For the class of planar graphs, the pfaffian plays a big role. Kastelyn showed how every planar graph can be oriented in a way such that the pfaffian exactly equals the number of perfect matchings. (This was used by Valiant in to give "Holographic algorithms" for various problems) Mahajan-Subramanya-Vinay showed how the pfaffian can be computed in $NC$ using modifications of clow sequences. (Kastelyn in fact gives an algorithm to find the embedding in $P$ but I'm not sure if the pfaffian embedding can also be computed in $NC$; if yes, that would mean that counting perfect matchings in planar graphs is in $NC$.) And a recent result of Vinodchandran-Tewari show that the isolation lemma can be "derandomized" for planar graphs (using Green's theorem!) to put planar reachability in $UL$. But $NC$ algorithms for planar matchings are still open (thanks to Raghunath for correcting my claim that it is in $UL$). An $NC$ algorithm for bipartite planar matchings was given by Datta-Kulkarni-Roy Hope this helps. 

If a number x is accepted by the automaton, without the size $v^x_i$ of the nonzero counter at the beginning of a phase $i$ ever going $\leq s$, then there exists an integer $D>0$ such that all the numbers $x + n D$, $n\geq 0$ are accepted. If a set $X$ contains at least $s^2+1$ accepted numbers such that for each number $x\in X$ there is a phase $i$ such that $v^x_i\leq s$, then we can find $p, r\in X$, and integers $K_1,K_2$ such that 

becomes . Individual and can first be changed into the combined form. and are unchanged. would be unchanged, except that in our case we still have one final check to do: we need to ensure that there are no prime factors in the number other than 2,3 and 5. Since our particular 3-counter automaton zeros the counters it uses when it accepts, this is simple: just test that the final variable is 1, which can be done by jumping to the code 

So let's end with an explanation of the gist of the general method from the above linked paper by Ibarra and Trân (freely downloadable version) for how to prove that certain problems aren't solvable by a 2CA, and how it annoyingly breaks down in our case. First, they modify every 2CA into a "normal form", in which the two counters switch in "phases" between one only increasing and the other only decreasing until it reaches zero. The number of states $s$ of this normalized automaton plays an important role in the estimates. Then, they analyze this automaton to conclude that they can construct certain arithmetic sequences of numbers whose behavior are linked. To be precise (Some of this is not stated as theorems, but is implicit in the proof of both of their two main examples): 

Polylog independence may not be the only way to fool $AC^{0}$ circuits. To illustrate this example, consider the class of linear polynomials. Any zero set of a linear polynomial is $(n-1)$-wise independent but of course this doesn't fool linear polynomials. Hence, $(n-1)$-wise independent distributions do not fool this class. This of course doesn't mean that only $n$-wise independent distributions fool this class ($\epsilon$-biased spaces fool them, and are polynomial sized spaces). I guess what one means when they say "$\log^{O(d)} n$-wise independence is necessary" is that there are examples of distributions with smaller independence, and it is known that they do not fool $AC^{0}$. 

Let $S \subset [N]$ be a fixed set of size $n$. Suppose $p$ is the probability that a random set $T$ of size $m$ intersects $S$ in $k$ or more points. That is, $$ \Pr_{\substack{T\subset [N]\\|T| = m}} [|T \cap S| > k] \quad=\quad p $$ Question: Given a (sane) parameter $t$, is there an explicit family of sets $\mathcal{F} = \{ S_1,\dots S_t \}$, where each $S_i$ is a subset of $[N]$ of size $n$ such that: $$ \Pr_{\substack{T\subset [N]\\|T| = m}} [\forall i \in [t] \;:\; |T \cap S_i| > k] \quad \leq \quad p^{t/1000} $$ Here are some examples for which such a bound holds: 

Perhaps this isn't really an answer to your question. But to just give one example of why sometimes $\bmod m$ gates (for composite $m$) are more powerful than $\bmod p$ gates: Consider the class of constant depth circuits that consist only of $\bmod p$ gates, and inputs and constants at the leaves. Then, one can easily show that the OR function (for example) cannot be computed by such circuits, regardless of the size of the circuit. (This is because any such circuit computes a low degree polynomial over $\mathbb{F}_p$, and the degree of OR is $n$). However, if we consider circuits that consist only of $\bmod m$ gates where $m$ has at least two distinct prime factors, there is a depth $2$ circuit (of exponential size) for the OR function. And prior to Ryan's result, $AC^0[\bmod 6]$ was I guess the smallest class for which we didn't have any decent lower bounds. 

So people keep nagging me to post this even though it only solves a simplified version of the problem. Okay then :) At the end of this, I will put some of what I learned from the paper of Ibarra and Trân, and why that method breaks down on our general problem, but perhaps still gives some useful information. But first, we'll look at the simpler problem of trying to decide the set $L = \{ 2^n \mid $ the ternary and binary representations of $2^n$ have both even length or odd length$\}$ Note how this has $2^n$ rather than $n$ as in the original problem. In particular if the input number is not a power of 2, we want to reject it rather than attempt to calculate its length in any base. This greatly simplifies matters: If the original number is written prime factorized as $2^{v_2} 3^{v_3} 5^{v_5} 7^{v_7} ...$, then for all the $v_i$ except $v_2$ we just need to check that they are all $0$. This allows us to solve this simplified problem by using a wrapper around the old method (by Minsky I assume) of encoding the state of a $k$-counter automaton in the exponents of the prime factorization of the single variable of a multiplication/division automaton, which as noted in the OP above is pretty much equivalent to a 2-counter automaton. First, we need a $k$-counter automaton to wrap. We will use 3 counters, named $v_2$, $v_3$ and $v_5$. The automaton will accept iff for the initial counter values, the ternary and binary representations of $2^{v_2}$ have both even length or odd length, and both $v_3$ and $v_5$ are zero. When it accepts it will first zero all its counters. Here is some code for that, in an assembly format similar to the OP (I've just added variables to the instructions). I haven't actually tested it, since I have nothing to run it with, but I consider this a formality: 3-counter automata are well known to be Turing-complete, and to be able to construct any computable function of one of their initial values. 

The sort of set system I have in mind is something like a Nisan-Wigderson design (where pairwise intersection between the $S_i$'s are small, and $t \gg N$). The vague intuition is that mutually disjoint sets are independent events, and for almost disjoint sets we ought to have an exponential drop (maybe not $p^t$, but $p^{t/1000}$ or $p^{\sqrt{t}}$ or something). 

Before [AKS], Primality had coRP and RP algorithms (Miller-Rabin for coRP, Adleman-Huang for RP). A natural zero error extension would be to run both simultaneously until you push the error down to $\frac{1}{n^c}$ or so, and then run the brute-force algorithm. The expected running time would then remain $\text{polylog}(n)$. 

The traditional definition of PCPs have perfect completeness -- If $x\in L$, then the prover can give a proof on which the verifier (on reading constantly many bits) always accepts. Suppose we modify the definition as follows: 

One possible implication of this would be that $NEXP \nsubseteq P/poly$ from Ryan William's result (since you would then have an co-nondeterministic algorithm for CircuitSAT running in time faster than exponential). Not really negative evidence, but still... 

There was a subsequent extension by Grigoriev and Razborov (PS) that has a shorter exposition of proof. 

Suppose $G$ is an undirected $d$-regular $n$-vertex graph for some constant $d$. Let $\lambda_k$ be the $k$-th largest eigenvalue of the normalized laplacian $L$ of $G$ (defined as $I - \frac{1}{d} A$ where $A$ is the adjacency matrix of $G$). If the degree is constant, we can have good expanders of course but I want to say that the lower eigenvalues cannot be too close to $1$ if the degree is constant. I was wondering if there is a quantitative statement of this sort. Are there any bounds known about how close $\lambda_k$ can be to $1$, as a function of $k$? In particular, is something of the following form known: 

For their own examples they also frequently use the fact that $D,K_1,K_2$ have no prime factors $>s$. To prove impossibility, they then derive contradictions by showing that such arithmetical sequences cannot exist. In our problem, getting a contradiction from this breaks down with the second case. If we have $K_1 = K_2 = 6^k$, where $k$ is large enough that no number between $p$ and $r$ is divisible by either $2^k$ or $3^k$, then there will also be no powers of 2 or 3 between $p + 6^k n$ and $q + 6^k n$, so they are either both accepted or both rejected. Point 1 can still be shown to be impossible, because powers of 2 and 3 mostly grow further and further apart. And I believe I can show the second case impossible if $K_1\neq K_2$ (I've emailed @MarzioDeBiasi the argument). So perhaps someone could use this information to restrict the form of the automaton further, and finally derive a contradiction from that. 

The code on my website also has an initial check that the number isn't zero, which I've just realized is redundant with the v3, v5 zero checks, oh well. As I mentioned, the above method works for the simplified problem, but it really has no chance of working for the general one, because: In the general problem the precise value of every prime's exponent counts for deciding its general size and thus which lengths it has in various bases. This means that: 

The place from which I had read the proof was Completeness and Reduction in Algebraic Complexity Theory by Peter Bürgisser (.ps). As for the determinental complexity of the permanent, the best lower bound known so far is $\Omega(n^2)$ by Mignon and Ressayre. 

Another result that I'm aware of is by Arvind, Joglekar and Srinivasan -- they present explicit polynomials computable by linear sized width-$2k$ monotone arithmetic circuits but any width-$k$ monotone arithmetic circuit would take exponential size. 

Also, fourier analysis of boolean functions (here is a great course by Ryan O'Donnell) has a HUGE collection of awesome results, my favourite being the Kushilevitz-Mansour-Nisan's proof of the Goldreich-Levin theorem. Scott Aaronson had in fact given a tutorial at FOCS'08 on the "The Polynomial Method in Classical and Quantum Computing (ppt)". Hope this helps. 

There is an easy reduction from $3SAT$ to $\text{Max}2SAT$ (problem 5 in this) that gives a simple $iPCP$ for $3SAT$. And by definition, the class is closed under complement so it includes $coNP$ as well. Therefore, it is unlikely that there is a way to convert an $iPCP$ to a $PCP$ with the same parameters (well, that would at least mean $coNP = NP$). (thanks to Peter Shor for pointing this out) Have such incomplete PCPs been studied earlier? 

Grigoriev and Karpinski (ps.Z) showed that any depth-3 circuit over a fixed finite field computing $\mathrm{Det}_n$ requires $2^{\Omega(n)}$ size. I had the misconception(?) until recently that the same proofs also goes through for $\mathrm{Perm}_n$ as well. To add to this, quite a few papers in arithmetic circuit complexity state in passing that the lower bounds is for $\mathrm{Det}_n$ and $\mathrm{Perm}_n$. As far as I can see, the proof technique of Grigoriev and Karpinski when applied to the $\mathrm{Perm}_n$ fails at a subtle technical point. Their proof (for $\mathrm{Det}_n$) uses the following fact: