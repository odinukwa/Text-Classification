In Feldman-Gopalan-Khot-Ponnuswami 06 the authors show that agnostically learning parities reduces to learning parities with random classification noise. They also remark (among other things) that learning conjunctions in this model is NP-hard. Though I can't remember the source right now, I also recall that a random projection argument gives a reduction from learning parities with malicious noise to learning parities with uniform random noise. Is there a reduction from learning conjunctions with malicious noise to learning conjunctions with random noise? 

The "number in hand" model is the one I was thinking of, and there is quite a bit of literature about it. In particular I found this paper of Jeff Phillips, Elad Verbin, and Qin Zhang from SODA 2012 [1]. In particular they prove lower bounds on the problem I was interested in using, the undirected graph connectivity problem, of $\Omega(nk / \log^2(k))$. Here $k$ is the number of edges provided to each player and $n$ is the number of vertices in the graph. [1] $URL$ 

is it possible to adapt the above proof to handle distinct orders? if so, what part of linear algebra carries over to this setting? if we can compute the rank for permutation groups as suggested above, are there other, presumably infinite, groups for which this is doable efficiently? are there any algorithmic applications of this notion to problems involving graphs or permutations? 

It is commonly accepted that matroids provide an abstract setting for which greedy optimization works (although there are more general structures known as 'greedoids'). I was wondering whether there had been attempts to formalize a notion of 'semi-greediness'. Intuitively, it means that we would still construct a solution iteratively, but going from a solution $S$ of cost $i$ to a solution $S'$ of cost $i+1$ would work differently: instead of having $S'$ of the form $S \cup \{x\}$, we could for example have $S'$ of the form $(S \Delta T) \cup \{x\}$ i.e. we would replace $r$ elements present in $S$ by $r+1$ elements exterior to $S$. This could possibly model interesting problems such as bipartite maximum matching, optimizations in $\Delta$-matroids or 2-polymatroids. 

I have been reading a bit about the sum-of-squares method (SOS) from the survey of Barak & Steurer and the lecture notes of Barak. In both cases they sweep issues of numerical accuracy under the rug. From my (admittedly limited) understanding of the method, the following should be true: 

My first question is whether the above claim is true (is there a naive argument that doesn't use SOS to solve this?). The second question is where numerical accuracy fits in. If I want to get an assignment that satisfies all constraints to within additive $\varepsilon$ accuracy, how does the runtime depend on $1/\varepsilon$? In particular, is it polynomial? The motivation for this is to, say, apply a divide-and-conquer approach on a large system until the base case is an $O(1)$-size system. EDIT: From Barak-Steurer, it appears that the "degree $l$ sum-of-squares algorithm" on p.9 (and the paragraphs leading up to it) all define problems for solutions over $\mathbb{R}$, and in fact the definition of a pseudo-distribution in section 2.2 is over $\mathbb{R}$. Now I am seeing from Lemma 2.2, however, that one is not guaranteed a solution/refutation at degree $2n$ without binary variables. So I can refine my question a little bit. If your variables are not binary, the worry is that the sequence of outputs $\varphi^{(l)}$ is not finite (maybe not even monotonic increasing?). So the question is: is $\varphi^{(l)}$ still increasing? And if so, how far you have to go to get additive accuracy $\varepsilon$? Though this likely does not change anything, I happen to know my system is satisfiable (there is no refutation of any degree), so I am really just concerned about how large $l$ needs to be. Finally, I am interested in a theoretical solution, not a numerical solver. 

is it possible to define a corresponding notion of tree-width that is well-behaved? E.g. we would expect forest posets to have tw at most 1, and series-parallel posets to have tw at most 2. is it possible to get a width measure that is algorithmically useful? A possible application would be the counting of linear extensions: it is feasible in $n^{O(w)}$ for a poset of width $w$, but the extension to path-width / tree-width seems unlikely at this point. 

A partial semigroup (or PSG) consists of a set $X$ and of a partial composition law $*$ defined over $X$, that is to say: (1) $x*y$ is not always defined, (2) if $(x*y)*z$ is defined, so is $x*(y*z)$, and the two values are equal, (3) if $x*(y*z)$ is defined, so is $(x*y)*z$, and the two values are equal. Let $\mathbb{R}$ be a ring. We may then define a convolution product over $\mathbb{R}^X$ : given two functions $f,g$, we define a function $f*g$ such that 

I'm a newcomer to communication complexity, and so far I've read the chapter in Arora-Barak and some papers giving lower bounds in various applications. A priori the definition of multiparty communication complexity is strange. Forgetting the fruitful applications of the "number on the forehead" model, I would imagine people first thought to have each player see only their own input and not all but their own inputs. The computation of the protocol would happen in parallel, and communication could be player-to-player or broadcast. I have reason to believe that lower bounds in such a model will aid me in proving lower bounds in a problem I'm currently looking at. Does this model have a name? What is known about it? 

Some structures have a property of closure by a "sum" or "product" operation. Given a family of structures $(S_i)_{i \in I}$, we can then define a new structure denoted by $\sum_{i \in I} S_i$, resp. $\prod_{i \in I} S_i$; structures enjoying this property are vector spaces for instance. I consider these operations as "undirected" as the result does not depend on the order of the summands, up to isomorphism. I am interested in structures for which we can define a "directed sum" / "directed product" operation, meaning that the definition of the sum / product could now depend on the structure of the index set $I$ (e.g. it could be a totally ordered set and we would compute non-commutative sums / products according to this order). To illustrate this, consider the following definitions for posets. Let $I = (G,\leq)$ be a poset and for each $i \in G$ let $S_i = (U_i,\leq_i)$ be a poset. 

What is the best known lower bound against (nonuniform) circuits of size $O(n)$? I understand that we don't know of any explicit functions that need circuits of size more than something like $5n$. But are there existential results saying, e.g., that $\text{EXP} \not \subset \text{SIZE}(n)$? 

I found this paper of Cuomo and Oppenheim, where they use a Lorenz system to define an encryption scheme for signals. There is also this blog post describing and implementing the technique. The technique uses an interesting property of the Lorenz system known as "synchronization," which as far as I know occurs in other dynamical systems as well. Moreover, the chaotic nature of these systems intuitively makes guessing the parameters of the system infeasible, so a natural shared secret key is the set of constants used to define the system. I was wondering if there has been any theoretical work in cryptography that deals with dynamical systems. Specifically, are there established cryptographic hardness assumptions for specific dynamical systems? It seems like it would be a different flavor of hardness assumption from the usual ones, because the methods naturally introduce some noise into the decrypted message. 

This is more of a 'meta' question as I cannot give a precise formulation of my question. Consider for example the category of total quasi-orders: we can then distinguish between a 'strict' order (where no two elements are equivalent) and a 'weak order' (where some elements may belong to the same equivalence class). It doesn't possible to define an intermediate notion of 'mild order' in this setting, so do you have any suggestions for other structures allowing a natural notion of 'mildness' which is well-behaved? I'd like to add a self-evident comment: in science the qualificative of 'mild' can be applied to certain notions, e.g. 'mild necessity', 'mild difficulty' or 'mild formalism' which describes well the questions that I'm asking. OTOH I guess we can't speak of a 'mildly correct' statement without resorting to statistics? 

Learning Parity with Noise (LPN) is usually stated with constant noise rate $\eta < 1/2$ on the labels, and it is believed to be hard to learn because of the high statistical dimension of the problem (they are not SQ-learnable even if the parity is assumed to be sparse). I'm interested in a noise rate which decays with $n$, the number of features, and I want to know where the known easiness threshold is for this problem. Specific notes: 

I heard of a result in approximate graph coloring, but cannot find the source. The result is: For every constant $h$ there exists a sufficiently large $k$ such that coloring a $k$-colorable graph with $hk$ colors is NP-hard. Could someone please point me to the relevant paper? 

Consider a poset $P = (V,A)$. We may define a path structuring of $P$ as a chain $\Sigma$ of the form $X_0 \subset X_1 \subset \ldots \subset X_n$ where : (i) for every $x \in V$, the set $\{ i \in [n] : x \in V_i \}$ is an interval, (ii) for every $i$ we have $X_{i+1}$ of the form $X_i + \{x\}$ or $X_i - \{x\}$, (iii) for every $i < j$, $P$ contains no backward arcs joining $X_j - X_i$ to $X_i - X_j$. The width of $\Sigma$ is defined as $w(\Sigma) = \max_i |X_i|$, and the path-width of $P$ is defined as the minimum width of a path structuring of $P$. In particular, the path-width is always smaller than the width (defined as the size of a largest antichain). The following questions seem natural in this respect: 

As per the comment, $K(x|y) \leq K(x|y^*) + O(1)$. Now denoting the first metric (with the *) by $d_1$ and the second by $d_2$, we have $\displaystyle \begin{align*} d_2(x,y) &= \frac{\max \left \{ K(x|y), K(y|x) \right \} }{\max \left \{ K(x), K(y) \right \}} \\ &\leq \frac{\max \left \{ K(x|y^*), K(y|x^*) \right \} + O(1) }{\max \left \{ K(x), K(y) \right \}} \\ &= d_1(x,y) + O(1/K) \end{align*}$ where $K = \max \left \{ K(x), K(y) \right \}$ All of the theorems in the paper give the metric inequalities and universality claims up to an additive factor of $O(1/K)$, so this fits.