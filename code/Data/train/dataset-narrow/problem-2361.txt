I'm looking to use something close to the following as a definition for a circuit class. This is obviously semi-informal. I am curious if any one sees any potential problems with it, or where something in it's informal nature can potentially be a problem when it is tightened up to something more formal. Also, I'm not aware of any vetted and accepted circuit class "like this", so pointers to something I missed are welcome as well. In particular, the use of time is critically important to what I'm working on. TIA. The circuit class TLU, or Table Look Up, is defined as taking $n \ge 1$ $\{0,1\}$ inputs that produces a single $\{0,1\}$ output. An individual $n$ input TLU gate is defined as an indexed array of $\{0,1\}^{2^n}$, where $n$ represents the index that selects a particular $\{0,1\}$ from the array. For example, $\{1,1,1,0\}$ would represent the common $\text{NAND}$ gate. An individual $n$ input TLU gate has a depth of $1$, a gate count of $1$, and takes atomically $O\left(1\right)$ time to produce an output from a given input. For each $n$ input TLU gate set, there are $2^{2^n}$ possible indexed arrays. All TLU gates advance in time atomically with respect to all other TLU gates. For some large value of $n$, one may wish to map a large $n$ input TLU gate to a set of $\lt n$ input TLU gate primitives. A common example would be the set of $\le 2$ input $\{\text{AND}, \text{OR}, \text{NOT}\}$ gates. In The synthesis of two-terminal switching circuits, Shannon proved maximum worst case bounds for gates and depth for the $\{\text{AND}, \text{OR}, \text{NOT}\}$ set. This result can be extended to make the following claim: As long as the set of primitives contains a complete boolean basis, there is guaranteed to be a maximum worst case bounds for gates and depth for that set of primitives. 

The follow link gives an overview of most CMOS gates. Note that "AND OR Inverted" (AOI) and "OR AND Inverted" (OAI) in the link. These circuits are typically a fraction of the size it would take to create the same circuit using their discrete components. For example, a OAI33 circuit (taken from a commercial foundries standard cell library) takes ~$1.62^2$ area, but building the same circuit using the equivalent discrete cells takes ~$3.82^2$ area. 

For approximate counting, the following paper (also in APPROX-RANDOM 2011) $URL$ describes the state of the art. As Anthony Labarre refers to in a comment above, there was a recent and unexpected breakthrough by Yufei Zhao showing a tight upper bound on the number of independent sets in an $n$-vertex $d$-regular graph. His proof used a very clever bijection. The extremal example, conjectured by Alon and Kahn and dating back to 1991, is simply a disjoint union of many copies of a $d$-regular complete bipartite graph. This area of research draws on many mathematical and algorithmic methods, and is an area of interest not only to theoretical computer scientists, but also to number theorists, probabilists, combinatorialists, statistical physicists, and more. These two recent papers might give you a good start, though there is a rich collection of deep and interesting papers on the topic going back decades. 

Perhaps what's implicit after "there are" is "asymptotically almost surely", which means that the property holds with probability tending to 1 as n tends to infinity; this is stronger than "in expectation". You might want to check out a standard textbook on random graphs (e.g. by Alon-Spencer, Bollobas, or Janson-Luczak-Rucinski). 

To add to Dai Le's answer, a more recent book by Dubhashi and Panconesi provides many examples of the use of probability in the analysis of algorithms. 

There are several competing notions of a "sparse graph". For instance, a surface-embeddable graph could be considered sparse. Or a graph with bounded edge density. Or a graph with high girth. A graph with large expansion. A graph with bounded treewidth. (Even within the subfield of random graphs, it is slightly ambiguous as to what could be called sparse.) Et cetera. What notion of "sparse graph" has had the most impact on the design of efficient graph algorithms, and why? Similarly, what notion of "dense graph" ... ? (NB: Karpinski has worked a great deal on approximation results for one standard model of dense graphs.) I have just seen a talk by J. Nesetril on a program of his (together with P. Ossona de Mendez) to capture measures of sparsity in graphs within a unified (asymptotic) framework. My question -- yes, maybe quite subjective and I expect different camps -- is motivated by the desire to catch a multi-faceted perspective on the use of sparsity in algorithms (and plug any gaps in my own understanding of the issue). 

Interestingly, VLSI circuit complexity has a tendency to treat depth as "irrelevant" as there is one and only one "depth" that matters: the critical path. For most practical purposes, an arbitrarily complex circuit can be treated as $O(1)$ with a latency of $n$. In fact, I'm not even sure that the concept of $DLogTime$ / $NLogTime$ directly translates in to VLSI circuit complexity. Even Shannons $2^n/n$ result does not readily translate: Shannons results is valid only for a boolean basis consisting of arity $\le2$ {AND, OR, NOT}. This is not the only basis, and the number of "gates" needed drops dramatically as you allow more and more gate types. The following are $area^2$ from a commercial VLSI standard cell library normalized to the size of a 2 input NAND gate: 

I have spent quite a bit of time looking for alternate formulations of the Halting Problem that are not just simple permutations of the original proof given by Turing. Question: Can you point me to a vetted proof of the Halting Problem that shares as absolutely as little in common with the one given by Turing? Please, instead of arguing with me as to whether or not my reasons are valid, or that I "don't get it and should just accept the proof given by Turing", it would be a great help to me and possibly someone else if you could simply help me locate an alternate proof. Yes, I am looking for proofs with certain properties, properties that inconveniently cull a number of candidates. Despite this, they are properties that I unfortunately need. 

I am asking this question again. I am aware of, and have read the other similar "alternative proof TM" questions, but unfortunately, they do help me. I am looking for a TM Halting Problem proof that does not have the following properties: 

To which the answer is: no, no one builds PARITY circuits in the real world this way. The last time anyone wanted to do this was when the only thing they had to work with was mechanical relays and this is why Shannons ~$2^n/n$ lower bound for most circuits result is for {AND, OR, NOT}. Even Shannon knew XOR could not be represented efficiently using just {AND, OR, NOT}: 

I find Cooks reasoning curious. Shannons result is valid for all $f : \{0,1\}^n \to \{0,1\}$, therefore if a $\mathcal{NP}$ problem can be described in $\{0,1\}^n$ bits, there must be a {AND, OR, NOT} basis circuit that can decide if it is satisfiable in ~$2^n/n$ gates (the actual paper gives a larger upper, but finite, bound for every possible function). What this tells us is that anything that uses more than $n$ gates, where $n$ is the upper bound for the size of the $\mathcal{NP}$ problem, is using more gates than is required. Using a larger, but complete, boolean basis only reduces the number of gates required. Using a different circuit complexity model, i.e. VLSI, gives even "better" result bounds. Curious. But we know for a fact that any solution to a $\mathcal{NP}$ problem that uses more than ~$2^n/n$ "gates" (where gates is used loosely for steps / operations) is doing so in a sub-optimal fashion... and there's an infinite number of ways to find a solution in a sub-optimal fashion. On the Complexity of VLSI Implementations and Graph Representations of Boolean Functions with Application to Integer Multiplication shows that predicting circuit complexity using a OBDD model over-estimates the actual circuit complexity: 

What I want to say is too long for (but really should be) a comment. If I am reading the question correctly, you want a FPRAS (fully polynomial randomised approximation scheme) for either of the above quantities, each of which includes various #P-complete problems as special cases. In particular, you want a general FPRAS in the case of planar graphs, by the use of cluster expansion. I doubt that this is possible due to the fact that NP-completeness of the existence problem (e.g. proper colouring) implies that the corresponding counting problem (e.g. number of proper colourings) is complete in #P with respect to AP-reducibility (approximation-preserving). See Dyer, Goldberg, Greenhill and Jerrum, Algorithmica (2004) 38: 471-500. But perhaps I've misread the question. (Actually, would you be able to explain to the uninitiated the meaning of high-temperature expansions?) 

Since parallel segments form an independent set in such a representation, this conjecture implies the 4CT, but perhaps is even stronger. The reference: West, Open problems. SIAM J Discrete Math Newsletter, 2(1):10-12, 1991. 

This is related to Travis's answer. In fact, it could be considered a stronger version. A paper by Bollob\'as and Thomason (Combinatorica, 2000) shows that in Erd\H{o}s-R\'enyi random graphs $G_{n,p}$ (with $p$ some fixed constant), every hereditary property can be approximated by what they call a basic property. Basic almost means graphs whose vertex sets are unions of $r$ classes, $s$ of which span cliques and $r-s$ of which span independent sets, but not quite. This approximation is used to characterise the size of a largest $\mathcal{P}$-set as well as the $\mathcal{P}$-chromatic number of $G_{n,p}$, where $\mathcal{P}$ is some fixed hereditary property. If $p$ permitted to vary, the behaviour is not well-understood. For more background to this and related work, there is a survey by Bollob\'as (Proceedings of the ICM 1998) which also gives an enticing conjecture along these lines but for hypergraphs. I find the deep connection between hereditary properties and Szem\'eredi's Regularity Lemma very intriguing, as it was used both here and in the Alon and Shapira result. 

Have student reps To address Jukka's supplementary request made in a comment to the question: Enlist two (or more) people -- one an energetic young researcher (student or post-doc) and one an engaging and extremely experienced academic (professor) -- to co-ordinate activities for early-stage researchers. The former would be responsible for planning drinks and other social events, while the latter can provide careers and research advice, perhaps in a special-purpose session of the conference or possibly at said drinks. (An optional third person could be an industrial contact.) 

I think it is far more interesting that the circuit complexity classes used by CS complexity theory make different predictions and use different metrics than those in the VLSI community. From The VLSI complexity of Boolean functions: 

Specifically, note the / gates which are / consisting of arity sized first function feeding the second function, where the number of first function gates is equal to the number of times arity appears. For example, represents "Two 2 input AND gates feeding a NOR gate". My point is: Taken separately, a function can be built using three 2 input OR gates and a 3 input NAND gate, for a total area of ~4.56, not including any area used for interconnect. Yet this primitive can be realized in an area of just 1.72, which means a discrete manifestation of the same boolean function consumes 2.65 times more area. Also note that the area for an $n$ input {AND, NAND, OR, NOR, XOR, XNOR} gate, where $n\ge2$, is much less than the area that it would take to build the same function using discrete 2 input gates. Also note that while the area given for {XOR, XNOR} for this process is "large" relative to the other gates, there are other ways to build the same $n$ input gates using less area. The propagation properties for the more complex primitives is also significantly better than what would be achieved using discrete gates. Why is this important? Because for me, at least, I've spent a simply enormous amount of time sifting through results from complexity theory that are built on a set of assumptions that has the effect of either rendering the result useless or wrong once the assumption is violated. The following is from Steven Cooks $\mathcal{P}$ vs $\mathcal{NP}$: 

The following describes an eight transistor full adder circuit, which is typically defined in boolean algebra as $s = a \oplus b \oplus c_{in}$. For comparison, a typical 2 input {NAND, OR, XOR, etc} gate is typically composed of four to eight transistors. 

A good place to find high speed, compact XOR / XNOR gates is in full-adders and Hamming ECC circuits (which are typically in the critical path). Also, the issue of circuit depth is typically not a concern in VLSI synchronous logic. The only depth of any consequence is the critical path, which defines the maximum clock period. The vast majority of combinatorial logic propagate their results in a fraction of the time for the critical path. Critical paths tend to occur with some combinatorial logic that needs to pass through several areas scattered over a chip. Many times it is possible to "pipeline" combinatorial logic to meet the timing constraints. This has the effect of creating a circuit that takes a new input and produces a new output every clock cycle, but has a latency of $n$ clock cycles before a given input is available on the output. This tends to make most circuits ~$O(1)$ in practice. You may find the following paper of interest, which discusses VLSI $AT^2 = \Omega(n^2)$ complexity: 

Probably there are too, too many examples to list, but one classical example (it is highlighted by Aigner and Ziegler as a "Proof from the Book") is the use by Lovász of a geometric representation to solve a problem in Shannon capacity. Though the proof was published in 1979 and solved an open question from 1956, this remains state-of-the-art. 

Schramm proved that every planar graph is the contact graph of some set of smooth convex objects in the plane in his PhD thesis (Princeton, 1990) using Brouwer's Fixed Point Theorem. A nice survey of this and other results related to Koebe's Theorem is in a survey by Sachs. 

Following on Yaroslav's answer, Luby and Vigoda were the first to show a FPRAS for #IS under a density condition (maximum degree 4, which I suppose is weaker than Weitz's result), while Dyer, Frieze and Jerrum showed that there is no FPRAS for #IS if the maximum degree of the graph is 25 unless RP = NP. References: Martin Dyer, Alan Frieze, and Mark Jerrum. On counting independent sets in sparse graphs. FOCS 1999. Michael Luby and Eric Vigoda. Approximately counting up to four. STOC 1997. See also Jerrum's ETH lecture notes, "Counting, sampling and integrating: algorithms and complexity". 

As a bit of unashamed self-promotion, Marthe Bonamy and I found more negative answers. In particular, Theorem 4 of $URL$ improves upon the aforementioned result of Král' and Sgall in certain cases. The examples we use are complete bipartite graphs, where we used some extremal combinatorics to analyse them. The work was motivated in part by this TCS overflow question. 

Gerd Wegner in his PhD thesis (Georg-August-Universität, Göttingen, 1967) proved that any graph is the contact graph of a set of three-dimensional convex polytopes (but he credits the first unpublished proof of the result to Grünbaum). This is a short proof. 

If you use triangles, it can be done. Perhaps not easier than Koebe though... de Fraisseix, Ossona de Mendez and Rosenstiehl. On Triangle Contact Graphs. CPC 3(2): 233-246, 1994. 

A limited number of tables/desks in a quiet room Problem: Not everybody will attend every session, nor will everybody be staying at the hotel to/in which the conference is nearest/held. Solution: A few tables, desks, chairs in a quiet room for undisturbed reading, writing, typing, browsing, possibly with power outlets.