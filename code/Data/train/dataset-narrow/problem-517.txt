The most straightforward way to have Access VBA interact with a web service would be to use a object like this: 

Yes. The Access Database Engine is designed to work with "real" Windows file sharing and Samba – or at least some versions of Samba – apparently do not implement all of the low-level features of the SMB protocol that are needed for the Access Database Engine to work reliably. (ref: Corrupt Microsoft Access MDB Causes - Samba) 

As you learn more about SQL Server you will discover (to your delight) a number of things you can do in SQL Server at the database level that you previously had to do in Access at the application level. Some examples include: Triggers: Procedures defined at the table-level to make stuff automatically happen whenever there is an INSERT/UPDATE/DELETE on the table. Stored Procedures: Somewhat similar to Access macros and those little VBA procedures (s) you built in Access to do "useful stuff", except that in SQL Server they are part of the database itself. So, you can write a Stored Procedure (SP) once and use it (almost) anywhere, even across applications that share the same database. Functions: These are somewhat analogous to the little VBA s you wrote in Access to incorporate into queries, except that SQL Server Functions, like SPs, are more tightly bound to the database. Also SQL Server Functions can be Scalar-valued (return a single scalar value) or Table-valued (return a rowset). Fun SQL tricks: There are lots of SQL features available in SQL Server that are not supported in Access (Jet/ACE) SQL. "Common Table Expressions" (CTEs) and " and " are the ones that gave me "'aha' moments" as I was getting started. I could go on, but these are the things I remember discovering early on that got me "thinking in SQL Server". Don't get me wrong, I still think Access is an excellent tool when used appropriately, and should be given serious consideration as a way to 

By this I want to drop oldest of 4th week partition data and recreate immediately for next year similar week inserts. I.E. I use Mysql 5.6.19-67.0, the partitions are by week 

Now I want to retain the structure by removing 4th week prior DATA ONLY . But I want to see the structure there WITHOUT data for that partition. 

I guess you are under a wrong impression that it is a OS related issue. Check your server bit if it is 32 bit and if you try to install 64bit package percona's xtrabackup . It would end up throwing such error messages. 

Table will be partitioned by RANGE weekly. On Master keep latest week's partition. And drop earlier partitions in below way. 

Since you are using to take a consistent value of master status. The internals of mysqldump will issue below commands to mysql server. 

Rollback: If it doesn't start for some reason put back the iblogfiles and start. Please see link $URL$ for detailed explanation 

1. Will it mess up the sync in any way - On a high availability architectural view "YES". As long you have Master up and steady you might not end up problems. 2. Will the changes get overwritten from the master during the next replication event 

Ah! This one could be application framework has the value passed at the time of creating connection to DBs to set wait_timeout or interactive_timeout for their session level, this overrides default mysql settings. If you enable general log on any test environment and have their application to point to that DB. You will find wait_timeout or interactive_timeout setting such values. Short Term Fix: 

One of the following will do it from T-SQL. xp_logevent RAISERROR … WITH LOG Structured Exception Handling (including the THROW statement) is the intended replacement for RAISERROR, starting in SQL Server 2012, but THROW does not have WITH LOG functionality. 

The Policy Management feature of SQL Server can do some of this. The Table facet has fields @HasIndex and @HasClusteredIndex (as well as other ones that may be useful, like triggers). A policy can be created to check conditions on all tables, in all databases, in a number of servers (using the Central Management Server feature). It can't, however, check the existence of a primary key index or constraint. I would have sworn that there was a field @HasPrimaryKey but it's not there in MSSQL2012. I'm either misremembering or going mad. Note: Policy Management is included with SQL Server 2012 Enterprise, Business Intelligence and Standard editions. It is not available in Express edition. 

The usual warnings about undocumented features apply. You can look at the source code of the procedure in master if you are curious or if you want to be certain it has no nasty side effects. It uses dynamic SQL to build a cursor, which is bad for performance (cursor=slow!), so only use this procedure for a one-off task. Additionally, is not available in Azure Database. 

The "Back Up Database" and "Save Database As" options do the same thing. The only real difference is that "Back Up Database" will provide a default file name like 

As you have discovered, the column type in Access does not support fractional seconds. If you need to preserve the precision (and not round to the nearest second) then you'll need to import the time values as and then populate another column with the times converted to tenths-of-milliseconds (which I have abbreviated to "tms"): After the initial import of the times as you will have something like this 

Note that the query uses syntax, which is generally preferred over the older method of putting the join conditions in the clause. 

Sometimes you can, if the database engine offers such a mechanism. For example Pervasive PSQL is the descendant of the venerable Btrieve database. It allows you to use the "MicroKernel Database Engine" (MKDE) to make "Btrieve calls", or the "SQL Relational Database Engine" (SRDE) to execute SQL queries against the same body of data. 

I recently had to do essentially the same thing, except that I wanted to migrate MySQL tables and data to SQL Server. I tried: 

HSQLDB stores text data as Unicode. You can verify that by opening the file for the database and looking at the data. For example, for a table with a row containing "Montréal" the file contains 

A wild guess - are the triggers based on CLR code? If CLR Integration has not been enabled on the machine you are restoring to then the triggers won't work. CLR Integration can be enabled with the following code. 

No it is not, because identity does not guarantee a unique value. The identity property can be bypassed with (in SQL Server - you didn't specify what RDBMS you are using). A primary key constraint (and a unique constraint) uses a unique index to enforce uniqueness. 

Note: I came here from another answer where I misstook SQLIO for SQLIOSim. Hopefully I'll get it right this time. :-) SQLIOSim is "NOT" an I/O Performance Tuning Tool because it uses random patterns, so is not repeatable (a primary requirement for benchmarking). It is a stress-testing tool. Additional links: 

If Bob is the owner of BobSchema then you don't need to set any permissions at all - object owners have full control over their objects. Additionally, If Bob is the owner of BobSchema then Bob will by default be the owner of all objects created in BobSchema (in SQL Server 2005+). So, one way to solve your situation is to create the schemas using something like . 

Log Shipping essentially involves three jobs. The first job backs up the log on the primary server and stores those backups in a local folder. The second job copies those files across the network to the secondary server. The third jobs restores those backups using the WITH STANDBY option. Log Shipping is set up on an entire database. The database on the secondary server is accessible but is read-only. Requires: A shared folder on the primary server. Firewall configuration to allow the secondary server to access the file share. Security set up so the proxy on the second server has permissions to access the file share. This may be an issue for you in a hosted situation. Caveat: Log shipping is not real-time. The common interval for the first job is 15 minutes. An unplanned failover will lose data. Caveat: Log Shipping uses transaction log backups, so its design must be done in conjunction with your backup design. Database Mirroring involves the primary server sending individual transactions to the secondary server, in either a synchronous fashion or an asynchronous fashion (asynchronous is enterprise edition only). Like Log Shipping, Mirroring It is set up on an entire database. Unlike Log Shipping, the secondary database is inaccessible. Requires: An endpoint to be created on each sever for the mirroring traffic. Firewall allowing traffic from and to that one port. In a synchronous mirroring setup, an unplanned failover will not lose data. Both Mirroring and Log Shipping are creating a copy of an entire database so there is no object requirements. Heaps, Clustered Tables, tables with and without keys - all get copied. Note: Given the requirement for file sharing, I disagree with Szymon's comment about log shipping being easier to set up and maintain and requiring less resource. Installing the File Server role on a Windows Server is increasing resource requirements as well as increasing the surface area of attack. Additionally, in a log shipping unplanned failover, bringing the secondary online is a pain. Lots of steps, most of which involve running stored procedures in a query window. 

You may give it a try using percona backup alpha version for Windows - Download_Link . Below are the steps after installation in Windows Bash. 

What I'm I missing here? I need data greater that ISODate("2015-01-11T00:39:40.121Z") I know many forums have different answers for the same issue. But non resolved. Need a different point of view on this. 

As long as you have done flush logs and you have taken the logfilename just before flush logs say it should be fine. Example current binlog file is bin-log00333 and after flush logs it is bin-log00334. You may scp the logs from bin-log00001 to bin-log00333 and don't take bin-log00334. 

Doing by this way the SELECT statements are performed in a nonlocking fashion, but a possible earlier version of a row might be used. Thus, using this isolation level, such reads are not consistent.When you say not-consistent it means recently changing records i.e.. DML transactions that are currently in process will not be read. I assume which is in your case it is acceptable. This is also called a “dirty read.” Otherwise, this isolation level works like READ COMMITTED. If I were to be you, the below order is what I follow. 

But looking at your table definition you have secondary indexes almost on all. You might want to verify on performance of all your selects. Or tune them inorder to add the partition name or partition key as a mandate clause in where clause. So its better you take perf against selects and updates you use and then think whether is it feasible to go for partitioning or archiving the table. Also if you are anyway going for partition, you need to define the partition in such a way older partition can be removed. So I would suggest to go for 

"…but I'm curious as to whether there's a better way." Yes - use hardware or software RAID to create one RAID0 (or maybe RAID5, but write performance might suffer) array across the three 700 GB disks. Better still (as the commenter below says), add more disks and create a RAID array with fault tolerance. SQL Server databases are more manageable with fewer files, not more files. Is it possible to physically remove the hard disks from the old server and install them in the new server? If so then restoring the database into the new instance of SQL Server is just a matter of attaching the data files. Edit: The poster below makes an excellent comment about fault tolerance. A RAID0 array has no tolerance for errors. Then again, neither does splitting the database across three individual disks. I am OK with SQL Server data files being on RAID arrays with no tolerance, if and only if (1) there is a great DR solution, and (2) the log file is on an array with fault tolerance, and (3) the database is in Full recovery Model (so the tail of the log can be backed up). 

In addition to the points in other answers, here are some key differences between the two. Note: The error messages are from SQL Server 2012. Errors Violation of a unique constraint returns error 2627. 

Instead of trying to insert directly into the table you could insert your data into a temporary table ... 

Your query is missing If you are just getting started with SQL then you might consider building simple queries like this in the Access query designer and then switch to SQL view to see what the query designer has generated. In your case we have 

An "After Insert" data macro can certainly modify records in other tables. You can use the LookupRecord, EditRecord, and CreateRecord Data Blocks as required. However, a data macro cannot run a VBA script. Your actions are limited to those available in the data macro environment itself. 

The only method that didn't fail outright was to pull the tables from MySQL into a Microsoft Access .mdb file (!) via MySQL ODBC, then import that into SQL Server. However, even that was an incomplete solution because the transfer from MySQL to Access omitted all of the primary keys, indexes, and AUTO_INCREMENT attributes. Still, it did transfer the tables and data, which was my main objective. I just had to write some T-SQL to re-create the indexes and whatnot. Since you don't need to transfer any data you might want to consider just hacking the mysqldump SQL code (good old Find & Replace...) to the point where SQL Server will execute it. 

For those who still need this. May try using binlog puller module by percona folks. It works perfect and you may add a watchdog for it. 

Depends upon the criticality of Master and your wallet. If reporting queries are bothering much to Master, you may add a slave to it. If your business grows then chances of reporting might require different indexes created on main data hence you might need to handle different backup policies for Master and Analytics slave. 

The only biggest benifit of switching to Mysql 5.6.5< is due to the feature that supports automatic failover utility which is default in Mysql 5.9.1 in a Mysql replication topology, this requires GTID level replication. Unless or until you don't really need an automatic failover within Mysql itself, we can work with the legacy features of Mysql 5.5 which is recognized most. But also I would like to highlight some cons of using GTID: A nontransactional storage engine (Myisam) mixed with updates to tables that use a transactional storage engine (Innodb) within the same transaction can result in multiple GTIDs being assigned to the same transaction. Thish might break sync between Master/Slave replication. In a nut shell: If your environment is stable in performance, High-availability and Security in your existing version it is better to keep untouched. If there is a necessity in any of the above then go for it. Management audit says a lot to keep latest version according to standards but they don't see technology wise what is standard? For this we need not spend time and effort to upgrade and sustain the same setup when it is feasible and effecient in current versions. 

As stated by other posters, datetime values are stored as datetime values, not as strings. If you want to get technical, they are stored as two integers - one representing days and the other representing clock ticks (3.33ms for the datetime datatype). I agree with you in not wanting to use CONVERT() on every field. I feel that the correct place for formatting region-dependant items is on the client. The web page or application should be formatting dates and times as appropriate to the user's locale. This can take advantage of any localisation features of the client programming language - something that T-SQL CONVERT() can not do. As an aside, in case you didn't already know, the format for inserting dates can be controlled with SET DATEFORMAT. As another aside, SQL Server 2008 added the date datatype which may make your life a lot easier. 

"…is there any benefit to changing the GUID generation to sequential using newsequentialid()?" No. Sequential GUIDs are only appropriate when there is a clustered index on the GUID column and you want to avoid page splits caused by inserts. Edit to address the comment below: All nonclustered indexes suffer from page splits when data is inserted. For example, when you enter a record for Homer Simpson, it gets entered into the "S" leaf pages for the LastName index, possibly causing a page split. You don't however, require that customers join in strict alphabetical order. Additionally, the latch system used for index leaf and non-leaf pages means that page splits require less processing time and resource than page splits on data pages. Further to this, what would it require for the OP to change to a sequential id? They would have to replace the Default constraint on the Key column to NEWSEQUENTIALID(). This will not affect any existing rows in the table (which is good because there are foreign keys using those keys) - just the new rows. From that point on, inserted rows will have increasing keys but those increasing keys are not necessarily going to be greater than the existing data in the table (NEWSEQUENTIALID() only guarantees that the GUID is greater than any other GUID generated by NEWSEQUENTIALID() on that computer since it restarted). This means whose inserts are still going to cause page splits in the nonclustered index! 

Folks, Is it required to turn off huge pages in Mysql 5.6 server under /sys/kernel/mm/transparent_hugepage/enabled -> To never? We have a heavy transactional database and calling the data segments quite often for a few particular tables by its index. How does a memory foot-print work here? Would it help or degrade the performance. In AnonHugePages: 109242368 kB ( I see entire innodb buffer pool size is occupied under AnonHugepages). No specific issues found so far due to memory. But need an advise on when to keep it on and when to turn it off. 

[difference this time is you are granting with the password for REPLICATION SLAVE GRANT] Show Grants: If you do show grants; after you give GRANT REPLICATION command. You should get below grants from same slave host via CLI. 

Check your server bit if it is 32 bit and if you try to install 64bit package percona's xtrabackup . It would end up throwing such error messages. 

The slave lag persist on slave0. Final Update (I guess)> I disabled Network consuming mysqlbinlog puller runs via cron. Seems to be draining seconds_behind_master from 700 to 490. 

I haven't tested this but this seems to be a working command, ensure that you have $URL$ - Build 14332 fixed to have prctl system call supported on Windows.