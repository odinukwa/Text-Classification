The data may contain inaccuracies, it may be missing rows or contain spurious values. But with any luck you'll end up with more than was in the month old backup. 

If I've missed the point, you can hopefully at least use the following script for creating some test data :) 

The odd looking choice of delimiter |Â¬| is a typically safe combination that avoids collisions in text fields. So far anyway. As you mention in the comments you can minimise growth of the log by switching to bulk-recovery. Given that you will have no other activity during this maintenance window you could switch to simple, run the process, switch back to full and take a full backup instead. Note that use of in the example will create a separate transaction for each batch. 

There was also a recent article on SimpleTalk discussing these flags, Statistics on Ascending Columns. 

For performance data you'll want to look at installing the Management Data Warehouse alongside your Central Management Server. It isn't as useful as the various 3rd party tools for collecting SQL performance data but it's an improvement on roll-your-own Perfmon/SSRS/Excel solutions. 

Login details are stored in , so it makes sense that they would be gone if you've replaced the database files. The files in the template data folder are referred to in User Instances for Non-Administrators as "clean system database". I don't have an Express edition to hand to check but I would assume that if your now using a "clean" database, the only login that would be present is "sa" with a blank password. 

COALESCE is internally translated to a CASE expression, ISNULL is an internal engine function. COALESCE is an ANSI standard function, ISNULL is T-SQL. Performance differences can and do arise when the choice influences the execution plan but the difference in the raw function speed is miniscule. 

If recovery is taking a long time to complete and there doesn't appear to be any activity you may need to restore from backups. If you're feeling brave you should start reading everything you can find about repairing/recovering suspect databases. More information on what happened before the database ended up in this state would make for better answers e.g. what actions you carried out, what errors are in the SQL error log etc. 

Do not "take vendor recommendations with a large dose of salt", unless they're from the sales or marketing department. Quite the opposite, if you can get access to their technical implementation guys, make friends. Likely this is one of the new breed appliances that virtualize the storage pool and include auto-tier capabilities. One example would be Compellent. They perform all manner of SAN voodoo such as: 

Despite this question having already being answered and the answer accepted, I'm going to try and make a case for the contrary approach... DO NOT EVER BLINDLY KILL A SPID Unless you know what work that SPID has done, you have no understanding of the scale of the rollback you're about to initiate. The worst kind of blocking chains are those which have unexpectedly brought your 24/7 system to its knees. 99% of the time, the query at the head of that blocking chain is going to be a plain old select query gone awry, queueing up any and all write activity behind it. In these circumstances, a KILL will save the day. The other 1% of the time, that rogue query will be the last step in a long running transaction that will take as long to rollback as it did to get to the stage it's at now. Taking a moment to understand how you got here will 99% of the time be unnecessary. 1% of the time, you could be saving minutes/hours/days of downtime. 

Better tool for the task is sp_whoisactive, which reports per transaction log statistics if the parameter is set. 

Typically, backups to file are appended with a timestamp (e.g. MyDatabase_FULL_201202060900.bak) so you have the option of using a script to generate the restore sequence given a directory of files. Google will give you dozens of examples, as will the SSC script library. I have this script in my library currently. If the server you've taken the backups from is available, you can generate the restore script from the tables and , example here. Note SSMS will generate the correct script for you and you can use Profiler to capture the queries it uses to do so. 

Misunderstandings of nested transactions and savepoint usage arise because nested transactions are not what we would expect them to be. To all intents and purposes there is no such thing as a nested transaction. The actions of a set of nested transactions are not committed until you issue the outer most and a will undo the actions of all. Nested transactions exist to support transactions in procedures which could be called from external processes that may have started a parent transaction, from parent procedures that may have done the same, or where no prior transaction exists. They do not provide nested commit and rollback behaviour. 

Yes, kind of, no, depending on your perspective. Change a record once and the modification will be recorded to the log and when a checkpoint occurs, written to disk. Change the record 15 times between checkpoints and you have 15 changes recorded in the log but the modified record will only be written to disk once, on the next checkpoint. 

Yes. The buffer and plan cache associated with the database before restore are not related to the database post-restore, so they will be cleared. To all intents and purposes the restored database is a completely separate, unrelated entity. There is nothing in cache that can be reused. 

My initial playing around with various queries suggested no pattern at all but on paying closer attention it appears to be predictable for serial plans. I ended up at KB314648 which @AustinZellner mentions: 

One approach I've taken in the past is to incorporate log backups in to the reindex/rebuild scripts. Record the log size and free percentage before processing each table, check free percentage and size afterwards. If less than x% of space is free or if log growth has occurred, backup the log. 

I'm inclined to agree with @Catcall, database recovery should be top of the list. The implications of both backup and recovery options are typically the most poorly understood outside of a DBA team and the most likely to result in disaster. 

Edit: @AlexKuznetsov's comment prompted my re-reading the question and removing the very obvious error in my answer. Note to self on late night posting. 

If it were a query that returned more than 1 row I'd speculate that someone at the vendor (way back when, given the version of SQL Server) stumbled on the query optimiser producing a preferable plan when was specified as a hint. As it's returning just 1 row, the explanation probably has more in common with the infinite monkey theorem than reasoned judgement. A junior saw a hint for and decided that was preferable to his query not being fast. 

Whatever is, it needs to feature in the index. Again, difficult to tell due to the obfuscation but probably as the sole indexed value, everything else . Can't deduce from the query why your doing it this way but the repeated joining to doesn't make much sense as it stands. Back to my earlier point, its all guesswork without the full picture. Original: The seek is on , you've included this rather than make it part of the index. Can't be sure this is optimal for the entire query without seeing the query and execution plan but to turn this particular lookup into the non-clustered index seek you're looking for: 

While they are not great examples of best practice, the SQL Server sample databases would be good place to start. They include an OLTP, data warehouse and analysis services databases for a fictional organisation. Studying the differences between them should help you make sense of how OLTP (transaction) and OLAP (analytical/BI) databases differ and why. $URL$ 

You haven't specified the database platform your considering but at this size/scale, it's unlikely to matter. 5kb per record is trivial. 1 million 5kb records is < 5GB, still trivial. 10 million 5kb records... still not something to lose any sleep about. If we were to get platform specific, a typically exhaustively researched white paper by Paul Randall on SQL Server filestream storage suggests that it outperforms table storage where files are 1MB or greater in size. Below 1MB file sizes, the positives are primarily around filestream bypassing the buffer pool. The positives for database storage: 

I bookmarked Phil Factor's blog post Normalisation and 'Anima notitia copia' today as it neatly summarises the case for and against normalising certain types of data. Run the following query on a SQL instance and see if you agree. 

Prove there is a problem. Identify and document crazy queries and data access patterns. Approach the vendor with your evidence of a problem. If you can show that a re-write of a query shows improvements are possible, it gets harder for the vendor to ignore your concerns. If your getting no response from your contact at the vendor, escalate. If you can't make enough noise to be heard, maybe your CTO can. If you've reached step 4 without any success, you're likely to be sacrificing follicles or noticing the telling first signs of grey. This is where a voodoo doll of the third party development lead can ease the stresses of the day. A dartboard featuring the company logo is an acceptable alternative. 

I'm going to have to disagree with Aaron (and the accepted answer). Aaron's approach is the way I like to deal with "DBA stuff" e.g. maintenance scripts. I would never want to do this with a library function that would be called by a user database. Why? You'll be heading for the database equivalent of DLL Hell. 

Simplistic approach, you could record current log usage from the dm_io_virtual_file_stats DMV before and after your batch process. This would be polluted by other server activity however, so only useful if you can test in isolation. 

Alternative approach for pre-SQL2008 is to use a CSV splitter. Jeff Moden has carried out exhaustive testing of various approaches to this. Your example would become: 

The syntax Aaron suggested would also function as you require at . The same would be achieved by specifying a hint on : 

If you aren't using any SQL2008+ only features (e.g. compression) you will be able to script out the objects you require and execute in SQL2005. If you're not sure if any SQL2008+ features are in use, trial and error (script and execute, see what happens) will soon tell you where the problems are. This is very different to restoring a backup from 2008 to 2005, which is not possible. 

All of the platforms you have mentioned can run close to zero data loss configurations. All of them could be deployed in a configuration that will fail. Platform choice is one part of the puzzle. It will be your implementation of the platform that determines whether or not your requirements are met. 

TempDB (assuming that's what you're referring to) has absolutely nothing to do with SQL Servers execution of transactions. Are you confusing the role of TempDB in snapshot isolation levels? 

Depends on the cloud environment you're using but typically it makes sense to put them on the same logical drive. If you need additional IOPS you can stripe across multiple volumes but still present a single drive. In a cloud environment you are not the sole consumer of the storage you are allocated. You get a small slice of a very large pie and as such all IO is random in nature. There is nothing to be gained from trying to separate sequential access (log) from random (data). Nothing to be gained from the recovery angle either as you can't insist that the two volumes will be allocated from different arrays. Also, there tends to be a different class of protection afforded to the availability of storage from the big players. Azure storage for example is triple replicated within the data centre, with an additional copy replicated to a failover data centre by default. 

That equates to 110GB on a 128GB server. Reasoning being that the data warehouse will likely continue to grow and could eventually grab more than you want it too, so put the correct limit in now. It will get forgotten about otherwise. When your data volume gets beyond the 110GB limit, pay closer attention to the servers free memory. If you consistently have additional GB to spare you may consider raising the limit by a few GB. Other than that your only other fine tuning options are with trace flags, which should be treated with the caution they deserve. Test, test and test some more. The three candidates I can think of that would make sense are: 

Edit: Following update to question regarding queue lengths: Queue length counters are oft misunderstood when evaluating SQL Server performance. One of the best analogies I've come across is from a Simon Sabin blog post, Disk Queue Length - a bit like buying Guiness. In your case, they are so low that you might get by with half the spindles. Would still be interesting to see the other stats from the above query though.