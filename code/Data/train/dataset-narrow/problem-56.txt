Colors shown on your display or saved to standard image file formats use 8 bits per component. So to store these colors it suffices to use four bytes (). These colors are usually using sRGB color space, which includes a non-linear "gamma" transformation that redistributes precision to make it somewhat more perceptually uniform, so it's actually pretty close to the limit of how finely humans can perceive color differences already. (However, the sRGB gamut is only a subset of all the colors that are physically perceivable. Wider gamuts need more bits.) However, if you're generating or processing colors in graphics software (and not just loading/storing them), there are various reasons you might want to use more precision. 

The file has no vertex normals, and by default Mitsuba will generate smooth normals for OBJ files that don't specify their own normals. This creates the magnification effect: the box with smooth normals forms a convex lens! By adding this line to the box object in the scene file: 

The GPU hardware only supports nearest-neighbor, bilinear, trilinear, and anisotropic texture filtering. However, nothing stops you from implementing your own texture filtering in the pixel shader. To do this, you'd use to grab a 2×2 block of texels around your sample point. Then you'd manually calculate the position of the sample point relative to the texel centers. Armed with the raw texels and the interpolation position, you can then apply whatever interpolation function you like. The position calculation would be something like this: 

It looks like the way you're using smoothstep isn't quite right. With lerp, the first two parameters are the endpoints of the output range, and the third is a 0–1 value specifying how far to interpolate. It looks like you're trying to use smoothstep the same way, but it doesn't work like that: the first two parameters to smoothstep are endpoints of the input range, and the third is a value in the input range; the output of smoothstep is always in 0–1. It's almost the inverse of lerp in that regard. (See also the GLM docs for smoothstep). To use smoothstep here, you'd want to do something like this: 

In other words, the wavelength dependence of photometric units is different to what you might be expecting. In ordinary RGB color spaces, white is (1, 1, 1) and has a roughly flat radiometric spectrum; but in a supposed "photometric RGB", (1, 1, 1) would not be white; it would be a purpleish color, with less energy in the green range and more in the red and blue ranges. A similar problem would afflict spectral renderers trying to measure all their bins in wavelength-dependent photometric units, but even worse, as the radiance needed to generate a given luminance diverges toward either end of the visible spectrum, where the human luminous efficiency function goes to zero. So, if you wish to use photometric units, IMO it's better to "cheat" a bit and not use true wavelength-dependent photometric units, but just use some fixed wavelength (such as 555 nm green, which is the peak of the human luminous efficiency function), or perhaps an average over the spectrum, as a reference unit, and apply that single unit to measure all wavelengths. This will get you in less trouble when importing RGB colors and spectra from other sources, and when generating them as outputs. 

Texture baking can be accomplished by simply rendering the mesh in texture space. In other words, you set up a render target matching the size of your texture, and draw the mesh with a vertex shader that sets the output position to the vertex's UV coordinate (appropriately remapped from [0, 1] UV space to [−1, 1] post-projective space). The vertex shader can still calculate the world-space position, normals, and whatever else it needs and send those down to the pixel shader. Then the pixel shader can do its thing without even being aware that it's rendering into texture space instead of the usual screen space. Lighting and shading calculations will work as usual as long as the pixel shader inputs are hooked up correctly. You don't have to do any complicated inversion operation; you just take advantage of the GPU rasterization hardware, which is happy to draw your triangles at any coordinates you choose. Naturally, this requires that your mesh already has a non-overlapping UV mapping that fits within the unit square, but texture baking doesn't really make sense without that. 

This will weight all pixels equally within a box. To make a more circular shape you can change the iteration bounds based on the value of . Also, I doubt you need or want to round off to the nearest pixel as you're doing. The UVs coming into the fragment shader should already be correct, and moreover they are not multiples of the pixel size; they are the coordinates of pixel centers, so they contain a half-pixel offset (which is exactly what you want for sampling a screen-sized texture). For a more efficient algorithm, you could downsample the original image 2x, then run a 5px-radius neighbor search on that one instead of a 10px-radius search on the original. This will make the circle shape somewhat less precise but depending on what you're doing, it may not matter that much. Another approach that's less brute-force but still precise would be to build a spatial hierarchy of the green points. This could be as simple as a uniform grid where each grid square stores a list of the points inside it, or as complicated as a kd-tree. Your fragment shader could then walk this data structure to efficiently search the nodes within a radius of the fragment, and count the points it finds. 

The above approach will work for pretty much any kind of surface that can be parameterized, not just spheres. However, in the case of spheres with the usual $\phi, \theta$ coordinates, there's an even quicker shortcut that you can do with just vector math, no trig at all. Let $C$ be the center of the sphere, and $A$ be a vector representing the axis of the spherical coordinates—i.e. pointing from the center toward the north pole. Then you can just calculate $T = \text{normalize}\bigl(A \times (P - C) \bigr)$, then $B = N \times T$ as before. 

Projective transformations (represented by 4×4 projection matrices) are invertible. You can go from NDC coordinates back to view space using the inverse of the projection matrix, in the same way that you go from view space to NDC. That is: take your NDC $x, y, z$ coordinates, append $w = 1$ to make a 4D vector, transform by the inverse projection matrix, then divide out $w$ to get back to 3D. View space and NDC space are both fundamentally 3D spaces, though with a 4D homogeneous representation, so the transformation between them doesn't throw away a dimension, despite the divide by $w$. (Some information is lost, though: the sign of $w$, which corresponds to whether a point was behind or in front of the camera. However, this isn't usually a concern, since we cull or clip everything behind the camera during rasterization anyway.) As for the shader code, it can presumably be simplified according to which components may be nonzero in the inverse projection matrix. You may only need the $x, y$ diagonal elements and the lower-right 2×2 submatrix, as long as you don't need to support off-axis projections (which have additional nonzero components in their matrix). 

In terms of the HSV color space, "muted" colors are those with lower saturation and/or value. "Deep" colors are saturated but not too high in value (e.g. deep red) while colors with both high saturation and value might be called "bright" (e.g. bright red). "Colorful" is somewhat more vague; it might mean bright colors, but it also might mean having a range of different hues within an image or palette, such as including complementary colors or analogous colors rather than just variations on a single hue. Note that these are general terms that would be understood by any graphic designer or artist, but the specific algorithms that Adobe Kuler attaches to these words are proprietary and only known to Adobe. 

To accurately simulate this phenomenon, offline renderers use techniques like path tracing and photon mapping. For real-time purposes, we either precalculate it offline, or we approximate it somehow. Screen-space ambient occlusion (SSAO) is based on the observation that you can detect corners and crevices by looking at the depth buffer (and possibly also the normal vectors) of a rendered image, and so you can calculate approximate AO as a post-pass. The depth buffer is a coarse representation of the geometry in the scene, so by sampling depth buffer values in the neighborhood of a target pixel, you can get an idea of the shape of the surrounding geometry, and make a guess how darkened by AO it should be. 

What is wrong with it seems to be explained on the next two slides: it leads to non-energy-conserving results, where the apparent brightness of the volume changes depending on the scattering coefficient. My read of it is that the "wrong" integration code implicitly assumes a constant transmittance over the extent of each voxel. But the trouble is, when the scattering coefficient is high, the transmittance will fall off significantly within a single voxel. For instance, if you have voxels of size 10 cm, but the scattering coefficient is 1 / (5 cm), then the transmittance falls off by exp(10 cm / 5 cm), or about a factor of 7.4 over the length of the voxel! Therefore, according to slide 28, a better approach is to explicitly account for the transmittance falloff over the voxel length, while still assuming constant inscattered light. Fortunately the integral has a simple closed-form solution. So, they're saying to evaluate that formula per voxel in the integration loop, in place of the former "wrong" one. For completeness: in the code from slide 27, replace this line: 

The rule is that to compute the next mipmap size, you divide by two and round down to the nearest integer (unless it rounds down to 0, in which case, it's 1 instead). For example, a 57x43 image would have mipmaps like: 

With a good-quality upscaling filter (bicubic, for example), there's no particular importance to the 3:2 ratio. Starting from a higher-resolution source will produce a better-looking final image. It's true that if you're programming the upscaler from scratch, making one for a fixed ratio like 3:2 is much simpler than making one that handles arbitrary ratios. But image-processing software, monitors, etc probably have arbitrary-ratio upscaling built in anyway. 

Baked indirect lighting The next "level", so to speak, of techniques involve baking (pre-computing offline) some representation of the indirect lighting in a scene. The advantage of baking is you can get pretty high-quality results for little real-time computational expense, since all the hard parts are done in the bake. The trade-offs are that the time needed for the bake process harms level designers' iteration rate; more memory and disk space are required to store the precomputed data; the ability to change the lighting in real-time is very limited; and the bake process can only use information from static level geometry, so indirect lighting effects from dynamic objects such as characters will be missed. Still, baked lighting is very widely used in AAA games today. The bake step can use any desired rendering algorithm including path tracing, radiosity, or using the game engine itself to render out cubemaps (or hemicubes). The results can be stored in textures (lightmaps) applied to static geometry in the level, and/or they can also be converted to SH and stored in volumetric data structures, such as irradiance volumes (volume textures where each texel stores an SH probe) or tetrahedral meshes. You can then use shaders to look up and interpolate colors from that data structure and apply them to your rendered geometry. The volumetric approach allows baked lighting to be applied to dynamic objects as well as static geometry. The spatial resolution of the lightmaps etc. will be limited by memory and other practical constraints, so you might supplement the baked lighting with some AO techniques to add high-frequency detail that the baked lighting can't provide, and to respond to dynamic objects (such as darkening the indirect light under a moving character or vehicle). There's also a technique called precomputed radiance transfer (PRT), which extends baking to handle more dynamic lighting conditions. In PRT, instead of baking the indirect lighting itself, you bake the transfer function from some source of light—usually the sky—to the resultant indirect lighting in the scene. The transfer function is represented as a matrix that transforms from source to destination SH coefficients at each bake sample point. This allows the lighting environment to be changed, and the indirect lighting in the scene will respond plausibly. Far Cry 3 and 4 used this technique to allow a continuous day-night cycle, with indirect lighting varying based on the sky colors at each time of day. One other point about baking: it may be useful to have separate baked data for diffuse and specular indirect lighting. Cubemaps work much better than SH for specular (since cubemaps can have a lot more angular detail), but they also take up a lot more memory, so you can't afford to place them as densely as SH samples. Parallax correction can be used to somewhat make up for that, by heuristically warping the cubemap to make its reflections feel more grounded to the geometry around it. Fully real-time techniques Finally, it's possible to compute fully dynamic indirect lighting on the GPU. It can respond in real-time to arbitrary changes of lighting or geometry. However, again there is a tradeoff between runtime performance, lighting fidelity, and scene size. Some of these techniques need a beefy GPU to work at all, and may only be feasible for limited scene sizes. They also typically support only a single bounce of indirect light. 

Both methods end up doing the same calculations when you break it down. Rotating a vector $u$ with a matrix: $$\begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix} \begin{bmatrix}u_x\\u_y\end{bmatrix} = \begin{bmatrix}u_x \cos\theta - u_y\sin\theta \\ u_x \sin\theta + u_y \cos\theta \end{bmatrix}$$ Rotating a vector $u$ using complex numbers: $$\begin{aligned} (\cos\theta + i\sin\theta)(u_x + iu_y) &= u_x\cos\theta + iu_y\cos\theta + iu_x\sin\theta - u_y\sin\theta \\ &=(u_x\cos\theta - u_y\sin\theta) + i(u_x\sin\theta + u_y\cos\theta) \end{aligned}$$ I wouldn't expect either one to be appreciably faster or slower than the other, since they all end up doing the same set of basic operations, i.e. 4 multiplies and 2 adds. Conceivably, complex numbers could be faster when you have a large number of rotations stored in an array, because they have only two components instead of four and therefore more of them fit into each cache line. 

Using the same name is exactly how you tell OpenGL that you want the value passed through from vertex to fragment. You say "we already hand the color value from vertex shader to fragment shader", but that's not correct. Usually, the only value that's passed between shaders automatically is position, and that's only because it feeds into the GPU's rasterization hardware to draw the triangle on the screen. Any other values such as color, normal, texture coordinates, etc. that you want passed between shader stages have to be explicitly hooked up by the shader author. And the way you do that in GLSL is to create an variable in one stage, and an variable in the next stage, with the same name.