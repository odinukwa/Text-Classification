If you have already selected the keywords you want grouped, why not write a function that finds all occurrences of words in the list and replaces it with your one, core word? What you're describing sounds like a more advanced version of stemming. 

You should try compensating for the imbalanced data and then can you try a lot of different classifiers. Either balance it out, use SMOTE to interpolate (this always struck me as too magical), or assign weights. Here's a nice article walking through it with caret, which is what it appears you're using: $URL$ 

Creating a model is having the computer write a small program. If you have a multi-layer network, then multiple nuggets might be interacting to produce complicated results. That its pretty much how things like face detection work - you start with a series of really dumb "detectors" that look like basic rectangles and they combine in ways so that at the top, you're recognizing Brad Pitt. 

I would figure out what exactly you want for #3 in terms of the number and type of data elements and then choose a data storage method only then. Unless you are going to be passing the unstructured documents directly to your model, you don't need MongoDB's capabilities. 10,000 records is small, but since you mentioned you want to calculate some aggregated statistics on your patient-level data, you could likely get by with something as simple as MySQL or SQLite. Spark and Map/Reduce are actually competitors, with Spark stealing MR's spotlight lately. You might need one or the other for feature extraction but they're probably overkill for the rest of the stuff you described. 

You want to write structured queries (have your cake) but have unstructured files (eat it too). Is there any way you can play with the files a bit and have a common schema between them, with some fields just empty? That would make it easier. That being said, BigQuery is probably your least bad solution as you found. Amazon's new Redshift Spectrum is kind of similar but will also require a schema to be defined. 

Load your saved model. Repeat steps #2 and #3 from above. Send the cleaned data into your model for a prediction/recommendation. 

In general, you perform object detection to find potential faces, and then you perform face recognition on the face(s) detected. This article walks through the various steps quite well: $URL$ The author also has a Python library that does recognition of multiple faces in a scene with video examples: $URL$ 

This dataset is included with scikit-learn, a popular ML library for Python. $URL$ It is postings to Usenet and categorized by the group. The group titles are not exactly "categories" like you would see on Google News, but each newsgroup is supposed to be on a specific topic as indicated by the name, so the concepts are similar. For example: 

Yeah, some (not all) libraries have that numeric input limitation. What you're looking for is called "one-hot encoding" which can basically convert an N level factor like your product categories to N binary columns. Here is a post describing a few ways of doing that: $URL$ 

When users enter company information, they should be prompted first to select from a clean list and only type in a value if they say it is not present. After the name of the organization, enter some unique ID at the end in []. Hopefully the imported goods have that on the package. Then parse that unique number out and use that matching. 

You already made it log scale which would have been my first idea. How about use lattice to make it three plots with different Y start and end? Each plot should have a center point that covers one of the three groupings you have and you can stack them 1x3 (vertically). 

What kind of aggregations are you doing? Most DBs can handle basic counts pretty easily. 10 million in a month sounds like something a large MySQL instance could handle. How quickly does everything have to update? If the users only expect the summarized data to be updated monthly, you have a lot of time. If you have to update it more often, it might make sense to have MySQL collecting it but Redshift presenting it. Redshift's analytical focus and columnar storage means you may not need pre-aggregated data until your volume gets much higher than what would require aggregation in MySQL. I was using a 4 node Redshift cluster with the smallest instance types and it could do a count across >300 million rows in 17 seconds. 

Yes. Many data sets are one-hot encoded into only 0s and 1s because some ML algorithms work best that way. $URL$ 

Get dplyr Add columns to your dataframe for year and month. Group_by year and month and take averages. 

Michael Stonebraker started a company that claims to do just that, schema matching using machine learning: $URL$ Their site no longer has many details on their approach but this article talks about some of the techniques they used, like : 

I would try a semi-supervised learning technique where it passes you scraps and asks you to label them. What you're looking for will likely be kind of domain specific depending on the type of site. In the end you'll probably have a bunch of heuristics like: 

I'm surprised you got 80% accuracy and think you will have to do some labeling if you want to improve. Even people have problems with this! For example: 

What you're looking for is called an ensemble model which means it is a compilation of several models to improve the results. This is a very common technique for winners in Kaggle competitions. Since you're using R and caret is a popular way to do ML in R, here's a package just for that purpose on caret: $URL$ 

Define what features you want. Figure out how to extract those features. Figure out how to store those features. Pass that dataset to a ML model for training. 

There are plenty of NLP libraries for .Net if you Google that. Please be more specific as to what you want to accomplish when they query the DB. 

Anywhere between 1 feature like a simple linear regression done in Excel and 1 billion (see Vowpal Wabbit). 

How random is the distribution? You could "smooth" the problem away by taking daily, weekly, or monthly averages of those KPIs per employee and clustering on that. 

People - you could try using IMDB images of actors. License plates - that's going to be much tougher. Do you want to recognize which country a license plate is from, or do you want it to be able to read the unique letters and numbers off a license plate? If it is the first case, you could use sample license plates that are typically published government offices. If it is the second case, you would likely have to find public images that happen to include license plates. I know Google likes to blur them out on Street View.