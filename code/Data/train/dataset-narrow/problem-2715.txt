From Unity 4.5, you have to register the event. "iOS: Added support for render events (GL.IssuePluginEvent). Please note that you need to manually register them, as iOS do not support dynamic libraries. Check trampoline for UnityRegisterRenderingPlugin function." For example: 

Because there has to be a heirarchy of stuff. Common, uncommon, rare and ultra rare. It hooks into that primal part of people that likes getting stuff, and gets frustrated if it doesn't. Slot machines are built this way. They pay out often, but only occasionally give the big prizes. The big prizes could be bigger if they only ever paid about big prizes. But then people wouldn't play, because losing 100 times in a row makes them quit. But if they win a buck here and 10 bucks there, they keep playing. So you do the same with your loot drops. The player gets used to the hope every time something drops that it's something good. 

We have two main subclasses of GameInfo for the two game types we have. I'm wondering if it's possible to get one of these to read a different .ini file because we'd like to move a few of the controls to different buttons for that mode. It looks to me like it's just going to read Input.ini for this, regardless of the game type. 

The function returns regardless of whether or not the components intersect. However, the two closest points returned overlap when they intersect. I think this is a bug in the function. Have reported it to Epic; we'll see what they about that. This works very well for rectangular solids, spheres and cylinders (the three types of PrimitiveComponents). 

Put the profiler on it. Try a huge sheet and a small one. Measure the difference. This is the very best way to answer these questions, since you don't want opinions: you want answers. Doing this test yourself is the best way to get an answer, since it will be for your actual use case. 

Use your common memory management sense. Use pools/freelists for things that are frequently allocated and deallocated (i.e. particles), free memory blocks that aren't in use. However, don't try to preallocate large chunks of memory upfront. iOS doesn't guarantee memory nor does it swap, so any memory you have allocated is taking resources away from the system and other apps. If the system cannot get enough memory because your application is hogging all of it, then it will resort to killing your app. You should handle the applicationDidReceiveMemoryWarning for what it was designed for by deallocating as much memory as possible. Although iPhone 4 has lots of memory, but with all the multitasking features and the way people tend to use them, you may actually have less memory than you think. (Also, don't forget to profile your app to see whether memory allocation is a bottleneck!) 

In my opinion, since your only job is art and design and you are by yourself, you have a very weak stance getting to become the owner of the project since you have the least contribution of work out of the whole team. You are highly dependant on them to make your project work, whereas they do not need you to get their passing mark on the finals. If you want full ownership, either work alone on your project, pay people to put it together, or somehow manage to convince the people you are working with to assign full ownership to you despite working for you for free. 

You are using a derivative of the song, which falls under copyright. Also it doesn't matter whether you credit the original artist or not make a profit. It is still copyrighted. 

There are a few relevant resources on the UDN about this subject (keywords "level streaming"), and even a convenient "hub" article that links to all of them: $URL$ "The level streaming functionality in Unreal Engine 3 makes it possible to load and unload map files into memory as well as toggle their visibility all during play. This makes it possible to have worlds broken up into smaller chunks so that only the relevant parts of the world are taking up resources and being rendered at any point. If done properly, this allows for the creation of very large, seamless levels that can make the player feel as if they are playing within a world that dwarfs them in size." 

Are render targets supported in a general way? For the sake of argument, would it be possible to set up a camera to render depth from an alternate POV, then do shadow map-style depth comparisons while rendering the main view? 

If you were to employ post-process antialiasing such as SMAA, it would not differentiate between geometric edges and texel edges. This might be sufficient in conjunction with the nearest neighbor texture filtering. 

Direct3D 10+ applications don't have this concept of a lost device state, and avoid most of this bookkeeping work in the process, I believe by virtue of the fact that the WDDM implements "virtual memory" for video RAM. That might explain why you wouldn't see this behavior in Metro applications. 

Not a book, but you should check out the math curriculum over at the Khan Academy. I'm in the process of using these videos to brush up on my own math skills. They cover an extremely broad range of material, and the author has been praised for his teaching style. IMO, truly an amazing resource. $URL$ Algebra 

Just show them the design document. Game ideas/design are rarely the main factor in ones success. It's how you implement the idea and the team that puts it together is what will make the difference. You need not to worry about people copying your game idea, that is, if your game ideas were truly revolutionary, you will have many copycats but will always be one step ahead of them. Only you know how to implement your ideas in your unique way and no one can copy that. If your ideas was easily copied, it is not because someone stole your idea, it is because you did not have the ability to carry out that idea in a successful way. You will need this level of confidence to gain investors trust. 

Downvote me if I am wrong, but I don't see why people are recommending swapping y and z. That would make your coordinate system from being right handed to left handed. Try this yourself, swap the y and z, and reorient the axis so that x points right and y points up. You will see that z points the opposite direction from its original (away from the screen). The proper way is to rotate around the x-axis, which is swap y and z, and then invert the sign of the final z. I think the easier solution is to build your models with y value being up. 

When used properly, there shouldn't be any overhead in performance. But, they aren't exactly zero-overhead compared to raw arrays. Size overhead A typical std::vector container will use 3 pointers. Since you probably already track width and height yourself, your raw array could be done in one pointer since you already know the size. Allocation overhead You will incur some overhead when allocating the memory for the std::vector. However, this is the same overhead you incur if you allocate your own memory. Zero-initialization overhead std::vector will actually zero-init your memory when using resize(), which may or may not be significant to what you're doing. But, from a performance standpoint, accessing a std::vector is the same as accessing an array. 

You should definitely check out the Halo Wars GDC presentation, "The Terrain of Next-Gen." It discusses using full vector field displacement instead of simple height field displacement. For something a little less revolutionary, maybe check into geometry clipmaps. There's a good article in GPU Gems 2 here. 

The other answer doesn't give you the whole story, even with the addition of Tetrad's comment. It's of vital importance to take into account the non-linear distribution of precision for values stored in the Z-buffer. zNear has a much greater impact on artifacts such as Z-fighting than zFar does. For a better explanation of why, see this article: $URL$ 

Cg is not technically HLSL, but syntactically they are virtually identical. Having said that, it's an older book, but NVIDIA has made "The Cg Tutorial" free to read online. It covers the basics of the programmable pipeline in a very comprehensive fashion, and it should all translate to HLSL with minimal effort. 

Materials in the Direct3D 9 sense were simply parameters for the fixed function pipeline, which implements a Gouraud shading model (IIRC). The values of a D3DMATERIAL9 struct can be thought of as nothing more than shader constants. The definition of a "material" is entirely dependent upon context. It's the distinction between fixed function and programmable that is most important here. When you're using shaders, you define the shading model yourself. Therefore, even though many of the surface properties that are described by a D3DMATERIAL9 (e.g. diffuse color) would likely apply to your shading model of choice, it's up to you to do something with those values. The fixed function pipeline was essentially one monolithic "effect" (or pair of vertex/pixel shaders) that you couldn't change. You could only pass it different parameters to control the behavior, whether these parameters were render states or D3DMATERIAL9 structs. You might find this instructive: FixedFuncShader It's a partial reimplementation of the fixed function pipeline using D3DX effects. It'll help you understand the parallels between the two concepts. 

One thing I hated about backtracking in Metroid Prime was how when I first came across something new and interesting, I couldn't tell if I had the tools to interact with it or not. I'd leave, hoping I'd get the tools later. But then I'd hit a dead end. Then I'd wonder if I had the tools to navigate the dead end, if I was missing something, or if I had to go back to the previous place I was unsure about. This led to lots of aimless wandering; so I quit playing. Personally, I find it wrong to show the player something before he can do it (unless it's painfully obvious they can't do it, like what's that little hole at the bottom of the drain pipe in Banjo Kazooie? Braid only made this mistake once, and I thought it made a huge difference to its flow. It meant that if I couldn't figure something out, it was because I had more thinking to do, not more wandering (or trying to figure out if I had to wander or think)). If you're going to reuse level geometry by making the player traverse it multiple times, it is my opinion that this should not be done in such a frustrating way. Halo did this well in certain places. Like when you were going back through a bunch of levels you already traversed, but this time by flying. There was no presentation of something that you thought you could do but couldn't, but they still found something interesting to do when going back through. 

If you are 100% sure that the NPC's level will always be enough to specify their stats, then level in db/calculations on client is a good option. Less hitting the db means your game scales better with the number of players. However, it locks your design into this box. There are ways to patch on differences not related to level at a later date, but this could cause problems as the rest of your game might not be coded or designed with this in mind. So this decision depends on how likely it is that the current design will be the final design. 

There isn't a "it depends" on company. You don't get hired based on what you wear, so wear the standard interview gear (e.g. dress shirt, tie and dress shoes). This is universally accepted by all companies and dressing otherwise is running the risk of "it depends" on company. 

This is more of a question of, does the use of the accelerometer improves/or integrates well with the controls of your game or does it makes it worse. Having good controls contribute to the overall design of a game, so ask yourself whether you want to design a good game or a bad one, then implement the best controls (whether it be a touch interface, onscreen pad or accelerometer) for having the best experience for your game. 

If you plan to sell the game with their font in it (whether you transfer it to a bitmap or not doesn't matter), then you are distributing it outside of the scope of the license so you will need to contact CheapProFonts (whom I presume are the owners of the font) for a commercial license. 

Attending college not only builds your skills as a programmer, it gives you an opportunity to build your social, communication and teamwork skills. Just the social aspect of being with other people is a necessary experience for working in teams inside a company. Beware of online school scams, however (same goes for some colleges, but there are alot of online ones which are just scams). They'll take your money, give you a pass and hand you a diploma, but that diploma won't be recognized by any company once you graduate. Be careful! 

It is not possible to determine an accurate estimate because the rate of each click-through or impression changes for each ad that is displayed. However, if we take some made up stats from $URL$ it gives that you could theoretically have a estimated cost-per-1000-impression (eCPM) of $1.11. Based on that, it means you would need over 270,000 unique impressions a day to make $300. If your eCPM was only half of that, you would have to double the numbers. If we look at the cost-per-click model, and you assume the average cost-per-click is $0.30 (again, a made up figure obtained from the link above), then it will take 1000 unique clicks a day to make $300/day. Remember, the above stats are completely made up. Each person will have a different experience and the only true way to find out is to implement it in your app and record your own stats. 

I realized I should probably update this, in case anyone reading this neglected to click through to the thread on the UDK forums. I was told by a moderator that everything I want to do is impossible without engine modifications. 

Is it possible to override all the materials on the client (en masse), such as those being used for the terrain or BSP geometry in the current level? (For implementing alternate vision modes and things of that nature.) 

realtimecollisiondetection.net blog The blog of Christer Ericson, author of the excellent book Real-Time Collision Detection (who'd have guessed? ;). Equal parts general programming, physics programming, and graphics programming. Selected reading: 

The Cg toolkit supports its own version of effect files, CgFX. These support techniques, states, annotations... I don't have any experience with them personally, but they are supported by FX Composer, at least. 

If by hardcoded vertices, you mean glBegin ... glEnd (OpenGL), or Draw(Indexed)PrimitiveUP (Direct3D), these methods are inferior because they result in the vertices being transferred from system memory to the graphics card every time they're called. If, however, you're putting your vertices into a vertex buffer like you should be, it really doesn't matter. If your cube is logically an art asset, like the Horadric Cube, or the Weighted Companion Cube, it should probably be stored as a model on disk. If your cube is, on the other hand, intended to be used for skybox rendering, and therefore immutable, hard-coding it is probably fine. 

I understand that UE3 employs deferred rendering. I know you can access the color and depth at the current pixel in a post process. However, is there any way to access these or other G-buffer attributes in a more general way? (Normals, position, values at neighboring pixels...)