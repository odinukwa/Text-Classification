The function problem which Goodhart's Law described is the changes of the underlying model from which the data is generated. One way to look at this is that the data generating process in the past is bias. Say we our try model is a product of two variables $z = xy$. In the past, $y$ has always been constant at $a$, so you come to the conclusion $z$ must equal to $ax$. But this is only true when $y=a$. In Goodhart's setting (economic policy and social science in general), it is usually the prediction value of $z$ that causing the changes in $y$. It is a dynamic system of which we only have a bias static slice of data. 

There is no point in using LSTM if your window size is 2. You will end with one input and one output. There is no longer term dependency to be learned in this setting. You might use a fixed window approach if your individual sequence is very long. You can slice your series using the window approach. The benefit of doing this 

Answer In ensemble methods, the predictions are typically made by majority voting. Using python and Numpy. 

That's just the problem of over fitting. @tom has covered the most commonly used techniques. The only thing I will add is using larger sample size. Sometime this can be achieve cheaply by using data augmentation techniques (i.e. image rotation etc). 

No there isn't one in general. The architecture of the network (including depth) is considered to the new "feature engineering". It's more of an art than science at the moment. Anything above 3 layers worth considering. 

CIFAR10 has 10 class label. So by random guessing, you should achieve an accuracy of 10%. And this is what you are getting. This means your algorithm is not learning at all. The most common problem causes this is your learning rate. Reduce your learning rate by replacing your line, 

This is actually really easy to implement in any deep learning framework with automatic differentiation capability. You need two neural networks. One to approximate the $X$, another to approximate the $dX/dt$. 

Side Note Also, K-Fold is generally used for hyper-parameter tuning. After you made a decision on the hyper-parameter you would refit the model on the entire dataset. If you want to use ensemble methods like random forest or gradient boosted tree, you would apply the algorithm to the entire dataset. It is not clear to me whether your method of ensembling will have any benefit over the standard approach. In your approach, each classifier is trained on the smaller set of data on all features available, and I can't think of any reason for us to do this. 

Convolution Neural Network (CNN) is the state of art algorithm for your problem. The standard terminology for this kind of problem is "feature detection". Here is a good resource for the basics. $URL$ Your biggest problem probably is going to be the size of your training images. 1,000 is most likely not going to be enough. You should try to augment your data by rotating and translation etc. 

Say we have a pandas series with the following values What is the most efficient way fill the nan value with 0 in the middle. so we have 

In line 809, you can find that takes a as an argument to the constructor function and it defaults to . 

There is a hole a the wall you want to cover.Let's think about a series questions you would like to ask? 1. What is the shape of the hole? Is it a circle, a square, a ellipse or maybe a triangle? Notice some of those shape is more general than the other, like a square is just a special rectangular. 2. How do you describe the exact size of that hole? what's the diameter of the circle? How long is the size of the square? What does this has to do with fitting the model? The first question about the shape of model is an analogy to the problem of model selection. There are lot of different model you can use to fit a model. There is ordinary linear regression where the model can be represented as a straight line. There is also more complex model like polynomials. And like how square is a special rectangular, linear model is just a very special polynomial. Once we have a particular type of model we have in mind, we need to find a set a numbers that completely describes that model. This the process of fitting your model. In the case of linear regression, the model is a straight line, and the fitting the model is to find which straight line. 

Scaling Standardize you data by $(x - mean(x)) / std(x)$ Most K-mean implementation by default uses Euclidean distance which assumes the equal importance of all features. This requires proper scaling to prevent one action dominates the others. Dimensionality K-mean is not robust against the curse of dimensionality (see this post). So as always it good to reduce you dimensionality before feed it to the algorithm. Do some feature engineering first. For example, login and page load can be group together as a measure of passive engagement, page edit and page created could be considered as a single activate engagement feature. Also, you can try using some standard dimensionality reduction algorithm like PCA 

This is not recommended because any error in prediction is compounded over time. please find my answer in the following post for a detailed explanation Is time series multi-step ahead forecasting a sequence to sequence problem? How can this behaviour be explained? If you are getting constant prediction value, the chance is you are having so-called "gradient explosion problem", try to clip the gradient before backprop might be the solution. 

The intuitive explanation goes like the following. In ELU, whenever x became small enough, the gradient became really small and saturated (in the same way it happens for Tanh and Sigmoid). The small gradient means that the learning algorithm can focus on the tuning of other weights without worrying about the interactivity with the saturated neurons. Consider a polynomial of degree 2 which can be represented as a smooth surface in a 3-d space. To find the local minimum, a gradient descent algorithm will need to consider the steepness in both x and y-direction. If the gradient is both negative in the x-direction and y-direction, it is not clear which way is better. So it is sensible to choose a path somewhere in between. But what if we already know everything is flat (zero gradients) in the x-direction, then it becomes a no-brainer to go for the y-direction. Or in other word, you search space become much smaller. special note In deep learning, there is a lot of claims without enough empirical evidence or in-depth understanding to support it. In ELU's case, while it might be true that it results in faster convergence for some datasets, it could also be true that it makes the learning algorithm to stuck at the local maximum for a different dataset. We just don't know enough yet.