Google Code supports several version control systems (SVN, Mercurial and git as of this writing). Hedgewars appears to be using Mercurial, as you can see from the Source -> Checkout tab on Google Code. To get the code, you must clone the repository as described on that page, using either the command line Mercurial tools or a GUI wrapper like TortoiseHg. In general, to get the code from any Google Code project, you navigate to that project's Source -> Checkout page and follow the instructions. Some projects will also create .zip or tarballs of the their code and host them on the Downloads tab, but Hedgewars does not appear to do this currently, so you'll have to get the code the aforementioned way. Some projects also provide a wiki page, like the one you linked, that further explain any dependencies or special steps you'd need to take to acquire and build the code. Not all projects do this, however. 

I don't know anything about them specifically, but I would think that in general HTML5 is probably a decent way to prototype games even without a framework or engine: quick, direct, low overhead and easy to iterate on. 

This documentation on the 2.0 Unity tools preview for VS implies you have to both enable Unity exception support via (setting "exception support" to true, as you've done) and enable the appropriate CLR exception masks in Visual Studio's built-in exception settings window (check the box next to "Common Language Runtime Exceptions"). For best result you may want to check and then un-check the CLR exceptions tick, as that will ensure all the exception subtypes are enabled to get started with. 

Probably, but not because voxels are faster or cheaper to render (in fact, they're more expensive to render since you must convert the data set). Voxel representation lends itself to manipulation easier than polygons -- so it's much easier to add and remove chunks, bits and pieces of the world represented by the data set. 

Double-dispatch doesn't help with collision detection per se, it helps with the prelude to collision detection: given two arbitrary shapes, determine which collision detection algorithm to use (ray-triangle, ray-circle, ray-box, triangle-circle, et cetera). In many programming languages -- like C++ -- dynamic dispatch (what happens when you call a virtual member function) resolves based only on the type of a single parameter to that function (the "this" pointer, in other words the type of the invoking object). Double-dispatch is a way to resolve a function call based on two of the function's arguments. In a collision detection scenario where you have two objects ( being the base class for all possible concrete collision shapes), you want to select a collision detection function based on the actual run-time type of both objects. There is a lot of information on implementing double dispatch (or its more general cousin, multiple dispatch) in C++ floating around. Many of them apply equally well, excepting in syntax, to whatever other C++-like language you may be using: 

I would write the server in the language/toolchain you know best, rather than learn a new one for that purpose, especially if you have any concerns about making the server "watertight." You're more likely to make mistakes with technology you are unfamiliar with. Unless the cost is prohibitively more to run a Windows server, I would stick with that platform. However, you may also be able to find hosting providers that will run *nix servers with mono available, so you can run your master server that way. 

You are missing a few key points. After the application of the projection matrix, you have a 4-component vector in clip space (not screen space), which is a homogeneous coordinate system in which clipping will be performed (after your vertex shader). After clipping, the surviving coordinates are divided by the component to get normalized device coordinates in (-1, 1). A transformation will then be applied to move from NDC space to window coordinates, where the X and Y coordinates are normalized based on the viewport provided to OpenGL and the Z coordinate is normalized based on the depth range, which is ultimately what gives you your (0, 1) range for depth (unless you use to set a different range). If you want to access this normalized Z value in your vertex shader, you will need to do the computation manually in the shader (based on the information above). 

If you are reconstructing the vertex data for every object every frame, you're doing it wrong. Or at least, you're probably doing it inefficiently. Moving objects in a scene can and should usually be done by changing the world transformation matrix used to render that object, not changing the vertex data directly. Skeletal animation has entirely on-GPU solutions, as well. Decomposable objects can be rendered with distinct transformation matrices, such that they appear together until they need to be decomposed, and then the matrices are adjusted to make them split apart as desired. Objects that are not visible or not spawned need not be rendered, even if their vertex data is on the card from a prior frame -- they don't need their buffers destroyed/recreated every time their visibility toggles. 

Minecraft's item and crafting recipes are defined in code, not in data. So you'll have to decompile the files to do any useful analysis. To include mods in this analysis, you'd need to decompile them as well. The main Minecraft server contains an file that lists all items along with their item IDs. The format of the calls that insert items into the registry is pretty simple and amenable to extraction via and some regular expressions. For example, on a not-necessarily-very-recent copy of the file, they look like: 

Building tools to aid in the development and management of content is an excellent idea. However, it costs quite a bit of time and energy, so you need to weigh those costs carefully against the needs of your project (and potential future projects which would use the same technology platform) and decide if it's going to be worth the investment or not. Tools allow you to create abstractions that allow your game assets to be interpreted by two (often quite different) world views: that of the programmer consuming the resources in the game engine, and that of the designers producing those assets. Renaming For example, in an ideal world, you'd refer internally (that is, in code) to resources by a fixed identifier that will never change regardless of what you do to the resource (for example, a UUID). When assets are referred to by name, renaming or moving an assets correctly involves finding all existing references to that asset and update them. This can be quite slow, and cause quite a few problems (especially in larger teams). But using UUIDs sidesteps that problem entirely. You can move and rename assets all day long, and since the relationships between them are established via identifiers that never change, there's no problem. Of course, UUIDs are ugly and cumbersome to remember and talk about. Thus, you build content management tools that expose primarily the names of assets, allowing designers to reason about assets by name, but which internally maintain the links between assets by GUID. You can build anything along the sliding scale of that abstraction: you can have no tools, have designers hand-edit XML files and manually deal with UUIDs; you can build a complete toolchain for them that deals with everything; or you can do something in between. For example, you can refer to everything by name, eschew UUIDs, and provide a much simpler tool that does assists in renaming or moving assets when needed (handling all the fixups, et cetera). This involves much less effort and investment and solves a good percentage of the serious problems involved with reference-by-name systems. Collaboration There are similarly two simple solutions for dealing with the collaboration problem you mentioned: either use exclusive editing... "locking," which you mentioned, which you can define by policy -- if somebody has a file checked out, don't edit it -- or enforce via tools; in both cases the practicality of this approach is gated by whether or not your source control system exposes that notion. Distributed systems like Git do not, but systems like Perforce do. If your SCM doesn't do it you'd have to manually implement it, and that's a fair bit of investment alone right there. You can also design the pipeline to support merging. You can either choose a text-based format that is suitable for manually merging (XML is possible to hand-merge, but it's not the best option, and it's easy to make mistakes), or you can build a system where your content data is actually transactional when stored (instead of "this value is five" you store "this value is changed to five") and built a merge system yourself using that transactional data. We did this at ArenaNet for Guild Wars 2, and it is the crux of how the game can ship such extensive content updates every two weeks. This again is a choice along a sliding scale: the system built at ArenaNet is powerful and saves tons of time. But it also took a lot of time to implement (two full-time engineers over a few months). You will need to make hard choices about what problems you really need to fix and what problems you can live with based on the needs of your project and your available resources; it's quite possible your team is small enough you can solve the collaboration problem with zero engineering effort by just having everybody talk to eachother and coordinate work. Tips If you're going to go and invest in building a full-featured editor for your content, I'd strongly advise you to consider an approach where you don't hand-build every editor screen. It's a massive time sink; instead, consider an approach where you can generate the UI based on the properties of the object you are editing (much like the Windows Forms does). This approach, combined with a simple extensibility system for plugging in editors for the very few types that might need special editor views, is much easier to develop and maintain. It's the basic premise we used at ArenaNet for our tools platform, and it saved man-years of engineering time. You should also focus on doing the smallest, fastest thing that could work first. Create a minimum viable product and iterate on that towards a larger future goal: don't stop all other development for two months sinking time into some kind of crazy super editor. Always keep in mind what you actually have to accomplish for your project and what your actual available resources are. Make sure you are chasing after features and tools you need and not those that sound cool or sound useful. 

The class was made assembly-internal in commit 101497, which was aimed at centralizing the input validation for the triangulation algorithms in a single place according to the check-in comment: 

and while one can accomplish some of what is neccessary by hooking the various events of the class, those events don't provide an obvious place to put the bits of code to perform constant periodic updates to game logic objects or to begin and end a render frame. What technique should such a game use to achieve something akin to the canonical 

This will allow you to avoid problems when users don't have the required hardware, without having to worry yourself about which specific hardware supports which sensors (especially since that information will certainly change over time). The battery of sensors available in the Windows Phone API are detailed in the documentation. Currently: 

where is a unit vector representing the axis of rotation and is now a 3D point in model space representing the rotation point. As it happens, quaternions can be converted to and from matrix representations, so you could do your concatenation that way should you so choose. Or you could just leave everything as matrices (quaternions have some nice advantages such as being easier to interpolate in a sane fashion, but whether or not you need that is up to you). Also: 

You have to set entire state blocks at a time, even if you want to change only a single value. You should create all the state blocks you need and hold on to them as long as you may need them. Don't create-set-release. That introduces unnecessary resource churn. The API will share redundant state block data under the hood so you shouldn't need to worry about that. The underlying API will handle state changes for an entire state block efficiently. The debug layer will notify you of entirely redundant state block sets, however. To improve performance, aim for minimal state change anywhere in a graphics API. That usually means batching operations on geometry that shares needed state. However, don't be afraid to change state when you need to. 

Chromatic aberration is caused when a lens can't focus every color to the same focal point. A simple way to fake this effect, and render it as a quick full-screen post-process, is to apply an offset to each color channel in a fragment shader. By using a different offset for each channel, you can achieve a reasonable facsimile of the desired effect. An example of this technique can be found here; the fragment shader would look something like this: 

Yes. A "diffuse texture" is generally just a flat, plain color map. It's one of the oldest kinds of texture map still in widespread use today. An ambient occlusion texture map takes into account local effects of light sources (and is usually just comprised of magnitudes, not actual colors). The Wikipedia page on AO has an image that illustrates the difference visually. 

is a fine way to store "dynamic" (as you call them) things like items, but the real benefit of the vector is not that the thing you are storing can change, but that the number of items in the vector can change without minimal effort on your part. To illustrate, were you to have store your objects as an array, you'd have to fix the size of the array at compile-time (), which means you have a fixed upper limit on items (among other issues, which aren't as relevant to this specific subject and so I will skip over them). You could also dynamically-allocate the array at runtime () which would let you resize the array later by allocating new storage, copying the items over, and deleting the old storage. This, however, is a lot more work for you to do. Fortunately, that's what does for you -- it is a dynamically-sized array implementation, essentially, and handles the memory management for growing the array beyond its current capacity for you. It's definitely the option I'd suggest you pursue for now -- but note that you'll still want to create an class. It's very easy to use: 

Your constructor checks and starts or stops the timer. However, the only code (that you've shown) that could ever set is in the callback. is called when a key is pressed, and will modify appropriately, but no code will ever check again. The constructor already ran to completion, so unless there is code you haven't shown here, nothing will ever stop the timer. Presumably the animation only starts because you initialize to . Why don't you just start and stop the timer inside ? 

Remember that the point of "entities should not draw themselves" is to remove the dependency on the renderer from the entity system and to allow render logic to be factored out into a few places as possible. Both increase quality the code through improved maintainability and decreased coupling. A class is a good approach, however, if the entity itself generates it, you're still in the domain where the entity "knows things" about how to render objects. There is no difference between an entity that takes a renderer and says "renderer, put a textured quad here" and one that has a function that returns an object saying "renderer, draw a textured quad there." Instead, remember that non-member functions increase encapsulation and move that logic for assembling a render object for a given entity out to a level of abstraction that already knows about both the entity system and the renderer: the game itself. This way there is no loss of generality. The "game" object is already the thing that ties all your various disparate systems together. At a high level, in other words, your game loop might contain something like this: 

RenderMonkey and FX Composer were both tools like this, although they've been relatively abandoned recently. 3D modelling tools like Max or Maya often usually have shader/effect/material pipelines built in, but they are probably prohibitively expensive. 

is the desired rotation angle (your ). In addition to the linked Wikipedia page, you may want to read The Matrix and Quaternion FAQ. 

As has been noted, if you are using , you don't need to handle the cleanup as the resources will be released when the context is lost. 

The last one is probably the approach you'd take if you were starting from "scratch" trying to dig up any undocumented options. It's not a guaranteed win by any stretch, but it's a nice balance of simplicity and effectiveness that makes a good starting place.