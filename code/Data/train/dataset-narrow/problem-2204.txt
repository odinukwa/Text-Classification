Over $\mathbb{C}$, testing for zero and evaluation is "almost" the same in the following sense: Assume you have a decision tree which tests whether some irreducible polynomial $f$ is nonzero. We are working over $\mathbb{C}$, therefore we can only test for equality but we do not have "<". That is the important difference to the second example in the question! Now take the typical path, i.e., the path taken by almost all inputs (we always follow the "$\not=$"-branch). Furthermore, take the typical path of all elements in the variety $V(f)$. Let $v$ be the node at which these two paths take a different branch for the first time. Let $h_1,\dots,h_m$ be the polynomials that are tested along the common prefix of the two path. Since $V(f)$ is closed, all elements that lie in $V(f)$ and reach $v$ also lie in $V(h_m)$. Therefore, if $f(x) = 0$, then one of the $h_i$ vanishes on $x$. We apply Hilbert's Nullstellensatz to $h_1 \cdots h_m$ and get that $f g = h_1 \cdots h_m$ for some polynomial $g$ that's coprime to $f$. In short, while we are not computing $f$, when deciding whether $f(x) = 0$, we have to compute $fg$ for some coprime $g$. 

Valiant's classes are defined over some field. They can use arbitrary constants from that field. To draw some conclusion about Boolean complexity classes, one needs to replace these arbitrary constants by small discrete constants. Here GRH comes into play, since it ensures the existence of enough primes with certain properties. 

Essentially, your are asking why people believe that DFT or DHT do not have linear size arithmetic circuits. First, researcher tried to find such circuits for decades but did not succeed. Second, there are superlinear lower bounds (Morgenstern's theorem, see the book by Bürgisser, Clausen, Shokrollahi) in restricted models. From Morgenstern's theorem, in follows that in linear arithmetic circuits of linear size over the complex numbers for the DFT, the absolute values of the constants used in the circuit go to infinity as $n$ goes to infinity. 

If you know the factorization of $m = p_1^{e_1} \cdots p_n^{e_n}$ you can compute modulo each $p_i^{e_i}$ separately and then combine the results using Chinese remaindering. If $e_i = 1$, then computing modulo $p_i^{e_i}$ is easy, since this is a field. For larger $e_i$, you can use Hensel lifting. 

Deterministically, you can do it as follows: Give your edge $e$ the weight $n+1$ and add a self-loop of weight zero to each node that does not have one so far. Existing edges get weight one. By removing the weight zero loops, a maximum weight cycle cover in the new graphs induces a partial cycle cover in the old graph that (1) contains your edge $e$ and (2) has a maximum number of edges. The vertices of the partial cycle cover induce the graph you are looking for. If the cycle cover in the new graph does not contain $e$ at all, then there is no such subgraph. You can compute a maximum weight cycle cover by reducing it to maximum weight perfect matching. 

Commutative algorithms are not studied that much, because you cannot use them recursively by cutting larger matrices into smaller blocks like you do in Strassen's algorithm. Since every noncommutative algorithm is a commutative one, commutative algorithms can be trivially as efficient as noncommutative ones. 

De Groote (On Varieties of Optimal Algorithms for the Computation of Bilinear Mappings. II. Optimal Algorithms for 2x2-Matrix Multiplication. Theor. Comput. Sci. 7: 127-148, 1978) proves that there is only one algorithm to multiply $2 \times 2$-matrices with 7 multiplications up to equivalence. This might be a unique feature of $2 \times 2$-matrix multiplication. (Note: You will find different variants of Strassen's algorithm in the literature. They are all equivalent with the right notion of equivalence.) If you now start to prove a lower bound for $2 \times 2$-matrix multiplication - see the book by Bürgisser, Clausen, and Shokrollahi how to do that - then Strassen's algorithm or some variant shows up quite naturally. You will find out a lot of identities that determine how the products look like. Then you can finish by some guessing. (De Groote's proof shows that even guessing is not necessary.) Schönhage once told me that Strassen once told him that he found his algorithm in this way, by trying to prove a lower bound. 

I assume that you mean $P_n(x)= \sum_{i=0}^n \frac{x^i}{i!}$. Peter Bürgisser shows that if this problem is hard, then computing the permanent is hard ($URL$ To my best knowledge, your problem is open. 

If we request that each $|V_i| \ge 3$, then this is the 2-factor problem, see the book Combinatorial Optimization by Schrijver. If you allow $|V_i| = 2$, then we can solve this by replacing each undirected edge by two directed ones and compute what is called a cycle-cover. This can be done in polynomial time by reduction to bipartite matching. 

I think such a behaviour is true for 1-Tape-DTMs. On the one hand, we have $\mathrm{DTIME}_1(O(n)) = \mathrm{DTIME}_1(o(n \log n))$. Unfortunately, the only reference I know is in German: R. Reischuk, Einführung in die Komplexitätstheorie, Teubner, 1990, Theorem 3.1.8. On the other hand, it should be possible to separate $\mathrm{DTIME}_1(O(n))$ and $\mathrm{DTIME}_1(O(n \log n))$ by the language $\{ x \#^{2^{|x|}} x \mid x \in \{0,1\}^* \}$ using a standard crossing sequence argument. 

I think the following procedure computes a basis of the column span of an $n \times n$-matrix of rank at most $\log n$ in $\mathrm{AC}_1$. If you have a matrix of size $n \times 2 \log n$, you can find a basis of its column span in $\mathrm{AC}_0$ by running over all subsets in parallel and checking in parallel whether a nontrivial linear combination vanishes (say over $\mathbb{F}_2$). From all sets of maximal rank you select the lexicographically smallest. Now given an $n \times n$-matrix of rank at most $\log n$, we can find a basis of its column span in $\mathrm{AC}_1$ as follows: Divide the columns into two halfves, compute a basis recursively. From these two bases we can compute a common basis using the algorithm from above. 

A matrix is called totally regular if all its square submatrices have full rank. Such matrices were used to construct superconcentrators. What is the complexity of deciding whether a given matrix is totally regular over the rationals? Over finite fields? More general, call a matrix totally $k$-regular if all its square submatrices of size at most $k$ have full rank. Given a matrix and a parameter $k$, what is the complexity of deciding whether the matrix is totally $k$-regular? 

The exponent of computing a basis of the kernel is the same as the exponent of matrix multiplication, see the book Algebraic Complexity Theory by Bürgisser, Clausen & Shokrollahi. So it can be done in time $O(n^{2.38})$. 

This answer summarizes and expands my comments above: 1) If the size of the underlying field is large enough, then this problem has efficient randomized algorithms: You have to test whether the resulting determinant is nonzero as a polynomial. You can use the Schwartz-Zippel lemma for this. (Search for the keyword "Polynomial Identity testing" for further algorithms/results.) 2) The question whether this problem can be derandomized is open. Derandomizing it implies circuit lower bounds. See the work by Kabanets & Impagliazzo. (Comput. Complexity 13, 2004, $URL$ the problem is called SDIT (Symbolic Determinant Identity testing) in the paper. 3) Gurvits has a deterministic algorithm for some special cases of SDIT ($URL$ He refers to SDIT as "Edmond's problem". (This item was provided by Joshua Grochow. I suggest that when you want to upvote my answer, you should upvote any of his answers with probability 1/4 instead.) 4) Over finite fields (size is fixed), it is NP-complete. Over GF(2), you can just arithmetize a given formula in 3-CNF (that is, write it as an equivalent arithmetic formula) and then use fact that you can write any arithmetic formula as a poly-size determinant (proven by Valiant, see also Bürgisser, Clausen, Shokrollahi, Algebraic Complexity Theory, Chapter 21). Over other finite fields, you can start with appropriate CSPs instead of 3-SAT.