Bernoulli Naive bayes does not assume gaussian distribution of all continuous features, because it does not make sense. Gaussian Naive Bayes assumes gaussian distribution for continuous features and it is the appropriate way for using Naive Bayes approach if you have continuous features. On the other hand, if you have binary categorical data then the appropriate approach is Bernoulli Naive Bayes. If your features are categorical but not binary then you could transform them into binary categorical using dummy boolean variables for each available value of the categorical features. The main point of Naive Bayes algorithm is the assumption of feature independence, which in some real world classification problems does not hold. You need to specify a conditional probability p(x|y) of the feature value x given the class label y. Since Naive Bayes assumes that all features are conditionally independent given the class, you can mix different likelihood models for each feature considering any prior knowledge about it. For example, considering a continuous feature you might assume that p(x|y) is normally distributed, then you can stimate the mean and variance for this feature under each class in the training set and after that you can use the PDF of the Normal Distribution to estimate p(x|y). Considering another feature which is categorical, you can estimate p(x|y) using a Bernoulli or multinomial event model and multiply the two conditional probabilities together in the final prediction (since they are assumed to be independent anyway). 

A unit vector in a normed vector space is a vector of length 1. A unit vector is an euclidian vector of length 1. Not every euclidean vetor has length 1. 

It depends on the different families of the learning algorithms. For example, Naive Bayes and all the semi-Naive Bayes models assume that the features are indpendent, which is not true in most real world problems. The latter is a disadvantage of Naive Bayes learning algorithm, although in some problems as in natural language process these models usually perform well. Most of the machine learning algorithms do not assume such thing (feature independence). Finally, there is no holistic rule for that. 

Apache Spark is running applications up to 100x faster in memory and 10x faster on disk than Hadoop, because it is able to reduce the number of read/write cycle to disk and store intermediate data in-memory. Hadoop MapReduce reads and writes from disk, so it less fast than Apache Spark. In Apache Spark it is easier to program, because there is a plenty of high-level operators with RDD. In contrast, in Hadoop MapReduce usually someone needs to code every operation. Apache Spark is capable of performing batch processing, Streaming Machine Learning or Analytics Applications all in the same cluster. So there is no need to manage different component for each need. Hadoop MapReduce only provides the batch engine. So it is dependent on different engines. As a result, it is not easy to manage too many components. Apache Spark can process data coming from real-time event streams at the rate of millions of events per second. In contrast, Hadoop MapReduce fails when it comes to real-time data processing as it was designed to perform batch processing on huge amounts of data. Apache Spark – Spark is little less secure in comparison because it supports the only authentication through shared secret password authentication. Both Apache Spark and Hadoop MapReduce are fault-tolerant. So there is no need to restart the application from the beginning in any case of failure. 

Data Cleaning or Data Munging as it is referred in most cases, is the process of transforming the data from the raw form that they exist after their collection into another format with the intent of making it more appropriate for their future process e.g. training models etc.. This process is taking place at the beginning of the whole procedure and before the training and validation of the models. In text mining problems, you have also to treat the punctuation marks, remove the stopwords (it depends on the data representation that you will choose, for unigrams it is fine, but for bigrams it is not recommended at all) and also do the stemming or lemmatization processes. 

Good performance on the training set and bad performance on the test set is due to overfitting. So you should try to find ways to tackle overfitting, such as regularization of parameters, parameter tuning using cross-validation etc... 

Since the problem you are trying to tacle is image classification, then classification accuracy is the appropriate measure of comparison. Also you could consider the Precision, Recall and F1 metrics (for multi-class problems). See below there is description of an extension about these multi-class metrics. A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems Another appropriate metric is AUC/ROC, which has to be extended cosnidering the multi-class case. See the link below Receiver Operating Characteristic (ROC) 

The range of the dataset values can be influenced by extreme values (outliers), so it is not the right way to check if the training set and the test set follow the same distribution. First of all you need to make a comparison using the five-number summary (min-Q1-median-Q3-max) and make a first conclusion for the distribution equality. A second option could be to perform a statistical test as the Kolmogorov-Smirnov test to check for distribution equality and see if there is a significant difference or not between the two samples. If the is a significant difference, then the classifier trained using a dataset with different distribution from the test set will have a poor performance on the test set. A solution could be to use Transfer Learning/Domain Adaptation Methods to tackle this difference and adapt the classifier trained on a different distribution for the test set instances. 

Actually both roles are recommended for someone from coding background. It depends more on the specific characteristics of each role in a company. Data engineering is more about infrastructure work, which means parsing data files, storing data in particular databases (SQL or NoSQL e.g. Mongo-DB), designing databases or designing the pipeline of the data process. Data Science is more about building models, selecting appropriate variables, performing exploration or validation of statistical models, hypothesis testing etc.. All these need good knowledge of at least one scripting programming language like Python, Matlab and R. In some cases there is also a need of Software Engineer skills for the implementation of applications related to Predictive Analytics or Machine Learning(or Statistical Learning). 

In the case that your training data is outdated, you might have a problem if you train a model considering that dataset and then applying it at a test dataset which is newer and not outdated. In the case that you have available some new data at the training time, one solution could be to perform domain adaptation/transfer learning methods to tackle this difference e.g. in the distribution of the features taking into account the small newer dataset and the large outdated dataset. Actually you are going to find out which part of the outdated dataset remains the same or which part could be useful after some transformations. You could check the related bibliography for more information about transfer learning and domain adaptation. 

About what you are talking about for the programming languages the right definition is reserved words. A reserved word is a word that cannot be used as an identifier, such as the name of a variable, function, or label, so it is "reserved from use". This is a syntactic definition and it is related to the compiling procedure of the source code of a program. Below you can find a list with the reserved words for the most popular programming languages Reserved Words of Programming languages Stop words are not the same as reserved words. Stop words are words which are filtered out before or after processing of natural language data. Stop words usually are the most common words in a language and there is no single universal list of stop words for each different language. Stop words are related to the natural language (text) and not to the source code in any programming laguage. 

Apart from having an average accuracy of the trained model, also you can use cross-validation to approximate the optimal value for the k parameter and the optimal metric. 

Multicollinearity could be a reason for poor perfomance when using Linear Regression Models. Multicollinearity refers to a situation where a number of independent variables in a Linear Regression model are closely correlated to one another and it can lead to skewed results. In general, multicollinearity can lead to wider confidence intervals and less reliable probability values for the independent variables. Also maybe other assumptions of Linear Regrresion do not hold. Linear regression needs the relationship between the independent and dependent variables to be linear. It is also important to check for outliers since linear regression is sensitive to outlier effects. The linearity assumption can best be tested with scatter plots. Linear regression analysis requires that there is little or no autocorrelation in the data. Autocorrelation occurs when the residuals are not independent from each other. 

First of you you should check if there is any among the features after adding the new ones. After that, you have to check if each feature is informative taking into account the target variable of the problem. For the latter you can perform a statistical test and check the p-value to determine if there is statistical significance, otherwise you could compute the Mutual Information between each new feature and the target variable. 

If you want to get more into Machine Learning and the mathematical/theoritical aspects that it includes, the book mentioned below is an option: Machine Learning by Tom Mitchell, McGraw Hill, 1997. 

Bootstrapping is any test or metric that relies on random sampling with replacement.It is a method that helps in many situations like validation of a predictive model performance, ensemble methods, estimation of bias and variance of the parameter of a model etc. It works by performing sampling with replacement from the original dataset, and at the same time assuming that the data points that have not been choses are the test dataset. We can repeat this procedure several times and compute the average score as estimation of our model performance. Also, Bootstrapping is related to the ensemble training methods, because we can build a model using each bootstrap datasets and “bag” these models in an ensemble using the majority voting (for classification) or computing the average (for numerical predictions) for all of these models as our final result. Cross validation is a procedure for validating a model's performance, and it is done by splitting the training data into k parts. We assume that the k-1 parts is the training set and use the other part is our test set. We can repeat that k times differently holding out a different part of the data every time. Finally, we take the average of the k scores as our performance estimation. Cross validation can suffer from bias or variance. Increasing the number of splits, the variance will increase too and the bias will decrease. On the other hand, if we decrease the number of splits, the bias will increase and the variance will decrease. In summary, Cross validation splits the available dataset to create multiple datasets, and Bootstrapping method uses the original dataset to create multiple datasets after resampling with replacement. Bootstrapping it is not as strong as Cross validation when it is used for model validation. Bootstrapping is more about building ensemble models or just estimating parameters. 

It depends on the ensemble model technique. If you are going to use a bagging approach then the term individual models or alternative models is the appropriate. But in the case of boosting methods the appropriate term is weak learner (classifier). Boosting algorithms is a family of machine learning algorithms that convert weak learners to strong ones. A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). 

Fine-tuning is the process in which the parameters of a trained model must be adjusted very precisely while we are trying to validate that model taking into account a small data set that does not belong to the train set. That small validation data set comes from the same distribution as the data set used for the training of the model. The split of the available data to train and validation set is random. Transfer Learning or Domain Adaptation is related with the difference in the distribution of the the train and test set. So it is something broader than Fine tuning, which means that we know a priori that the train and test come from different distribution and we are trying to tackle this problem with several techniques depending on the kind of difference, instead of just trying to adjust some parameters (usually we are doing this for reasons as preventing overfitting etc.) 

You can try to measure the similarity of the products that a user has bought so far with other user's purhcases (user based recommendation) or you can try perform associate rule among the items that the user has bought and other items (item based recommendation). You can also perform some clustering techiniques to find group of similar items or users. Another approach could be the following, if you have available the information for a user's purchase then you can try to predict the user's next purchase. This approach can be a Markov Model. In a Markov model the most recent state is predicted based on a fixed number of the previous states, and this fixed number of previous states is called the order of the Markov model. At your case, each state could be a different purchase. 

You could stepwise (backwards or forward) remove or add features to your feature subset. For the Feature Selection procedure, you need a metric to measure which features should be included in the reduced data set of your available data. One important entropy measure is Mutual Information. Mutual information is a measure between two (possibly multi-dimensional) random variables X and Y, that quantifies the amount of information obtained about one random variable, through the other random variable. The mutual information is given by I(X;Y)=∫∫p(x,y)logp(x,y)/p(x)p(y)dxdy, where p(x,y) is the joint probability density function of X and Y, and where p(x) and p(y) are the marginal density functions. The mutual information determines how similar the joint distribution p(x,y) is to the products of the factored marginal distributions. If X and Y are completely unrelated (and therefore independent), then p(x,y) would equal p(x)p(y), and this integral would be zero. If we assume that X is one Feature and Y is the target variable then we could measure their Mutual Information. We would like to keep the features with the highest mutual information between them and the target variable. Apart from the stepwise algorithms for selecting the appropriate features there are also some greedy methods also trying to maximize the Mutual Information between the joint distribution and the target variable. Below you can find some indicative links A review of feature selection methods based on mutual information Feature selection using Joint Mutual Information Maximisation