Complexity classes in communication complexity were introduced by Babai, Frankl, Simon in the paper quoted by Noam. The paper also develops the idea of completeness under suitable reductions. If you for instance describe the classes NP and co-NP it makes a lot of sense to describe the (co-NP complete) Disjointness problem as well. As to your second questions, if P is (in communication complexity) the class of problems solvable with polylog(n) communication deterministically, then the class EXP should be the set of problems solvable with poly(n) communication, which simply is everything. So it seems that such classes are not interesting. However, there is another way to get larger classes. Already PSPACE is defined (by Babai et al.) not in terms of some notion of space, but in terms of alternation. Interactive proofs are another way to get large complexity classes. So you can define the class MIP as the set of problems that can be solved in a communication game with two provers (who cannot talk to each other) and two verifiers (who can talk to each other and to the provers ). In the Turing machine world, MIP=NEXP, but what about in communication complexity (where NEXP does not seem to make sense)? First of all, MIP is not just the set of all problems due to a simple counting argument. Andrew Drucker (in his masters thesis) has shown something interesting about this class. He considers PCP's in communication complexity, which (by standard techniques) are equivalent to MIP protocols (his result is a little stronger than what I state here). What he shows is that for every problem in NP (the Turing machine class) and any way to split up the inputs, the resulting communication problem has an MIP protocol with communication polylog(n) (i.e. the problem is in the (communication complexity) class MIP). So, while MIP is not everything, finding an explicit problem that is not in MIP should be hard (not because we cannot find problems that are not in NP, but because it is not easy to imagine how the Turing machine complexity can come into play). That showing lower bounds for MIP is hard should maybe not be too surprising, because we don't even know how to prove lower bounds for AM protocols. 

Techniques for the design of algorithms often help in reductions as well. Therefore it may be good to learn about techniques used to design FPT algorithms, for which the notes of the Spring School on Fixed Parameter and Exact Algorithms (2009) may be a starting point. In particular, you may want to look at the following excellent overview talks: 

With respect to exponential time complexity, general instances and instances with constant maximum degree are equally hard: The sparsification lemma of Impagliazzo, Paturi, Zane (2002) shows that $n$-variable instances of $d$-Sat can be reduced to instances of $d$-Sat with at most $f(d,\epsilon)\cdot n$ clauses in time $\exp(\epsilon n)$. As observed in joint work with Husfeldt and WahlÃ©n, the sparsification lemma works for the counting versions of $d$-Sat, too, and especially for the case of counting $2$-Sat (which is equivalent to counting independent sets and counting vertex covers). Moreover, counting independent sets in an $n$-vertex graph cannot be done in time $\exp(o(n))$ unless the exponential time hypothesis fails. This is a yet unpublished observation announced in a talk during the Dagstuhl Seminar Computational Counting. 

Eickmeyer and Grohe (2010) prove that your candidate construction can be made explicit: take $d$ somewhat linearly independent linear hash functions $h_1,\dots,h_d$ and connect left vertices $v$ with right vertices $h_1(v),\dots,h_d(v)$. Eickmeyer and Grohe show that this construction gives $(k,\epsilon)$-expanders with left degree $d=k(t-1)/(2\epsilon)$, whenever $t$ is an integer, the left vertex set has size $n=q^t$, the right vertex set has size $m=dq$, and $q>d$ is a prime power. The hash functions $h_1,\dots,h_d$ are chosen in such a way that any $t$ of them are linearly independent. 

I want to remark that the answer is yes if you consider the input to be a clocked Turing machine, i.e., there is a clock that lets the Turing machine perform $p(n)$ steps and then accepts/rejects. Now checking whether the language decided by the machine is in NP is a syntactic property that boils down to deciding whether the machine is a well-formed nondeterministic Turing machine with a polynomial clock. 

The induced subgraph isomorphism problem has NP-incomplete "left-hand side restrictions" assuming that P is not equal to NP. See Y. Chen, M. Thurley, M. Weyer: Understanding the Complexity of Induced Subgraph Isomorphisms, ICALP 2008. 

Luo's book on the Extended Calculus of Constructions is also a good reference. ECC was quite influential in the design of Coq's type theory. 

We have been working on one for the last few years: Lem, a higher-order, typed language with backends for OCaml, Coq, Isabelle/HOL, HOL4, LaTeX and HTML. Lem has been used internally in the group to formalise various machine models (PowerPC, ARM, and so on), memory models (the C memory model, etc.), programming language semantics (OCaml Light), and so on. 

You might want to take a look at the compiler literature for some ideas. What you're describing is akin to a well-known compiler optimization (virtually any modern compiler worth its salt will do it) called constant folding. Of course, constant folding goes further in not just identifying constant expressions, but replacing them by their value. In particular, compilers use a dataflow analysis to identify such constant expressions. Any good compiler textbook will discuss this. 

For verified complexity analysis in other theorem proving systems, see e.g. Tobias Nipkow's paper on this subject using the Isabelle theorem prover ("Amortised Complexity Verified" at ITP 2015) which presented a framework for deriving amortised cost bounds of functional data structures and applied it to a number of well known data structures. The code is freely available on the AFP. I don't think anything Nipkow does is tied to Isabelle and could be easily ported into e.g. Coq. 

Levy's call by push value calculus makes a distinction between values and their thunks. For a value of type the computation has type . Lindley and McBride's Frank language, inspired by CBPV, also makes this distinction between computations and values explicit, though unlike Haskell, Frank is strict. 

For type theory, the TYPES/announce mailing list appears to be the place where these are announced. There's also the LOGIC mailing list where theoretical jobs are announced in the German speaking world. The Haskell subreddit at Reddit.com also has some announcements from time to time related to functional programming and type theory. The best thing to do is probably to subscribe to a big university theory group's announcement mailing list. Announcements get CC'd to those as a matter of course.