Here is one way to rewrite the query that should give the same results without nested CASE statements. I'm checking for NULLs first, to potentially short-circuit additional testing. I considered using a single WHEN, with an OR between each expression, but OR isn't deterministic so this may perform better. If those columns are mostly null (vs mostly filled), you could try rewriting it to test for IS NOT NULL instead of IS NULL, and see if there is a performance difference: 

Regarding your comment about the scan, that happens because using ISNULL() in the join manipulates the field, so it's no longer SARGable. It may be faster to split this up, rather than try to do all the work in a single query. For example: 

Add GO statements every few thousand lines, which helps break things up into smaller batches. Use Powershell or another language to create a stream reader send every line to sqlcmd. Chrissy LeMaire created a great script to do this for CSV files that you could use for inspiration: $URL$ I've used a variation on this method to import files with a half-million INSERT statements in less than a minute. 

There is no connection between these two tables, so a join won't really get what you need. If everything goes into a single result set, you need to UNION them together. Note that when doing a union, you get one set of columns, and you can't mix datatypes within a column (no concern here since all your data are FLOATs). Here's an example starting point: 

EDIT: @Peter the table has the below clustered index. This is the only index, there are no primary or foreign keys or other constraints. 

For each account, I need the first transaction date, the largest transaction amount, and the time taken to get from first to largest. Data is truncated and reloaded frequently, and I have read-only access (no schema or index changes). The table has well over a million rows, so creating and updating a temp table is way too slow; cross apply seems inefficient as well. With a CTE and window function I only have to hit the table twice. But is there a better approach? Simplified example of the table and my query follow: 

It's possible that you hit SQL's size limit for batch size. Check out $URL$ If your file is just line after line of then you have some better options than splitting the file. 

This has happened to me before when the InteractiveHeight was set to 0. The query only returned a few thousand rows, but since there was no paging in the report, SSRS was trying to put everything on a single page. Eventually I would hit the SSRS timeout before it reached the rendering phase, and have 0 rows returned. If that doesn't apply to your situation, please check out this link for some additional troubleshooting ideas: $URL$ 

EDIT based on comment: Since there can be many orders, what about this? It is SARGable so should index well, and you're not querying the table multiple times. 

I'm transitioning from an old web/sql box combined to a much more robust solution on a completely different host (800hosting to RackSpace). Our application has very high uptime requirements. It runs 24x7x365 and impacts literally thousands of sites, and I want to ensure the minimum possible downtime during the transition. (Don't ask how we're pulling this off with a single box right now.) My plan for the web server is a little bit of DNS work to point a new subdomain to the new server setup, and forward requests from the existing server. That'll take care of flipping everyone to the new web box fast enough. The problem is the database - how do I keep it in sync until I'm ready to flip the switch on the website (or is it worth the effort vs. just accepting the downtime to stop the app, backup, compress, transfer, and restore?). A few details. We're running Sql2k5 on the old system, 2k8r2 on the new. The database itself is ~30 gigs for the primary, ~60 gigs for the warehouse. I can live with downtime on the warehouse, but really want to minimize the impact on the main database. Any suggestions for the best way to migrate the database across from the old setup to the new? 

resizing files (data and log both) large number of inserts happening while 1. was going on one particularly heavy query running while 1. was going on a hard drive with some pretty high i/o times in general lack of memory on the server, so hitting the page file on top of everything else 

change the autogrowth settings so I'd have consistent growth when it happens, manually grew both the temp and primary database files to have more space, added additional vlogs to the temp database, and set up a notification so I can manually grow when the database gets below a certain space level. [nothing to be done about it] made the heavy query run less often; it was loading up data the user didn't always need, so changed it to run on-demand [working on getting a new server, this app is growing fast] [working on getting a new server, this app is growing fast] 

I have a fairly high-throughput application that occasionally decides to collapse on me. It's not very often - about once every ~3 weeks or so. When it does, if I check out perfmon, I see 100% "Avg. Disk Queue Length" pegging the server. During these times, I also see lots of nice connection failed messages from SQL Server. I'm no SQL Server expert, I can do the basics for indexing, taking backups, etc., but that's it. What would cause something like this? I was thinking perhaps it was a resize of the database (it was down to ~300MB available [and it's a 30 gig database]), or maybe some reindexing gone nuts? I do have one table in particular that has tons of inserts. Very few reads, but many inserts per second isn't unusual at all. The server has only ~4 gig of RAM as well, but we do have a dedicated warehouse box that rolls up data every night where most of the heavy querying is directed. Anyone got any thoughts on what might cause that huge queue length?