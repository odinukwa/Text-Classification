If you have a clustered (or covering) index starting with (Key1, Key2, Key3, ...) on the table that the CTE is querying, this should be a well-performing query. Can you add a "Key4", preferably an identity column, in the source table to your non-unique clustered index, so you perhaps can make the index unique? That way, you could also set "Key4" as your in the . This might improve performance, but it's just a guess. The fact that the Clustered Index Merge operator is over 90% of your plan doesn't by itself indicate a performance problem, it just says that there's not that much else to do in the query plan apart from the join. In fact, it may indicate that you have a well-optimized plan - when you have the proper indexes, a can be very efficient. 

How do I reset the rowcount to the actual contents of the table without using ? I've tried , and , but the statistics rowcount remains 10 million rows. Inserting a row or even deleting all of the rows doesn't update the statistics, because the change is too small. 

Conversely, here are customers that have receive an e-mail in the last 7 days OR have received two or more e-mails this month: 

One of the nice things about is that it allows you to access the source columns as well as the built-in and tables in the clause. My code may contain errors, as I haven't actually tested it. My blog post from a few years ago goes into a little more detail, including on query performance. 

Identical subqueries are normally only performed once. However, the way you write a query isn't neccessarily the actual execution order, so there are no guarantees. To make sure, view your estimated execution plan with one or both of the subqueries, and compare them. In your example, you could place the subquery in a JOIN instead, which would make it both reusable and more easy to read. 

Here's a long shot: I think your Management Studio has for some reason changed the keyboard mappings to the 2012/2014 default setup, which is more similar to the regular Visual Studio apps. This changed as of SQL Server 2012 so, among other things, Ctrl+F1 now opens Books On-Line. To verify if this is the case for you, you probably won't be able to execute queries using Ctrl+E or view estimated query plans using Ctrl+L. I found a way to reset the keyboard shortcuts to the "classic" setup and wrote a blog post about it a while ago. The short version is: 

Not sure if you can build a "trigger" to capture role changes. I would probably set up a SQL Server Agent job that runs pretty frequently. Check 

The order in which tables are joined isn't defined in the query (unless you use a join hint, which I wouldn't recommend), so you can't be sure that A is joined to B before B is joined to C. To answer your question, in most cases SQL Server can tell from the statistics on the table (or other criteria) that the query won't return any rows. In that case, another JOIN won't make any difference from a pure performance perspective. 

Not that I know of. The clause requires an existing table. On a related note Using as opposed to or can have negative side-effects depending the situation. Some examples why I think you should create the table first: 

The above VBA function will return October 1, 1971. The parameters are . Also, the following will work (thanks, @ypercube!) 

This pattern may be a general solution for column comparisons, generated by a BIML package or similar ETL tool. 

For more reading on this type of operation, take a look at , aggregate functions (like and ) and in T-SQL (although I used a "manual" construct to build a pivot table instead of the built-in syntax). 

Your clause contains an explicit datatype conversion, which is terrible for performance, and can also cause this type of problem. This query will probably show you what's wrong: 

This expression is sargable which is what you want for optimum performance. Like @Mikael indicates, you would do well to design one of your indexes so that is the first column, and that all the other columns used in the query are included in the index. If you have lots of columns in the query, using the table's clustered index is probably best. Read up on covering indexes. 

Note that we have an invalid date in the table on the "29th" of february 2015, stored as a varchar. But since we're querying 2016, we don't expect to touch that row. However, there are multiple ways to solve this query: 

When you have a specific date format in your varchar column, you can tell what that format is in order to get a correct conversion, independently of language or regional settings. In your case, 

... and repeat this process for each table. You may run into problems with foreign key constraints, so you may have to arrange the order of the tables carefully, but in the end you may be better off not having any foreign key constraints at all in the local database. If your existing rows can change, the SQL script looks similar, except you download the data to a temp table first, assign a unique clustered primary key to the temp table, then ("upsert") the contents of the temp table to your local table. If you don't like or trust , you can obviously use and the old fashion way. Note that you won't catch rows that have been deleted from the remote table this way. Using SSIS With SSIS (or if you decide to build your own local application), the work is the same, but you won't need linked servers, and instead of using , you'll dynamically build SQL statements and run them through an ADODB or similar database connection. I've included the "build your own application" option for completeness; I wouldn't go down that road. Indexing considerations For the remote query to be as efficient as possible, you may want to investigate if you can get the remote DBA to set up an index on the "last updated" column of each table. Backup option In the end, I would probably consider Aaron Bertrand's suggestion and try to set up a solution where you download a backup of the remote database (perhaps a full backup, once per night), restore that backup locally and perform the sync operation locally on your server. Real-time Returning to the real-time issue - the driving parameter here has to be the business. If management needs up-to-the-second accuracy in your database, somebody is going to have to accept the considerable increase in work and costs, not to mention maintenance. In the end, you may find that a high-speed leased line may be a lot cheaper, and that you can archive the data with a giant download once a month or so instead. 

SQL injection relies on readily accessible data. You can restrict this by only allowing data access through a controlled form, like views or stored procedures. This would mean not giving the user any access to the raw tables, but rather just access to stored procedures or similar. For what it's worth, I wrote a series of blog posts that cover different aspects of permissions, execution context and ownership chaining. See (1), (2), (3) and (4). But first and foremost, you need to protect your app against SQL injection. This cannot be overstated, and other security measures can only augment it, never replace it. 

You could try adding a non-clustered index with the same definition, that would make the index considerably smaller in size (particularly if your table is wide). You won't eliminate the scan, but it would be much faster. Clustered indexes contain the entire table's data, whereas non-clustered ones only store the specific columns that you define (as well as a pointer to the clustering key or row ID, but in your case those would already be in the index key). 

Note: , , etc are deprecated and will probably stop working in a future version of SQL Server. I go into a bit more detail on SQL Server security in my blog post. 

Workaround suggestion: If you can change the functions, try making them multi-statement table value function instead of inline functions. When doing so, you define an output table in the part. In the DDL of the output table, declare a good clustered primary key. Example: 

Assuming the column is an identity column and you want to retrieve the identity value of the record you just inserted, you can use the OUTPUT clause, like this: 

A table value function that isn't inline, however, stores its output in a form of temporary table that it returns to the caller, so it isn't expanded the same way. 

defines a partition that spans over all of the rows for the current ItemCode and FiscalYear. Within this partition, you define a window using "rows unbounded preceding" that spans from the first FiscalMonth up to and including the current FiscalMonth. Those are the rows for which you the starting quantity and the period change. All of this assumes that the starting quantity is given for the first month of each year and zero for all other months. Now, all you have to do is apply a to this in order to get the output in neat columns. If you're on SQL Server 2008 R2 or Azure SQL Database, ordered window functions aren't supported. You'll have to resort to a much less pretty join-based solution, which also comes with a potentially hefty performance penalty. Something like: 

For a minimally logged operation to happen, a number of conditions have to be true, including no backups currently running, database set to recovery mode, and depending on your indexes, the target table may have to be empty. Some of this behaviour also changed (improved) from SQL Server 2005 to 2008. Then again, without knowing the specifics of your table and data, any of your other options may well perform better. Try using 

I've worked for a long time with a range of different types of accounting systems (albeit predominantly with European accounting standards) and the norm is that you use positive numbers for debit entries and negative numbers for credit entries. This means that your assets are positive and your liabilities negative, but your income is negative and costs are positive. So when you're building income statement-related reports you'll need to invert the numbers. I've seen instances of systems that use separate columns for debit and credit, but to me that is just anti-normalization. Typically, most of those systems also provide a "total" column, which is positive in debit and negative in credit. As you say, this way every transaction has to balance out to zero (you can probably write some type of constraint to make sure it does).