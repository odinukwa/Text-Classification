This a poorly written distributed bot. As far as I can tell these hosts are trying to authenticate over an unencrypted connection. If you require a secure connection to authenticate, they will fail even. However, it appears you are allowing authentication on unencrypted connections. By default will miss this condition, but blocks after 3 attempts in 10 minutes. You can create a file to adjust the number of failures required to get banned by or increase the ban time. also allows you to adjust the configuration while the server is running. You may need to create a file in to match the lines that are being generated. I have included the contents of my . You can use to test regex (you will need to replace python includes in the regex). 

These headers are from a recently received spam message. This host has an incorrect PTR record so fails rDNS validation. The domain in the HELO command does pass rDNS validation. The second header is the message being delivered by a program on the server that sent the spam. The missing domain for the IP address is highly indicative of a spam message. 

Freemem is unallocated memory which is immediately available. This gets used when launching programs and normally you will will experience problems if this gets too low. Monitoring to ensure you have a few MB free should be sufficient. I monitor for 1 MB on my 32 MB OpenWrt router, and 10 MB on my Ubuntu severs. I also monitor swap usage as that will decrease as memory load goes up. When programs start-up they usually don't use all the required memory initially. It is common for the virtual size of a program to be much larger than the resident (in memory) size. The remaining memory can be allocated from disk (code from the program image and libraries it utilizes) and swap (allocated but not used). You will likely see significant performance issues if the cached memory gets too low. Depending on how swapping is coded and configured, inactive pages are likely to be swapped to make space for your program. This usually results in far less page swapping than you expect. I would monitoring the cached memory to ensure performance of running programs rather than to ensure memory is available to start huge programs. 

Prior to the switch over cut the TTL (time to live) on the address to a short period (say an hour). You can override the TTL for individual addresses. This should be done 2*TTL in advance of the change over. A couple of hours before the changeover you can cut the TTL to a shorter period (5 to 10 minutes). When you do the cutover, try to notify all your secondaries of the change so that the pull the change immediately. Verify all your secondaries have the change. When you are satisfied with the change, increase the TTL on the address. The easiest way to do this is to remove the TTL from the address and let it use the TTL for the domain. Due to the use of fast flux DNS for malware distribution and botnet control some DNS sites may limit the minimum TTL they use internally. You may want to watch for your changes to show up on OpenDNS and Google before shutting down service on the old IP address. Alternatively, you can just watch the logs for traffic to drop off. 

Redirecting to your new server can be done by individual aliases, a redirect router (see chapter 22 section 3), or a rewrite rule (see chapter 31 Address rewiting) such as: 

# Default deny deny message = AUTH not accepted You could also enforce in a mail . This ACL allows external senders to send mail in. 

records need to be setup by the own of the IP address range. Contact your ISP/network provider and have them create the necessary record(s). It is possible to have then delegated to your servers, but few ISPs will do so. 

The header is usually the same as the addresses of at least one of the recipients. Header addresses are like the contents of a letter. A letter can be addressed to anyone or noone, but it has no impact on who the mail is actually delivered to. The letter is placed in an envelope and the envelope is addressed to someone. Delivery depends on what is on the outside of the envelope, not the contents of the envelope. Email delivery works like a letter. The headers and contents of the message are treated like a letter, and are ignored when it comes to delivery. The delivery mechanism relies on a separate set of addresses known as the envelope addresses. Delivery will be attempted to any destination addresses specified on the envelope. For most software, the addresses specified as or will be the recipients will be written to the headers. Additional address will be added to the envelope, but not written to the headers. Programs which generate email may not follow this behaviour. Spambots (programs sending Spam) often have a fixed message which is sent out to a long list of recipients. This is one type of software which will generate the kind of email you received. There are legitimate cases which lead to the behavior you are seeing: 

The backuppc package has a link to a minimal rsyncd implementation. It is based on cygwin, and only provides the rsyncd daemon service. It is a 32 bit implementation, but runs well on Windows 7 64 bit. There is also a link to another set of packages package that includes an sshd service as well. 

Use separate config files and start one of the servers specifying the config file. To use a different file use option followed by the config file you want to use. If you want to make this persistent, you will need to create a copy of the script that starts sshd and modify it to use a different pid and the new configuration file. More modern distributions use which uses different control files, for which you will need to create new copies. It may be possible to achieve what you want with a single sshd and a block. (as mentioned by joschi). You will need to add the additional port(s) and/or address(es) to the configuration. Match will only allow some properties to be changed. See the documentation for which may be output by the command . You match line might look like: 

I have seen a spike in rejection and deferals. There has been an increase in emails with domain literals like [192.0.2.14]. Main blocks which work for me: 

The configuration should be in . You can edit this file directly and reload exim4. However, the recommended update procedure is to run the command and change the host name when prompted. 

The built-in chains have a POLICY which can be ACCEPT, REJECT, or DROP. This applies if no rules match. User defined chains have an implict RETURN policy, which can be overridden by ending the chain with a rule with the desired action that matches all packets. 

I ran some queries like on the results you provided. It appears you have not configured SPF for to allow google to deliver email for that domain. That explains the SPF failures for deliveries from Google. The query above is based on the domains listed in the record. There are some records that do appear to be Spam, so they should be in the list. You may have similar problems with email not having appropriate DKIM signatures. Some may be Spam, or you may have delivery paths that do not sign the email with an expected signature. 

Avoid all of the above, and you will have a much better chance of getting your mail delivered. EDIT: I have found Port25 Solutions Inc. has a very good automated verification service listed on their E-mail Authentication page. Many thanks to them for their fine service. It is designed to verify DKIM signatures, but gives excellent feedback for most of the items listed above. Check in the Port25 resources section, and use the appropriate email address to get the results mailed to your desired e-mail address. Remember if you need to do DNS changes it can take a day or so to be reflected in all the caches. Worst case should be two times your Time to Live setting. 

If is not in you document root, you can replacing %{DOCUMENT_ROOT} with the on disk path to the /public directory. The may contain the file path to the content directory for the file Unless you have the same file in as in , the won't match. I avoided this whole issue by using my equivalent of as my docroot. If you can't put in , you can use to access it where it is. Your approach involves rewriting all matches. An alternate ruleset using as your document root would be: Alias /index.php /var/www/index.php RewriteCond %{REQUEST_FILENAME} !-f RewriteCond %{REQUEST_FILENAME} !-d RewriteRule . /index.php [L] You could also handle the missing files with a custom 404 error page. 

SSH uses pre-generated public and private keys. Once generated these keys are stored for future use. The content of the keys should not be related to the hardware or O/S, but do depend on the random numbers they provide. There are various formats used to transport the keys. If you move from one platform to another you may need to change the key format. Putty uses a different format than OpenSSH, but there are tools for both to convert the format. The private key should rarely need to be transported. If they are they must be kept secure. Public keys and are freely distributable, and are automatically exchanged during the connection. Usually known keys are stored so that the verification dialog is not required on subsequent connections. I generally generate new keys for new devices as this is more secure. It does require re-establishing trust relationships. Copying the old keys may allow the transfer of trust. Sometimes the trust includes other information such as hostname and/or IP address preventing the transfer of trust. An existing known host list can be transferred. This allows you to transfer the list of devices you trust. This does not guarantee they will trust you. 

I would expect you to have a different /64 on either side of the server. You will need to configure bridge the interfaces to make the routing work correctly. Normally, your provider will provide a block of /64 networks for your use. You can then configure these on your internal network, and normal routing will make the addresses available externally. You would run a process to allow hosts to autoconfigure, or manually configure addresses. WARNING: These IPv6 addresses are globally routable. Access controls need to be considered. 

Try a regular expression with a negative look ahead for the two words you don't want forwarded. You could do this in an if condition or in a rewrite rule. 

is the likely the name KVM it chose when it created a bridge for you. It is difficult but not impossible to configure your own bridge. From my reading of the documentation on configuring bridges, you should have only one bridge connected to a physical device (eth0, bond0, etc). You could configure a second bridge and use IP forwarding to handle traffic routing. This is fairly easy to do. You should be able to tag multiple vlans on the same bridge. This is likely the simplest method. 

I would consider backuppc. Handles de-duplication of backups across multiple servers and historical backups. If you use rsyncd on the clients, then only the first backup will pull down all the files. After that all backups retrieve only incremental changes. It handles laptops and periodically connected servers well backing them up when the connect. Incomplete backups are resumed when the laptop reconnects. The repository can be backed up using a number of options including rsync. For this large a backup, I would consider using bacula to backup the repository. For a distributed network, you may want to distribute your backup servers, and backup locally. Then replicate the databases. Initial replication can be done using tape exchanges, followed by rync replication of changes. Getting recovery images for Windows is difficult due to locking issues. However, a site your size should have standard images. The user's data should be recoverable from the backup. Test the solution to ensure it works in your environment. 

You are seeing normal behavior. Don't confuse used memory with unavailable memory. Many structures held in memory can be quickly downsized when memory is required. Expect your usage to increase as long as the system is up and lots of memory is available. Space used for disk cache counts as used. Once you read or write a file, the data tends to be cached until this space is needed. Log files tend to be write only, but get cached as they are generated. A few things that are noticeable from my usage graph: 

How are you maintaining the time on your server. If you have a poorly running clock, then the time may need to be jumped periodically. This may cause sudo to notice future timestamps and try to fix them. However, I would expect different message. If you are using with poor conectivity or a large step setting time may jump far enough that sudo would be concerned. 

Try filtering on referrer. Either mod-rewrite or mod-security can be used. See Debain Administration site for examples. 

Your external access on IPv6 may be restricted to a single IP address. Normally you would be provided with one or more /64 subnets for internal use. These would be advertised by one or more servers on your local network. You will need to check with your IP provider to see what mechanism to use, and which subnets are assigned to you. IPv6 clients usually self configure if they receive router advertisements. If you require static IPv6 addresses for some servers, you will configure those yourself. This can be done with a stanza in . Do consider using a firewall builder such as to build the firewall. It should be able to make the appropriate kernel configuration adjustments to forward your traffic appropriately. 

You are likely running into windowing effects. Data is transmitted in packets of up to 1500 bytes. There is a transmission window which allows a certain amount of data to be transmitted but not acknowledged. Your system will fill the pipe and then has to wait for the data an acknowledgment before sending more data. Some systems use a timeout before transmitting the acknowledgment, so you may get an acknowledgment for several packages at one, allowing your system to send that many more packets. This can lead to bursty behavior such as you see. There are a number of factors which can increase burstiness. Sampling errors can increase the apparent burstiness. Longer sampling periods tend to minimize sampling errors. It is rare that a single stream can run at the full bandwidth. To do so there must be sufficient bandwidth along the whole path. The latency (end to end transmission time) must be low enough that the transmission window is never fully used. The higher the bandwidth the harder it is to do so. 

Data buffers will remain in the pool after they have been written. These blocks will be quickly reclaimed if the system has a need for free memory. I've used systems where it would take several days to fill memory to the point where data buffers needed to be reclaimed. While it appears you are using slightly over half the available memory excluding buffers and cache, the buffers and cache can contribute significantly to performance. If this was happening all on one server, this might force block from mysql files out of the buffer pool. These would have beed served from memory rather than disk when a read was requested. This might slow down the mysql performance until the blocks are replace. However, on the remote system actively read blocks may be forced out as the data is read into memory.