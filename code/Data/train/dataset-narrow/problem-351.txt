This comes from a corporate mindset where security is high on the priority list if not on top. In corporates NO external access is allowed to internal machines, all public facing/accessible machines are in a DMZ behind FW controlling inside and outside access. Access to internal machines would be provided to staff through a 2 factor authenticated VPN link in most cases, no need for public IP's on internal machines. If you opt for the recommended NAT capable devices between core and rest of Campus network, you can do one-to-one nat there as required and all internal hosts can have private ip's. This also provides the possibility for better security as only specific services can be allowed inbound to hosts. 

A normal switch works independently of the rest of the network. A OpenFlow/SDN switch, when it receives a packet, that it does not have a flow for (Match + exit port) will contact a SDN controller(Server) and ask what must it do with this packet. The controller can then download a flow to the switch, possibly including some packet manipulation. Once the flow is downloaded to the switch it will switch similar packets at wire-speed. Why is centralizing the decision making so wow ? Having a central server that knows the network layout and can make all the switching decisions and build the paths gives us new capabilities. 

Let's say that there is a SVI for VLAN 100 and it's been assigned the first usable IP address in subnet 192.168.1.0/24. There is a host on VLAN 100 with an IP address of 192.168.1.130. We need to change the subnet on the SVI to a /25, but we need to do it without causing a significant disruption to the host or changing it's IP address. How would you do it? 

You probably do not want to add any automated configuration capability to your imaging script/PE environment. That wouldn't really jive with most change control systems or frameworks. If they do not want to allow PXE booting or imaging in general on the production network then you will either want to prep new computers on a dedicated port in your office and rely on USMT to backup and restore customer data from a file server. This is what we did at my office. The above is easy to do when you are on-site, but quickly becomes cumbersome when you are working remotely. We had to "set customer expectations" and ship newly imaged machines with their data restored to the customer. A "technical contact" would assist in its installation and they would ship the old unit to us for disposal or redeployment. In short, without any sort of assistance from your network team I don't believe that there is a good automated solution. 

So we got CenterHubx1->RegionHubx5->Spokesx4->Computer Computer pings -> Hub , does nhrp lookup on Spoke, set's up new tunnel to CenterHub. Problem Spoke vpn in subnet 172.16.1/24 and Hub 172.16.0/24 The Spoke should not try to setup the direct connection to the CenterHub, only to other spokes of it's RegionHub To prevent this use different NHRP network-id's for different tunnel subnets. Quote: 

Problem1: You should have consistent ospf cost, the L3 routing is independent of the L2 hsrp gateway redundancy Problem1b: You host vlans hanging off the Nexus devices should be passive for OSPF, why do you want active ospf here ? Problem2: On your vlan2 (192.168.0.0/24) you should not have hsrp, if all the devices are using ospf to interconnect no need for L2 shared ip, only needed if you are doing static routing and need l2 ip failover. Problem3: If you have dual Nexus devices you probably have vPC configured, although you have not attached any config. vPC has specific rules for interconnecting L3 devices and traversing the peer link. see $URL$ Problem3: Recommendation is to use a dedicated L3 link (not Vlan) to connect to other ospf routers. 

This issue occurs with your Quad Zero route. You have routes bound to interfaces and if you are fulfilling requests from a priv or dmz interface and you have a 0.0.0.0 route offered on that interface that is the path the return traffic will take to get back to the requesting source. If you need the traffic to return from the interface it was received from your will need to redo your routing on the device. Suggestion If you have a service window you can schedule. Research how you would like your 0.0.0.0 routing to work per interface. I would imagine you might have some end user requests for Internet passing through the device too and that would need to be taken into consideration. Modify your txt config file to what you believe is functional and load it up during your service window. Have your testing regiment planned out and try and hit every item. If you fall into an abyss, load your config that you know is functional and return to a steady state and revisit the issue during your next window. Plan it, test it, nail it. Good luck. 

According to this article two routers will get stuck in the exstart adjacency state when their configured MTU sizes do not match. Does a VLAN tag affect this as well even if the routers involved are unconcerned with which VLAN the packet is coming from or heading to? I'll try to clarify the question if it is unclear at all. 

I've been asked to assist in the design of a "OOB" management network, but I have a limited number of resources available. I have the following: 

We run a Layer 2 Collapsed Core network topology with a Cisco 6509-E VSS core. We have 128 access switches connected to our core via 1Gps Port-Channels. It's a mix of copper and fiber uplinks. Fiber to the second floor and copper in our datacenter. The current thinking by our consultant is to configure the management SVI on each of our access switches with their own VLAN uplinked to our 2690 "aggregate" switches. The aggregate switches will in turn be uplinked to the 3750-X stack via a 802.1q trunk and configured with IP Unnumbered to emulate a Layer 3 link bypassing, in essence, L2 communication between the production access switches. The 3750-X stack will have a Loopback configured for each individual VLAN that the production access switches will use as a Default Gateway. The idea/concern is that we do not want our management network to pass STP traffic through to the other access switches or risk any sort of network convergence between the two separate networks. Kind of a "Poor Man's" Private VLAN setup. I'm wondering if this is the best or most efficient way to set this up or if there is a better way to go about it.