The first predicate matches the rooms, obviously, and the other two determine if the ranges intersect. Any two ranges intersect if the beginning of each is less than the end of the other. Secondly, take the time difference between and . Finally, just aggregate the time differences per user. So, the complete query might look like this: 

It is possible to specify your joins in such a way that logically the join between and happens first. Performing that join first will let you outer-join the resulting set to matching the latter's against either or , so that rows are always populated in the output. There are two ways to implement that. The first option uses the nested join syntax: 

You are making this needlessly complicated. You are trying to build a dynamic query where a static one would do just fine. Assuming the logic in the dynamic SQL is correct, the entire procedure could look like this: 

This is a classic anti-join scenario. You want rows from one table excluding those matching another table. In English, you could put the condition like this: 

There is no such built-in function – or system table, or view – that would give you a list of all styles (formats). Otherwise people would not have been creating custom-made style lists (like this one, for instance) – as a way of a reminder, I guess, because the same information can be found in the online documentation, even if slightly differently arranged. Go ahead and create one for yourself. New styles are sometimes introduced with new major releases, but those do not happen too often, so maintaining such a list should not be much of a burden. Another thing is whether you really need to have such a table. Unless you are still using a pre-2012 version, you can format your datetime values arbitrarily with the FORMAT() function. 

Use that as the in the IF EXISTS pattern mentioned earlier and specify the appropriate action to do when the predicate evaluates to true (see above). One last point I would like to make regarding this test depends very much on how you interpret the wording in your assignment question. My basic interpretation is this: a member is allowed to have two pending rentals before renting again. Assuming that the person will be allowed to rent no more than one book at this point and keeping in mind that the new rental is already in the table when the trigger is executed, the total number of rented books allowed in the table for any given member should in fact be three. Therefore, the last query's HAVING should actually read: 

The query additionally shows each table' schema name, because sometimes systems are built using more than just the default dbo schema. I have also included the filter in the query because I am assuming you want to exclude predefined/system tables from the search. 

I would suggest rewriting the query in a way that minimises the number of columns necessary to put into GROUP BY. In your case you can do that by applying the grouping to the table only. According to your example, that table has the following entries: 

Those columns are not needed in the output, they only serve as steps of the solution. In order to avoid returning them, use the query as a derived table, so that only the columns you want can be pulled from it: 

Get the maximum for a given from each table, combine the results into one row set and then get the maximum from it 

will only ever evaluate to 1 if the is , not when it's or or anything of the kind. By the way, when you want the function to return a substring from a certain position to the end of the string, you can specify any fairly large number as the length parameter, no need to use . Very often something like 

Note that rather than "fixing" your table like this, it would be better to normalise it by creating an actual reference table with columns and that you would then join back to your data table whenever you need to retrieve . Consequently, the column in the original table would no longer be needed. 

This is a typical greatest-n-per-group problem. The tricky part was just to determine what to group by. 

The latter option – doing the job in two distinct steps – might be faster, depending on whether MySQL will find the single-query solution too complex to come up with an efficient plan, and also depending on the amount of data in your tables. Test both options for yourself to choose the one that works best for you. 

This achieves the same logical sequence of joins: first with , then their join result with , the rows being on the outer side the join, i.e. they will always be present in the output, same as with the previous query. The only difference is that if you use in both cases, the order of columns in the output will not be the same. Columns will be listed based on the order of tables as mentioned in the query: columns, columns, columns in the nested query output, and , , in the right join one. But using in production is rarely a good idea anyway. Thus, if you specify the output columns explicitly, there should be no difference at all between the two options, especially if you take into account that a right join is not very popular among developers either. 

You could also have an alternative substitution for the missing LAG functionality, which uses OUTER APPLY: 

Double quotation marks are name delimiters. They are reserved for delimiting names (column names, table names etc.) that contain non-standard characters or those that you want to explicitly make case-sensitive (because that is their effect in PostgreSQL, which is according to the standard, too). So, that is why all attempts with fail: PostgreSQL indeed interprets those as names (specifically column names in each of those contexts). The one case without quotes fails simply because is an invalid token sequence. Numbers and some other constants can be represented in PostgreSQL without quotation marks but the tokens you specify there cannot be interpreted either as a number or anything else. Finally, the one where you are using single quotation marks around the IPs fails because PostgreSQL is trying to interpret those as JSON literals. Why? Because the column is of type – that is the type of column values returned by . So, in order for that second attempt of yours to succeed, you should first of all represent the IPs as valid JSON string items. That means you need to enclose them in double quotation marks and then in single quotation marks, like this: 

In the event of a DBA leaving an organization, what options can the surviving team members pursue to change passwords for service accounts portfolio-wide? While using Configuration Manager appears to be the de-facto method, can PowerShell or another scripting or batch language be used to reset them in bulk? I figure if you're changing the login name for the service accounts, Configuration Manager is essential as registry permissions and the like need to be propagated. For just a password change to the existing account however, is a scriptable method OK as long as it's performed during approved maintenance windows for the service restarts? 

I'm the primary DBA for a 100-user data warehouse with approximately a dozen developers and analysts regularly contributing additions to the data model and codebase (mostly stored procedure and fact table additions). I'm following a relatively traditional deployment process focusing on regularly-scheduled code reviews and deployment windows with change control tickets twice a week with me an a colleague DBA handling the deployments. I've used SQL Audit, default trace data and RedGate DLM Dashboard to keep tabs on all the schema changes. The tempo has increased steadily in the last two years and some of the lead analysts would like to go towards a DevOps deployment method using automation. The director and app owner is the most accomplished developer creating assemblies for the advanced ETL, looking to perform deployments independent of me and my colleague, asking for sysadmin privs when we're backlogged or short-handed during vacation. I've declined requests to share sysadmin privileges with the director because of the risk of setting a precedent for other developers, the liability if access is opened too broadly in error and the risk of a deployment error which will get me paged at 3am when I'm off-call. On the flip side, I'm aware of the business pressure on the developers to get new content to market faster, even if it's not properly tuned. Finally, the political and departmental element of reporting to the director is present; it's easier to say 'No' if you're a DBA in a separate unit, but harder if the director signs your paycheck. I've told my director in the past that in the event of a breach or system failure, all the sysadmins need to be at the IT Security board of inquiry to explain what happened, which has given him pause in the past, but the business risk of delaying content is getting stronger. What options can I offer to the director that go short of sysadmin rights that give him more latitude to deploy but have the proper level of auditing in depth? I'm familiar with the various server and database fixed roles, but I'm looking at departmental protocol options as well. 

Is this expected for a FlashArray config or can it be configured at the controller or LUN level to be more optimal for reads than writes? The host will host a data warehouse with tables already in the 250 GB+ range. 

My SQL Server instance is hosted on an internal network server (10.x.x.x address). It's a named instance currently using dynamic ports. An external application at a third-party hosting location connects to the instance through use of the IP address and port #. After each maintenance window I have to confirm the dynamic port # in use is the same as before as the firewall rule is set to allow connections between application host and database host on that specific port #. Is this the correct way to ensure the externally-hosted app can connect to the host without having to change port numbers in the connection string or should another port forwarding method be used (i.e., port 9999 on application host is mapped to port 1434 on SQL host)? What configuration manager changes in the Network Configuration section should be applied to allow the external application to use the same port # but internal applications to rely on the Browser server to make the correct mapping? 

I'm attempting to upgrade a SQL 2005 SP3 clustered instance to SP4. During installation, a rollback event took place with the error indicating there was insufficient space on the the disk. The system volume and application binary volume both have adequate space. On examination of the hotfix logfile, the config.msi target folder for the rollback files was the volume used as a mountpoint folder (in this case, M: was the target but it's only 128 MB in size, hosting only folders to be used as mountpoints for other physical disks, like M:\DATA00). Why would the SP install routine use that volume instead of the system volume or SQL binary volume for the rollback file location and can the target path for the rollback file be altered? 

I'm restoring to an alternate server in a DR drill and have restored master but can't restart the instance, even with /f and /m switches. The log is indicating tempdb can't be created, probably looking for the paths from the source instance. How can I determine which path it's trying to create tempdb's DB files in? I was able to restore the master backup under an alternate name on a different instance so I can browse the system tables. Is there a spot where I can look for them? I have to go with the assumption that access to the source server is lost so I can only rely on the backups to determine the correct configuration. 

I'm conducting time trials on a new SSD array running both SQLIO tests and a real-world workload of DB restores and DBCC CHECKDB calls. I'm seeing a major discrepancy between the IOPS and throughput generated with my SQLIO batches and what I'm observing with the workload, with the workload only requesting a fraction of what I was able to observe with SQLIO, usually in the 5,000 IOPS range and generating no more than 400 MB/s throughput. Is there an inherent limitation as to how many resources DBCC CHECKDB will consume event if the hardware has more than sufficient capacity to handle the load? What settings can I experiment with to expand DBCC CHECKDBs usage of CPU and disk resources? Here are the specifics... From