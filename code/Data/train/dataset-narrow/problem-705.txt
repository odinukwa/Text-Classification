The next option you have is to create a new tablespace (and associated datafile(s)) and move the current database objects to the new tablespace. You can do this by using datapump, or do it manually. Tim Hall has an excellent article on how to perform this here. 

Amazon Athena uses Presto, so you can use any date functions that Presto provides. You'll be wanting to use , or similar. 

Oracle exposes its data dictionary through views. You can use these views to query the information you need. The main views that you'll need to use are: 

You are granting the privileges to the incorrect user. The schema owner owns the table, and is therefore the user that needs to be granted the relevant permissions on the tablespace. 

Note that I have used as you specified that the token should be "numeric characters". The above is the same as using an column in MySQL. However, to accommodate for the fact that you only want to generate a token when the invoice_no , you need to use a trigger as follows: 

.. to reload the kernel settings. That'll set the maximum shared memory segment size to 2Gb & should solve your issue. 

Obviously you may want to on more than just (depending on what you consider a "duplicate", and may want to order the in some way, but this points you in the right direction. Edit to address your comment: 

An oracle database is the physical files that make up the database itself (control files, data files etc) - Documentation link. A database is associated with one or more instances, with multiple instances making a RAC setup - Documentation link. As far as your second question is concerned, multiple instances of a single database is a RAC setup. 

You can also do this with Virtual Private Database, but I think it's an expensive extra licensed option. You use DBMS_RLS to configure the relevant security policies that you require. 

I'll answer this at a high level for you. The two backup methods work at different levels. An backup is a physical backup and a Data Pump backup is a logical backup. A database dump using is a 1-time export of one or more database schemas. It backs up DDL (table structures, views, synonyms, stored procedures, packages, etc), plus data. An backup is a point-in-time backup of an entire database (for the purposes of this question). It backs up the physical blocks that make up the database (data files, control file, archive logs etc) and, in combination with the database archive logs, allows point in time recovery options. In the event of a complete database loss an backup can be used to restore the complete database. However, a data dump taken using would need a new database creating before the data could be imported using . For a hobbyist Oracle XE database (that may not be in archivelog mode), backups using will probably suffice. The Oracle Documentation covers this far better than I could ever explain. OracleÂ® Database Concepts - Backup and Recovery 

If your local machine is not configured to use ezconnect, you need to specify the TNS name for the database, like so: 

I'd guess that you're not using SQL Plus to test & are testing in another tool (such as SQL Developer) and it's not set or you haven't configured the tool to correctly display the output from the RDBMS. Or, it could be the fact that is a typo :) 

Failing that, if you have the means to do so server-side, you could use Oracle SQL Developer to give you a graphical representation of the query plan - Though this is really just the same as the ASCII version with a few icons. Also, take a look at , and - They may help you format your XML output to prettier HTML. 

You then have to the from the sequence and re-alter it to increment by the usual value to restore "normal" operation. To check the current value without incrementing: 

The clause wants adding between the and clauses. The Postgres documentation on this is here, and mentions that this may not be performant for large values. It will be ok for small-ish datasets if the column is indexed. 

Looking for DDL statements in the file should show you what schemas are needed in order to perform an import. For example, from my test dump: 

You have to drop and recreate. If you look at the documentation here, you can see that only modification of the storage clauses are allowed. 

You could, instead of using CTAS, use a "normal" statement, then the rows manually. This allows you to specify the and any constraints. eg: 

Without using quotes, MySQL is essentially inserting the integer 1980 ( which is ) as a date, as it treats it as mathematics: 

This is happening because your default database block size is set to something other than 32k, and as a result there's no 32k cache buffers available. To fix this, add: 

Just guessed your requirements, basically. Might be wise for you to read a regular expression tutorial. 

There is no simple way to achieve this. You can either do it by tracing the session or by using a login trigger. Here's an example of how to do it using a DB login trigger. Logging table: 

It succeeds because the unique index hasn't been disabled/dropped when the constraint was disabled. Note that the original error was , whereas the index constraint is violated once the PK constraint is disabled. Scenario 2: Scenario 2 is creation of the PK upon table creation: 

Can probably be done with some simple maths along with the source data, but my brain is a little slow this evening ;) 

You're not supplying a correct connection string. If you're using ezconnect, you need to pass the connection string in the format: The expdp command-line should be: 

The first option you present is a "NoSQL" design pattern & doesn't allow for easy querying in a normal RDBMS. Your second option is the most suitable. Have a 3rd table that tracks which products are in which store. Possibly called "Stock" - you could add a count to this if you wanted to track how many of each item each store has. 

The easiest way to do this with built-in functions would be to just convert the number into base 36: 

Given your query, you have the following: Owner (Schema Name): Table Name: The user will not just be able to select from unless there is a Public Synonym pointing to it. So, to select from the table you have to fully qualify the name by prefixing the table with its schema: 

The create table statement can contain anything you want - the above is just showing that the statement can be dynamic. So, in your case you'd have simple logic to alter the and table name for each statement. Bind variables will offer no gain unless you're executing the same single query multiple times (extremely simplified for the sake of this answer) - in this case you're executing many different queries that are performing large bulk inserts. Creating tables with is not advisable if this is a production system. Also be aware that not logging and can have backup/restore implications. This Ask Tom thread may be of particular interest. To clarify, using CTAS is the way to go. You only have to parse the CTAS statement once to insert your millions of rows. If you use individual INSERT statements you have to parse each statement over & over again, which adds a massive overhead! 

Unfortunately in Oracle 11.2 only supports SHA1 (documentation link), which is 160-bit. . in Oracle 12.1 supports SHA2 (documentation link), which does what you require. There are some free implementations of SHA2 just a google away. This blog post, for example. As for decrypting a hashed password? I don't think you understand hashing. Hashing is 1-way, unless you brute-force it, or use Rainbow Tables. To check if a password is correct, you would hash it and compare it to the stored hash. 

Flashback Database Flashback Database relies on flashback logs (stored in the flash recovery area) to restore a database to a particular point in time. The time taken to restore a database using Flashback Database would therefore be dependent on the amount of flashback log created between the restoration period and the current point in time. See this link for more information & limitations. I'll add that it's lots quicker than a point-in-time restore using RMAN (I imagine that's what you were eluding to when you asked the question in the first place), but there is also the extra overhead in writing flashback logs that needs taking into account. 

$ORACLE_HOME/dbs You can adapt the above script to look in all s listed in the , and search for initSID.ora and spfiles for any instances: 

I've done it using a conversion into a Julian date, plus a hacky division, truncate, then multiply. Source data to show the upper window is correct, and different for each user: 

If it says and doesn't throw a error, it's fixed. If it throws another or a message about a bad control file, we need to try the other control file. To do that, make sure the database is down (as above), then copy the other control file and try the startup again: 

... the local time is 08:00:00, which is 16:00:00 UTC (08:00:00 + 8 hours, because it is 8 hours behind UTC). 

If you into the table, and ommit a value for (IE: it's ), the current timestamp will be used instead. Also, never use / datatypes to hold dates (which I suspect you are trying to do with your column) - use proper date datatypes. Untested, by the way - this is from the documentation. 

A operation is . When using , remember that any operations you execute will implicitly the current transaction. 

You are receiving an Oracle ORA-03297 error because the HWM (High Water Mark) of a table is beyond the size you tried to shrink a datafile to. First try and "shrink space" for each affected table: 

A process is created per connection, which can then execute SQL statements for the duration of the connection. A SQL statement may use more than one Oracle background process (for things like Parallel Query) during its execution, but these aren't forked specifically for the SQL statement. Note that I'm ignoring things like connection pooling (DRCP) and shared servers (MTS) to keep the answer simple. 

The stored procedure takes an input, which is the start offset. Create another cursor, and use your concatenated ID in the clause. Why do you need to process 1000 records at a time? It'll be slower iterating this way. Often you can achieve the same thing using one or two lone SQL statements, which end up way more efficient.