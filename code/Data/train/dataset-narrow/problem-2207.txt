I hope you can make out the idea of the graph: using the implication properties of the tournament above, we can construct a graph in which each transitive feedback arc set both includes and doesn't include the edge $A$, i.e. a contradiction, which means the graph doesn't have a transitive feedback arc set. Any completion of that graph cannot have one either since the same contradiction will remain in any completion. I left out a large number of vertices, all of which can be derived from substituting the tournament above for the implications. 

This tournament has the following properties (which I checked using a program, I didn't prove it formally): 

(The specific problem I have is stated as precisely as I could in the very last paragraph which starts with a boldface "Question:", up until then the question provides context for it.) Say we have an undirected vertex-weighted graph $G=(V,E,w)$ on $n$ vertices and want to find an optimal fractional vertex cover. You could use the following linear program (see these lecture notes) to do this: \begin{align} \text{minimize} &\sum_{v \in V} w(v)\cdot x_v \\ x_v &\leq 1 \qquad \forall v \in V \\ x_v &\geq 0 \qquad \forall v \in V \\ x_u + x_v &\geq 1 \qquad \forall \{u,v\} \in E \end{align} That gives you a polynomial time algorithm since there are polynomially many constraints, and there are polynomial time algorithms for linear programming. However: what if there are superpolynomially many constraints? I have come across claims that you can still solve linear programs of that kind if you only have polynomially many explicit constraints (i.e. a constraint matrix of polynomial size) and for the remaining constraints, you get an implicit specification called a "separation oracle" (see e.g. these notes): such an oracle is a p-time algorithm that can check whether a constraint was violated and if so, produce a violated constraint. This works for the ellipsoid algorithm since the violated constraint defines a separating hyperplane (if I understood that correctly). An example for such a linear program is the one Even et al. used as part of an approximation algorithm for the minimum linear arrangement problem, where given an edge-weighted, connected, undirected graph $G=(V,E,w)$, we want to find a bijection $\pi: V \to [|V|]$ that minimizes $\sum_{\{u,v\}=e \in E} w(e)\cdot |\pi(u) - \pi(v)|$. The LP is this: \begin{align} \text{minimize} &\sum_{e \in E} w(e)\cdot l(e) \\ &\forall U \subseteq V:\, \forall v\in U: \sum_{u \in U} \text{dist}_l(u,v)\geq \frac{1}{4}(|U|^2-1) \\ &\forall e\in E:\,l(e) \geq 0 \end{align} Here, $\text{dist}_l(u,v)$ is the shortest-path distance between $u$ and $v$ according to $l$. The second line specifies exponentially many constraints, however a separation oracle exists: first, run all-pairs-shortest-paths with respect to $l$, then for each vertex, sort the distances to all other vertices in ascending order. A constraint belonging to the second line is violated iff for some vertex $v$, the first $1\leq k \leq n-1$ distances sum to less than $\frac{(k+1)^2-1}{4}$, and in this case a witness for the violation consists of the endpoints of those shortest paths. I've also seen claims that instead of the ellipsoid algorithm, one can use interior point methods. From what I've read, "interior point" doesn't refer to a single algorithm (or set of close variants of one algorithm) that solves the linear programming problem, but a number of different algorithms for different classes of problems. The paper where Ye's algorithm was introduced has extensive references on the development of interior point methods. I was also unable to find references that outline how to use interior point methods to solve implicitly specified linear programs that would have superpolynomial size if specified explicitly. Question: Which interior point methods can be used to solve implicitly defined linear programs (which would have superpolynomial size if defined explicitly) in polynomial time, and how does the implicit definition have to be specified? 

For example, in the case of a heap, it will be slightly differently organized, but the work is the same : 1. Find the min record in $0(1)$ 2. Remove it from the structure in $O(1)$ 3. Fix everything so that next time #1 and #2 are still $O(1)$ i.e. "repair the heap". This needs to be done in "O(log n)" 4. Return the element. Going back to the general algorithm, we see that to find the record in $O(log n)$ time, we need a fast way to choose the right one between $2^(X - log_2(P))$ candidates (worst case, memory is full). This means that we need to store $X - log_2(P)$ bits of information in order to retrieve that element (each bit bisects the candidate space, so we have $O(log n)$ bisections, meaning $O(log n)$ time complexity). These bits of information might be stored as the address of the element (in the heap, the min is at a fixed address), or, with pointers for example (in a binary search tree (with pointers), you need to follow $O(log n)$ on average to get to the min). Now, when deleting that element, we'll need to augment the next min record so it has the right amount of information to allow $O(log n)$ retrieval next time, that is, so it has $X - log_2(P)$ bits of information discriminating it from the other candidates. That is, if it doesn't have already enough information, you'll need to add some. In a (non-balanced) binary search tree, the information is already there : You'll have to put a NULL pointer somewhere to delete the element, and without any further operation, the BST is searchable in $O(log n)$ time on average. After this point, it's slightly sketchy, I'm not sure about how to formulate that. But I have the strong feeling that each of the remaining elements in your set will need to have $X - log_2(P)$ bits of information that will help find the next min and augment it with enough information so that it can be found in $O(log n)$ time next time. The insertion algorithm usually just needs to update part of this information, I don't think it will cost more (memory-wise) to have it perform fast. 

In real-world medical practice, and in many other regulatory contexts such as aircraft safety, commonly it is legally and practically necessary to distinguish between hardware and software, even though all parties recognize that the mathematical foundations for this distinction are highly imperfect. 

Our practical computational experience has been that estimating rank over $\mathbb{C}$ is generically tractable by steepest-descent methods ... as we understand it, this robustness arises for a geometric reason, namely, the holomorphic bisectional curvature theorem of Goldberg and Kobayashi. This is far from a rigorous proof, needless to say. 

The references given in answer to Quantum mechanics as a Markov process — in particular Carlton Caves' on-line notes "Completely positive maps, positive maps, and the Lindblad form" — survey physical ideas and mathematical tools that are helpful in answering the question. A key point is associated to the specific question asked "How can I get Kraus operators for the operator-sum equivalent of $M$ that are in a useful form?" For large quantum systems, a generic superoperator $M$ will not have an algorithmically compressible form. Moreover, Kraus representations are non-unique, and to the best of my (non-expert) knowledge there is no procedure that is both general and efficient for finding Kraus representations of a given $M$ that have a "useful form" (by whatever criteria are given for a form being "useful"). That deciding quantum separability is NP-hard suggests that no efficient, general representation-finding algorithm exists, even when $M$ is numerically given in its entirety. To make progress, it may be helpful to ask heuristic questions: "What is special about my particular superoperator? Can I exhibit a set of Lindbladian generators for it that have useful symmetry properties and/or generate compatible compressive flows on the Hilbert state-space? Are these Lindbladian properties associated to a natural Hilbert basis in which $M$ has a sparse, factored, or otherwise algorithmically compressible representation?" If questions like these could be efficiently answered by "turning an algorithmic crank", then quantum physics would be a far less interesting subject! :) 

To add a little context, here's a construction for a graph that doesn't have a transitive feedback arc set. For this construction, I'll use the following gadget graph: 

You'll notice that for each implication, the two edges are pairwise disjoint, so the following construction works: 

Note the difference to the traditional feedback arc set problem: I don't care about the size of the set, but I do care whether the set itself has a certain structural property. Have you encountered any decision problems that feel similar to this? Do you remember whether they were $\mathcal{NP}$-complete or in $\mathcal{P}$? Any and all help appreciated. 

(This question is a bit of a "survey".) I'm currently working on a problem where I'm trying to partition the edges of a tournament into two sets, both of which are required to fulfill some structural properties. The problem "feels" quite hard, and I fully expect it to be $\mathcal{NP}$-complete.For some reason I'm having a hard time even finding similar problems in literature. An example of a problem that I would consider comparable to the one I'm dealing with: 

Very long answer with inexact flaky pseudo-math : Note : the very end of the second part is sketchy, as mentioned. If some math guy could provide a better version, I'd be grateful. Let's think about the amount of data that is involved on an X-bit machine (say 32 or 64-bit), with records (value and priority) $P$ machine words wide. You have a set of potential records that is partially ordered : $(a,1) < (a,2)$ and $(a,1) = (a,1)$ but you can't compare $(a,1)$ and $(b,1)$. However you want to be able to compare two non-comparable values from your set of records, based on when they were inserted. So you have here another set of values : those that have been inserted, and you want to enhance it with a partial order : $X < Y$ iff $X$ was inserted before $Y$. In the worst-case scenario, your memory will be filled with records of the form $(?,1)$ (with $?$ different for each one), so you'll have to rely entirely upon the insertion time in order to decide which one goes out first. 

These issues are clearly and accessibly covered in Sanjeev Arora and Boaz Barak's Computational Complexity: A Modern Approach, specifically in the chapters "The computational model and why it doesn't matter", "NP and NP completeness", and "Boolean circuits" (respectively chs. 1,2,6). Very generously, an on-line draft of this book has been provided by the authors ... this has been a big help to me in my own researches! From the viewpoint of complexity theory, the various theorems and proof technologies in Arora and Barak's exposition provide little grounds for "divorcing the process of computation from the agent of computation" (in the phrasing of the question). From a regulatory perspective, however, it is well to keep in mind the legal maxim: 

Note: The text below was intended as a comment … it definitely is not an answer, but rather a pragmatic observation that arose out of a restating of Charlie Slichter's Principles of Magnetic Resonance in the language of symplectic geometry and quantum information theory (which pulls back naturally onto polynomial-rank tensor-product state-spaces). At present we have a partial geometric understanding of these tensor-rank methods, a marginal quantum informatic understanding, essentially no complexity-theoretic or combinatoric understanding, and a working (but largely empirical) computational understanding. We are very interested to broaden, deepen, and unify this understanding, and so we hope other folks will post further answers/comments on this subject.