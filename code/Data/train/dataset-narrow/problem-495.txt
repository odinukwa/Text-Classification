I have been finding strange error messages on the sql error log: Bocss: same deadlock taking place every hour – needs investigating Also lots of recompiles are listed in the error log for other SPIDs as per following examples: 

I would like to drop the unique index, and alter the primary key so that it is CLUSTERED. I will keep the same primary key, just change it from non clustered to clustered. This table is part of transaction replication I would get this done on the subscriber database only.Not in the publisher. It is a table with over 9,293,193 rows. 

I am aware that partition generally is not meant to speed up my queries, it is a management feature, however, it can speed up queries on large tables. I will start by stating what I don't want. I don't want to remove any partition from my table. what I want? I want to delete all data from a partition in the quickest possible way: can something be quicker than this? without considering deleting in batches 

I have followed the answers and comments and not sure what else, my subscription is now working, however, it shows up twice now, and I can't find a way to get rid of the one that is not working. 

It would seem that Oracle at one time had plans to give a different definition to VARCHAR than to VARCHAR2. It has told customers this and recommends against using VARCHAR. Whatever their plans were, as of 11.2.0.2 VARCHAR is identical to VARCHAR2. Here is what the SQL Language Reference 11g Release 2 says: 

Update: Based on your update you could embed this code in a database job and run it once a day/week/etc. 

Very high is a relative term that depends on perspective. You could mean 1 hour, 1 month, 1 year, etc. Assuming you mean something between one minute and one month, the answer will depend on how much undo is being produced and how much storage you have available. I haven't seen anything higher than two weeks. Oracle automatically manages the undo space and will dynamically decrease the retention below the undo retention setting if space constraints require it. The caveat to that is when the undo tablespace has guaranteed retention. The Oracle Database Administrator's Guide covers all this information on one page. Here are some excerpts: 

You can logically see that the index would narrow the scope of the count, but the database can't make the same determination. To use a function based index you need to use the function itself in your query. Assuming the data is such that the index will be useful, something like this should cause it to be used: 

Question is: How can I fix this error? Basically get rid of the ghost subscription showing up in the replication monitor and causing it become red flagged? 

My question(s) are: I am dealing only with the query returning columns, should I worry about this warning? is it related to the data type? is there any way to change the code and get rid of this warning? if it is only noise, can I regulate it? 

then I can see the job runs several packages. how can I find using T-SQL, all the users that have permissions to run this job? 

while comparing the execution plan of 2 stored procedures, on the second one I get a warning sign (marked by the red arrow). what does it mean? 

I am working on a SSRS report to display the logins permissions in a set of databases on a specific server. the server, the logins and the databases are all parameters. NULL shows them all (logins and DBS) server - only 1 - must be specified. The Procedure accepts MULTIPLE parameters, which I pass to the procedure as a COMMA DELIMITED STRING. and here is where I think I am making a mistake. the report must list the permissions only for the list of logins supplied, and list of DBs supplied. Here is a picture of the report, collecting the parameters to be passed to the stored procedure: 

The proper way to query all entities belonging to a given set is to your tables and use a clause to limit the set to the given set. 

Putting business logic in the database is admirable, and you certainly should implement constraints for things like this when you can. However, a trigger isn't the only alternative. You can solve the problem in the PL/SQL package doing the insert. By doing so you gain other benefits such as reduced client side code, fewer context switches, automatic binds, client application independence, etc. Here is an incomplete example (without the locking that would be necessary to run this concurrently). 

You might classify the third as simply a less efficient variation of the second, but it does technically "use a FORALL statement to insert the data". The second variation is probably your best option. 

It is good to see thinking on how to handle race conditions and more importantly how to unit test them. 

Not exactly but you can get the SQL statement causing the lock and in turn identify the related lines in the procedure. 

I don't know if you can do something like that in MYSQL, so here is a version using a GROUP BY and self join that works in MySQL 5.5.28. 

In the strictest sense a can do more than just query data. At least in Oracle there is the for_update_clause that “lets you lock the selected rows so that other users cannot lock or update the rows until you end your transaction.” (From the SQL Lanugage Reference). 

I have installed 2 instances of sql server plus SSIS on the following server. Note the amount of RAM is nearly 384 GB 

But although I have set optimize for ad hoc workload to 1, when I look at the SQL Server Memory Usage I see that lots of it is still used by the plan cache instead of buffer cache. as can be seen on the picture below. 

then I want to see all the permissions of the objects. when I check my stored procedure I run this script: 

quite often while tuning queries and stored procedures I come across the situation where I have a number of different types of the same object, each type has different characteristics but all of them have similarities in the way they behave. for example, please see the code of a view below: 

I got it working maybe not the best but it is ok for now. what I did is, I have added the following code to my script: 

Is there any other way to get this done? On this occasion this table is not big - about 500,000 records on the live system. the delete is part of a SSIS package, it runs daily and deletes about 10-15 records a day. there are problems in the way the data is structured, I just need one AccountCode for each customer but there could be duplicates and if they are not removed, they break the package on a later stage. It was not me who developed the package, and my scope is not to re-design anything. I am just after the best way to get rid of the duplicates, in the quickest possible way, without having to refer to index creation, or anything, just the T-SQL code. 

On some of our databases we are using DDL triggers to catch changes and save them to a table. We then have a web interface to pull up these previous versions. It has severe drawbacks, which is why I am looking for alternatives, but it is easy and is better than no version control. 

When doing a to_date on a two digit year, use when you want the first two digits of the year to match the current year, but use when you want the first two digits to be 20 for years ending in 0-49 and 19 for years ending in 50-99. See the SQL Language Reference for more information: 

We have a package on a 10.2.0.4 database(A) and a package on an 11.2.0.2 database(B) that references A's package. When something on A changes that requires the package on A to do an implicit re-compile, sometimes the package on B that should also implicitly re-compile goes invalid. The error it shows is PLS-00907: 

When converting characters to numbers the format specification must match exactly, but when converting numbers to characters the number can be selectively extracted using the format specifier. 

Only you will know which answer gives the results you are expecting, but they are all correct answers to the question as given. Your question is better than many because it is self contained and includes the source data, but it will help if you include the output you are looking for as well. I recommend you add that to the question and make sure the accepted answer matches those results. 

I am having a problem sending email from one of my servers. I want to use DatabaseMail and/or msdb.dbo.sp_send_dbmail I have other production servers that use all the same databaseMail settings and the same SMTP server, and they are all working fine, only this one particular server is not sending emails via Sql Server and I need to find out why. The first thing I test is the connection to the SMTP server, and for testing that I use Powershell to send an email, and it works fine!!!! Here is the script as how to send emails using Powershell. 

It is a known fact that the DMVs dont hold accurate information regarding number of pages and count of rows. However, when you have the stats updated, I can't see why they wouldn't. I am working on a monitoring tool, want to know disk size of each index and data, etc. Eventually I would like to find the right fill factor, and other things etc. The space used by my function and the old sp_spaceused differs a little bit on the space usage, but not on record count. Can you see if there is anything missing in my select? this is the sp_spaceused (then I convert the numbers in MB): 

is it maybe something else running at the same time? like backups to take or other types of backup (non-sql server) 

We have a big mysql performance issue on a production environment with 18 "front" servers sending mysql queries to a unique mysql server. We are using Redis on each front server for a number of things, but there is still one important job done only on the mysql server. So we have 5 processes on each of the 18 servers, sending big select queries to our mysql server. The where condition is changing for each query because the result is cached on each ad server in a redis cache and reused when the where condition would be the same. Since we have a different caching system, this query is using the SQL_NO_CACHE option. There are other queries running on the system, many small writes and a few large reads and writes, but most of the load comes from this query. The query is quite big: it uses 17 tables, some of them with several million records, the query can return up to 5000 records, each record is quite large. But the query is well optimized and runs in 0.02s when the load is low. With our usual system load, the query runs on average in 1.5s. We are handling query queues on each ad server and monitoring the average time a query stays in the queue before it’s executed. Usually a query stays in a queue about 2 minutes. But lately, with our traffic growing and after adding a few servers, the average execution time increased to 2s and the time-in-queue exceeded 20 minutes. So we decided to try some fine tuning... Our mysql server is a 24 cores running Debian 6 with 64G RAM and MySQL 5.1 (InnoDB only). Since the CPU and RAM were not used at 100% we thought there could be another bottleneck. We noticed that we had many TCP sockets on the server in the time_wait state and thought that maybe the number of TCP sockets available was a bottleneck so we changed from using a single IP address to using 5 IP addresses for our mysql server. We also changed the TCP sockets lifetime from 2 hours to 2 minutes. After 2 hours, the performance increased very nicely: the query average execution time went down from 2s to 0.5s and the time-in-queue went down from 20 minutes to 10 minutes… but after one and a half day the execution time went back up to 15s and the time in queue to 20 minutes. We have tried reversing all the changes in order to go back to the previous situation but it didn’t work. We are looking for a senior DBA with good experience of InnoDB/Debian who could help us fix this performance issue, then help us find the bottlenecks and optimize the tunning of our mysql server. Here is our my.cnf file: 

It seems like there would be an easier way than this. I don't know how to get the default isolation level for the session if that is what you are looking for. 

If you must kill the session from within the script, then here is a method I would investigate. Create a table to store the run status. It need only have one column for an SID. The first thing your script would do is start a second instance of itself running in the background and passing it a parameter to indicate that it is the background process. The background branch of the script would sleep for the maximum amount of time you want the process to run and when it wakes up it would check the table and if there is an SID stored will kill the session and end. In parallel to this, the original/foreground process after kicking off the background version of itself would create a session, insert it's SID into the new table and commit. It could then run all the statements necessary and end by deleting it's SID from the table. If the script takes less than the allotted time, then the background process will have nothing to do when it wakes up. If the script takes too long the background process will kill the foreground session. In theory this seems like it would work, but I haven't tried it. 

the first one does not even bring the most correct results, the query 3 (as suggested in the comments by spaghettidba and ypercubeᵀᴹ ) is the one with best performance in my live environment, with my real table and data, as you can see below, the query I had originally and the one based on query 3: 

when I run my stored procedure it is a complete success, even if I run it through exec. Running these: 

returns 60 rows. question: is there a way I could modify my function so that it would return the whole 398 lines of the procedure? 

this was really nonsense. I right click on ssms and run as administrator, and added the permission there without a problem. 

I have a script to generate the create table script. You can see it on my answer to this question. However, now I need to script my partition functions and partition schemes On this question here, see the answer has examples on how to add a partition to a table and how to remove a partition from a table, and I would like to have a look at the scripts at any stage. I have found this link: how to find partition function text applied to a table how to generate the scripts for create partition function and partition schema? I was working on a script to generate the as you can see below, but it only works for my own partition function because it tricky to get over the fact that sys.partition_range_values has a sql_variant data type as value. when I used a case I got the following error: 

I have a pl/sql block in my glogin.sql file to show information from v$instance when I connect on the database server. Is there a way to suppress the ORA-01034 we would naturally get when the instance is idle? An idle instance reports that it is idle when you connect, so anything additional is not necessary and could be confusing. 

Have the development systems use the same HSM Partition as production. This would be the most obvious solutions, but Oracle does not recommend it and this could potentially allow development to affect production. Drop encrypted columns or drop them and add them back. - This would be relatively fast, but leaves the columns empty, preventing processes that use the data from being tested. It also changes the column order, which could be problematic. Upgrade to 12c and use the reverse migrate command to move back to a wallet from an HSM. - Same problem as #1, albeit for a shorter time, but also requires an upgrade which we cannot do. As I understand it, this functionality is not available in 11g. Drop/recreate the encrypted columns and populate them using queries from production as part of the data manipulation part of the refresh. Drop the tables and import them from an export of production. Rather time consuming.