I'm trying to understand the policy gradient approach for solving the cartpole problem. In this approach, we're expressing the gradient of the loss w.r.t each parameter of our policy as an expectation of the sum of gradients of our policy gradient for all actions in a sequence, weighted by the sum of discounted rewards in that sequence: $$\nabla_\theta L(\theta) = E[ G(S_{0:T}, A_{0:T})\sum_{t=0}^{T}\nabla_\theta log\pi_\theta (A_t|S_t) ]$$ and we estimate it using an empirical average across all samples in an episode - which makes sense intuitively. BUT the less intuitive part is that I saw a common practice to normalize advantages in between episodes in several implementations (and indeed it works better). So after they calculate the they wouldn't directly use the advantage, but rather would normalize it, e.g. here they do after every episode: 

I hope that this answers your question. Note: If you wish to know what's the difference between RDD and DataFrames, I advice you to read Databrick's blog entry about it here. 

Like @SeanOwen pointed out its called . spark.mllibâ€™s FP-growth implementation takes it as a hyper-parameter under . It is the minimum support for an itemset to be identified as frequent, e.g : if an item appears 4 out of 5 transactions, it has a support of 4/5=0.8. Usage: 

FP-growth is a frequent pattern association rule learning algorithm. Thus it's a rule based machine learning algorithm. When you call the following : 

The following env describes a simple task: there's a bit-vector of size 10, with all bits initially set to 0. The agent must learn to switch it all to 1s. It has 10 possible actions - each one switches the corresponding bit to 1. Switching a bit which was 0 to 1 gives a reward of 1, but trying to switch a bit which is already 1 gives a reward of -1 and ends the game. The agent sees the entire bit-vector after each step. Why isn't it very very simple? I wrote a policy-gradient approach agent but it doesn't arrive to sufficient results (stay at around ~3 points average) after 10K episodes. What am I missing? Is it indeed such a hard task??? 

If you want to understand how spark internals work, I suggest that you watch the presentation made by Databricks about the topics 

You are actually creating and Frequent Pattern model without generating candidates. So actually you'll need to generate afterwards if you need to use them with : 

You can also set a threshold where you can drop a recommendation at a certain limit of your prediction level. E.g. Let's say an user alpha has the following recommendations with a threshold of 0.7 

I trained a tree-ensemble classifier (XGBOOST) on population A, validated it and I'm satisfied with its accuracy (AUC 0.78). Now I'm trying to transfer it to a slightly different population B, and there the accuracy of the model deteriorates badly (AUC 0.68) I tried isolating which of the features did not transfer well, both by simple univariate analysis (comparing distributions), and by comparing each feature correlation with the label, and couldn't find anything obvious. Is there a way to debug and understand which of the model assumptions which held at A do not hold at B? I thought about comparing the label distributions at every node in every tree for the validation populations in A and B, thereby testing all conditional probabilities the model assumes are holding actually hold at B. Would that help me understand what broke? or would I just get tons of tiny differences? Is there some other simple way I'm missing? (related to this survey Machine learning learn to work well on future data distribution?) 

EDIT: To avoid confusion for some concerning PCA and Dimension Reduction, I add the following details : PCA will allow you compute the principal components of your vector model, so the information are not lost but "synthesized". Unfortunately there is no other imaginable way to display 39 dimensions on a 2/3 dimension screen. If you wish to analyze correlations between your 39 features, maybe you should consider another visualization technique. I would recommend a scatter plot matrix in this case. 

Personally, I don't use vagrant with a local provisioning. I have installed a Spark cluster locally without HDFS which allows me to experiment and develop easily without the overwhelm of a virtual machine. HDFS is not a requirement for local clusters, and it's also a kind of a system administration nightmare if you just need it for local testing. Spark works very fine with local file systems which you'll also have to port when deploy on your Cloud of course. Nevertheless, You can use vagrant with AWS provisioning to create a cluster for heavier testing. Note: AWS works with S3 and not HDFS. AWS's HDFS version is ephemeral and if you put down the cluster, you'll loose all your computations. For persistence, you'll need to write back to S3. 

I'm training a model for gout disease on a training set I sampled 1-to-7 case-control ratio (enriched in cases). I have 220 features and I reach a cross-validated AUC of . I'm using a special value for missing values and I don't tell that to XGBOOST - I let it treat missing values just like any other value. I then use it on the general population, with the true ratio of about 1-to-13 case-control. I get slightly worse AUC , and my prediction mean is a reasonable . THEN, I tell XGBOOST that : 

You can also write it using a SQL dialect by registering the DataFrame as a temp table and then query on it use the SQLContext or HiveContext : 

You can drop the i3 recommendation since you don't consider it good enough. Nevertheless, the parameter of your recommendation engine must be determined of course with the help of your evaluation metrics. Considering software solutions, I use Apache Spark MLlib with Scala as a base for my recommendation engine algorithms where you can compute item cosine similarity easily per example where you are using in-house implementation or an approximation with the DIMSUM algorithm. I hope this helps! 

the cross-validated performance on the train population stays the same. BUT the performance on the general population degrades from AUC to AUC . Also, I noticed that the prediction mean is changing from to (!) 

Just adding the algebraic part to @Winks answer: The second equation should have its sign reversed, as in: $$\sum_{i=1}^n\frac{1}{2}h_i[f_t(x_i) - (-g_i/h_i)]^2 + constant = \sum_{i=1}^n\frac{1}{2}h_i[f_t^2(x_i) + 2\frac{f_t(x_i)g_i}{h_i} + (g_i/h_i)^2] = \sum_{i=1}^n[g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i) + \frac{gi^2}{2h_i}]$$ The last term is indeed constant: remember that the $g_i$ and $h_i$ are determined by the previous iteration, so they're constant when trying to set $f_t$. So, now we can claim "this is exactly weighted squared loss with labels $-gi/hi$ and weights $h_i$" Credit goes to Yaron and Avi from my team for explaining me this. 

You are probably thinking in terms of regular SQL but spark sql is a bit different. You'll need to group by field before performing your aggregation. Thus the following, you can write your query as followed : 

Since you are using Spark 1.6, I'd rather do these kind of transformations with DataFrame as it's much easier to manipulate. You'll need to use SQLContext implicits for this : 

Now, concerning the usual flow of building and validating such models, your code doesn't deal with that. It's usually done through quality measures variations. It can also be done through Feature extraction techniques. With such techniques, you should consider the following. For each association rule, you'll need to measure the improvements in accuracy that a commonly used predictor can obtain from an additional feature, constructed according to the exceptions to the rule. In other terms, you'll have to select a reference set of rules that should help your model perform better. I strongly advice you to read this paper about the topic. So now what does that mean ? This means that you'll need to implement that pipeline yourself because it's not implemented in Spark yet. I hope that this answers your question.