Apart from the licensing question, there's a question about the technical limitations of Web/Standard edition. Web edition is limited to "Lesser of 4 sockets or 16 cores", but for a Virtual Machine the sockets and cores are reported by the hypervisor, and may differ from the underlying hardware. That appears to be the case here, as the hypervisor is presenting the VM with 8 VCPUs and claiming that they are on 8 separate sockets. SQL Server on startup looks at the reported sockets and cores when and will limit the schedulers accordingly. Here that's limiting the CPU use to half of what it should be. To fix this, the hoster should reconfigure the hypervisor to accurately report the socket and core count of the hypervisor host. 

Service broker is quite complex, as all messages are sent on potentially-long-lived duplex conversations, and you must program against it in TSQL. Once you grok the design, and the strange troubleshooting a lot of the TSQL is boilerplate. The best thing about Service Broker is that it's not a separate repository and your messages are have the same HA/DR and Backup as your other data. And it's trivial to enlist SEND or RECEIVE in a transaction involving other application data. If you don't want in-database messaging, consider Azure Service Bus is much simpler to use, more functional, is operated for you in the cloud, and is able to handle that level of scale quite easily and cheaply. Also for many scenarios you can just use a simple table as a queue. See eg $URL$ from Remus Rusanu, who used to be a SQL on the SQL product team that owned Service Broker. You don't get blocking reads, but if your latency requirements can tolerate a reader in a polling loop, it can be a good option. 

IX_User_Email is enough, although you might want to make it a unique index to prevent multiple users with the same email. And this query, 

No. But stepping back, you should plan to have multiple federated databases holding your tenant data. You can still go with a multi-tenant database design, but in the end state you will have a mix of multi-tenant and single-tenant databases. There are many important advantages to isolating a tenant in a database. 

Each session is deleting multiple rows from the same table based on the value in a secondary index. Even if you don't attempt to delete the same row in multiple sessions, the sessions will search for the rows to delete using the secondary index, reading with a U lock, and then convert to an X lock for each row. This creates the possibility of deadlocks. You can avoid this by searching for the rows to delete without a U lock, and then deleting them in a separate statement. Some of the rows you find in the first query may be deleted by the time you attempt the second. But you probably don't care about that. So something like: 

Database Mail does not send the mail through SMTP when you call sp_send_dbmail. The message is put on a service broker queue, and sent by a background process. So if the transaction in which the trigger executes is rolled back, then the message will be rolled back from the queue, and the mail will never be sent. So that's one reason why your mail may not be sent. Otherwise you can check the log table: 

Please read: Enhanced Azure Security for sending Emails â€“ November 2017 Update which outlines the recommendations, options and restrictions for sending emails in Azure. In particular: 

If the alternative is to have a data structure that efficiently filters by tenant, then no. If TenantId is the leading column in your clustered index, then single-tenant queries will be pretty fast. The NCCI row groups will not be segregated by tenant, and so all the row groups will need to be scanned to find rows for a single tenant. As rows are inserted, they are inserted into the "middle" of the clustered index, typically at the "end" of the rows for the tenant. But rows are always inserted into the delta store for an CCI/NCCI. And whenever the delta store has 1 million rows in it, it's rebuilt into a columnar row group. So each row group will end up with the last million rows inserted across all tenants. You can fix this with partitioning by TenantID, which could give each tenant (or group of tenants) a separate physical table (or NCCI). Each partition will have its own delta store, which will fill up with single-tenant rows. If you do partition, each partition might end up too small for a Columnstore to be useful. You'll want a few million rows per partition at least. In general with multi-tenant data, you want to avoid interleaving tenant data in a way that requires you to read all tenants data to retrieve a single tenant's data. 

If the content being versioned is not stored as a blob, but is in separate tables, the pattern still holds. All the tables that store the versioned data need the VERSION_ID. It would certainly be an interesting research project to explore all the different ways this can be done, and discuss options for conflict resolution, merging changes and sketch out what a general solution might look like. 

Migrating to in-memory tables always requires recreating the tables from scratch. SSMS will rename your old tables, create the new in-memory tables, load them from your old tables, and drop your old tables. 

The trigger is not being recreated. sql_text in sys.db_exec_requests is at the batch level, not the query level. See eg $URL$ For an example of how to parse out the currently-running query from the larger batch. That wait is to fetch the page that the row is written to, either on the Clustered Index or some non-clustered index. So I would look at the index design of the table, and see if it should be better-optimized for inserts. 

--Microsoft SQL Server 2012 Internals, pp 10 Kalen Delaney, et al This is in contrast to Oracle, which has the "Oracle Net Listener" which is a separate process. $URL$ 

An alias is a client-side concept. To be effective you must configure the alias on every client computer connecting to SQL Server. The alternative is to create a Hostname Alias to enable that name to be resolved by all clients on the network. There are four easy steps to creating a Hostname Alias, but they are poorly documented and often misunderstood. All you have to do is: 1) Create a DNS Record pointing to the IP address of the target server 2) Configure SQL Server to listen on port 1433 on that IP address 3) Add SPNs to enable the SQL Server Service account to use Kerberos Authentication (optional if you don't use Kerberos). 4) Add BackConnectionHostNames entries to enable NTLM Authentication $URL$ 

Ensure that the NUMA configuration of the host is reported accurately by VMWare, and he out-of-the box configuration of SQL Server will do that. Be sure to apply Service Pack 2 to get all the latest fixes. There's some good stuff back-ported from SQL 2017 in there. 

Process Working Set 31GB, which is the portion of the process committed virtual memory currently in RAM. So the SQL Process is using 31GB of RAM. SQLOS VM Committed is only 5,687MB, and Locked Pages Allocated is 0. SO SQLOS can only account for 6GB of the memory usage. So something in the process is using 25GB of memory, and it's not SQL Server. The typical culprit here a linked server driver, which allocates memory in the process which is not tracked by SQL Servers memory clerks and pools. You mentioned linked server to Oracle. What OleDB driver and version are you using? Any other linked server drivers used? Eventually you need to figure out what's leaking memory in the SQL Server process and fix or eliminate it. In the short term you can bounce the SQL Server process periodically. A good way to eliminate loading linked server drivers in SQL Server is to use SSIS. For reading data from remote sources the SSIS Data Streaming Destination enables you to isolate the 3rd party data access components in a short-lived process and still query from them like a linked server. 

This is not something you should ever see on an Azure SQL Database. You should engage support on this. There are a couple things you could do yourself, if you want. First just try to CHECKPOINT the database. Second if you are sure you have no active transactions, change the database SLO. Taking it out of the Elastic Pool, then putting it back in. 

Leave it. That's tiny, and not worth adding complexity. Also, even if both the table and the LOBs were larger, the answer is still to leave it. Large LOBs will be moved off of the row, and you can set a configuration option for all the LOBs to be stored off-row. You can also configure the table for the LOBs to be stored on a separate filegroup. So there's no reason to change the logical model here. See: 

You've found it. It is expired, and out-of compliance. It will stop working soon. There should be no need to wipe and reinstall SQL Server. You can perform a quick, in-place Edition Upgrade. The supported Edition Upgrade paths are listed here, and in this case are: From: SQL Server 2016 (13.x) Evaluation Enterprise** To: SQL Server 2016 (13.x) Enterprise (Server+CAL or Core License) SQL Server 2016 (13.x) Standard SQL Server 2016 (13.x) Developer SQL Server 2016 (13.x) Web Of course any time you are installing software on your server, you should ensure that you have a current set of backups. 

It's pretty important that you don't grant the user the ability to alter a schema owned by another user, especially one owned by dbo. If you do ownership chains will allow the user to skip permissions checks on the schema owner's objects in all schemas. 

You can grant access to the view without granting any rights on the underlying tables using Ownership Chaining. In this scenario you would need to enable Cross Database Ownership Chains, and grant the user access to the target database, but not SELECT on any of the tables. 

As Aaron noted, there's no reason to retain multiple differential backups, as you would only use the latest, and only as an optimization. The limitation on one encrypted backup per file is in the main doc page for Encrypted Backups: 

Not in the DMVs, but you can turn on the Blocked Process Report: blocked process threshold Server Configuration Option 

There is ZERO additional documentation available to MSDN subscribers. All published documentation and support material is published to the web. The best sources for SQL Server Internals are the work of Kalen Delaney $URL$ and Paul Randal $URL$ In particular Paul has some posts on reading the SQL Log. eg $URL$ 

Yes. It is generally safe, and often useful to create indexed views on subscriber tables in transactional replication. You may need to drop and recreate the indexed views using custom scripts before and after initializing from a snapshot. See Execute Scripts Before and After the Snapshot Is Applied You can also use a different indexing scheme on the subscriber (also by using post-snapshot scripts). In particular you can make the subscriber tables Clustered Columnstore tables, or add non-clustered Columnstore indexes on them for reporting. 

In SQL Server with Table and Index Compression (Row, Page or Columnstore) NVarchar columns don't use two bytes per character. They use Unicode Compression. Here's an example: 

A better practice would be to create a staging schema, owned by the ETL user. Then the ETL process can truncate tables, disable constraints, perform partition switching, etc within the staging schema. The ETL user would only need limited permission on the other schemas. You can also use a database role instead of a single user. Of course you can also enable your limited user to perform table truncations with a dbo-owned stored procedure, like this: 

eg this will emit a script based on the configuration of an instance that can be applied on another instance. 

Yes. StoredProcedure1 will execute first, and any resultsets from StoredProcedure1 must be consumed by the client before StoredProcedure2 is executed. It's possible that some client driver or DAL processes all the resultsets and then returns them to your calling code in a different order, but for SQL Server and the Microsoft client drivers, you will get the resultsets in the order in which they are produced. 

It may be that with enough joins and subqueries you could get a bad plan. But it's not the first place to look. Use the Query Store (SQL 2016+), or the plan cache DMVs, or XEvents to see the query plans and associated resource costs. For instance on AdventureWorksDW, these two queries use the same plan. 

That's just a total of the row count messages returned from SQL Server. These are sent as queries are run, before the transaction has been committed or rolled back.