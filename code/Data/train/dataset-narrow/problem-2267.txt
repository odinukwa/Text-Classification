Now it is clear that if $T$ halts, then $M(T')$ returns $\mathit{false}$, as the set of indices halting Turing machines is not a decidable (recursive) set. If $T$ does not halt, then $T'$ does not enumerate any numbers, which makes it exactly the class of problems containing no indices! Therefore $M(T')$ answers $\mathit{true}$, since that class is decidable (by the machine that always rejects). Therefore, $M(T')$ returns $\mathit{true}$ iff $T$ does not halt, and $\mathit{false}$ otherwise. Thus the existence of $M$ allows us to solve the halting problem for an arbitrary machine $T$, which is a contradiction. 

Finally, you might want to check the page on post-quantum cryptography to see some alternative approaches to public-key cryptosystems that rely on hard problems. 

In general, in a language with datatypes (like lists, trees, etc) it's easy to describe a language of functions which behave exactly like we expect primitive recursion to behave. For example if the datatype is $D$, and the constructors $c_1,\ldots, c_n$ have type $$ c_i : T_1^i\rightarrow T_2^i\rightarrow \ldots\rightarrow T_{k_1}^i\rightarrow D\rightarrow\ldots\rightarrow D$$ then the recursor $\mathrm{rec}_D^O$ for output type $O$ will have type $$\mathrm{rec}^O_D:(T_1^1\rightarrow\ldots T^1_{k_1}\rightarrow D\rightarrow \ldots\rightarrow D\rightarrow O\rightarrow \ldots\rightarrow O)\rightarrow\ldots\rightarrow D\rightarrow O$$ and the operational semantics will be: $$\mathrm{rec}^O_D\ f_1\ \ldots\ f_n\ (c_i\ t_1\ldots t_{k_i}\ d_1\ldots d_m)\rightarrow\\ f_i\ t_1\ldots t_{k_i}\ (\mathrm{rec}^O_D\ f_1\ldots\ f_n\ d_1)\ldots (\mathrm{rec}^O_D\ f_1\ldots f_n\ d_m) $$ for each $i$. Something of a mouth-full! At least for natural numbers, we indeed get $$\mathrm{rec}_{\mathbb{N}}^O:(\mathbb{N}\rightarrow O\rightarrow O)\rightarrow O\rightarrow\mathbb{N}\rightarrow O $$ $$\mathrm{rec}^O_{\mathbb{N}}\ f_0\ f_1\ 0 \rightarrow f_1\ 0$$ and $$\mathrm{rec}^O_{\mathbb{N}}\ f_0\ f_1\ (S\ n)\rightarrow f_0\ n\ (\mathrm{rec}^O_{\mathbb{N}}\ f_0\ f_1\ n)$$ as hoped for (note that the zero constructor has zero arguments!). If now we allow for constant functions and projections, and allow arbitrary uses of $\mathrm{rec}^O_D$ for non-function types $O$, then you have exactly primitive recursion. Reassuringly, if all the $T^j_i$s are non-functional as well, then the usual GÃ¶del encoding of the datatype gives the same primitive recursive functions. 

There is a quite precise definition of what an algebra is in category theory: see this article for instance. It took a few years to understand how a structure with bound variables could be understood in the same context as the term algebra structure commonly used in mathematics and computer science, and it turns out that the categorical concept of F-algebras is capable of unifying the two. I am not sure bout the historical aspects of the solution but one possible approach is the presheaf algebras introduced by Fiore, Plotkin and Turi (available here) settled the question and spurred different but similar approaches, see e.g. Hirshowitz et al. and his phd student Julianna Zsido. Some exiting research remains to be done on how to use the categorical concepts to refactor and deepen our understanding of structures with bound variables, in the hopes of eliminating syntactic "cruft" which usually comprises the most boring chapters of theses on $\lambda$-calculi and related structures. 

The answer to your question depends on several things, the most important of which is the size of your function spaces. I'll explain. Define $$O_0 = nat $$ $$O_{n+1} = \mu X.\ 1+X+(O_n\rightarrow X)$$ As you noted in your answer, each $O_n$ can be considered internally to be the $n$-th regular cardinal of your system. In set theory, this datatype can be represented by an actual ordinal and is appropriately huge. However, such constructions may be added to some version of type theory, and the question becomes: what ordinal is needed to give a set-theoretic interpretation to this construct? Now if we restrict ourselves to constructive semantics, a natural idea is to try to interpret each type by the set of "realizers" of this type, which is a subset of the set of $\lambda$-terms, or equivalently, the natural numbers $\mathbb{N}$. In this case, it is easy to show that the ordinal is countable for any $O_n$, but that this ordinal grows very quickly. How quickly? Again, this depends on the amount of freedom you have when trying to build functions. The theory to building such ordinals is described in the theory of Large Countable Ordinals, of which Wikipedia has, surprisingly, a lot to say. In general it is easy to show that the ordinals in question are smaller than the Church-Kleene Ordinal, unless you allow non-constructive means of building functions (say $Beaver(n)$ that computes the busy beaver number for machines with $n$ states). This isn't saying much though, except that in a constructive theory, you only require constructive ordinals to build interpretations. There is a bit more to say though. First, there is a very nice presentation by Thierry Coquand that details that in the absence of an eliminator for all other types but $nat$, you can build $O_1$ in exactly $\epsilon_0$ steps. In general there seems to be a correspondence between the logical strength of a type theory, and the size of the largest ordinal that it can represent in this manner. This correspondence is the subject matter of Ordinal Analysis, which has been studied at great length since the late sixties, and is still under study today (with some amazing open questions). Warning though: the subject matter is as technical as it is fascinating. Hope this helps. 

How do we do this? Well as Wadler prescribes, we take $T=\exists X.(X\rightarrow F(X))\times X$. In a similar manner than before, we have $$ cofold = \lambda z:Z.(Z,\omega,z) : Z\rightarrow T$$ This construction would not have worked if we had instead taken $T=\exists X.X\rightarrow (X\rightarrow F(X))$. 

If $a\rightarrow_S b$, then $a\rightarrow b$ (sub-ARS condition) If there exists $b$ such that $a\rightarrow b$, then there exists $b'$ such that $a\rightarrow_S b'$ (same normal forms). 

There has been a lot of work around the idea of statically checking the memory safety properties of programs. Francois Pottier give an excellent overview of various approaches in this presentation. The main technique you probably would be interested in is region typing. The Wikipedia article is quite complete, but the basic idea is to keep explicit regions which references are required to explicitly be assigned to a given region, which can be managed by the programmer or left to be inferred by the compiler. This is somewhat reminiscent of the RAII approach to memory management used mainly in C++ (the wiki article is terrible however). In practice, region inference hasn't gained widespread use in compilers, mainly because of the complexity of the analyses involved. Again related, there has been a flurry in work on separation logic over the last decade, which is an extension of Hoare Logic which allows reasoning over separate regions of the heap, in order to analyze whether objects can share memory locations or not. This method of reasoning is quite difficult to apply to a language like C with "real" pointers, because of the possibility of pointer aliasing. 

The idea is that you can use map to "build up" arrays of partially applied functions, and then use fusion to apply that array of functions to an array of arguments. In the case of $+$ you can just write 

There's a comprehensive treatment of different Turing-complete computation models and proofs of their equivalence in Martin Davis, Computability and Unsolvability. Some of the most popular systems are described, including Turing-Machines, Post problems and general recursive functions. 

Short answer: yes! You don't need that much machinery to get the proof to go through. One subtlety: it seems on the face of it that there is a use of the excluded middle: one builds a set $D$ and a number $d$, and shows that either $d\in D$ or $d\not\in D$ which leads to a contradiction. But there is a lemma, true in intuitionistic logic, that states: $$ \mbox{ for all statements } P, (P\iff \neg P) \Rightarrow \bot$$ This suffices, along with the usual proof. Note that in general "surjection" may have some subtle nuance in constructive/intuitionistic logic (without choice) so you have to make due with "right invertible" instead. A very standard proof in Coq (which for some reason I couldn't find online) might go as follows: 

It would be nice to have a more elegant description of this process though. That's were Carlos' answer comes in: these datatypes can be described more elegantly in category theory as the initial algebras of certain functors, often called polynomial functors. The recursor is then just (a variant of) the initial morphism of this algebra, sometimes called a catamorphism by people trying to confuse things. This morphism exists by construction of the initial algebra. A paramorphism is just the particular variant I described above. 

One could ask the same of set theory: isn't the set of terms that can be written down countable? Why isn't there a surjection from $\aleph_0$ to any set? The answer, of course, is in the difference between internal truth and external truth. Externally, the set of well typed terms of type $A$ (say in the empty context) is recursively enumerable in any reasonable type theory (like MLTT), so there is an effective surjection from $\mathbb{N}$ to that set iff it is non-empty, by a classical result in recursion theory. Internally however, it is generally not possible to prove that such a surjection exists. In particular, a bijection between say, $\mathbb{N}$ and $\mathbb{N}\rightarrow\mathbb{N}$ implies a contradiction by a simple diagonalization argument (see, e.g., here). In fact the proof works even if we only require a retract instead of a bijection. However this doesn't preclude a surjection from $\mathbb{N}$ to $\mathbb{N}\rightarrow\mathbb{N}$! It's only necessary that this surjection does not have a right inverse (in set theory, these right inverses always exist, but the proof of that requires the axiom of choice). It is in fact consistent, but not provable, to assume that such a surjection exists. 

Your algorithm is flawed: it doesn't distinguish $$ \lambda x.\lambda y. x\ y$$ and $$\lambda x.\lambda y. y\ x$$ It's not completely trivial to fix: if you want to prove the equivalence of $\lambda x.\lambda y.x\ y$ and $\lambda y.\lambda x. y\ x$ (which does hold!), you have to find a new variable $z$, and replace $x$ by $z$ in the first term, and $y$ by $z$ in the second. If you simply replace $y$ by $x$ in the second term, you'll get $\lambda x.\lambda x.x\ x$ which is not what you want. It seems you can "pre-process" your terms by requiring all bound variables to be distinct. This is called hygiene. Unfortunately hygienic terms like $$ (\lambda x. x\ x)\ (\lambda y.y)$$ become unhygienic when applying $\beta$-reduction: $$ (\lambda x. x\ x)\ (\lambda y.y)\rightarrow_\beta (\lambda y.y)\ (\lambda y.y)$$