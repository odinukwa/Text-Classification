This should fix you right up! Here is a SQL Fiddle for you to play around with if you like. Edit: And as suggested... The Oracle version: Schema and Data 

I'm sorry, I don't have a corrupt table to help perform any verifications. My answers here must, by necessity, be heavily based on speculation. With the limited information we have, I am guessing that you are attempting to do a full database , along the lines of 

OK, so you've listed a few questions here, so I'll try my best to answer them. Is your query correct? First, you said: "I am interested in finding out how many (pl_namespace,pl_title) pairs in the pagelinks table show up in the page table as (page_namespace, page_title)." Is the query you're performing correct for that, or did I misunderstand the description? If you run 

with significantly less burden. Also, rather than Postgres being required to maintain a large composite index, you could build several different PK indexes on each of the "version" tables, using only . Last but not least, when you are done with old versions forever (say they are a few versions old and you want to be rid of them), with partitioning you will simply be able to delete the whole table which corresponds to the old version, rather than crawling through one big table to delete the relevant rows corresponding to the obsolete version. Finally, I would recommend looking into partition management if you go this route. Cheers! 

Review How Merge Replication Detects and Resolves Conflicts A lineage column is used which contains the generation and the node that made the change. The actual winner decision during the conflict depends upon the conflict resolver used for the article. Using the default prority-based resolver, a priority value per node should be assigned per subscriber to determine the winner for all conflicts. With regards to referential integrity and the concept of conflict resolution in mind, the processing order with which tables (articles) are processed is your only means of guiding the conflict resolution before your child tables. You can modify the order of the article processing in the merge publication via the @processing_order parameter for sp_addmergearticle 

Yes. Create (database) MASTER KEY first, then BACKUP, DROP, and CREATE the certificate (importing from earlier BACKUP), but this time WITHOUT a password. This will encrypt the certificate's private key for the certificate using the database master key, which will automatically open the private key as needed. Realize by doing this, you are exposing your data (via the private key for the cert) to anyone that has access to the database and sysadmin to the server. In reality, all you have done is kept the data-at-rest encrypted on disk. Anyone with access to your backups and the service master key can decrypt your data. Make sure you follow best practices for keys (and certificates) in SQL and backup the database master key. You can view the private key encryption type by looking at the pvt_key_encryption_type_desc column in sys.certificates database dmv. 

where you add your measurement values as (key,value) pairs of (timestamp, measurement). Use case: This is an implementation probably better left to someone who is more comfortable with PostgreSQL, and only if you are sure about your access patterns needing to be bulk access patterns. Conclusions? Wow, this got much longer than I expected, sorry. :) Essentially, there are a number of options, but you'll probably get the biggest bang for your buck by using the second or third, as they fit the more general case. P.S.: Your initial question implied that you will be bulk loading your data after it has all been collected. If you are streaming the data in to your PostgreSQL instance, you will need to do some further work to handle both your data ingestion and query workload, but we'll leave that for another time. ;) 

romeo, before I craft a reply, I want to ask: Is there any chance you have the opportunity to edit the schema in this case? If you can, you definitely should, because the problems you're running into are because of denormalization. Normalization of the schema From your example table above, it appears this should be split into at least two separate tables. From the information you've given you should have an table and a table, with (pseudocode) definitions as 

I have a large SQL insert statement that updates a summary table. The process normally runs hourly, and takes about 5 to 10 minutes to calculate uniques over that period. I "foolishly" chose to run it over a period of 15 days, and I'm wondering if there's anything I can do to gain visibility into where it's at. The process has been running for 9 hours already. I would simply like to know if it's still the SELECT query that's running or if it's inserting data: I need to know if I should stop it and do smaller batches, or if I should just let it finish. I know the server's been at it for 9 hours because of that statement: 

I have a few million rows in my database already. I didn't know about the PostgreSQL UUID data type when I designed my schema. One of the tables has 16M rows (about 3.5M to 4 M records per shard), growing at about 500K records per day. I still have the luxury of taking the production system down for a few hours if required. I won't have this luxury in one or two weeks. My question is, will it be worthwhile to do so? I'm wondering about JOIN performance, disk space use (full gzip'd dump is 1.25 GiB), things of that nature. Table schema is: 

Unfortunately, the SQLAgentOperatorRole msdb role is the most privileged role for "managing" only jobs on a SQL server, and it only gives the users the ability to disable/enable jobs/schedules not owned by the user. The only supported way for giving full access to all jobs AND manage them using SQL Server Management Studio is to add the users to the sysadmin server role. The reason for this is the way the sp_update_job* procedures are written, they are actually checking role membership for access. Excerpt from sp_update_job is below. Note the comments and the explicit check for @x_owner_sid <> SUSER_SID() along with NOT being in sysadmin. 

Execute the above right after you have updated the stats being used, and note the modification_counter value. It should be zero. Now run the original query, then the above again. The difference between the estimated rows minus the last modification_counter value (assuming you are not deleting rows), then you should have your answer. The stats aren't the same because your data has changed. 

You are talking about removing data piecemeal from a database. That is 3rd party only if you are trying to use backups only, but that really isn't an ideal scenario for that use-case. If you are that short on space (so much so that you cannot have a third copy of the full database), create another database (name it "DevDB" for example) on the stand-by server, setup an ETL process to script off the objects of your database, and then import only the data you want into the "DevDB" database. 

To me, it looks like you're on the right track, if I'm understanding your question clearly (which I'm not sure that I am. :P ) To me, it looks like you simply need a block, where you are declaring variable values which will persist throughout the function block. Add 

There aren't many cases where I'd recommend this course, only if this is a tiny project which doesn't warrant much of your time. Dimensions and Facts So, if you've cleared the hurdle of question (1), and you want a more performance schema, this is one of the first options to consider. It includes some basic normailization, but extracting the 'dimensional' quantities from the measured 'fact' quantities. Essentially, you'll want a table to record info about the trips, 

Erwin, since this was our discussion in the comment thread from before, I decided to poke at it a little further... I have a very simple query from a reasonably sized table. I typically have sufficient , but in this case I used the commands 

yet no values from the table are in your . The good news is that this is not affecting your performance, because the query optimizer has wisely left the table out of the query entirely. This does concern me a bit just in principle. Odd Query Plan In your query plan, it chooses to enforce the conditions of 

The difference is purely based the cardinality of the data at the point when the stats were updated, and the modifications made prior to running your query. 

A TSQL step in the agent job does not support cmd mode type options (like switching servers). You can run the script (when saved to an external file) by using an operating system step, directly calling sqlcmd.exe The best option for periodic data movement/consolidation cross SQL instances is to use a linked server or an SSIS package. If you need near-realtime, then using transactional replication would be next. 

Just place a DENY ALL on the table for that user. DENYs are always evaluated first, and will override the GRANT off SELECT to the overall schema: 

Review the backupset and related tables in msdb as the source for if a successful backup was taken. If you see gaps in when a scheduled backup was to have occurred, your job either failed or never ran. Every backup command generates an internal checkpoint, which in turn, moves the log sequence number (lsn). Review the first_lsn and last_lsn pattern with every log backup in the backupset table. Likewise, a file will always be generated if your job backs up to a time-stamped filename (default behavior of maintenance plans). You can verify this behavior by manually/interactively running a log backup command multiple times on a database without an transactions between runs. 

I know that I should test my SQL script as well, but I don't have time to make a test before leaving for work! I can make a SQL Fiddle later, but for now it seems that no one has quite answered the question correctly. 

I can't build a full fledged test just right now, because I've gotta hit the road. Just note that this code is as of right now untested, but you can easily give it a shot since you already have the data tables. ;) Using JSON + + CTEs It looks like to me, from your description, that a entry is an array of JSON. I'm just making this clear because you earlier stated that is a JSON, but your data sample seems to conflict. In this case, why not use a simple Common Table Expression (the keyword) along with to extract the relevant JSONs as rows, and then perform your query? 

This way, you save on storage, and you can easily update/modify any category or subcategory entry in the list without needing to modify your entire table. Further, you can simply permit the column of the table to permit entries, if need be. Hope this helps! 

Optimization Barrier in CTE Here, I've tried to make use of the optimization barrier found when using a Common Table Expression. Any time you use the expression, the Postgres optimizer treats it as a separate block which it must calculate separately from the overall optimization, sort of like a 'forced materialization'. In this case, I try to use that to force the most selective predicate, the portion, to whittle down the result set first. Try out this query and see if it helps! work_mem Last but not least, there's been a lot of talk abou settings today. As @CraigRinger mentioned, we really need to see an to get the complete picture, but I'm wondering if this setting is sufficient. Check you file, and report back your setting, along with that , if you're still having trouble. Hope this works!