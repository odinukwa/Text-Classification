Hack your Team Bringing about change in your organisation is hard. People have habits, they resist change, and they are often comfortable with the status quo. To bring about change, in no particular order, here are a few tools you can use. 

Create a copy of your master/trunk branch and call it Find the branch with the greatest divergence from master/trunk. Determine if the branch needs to be kept, archived, or deleted. 

We have had problems a couple times now where the links to the CDN in our components don't get properly updated between the team, qa, and staging environments. Is there a way to force an error if the QA environment tries to access the team CDN or vice-versa? Or some other well-known solution to this problem? 

NO I would argue that Mature DevOps operation, does require a Mature Agile process. You are unlikely to be able to get the full confidence to continuously deploy or allow your developers to initiate the deployment process without a mature Agile process in place. However, I believe it is very important to make it clear that an organisation does NOT need to adopt their agile process before building up their DevOps culture and infrastructure. In fact, I would argue that it is actually easier to adopt Agile once you have some basic DevOps working in your company. Rather than Agile being a prerequisite for DevOps, I would suggest that DevOps be used to help advance your agile implementation. 

If you have docker compose installed on the Jenkins Build machine you can use to run docker-compose from a new shell. In general, you can run any command from your JenkinsFile using the command. 

Our team has two separate repositories for a frontend/backend system. They would like to have these two repositories deployed to the shared team environment together. My understanding of the JenkinsFile is that it will only work on the repository it committed to and only run when the SCM system sends a request to Jenkins informing it of new changes. Is it possible to have the JenkinsFile in the two separate repositories communicate with each other so that when one is built the other is built as well and only after both are built, they will then be deployed? My main goal here is to avoid creating a separate Jenkins job within the Jenkins UI. 

Cause others to experience the problem that DevOps solves. Many times the benefits of DevOps is only understood on a theoretical level by your team. Most of the problems that happen during deployment are hopefully and rarely experienced by the rest of the development team or management. To fix this, make sure that you are vocal about issues when they arise and mention how this problem wouldn't have happened if the team was using a continuous integration solution. Another possibility is to be sure you ask developers to fix problems that their code caused during deployment instead of fixing it yourself. Find the leaders. It is common for people to follow the leaders, be they management or just the most popular/commanding person in the group. Get those leaders on board with your desire to move to a DevOps culture, and devise public ways in which they can be seen using or advocating best practices. Build Trust. We are more likely to agree to things from people after we have already agreed with them once or twice before. Ideally, you can find small improvements that can be made without a shift in culture and build upon that success. However, if that isn't an option, ask them simple questions and offer simple suggestions so they get into a habit of saying yes or agreeing with you. Don't be ashamed to repeat yourself. Repetition works and eventually sinks in. Whenever possible mention how great things would be if the team was using DevOps. However, this only works if you have first built trust within your team. Make it pleasurable. If you are allowed to build a proof of concept for your DevOps situation, use cute emoticons and cheerful colours in the reports and notifications. Post funny gifs when a build fails. Make sure you aren't annoying with your updates. 

With Bamboo, a best practice for infrastructure management of agents seems to be using Docker: any time you need a new version of compiler or test tool, the path to deploy a modified version of an agent is very fast. How could this work with GitLab? 

Today, you could more or less setup a CI/CD toolchain by youself, even locally. Indeed I have seen teams doing it in an ITSM environment because the communication with management did not work. Nevertherless we know CI/CD is a very important, if not most important, company production line - it produces value, even if it is not seen so by ITSM to give it operations status and priority (in short, SLA for systems which should survive project time instead of setting them up in every project and department over and over). Let's ignore for simplicity network downloads. Question: what should be the execution speed of CI/CD relative to a setup on a local machine (if possible): 

Docker daemon allows, according to documentatio, using memory storage driver for its images. Has anybody any daily business experiences with that? 

In first place I look for default JIRA functionality e.g. without 3rd party plugins to simlify and improve the issue workflow. Examples: 

With a private registry, I cannot pull images because certains image layers fail to download. The pattern seems to be that this happens with larger, 50MB sized layers. It shouldn't be a registry neither network problem as colleagues with other systems manage to get these images. 

Then, with you can run your 10000 Ubuntu's on one or - probably better in this case - a set of Swarm hosts. This could be bare metal or AWS. Enjoy! For MAC address is very interesting question which I hope other colleagues here can answer. Geolocation should be no problem as this is AFAIK a system setting - virtual OS does not implement a physical GPS sensor. Docs give an example how to set a fixed MAC address on - possibly this can be translated to the stack YML file as well. 

As you can see, in the first RUN statement I can list tables I have created in a new database, but in the second RUN statement the database is gone. Note. Normally, you would not transport your application data in production containers (immutability principle) but for demo and test purposes this can be nevertheless useful. 

What is the best practice either reference value in terms of an SLA spec to expect the infrastructure to: 

Strange enough node.js unlikely maven installs the packages the software needs inside the project directory. Is there a way to introduce a central repository which also caches different versions of components and would allow to pull them on build like we know it from other ecosystems? 

Now I have the impression if I have a code piece like which I want to pass to other context, Bamboo's templating logic will kick in and render this part as an empty string. Proof: 

I have found this picture in a comment to this Reddit thread and would like to better understand its message. I do not know the image's original source, maybe it will come up later. 

Or does blue/green mean, that exactly such considerations have to flow as well into the software design? 

These components have their bugs as well! Acceptable answer: at least one example of a GitHub project with working code going in this direction which is more than 6 months old but with updates within this timeframe (Maven or Gradle both ok). 

Same would be if you decide to configure all fixed dependencies' versions in on place, and for a specific stage/release. For such cases, what is the best strategy to have environment-specific POM properties files for Maven, and is it a good approach? 

Both Docker Registry and Docker Engine have API interfaces. I consider it should be possible to implement a Docker plugin for this integration to have images scanned for example as somebody pushes them. (side note - Docker registry is an image registry, not container registry). This scanner functionality is also possible through the commercial Docker Enterprise edition. $URL$ UPD here an example with clair/docker, looks quite simple $URL$ 

This list is far from complete, just gives an idea of what's out there. The binary repository can allow to host under one roof all of these making their management much simpler for teams. Note that you do not need a very large team to start reaping benefits from binary package management. The initial investment is not very large and the benefits are felt immediately. Especially that more and more platforms, frameworks and languages are integrating this dependency management directly in them. Their biggest advantage I have found however was to create an environment that your programmers will find natural and comfortable making it essential. It helps you as a devops creating a solid tool-chain and it helps them making the overall experience fit naturally in their stack of choice. As I said earlier there are many products out there that can serve as binary package managers some more generic than others in their target usage varying also widely in their accessibility and prices. My personal opinion is that binary repositories are as vital part of a well designed devops setup as would the source code repository or the continuous integration. Hope this helps 

Artifactory is a product by JFrog that serves as a binary repository manager. That said very often one will use an artifactory as a synonym of the more general binary repository, much like many people use Frigidaire or fridge to denote the refrigerator regardless if it is a Frigidaire brand or not. The binary repository is a natural extension to the source code repository in that it will store the outcome of your build process, often denoted as artefacts. Most of the times one would not use the binary repository directly but through a package manager that comes with the chosen technology. In most cases these will store individual application components that can later be assembled in a full product. Thus allowing a build to be broken in smaller chunks making more efficient use of resources, reducing build times, better tracking of binary debug databases etc Here are some of the most popular package manager that can be managed using a binary repository: 

You could use per repository config to override the user's config on a per-repository basis. When done on the repo considered to be the central source it should propagate with clones and pulls to other repos including local ones thus overriding centrally the local config. You can further enforce this through hooks in your central repo to check that file endings are what they supposed to be and reject the merge/push should it not be kosher. However these hooks will not clone with the repo, least not directly but this is not necessary since the rules only really need to be enforced on the central repo. Using templates in your git init you can ensure that your repos are created equals with all the gitignore, gitattributes etc files just the way you want them to be. Last thing, not directly related to your question, I found the biggest friction point when taking svn saavy team to a git based workflow what to get them used to the extra repo in the middle. I found that this visual cue sheet helped a bit explain which command had what effect to which part. Hope this helped a bit. 

Sadly I just switched jobs and I don't have access to a Jenkins instance (least not yet) so I cannot try it out for you. That said some ideas quickly jump in my mind that could get you there. I've had some success with the conditional build step plugin though I don't know if it would work for build timeouts. If not directly perhaps as a means to change some values through the rest API ? Maybe using job parameters ? You could also split the job in two and call the appropriate one (with or without timeout) using a conditional build step (mentioned above)... If you are int a pipeline you could use the timeout in a jenkins file. $URL$ let me know if any of this is helping you.