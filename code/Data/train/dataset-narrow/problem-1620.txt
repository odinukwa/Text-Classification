Your research is right. You cannot redirect with a CNAME because DNS does not work this way. You would need some sort of web server to accept the request and rewrite the URL to HTTPS. Some domain registrars allow you to do a redirect on their web server if you use their name servers. DreamHost may provide this service but I have never used them so I cannot say. 

This is not possible in the current OpsWorks system. Even if you could, it would require the server to have the OpsWorks agent and Chef to be installed, which is handled already in the OpsWorks AMIs provided by Amazon. A bit late, but I thought I'd give it an answer. 

You can also do various regular expressions so you can avoid from having to add a new one each time a new 'perma-link' is added. 

If it's not set before it reaches this location block, then it will set it to a blank string. You can just as easily add a string between the quotes. I do not get any errors when doing a configuration test using this. Please let me know if you're seeing otherwise. 

Providing that you have loaded your private key on your client, then it sounds like this might be a permissions issue on the 'git' user home directory and .ssh directory. Please try changing your /home/git directory to a mask of 0711: 

Rather than having a location block for each redirect, you can always just add rewrite rules into an existing location block: 

If you change your custom cookbooks, you need to update the cookbooks on each instance. You can do this under the "Deployments" area by clicking "Run Command", selecting "Update Custom Cookbooks" from the drop-down and then pressing the "Update Custom Cookbooks" button. You can usually leave all instances checked, unless you don't want to update one for some reason. 

Marionette may be able to help you out here, and possibly simplify your life with creating other network configurations for testing as well. 

I'm setting up some new Ubuntu servers, and I'd like to secure the data on them against theft. The threat model is attackers desiring the hardware or rather na√Øve attackers desiring the data. 

One of the basic principles of computer security is never to run anything you don't need. I was ping for a process today when I noticed that my Ubuntu 9.04 (desktop) machine was running a git server daemon. After a quick swear, I discovered that the package had been (probably inadvertently) installed, and removing it got rid of that process (and ensured that it wouldn't be restarted later). But in other cases, I want the server package to be installed, but don't want the server daemon running. For example, I use for internal testing (it's started by specific test scripts for some applications, and listens only on localhost in those configurations) but I don't want it listening for outside connections with some random config file. (If I wanted to run one listening for outside connections, I'd configure and run it myself.) I really don't like running all sorts of random servers I don't need on Internet-exposed machines, since who knows what security holes they open up. And I prefer not to have to muck about with firewalls, since that's yet another potential source of errors and misconfigurations that can open up security holes. It's not so hard to have Unix machines configured not to start any servers unless specifically asked to do so by the admin; NetBSD (and OpenBSD, too, I think) come this way by default. How do I configure my Ubuntu systems never to start any kind of server daemon unless I specifically tell it I want it started? (Asking to have a package installed is not, in my book, asking to start a server. If it is supposed to be, it's a terrible user interface, since many package installs don't even have a server to start, so it makes it far too easy to inadvertently start a server without realizing you've done so.) EDIT: Just to make it clear, the problem is not that I want to be able to stop existing servers. The problem is that I don't want new servers started without an explicit request. This means I should be able to do any sysadmin task, such as installing a package, and be confident that no servers have started. Most responses do not address this point. 

It's probably not running because it's unable to find the binary. Without any error log though, that's just an assumption I'm making. Try adjusting the part in your script to use its full path. I'm not sure how you installed (either via yum or manually), but you can find out its full path by running . 

The Amazon free tier is only valid for 12 months since creation of your account and it has quite strict guidelines in terms of what you can do - if you've stuck by these and still being charged, it's best to contact Amazon billing and ask them why. It really depends what instances you're running, what AMI you've used to launch the instances, how much disk space you're using, how much I/O activity you have going on the servers, how much bandwidth you're using and if you're using other Amazon services. Without being able to see the activity statement, it's hard to analyse it. Usually if you give the Amazon billing department an e-mail, they will be able to help you out quite well. If you have anything more specific about the usage though, I'm happy to help you answer it. 

Below is a sample configuration for the HAProxy frontend and backends which I just wrote up. This is not the full configuration file, so please use the /etc/haproxy/haproxy.cfg as a guide to having a complete configuration. 

I recently setup a Git server where I work which required SSH and HTTP(S) access for public and private repositories. I first setup the entire thing manually which took some time, but then I came across Gitlab which seemed to solve all my problems - it's pretty much Github, but your own private hosted version. I'd strongly recommend using Gitlab over setting everything up manually, given that it has so many great features and is easily manageable. There's a great tutorial here: $URL$ However, if you do want to run Git through HTTP(S), you will need to setup Apache or Nginx to use the git-http-backend binary which does all the work. There are plenty of tutorials online depending on which configuration you decide to go with. 

If your machine has multiple ethernet interfaces, you might have to specify the one to use to capture the traffic. If you find yourlself capturing no traffic, use to see the list of interfaces and their addresses, chose the appropriate one, and add the (or whatever interface) options to the command line. Once you have a pcap file, you need to analyze it. For interactive use, Wireshark is an excellent tool. Copy the pcap file to your workstation and run . You can use the filters to find the packets that have "Host: some.server.of.interest.com" headers; those are the ones for the virtual host of interest. Analyze / Follow TCP Stream will show you the entire HTTP conversation in a very readable format. If you want to do automated analysis, you'll probably have to write some custom code. One option would be to use a tool such as tcpflow to dump all of the HTTP sessions to files, and then use a scripting language (or even grep) to select the files of interest and analyze them. It's also not difficult to read pcap files directly, but doing re-assembly of the TCP conversations is a lot more work. 

We've got a couple of dozen Ubuntu systems (ranging from 8.04 to 9.40, both desktops and servers with only serial console access on which we'd like to run a patched version of glibc. In particular, this is to fix the inability of glibc's resolver to allow programs such as the ssh client from OpenSSH to set the AD bit; the general gist of what we'd like to do is available at the blog post How to get OpenSSH to see DNSSEC AD flags on SSHFP lookups with glibc. We're contemplating doing a slightly more more extensive modification, adding AD bit support to the header file so that we can compile a standard ssh binary, though that would involve replacing the ssh client as well. So my question is, what's the easiest way to make the change, maintain it in sync with regular updates from Ubuntu, and distribute it to all of our machines? I'm guessing that the optimum thing might be to maintain our own copy of the Ubuntu source packages, updating ours with every update, building new versions as appropriate, and distributing them by setting up a server that goes in the list of package servers for each machine. But I don't know enough of the details of how to do this to know if this is the best option. If it is, I'd like some pointers to the details of setting this up. Some will probably be wondering why I would like to do this. There are two reasons: 

I do not believe this is possible as the name servers stored in the domain name do not have any given priority. The client will use the name server that it is given. 

I just tested this and it looks like the GitLab API response is using pagination. According to the documentation ($URL$ the default number of results per page is set to 20 and the starting page is 1. To adjust the maximum results per page, you need to use the variable in the HTTP request line. You can change the page number by using as well, if you have more repositories than the maximum value of . You can specify a maximum value of 100. For example, you request may look like: 

This seems to work without any issues for my servers with multiple network interfaces. If you then restart your network service or interfaces, you should be able to check the routes to see that this is actually working: 

You can't map a port on a public IP address to more than one private IP address, it just won't work because how is the router supposed to know which one to go to? If you're using name-based virtual hosts, you could achieve this by sitting a HAProxy instance in-front of the web instances and direct all traffic from the router to the HAProxy instance. On the HAProxy instance, you create a front-end and specify the domains and the backend to use. Then, depending on which domain is accessed via HTTP, it forwards the request to the appropriate back-end to serve the request. I do it all the time when I want to conserve server or IP resources. 

Using , as dlawson pointed out, sounds like an excellent idea to me. This allows me to run a script once a minute that ensures everything but http and ssh is shut down: 

You get a message along the lines of , in which case you have the URL to your repository wrong. Find out where your repository is really located. It will have in it files and directories such as , , , and . Your URL is not for the root of your repo, but is within the repo (this will be the case if the version numbers in the log are not sequentially decreasing). Drop the last element from the end of the path in the URL and try again. 

(Note: this was posted before the question poster clarified that by "looking in [the repo]" he was not looking at the files stored in the repo, but at files outside the repo, those being the svn database files.) on the root of the repo (change the URL as appropriate) will give you all of the changes to the repo, from most recent working backwards, listing the files that were added, deleted and modified. Using this, you can figure out if someone moved or deleted the files for which you're looking. If that doesn't work, you have one of two problems: 

So what are my options for setting this up? As I said before, I'd prefer to have everything (aside from perhaps a small boot partition that does not contain /etc) encrypted, so that I don't have to worry about where I'm putting files, or where they're accidentally landing. We're running Ubuntu 9.04, if it makes any difference. 

By default, when you create an account in WHM, it will configure local mail accounts and it will set the local DNS to use the local MX records. However, if you do not wish to rely on the local DNS for the MX records, then you can force WHM to always use a remote mail exchanger. In WHM, goto: DNS Functions -> Edit DNS Zone -> (select domain name) At the bottom, you will see something called Email Routing. Set this to Remote Mail Exchanger and then ensure the DNS server is restarted once saved. 

Based on your location blocks, files ending with '.php' will only be passed through PHP. So if you have an actual file in the '/images' directory, it should serve it as a static file, or otherwise return a 404 error. If the file ends with '.php' and is in the '/images' directory, it will run through both location blocks in order. Should it do otherwise? 

The and variables are not required as they have default values, so you do not need to include either if you don't want to. Hopefully this solves your problem. 

Replace /home/git with whatever your home directory for the 'git' user is, if it was different in the tutorial. If it's not permissions, then please let comment and we'll see what else might be the issue. 

We did a lot of Moodle performance optimisations where I used to work and used PHP Xdebug profiling to help out with a lot of it. By enabling profiling, it will produce a *.cachegrind file which is then readable by many readers - I use QCacheGrind on Windows, or KCacheGrind on Linux. You should then be able to pinpoint the function which is taking its time. From memory, it is probably the amount of course data that it's trying to load when the user logs in. If there is too much data, and it's grabbing all of the data from the SQL server rather than only whats necessary, this could slow things down quite a bit. This can also be identified using the slow log as mentioned in a comment above, providing that its the SQL query that's causing the performance issue. I would also definitely advise upgrading your Moodle instance to something newer as I know there were a lot of performance fixes pushed across to the upstream repository since Moodle 2.2.3. We used to push these over to Moodle as an official Moodle partner.