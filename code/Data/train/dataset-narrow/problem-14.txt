As a python shop, you will most likely be comfortable with either SaltStack or Ansible and would want to be sure to install The Foreman with support for your chosen configuration management software. This software comes with a REST API that can be leveraged to trigger vm deployments automatically from a build pipeline (Eg, Jenkins, Bamboo, TravisCI, CircleCI, etc) or you could have the VM trigger testing after configuration is complete (or both). 

This is a great question as some of the terminology can be very confusing - particularly surrounding the (unfortunate) use of "SFTP" to describe multiple similar technologies. Will the real SFTP please stand up This comes mainly from the fact, that RFC 913 defines one SFTP protocol, the "Simple File Transfer Protocol" which offers no encryption and went largely unadopted, with users favoring TFTP and FTP depending on their needed application. Since the FTP protocol, defined in RFC 1738 in 1985 is completely encrypted and packet sniffers are able to dumping plaintext passwords from this protocol, Some thought that it would be a good idea to secure this protocol by implementing FTP over an SSL connection. In 1997 a mechanism for doing that was released in RFC 2228 which often was referred to as SFTP. 

You can do this a couple of ways, but I have never seen anyone do this with a CNAME. One option, if you have control over your IP addressing is to use BGP to fail-over (or load balance) an IP to your cloud hosting providers. A second option is to invest in a DNS-based geographic load balancing solution, such as the GTM from F5, the GSLB solution from A10, or the GSLB solution from Kemp (F5 is the most commonly used in the industry). With these solutions you will set your name server (NS1 and NS2) for your domain to be the GSLB solution of your choice. You will set a very low TTL for your DNS records and configure health monitors to your ELK stack from the GSLB solution. These can be as complex as logging into ELK and scraping for data or as simple as making sure you can establish a TCP connection and/or ping your server. When this health monitor fails, the GSLB solution will failover traffic to the alternate data center automagically. You will be able to do this as an HA pair with one GSLB solution at once DC as NS1 and the other at your secondary site as NS2. The GSLB will then forward lookup to your previous nameservers which will publish IP addresses for both sites. These will be rewritten to eliminate the IP that the GSLB does not want to direct traffic to at the time (again, this can be used for active-active sites - you can even use geolocation to send users in Europe to a Eurpoean cloud provider and US based users to a US based provider, or do a simple active/standby) A final way to do this might be with local load balancing solution, such as F5's LTM, A10's SLB, or Kemp's solution. You may also be able to use nginx for this for a cheap solution. In this scenario, you might have this solution onsite in an office with the URL to the load balancer served up by your local DNS server to a local IP (a Virtual IP) on your load balancer. You would then configure the load balance to direct users to the desired cloud provider by configuring a health monitor (of whatever complexity). You would then need to configure this load balancer to bridge your SSL connection. You would add your certificate and private key to the load balancing solution. Users would establish an encrypted connection to the load balancer. This encryption would be stripped off by the load balancer and a new encrypted connection established to the cloud provider by the load balancer, and the data forwarded along over the new SSL tunnel. 

If you know the root password, typically you can type "su" to switch to a super-user. You can then edit your sudoers file to fix sudo. 

In terms of development procedure, you should consider using the agile method. This is the most widely used method of software development and has proven itself. Within the scope of the agile method Pair programming will allow you to spool up another developer to assist with your projects when you are ready and find that you have more work than you can handle. You can then move into a standard agile method like scrum/kanban/scrumban (or continue the pair programming if you find you like it!) One particular reason for choosing this method is that is lends itself well to DevOps and Continuous Integration. In fact, one could describe DevOps as a specific way of implementing the agile method. The Iterative, incremental and evolutionary philosophy of agile and the very short feedback loop and adaptation cycle marry beautifully with DevOps/Continuous Integration and provide a solid foundation to build upon. 

This can change depending on how your environment is architected. For example, if you run all of your data off of NFS mounts, the Storage layer would sit beside compute instead of behind it. If you have a C++ based application that workstations connect to, you might not have a front-end layer. If your application uses flat files, you might not have a database layer. If you do not use virtual machines or containers and use local storage, you might not have a compute and storage layer. You will want to monitor the above 4 basic resources at every layer. This represents the basic monitoring that can be given by your vendors, software and hardware. Covering these four basic resources at every layer should be your first goal. Once 100% coverage is achieved, You can additionally build hooks into your application to report additional health statuses, but by their nature, these will typically be built as a reaction to outages and you would have to work with your internal developers to build these kinds of hooks. Monitoring these 4 basic resources should catch probably 80% of issues that cause outages however, and then you can start working on the remaining 20%. 

One consideration might be lock-in. For example, Amazon's cloud can become expensive very quickly - either because compute ends up being more than you estimated or there is always the possibility that Amazon decides to raise the cost of their compute price. This could cause you to want to change cloud providers to a cheaper one or perhaps you scale to a point at which it makes more financial sense to insource. Another scenario could be that Amazon's cloud somehow becomes breached. For example, the Spectre and Meltdown exploits are particularly nasty on hypervisors and offer some possibility and prospect of breaking out of a guest into the hypervisor or gaining sensitive information from another tenant on the hypervisor. These hypothetical and potential scenarios might or might not become realized and may or may not be realistic but nevertheless by choosing something that is easily exported to another cloud allows you to remain agile and easily deploy into another cloud or to insource. This guards against these above scenarios or any others that could arise. There is nothing more important in this regard than your configuration management system - especially if you are using an [immutable architecture] and/or a cattle-not-pets philosophy as this allows you to simply begin spinning up new compute instances in the new cloud or cluster immediately. You needn't migrate virtual machines. (and hopefully you don't have a large amount of data to export). So that is to say: Perhaps it would be wise to avoid AWS stacks so you aren't locked into this particular vendor.