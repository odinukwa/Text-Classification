The posed question is really a hard one since most people have no idea what computer scientists in general do. This is very different from other disciplines. I like to use the following analogy: (T)CS is for computers what physics is for CD-Players (i.e. the laser). This actually works quite well because most people have an idea of what a physicist deals with, be it correct or not. More specific examples include those things most people can relate to 

As Peter Shor mentions in a comment, the number of full binary trees with $n+1$ leaves is given by the $n$-th Catalan number $C_n$. It is known that the Catalan numbers have generating function $C(x) = \frac{1-\sqrt{1-4x}}{2x}$. On the other hand, it is known (Chomsky, Schützenberger; 1963) that regular languages have rational generating functions. Clearly, $C$ is not rational. This implies that there is no regular encoding of full binary trees where each tree has a unique representation. (Note that the generating function of arbitrary binary trees is not rational, either.) 

For all alphabet sizes greater than $1$, runtimes only change by a constant factor since $\log_k(x) \in \Theta(\log_l(x))$ for all $k, l > 1$. Elaboration: In $t$ timesteps, the assumed Turing machine can process at most $t$ positions/bits. Bits are taken from a $k$-nary alphabet, wlog $\{0,1,\dots,k-1\}$. Create a new Turing machine by replacing every transition by $\lceil \log_2(k) \rceil$ transitions; every old bit is encoded by $\lceil \log_2(k) \rceil$ bits in $\{0,1\}$ (blanks are reserved to mark unused cells). Note this is essentially binary coded digits. Obviously, the resulting Turing machine executes at most $\lceil \log_2(k) \rceil \cdot t \in \mathcal{O}(t)$ steps. Addition: Above argumentation breaks because operations that overwrite an input symbol with a bit not in $\{0,1\}$ can not be translated directly; the input has to be shifted. This can be amended by translating the original input before starting computation (essentially padding); this can be done in time $\mathcal{O}(n^2)$, resulting in a total runtime of $\mathcal{O}(n^2) + \lceil \log_2(k) \rceil \cdot t$. Consequently, using only two symbols for encoding intermediate results is of no asymptotic impact if $t(n) \in \Omega(n^2)$, but preprocessing dominates faster algorithms. Since most interesting functions are in $\Omega(n^2)$ (e.g. adding two numbers), one might consider the problem negligable. 

If you want to have definite code (i.e. little to no math, close to real programming) you might want to consider to have code that actually compiles. This has several advantages: 

What machine model do you want to use? On RM, both are equally hard if you assume that $+$ and $<=$ are equally expensive (common assumption). On 1-TM, sum is slightly more expensive since we have to add all bits compared to comparing only the the most signifikant bits (up to the first difference). 

Especially in TCS/maths, the good old blackboard can be of good use for proofs and examples. If there is none available or you have to project or record, try software implementations such as Lecturnity. On the hardware part, get some pointing device as using your hand is bad style. Some people use sticks, others lasers. Note that there are wireless (via USB dongle) gadgets that combine a laser pointer with keys to navigate a slides. I think those are very useful since you do not have to move to your PC for every slide change. Last but not least, you need a timer/alarm-clock you can easily and inconspicuously read off to check your time. 

Now we observe the following implications: $\begin{align*}  M(x) \downarrow \quad &\Rightarrow \exists n_0 \in \mathbb{N} : M \text{ halts on } x \text{ after at most } n_0 \text{ steps} \\ &\Rightarrow \forall y : n \geq n_0 \Rightarrow M^*(y) \text{ executes } n^2 \text{ arbitrary steps} \\ &\Rightarrow T_{M^*}(n) \in \mathcal{O}(n^2) \end{align*}$ and $\begin{align*}  M(x) \uparrow \quad &\Rightarrow \forall n \in \mathbb{N} : M \text{ does not halt on } x \text{ in less than } n \text{ steps} \\ &\Rightarrow \forall y : M^*(y) \text{ executes } n^3 \text{ arbitrary steps} \\ &\Rightarrow T_{M^*}(n) \in \Omega(n^3) \end{align*}$ Therefore, $H(M,x) \Leftrightarrow P(M^*,2)$. Assuming $P$ was algorithmicaly decidable, so would be $H$, which yields a contradiction. $\square$ 

On a more abstract level, encode events as symbols in your trace, e.g. . As far as I know, this is standard technique when modeling (possibly labeled) transition systems. This enables you to easily reason about properties your system might have, for instance statements (in temporal logic) like "After any timeout, my machine is in state ". It does not yield a good model for implementations, though. 

I assume that by "best starting room" you mean that it minimizes the sum of shortest walking distances from a fixed room to all the others. Model your floor as undirected graph. Each room is a node. Nodes are connected if and only if there is a direct physical, walkable connection between them (such as a door). You can model hallways as a series of sufficiently small rooms, such as one piece per door. If you want, you can even model actual walking distances (center nodes in their rooms) by edge weights. Once you have done this perform a SSSP (single source shortest path) algorithm for every non-hallway node. Choose the starting node that minimises the sum of distances found to all non-hallway nodes. Note that this solution does not account for "saving" distance when visiting several rooms in a row. Such scenarios can be solved with the same model, but would become unpleasantly close to TSP (traveling salesman), but for your input sizes things should still work out. (I am minimising the wrong thing, aren't I?) 

Two times no. First, most HPLs are not context free. While they usually have syntax based on a CFG, they also have what people call static semantics (which is also often included in the term syntax). This can include names and types which have to check out for a correct program. For instance, 

Interpretation vs. compilation issues aside, memory IO is indeed a big issue (maybe the issue) in my experience, in particular caches. When writing a program in C or maybe even C++, you (can) control what ends up where in memory. You can not in languages like Java. This is fine for many purposes, but consider the following. Imagine you want to store a (huge) matrix of objects, say pairs of numbers. In C, you can just create an array of those. Iterating over the matrix is about as fast as if the matrix contained single numbers. Now, in Java, pairs would be objects which would end up somewhere on the heap, the matrix containing only pointers. If the JVM is bad at recognising and acting on demand for locality¹, i.e. the object are scattered over the heap, this means cache becomes useless if you iterate. The problem tends to explode if you have parallel algorithms, by the way. [1] I do not know what it actually does in this regard, but my experience indicates it is not too clever about it. 

which is injective since both (in general) and (by induction hypothesis) are injective. This can be generalized to honor types (i.e. ) by enumerating types (known at compile-time), storing objects' types in a designated member and using above scheme. That might even work for nasty things like generics. Note that finite lists can be encoded with this scheme. Streams are problematic, of course. 

In a course I got to know Isabelle, a "generic proof assisstant". It supports (total) functional programming (close to ML) and higher order logic. You can define yourself (or find) languages for LTS and LTL and prove theorems on those. I do not know if this qualifies as easy, but it certainly works. 

Regarding your first question, I thought of Quicksort first but that should be obvious. There is a string matching algorithm (Nebel, 2006) that uses probabilistic ideas. I do know wether this is the fastest approach existing, though, and you apparently need some samples for training. 

Donald Knuth's The Art of Computer Programming has algorithms for generating many common distributions in volume two, chapter three. 

As far as I know type correctness tends to be undecidable for interesting cases so clearly formal grammars cannot capture every type system you can think of. I know that major compiler generators allow arbitrary predicates for rules that prevent a rule from being executed if the predicate does not evaluate to , e.g. . This concept can easily be formalized; appropriate restrictions on the allowed predicates then can yield decidability by LBAs. Deciding such predicates is at least difficult with respect to runtime so I guess people tend to perform an iterative parsing, collecting information in phase $k$ in order to check feasibility in phase $k+1$. For instance, you can build up a name table in one pass and then check that all used variables are (visibly) declared in a second run. 

I honestly think there is no "natural" notion for computiational hardness; it is inherently a creature of human minds. Maybe a simple analogon can help to understand polynomial growth. Give your students (virtually) the following task: With a sensor that can scan one cubic centimetre, find a cent piece. How long does it take you if you have to search 

First off: If your are interested in real-time collaborative editing, try something like gobby. It lets you literally edit a document at the same time. As for revision systems, I am only familiar with SVN. This is what you do, after installing subversion, of course: 

Springer's International Journal of Parallel Programming offers authors the option to publish openly. They offer online first and open access articles, both with feeds available. (I guess they have other journals with the same, but I have not checked.)