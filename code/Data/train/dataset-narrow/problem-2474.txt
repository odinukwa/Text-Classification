The idea for this answer came from a discussion with vzn about sufficiently focused questions. The motivation to spend time on graph isomorphism at all also came from vzn's repeated prodding. J.-E. Pin asked in the comment whether there are any specific reasons to consider inverse semigroups. The idea was to have a structure slightly generalizing groups, which is GI complete. I wanted to better understand the relation between group isomorphism and graph isomorphism, but I fear this answer doesn't provide any insight of this sort. 

Obviously $S^*$ is in NL, if $S$ is in NL. One guesses the separation between the first part and the rest and check that the first part is in $S$, then one guesses the next separation and so on. Since DLOGSPACE is a subclass of NL, it is sufficient to find a $S$ in DLOGSPACE for which $S^*$ is NL-complete, as suggested by Mikhail Rudoy in his comment. A suitable variant of DAG-reachability can be used as NL-complete problem here, and a suitable encoding of single-level DAG reachability as language $S$ in DLOGSPACE. The encoding of single-level DAG reachability goes as follows: The number of leading zero specifies the node of the input layer, then comes a description of the single-level DAG by the number of input nodes, the number of output nodes, and the connections between those nodes. The trailing number of zeros specifies the node of the output layer, where the complementary scheme "number of output nodes" - "number of trailing zeros" is used. The variant of DAG-reachability which reduces to $S^*$ is the one with a given separation into levels such that connections only exist between consecutive levels. We now only look at the words for which the number of output nodes matches the number of input nodes of the next single-level DAG, and where the number of zeros between two descriptions also matches that number. The information about the splitting of the input word into the different parts now provides the nondeterministic guess for how to traverse that DAG. 

A compiler can not convert into because function equality in turing-complete languages is undecidable. My question is: is there a system strong enough to express both functions, that is also weak enough to a compiler can make the conversion? Or, even better, is there a simple system where the mere reduction of would fall into ? 

An answer to the traveling salesman (and similar) problems can be easily verified on light lambda-calculi. Also, if I understand correctly, the light lambda-calculi can compute every polinomial-time computable function. That way, if one can prove that the traveling salesman problem can't be encoded on the light lambda-calculi, that would also prove the problem can't be solved in poly-time, which would also prove P!=NP. Is that correct, or am I confusing some concepts? 

This reads like a claim that $f$ is a homogeneous polynomial iff $f=0$ is a union of hypersurfaces (through the origin). But $f(x,y,z)=x^2+y^2-z^2$ is a homogeneous polynomial for which $f=0$ is not a finite union of hypersurfaces. 

because a (semi-)lattice is also an (idempotent commutative) inverse semigroup. Proof of theorem from technical report: 

It might be possible to keep the "$\mathbb{N} \to \mathbb{N}$" part, and replace the "(partial) functions" part with something else (I am thinking here of an analogy Fermions <-> "(partial) functions", Bosons <-> "something else"), but the Church-Turing thesis would probably still hold in such modified settings. 

It should be clear that $\mathbf{AC}^0$, $\mathbf{TC}^0$, and $\mathbf{NC}^1$ are models of parallel computation, especially in their non-uniform version as implemented in actual hardware. I wrote about dreams of fast solutions in the sense of allowing software the same parallel capabilities like hardware already has today. I am still working through all the material referenced there, but at least I have worked through Algebra, Logic and Complexity in Celebration of Eric Allender and Mike Saks by Neil Immerman. The last slides describe a typical place where $\mathbf{AC}^0$ occurs, namely all "natural" NP complete problem stay complete under $\mathbf{AC}^0$ reductions, and all such problems are actually $\mathbf{AC}^0$ isomorphic. Also true for $\mathbf{NC}^1$, $\mathbf{sAC}^1$, L, NL, P, PSPACE, etc (complete problems). Edit (in response to Kaveh): The original statement of that theorem can be found in Reducing the Complexity of Reductions by M. Agrawala, E. Allender, R. Impagliazzo, T. Pitassi, S. Rudich from 2001: 

There is a structural result for odd-hole-free graphs by Michele Conforti, Gérard Cornuéjols, and Kristina Vušković: 

Many thanks to Hsueh-I Lu, my teacher in National Taiwan University, who provides the following solution. It turns out that the answer is rather simple; in the initialization of a data structure, we don't have to construct the structure by the queries and operations it supports. Of course, we cannot perform queries during the construction like we usually do if we build the tree by link operation; but in this separator decomposition application we don't need it. That is to say, in the application to the separator decomposition, when we have to construct the link-cut trees L(T) for the spanning tree T on input graph G, instead of computing the structures and values of the trees L(T) by link operation, we directly compute the heavy-light path decomposition of the tree T, and construct the corresponding binary tree structure for each paths, and gluing the binary trees according to the path decomposition. All the steps can be done in linear time. For each necessary values on the nodes of L(T), since each value can be compute from T in linear time (in this application), instead of assigning values on binary tree structures and updating when gluing the trees, we assign the precomputed values directly onto the nodes of the final link-cut trees L(T). There are only a constant type of values need to be assigned, and again this can be done in linear time. 

I recently read a paper that presented a proof calculus where the verification of whether a given proof is valid was NL-complete. The authors apparently decided that the checking procedure was not local enough, and defined a modified notion of proof where (practically speaking) the execution trace of the dynamic programming algorithm doing the NL-complete checking in polytime was part of the proof. That modified notion of proof could now obviously be verified by very local checks. I was both negatively shocked and positively surprised by this: 

Minimum Dominating Set is not an isomorphism problem, hence there is no reason why it should be expected to be reducible to GI. 

Emil Jeřábek's remark "(or even uniform $\mathbf{AC}^0$)" seems less relevant, because $\mathbf{AC}^0$ is easily separated from $L$ (or even uniform $\mathbf{NC}^1$). Because the resulting formula actually depends only very locally on the input for the $PSPACE$-Turing machine, one could even claim that $TQBF$ is $PSPACE$-complete under $L$-uniform $\mathbf{NC}^0$ (instead of uniform $\mathbf{AC}^0$). But I find this misleading, because the circuit doesn't really do any non-trivial work at all, it basically just writes down a template. (The reduction writes down $n^k$ times $\exists c_{1.5}\forall (c_3, c_4) \in \{(c_1, c_{1.5}), (c_{1.5}, c_2)\}$ with systematically varying indices, where each $c_i$ are actually $n^k$ Boolean variables. This entire part only depends on the length of the input, hence the corresponding output variables are hardwired to constants. The final formula could be written as $\forall z. (z_1=i_i\land\dots\land z_n=i_n)\land \phi(x,y,z)$ such that only $i_i,\dots,i_n$ depends on the actual input, and such that this is the only place where $i_i,\dots,i_n$ appear. So this is nearly trivially an $\mathbf{NC}^0$, the entire work has been done by the $L$-algorithm writing down the circuit, whose outputs are nearly all directly hardwired to constant.) 

A class of lambda terms can be evaluated using Lamping's abstract algorithm - that is, converting them to interaction nets and applying a set of rules. In order to get the result, you have to read back lambda terms from normalized interaction nets. For example, this net: 

Is it possible, from the configuration of the net alone, to infer the tags and thus readback the same lambda term without them? 

is a closed term on the shape . is on the shape , where is the only variable. and have no lambdas within applications. 

Now, suppose we take the elementary affine logic and extend it with dependent types and . It is easy to prove that language is consistent and normalizing as a consequence of EAL. It can, I believe, express all the type families from Coq, with lambda-encodings and since it has . A simple expression of recursive algorithms is possible with church-encodings for iteration and scott-encodings for matching. And it is obviously simple, as it only has , and . Moreover, such language would have the very comforting property that it can be reduced optimally using the abstract fragment of Lamping's algorithm. All things considered, such language looks like a perfect candidate for the role of a small functional core that serves as an universal code-interchange format, as proposed by Gabriel. Is my suspicion correct, or is there any problem with this reasoning? 

he prove that $\mathsf{BPL} \subseteq \mathsf{ZP^*L}$, which is rather surprising because it seems that two-way access may be stronger than the one-way access. 

There is a close connection between sub-exponential time solvability (SUBEPT) and fixed parameter tractability (FPT). The link between them is provided in the following paper. 

In brief, they introduced a notion called miniaturization mapping, which maps a parameterized problem $(P,\nu)$ into another parameterized problem $(Q,\kappa)$. By viewing a normal problem as a problem parameterized by the input size, we have the following connection. (See theorem 16 in the paper) 

For the first question, it is open whether finding a tree decomposition for planar graphs can be done in polynomial time. The best approximation algorithm may be the RatCatcher algorithm by Seymour and Thomas, which computes the branchwidth of the planar graph, so it has a 1.5 approximation ratio by the relation between branchwidth and treewidth. For the second one, we have the following theorem about the treewidth of $k \times k$ grids: