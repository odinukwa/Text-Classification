Consider the following graph problem. We are given a graph $\mathcal{G} = (\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ is the set of vertices and $\mathcal{E}$ is the set of edges. For each vertex $V \in \mathcal{V}$, there is a weight $W(V)$. For a clique $Q=(Q_V,Q_E)$ where $Q_v \subset \mathcal{V}$ and $Q_E \subset \mathcal{E}$, the weight of clique $Q$ is defined as $W(Q) = \max_{V \in Q_V}W(V)$. Now, we want to find a set of cliques covering all vertices $\mathcal{V}$ such that the sum of weights of these cliques are minimum. In this problem NP-hard? I think since the graphs are general, when the weights are equal, then the problem become minimum clique cover problem in which we want to cover the graph $\mathcal{G}$ with minimum number of cliques. Since this problem is NP-hard, the problem with arbitrary weights is also NP-hard. Am I right? How can we find an approximation algorithm for this problem? I think the most trivial algorithm that comes to mind is this but I don't know how good it is. Sort the vertices based on their weights in decreasing order (the first one has the highest weight) and if two have the same weight we sort them based on the number of edges of them. Assume the sorted list of vertices is $V_1, \ldots, V_{10}$. Now, we start from the clique $C = V_1$ and check the list $V_2, \ldots, V_{10}$ from left to right. For each element, we check whether adding $V_j$ to $S$ results in a clique or not. If yes, we form a clique of $S$ and $V_j$, and remove $V_j$ from the list. We continue this process (now for $S = \{V_1,V_j\}$) until we reach the end of list $V_2, \ldots, V_{10}$. Now we remove the vertices of set S, from $V_1, \ldots, V_{10}$, and repeat the above procedure for the first element of $V_1, \ldots, V_{10}$ minus the set $V$. 

Here is the sketch of a proof I know. Let us draw $s = \max\left(\frac{4M}{\varepsilon},\frac{c}{\varepsilon} \log\frac{1}{\delta}\right)$ samples from the unkown distribution (where $c$ isconstant), and feed them as input for the mistake-bounded algorithm. We can assume the learning algorithm is conservative, i.e., only changes its working hypothesis after an incorrect prediction has been made. Look at the sequence of $k \le M$ hypothesis produced by the learner; we claim that, with probability $1-\frac\delta2$, at least one of them has error $< \frac\varepsilon2$. Assume the contrary and observe that any hypothesis with error $\ge \frac\varepsilon 2$ will be found to be wrong within the next $\frac{2}\varepsilon \le \frac{s}{2M}$ random examples on average. Therefore the expected number of samples before the last hypothesis is produced is bounded by $s / 2$. By using a Chernoff-like bound for the sum of geometric random variables, we can bound the probability that a hypothesis with error $< \frac \varepsilon 2$ is not found within $s$ samples by $\delta / 2$ (for a suitable value of $c$). The rest is easy: we know that one of the hypothesis is good, so we just draw $O\left(\frac{1}{\varepsilon} \log{\frac{M}{\delta}}\right)$ additional samples, test each of them against this new sample set, and select the best one. By Chernoff bounds, it holds that with probability $1-\frac\delta 2$, the good one has empirical error $< \frac{3}{4} \varepsilon$ on this set, while any wrong hypothesis with error $> \varepsilon$ will have empirical error $> \frac{3}{4} \varepsilon$. All in all, a hypothesis with error less than $\varepsilon$ will be selected with probability $1-\delta$. My guess is that the bound is optimal, but I haven't found a reference for this. 

If my analysis is right, this is a reversible Markov chain that eventually converges to a uniform distribution of legal sequences of coloured balls, so if you run this chain for long enough, you will get very close to this uniform distribution. How can you tell when this has converged? I would suggest watching the entropy of this sequence, and stop when it stops increasing. How do you compute the entropy? There are two main terms in the entropy calculation: the distribution of run lengths, and the sequence of colors each run has. For the distribution of run lengths, assume that there are $n_{i,k}$ runs of colour $i$ with length $k$. The contribution of these to the entropy is $$ \sum_i \ \log_2 \ { \sum_k n_{i,k} \choose n_{i,1}\ n_{i,2}\ \ldots \ n_{i,r} }, $$ where $r$ is the maximum permissible length of a run. Now, let us consider the contribution of the colour sequence to the entropy. Suppose there are $m_{i,j}$ places where a run of colour $i$ is immediately followed by one of colour $j$ (so $m_{i,i}=0$). The contribution of this to the entropy is $$ \sum_i \ \log_2\ { \sum_j m_{i,j} \choose m_{i,1}\ m_{i,2}\ \ldots \ m_{i,c} }, $$ where $c$ is the number of colours. (In the interest of accuracy, let me note that we are leaving out a number of contributions to the entropy, including the colour of the first ball, but these are lower order terms which should be safe to neglect.) UPDATE: There should be ways of speeding this up. I believe that for steps c and d, you can use analysis to perform both these steps over all runs of one color at once. For steps a and b, this is equivalent to the question of finding a random sequence of colored balls with the constraint that no two balls of the same color touch. There should be some good way of doing the mixing for this problem. Then you just have to alternate a/b steps with c/d steps, where each step mixes over those two moves completely. I think this should converge quite fast, although I don't have any rigorous analysis for this Markov chain. 

Intuitively, the theorem says that a line is not a finite union of points, a plane is not a finite union of lines, etc. The simplest proof is to observe, for example, that a finite union of lines has zero area, whereas a plane does not. More concretely, observe that it is enough to prove the claim for manifolds on $\mathbb{R}^n$ by passing to their closures. Consider an affine manifold $M\subseteq \mathbb{Q}^n$ given by the set of solutions to the linear system $A x = b$; its closure will be precisely the set of solutions to the same system over $\mathbb{R}^n$, hence this step does not affect the dimension of the manifolds involved. Also, the closure of a finite union equals the union of the closures. Now note that the $d$-dimensional Lebesgue measure of a manifold of dimension $\le d - 1$ is null. Therefore the $d$-dimensional Lebesgue measure of a finite union of such manifolds is still zero. But the $d$-dimensional measure of an $d$-dimensional manifold is infinite, hence non-zero. As for your second question, I'm not quite sure what you mean. But if the base field $\mathbb{F}$ is finite, then any $d$-dimensional affine manifold over $\mathbb{F}^n$ contains $|\mathbb{F}|^d$ points. So by a similar counting argument, you need at least $|\mathbb{F}|^d/|\mathbb{F}|^{d-1}=|\mathbb{F}|$ affine spaces of dimension $\le d - 1$ to cover an affine space of dimension $d$. 

This was originally a comment, but it got too long. If you look at DEFLATE, what is being compressed by Huffman is the output of LZ77; LZ77 works by (when this takes fewer bits than the raw data) sending an pointer earlier into the string being compressed, and a match length that tells how many symbols to take after the pointer. The theory shows that, even without additional compression, this technique eventually converges to the source entropy. However, in data compression, any time you have a distribution that isn't completely random, you might as well compress it. There's no reason to believe that the output of LZ77—the pointers and the match lengths—are completely random. They have to converge to complete randomness in the asymptotic limit, since LZ77 is asymptotically optimal, but in practice you only use a finite dictionary, so they presumably stay far enough away from being completely random that you win by doing further compression on them. Naturally, you use one Huffman code for the pointers and another for the match lengths, since these two processes have different statistics. Why use Huffman rather than LZ for the second round of compression? The big advantage LZ has over Huffman is in treating dependencies between symbols. In English, if one letter is a 'q', the next is extremely likely to be a 'u', and so on. If the symbols are independent events, then Huffman is simpler and works just as well or better for short strings. For the output of LZ77, my intuition is that the symbols should be fairly independent, so Huffman should work better. 

It's not "obtained", but rather the bound the authors want on $\mathrm{Prob}[|u_1|\ge s]$. The Chernoff inequality says how large $s$ needs to be in order to guarantee the desired upper bound. As they assume $d \le n$, it suffices for $s$ to satisfy $s^2 \cdot d/2≥\ln(20n^2)$, which leads to $s=c\cdot d^{-1/2} \sqrt{\log n}$ for some appropriately chosen constant c. 

There seems to be a typo; I assume you mean to find $u \in \{0,1\}^n$ which is not the sum of $(\log n)^{O(1)}$ vectors among $v_1,\dots, v_m$ (not $n$). It's not clear to me if any constant in $(\log n)^{O(1)}$ works for you. If you can settle for sums of less than $\log m$ vectors maybe there's something to be done. But If you want this quantity to be $(\log m)^{1+\delta}$, then I think it is quite hard (I have been working on this problem for a long time). Still you may be interested to know that this is an instance of the Remote Point Problem of Alon, Panigrahy and Yekhanin ("Deterministic Approximation Algorithms for the Nearest Codeword Problem ") for certain parameters. Let $m > n$ and $v_1,\dots,v_m$ be the columns of the parity check matrix of a linear code in $\{0,1\}^m$ of dimension $d = m - n$ (if this matrix didn't have full rank, the problem would be trivial). Then your problem is equivalent to finding $u \in \{0,1\}^n$ that is $(\log n)^{O(1)}$-far from the code. This setting of parameters, where the dimension is very close to m, is not studied in the paper. However, they can only achive remoteness $\log m$ up to dimension $d = cm$ for some constant $c$. In fact, I don't think we know of any polynomial-sized certificate that allows us to prove that some vector is more than $\omega(\log m)$-far from a space of dimension $\Omega(m)$, let alone find it. Another connection is with learning parities in the mistake-bound model. If one can efficiently learn $(\log n)^{O(1)}$-parities (defined on ${0,1}^m$) with mistake bound strictly less than $n$, then one can set arbitrary values to the first $n - 1$ bits of $u$ and ``force a mistake'' on the last bit by setting it to the opposite value to that predicted by the learner. This seems much stronger though. The problem is also related to separating EXP from certain reductions to sparse sets. 

Consider the following graph problem. For a number $K$ and a set $\mathcal{K} = \{ 1, \ldots,K\}$, Vetices: we have a set of vertices $V_{k,s}$ for all $s \subseteq \mathcal{K} \setminus \{k\}$ for all $k \in \mathcal{K}$. The weight of vertex $V_{k,s}$ is denoted by $v_{k,s}$ where all the wieghts are non-negative. Note this graph has $Q_K = K \times (2^{K-1}-1)$ vertices. Edges: There is an edge between two vertices $V_{k,s}$ and $V_{l,t}$, if $k \in t$ and $l \in s$. Goal: Now that we have described how our graphs look like for a given number $K$. The goal is to find a set of cliques covering all vertices such that the sum of weights of these cliques are minimum. Weight of Cliques: For each clique (complete subgraph) of our graph, the weight is defined as the maximum of the weights of the vertices of the clique. More specifically, if vertices $X_1, \ldots, X_k$ form a clique where $x_i$ is the weight of vertex $X_i$, then the weight of this clique is $\max\{x_1,\ldots,x_k \}$. Now, considering $Q_K$ and the weights of vertices as the input, I want to show that this problem is NP-complete. The first step is to show that this problem lies in NP. I think this is easy to show that there exists a linear certificate for the decision problem of our problem. (Given a solution, checks to see whether the components are actually cliques, they cover all vertices, and their total weights are lower than a specific number). However, for the reduction from a known NP-complete, I don't have any idea. Can anyone help me with this? 

If you're asking whether a quantum computer can compute any function that a classical computer can compute without using many more elementary computational steps, then the answer is yes: a quantum computer can perform any reversible classical computation, and if you keep the input around, any classical computation can be made reversible at a cost of multiplying the number of steps by a small constant factor. If you're asking whether a quantum computer can compute any function that a classical computer can without using many more resources, the answer is much less clear. The construction that lets you make a $T$-step computation reversible using $O(T)$ steps also takes $O(T)$ space (i.e., memory cells). You can achieve a smaller blow-up in space at the cost of a superlinear number of steps. See Time/Space Trade-Offs for Reversible Computation by Charles H. Bennett. For an actual physical quantum computer, it's very likely that you might also be able to make it faithfully simulate a classical computer by letting it lose coherence, but in this case it's no longer really working as a quantum computer, and if you try to use a quantum computer that is operating this way as a subroutine in a quantum computation, it might not work properly. 

Mergesort satisfies all three requirements (when merging is performed in place). See Pardo, L.T., "Stable sorting and merging with optimal space and time bounds", SIAM J. Comput. 6 (1977), 351-372. 

Are there any known natural examples of optimization problems for which it is much easier to produce an optimal solution than to evaluate the quality of a given candidate solution? For the sake of concreteness, we may consider polynomial-time solvable optimization problems of the form: "given x, minimize $f(x, y)$", where $f:\{0,1\}^*\times\{0,1\}^* \to \mathbb{N}$ is, say, #P-hard. Such problems clearly exist (for instance, we could have $f(x, 0) = 0$ for all $x$ even if $f$ is uncomputable), but I am looking for ``natural'' problems exhibiting this phenomenon. 

The paper "Random low-degree polynomials are hard to approximate" by Ben-Eliezer, Hod, and Lovett answers your question. They show strong bounds on the correlation of random polynomials of degree $d$ with polynomials of degree at most $d-1$, by analyzing the bias of random polynomials. See their Lemma 2: the bias of a random degree-$d$ polynomial (up to some $d$ that is linear in $n$) is at most $2^{-\Omega(n / d)}$, except with probability $2^{-\Omega\Big(\binom{n}{\le d}\Big)}$. 

In 2013, when P$\neq$NP has been a Clay prize problem for a dozen years, it may seem difficult to believe that any mathematicians actually had such attitudes; however, I can personally vouch that some did. Strassen continues by saying that we should not give up looking for a proof of P$\neq$NP (thus indirectly implying that it is indeed a mathematical conjecture): 

Implementing algorithms well is a skill which takes a different set of tools than just proving theorems. Many algorithms which were discovered by the theory community have indeed been implemented in practice (although I would like to see the theory community take a bigger role in this process). Physics doesn't ask the same researchers to do theory and experiment, although it is expected that the two groups communicate. Why shouldn't you expect to see the same divide in computer science? ADDED IN EDIT: Expanding on my comment in answer to Suresh's about what I meant by "role" above, at Bell Labs and AT&T Labs, researchers in algorithms were encouraged to talk to people in development. I didn't do as much of this as I probably should have, but I did get at least one paper out of it, and I think it would be good for the field if there were more communication between people in theory at universities and practitioners. This doesn't mean that I think everybody who comes up with an algorithm should code it (even if it's practical). On the other hand, coding algorithms (or having a student code them) that you think might be practical can be useful in getting them adapted by practitioners. Consider one example. Lempel and Ziv wrote two technical papers in 1977 and 1978 on new data compression algorithms. Everybody ignored them. In 1984, Welch wrote a much less technical paper giving a slight twist on LZ78 that improved its performance somewhat, and gave the results of a small study comparing its performance with other data compression methods. It was published in a journal read by a number of programmers, and the algorithm was given by a few lines of pseudocode. The method was quickly adapted in a number of places, eventually resulting in an infamous intellectual property dispute. Of course, one of the best ways for algorithms researchers to communicate with practice is to produce grad students who go off and work at Google, IBM, or other companies, and we're already doing that. Another way might be to answer practitioner's questions in this forum. Hopefully, we're doing a reasonable job of that as well.