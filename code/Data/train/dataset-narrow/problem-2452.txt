Suppose I have some set of points in d-dimensional space, each with some mass. Our problem size will be the number of points in this set. After some roughly (within polylog factors) linear initialisation using roughly linear storage, I would like to know the total mass of all the points that fall within various queried subspaces in polylogarithmic time. If our queries are all axis-parallel boxes, ie sets of points defined by a cartesian product of intervals on each axis, we can apply some standard, easily-googleable range searching methods and achieve this without too much difficulty. If our queries are all simplices, there are currently no known methods which satisfy both these criteria, either we use polynomial space and initialisation to achieve a polylog query time, or we use roughly linear space and have sublinear, but not logarithmic, query times. What if our queries are all boxes, but each is rotated in some way? We can define such a box in a few ways, but for the sake of concreteness suppose I will give you a sequence of sets of hyperplanes, each with exactly one parallel partner, intersecting all others orthogonally, defining some boxey subspace between them. Is there a way of solving this slightly simpler problem with roughly linear initialisation and storage but polylogarithmic queries? Alternatively, if I were to give you a method of doing this, would you be able to use it to solve the simplex case of this problem in a similarly easy way? 

I've been reading about convex volume estimation, and have found the paper "Simulated Annealing in Convex Bodies and an $O^{*}(n^4)$ Volume Algorithm" by Lovasz and Vempala, which can be read here. The algorithm provided relies the standard multi-phase monte-carlo technique of producing a series of values, the first of which is easy to compute, the remainder estimatable by random variables, and then combining each of these variables in some way to produce an estimate of the volume. This particular technique involves extruding an $n$-dimensional shape into an $n+1$-dimensional pencil, and estimating the volume of that pencil. The pencil is defined as the intersection of a cylinder over the convex body and a cone. Let $K$ be the convex body, and $$ C = \{{\bf x} \in \mathbb{R}^{n+1} | x_0 > 0, \; \sum^n_{i=1}x_i^2 \leqslant x_0^2\} $$ The pencil, $K'$, is defined: $$ K' = ([0,2D] \times K) \cap C $$ By assumption, $K$ contains the unit ball, so the set of points in $K'$ with $x_0<1$ is exactly the $n+1$-dimensional hypercone with height $1$ and whose base is the $n$-ball of radius $1$. This cone is referred to as $C_B$. We also denote by $\pi_n$ the volume of the unit n-ball. The estimates of the volume of the pencil are provided using the following function: $$ Z(a) = \int_{K'} e^{-ax_0} d{\bf x} $$ For a sufficiently small value of $a$, $Z(a)$ can be shown to be a good estimate for the volume of $K'$. For $a \geqslant 2n$, it is claimed that $Z(a)$ is close to the above integral taken over the entire cone $C$. We know that $K' \subseteq C$ from its definition, hence $$ Z(a) = \int_{K'} e^{-ax_0} d{\bf x} \leqslant \int_{C} e^{-ax_0} d{\bf x} = \int_0^\infty e^{-at}t^n \pi_n dt = n! \pi_n a^{-(n+1)} $$ We know also that $C_B \subseteq K'$, hence $$ Z(a) \geqslant \int_{C_B} e^{-ax_0} d{\bf x} = \int_0^1 e^{-at}t^n \pi_n dt $$ At this point, I can no longer follow the paper's reasoning. It is stated that $$ \int_0^1 e^{-at}t^n \pi_n dt > (1-\varepsilon) \int_0^\infty e^{-at}t^n \pi_n dt $$ No bounds are specified on $\varepsilon$. Indeed, our estimation algorithm should be able to take an arbitrarily small value of $\varepsilon$. The integrand, hovever, is non-negative across its domain of integration, so it must be the case that $$ \int_0^1 e^{-at}t^n \pi_n dt \leqslant \int_0^\infty e^{-at}t^n \pi_n dt $$ Surely, then, the inequality in $\varepsilon$ would only hold for a sufficiently large value of $\varepsilon$, rather than for an arbitrarily small one. Further, stating that $a\geqslant 2n$ gives no information about a relationship between $\varepsilon$ and $a$. How is it even possible to reach the conclusion from the premise in this case? Further still, an independent talk on the algorithm found here suggests instead that the inequality only holds for $a>6n$, rather than $a>2n$. The paper states that this is true by standard computation, so I do feel that I must be missing something here, but I cannot tell what. I've emailed both of the authors at the email addresses I can find for them, and haven't received any response from them, so hopefully someone from here can help. 

The abstract machine would look like i.e. the untyped lambda calculus or the SKI combinator or just some kind of a LISP dialect. In hardware it would probably look like a biologic cell, calculating via pattern matching on the genes in the core – directly running that LISP dialect. (You might like to read this blog entry: "New computer language based on Lisp enables biological modeling") 

I have a special variant of BinPack problem. Does anyone know how to reduce this problem to something known? 

Of course, there exists a simpler proof by reducing this parity problem to that of the existance of an Hamiltonian path, but that proof would have been too easy. 

can trivially be designed to contain the longest of all paths that start at s. So, either the longest path of the whole graph is partially in A or in B. and can be created out of two arbitrary graphs with a longest path of unknown length like this: Starting with an arbitrary graph, 

each edge will be expanded to length 2, and each vertex will be connected to an additional "dangling" edge of length 2. 

Example 2: $I=[8,14,5]$ $B=[10,10,7]$ $allparts=[8,10,2,2,5]$ after partitioning the second item $usedparts=[8,2,10,5,2]$ because we need all to fill the bins $numparts=5$ 

One node s connects three components of a graph, and the longest path goes through that node s, like this: 

Example 1: $I=[100,5]$ $B=[10,10,7]$ $allparts=[10,10,7,73,5]$ after partitioning the first item $usedparts=[10,10,7]$ because we do not need the other parts anymore $numparts=3$ 

No, it is not possible to efficiently calculate the parity of the longest path of an arbitrary graph, because it is possible to design such a graph, where any calculation of the that parity implies the comparision of the lengths of longest paths of two or more sub-graphs. Consider this: 

Knowing the parity of the longest path of this resulting graph implies knowing whether the path ends in or . Iff the longest path of the resulting graph ends in , then the longest path of the original graph of has to be longer than the longest path of the original graph of . 

The longest path in this resulting graph is always even and will map to the longest path of the original graph. To "catch" one end of that longest path (in the resulting graph), each vertex will be directly connected to an additional "starting vector" s (or t). All longest of those paths starting at that specific vector are now odd. 

Two such constructed graphs ( and ) can now be combined with an edge between their individual starting vertices (s and t). One of those vertices will be connected to , so that the longest path of the resulting graph will have one end in and the other end in either or . 

Iff there would be an efficient algorithm to determine the parity of the longest path in a graph, then there would be an efficient algorithm to compare the lengths of the longest paths of two different graphs, and vice versa. 

The problem: There are items $I$ and bins $B$ in specific quantity and size. $|I| ∈ ℕ, |B| ∈ ℕ$ $s : (I ∪ B) → ℕ$ The sum of all item-sizes is not less than the sum of all bin-sizes. $∑ _{i∈I} s(i) ≥ ∑ _{b∈B} s(b)$ Each bin has to be filled with items or parts of items so that it is filled completely. $s(b,i)$ is the size of that part of $i$ that is in $b$, or $0$ iff not. $∀ b ∈ B, i ∈ I: s(b,i) ∈ ℕ ∪ \{0\}$ $∀ i ∈ I: ∑ _{b∈B} s(b,i) ≤ s(i)$ $∀ b ∈ B: ∑ _{i∈I} s(b,i) ≥ s(b)$ The goal is to minimize the number of item-parts used to fill all bins. $numparts = |\{ (b,i) ∈ B×I\ |\ s(b,i)>0 \}|$ $minimize\ numparts$ 

See the paper "A parallel approximation algorithm for positive linear programming." by Luby and Nisan. (Some kinds of) linear programs can be approximated in log^(O(1)) n time. 

The posting on MathOverflow tells how to go from a small number of independent Uniform[0,1] random variables to a larger number of pairwise-independent Uniform[0,1] random variables. You can of course go back and forth between Uniform[0,1] and Gaussian by inverting the CDF. But that requires numerical analysis as the CDF is not closed-form. However, there is a simpler way to from Gaussian to uniform. Given two independent Gaussians $X_1, X_2$, the angle $\arctan(X_1/X_2)$ is uniform in the range $[0,2 \pi]$. Similarly, the Box-Muller method transforms two independent Uniform[0,1] variables into two independent Gaussian random variables. Using these two transformations, you consume two Gaussians to produce a uniform or two uniforms to produce a Gaussian. So there is only a factor of $O(1)$ in the sampling efficiency. Furthermore, no inversion of the Normal cdf is required. 

I am trying to find about algorithms that, given graph $H, G$, determine if $H$ is a topological minor of $G$ (and if so, exhibit this explicitly). Most of the literature on this topic seems to be focused on the case when $H$ is a fixed small graph while $G$ goes to infinity --- e.g. testing if $G$ is planar. What are the algorithms when $H$ and $G$ are comparable in size? If this problem requires exponential time, I would still be interested in the best algorithms and/or real implementations 

The following fact seems to be used implicitly in cs theory, particularly algorithms. Given a RAM machine $M$ running in time $O(f(n))$, another RAM machine $M'$ can simulate $M$ in time $O(f(n))$. This differs from the case for Turing machines, where $M'$ may require $O(f(n) log(f(n))$ time. I say this is often used implicitly because many papers will simply say something like "run $M$, but keep track of certain auxiliarily information as you do so". This is really simulating $M$, but for RAM machines the distinction is not so important because running times are not (asymptotically) affected. Is there a reference for this theorem? I am summarizing the situation correctly? 

I have been having a great deal of difficulty finding a reference that gives simple and straightforward explanation of the following: Suppose we have $n$ random variables $Y_1, \dots, Y_n$, each of $b$-bits long. (I.e. with values in $\{0, \dots, 2^b-1 \}$). We want a probability space where each $Y_i$ is unbiased (takes on each value with probability exactly $2^{-b}$), and has $k$-independence. That is, for any $i_1 < \dots < i_k$ and any $y_1, \dots, y_k$ we have $$ P(Y_{i_1} = y_1 \wedge \dots \wedge Y_{i_k} = y_k) = 2^{-k b} $$ When $b = 1$ you can always get a probability space of size $n^{k}$ and sometimes you can get $n^{k/2}$ -- is there any clear statement about when these are possible? Can someone point me to references about what happens when $b > 1$? Thanks 

Seemingly, giving $\rho(\pi(1))$ the distribution on the minimum of $n$ uniform variables, and so on, would work. Is there are any reference? 

Another typical case of $NPI$ problem is when there is a witness of length $\omega(\log n)$ but smaller than $n^{O(1)}$. The problem of the existence of a clique of size $\log n$ in a graph is a typical example -- in this case, the witness (the specific clique) requires $O(\log^2 n)$ bits. Assuming the Exponential Time Hypothesis, such a problem is easier than an $NP$-complete problem (which requires time $\exp(n^{O(1)})$) but harder than a polynomial time problem.