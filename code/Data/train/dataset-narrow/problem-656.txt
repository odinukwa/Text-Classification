partitioning - not help from bad queries and can make situation more bad split operations - write on master, all read (especial users) - on slave on master optimise structure for insert update - only necessary indexes, which really need for improve updates On slave(s) - indexes for select operations query must reduce number of rows as much as possible - depend from logic, but it user_id, date, location and etc - need analyse queries and indexes cardinality then good idea to ask - for what reason in table with very simple structure: (phone_number,start_time,stop_time,time_lasting,fare) 

would be fast, just because first layer of filters return You less than 100 rows this logic was successfully used with 10M+ rows tables 

This is due to MySQL Master and Slave version mismatch The message below is clear, and confirms the master/slaves run different versions. The variable is not supported with but is supported with 

We have some huge tables in MySQL database, We have already archived/deleted data older than 2012 in 2013, Now we need to archive/delete data older than 2013 means we need to archive data for year 2012 without application downtime. 

You have chain replication as Master--> SLAVE A--> SLAVE B (Slave A is Slave of Master and Slave B is slave of Slave A) As you told On Slave Server(A) binary log is enabled. 

You are missing column name of parent table to which your child refer, while creating child table, do like below 

this is just my variant, what better 2 FULL scan (as You can see total number of scanned rows bigger) or file sort at the final possible decide only on real data (depend from size of sorted data) 

First of all - think Your server not crash, but just stoped (rebooted) by VPS provider - they normally have script which restart server in case of loading. in Your case You can check - all of Yours tables InnoDB (if not - convert) and add --single-transaction option 

source - $URL$ and not a lot of ways for optimise queries over it there are few recommendations (very standard for any queries) $URL$ recommendations is really standard and in Your case - with count(*) not help. 

In order to change we should also change variables. The definition of : The number of seconds the server waits for activity on a noninteractive connection before closing it. On thread startup, the session wait_timeout value is initialized from the global wait_timeout value or from the global interactive_timeout value. Here is the test Query without changing any parameter , all the values are set to 8 hours by default 

Alias inner as something different than as it is good practice to not use it as alias , As i have used . 

Whatever you are trying to do can easily be done by USING ON DELETE CASCADE AND ON UPDATE CASCADE(IF Required) While creating the foreign keys on your table. For your reference have a look at FOREIGN KEY Constraints. 

The simple answer - You need Galera or NDB cluster This is very simple form of answer, because new problems could come after this. Some often "next" questions from projects - all was fine on my old server, but now my UPDATE of 5000000 records - always fail! Or - I have 3 nodes, but my insert start work much slower, help me please! I just every few minutes run SELECT GROUP BY over 500 000 rows - but it was perfect work before. If high safety - it mandatory, cluster + replication (or any other solution for reporting part of application - such as AWS Redshift and etc) - $URL$ The best way - install and test! 

UPDATE What will be the estimated upper bound (Max size of table) with 1 million records and how can we estimate it. 

As you need to perform a read,search operation faster you can use the MyISAM as the table engine if the table will not have the High write's in future. When you will use the MyISAM Engine for fast read/search for this table ,you need to set the Key_Buffer_size to some appropriate value depending upon the index size of your MyISAM tables and also the amount of available RAM. As the Table having two columns id and domain name, so you can use the FULL TEXT index on domain name so it will make searching fast on the column domain name. Look the structure which I have Created 

It return You information about which indexes from present MySQL will use when run query Than You will check cardinality of the indexes 

looking for Your data - Your columns is VARCHAR, but not date, this is make query non working with any non readable character in data by run UPDATES: 

About Variant B - I personally look for tools similar pt-table-sync as for excellent but occasional tools in Your toolbox, and not for permanent solution. I can not explain drawbacks, it just personal vision Variant C: Tools like - SymmetricDS Variant D: I personally actively work now with CDC variants, when You parse bin-logs and ship them to subscriber over message queue. I have tested under loading at least 2 working solution (really it more for now): 

What I understand from your question I am answering on that basis. If you want database dump use client utility.You can Execute it as 

If you face the above error set to some higher value. Otherwise in my opinion increasing it to some higher value will not allow you to quickly load a 700MB file. 

In my opinion your sql file contains the statement And have not the permissions to create that database.So generate the dump without the create database statement. Or comment it out in your sql file use for commenting a line in sql file. 

I have to investigate a MySQL Production server,While investigating I found a query in which was taking more than 400 second in state, I tried to find the query in slow log but i was unable to find the query in slow log. 

Possible or not increase speed of insert? it depends from Your current server settings and configuration - may yes, may be not Also it depend from other server loading: 

I not test pre-calculated columns on 1B rows tables, but use it on 50M+ work fine, so it must be tested 

just not forget in this case You must manually parse situations when parent_id show to 1, because You not use cascade 

now much more similar tools present on market, many of them free in parts cover 99% of life cases. And ETL it more often solution rather than work with build functions only. 

You can use several ETL tools for this, it could be on premies, or cloud based, final decision always depends from cost, corporate standards and many other moments good starting point as most close to Your request: 

this will create the TableName.txt file and TableName.sql files at the location as you specified with --tab option where TableName.txt is your CSV you can rename it as TableName.csv. Note : use the (--tab="PATH") path where the mysql has write permissions. for various other options of mysqldump. please see.. 

There is no constraint like You can load a GBs file. Depending on the size of the rows, you'll probably exceed this.One way would be to split it up into several chunks. This is probably a good idea anyway. LOAD DATA isn't subject to the limit. If you are adjusting your (default is 1M). Set it as 

Initially we will have same table structure and data on slave as well , but assume you created a on column on slave, and someone inserted id value as 2 on master on master the insertion would be successful but it will fail on slave as value 2 already exists there and the column is primary key indexed. So the insertion will fail and replication will break.