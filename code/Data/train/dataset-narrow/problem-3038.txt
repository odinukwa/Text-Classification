The classifier doesn't matter as much in this case as the underlying algorithm's ability to handle sparse matrices. It's also worth noting that you can handle sparse boolean features by mapping them to arbitrary numeric indices and running them through a tree based algorithm (e.g., Random Forest or Gradient Boosting). In practice, this seems to work well. 

This is a hard problem and researchers are making a lot of progress. If you're looking for supervised feature selection, I'd recommend LASSO and its variants. Evaluation of the algorithm is very straightforward with supervised learning: the performance of whichever metric you choose on test data. Two major caveats of LASSO are that (1) the selected features will not automatically detect an interaction, so you have to craft all of your features a priori (i.e., before running them through the model) and (2) LASSO will not identify non-linear relationships (e.g., a quadratic relationship). A way to try and get past these two caveats is to use Gradient Boosted Machines which does feature selection automatically. It's worth noting the statistical properties of GBM are a little more ambiguous than that of the LASSO. If you're looking for unsupervised feature selection, it seems there's a similar regularization approach used by these researchers, but evaluation in this particular case becomes less obvious. People try a lot of different things like PCA/SVD or K-Means which ultimately will try to find a linear approximation to the data. In that case, the typical measures of performance are the reconstruction error or the RMSE of the clusters. In terms of software, R and Python both have GBM, LASSO, K-Means, SVD, and PCA. GLMNET and XGBoost in R and Sklearn for Python are the relevant libraries. 

As @Spacedman said, the paper provides a very clear explanation of the algorithm on page 2. If you're a little less comfortable with the math of the notation, here's the intuition/explanation in words. 

In this simulation, we see that the performance of the model slightly worse, but fairly similar, from including the clusters. 

As the link by @Spacedman shows, binomial regression works well. If you don't have the attempts and successes but, instead, have just the proportion, then you'll want to use beta-regression. After a little digging, it doesn't seem like this is available in Python. Here's a blog post demoing it in R. 

This is a really interesting problem! Like most really interesting problems, you're unlikely to find an out-of-box solution for this, but I think the field of graph/subgraph similarity has some promise here. I'll go into more detail, but, at a high level, I think you can view players' paths during a play as a collection of five traversals through x,y space, with a vertex being any x,y point you have available (presumably there's some level of time granularity here), and an edge describing a player's movement from one point in x,y space to the next over time. It should be possible to cluster your data using a similarity metric (e.g., see Koutra et al., 2011 for a nice overview). Then, using your own domain expertise, you should be able to identify whether the clusters you've derived have some real-world meaning in basketball. 

I've done something like you're describing using mongoDB--I think you'll best use your time using some sort of NoSQL approach, rather than creating a specialized one-off solution. If you're using Python, I've had excellent experiences using PyMongo to handle reads and writes from within my code. I would strongly caution you against adopting your approach #1. This could break very easily in the future, and there are databases designed to handle your exact problem! 

It might also be worth looking into optimizing your analytics pipeline. For example, is file I/O, feature generation and feature extraction part of the the workflow you're using, or have you done all of that off-line and are just concerned with the SVM evaluation part? 

Separate out $10\%$ of your data for hold-out evaluation. Divide the remaining $90\%$ into $5\%-10\%$ parameter optimization, and the remainder for final parameter eval. Compare the feature distributions of the parameter optimization and final parameter eval data sets, if they're not comparable, redraw the samples (within the context of the $90\%$ development sample) When you have a good subsample, run your parameter optimization experiments on the small parameter optimization data set. With your final parameter settings, evaluate with cross-validation on the the $90\%$ combined parameter optimization and final parameter eval data sets. Perform final model analysis by training on the $90\%$ data set and classifying on the $10\%$ hold-out evaluation set. 

It is usually called reshaping! For a great description of the process, see this walkthrough, or read up on Hadley Wickham's documentation for the package! 

What is your output? Is it a 0-1? If so, you shouldn't be using RMSE and should be using cross-entropy as your loss function. 

Intuitively, you're using KNNs to define a network graph and assessing how similar the features are according to your distance metric. This is just as good of a measure of feature importance as any other but will also has its pitfalls, just like all of the others. 

AUC is based on rank order of your predictions, not the actual class to which it's assigned. It's very likely that the scale of the output is misbehaving. Look at the values of your predictions, I suspect that the predictions of your model are within a tight range. If that's the case, the argmax will yield the same class for all of your observations (which is what's happening). You may wish to tinker with some of the hyperparameters to see which one is causing this exactly (might start with the learning rate). It's probably worth testing if a logistic regression gives you the same problem, which will help identify whether or not it's a problem with your inputs/features. 

Make a k-nearest neighbor's graph. That is, for each observation, define an edge in the Graph for that observation if another observation is one of its k-nearest neighbor's. If you're using a supervised algorithm you can define an edge if they share the same label. If any two nodes (observations) are connected, define a weight matrix S measuring the similarity between those two nodes (using some distance measure). For each feature define the Laplacian graph. Compute the Laplacian score based on their equation. 

Mapping your inputs into a sparse matrix and using logistic regression with an $\ell_1$ penalty should work well. In R this is very straight forward with the glmnet and matrix libraries. Additionally, if you have boolean features with a low frequency, you can filter some of them by taking a subset with at least some number of observations. Here's a quick simulation in R of a lasso logistic regression with sparse features. 

It does not become biased, at least not in the statistical sense. Depending on the clustering mechanism, the information may be redundant. If you're using regression with the RMSE as the loss function and the squared-error as your loss metric for your clustering algorithm, then the information will work out to be redundant. Regardless, it's harmless to throw into the model and test this. The LASSO should learn that this information is redundant. Here's a simulation in R. 

Blast. It looks as though ARIMAX is currently a work in progress for the cloudera/spark-timeseries community. Although some work has been done on the model development side, it doesn't appear to have been merged in. 

Cloudera recently added the spark-time series library to github. According to the user docs, it definitely can fit autoregressive integrated moving average (ARIMA) models, but I see no mention of ARIMAX, which takes into account explanatory variables. Does anyone with more experience with this library know whether ARIMAX is possible in the library's current implementation? Mathematically, I understand the difference between ARIMA and ARIMAX, and I know extending it myself wouldn't be terribly difficult, but I'm right now looking for an off-the-shelf solution in spark. Can anyone recommend an alternative spark implementation? 

Data Science is an interdisciplinary field generally thought to combine classical statistics, machine learning, and computer science (again, this depends on who you ask, but other might include business intelligence here, and possible information visualization or knowledge discovery as well; for example, the wikipedia article on data science). A good data scientist is also skilled at picking up on the domain-specific characteristics of the domain in which they working, as well. For example, a data scientist working on analytics for hospital records is much more effective if they have a background in Biomedical Informatics. There are many options here, depending on the type of analytics you're interested in. Andrew Ng's coursera course is the first resource mentioned by most, and rightly so. If you're interested in machine learning, that's a great starting place. If you want an in-depth exploration of the mathematics involved, Tibshirani's The Elements of Statistical Learning is excellent, but fairly advanced text. There are many online courses available on coursera in addition to Ng's, but you should select them with a mind for the type of analytics you want to focus on, and/or the domain in which you plan on working. Kaggle. Start with kaggle, if you want to dive in on some real-world analytics problems. Depending on your level of expertise, it might be good to start of simpler, though. Project Euler is a great resource for one-off practice problems that I still use as warm-up work. Again, this probably depends on the domain you wish to work in. However, I know Coursera offers a data science certificate, if you complete a series of data science-related courses. This is probably a good place to start. 

You should use dummy variables and then you can toss it directly into K-means. If you have a lot of categories the efficient way to do this is through a one-hot-encoding (sparse-encoding). Here's a little demo using clustering and then using the clusters in a regression model. In general, you should avoid doing that but it's illuminating in this case. 

What others have said is accurate, that you need to build a regression model of some sort. Depending on the scale of the duration of your task, you will have to model it slightly differently. In fact, there's an entire class of models that try to predict duration. These are called survival models. Here's a Python library on survival analysis. But these models are fairly academic. A typical hack is to use Gamma, Poison, or Log-Normal regression which works out nicely because these models predict non-negative values. 

Yes, this is classic sentiment analysis. You'll need training data in order to write this program. Typically, people will build a language modeling using the text data in the article, tweets, posts, etc. Most folks specify the language model using a Recurrent Neural Network with a Long Short Term Memory. Here's a blog post that has code linked to their github. Before diving into a more advanced deep neural network like an LSTM, it may be easier to start with a multilayer perceptron using n-grams. It seems like you may be asking for a couple of different things. So, let me be very explicit. The models I listed above take as input the articles and a label that evaluated the articles as either positive or negative sentiment, the model then returns the likelihood of a given article being positive or negative sentiment. It in no way returns a topic. To return the "topic" your best bet would be to run Latent Dirichlet Allocation, which would then return the likelihood of a given article being generated from a given topic that was previously learned. It's important to note that these will only be from topics learned on previous training data and not on new topics. There are standard algorithms available to visualize topic models (e.g., LDAVis) and the embeddings of language models (e.g., word2vec visualizations). It's worth noting that there is research being done in learning these items jointly (i.e., the latent topic and the classification of good/bad). I'd recommend reviewing this paper. It may be worth reviewing these items more thoroughly before actually diving into applying them. It sounds like you may not have a complete picture of what your goal is and how deep learning can help you.