Maximum weight matching on $G$ is equivalent to maximum weight independent set on line-graph of $G$, and can be written as follows $$\max_{\mathbf{x}} \prod_{ij \in E} f_{ij}(x_i,x_j)$$ Here $\mathbf{x}\in\{0,1\}^n$ is a vector of vertex occupations, $f_{ij}(x,y)$ returns 0 if x=y=1, 1 if x=y=0, otherwise weight of the node that's not 0. You can generalize by allowing other choices of $\mathbb{x}$ and $f$, for instance 

Tree decomposition is hard in the worst case but greedy method seems to be near-optimal on small real-life networks. 

I've tried the following LP relaxation of maximum independent set $$\max \sum_i x_i$$ $$\text{s.t.}\ x_i+x_j\le 1\ \forall (i,j)\in E$$ $$x_i\ge 0$$ I get $1/2$ for every variable for every cubic non-bipartite graph I tried. 

Suppose I want to find vertex subset $S$ of graph $G=(V,E)$ such that any simple closed walk that visits vertices both in $S$ and in $V\backslash S$ has length $\ge g$ The idea is to relax requirements of tree decomposition to get bag-graph of large girth instead of a tree, and form an approximation algorithm based on this decomposition. Does this come up in any literature? Clarification Bags in the context of tree decomposition are sets of nodes. Taking bags produced by tree decomposition algorithm and connecting any pair of bags that are not disjoint gives a tree. This trade-off idea is briefly mentioned in Koller's "Graphical Models" book where she calls it "Cluster Graph" and defines it similar to a tree decomposition except that the graph formed by connecting overlapping bags is not restricted to be a tree. In the context of probabilistic inference, this approach gives a way to trade off cost and accuracy, with exact tree decomposition-based approach as a special case. Since quality of approximation improves with girth of such graph while complexity increases with size of bags/clusters, a natural question is when it's possible to find a "(relaxed?) tree decomposition" where largest bag size is bounded above and girth is bounded below 

This was too long to be a comment -- I wonder if those matrices have structure that make them behave differently from random matrices. Products of random sparse matrices go to zero or become non-sparse quickly. Here's a simple experiment -- take 200 random binary 50x50 matrices, and plot number of non-zeros as a function of number of matrices multiplied. Plots below show standard deviation over 2000 runs. First plot is for 2% sparsity, second plot is for 3% $URL$ $URL$ this took 3 minutes on my laptop using standard matrix multiplication 

Is there a tractable way to define an approximate distribution over small vertex separators of the graph? I'm looking for something along the lines of [Bayati,2008], a way to turn a single "this is a separator" constraint into a product of several constraints that are easier to enforce. Small true separators may be impossible, in which case approximate separators would do (any cycle that goes through approximate separator must have length >k). Motivation is constructing a message-passing scheme without short feedback loops. 

What are obstacles to making SAT solvers competitive with specialized graph algorithms? In other words, is it feasible to expect SAT solvers that can replace the role of algorithm designer -- ie, be able to automatically recognize problem structure and then solve it as quickly as a specialized algorithm? Here some examples I think are challenging for today's SAT solvers: 

Can someone help me find some references for finding good execution schedule given memory constraint? Assuming computation graph is simple in some sense (ie, small tree-width) There is this reference about the problem being hard in general: "Inapproximability of Treewidth, One-Shot Pebbling, and Related Layout Problems", $URL$ but I'm interested in a practical algorithm 

Here's how you could come up with tree-width concept yourself. Suppose you want to count the number of independent sets in the following graph. $URL$ Independent sets can be partitioned into ones where top node is occupied, and ones where it is unoccupied $URL$ Now, notice that knowing whether top node is occupied, you can count number of independent sets in each subproblem separately, and multiply them. Repeating this process recursively gives you an algorithm to count independent sets based on graph separators. $URL$ Now, suppose you no longer have a tree. This means separators are bigger, but you can use the same idea. Consider counting independent sets in the following graph. $URL$ Use the same idea of breaking the problem into subproblems on the separator you get the following $URL$ Like in the previous example, each term in the sum decomposes into two smaller counting tasks across the separator. $URL$ $URL$ Note that we have more terms in the sum than in the previous example because we have to enumerate over all configurations on our separator, which can potentially grow exponentially with size of the separator (size 2 in this case). Tree decomposition is a data-structure to compactly store these recursive partitioning steps. Consider the following graph and its tree decomposition $URL$ $URL$ To count using this decomposition you'd first fix values in nodes 3,6 which breaks it into 2 subproblems. In the first subproblem you'd additionally fix node 5, which breaks its part into two smaller subparts. Size of the largest separator in an optimal recursive decomposition is precisely the tree width. For larger counting problems, size of the largest separator dominates the runtime, which is why this quantity is so important. As to the notion of tree-width measuring how close graph is to a tree, one way to make it intuitive is to look at the alternative derivation of tree decomposition -- from correspondence with chordal graphs. First triangulate the graph by traversing vertices in order and interconnecting all "higher ordered" neighbors of each vertex. $URL$ Then construct tree decomposition by taking maximal cliques and connecting them iff their intersection is a maximal separator. $URL$ Recursive separator and triangulation based approaches of constructing tree decomposition are equivalent. Tree width+1 is the size of the largest clique in optimal triangulation of the graph, or if the graph is already triangulated, just size of the largest clique. So in a sense, chordal graphs of treewidth tw can be thought of as trees where instead of single nodes we have overlapping cliques of size at most tw+1. Non-chordal graphs are such "clique trees" with some clique edges missing Here are some chordal graphs and their tree-width. $URL$ 

How about this one? $URL$ Update: Here are some if you restrict attention to connected 3-regular bridgeless graphs, from Mathematica's database $URL$ 

Let $\mathcal{X}= \{1,-1\}^n$, $E$ the set of edges and $J$ some real-valued matrix. Van der Waerden's theorem gives constant $c$ and set of edge weights such that $$\sum_\mathbf{x\in \mathcal{X}} \exp \sum_{ij \in E} J_{ij} x_i x_j = c \sum_{A\in C} f(A) $$ Where $C$ consists of Eulerian subgraphs over $E$, $f(A)$ is the weight of $A$ defined as the product of weights of edges in $A$ Edge weights are strictly below 1 in magnitude. Suppose only a small number of self-avoiding loops on the graph have non-negligible weight. We can approximate the sum by only considering Eulerian subgraphs including these loops. However, a small number of such loops can give a large number of Eulerian subgraphs. Is there a more efficient way? 

If you allow arbitrary non-negative $f$, this becomes the problem of finding the most likely setting of variables in a Gibbs random field with $f$ representing edge interaction potentials. Generalizing further to hypergraphs, your objective becomes $$\max_{\mathbf{x}} \prod_{e \in E} f_{e}(x_e)$$ Here $E$ is a set of hyper-edges (tuples of nodes), and $x_e$ is restriction of $x$ to nodes in hyperedge $e$. Example: 

Suppose we want to find $$\sum_x \prod_{ij \in E} f(x_i,x_j)$$ or $$\max_x \prod_{ij \in E} f(x_i,x_j)$$ Where max or sum is taken over all labelings of $V$, product is taken over all edges $E$ for a graph $G=\{V,E\}$ and $f$ is an arbitrary function. This quantity is easy to find for bounded tree width graphs and in general NP-hard for planar graphs. Number of proper colorings, maximum independent set and number of Eulerian subgraphs are special instances of the problem above. I'm interested in polynomial time approximation schemes for problems of this kind, especially for planar graphs. What graph decompositions would be useful? Edit 11/1: As an example, I'm wondering about decompositions that might be analogous to cluster expansions of statistical physics (ie, Mayer expansion). When $f$ represents weak interactions, such expansions converge, which means that you could achieve given accuracy with $k$ terms of the expansion regardless of the size of the graph. Wouldn't this imply existence of PTAS for the quantity? Update 02/11/2011 High temperature expansions rewrite partition function $Z$ as a sum of terms where higher order terms depend on higher order interactions. When "correlations decay", high order terms decay fast enough so that almost all of $Z$'s mass is contained in finite number of low-order terms. For instance for Ising model consider the following expression of its partition function $$Z=\sum_\mathbf{x\in \mathcal{X}} \exp J \sum_{ij \in E} x_i x_j = c \sum_{A\in C} (\tanh J)^{|A|} $$ Here $c$ a simple constant, $C$ is a set of Eulerian subgraphs of our graph, $|A|$ is number of edges in subgraph $A$. We have rewritten partition function as a sum over subgraphs where each term in the sum is exponentially penalized by size of the subgraph. Now group terms with same exponent together and approximate $Z$ by taking first $k$ terms. When number of Eulerian subgraphs of size $p$ doesn't grow too fast, the error of our approximation decays exponentially with $k$. Approximate counting is hard in general, but easy for "correlation decay" instances. For instance, in the case of Ising model, there's correlation decay when $f(k)$ grows slower than $(\tanh J)^k$ where $f(k)$ is the number of Eulerian subgraphs of size $k$. I believe in such case, truncating high temperature expansion gives a PTAS for $Z$ Another example is counting weighted independent sets -- it's tractable for any graph if the weight is low enough because you can make the problem exhibit correlation decay. The quantity is then approximated by counting independent sets in bounded size regions. I believe Dror Weitz' STOC'06 result implies that unweighted independent set counting is possible for any graph with maximum degree 4. I've found two families of "local" decompositions -- Bethe cluster graphs and Kikuchi region graphs. Bethe decomposition essentially tells you to multiply counts in regions, and divide by counts in region overlaps. Kikuchi region graph method improves on this by taking into account that region overlaps can themselves overlap, using "inclusion-exclusion" type of correction. Alternative approach is to decompose the problem into global tractable parts, like in, "Variational Inference over Combinatorial Spaces". However, local decompositions allow you to control approximation quality by selecting region size 

You can pick any other node as root and redirect nodes away from it to get Markov equivalent model. Markov equivalent means that two graphical models correspond to identical spaces of distributions. Let "unshielded collider" be structure of the form A->B<-C Theorem: Two directed acyclic graphs D1 and D2 are Markov equivalent if and only if D1 and D2 have the same vertex set, same adjacencies, and same unshielded colliders. See Ch.4 of Meek's "Related Graphical Frameworks: Undirected, Directed Acyclic and Chain Graph Models" $URL$ 

I just came across a relevant paper -- Kloks/Boedlander's "Only few graphs have bounded tree width". They show that almost all graphs with $n$ vertices and $\delta n$ edges have treewidth on the order of $n^\epsilon$, $\epsilon=\frac{\delta-1}{\delta+1}$. For instance $\delta=3$ means typical tree-width grows as $\sqrt{n}$ So even if greedy method found the best decomposition for all graphs, the resulting algorithm would still be intractably slow for typical graphs 

Implement automatic differentiation method from "Exact inference and learning for cumulative distribution functions on loopy graphs" paper. The idea is to use the tree decomposition of the expression graph to do symbolic differentiation exponentially faster than direct method. Their experiments compare tree-decomposition method with Mathematica's built-in differentiator, and they seem happy to provide code they used for experiments 

Is true for all connected cubic non-bipartite graphs? Is there LP relaxation which works better for such graphs? 

What are "easy regions" for satisfiability? In other words, sufficient conditions for some SAT solver to be able to find a satisfying assignment, assuming it exists. One example is when each clause shares variables with few other clauses, due to constructive proof of LLL, any other results along those lines? There's sizable literature on easy regions for Belief Propagation, is there something along those lines for satisfiability? 

Generalizing in another direction, suppose instead of a single maximum matching, you want to find $m$ highest weighted maximum matchings. This is a special instance of finding $k$ most probable explanations in a probabilistic model. The objective can be now written as $$sort_\mathbf{x} \prod_{e \in E} f_{e}(x_i,x_j)$$ See [Flerova,2010] for meaning of objective above. More generally, instead of sort,$\prod$ or $\max,\prod$ over reals, we can consider a general $(\cdot,+)$ commutative semiring where $\cdot$ and $+$ are abstract operations obeying associative and distributive law. The objective we get is now $$\bigoplus_x \bigotimes_e f_e(x)$$ Here, $\bigotimes$ is taken over all edges of some hypergraph $G$ over $n$ nodes, $\bigoplus$ is taken over $n$-tuples of values, each $f_e$ takes $x$'s to $E$ and $(\bigotimes,\bigoplus,E)$ form a commutative semi-ring Examples: 

Some more comments: A local algorithm for counting will compute the count from a set of per-node statistics where each statistic is a function of some graph neighborhood of the node. For colorings, those statistics are related to the "marginal probability of encountering color c". Here's an example of this reduction for a simple graph. It follows from Alan Sly's recent paper that counting independent sets using a local algorithm is as hard as counting independent sets using any algorithm. My suspicion that this is true for general counting on graphs. For local algorithms, hardness depends on how correlation between nodes behaves with respect to distance between nodes. For large enough distances, this correlation essentially has only two behaviors -- either correlation decays exponentially in graph distance, or it doesn't decay at all. If there's exponential decay, local statistics depend on a neighbourhood whose size is polynomial in size of the graph, so the problem of counting is easy. In statistical physics models it was noted (ie, de Gennes, Emery) that there's a connection between self-avoiding walks, correlation decay, and phase transitions. The point at which generating function for self-avoiding walks on a lattice becomes infinite corresponds to the temperature at which long-range correlations appear in the model. You can see from Weitz' self-avoiding walk tree construction why self-avoiding walks come up in correlation decay -- marginal can be represented exactly as a root of a tree of self-avoiding walks, so if the branching factor of this tree is small enough, leaves of the tree become irrelevant eventually. If "local hardness" implies hardness, then it's sufficient to quantify properties that determine the growth rate of self-avoiding walks. Exact growth rate can be extracted from the generating function for self-avoiding walks, but it is is intractable to compute. Spectral radius is easy to compute, and gives a lower bound. 

Suppose $Z_G(J,h)$ is a partition function of Ising model with coupling $J$ and magnetic field $h$ on graph $G$. What is the complexity of finding the gradient of Z at $\mathbf{0}$? Specifically, if $n$ is the number of vertices, $E$ is a set of edges, $\mathcal{X}=\{1,-1\}^n$, the gradient is of the following function $$Z(J,h)=\sum_{x\in \mathcal{X}} \exp\left(J \sum_{(i,j)\in E} x_i x_j + h \sum_i x_i\right)$$ 

Set is a vertex cover iff its complement is an independent set, therefore this problem is equivalent to counting independent sets. Algebraic counting of independent sets is FPT for graphs of bounded bounded clique-width. For instance, see Courcelle's "A multivariate interlace polynomial and its computation for graphs of bounded clique-width", where they compute a generalization of independence polynomial. Adding up coefficients of independence polynomial gives the number of independent sets. Graphs with maximum degree 3 can have unbounded clique-width. Numerical counting of independent sets is tractable when the problem exhibits "correlation decay". Dror Weitz (STOC'06) gives a deterministic FPTAS for counting weighted independent sets on graphs of maximum degree $d$ when the weight $\lambda$ is $$\lambda<\frac{(\Delta-1)^{\Delta-1}}{(\Delta-2)^\Delta}$$ $URL$ Regular (unweighted) independent set counting corresponds to $\lambda=1$ so his algorithm gives FPTAS for number of vertex covers on graphs of maximum degree 5. His algorithm is based on building a self-avoiding walk tree at each vertex, and truncating this tree at depth $d$. Branching factor of self-avoiding walk trees determines the range of $\lambda$ for which small depth $d$ gives a good approximation, and formula above is derived by using maximum degree of the graph to upper bound this branching factor. 

What's bringing all of these generalizations together is that the best known algorithm for specific instances of the problem above is often the same as the most general algorithm, sometimes called "Generalized distributive law" [Aji, 2000], which works in $O(1)$ time for bounded tree-width hypergraphs. This puts exact solution of the problems above in a unified framework, however such framework for approximate solution is lacking (and I want to hear about it if you think otherwise) 

Suppose I have a graph $G$, with nodes $\{1,2,\ldots,n\}$, subsets of nodes $\{b_1,\ldots,b_k\}$ and functions $\{f_1,\ldots,f_k\}$ where $f_i$ maps independent subsets of $b_i$ to reals. The operation on $f$ is setting or getting all values in "canonical order", ie, $\langle f(\emptyset),f(\{1\}),f(\{2\}),f(\{3\}),f(\{1,2\}),\ldots \rangle$. What is a way to do this more efficiently than with an array of size $2^{|b_i|}$ for $f_i$? This comes up when counting independent sets on graphs