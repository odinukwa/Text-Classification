Grouping a Primary Key is wasted time, since you get groups of one record only. Hence, the SUM function is unnecessary, you just need to get the column. But, the real problem you are trying to solve, is a bit unclear. If you ever have 2 tables in a database, with EXACTLY the same Primary Key, there is something wrong. Obviously, they should be 1 table only, and then you don't need elaborate UPDATE statements. You may need an UPDATE statement, but nothing like above. Explain us why you have identical Primary Keys in different tables. 

If you need best practices, not minimum required values, this is how to proceed : plan a meeting between the OS main administrator and the DB main administrator, and bring along the list of minumum requirements. If those people can't figure out "best practices", nobody can. 

I would move the old records (closed incidents) to another table, not to another partition in the same table. Why ? Because if there is a LOGICAL reason, in my mind it makes more sense to put them as much seperate as possible, on a physical level. Consider this : you can store incidents, but your data schema can also store "open incidents" and "closed incidents", as a separate component. Partitions are nice, but behold the fact that your closed incidents counts, will continue to grow. So, you can't say that when you create partitions, all partitions are behaving similarly. That on itself, looks wrong to me. But technically, it's possible. 

Long shot perhaps, but worth checking file \Program Files\Microsoft SQL Server\100\DTS\Binn\MsDtsSrvr.ini or the equivalent on your setup. You may have to manually edit it with the instance name. Otherwise SSIS connections might be looking for an msdb of a default SQL instance that doesn't exist. 

Unfortunately, as is the case with other counters, the definition of 'reads' is not identical across the board. If a plan has calls to UDFs, statistics IO may under-report them (or hide them completely) while profiler still displays them. Regarding the PLE counter, if the server has more than one (physical) NUMA nodes, it is important to use the ones under 'SQL Server: Buffer Node'. The PLE figure under 'SQL Server: Buffer Mgr' is an average of the Node PLEs, and can be hiding horrors sometimes (I've seen PLE of 400 on one node and 7000 on the other, with SQL supposedly using both nodes). 

Definately, exporting that many rows to a tool like Excel, sounds like the worst solution to what sounds like a normal action. This being : updating data. In the opening post, I don't see any reason why the data has to leave the Oracle database. Certainly not to a tool like Excel. 

The size needed in a temporary tablespace depends on the data volume manipulated, but including potentially other actions also requiring temporary space. What you should be doing is this : monitor temporary tablespace usage, (temporarily) expand the temporary tablespace. Maybe, you need to recreate your temp tablespace. That is, if there is corruption of some kind. One way - but not the only one - is stopping the database, rename the file(s) of the temporary tablespace, tail the alert.log, and startup the database. Obviously, don't do without backup, and don't test this on production. 

The answer is kind of "no" but more "your question does not have an answer because it's based upon incorrect assumptions". 1) Tables are not really stored in a particular order. 2) Even if they were, how would the DB know that the question had been answered until it had finished reading the entire table. Putting things "up front" would not help. 3) What you ask (range of values) is in the realm of an index. An index has more structure and roughly the concept of "beginning and end" so would be the way to go. Build an index on the values you intend to query. 4) The way indexes are structured means that individual rows can be selected from billions with just a handful of I/O reads. However you need to have the right values indexed to match the queries. Look at the plan to see if it's working for you. 

If performance is important, Option1 has the clear edge. Every time a query goes across a linked server, performance takes a hit as what you expect to behave like set-based operations become serialised by OLEDB. They go row-by-row. It helps if you follow best practices for linked server queries (OPENROWSET for example) and ensure all remote processing is done on the other side, but the results will still come across serialised. (Look for Conor Cunningham's presentation on Distributed Queries). If all your remote tables are small lookup tables this may not be a serious issue, but if millions of rows have to come across linked servers then performance will suffer. There used to be an extra layer of problems with invisible statistics (for data readers) over linked servers, but as you're running SQL 2014 this will not affect you. If you can afford the Dev cost of eliminating the linked servers, Just Do It! 

RMAN allows you to either try to fix the file (recover) or to recreate the file (restore). Before you give up (if ever), you should be at least trying both of these options. 

It COULD be simple, but then it's down to luck, more than to anything else. The way of working is very simple : get all processes listed of the main database, ignore the ones from the standby. Then, do the same with the standby database : collect all process info. What is important here, is that you capture this while all is normal. Then you compare both lists, in detail. Chances you find some process description active on one, but not on the other, is pretty high. There you go, differing both databases on OS level. It must be said, although you technically may be able to differ (as explained above), it's never a good sign if questions like that are asked. Either no permissions are granted, or knowledge is limited. Both are not good. The good way of differing both databases, is querying some V$ view. What I'm describing above, is an alternative method, but you should not rely too much on it. Why not ? Because when alternative situations emerge, the procedure may not work anymore. I'm thinking of; databases being started up and such, but you described already that things may not work if different database modes are used. 

The most comprehensive way in my view would be to encrypt/decrypt the database with TDE. This will ensure that each and every page will change in memory and will be flushed to disk. I've tried this with success on 'legacy' dbs that were originally created in SQL2000, after I discovered that several pages didn't have actual checksums on them (0x...200) if you look at the header with dbcc page. If you were to try this, I would recommend testing it on a restored version of the live db, just in case you have undetected corruption that could be caught and stall the encryption process. There are flags to deal with it, but better play it safe. Obviously you'll want to backup the certificate used by the encryption, so you are covered for any eventuality during the time the db is encrypted. If anyone has a better idea for writing checksums on all pages, I'd love to hear it :-) 

Every R (relational) DBMS I've used has. MySQL is peculiar in so many ways I struggle to put it head to head with any of the "normal" RDBMS for anything other than a basic data dump for a simple application. There are plenty of DBs that are not relational that don't have the concept. As far as I know though, there are none other of either sort that support the syntax without the functionality. 

If you're worried about not having SQL support, just wait until they realise you're hacking data into the DB. I'd put money on them refusing you any other support from that point on. That said, it sounds like somewhere there is a "max ID" table for all the other tables and you're hitting a problem there where its idea of "max ID" is wrong because you've taken it. Of course that's just a guess. Also, the max+1 is really the wrong approach both from you and the vendor. It causes contention if you can do it right and duplicate errors when you can't. The best way is sequences or identity.