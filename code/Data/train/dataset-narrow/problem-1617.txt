You can login to EC2 instance with the same access key you use for default user. Suppose, you are running Ubuntu and default username is . Then your public key is located in Create new user as described in AWS documentation and add public key to its . Now you should be able to login to you instance with original key pair. 

I ssh on remote host but terminal performance is poor. Symbols I am typing are not shown immediately, but with some delay. Sometimes two symbols are shown at one time after delay. 

I want to figure out some problems I faced performing the task. First, I had to mount .iso from Settings menu and put the tic "Connect on power on" - it was unchecked by default, and VM tried to load from net. Then, ESXi 4.0 has no clone fascility. You need to manually copy all the files from folder VM1 to VM2, cd VM2, right mouse click on .vmx file - 'Add to invertory'. Change the hostname and IP of the second VM - its all! 

I have VMWare ESXi 4.0 preinstalled on the server with IP A. I also can use IPs B and C. I have KVM access to this server and I can connect to it using vSphere Client. CentOS iso-image isn't on the server. My task is to create two virtual machines on this host and install CentOS 5.4 on both. Then I have to configure network, so guest1 is accessible on IP B and guest2 is accessible on IP C. Can anyone provide me a brief howto on this topic? I tried to find it, but found nothing. 

I can, however, not find a way reload these values with out rebooting. I have read that the values are reloaded when logging in; it works when I do but it does not work through . I have the pam_limits.so in /etc/pam.d: 

So and are clearly busy taking up 64% and 53% of a CPU respectively, but not near 100%. The chunk size (128k) of the RAID was chosen after measuring which chunksize gave the least CPU penalty. If this speed is normal: What is the limiting factor? Can I measure that? If this speed is not normal: How can I find the limiting factor? Can I change that? 

But I only seem to get the basename of the file and not the full path to the file. And since a lot of my files are called the same (but in different dirs) then that is not very useful. Also it seems to give me more information that just inode 1. I have a feeling that I can use and get that to tell me what files are using blocks in the first 1 TB, but I have been unable to see how. (By using the mount option the file system will not give No space left on device, but if you later forget to use mount option then you will get No space left on device again. I would like to avoid the mount option as the filesystem may be mounted by other people on other systems, and they will forget that and thus get a surprising No space left on device). 

First, you must be sure, that there is your ISP fault. Try utility. It is combination of and which can provide you more information about latency bottlenecks. From its output you will find out, where the problem occurs. Simple usage example: 

Account will expire at the end of 2009-07-03. Logged in user won't be kicked out. He won't be able to log in after he logout. 

I think you have some kind of MTU problem. It happens when you send packet larger then minimum MTU on the network path (PMTU) with bit set, and ICMP error message is somewhere blocked. You should check local and remote firewalls to allow ICMP first. Then trace path to see what the PMTU is and where package drop can occure. Turn on bit! You should do this on the same port you use for client-server communications. Use hping2, for example. If nothing helped, turn off PMTU discovery on both machines. 

Maybe some program is locking this file? It can be another copy of your program. Does show anything? 

While installing package I get some additional packages on dependency. But when I remove installed package with yum, the dependency packages are not removed, though I don't need them anymore. How can I remove them automatically? 

I've loaded this files to document root and opened in Firefox 3.5. I see the first frame, see controls, but can't play video. This video plays good even from my server. How should I encode video to view it in browser? UPD: If I start playing video from the middle, everything works fine. 

Installation A personal installation does not require root access. It can be done in 10 seconds by doing this: 

There are several solutions. The workers are behind NAT. No access to a jump host. No access to firewall. For this you need some sort of VPN that can traverse the firewall. A TOR hidden service for port 22 on the workers can be used here. If all the workers are TOR-enabled: 

The magic seems to be and then only giving the devices that are known good + the last failing device. For the test scenario that would be: 

I have a RAID60 that I want to expand. The current is: 2 axles each having 9 disks + 2 spares. The future is: 4 axles each having 10 disks + 1 spare. So I need to do some --grow to reshape the drives. I thought this would be enough: 

Consumer grade DSLs are way cheaper than getting at leased line, so for my building complex I am considering getting a bunch of DSLs instead of a leased line. I feel confident that I am not the first to do that, so I am wondering if there is a Linux router that will somehow distribute the traffic over these DSLs. We are not talking about real bundling as the ISP should not be required to do anything. My initial ideas are along the lines of computing a hash based on sender IP and recipient C-class and based on this hash choose the DSL, so all packets between a given client and server will always take the same DSL. And if a DSL goes down it should of course not be chosen. But as I said: I cannot be the first to do this, so I feel confident there is a tested way of doing this. 

I want to connect to VPN every time goes up. I have already a script to do connect to VPN. How can I run it on interface up event on Fedora Core 12? 

I have HAProxy 1.5 running on Ubuntu 14.04 (modified). It accepts connections on http and https ports. Two backend applications process requests using persistent connection. When I create around 2200 client connections haproxy stops accepting additional connections. But I want this system to accept at least 10K simultaneous connections. Here is connection statistics: 

I need to monitor a single process (e.g. be warned when there are more than 3000 connections established) and collect statistics on it (e.g. to determine how many connections were established today 01:20 AM, when the server worked too slow, as client said). What tools should I use? 

I created , create and other directories in it, copied libraries, bash and perl binaries in proper places. Also I placed my script into . Now I can run the script such a way: 

You need to use non-standard ssh-ports for your VMs. I use openVZ on my server and have three virtual machines with VIDs 101,102,201. I thought, that it would be convinient to ssh into 101 VM through 101 port, into 102 through 102 and so on. To connect to the hardware node you can use 22 port. So I added this routing rules to iptables on hardware node: 

This took 20 minutes on a 50 TB filesystem. Oddly enough most of the time was CPU time and not waiting for disk I/O. It used in the order of 100 GB RAM. Now the file system is usable again: 

But this does not work because the command is dequoted by ssh twice where as GNU Parallel only expects it to be dequoted once. So instead use again: 

I have added it in /usr/lib/zabbix/externalscripts where I have a different script, that works. I have configured an item: 

The workers are behind NAT. Access to a jump host. If you have access to a jump host from which you can reach the workers the obvious would be: 

/dev/sdam is clearly (hd13). The rest of the drives are a software RAID60. Can I force GRUB to install on /dev/sdam1 (hd13) without probing? 

GNU Parallel is a general parallelizer and makes is easy to run jobs in parallel on the same machine or on multiple machines you have ssh access to. If you have 32 different jobs you want to run on 4 CPUs, a straight forward way to parallelize is to run 8 jobs on each CPU: 

If the servers are world wide (i.e. not on your local 10 Gbps network), then $URL$ might be a solution, too. 

But how do I identify the data to move? I cannot go by age, because the first 10 TB was filled the same day using . I have tried: 

I want all the files in directory be owned by nginx.devel. I have performed once and update these files using . But if I create a new file and then it, it will be owned by user.user and I need to run with privileges on it. 

I think you should start of monitoring your memory usage/cpu usage and then monitor your server network activity. Try this tutorial. It describes the main linux monitoring tools that will help you. 

I tried to measure network performance by soon discovered that terminal was fine. What has happened? We have a load balancing between two Internet channels router. Sometimes it routes my ssh traffic through wan1 and sometimes through wan2. I proposed, that there is something wrong with only one channel. So I measured network performance with mtr (great tool!) for two channels separately. yeah! wan2 has 21 hops with 110 ms and wan1 has 15 with only 21 ms! wan2 latency is the problem. 

I want maximum information in minimum letters. First, to be able to get information from SMS. Second, to read only the headers of e-mail messages in 90% cases. For example, the previous message can be just "" where H stands for . If this server is down, I would like to get such a message: . Instead of message, that "Swap usage on server is CRITICAL" I would like to get "[!] S: server/swap usage is >50%" where 50 was taken from the nagios config for check_swap, not hardcoded into the message. And if "Router/wan2 is CRITICAL", I want to see "ADSL channel is off". So, to summarize, I want to customize message for every service and its state separately, with ability to use plugin parameters in text. How can I achieve this? 

Again this confirms that Linux sees the device, which is already known. /dev/disk/* sees the RAID'ed internal disks as one device with 2 partition. It does not see the external disk: 

If your command returns on success you can even run on cheap ass spot-market servers: By using you can ask GNU Parallel to re-do the job on another server if one server breaks down (i.e. returns not ). 

So you are running around 600 jobs per second. The overhead for a single GNU Parallel job is in the order of 2-5 ms, so when you are getting more than 200 jobs per second, GNU Parallel will not perform better without tweaking. The tweak is to have more s spawining jobs in parallel. From $URL$ 

The two servers are the same model. They have the same modules loaded, but they are not using the same modules. shows that the two servers of the same model use very different RAID controllers (I hate it when vendors do that: F*cking change the model number if it is not the same model!). On the server, where the disk works, you do not need to setup anything to access a disk that is not in a RAID. But on the server, where the disk does not work, you need to set the disk up as a volume in the RAID controller before Linux detects it. So I have now done that, and the disk is now accessible. Thanks to Falcon Momot and Sergey Vlasov for pointing me in the right direction. 

This command is available in most modern Linux distributions. It fills your disk with zeros 10 times. Information on the disk can be partially recovered only using very expensive techniques, that buyer of the old netbook, I think, doesn't posses. 

You should place it somewhere outside webserver root, for example. It has security considerations: webserver code must not be exposed to uploaded code, and it can happen, if you mix these two things and set wrong permissions. And you keep your content between upgrades. If users upload small files, the best way is to store them in database. 

I've 2 remote HP Proliant DL180 G6 servers with HP Integrated Lights Out enabled on both and I have to install RHEL5 on these servers. I try to do it with ILO - mounted ISO file from local hard drive as Virtual CD-ROM, and performed installation from ILO console. OS installation on the first server was successful. But I have to make 3 tries to achive this. The channel between me and the servers is too slow - they are in datacenter. The installation takes a long time - 5-6 hours. So sometimes Virtual CD accidently unmounts and installation hangs. How can I perform the installation on the other server in more convinient way? 

You need user account on the server (login/password may be different, than for your ftp-account). This user must be or have permissions to run . 

I have just installed a SAS disk into a Debian server. It was detected correctly and everything was fine. Then I moved the SAS disk to a different Debian server, the same hardware model and running same version of Debian, but here the SAS disk is detected as /dev/sg7 and not /dev/sdb. works fine, but and hang. I tried putting the SAS disk in another slot: Same problem. How can I force the SAS disk to be detected as /dev/sdb? 

Grrr.... is not needed. is needed. A restart of is only needed if was changed from no to yes. Disabling my own was needed very much! I had * statements in my which re-used the ssh channel and thus I would not discover the change. Thanks to Samed Beyribey and quanta, whose help gave me the idea to run which gives very different output when you have * statements. 

So that could be related to the problem. Edit 2: I tried removing and re-installing portmap and nfs-kernel-server. No luck. So I mucked around with and did so the output is now: 

It gives 77 workers. GNU Parallel is a general parallelizer and makes is easy to run jobs in parallel on the same machine or on multiple machines you have ssh access to. It can often replace a loop. If you have 32 different jobs you want to run on 4 CPUs, a straight forward way to parallelize is to run 8 jobs on each CPU: