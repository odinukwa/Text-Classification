OK first lets clarify terminology, “10Mbps” is used to denominate a unit of speed “megabits per second” which is 10,000,000 bits per second. Megabytes being 8 times larger and MB (MegaBytes) usually used to denote volume of data as a unit. I assume you meant the former, 10Mbps. Secondly, I think you have misunderstood some basic networking aspects based on the wording in your question and I don’t think the other answers are clearing up your misunderstanding, only worsening it. There is almost no link to the length of a cable and the speed at which data is sent across it. The speed at which electrical signals travel down a copper wire is related to the dialectric constant of the physical cable and casing properties. Long story short, they can travel at nearly the speed of light, copper cabling for all intents and purposes isn’t slower than fibre optics which many people seem to think. If you really want to say how much data a cable can "hold", irrelevant of length, the answer would be "1 bit" (a pair can be transitioning form DC+ to DC- or from DC- to DC+ at any one moment in time). Thirdly, speed is not the same as bandwidth. In networking nomenclature we use the word "bandwidth" to describe the volume of data that was moved. We can move 10MBs (megabytes) of data and 1Mbps (megabits per second), or we can move it at 10Mbps and it will take 1/10th of the time it would have taken at 1Mbps. The speed at which data is transferred over a copper cable does not change based on the length of the cable. We can have a 5 meter cable that runs at 10Mbps, 100Mbps, 1Gbps, 10Gbps etc (there is a slight increase in delay with longer cables as I said above, the electrical signal does have to propagate down the cable, but this is so tiny it is negligible, it doesn’t change the speed of the network link). - the bandwidth doesn't drop, it either "is" or it "isn't". The data was either received without error at the speed the link operates at (10Mbps, or 100Mbps etc.) or it wasn’t and the data must be resent. This is only if you are using protocols that support re-transmission and/or you are sending data which must be re-sent (e.g. live TV streams, dropped packets are lost and not usually retransmitted). Since speed and bandwidth are not the same thing, it is possible for the error rate to increase without the bandwidth dropping by sending larger frames or packets for example, so few packets need to be successfully send and received because they carry more data. - nope, bandwidth is not dependent on length. Again, the misuse of phrasing is making things unclear. I take bandwidth to mean "the goodput of data successfully transferred". - No. ACK is something that happens at the transport layer in TCP. The length of 100 meters likely stems from CMSA/CD (Carrier-Sense Multiple Access with Collision Detection). This is the process of listening on the wire for a specific time, to check if another device is transmitting, if no device is transmitting (nothing is received) then it is assumed to be safe to transmit without a collision occurring. How long should we wait before we know its safe to transmit? We wait as long as it would take for the signal to travel from a device 100 meters away! - this is a bit of misnomer. I touched on it above. The increase in delay from the extra cable length is so small it has no measurable impact. Wikipedia ($URL$ lists the speed of electricity to be in the range of 50%-99% the speed of light. At 0.5 c it takes 0.0000006 seconds for the electrical signal to travel 100 meters. For a 10 meter cable it takes “one less zero”, 0.000006 seconds. This has no measurable impact on you day-to-day application performance. What does have an impact is the operating speed, such as 10Mbps or 100Mbps. On a 10 meter length of Cat5e cable running at 10Mbps you will only get 10Mbps, no more, no less. Equally, if you now connect that same 10 meter length of Cat5e up at 100Mbps it will run at 100Mbps, no more, no less. What really makes a difference are factors like the encoding scheme used ($URL$ serialisation delay, the error detection and correction methods used (see $URL$ and $URL$ etc. This is because as the length increases so will the attenuation, NeXT, FeXT ($URL$ and $URL$ etc. resulting in (eventually) data loss. 

I am using the following interface configuration, although this is platform depending. This is a 7600 with LAN cards, on some ASR9Ks and ME3800s the timers are lower due to the better hardware support so you need to tune this to your own requirements: 

I have some ME3800's running 15.3(3)S which aggregate layer 2 circuits from customers. They all have a similar configuration of an incoming trunk on Gi0/1 and then Gi0/23 & Gi0/24 are layer 3 p-t-p links to neighbouring ME3x00 switches; 

So to be clear; both products are available, symmetric connections and asymmetric. Symmetric connections tend to be more expensive but it sounds like you are approaching this from a home user / small office point of view. Anyone with the money to spend who also has the need for lots of bandwidth isn't going to be using ADSL (or LTE). ADSL is everywhere because that’s what people can afford. There is no limitation here as such. Its more about what is commercially viable. 

I am looking at a 7609-S with a RSP720-3CXL running 12.2(33)SRE3. I have been asked to help with rate limiting some servers attached to a couple of ports running as L2 ports (), so this requires me to apply a policy-map to the L3 SVI that is the default gateway in this VLAN. When enabling the service policy under the SVI interface configuration I received the following error; 

Different devices and software/firmware use different load-balancing hashing algorithms, what equipment are you using and what load-balancing method are you using? Based on the minimal information you have given, you might just have unequal load balancing. With MLPPP you should be effectively be doing per-packet load-balancing across all the member links but that isn't always the case. If your set-up is doing per-IP balancing for example, you might have a couple of larger flows balanced onto the first link, or it may be that you have many flows between the same set of IPs which again are over-favouring for example the fist link. Also check that all the links are in the MLPPP bundle and that they are running at the same or very similar speeds (user configuration for example might state one link is 2Mbps and another 8Mbps, when they should be all the same). The RFC lists various discriminator factors used to determine which member links belong to which MLPPP bundle. Ensure those factors (whatever you have configured) are matching up so that all the links are actually in the bundle. Also do you have access to both ends of the bundle, is there any discrepancy between MLPPP end-points? 

Irrelivant of when to use active/on/desireable etc; It sounds to me like you didn't have STP configured at all/correctly. BPDU packets coming into the access port on the 2nd switch should have caused the port to err-disable. Configure correct STP protection schemes (which I am not going to explain here, as they are all over the Internet) and I don't believe this problem should have happened in the first place. (Also if your switches support it, broadcast storm controll as Ricky mentioned!). 

You have two default routes. Firstly you will presumably be receiving one via DHCP from your home router, this one: . That allows you to access the Internet without the VPN (otherwise how will your Linux machine know which path to take to connect to the far end of your VPN tunnel). You also have a 2nd default route which is presumably being installed by openvpn when you establish the VPN tunnel, this one: (technically speaking that isn't quite a default route, it should be 0.0.0.0/0) Note that the route has a metric of 303 so it will be less preferred than the default route via the tunnel. This causes a problem though: 

2nd Update Tested and works! The ME3600X/ME3800X are MEF 2.0 compliant devices so this is a required feature. 

Since you are wanting to measure a layer 2 circuit, I'd like to shamelessly push some free software that I have been writing. It's still in beta but does exactly what you want based on the information in your question. Unlike nuttcp, iPerf, jPerf et al, Etherate runs directly over layer 2 designed specifically for Ethernet testing. Like the others a laptop on each is all that is required and my Intel i3 laptop can max out a gig link with ease. Since it’s in beta stage the more advanced features aren’t ready yet like MPLS testing but throughput, latency and MTU testing (all directly on layer 2 Ethernet) are implemented. $URL$ 

1: This makes a massive difference. For a typical application on Linux that sends a packet, when the application calls the syscall for example, the packet is copied from user-space memory into Kernel memory (into an skbuff). The skbuff is then copied into another section of memory by the Kernel that is accessible to the NIC. The Kernel signals to the NIC that there is a packet waiting to be sent and the NIC then copies the packet from this area of memory (via DMA transfer - Direct Memory Access) into it's hardware tx-ring buffer. This means that to get a packet from an application and into the NIC tx-ring buffer, the same packet is copied three times. DPDK allows for the NIC to DMA the packet directly from the application memory space (this works by DPDK disconnecting the NIC from the Kernel and mapping the DMA memory space into the user-land memory space the application is using, hence "Kernel bypass"). 2: By batch processing packets DPDK can make very good use of the CPU cache. The first packet to be processed will incur an instruction cache miss whilst the i-cache warms up. All subsequent packets within that "batch" which be processed very fast using the same instructions which now are "hot" in the i-cache. There are many other software improvements in DPDK over using the standard Kernel IP stack however, these 2 have a major impact on DPDK's performance. 

The best methods (in my opinion) have already been mentioned, so simply incase you are on a device without those cool features, a fall back option is with an access list. 

There is some room for discrepancy to arise here between networks and customers. A standard billing issue example is that a customer of an ISP has a 50GB per month bandwidth allowance. Different operating systems use difference units of measurement for both speed and memory and display this using the different IEC and SI prefixes. This section of the same Wiki page on Operating Systems and Software lists the discrepancies between operating systems and software packages that mix and match different prefixes with units of measurement. It is possible for an ISP to measure 50GBs of data transfered as (1GB == 1073741824 bytes). and the user may disagree when the 50GB limit is reached. A users who's machine displays Gibibytes for example would show 46.57 Gibibytes transfered. From the network's perspective 

I know that MUX-UNI can only be configure on a 7600 running certain IOS version, SR and SX I believe. Does that not include SRE? Logged into a 7600 running 12.2(33)SRE9a, with the following configuration; 

- If you have configured PE1 as 8.1.1.1 with then that is how the routes should look. I suggesst you read more about BGP behaviour for MPBGP sessions and sending labeled updates for IPV4 unicast and VPNV4 etc. 

I disagree with the answer provided @Pedro Perez, I think his answer is mixing phrases and ambiguous so I have provided my own interpretation below. SDN: I think it is pretty much what the name says “Software Defined Networking”. This means to me that software is defining the paths that packets take across the network automatically (possibly with some upper bound / lower bound constraints provided by a human operator). It can also readjust paths automatically that are within those predefined constraints. I would consider a deployment that uses an off-box (centralised) control plane such as the OpenDaylight SDN controller with protocols such as Open Flow, BGP-LS or PCEP to be an SDN deployment. The SDN controller is talking to network devices and steering traffic based on live data it receives from the network devices. NFV: Again pretty much what the acronym suggests “Network Function Virtualisation”. This to me is about virtualising network functions – not how devices forward packets and make decisions (which is covered by SDN) but at the device level how one can virtualise and containerise network functions so that they may be rapidly deployed, re-replayed, migrated, decommissioned etc. To clarify a network function here would be say NAT, CG-NAT, ACLs, QoS, tunnelling, DNS or web filtering, load-balancing etc. I would consider a deployment that uses standard computing hardware (such as x86 servers for example) to run virtual machines or containers that are routers, switches, firewalls load balancers etc. to be an NFV deployment. With NFV one can chain multiple virtual functions together to form a service chain. For example running multiples virtual machines on a single x86 box that forms a pipeline and a packet must pass through the entire VM pipeline; the first VM could be a virtual switch and faces an office LAN, if the packet is destine to the Internet it could hand over to a virtual firewall, if the packet passes the firewall rules it can hand over to a virtual router, which can then send the packet out of a physical WAN link. NSO/LSO: “Network Service Orchestration” or “Lifecycle Service Orchestration” have an overlapping definition in my opinion regarding the (usually) automated process of applying changes to network devices, configuration and infrastructure and managing the infrastructure such as deployment, upgrades, decommissioning etc. I am bringing this up because a NFV based network can be completely deployed by hand; you might be using KVM on Linux and you SSH in to each KVM machine and start up virtual router images from the likes of Juniper and Cisco by hand, on the CLI using for example. It could also be VMware using ESXi and again you use the web console to do everything (these are just common examples). You are still virtualising network functions and potentially chaining them together if required, just slowly and potentially erroneously by hand. With SDN one could be using BGP-LS with an off box controller to automatically steer traffic flows around the network which could be built using physical routers (the traditional infrastructure build method), but the controller could also be controlling traffic flows on virtual routers (an NFV based infrastructure), SDN is agnostic of whether the devices are physical or virtual or just two bean cans and some string and an API layer above them. NSO/LSO approaches to network operations have existing for years, but not with the formal guidance that they have now (such as through the MEF LSO (Metro Ethernet Forum) standards). In its most basic form, engineers have been writing scripts that telnet or SSH to a device and apply commands via the CLI. If one expands that basic example a bit further to have scripts that automatically apply bits of config to devices vi the CLI, TFTP boot new firmware versions on devices, generate config files from templates, which is all stuff people have been doing for years, then we have an informal NSO/LSO platform. I hope that has made it clear that NSO/LSO is different from NFV and SDN, they are three concepts that can all exist without the other two or be used in any combination with the other two. Side note about network overlays, virtual networking and network virtualisation: