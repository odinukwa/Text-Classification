Why should one release from the second build? In my opinion one could merge into master when the build was successful. 

If you would really like to loadtest a k8s cluster, kubernetes has some documentation about this, e.g. this document explains how to load test a k8s cluster and in this case force autoscaling. 

Based on the definitions Software Development could both be seen as Manufacturing and a Project. It could be seen as manufacturing from a deliverables perspective as Development activities mean that that the software is created, but does not guarantee quality. Therefore QA is required and to get it online Operations tasks are needed as well. In summary, Software Development could be seen as manufacturing as multiple specialisms are required to make the feature available for a customer. It could also be compared with a project as the Dev team itself does R&D and creates a (software) product. From a DevOps perspective I would advocate to compare Software Development with manufacturing rather than project as DevOps is about let work multiple specialisms together and not about creating silos like project. 

I wonder whether it is really an issue as I have seen a lot of logs and also weird messages while everything worked. What I do in such situation is talk with the developer of the code. In this case I would create an issue in the Jenkins bug tracker and discuss whether this is an issue and whether the developers should change the code that triggers this message. 

What is DevOps? Why should I learn Java or a programming language that the team is using from a DevOps perspective? Java itself 

It looks like your k8s cluster has not been deployed well. I have verified that it works to deploy the nginx by using the template as defined in the question. In contrast to your cluster, the cluster I am using has several k8s images to host the dashboard etc. 

It is not clear whether there is a clear difference between a Director of DevOps and a Head of DevOps, e.g. according to this vacancy both roles are the same, while according to this the Head of Devops is the owner and responsible for DevOps, while a Director of DevOps is responsible for implementing a DevOps culture. 

Once the docker container runs (one could verify this by issuing ), one could run and run commands. Note: I personally prefer to investigate why the owner and group is not set to rabbitmq instead of fixing it in a running container as it is mutable. 

At other projects I preferred to deploy only tags to enforce the git flow (tags should be created otherwise no deployment to production). 

and deploy it manually or automatically using ssh and merge it with another ingress file that has already been deployed 

put everything that is required on debian like apt in Debian.yml and yum in Centos.yml. In summary, it is possible to let all OS types use the same role if the OS specific things are put in separate files. This is a more comprehensive example $URL$ 

Sysadmin vs. DevOps (personal view) Some companies talk about Dev, Ops and Test. If something needs to be tested then they say: "Test should do that". If something needs to be developed, Dev will do that and if software needs to be deployed, Ops will do that. In my opinion and what I have experienced at several companies is that this results in a "throw it over the wall" mindset that results in friction between people and teams. Personally, I sometimes feel that people work individually and say this is what I did, I have nothing to do instead of working as a team. According to me DevOps means that everybody in a team is responsible and busy with development, testing and operations. There is no I in a team and no separate departments. Everybody should release. Of course there are specialties, but in my opinion everybody should be able to do at least 25% of some work in every area. E.g. if someone was a Developer back in the days then one should be able to change some configuration management code, e.g. chef and deploy software. References Sysadmin According to Wikipedia: 

If one checks the Tomcat homepage then one could see some versions, but not only the latest. Off course I could parse the website and find the version, but I am looking for a Tomcat API that shows the latest version to prevent that I have to create a script that is error prone. Why? I would like to automate the update of our internal tomcat packages. At the moment someone is checking the latest version and create the debian package manually, but I want to see that that is automated. What has been tried? 

I found a couple of posts and it seems that this issue could be solved by changing the structure of the Dockerfile with the accent on separating the copy of the package.json, the rest of the files and running . 

One of the reasons for asking this question is that it seems to be impossible to access a container using kubectl exec when these are deployed in a pod. 

According to this github repo it is possible to test the Jenkinsfile, but this is run by Jenkins and if there are issues it will not run. Should one run such unit test in another CI? What are best practices to solve this issue? 

If for example the number of complex Java issues is over growing exponentially on the backlog this could be a sign to hire a Senior Java Engineer, but what are signs for hiring a CIO? 

and try again. The issue was solved by inspecting the logs. If for example one navigated to instead of the log indicated: 

Show the image in a presentation Talk 30 seconds about it During the elevator pitch more and more people understand DevOps and it is completely clear by them. 

Discuss this with the team and figure out what is the best solution for everybody. Multiple solutions are possible. Everybody should be happy with a certain approach. 

works, but then a pod should be running. The problem is that the file has to be copied before the pod starts. The jar file has to be loaded by an Application server. Destroying the pod after the copy will not work as the docker image is deployed again and the content will be wiped out. A configMap was misused to copy the jar before the pod starts, but that only seems to work for configuration files. This was confirmed by comparing the checksum, e.g. local vs. remote. 

According to this documentation it does not seem to be possible to override complete sections in docker-compose by using different docker-compose files. 

At the moment I am editing a Jenkinsfile and then let it run unless Jenkins reports a issue. This approach costs a lot of time. I prefer to validate the syntax before committing the Jenkinsfile. Is there a tool that solves this issue? In gitlab there is a URI, i.e. that makes it possible to submit a gitlab file, click on the check button and then the UI will indicate whether the syntax is correct or not. 

When one searches for make on dockerhub multiple images are returned, but none of them seems to be able to run make. 

A configuration management tool is not required to manage a PAAS. For example, when one has created a VM on Azure then it is possible to export the configuration snippet as powershell or as yaml and use yaml to deploy and manage systems. In my opinion one should restrict configuration management tools like puppet or ansible to manage configuration files and installing additional packages and yaml to manage the PAAS as I prefer KISS. If one would like to manage different PASS then one could consider to use terraform. 

It looks like that your agent connects to the wrong puppet master. Not the one that you run inside the docker container. Ensure that the agent connects to the right master. First of all try to check whether the agent could reach the master by running: 

Yes, that is possible according to this post. When a second jinja template was created and this was included in the base jinja it was called. 

What happens if the ingress goes down? Then all microservices are not accessible anymore. What about using an ingress for every microservice, i.e. one ingress file per git repo? 

Correct me if I am wrong, but that is one of the reasons why docker orchestration tools exists like docker-swarm to be able to deploy multiple docker containers by using one command line. 

In summary, use an ssh key with password. Let every user use their own ssh keys. Never allow sudo access via ssh. 

The UFW module document has been read, but it is unclear how to let ansible remove ports that are not defined anymore. It works to add new ports, but now 5 ports have been removed and I do not want to login to each of the more than 50 servers. It is possible to write some custom exec, but I would like to use the Ansible's UFW module's functionality. Does it support such a scenario? 

Dev, Tst, Acc, Prd (DTAP) is about isolating multiple environments to ensure that data will not be mixed, e.g. production data in development or lost, e.g. database removal removes all production data. DTAP is also about environments that are identical, e.g. if a database in prd, then there should be a database in dev as well, but the data should not be mixed. It is about prediction. The more the environments are identical the bigger the change that if a certain change works in dev, then it will work in tst, acc and prd. Discussion There are at least two options, i.e. one k8s cluster or multiple. one k8s cluster