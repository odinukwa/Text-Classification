I highly recommend partitioning your table now. I'd create a new table partitioned by year that looks just like your current table then insert into it. Going forward, you can more easily prune your table. Here are several links to a good partitioning discussions... $URL$ $URL$ I'm not a SQL Server guy - I saw an MS SQL Server 2016 documentation suggests there are some differences in 2014 features - but I can't find 2014 documentation. Here is a link to MS SQL Server 2008... $URL$ Good luck! 

This is an ugly solution that may introduce other issues. It is offered as a means to eliminate the lock you are experiencing. Auto_increment is preferable to managing numbers in this manner. The number could be managed in its own table incremented as needed. This table could be locked at no detriment to the main table. When write locked, other sessions are prevented from using the table, the locking session can then increment the number (and store it in a variable - something meaningful like @next_position) then releasing the lock immediately after the update. The number is then used in your main query - no join, simply "LQ.initialPosition = @next_position". You may still have contention on the position number table - but these locks shouldn't be held too long - only during the incrementation to ensure only one process increments the number at a time. The locking scheme could be eliminated, but this would require knowing the current number first which would then be tested in the update's where clause to eliminate race conditions with other sessions. A loop will be necessary to try again if the update doesn't work (another session made the update first). This complicates the use of the table but reduces the potential for trouble. 

When I test ApplicationIntent=ReadOnly connections from a machine within DevDomain I am correctly directed to the readable secondary. Note, the connection string specifies the Listener name as I am in the same domain. However, when I test ApplicationIntent=ReadOnly connections from my own workstation (different domain) I get the connectivity error below. Note, in this case I need to specifiy the Listener IP since the I am outside the domain and the name will not resolve. 

However, rather than grant permissions directly on the Certificate and Key, I like to have two stored procedures - one for encryption and one for decryption. We generally have different users that perform the encryption and decryption. Generally, the application is encrypting records as they are inserted into the database. Then it's a different user that needs to decrypt them for reporting purposes. Both procedures are created and the users are granted on the procedures. This way no users have direct permissions on the Certificate and Key and no user can both encrypt and decrypt. 

Update: Here are IOPS for the random workload. The reads make sense, though, I'm not sure what to make of 20000 for the writes. SAN cache is 3.2 GB. SQLIO tests are against a 25 GB file. 

In my personal experience, if I were keeping score, causes for database restores have been: accidental deletion of data = many, restore to dev for testing = many, IO system failure / corruption = none. (Knocks on wood) You indicate you have "no need to recover to any point in time previous to our last backup". If that is in fact the case, then yes, switch into SIMPLE recovery. 

Some insights from having actually built a couple of apps with contacts and companies. Firstly, you're missing several use-cases in your outline. Among the colorful ones I've run into over the years which you don't necessarily cover: 

On a personal note, in the case of a ticketing system, I'd have my eye on full text search functionality and bitmap index scan (i.e. the ability to single out the one or two disk pages that might contain rows that fit for idx1 and idx2 and idx3) functionality much more than raw performance. But that's just me -- ticket searches in trac, lighthouse, redmine etc is just dismal in my experience. By this criteria, as well as performance, Postgres fits the bill quite nicely. (And it's available on heroku.) 

Yes, it does. For all sorts of reasons; in particular because it caches plenty of things for faster lookup when plenty of RAM is available. The more RAM, the less HD access, the faster. 

Some organizations have sub-organizations, be them divisions, subsidiaries, you name it. Some people belong to several organizations. As in VP whatever of XYZ, chairman of ABC, and self-employed CEO/consultant -- each on a part time basis, hopefully. Some people can be contacts for an organization without actually belonging to it. E.g. when a high profile consultant is temporarily hired as a project lead, he could possibly become the primary contact for company XYZ without actually belonging to it. People can have sub-people too. And the latter can be in the same companies, or not. For instance, our VP of XYZ might prefer that you go through his main primary secretary over there, unless you contact him regarding ABC in which case that would be his assistant; or personally for consulting work -- a very busy person indeed, but there are people like that. Some organizations have no contacts associated with them. This can happen, for instance, in an app that outputs lists prospect companies (for sales people) in which contacts have yet to be identified. Conversely, contacts can have no organizations. For instance consumers. 

What is the fastest way to determine if an IP is contained within a CIDR block? At the moment, whenever I store a CIDR address I also create two columns for starting and ending ip addresses. The starting and ending ip addresses are indexed. If I want to see which network contains an address then I look which seems less than desirable. It occurs to me I can store the right shifted number and could match similarly shifted IP address (660510 in the case of @cidr)... 

An approach using shifted_netmask (undesirable - I'm incurring a full table scan to discover the number of bits in the netmask)... 

This behavior could be achieved programmatically... Create a table of databases and the status you would like them to be.. then your application can test the status of the database prior to use.. the application proceeds if the status is available and returns a message if unavailable. You could also do this with applications in general too (same thing as described above but with application instead)... You can then control applications use of the instance. 

Unfortunately, MySQL's regular expression function return true, false or null depending if the expression exists or not. The trick in effecting the desired behavior is to determine which substring begins with the character you care about, has the correct length, and is followed by a number. A series of substring_index functions are used to extract the string... 

The following is an untested attempt at distilling the problem into its basic components then unions the results together. The last query (with the correlated subquery) can be turned into a join if the introduction of orders does not cause a Cartesian product. I can not test with what is provided to see if this is a valid assumption. 

When looking at the read / write ratio of your server would you look at the NUMBER of reads and writes or the number of BYTES read and written? For example, consider the following numbers from sys.dm_io_virtual_file_stats: 93384 reads, 1080818 writes, = 8% reads, 92% writes 30630654464 bytes read, 60955863552 bytes written, = 33% bytes read, 67% bytes written 

For posterity, after much trial and error, we figured out how to get the expected throughput. As mentioned above, the NetApp had one virtual interface backed by four physical NICs. The host has two NICs and I had configured MPIO through the MS iSCSI Initiator so that there was a path from each NIC to the one virtual interface. The results were the throughput above - writes made sense at close to 200 MB or the speed of two NICs, but the reads were half that or the speed of one NIC. Upon closer inspection, our SAN guy noticed that traffic was only flowing through one of the physical NICs for the reads. I'm not sure if there was a configuration mistake on our end, but there were two things we tried and both got us our throughput. One was to change from one virtual interface backed by four NICs to two virtual interfaces, each backed by two NIC's. Then map one host NIC to one virtual interface. The other thing we tried was to use "aliasing" on the SAN side to present multiple virtual interfaces. (I'm not a SAN guy, so hopefully I said that correctly.) My take-away is that we just needed the SAN to present more than one interface so the Initiator truly saw multiple paths. Here is our throughput now: 

Secondly, since your outline mentions customers, be wary that customers can be a company or a (physical) person. You seem to have that part covered based on your diagram, so let's move on. If your next step is to add anything related to accounting, such as an "orders" table, be wary that anything accounting related is tied to a company, a contact, products, prices, etc. at a given point in time. These details can evolve in all sorts of colorful ways, and a frequent design mistake is to create the perfect normalized design and assume that you'll just update the tables as needed. Big no-no. If the taxman asks you to print out your invoices, and your IT says company XYZ at price P when accounting booked company ABC at price Q, you're very, very screwed. And don't get me started on archived/closed yearly accounts and reports that change due to similar design mistakes. Thirdly, be very, very, very wary of UI/UX issues that you might introduce by normalizing too much. If it does not work like the apps that typical users have in their hands (read: Outlook), you'll be a poorly trained secretary away from changing the company of every employee at XYZ to ABC when Joe gets a new job. I've actually seen this happen and it was not a pretty sight. (She kept her job, in case you're wondering.) Lastly, don't get me started on merging the inevitable duplicate companies, contacts, and so many other colorful things that couldn't possible turn up. Keep those in mind and make sure that your schema is very, very forgiving because it will happen. Now... in practice... Personally, I've come to actually turn a blind eye on normalization in this case. Contacts/Companies is one of those cases where DB design and beautifully normalized data as taught in school is a recipe for trouble in the form of resource hogging, over-complicated queries and ludicrously complex UI. It is, imho anyway, not the right thing to do. Start by creating a contacts table, complete with fields such as first and last name, display_name if needed, company_name, address, phone, cell_phone, email, email2, secretary_name, secretary_phone, etc. If it's available when you create a contact in Outlook, it probably belongs in there. You'll note that I didn't mention a companies table. That's because you usually don't want any strong ties between your contacts table and your companies table. If you need one, add one, and add a copmany_id in addition to the company_name. But then, make it a foreign key on delete set null. And be sure to keep it very, very loosely tied to company_name at the database level. Maintain it at the front-end level -- not anywhere else. This will keep a secretary from inadvertently changing the company of multiple contacts. Keep things sane. Anything that might reasonably show up in a list (i.e. select * from contacts where ... limit 10), be queried against, or be useful frequently ought to be in the table. No joins, nada. Query, iterate through the result, done. If you've really want additional garbage in there, you've two options. One is to create an extra_contact_details table. It can be an EAV table, or the full load of company_name, address, phone, etc. fields, or a hogwash of normalized bits and pieces. Whichever option you take, don't over do it. Either will create additional (potentially complex) queries, and heaps of thorny programming issues down the road when you create the UI. What's absolutely key here is, if you go this route, a secretary who has always worked with Outlook needs to make sense of it. The other, which I'll actually recommend upfront, is to add a text field called "extra_contact_details" in the contacts table and begone with it. No normalization at all. One set of primary details. Frequently used secondary details. Anything extra as plain text. Secretary gets it. End users get it. Done. Lastly, if you need to store a version of any data at a given point in time, be sure to duplicate the value of any key details -- read: whatever you need to print it exactly as it was when it got inserted.