I would like to find out from the code below, what was the name of the database where the last command was run. In this case I am looking for . his there any way of finding this? Is there any DMV that I should be looking at for this info? Also, what if the update was wrapped in a dynamic sql, would it be possible to track it then? 

these links below are interesting references regarding table partitioning: How to partition an existing non-partitioned table? What's the best way to archive all but current year and partition the table at the same time? Partition Key questions in SQL Server 2008 

I am trying to create the index above, because I am NOT interested in any situation where sintOrderStatusId IN (9-14) Of course I can create a view or indexed view, but I was trying to avoid that. just adding more info: sintOrderStatusId is a smallint NOT NULL and the possible values range from 1 to 30. the 9 to 14 are to be avoided, therefore the filtered index. 

Missing Join Predicate the Missing Join Predicate has been discussed here and Here too. Missing Column Statistics I have fixed it, or at least got rid of the warning by simply running 

I have been doing this, below, but I would really like to find out the list of the AD groups (that have access to this server according to the above picture) that this user belongs to. 

I have basically just done the number 3 and the linked server is working fine for me now. As you can see on the picture below. 

I don't know fully details of your environment, i.e. if the tables in question are mostly used for writing or reading. How often you do this delete? what is the primary key and clustered index of [sko].[stage_närvaro]? If I wanted to optimise this delete there are a few things I would consider: 1) an index on the underlying tables of the view ext.v_imp_närvaro with the columns used in the select (person_id, termin_fakta,[läsår_fakta]) you want an index seek there most likely (no need to include any columns because you are just going there for the EXISTS) 2) I have been using a lot filtered indexes and I would consider the following: 

I have many situation where I need to convert big float numbers with many decimal places to a rounded number with 2, or 1 or no decimal places at all. the way I have been doing this is demonstrated below: 

to me it appears that there is something wrong as the way mirroring has been set up. you could use the query below, on the server where the principal databases are. this would give you some visibility as things have been set up. 

So far whenever I did the replace from the view to this table, I got performance gains, because I am not doing and all the time anymore. However, I got the overhead of putting the table together at the first place, which can be an expensive operation. Questions: 1) at the moment, I am creating the table from the view without the clustered index, then adding the clustered index. Are there other things I could consider that might make a difference to reduce the overhead of inserting about 10 to 20 million rows into this table? I thought about putting this table on a different filegroup, but it will have to be on the same drive anyway so not sure if that would help. are there any trace flags to improve the insert? the database is already in simple recovery mode, with enough free space inside it, so to avoid an autogrowth. 2) as I find this type of situation not so uncommon, without changing the database design are there any other alternative that I could test against my current solution, so that I could compare both? 

But when have a look at the permissions for the all seems to be fine because the user in question has the update permission: 

these settings were decision of our development team: there are 2 settings: 1 - max degree of parallelism 2 - optimize for ad hoc workloads On OUR_DEV_SERVER: 

I recommend you have a look at the procedure sp_dbpermissions by Kenneth Fisher. I have been using it and it allows you to list all the permissions of a particular user in a database, and gives you all the scripts to copy all those permissions and apply them on a different database. In case you want to apply them on a different server as well, that is fine too, but don't forget to create the server login first. 

Can someone please post an example that works? Here is a good article that describes OPENROWSET usage. Examples of working scripts using OPENROWSET - please read comments 

I recently had to make the stored procedure spPartyOrderAllocation_AllocateItems to select and update tables in other databases, while keeping the pemissions levels to the minimum. This was necessary because an specific application Stock Allocation would connect to the databases and only run this stored procedure. The way I go this done is: 1) create a login that will be used to run the stored procedure in different databases 

I have the following procedure that is called over a million times a day, and I think it can be tuned for better resources usage. 

the problem is that I have to drop the primary key constraint and re-create it as clustered. this is what I would like to get done in the subscriber database: 

As you ca see on the picture, I have a table called "tblsapdispatch" that has 55,707,259 records. Last sunday 30-aug-2015 they run a big update on this table, and I can see that the statistics were updated. I am considering partitioning this table because I parts of it are never updated, just need to be there for reading purposes. The primary key is an identity field, as you can see on the picture below. questions: 1) this is a very busy table, what can I do to minimize the impact of having to lock the table and rebuild the clustered index? 2) if I leave the table as it is, how can I be sure that all the statistics for all the index were updated? Is there a script for this? 

I need to grant permissions to a group of users to run a specific job, when I have a look at the job 

I run into the same issue while schedule a job to create indexes. what I did is 1) - I run this script to identity what is the current value of the options 

I was intrigued as which one would perform better so I did a test and would like to share the results here. I create a table to keep put some data in XML and for testing later on 

And also on the Analysis Server Properties, the "Commit Timeout" and "Force Commit Timeout" as you can see on the picture below. 

that gave me one operation but not the second one, I would like rather see everything that happened after I opened the . and 

a quick example just to illustrate the well said answer by Scott Hodgin: first you create, and populate a table 

The options in Management Studio On Management studio Tools->Options->Query Execution->SQL Server there are options that are set and these prevail over the same options that are set on database level. 

If you haven't found the difference is in the Login_From. Basically what I am trying to achieve here is to find windows_logins that belong to a windows_group login that are also present in the same server. Inside my procedure the critical line that get the windows group for an individual login is the following: 

how is this that my records of 2010 are not on partition 1? can I find out exactly on which partition a record is? how do I add another partition, let's say next year (2018) I will add the partition for 2017, without having to rebuild the clustered index? last but not least, let's say in a few year time down the line, I will not be interested anymore in the rows of 2010. how can I get rid of all that partition? here below is how this database is created, basically my goal was to put each partition on a different file. 

On my above code, I don't list the permissions on my User-Defined Table Types. How can I achieve that? 

A few days ago I had a question about warnings in the execution plan Type conversion in expression may affect “CardinalityEstimate” - on a computed column? and that was solved, thanks Paul. Now I have another question regarding the , this time it is caused by the data type. I am using sys.data_spaces and sys.indexes to run a simple query to get some information about my indexes: 

this has been working very well as it is. the results and behaviour is as expected. question: 1) How could I have done this differently and more effectively? 2) have you noticed that instead of writing the procedure with EXECUTE AS I had to add this line 

I am using the table sysschemaarticles to collect information in my transactional replication. It is used for a variety of things including adding new articles and finding out who are the subscriber Servers. this script needs to be run in the publication database. 

Questions: How can we keep control of what is locked by who? how can I put a lock on a record using stored procedure and check if there is a lock on a record using stored procedure ? What would be the possible code of the and stored procedures? 

O.K. in this case in particular I noticed that this specific procedure could be tuned, I even found a missing index, as you can see below. 

I can see that the index on row 1 is contained in the index on row 8. index on row 2 is contained in index on row 4 based on the included columns and so on... questions: 1) performance in retrieving data from this particular table is a must, this database has much more reading than writing, is it ok to leave the overlapped indexes if I can see that they have been used? 2) How can I improve this script so that I will have a column called [contained] that would show the list of indexes where the current index is contained? for example on the first line it would have on the [contained] column "IDX_ItemNoLanguageID", which is the name of the index on the 8th row. 

How can I calculate the cost of having this indexed view? The overload on inserts/updates/deletes? The stats that I could look at so that I can decide if is it worth having it? 

I play with the data, in order to test the results later on, I will change some rows to NULL and some rows to empty 

the permission I explicitly granted but the other permission was not explicitly granted. I believe it is automatically generated when you create a login to the server, and a user for that login on any database that live there. that implies a connection. considering this, is there any specific reason why I should grant the CONNECT SQL at servel level? 

for replicating an specific set of data, you can create a view and replicate it. here you can see what data you can publish there is a small detail though: 

I have just installed SSAS on a server called SASBIO1, but when I try to run the script below, in order to grant Administrator rights to a specific AD group I get the error message: 

My complete implementation, working on my test environment is below. I have left everything there, select queries and procedures, to remind myself, where to go to get the data to be used as parameters to the sp_MSstartdistribution_agent and sp_MSstopdistribution_agent stored procedures. 

and inside the procedure, after all the calculations, when I am returning the final data (below an example) I check whether we are outputting to a table or just back to the screen and create the script dynamically. 

My problem is that I cannot retrieve the whole content of my script. I have tried to copy it using the mouse as you can see on the picture below: 

But I have some hairy scripts that are very long, and copying like that cuts them short of their whole. my question is: How to retrieve this from my table? it can be via select or even saving it to a file, I would prefer a T-SQL way of doing it if possible. Just a curiosity would that be the same way for images? 

then I connect to the distributor you got above and run the following query to find out what is going on. 

have you isolated or considered different filegroups for demainding objects\indexes? Have you checked the current available disk space in all drivers, specially those used in this DB creation? how big is the transaction log and why? Initially I like to set it 1.5 times the biggest clustered index. what are the autogrowth settings? what are the current permissions? 

what if I wanted as well the number of , , and ? How would I go around that? I think the exact calculation would need to have a reference to the number of seconds happened. Let's say for instance I am interested in how long a sql server job has been running for. I have jobs within the replication category that have been running for years, and the starting point is known. When not specified we could assume the number of seconds are up to . 

I would like to drop all the jobs related to a set of specific databases. So I have added all the names into a table variable and I was trying to generate the scripts to drop the jobs. how can I achieve this? 

I know... I did not like it either, but it would not work with me using EXECUTE AS. 3) MAYBE if both my login and user were domain accounts, would this make a difference? I don't want to add any database ownership chain, or trustworthy in order for this work, unless really necessary. also: we are still on sql-2005 on these test machines: Microsoft SQL Server 2005 - 9.00.5000.00 (X64) Dec 10 2010 10:38:40 Copyright (c) 1988-2005 Microsoft Corporation Developer Edition (64-bit) on Windows NT 6.0 (Build 6002: Service Pack 2) 

The script generated should be run in the publisher (server and database) this is the picture from the replication monitor: note on the right hand side before there were 2 lines, one with a red error cross on it, that was the subscription we wanted to delete and now it has been deleted: 

I have a procedure SP_MYPRO that updates data in MYDB1. 1) the performance of SP_MYPRO would be different if I place it in MYDB2 instead of MYDB1? 2) what if the procedure SP_MYPRO only read data, does not update anything? 

I get the error below: Msg 3930, Level 16, State 1, Line 11 The current transaction cannot be committed and cannot support operations that write to the log file. Roll back the transaction. 

this is the same value as you can see on the picture below: you get there by sql server server, right click properties,connections. 

as you can see in the query plan here and on the picture below, it uses partition elimination so I know I am doing something right. 

The way to run ssms as a different user is: 1- right clink on the ssms icon if it is already running otherwise follow from 2 2- witht eh shift key pressed right click the ssms icon and it will give you the options as shown on the picture below. 

After that you have either the info you need on the screen, or if you have passed a temp table as a parameter, it will have the data now. this is one solution I found, but I only use it for my own works otherwise this will be considered high risk for Sql Injection. 

I checked which .Net versions I had installed then I installed .NET 3.5 and all started to work nicely. 

It does not show on the replication monitor, therefore running the internal procedures to get hold of what has been read from the transaction log and replicated to the distributor I use the following procedure: 

that is all working. But when I try to apply the link below to save my varchar(max) to a file, I can't make it work. Using BCP to export the contents of MAX datatypes to a file following the procedure shared by Phil Factor, he shows in that link an example as how to save a to a file, but that is not working for me, it does not show any error message nor exception though. the code that comes out to be executed in sqlcmd is: