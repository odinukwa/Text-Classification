A core switch is a good idea but two core switches are better. In an ideal world all of your network paths would N+1 and this would include the core switch. You can then connect all of your distribution switches to your core switches, and your existing Catalyst 2921 to them. Again, unless you have a specific need for 10Gbps connections between your core switch and your distribution switches - I'd look at spending your $5000 dollars on two smaller switches to fulfill the roll of a network core. You should also endeavor to link your new office switch directly to the core and not cascade it through an existing distribution switch. Again, this is just good network hygiene. You don't want the loss of one switch to effect all of the switches that are downstream of it. And finally, unless you very attached to the idea of a core switch, you might be better of not installing a core switch/s and spending that budget on upgrading your existing Netgear switches to something that is fully managed. I realize this sounds like contradictory advice but which option is best is largely dependent on things I don't know, like possibility for future growth, available budget, acceptable network downtime, bandwidth usage, and so on. 

At the beginning of the year we performed an upgrade of our vSphere environment from vSphere 5.0 to vSphere 5.1 U1 Build 1063329 compromised of about a dozen ESXi hosts and a vCenter instance hosted on Windows Server 2008 R2 SP1. One of the outstanding issues from this project is the upgrade of Virtual Hardware for Virtual Machines. I'm having trouble understanding why I should go to work and downtime to upgrade the Virtual Hardware version on all of our VMs. Our newly created Virtual Machines are using Virtual Hardware v. 9, the most recent supported version in vSphere 5.1 U1 which resolves issues we were having with Windows Server 2012 R2 and WinPE 4.0 on our older vSphere 5.0 instance. All of our older Virtual Machines are compatible Virtual Hardware versions (KB2007240) so we are not forced to upgrade their hardware version. Am I missing some technical reason for upgrading all our Virtual Machine's Virtual Hardware to the "newest" Version 9 since guest operating system and ESXi compatibility are not issues? Upgrading the Virtual Hardware is not necessarily trivial since I have to shutdown the VM, take a snapshot or backup of it and then upgrade it for a few hundred VMs. Other than avoiding having to do this in the future and getting warm fuzzies that all our VMs are running on the most recent Virtual Hardware version why should I bother doing a straight cutover instead of a rolling upgrade as we replace our Virtual Machines? 

It depends on the method of load balancing you are using. If you're just simply doing round-robin DNS, then yes your worker-servers will be making their connections "directly" with their clients. If you're actually using a reverse proxy such as HAProxy, then no. Your worker-servers will use the load balancer as an intermediary. From the perspective of your worker-servers the client is the load-balancer, not the actual client. If you're concerned with the creation of Single Point of Failure by using a reverse proxy in this way there are strategies you can employ to mitigate some risk such as using redundant proxies. Details will vary based upon your implementation. 

Well that's not good. It looks like the WSUS site has magically migrated back to its standalone site because... FUN! If the SCCM SUP is looking for WSUS on 80/443 and it's no longer there no wonder it doesn't work. If I look at the registry key () that WSUSUtil.exe is manipulating I see that it still thinks WSUS should be running on 80. Maybe I just need to run more than once for extra... FUN? 

I could not figure out a way to make the Windows Server DHCP implementation act as "authoritative"; the behavior I wanted is when clients with leases from the old Debian-based DHCP server sent their DHCPINFORM packets to the new Windows Server, I wanted those clients to receive a DHCPNAK and the go through the whole process again to get a lease, thus "re-populating" the addressing space from .11 and up.... regardless, continuing on. 2) I cheated by expanding the Exclusion range on the new Windows DHCP server to include 192.168.61.100 - 192.168.61.199. This will force any clients who were assigned an IP in that range by the Debian-based DHCP serve to have their DHCPINFORM denied and then issued a new lease at the "bottom" of the addressing space (.11 and greater). 3) At this point I simply turned the Debian DHCP server off, and the Windows Server on and let the expiry time sort things out. Because of the "deny all clients" line in my dhcpd.conf, there were no clients with "old leases" in the .11 - .40 addressing space that could cause an IP conflict, and because of the Exclusion of .100 - .199 ranges all DHCPINFORM requests were denied (at least I imagine that's what happened... I didn't bother looking at the transaction using a packet sniffer... I probably should of) and the addressing space was re-populated starting from the lower bound of .11. 

The method to making these setting persistent is dependent upon your particular distribution of Linux. 

The difference between 0.0.0.0 and the loopback address 127.0.0.1 is that the loopback address is designed to allow a fully functioning IP interface within the host itself, regardless of what the rest of the networking setup, if any, looks like. Any traffic sent to the loopback device is immediately received on it. It's not so much that the loopback network "refers" to your own host... it's more of like you have a mini network segment in your host that devices, processes and sockets and can open and connect to. 

It might be that I am just tired but I am having a hard time really understanding what this means. My read of the documentation is that this Volume's snapshot reserve is completely full and the snapshots have spilled over into the unused space in the portion of the Volume that is allocated for normal use (How Snapshot copies and snapshot reserve use space in a volume). I have a few questions about this condition: 

Is there a reason I should use port 22 instead of 1234? Because that's where the other 99% of SSH services listen and people will expect it to be so. Convention isn't a hard and fast rule but you'll find that most administrators expect to see SSH on port 22, HTTP on port 80 and so on. Unless you have a very good reason to deviate from the established convention - I suggest you don't. Nine times out of ten, some one much smarter than you (or I) picked the default settings for a good reason and unless you have a better one why change it? (It's really annoying to have to spend your first day on the job nmap-ing everything to try to figure out which "non-conventional" ports the previous admin/consultant/BOFH decided to run everything on).