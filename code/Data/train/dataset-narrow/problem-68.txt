What you are missing is, that in OpenGL's NDC space (i.e. clip space after division by ) all 3 coordinates are in the range $[-1,1]$. So is in the range $[-1,1]$, while your computation assumes a range of $[0,1]$. So let's take your computation but add an initial step to map $[-1,1]$ to $[0,1]$: 

It has been suggested in comments repeatedly, but noone felt the need to give a proper answer, so for the sake of completeness, a straight-forward and common solution to this problem might be to use a texture as lookup table, specifically a 1D texture that contains all the values of your function for the possible input range (i.e. $[0,360)$ / $[0,2\pi)$). This has various advantages: 

A problem is that you don't easily know the window space coordinates of the camera position, since it is in front of the near plane, but the transformation from $[0,1]$ to $[near,far]$ is non-linear. One way to overcome this would be to just unproject a point on the near plane (depth = 0) as origin and the difference to the unproject of that same point on the far plane (depth = 1) as the direction. 

But all this only reduces the amount of geometry that you are generating and sending over for drawing. It does not do anything for reducing the actual computation you do to generate that geometry (rather on the contrary actually). It is not clear what exactly it is that's causing your script to take too long or acquire too many resources but I also don't know anything about Javascript or Ajax or whatever specific web-stuff you're doing. 

Fortunately your scenario is rather simple, your camera is on a line directly perpendicular above and centered on the quad you're concerned about. So what you want is for the quad to fill the whole screen, I assume (more or less) exactly the whole screen, i.e. the size of your projected quad in window space is your entire viewport. The mathematics of this are rather easy if you understand how the perspective matrix is actually constructed and what it does and if you know some basic trigonometry. The matrix constructed with takes as viewing angle the field of view along the y-axis and is based on goold old . And if we take a look at the actual matrix, we can see that it transforms a $1$ high object sitting on the view axis at distance $1$ from the camera to a height of $\cot \frac{\alpha}{2}$ ($\alpha$ being your view angle). If you move it further away it obviously gets smaller and if you increase its height, it gets larger, so it's $\frac{h}{d}\cot\frac{\alpha}{2}$ for camera distance $d$ and object height $h$. We want that height to be equal to $1$ (the height, from the center, of the view plane in NDC space). So all you need to know in addition to that is the size of your square in the y-direction. We take half that height (because the square is centered) and multiply that by $\cot\frac{\alpha}{2}$ to get the distance it has to be from the camera. So if your square vertices have the y-coordinate +/-H and given your other values: $$Y-X = H\cot22.5°$$ $$Y = X+H\cot22.5°$$ $$Y \approx X+2.41421H$$ 

Then when you would like to draw the thing, you can write and algorithm to trace the arc going from the start to stop angle. If you want to do further clipping, you could recreate the arc as a chain of points. Be careful about whether the returned segment was clockwise or counterclockwise. 

From what I can see, it looks like screenPosition.x is in some sort of screen space and screenPositionX is in terms of the actual pixel buffer in units of pixels. My guess is that cropImage wants its inputs in units of pixels but when you calculate min_x, min_y, ... you are using screenPosition.x when you should really be using screenPositionX. Also, where does radius come from? Is it in world space, screen space, or pixel buffer space? Regardless of whether or not that fixes your problem, try to be more explicit in the future about your variables names to keep track of which space/units you are in. Having a screenPosition.x and a screenPositionX which refer to different things is generally a bad idea. 

I don't know how the clipping library you are using returns the clipped objects, but if I understand your question, you want a way to represent your circles that does not use much memory? If you are only using perfect circles (and not ellipses) you could represent each arc segment as simply an origin point, a radius, and a start/stop angle. This is the part where it would be helpful to know what the clipping library returns. If it returns a chain of points as a separate object for each clipped arc segment, and you already know the radius and center of the circle (from before clipping), you could take the first and last point in the returned chain and compute the angle for each. 

If the the original polygon was convex then the new one should be as well which means you could triangulate it or whatever you need before you draw. A line which forms an edge of the clipping rectangle (if you extend it to be infinitely long) splits the entire space into 2 half spaces. You will need to provide a function to check whether a vertex is in the "inside" half space or the "outside" half space. Ex: left edge of rectangle is at x = -15. To check if the point is outside, do (point.x < -15). You will also need to provide a function to intersect 2 vertices. You have one vertex on the outside and one on the inside and somewhere in between there is a line that you want to clip against. For example if the left edge of the rectangle is at x = -15: 

The problem is, doesn't do what you think it does. Using actual double-precision vertex attributes and performing double precision computations is a very modern hardware feature (GL4/DX12) and is different from specifying the type in the good old call. This always worked and does nothing else than specify the input type, i.e. the type of your vertex data in memory. All it does is make the GL convert your doubles into singles and feeding them into a normal single-precision attribute (and failing when there is none), same with other input types like or . When specifying data for genuine double-precision attributes, you have to use (note the L) instead. The same holds for genuine integer attributes (/), which use . 

This is not true, $J^+J$ is not necessarily $I$. $JJ^+=I$ but the multiplication by the pseudoinverse is not commutative. Let's look at this in more detail: $JJ^+ = JJ^T(JJ^T)^{-1}= (JJ^T)(JJ^T)^{-1} = I$ $J^+J = J^T(JJ^T)^{-1}J = ?$ The underlying misunderstanding comes from the assumption that the matrices $J^T(JJ^T)^{-1}$ and $(J^TJ)^{-1}J^T$ are equal (since the latter would produce $I$ if right-multiplied by $J$), which is not true. They are both pseudoinverses of $J$ but they are not necessarily the same matrices, in fact they don't necessarily both exist. As the book says, the $(JJ^T)^{-1}$ only exists if the rows of $J$ are linearly independent, and in turn Wikipedia says that $(J^TJ)^{-1}$ exists if the columns of $J$ are linearly independent. However, they can't both exist if the matrix is not square, since one of the dimensions would necessarily have more vectors than the dimension, which can't all be linearly independent. Specifically, for a normal redundant joint system you probably have more columns than rows, making only the pseudoinverse $J(JJ^T)^{-1}$ exist. We can, however, see that for a square matrix they are indeed the same and actually the real inverse: $J^T(JJ^T)^{-1} = J^TJ^{-T}J^{-1} = (J^{-1}J)^TJ^{-1} = I^TJ^{-1} = J^{-1}$ $(J^TJ)^{-1}J^T = J^{-1}J^{-T}J^T = J^{-1}(JJ^{-1})^T = J^{-1}I^T = J^{-1}$ (But in that case the system would have a unique solution and there's no point to angle control anyway.) But you cannot resolve the brackets with a non-square matrix, since the individual matrices aren't square and don't have an actual inverse. The only situation when both matrices exist are square $J$s and then they are indeed equal, but then there's also no point to angle control. But I understand why you are confused and I am a little, too, since Parent actually seems to use the alleged identity $J^T(JJ^T)^{-1}=(J^TJ)^{-1}J^T$ in equation 5.20, where he derives the pseudoinverse's relevance for solving the inverse kinematics problem. He computes everything with the left inverse but than concludes with using the right inverse. I guess that works conceptually because they're both pseudoinverses, even if it doesn't make complete mathematical sense (to me and you at least). He seemed to have used the assumption of a square matrix as a trick for transforming the problem and then used the conclusion for a non-square matrix, which might be conceptually correct. I guess it simply doesn't matter which of the two pseudoinverses we use, since only one of them ever exists (for a non-square Jacobian) anyway and if the left inverse would exist (making your above assumption true), that means you have more constraints than DoF and thus your system is overconstrained and there's no point in angle control. 

You will also need to linearly interpolate all other attributes you care about (like texture coordinates). If you want to be really slick, you can write the clipping algorithm once and provide an appropriate isOutside and intersect function for each run. 

Imagine you want to represent transformations using matrices. Points could be stored as $$\begin{bmatrix}x\\y\end{bmatrix}$$ and you could represent a rotation as $$\begin{bmatrix}u\\v\end{bmatrix}=\begin{bmatrix}cos(\theta)&-sin(\theta)\\sin(\theta)&cos(\theta)\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}$$ and scaling as $$\begin{bmatrix}u\\v\end{bmatrix}=\begin{bmatrix}k1&0\\0&k2\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}$$ These are known as linear transformations and they allow us to do transformations as matrix multiplications. But notice that you cannot do translations as a matrix multiplication. Instead you have to do $$\begin{bmatrix}u\\v\end{bmatrix}=\begin{bmatrix}x\\y\end{bmatrix}+\begin{bmatrix}s\\t\end{bmatrix}$$ This is known as an affine transformation. However this is undesirable (computationally). Let R and S be rotation and scaling matrices and T be a translation vector. In computer graphics, you may need to do a series of translations to a point. You could imagine how tricky this could get. Scale, translate, then rotate and scale, then translate again: $$p'=SR(Sp+T)+T$$ Not too bad but imagine you had do this computation on a million points. What we would like is to represent rotation, scaling, and translation all as matrix multiplications. Then those matrices can be pre multiplied together for a single transformation matrix which is easy to do computations with. Scale, translate, then rotate and scale, then translate again: $$M=TSRTS$$ $$p'=Mp$$ We can achieve this by adding another coordinate to our points. I'm going to show all this for 2D graphics (3D points) but you could extend all this to 3D graphics (4D points). $$p=\begin{bmatrix}x\\y\\1\end{bmatrix}$$ Rotation matrix: $$R=\begin{bmatrix}cos(\theta)&-sin(\theta)&0\\sin(\theta)&cos(\theta)&0\\0&0&1\end{bmatrix}$$ Scale matrix: $$S=\begin{bmatrix}k1&0&0\\0&k2&0\\0&0&1\end{bmatrix}$$ Translation matrix: $$T=\begin{bmatrix}1&0&t1\\0&1&t2\\0&0&1\end{bmatrix}$$ You should work out some examples to convince yourself that these do in fact give you the desired transformation and that you can compose a series of transformations by multiplying multiple matrices together. You could go further and allow the extra coordinate to take on any value. $$p=\begin{bmatrix}x\\y\\w\end{bmatrix}$$ and say this this homogeneous (x, y, w) coordinate represents the euclidean (x, y) coordinate at (x/w, y/w). Normally you cannot do division using matrix transformations, however by allowing w to be a divisor, you can set w to some value (through a matrix multiplication) and allow it to represent division. This is useful for doing projection because (in 3D) you will need to divide the x and y coordinates by -z (in a right handed coordinate system). You can achieve this by simply setting w to -z using the following projection matrix: $$Q=\begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&-1&0\end{bmatrix}$$ 

You could try clipping the polygons in code before you send them off to the renderer. Then you could avoid the stencil buffer entirely. To do this you could clip against all sides of the rectangle one at a time. I can't say for sure if this is faster than what you were doing before but here goes: For this algorithm you need to represent a polygon as a list of points How to clip against one side: 

It just depends on which space your light directions are actually specified in. You compute both your normal and view vector correctly in view space in order to perform your lighting computations in view space. So you need your normalized light direction in view space, too. So it depends on how you actually transfer your light direction into the shader. Usually it's best to transform the light direction into view space before uploading it to the shader (from whatever space it's originally in in the scene, likely some kind of object space local to a certain light source, but in case of a directional light maybe already in world/view space). This has a few advantages: 

This clearly contradicts my practical experience and made me a little worried/confused. I took a look at the specification (both the newest 4.6 core as well as good old 2.0, which we're actually targetting in our project for compatibility) and it also says that occlusion queries count the number of samples passing the depth test. It does however not make such a definite statement as the wiki pertaining to the stencil test. And since the depth test technically comes after the stencil test, I'm not entirely sure if that maybe already includes the stencil test anyway and the wording of the specification is simply using the last stage for convenience. So, my question is, do occlusion queries respect the stencil test (or any other earlier per-fragment tests, I'm actually using scissor testing with occlusion queries in another unrelated algorithm)? I see a few possiblities here: 

You seem to do things more complicatedly than you need to, as adressed in the comments. What you actually want to do is simple the opposite of the normal render flow. Rather than drawing your mesh in world space and mapping the texture into it, you draw it in texture space and map its vertices onto it and let your graphics hardware worry about interpolation. I'll give the general outline here, as I honestly don't know the actual DirectX terms for the stuff here, but it should be easy to map onto real HLSL if you know your way around DirectX (which I'm sure you do when messing with Compute Shaders). You basically render into your output texture using an offscreen render target. You then render your mesh using its texture coordinates as positions rather than its model space vertices, which you just pass through to the pixel shader, where you finally transform them into screen space and write out as the pixel's colour: