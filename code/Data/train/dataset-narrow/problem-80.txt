"the information that $f(l_k,v)$ carries is low-frequency enough": As IneQuation explains, low-frequency was used to refer to the detail of the brdf function. I did actually mean that $f_r$ was low frequency though (which is the case with diffuse lighting), not $L_i$. "with respect to $L_i$": what this means it that, since there aren't any large peaks in $f_r$, no part of $L_i$ will be wheighted much more heavily than any other in the integral. If each function was, say, reciprocal of the other (meaning in this case that the peaks in each one would cancel out when multiplied by the other) the correct integral would be very different to the separated integral where this cancelation does not occurr. but since the two functions are not correlated (or they shouldn't be) this difference is likely to be fairly small. Also, if you use a really high frequency BRDF that takes into acount only incident light around the normal and you separate it, you will get something that's way off. in this example the estimate using separation is 4x larger that the correct result: $URL$ Experimental results can be generated easily. here is an example: $URL$ it shows a low frequency brdf $f_r$ and a high frequency incident light $L_i$ function being multiplied before and after integration. the ratio between the correct result and the one obtained using separation is of about 1.6 Addendum regarding your latest comment on my other answer: $$\frac{1}{N}\sum^{N}_{k=1}f(L_k) = \sum^{N}_{k=1}(f(L_k)\cdot \frac{1}{N})$$ Here we have "moved" or distributed $\frac{1}{N}$ into the sum. This formulation is equivalent but looks a bit more similar to an integral. if we set $\Delta L = \frac{1}{N}$ then, we can get $$\sum^{N}_{k=1}f(L_k)\Delta L\approx \int_{L}f(L)dL$$ keeping in mind that the separation that occurs is, conceptually, separation of integrals, and that every integral "needs" its own $dL$. it follows that every montecarlo estimator needs its own $1/N$ I hope this helps you understand where the extra$\frac{1}{N}$ comes from 

I really hope this information is of use in solving your issue, and good luck! Great to see new users on the site. 

If you have this data in a 3d array where a block is a 1 and no block is a 0 you can apply a 3d blur and then set all voxels that are left with a value higher than 0.5 to 1 and those that arent to 0. this will smooth out sharp details and leave flats mostly uneffected 

I do not have a derivation but i can explain the reasoning behind its usage. On the left side is a (monte carlo) estimator for the rendering equation. It states the very same thing that the rendering equation states but on a finite number of discrete samples. on the right side we have the $L_i(l_k)$ factor separated and "integrated" on its own. This approximation holds true when $L_i(l_k)$ is a constant, since $\sum_{k}f(k)\times n = n \times\sum_{k}f(k)$ - and, likewise, $\int_{k}f(k)\times n\space dk = n\times\int_{k}f(k)dk$ - when $n$ is a constant. $L_i(l_k)$ can be though of the colour of the incoming light and $\int_{k}f(l_k,v)dk$ can be thought of as the intensity of the reflected light. multiplying those together you get that now you can precompute $\int_{k}L_i(l_k)dk$, saving a lot of time in either texture lookups (in the case of sky / environment lighting / screen space ray marching) or raytracing calculations. However, since $L_i(l_k)$ is usually not a constant, this integral is not separable. Yet, in some cases (like when dealing with diffuse lighting or, more specifically, diffuse ambient lighting), the information that $f(l_k,v)$ carries is low-frequency enough that it can reasonably be approximated as a constant with respect to the distribution of $L_i(l_k)$. Experimentally, it can be shown that the final result is not too different from what you would expect. 

When rendering an image you are trying to find the outgoing light from a point into the camera $L(x \rightarrow \Theta$). To do this you solve "The rendering equation", which means you integrate the product of the brdf and the incoming light for every incoming direction. $$L(x\rightarrow\Theta )=\int_\Omega f_r(x, \omega\rightarrow\Theta)\cdot L(x\leftarrow\omega)d\omega$$ Since you don't have the outgoing light you cannot use the definition of the brdf to calculate it. $$f_r(x,\omega\rightarrow\Theta )=\frac{L(x\rightarrow\Theta)}{E(x\leftarrow\omega)}$$ Instead, you use a formula to compute $f_r(x,\omega\rightarrow\Theta )$ and some operation to find the incoming light $L(x\leftarrow\omega)$ (a raytrace, a env map sample, etc.), and use those to compute the outgoing light $L(x\rightarrow\Theta)$. These two calculations are usually done in a completely separate way. So there is no way the incoming light can effect the brdf in any way. 

Notch's engine most likely works using volume raymarching in a volume field. This means that you shoot rays that move a certain distance and check whether they are inside and object or not. Once they are, they return the position of the colission and some other data. You can either advance rays by a set amount per step until you hit something or refine the final collision position by doing a binary search along the last walked ray segment once you hit something. Both of these aproaches have the problem that they can overshoot, missing small geometry, or take a large amount of steps to traverse empty space where there is no geometry to hit. Binary search can add precision and reduce overshoots somewhat but comes at an extra runtime cost for doing a binary search per ray. Signed distance fields are essentially 3d arrays tell you how far away from any surface you are. This allows you to do raymarching with ray step sizes equal to that number and being sure that you wont overshoot, as well as stepping through large empty spaces quite quickly. This greatly enhances performance and reduces overshoots from a regular step size or binary search based approach. This last technique trades off runtime speed for precompute time and extra memory. So it is not always adequate for rendering exteriors or large scenes. Its memory usage an runtime speed can be further improved by using a hierachical data structure that gets re built every so often with more detail and data density near the camera and less further away from it. EDIT: Oh it seems i misread density for distance I really have no idea what a signed density field is. I've never heardthe term before. A quick goolge search throws results that talk about signed distance fields. I would guess it was either a mistake to write that, some obscure tech or something that Notch made up himself. 

Consider the following scheme that mimicks the generation of a sample for phong shading. where V is the viewer direction, N is the normal, R is the specular reflection direction and $S_i$ is some sample. This space (commonly called tangent space) uses the normal as one of its basis vectors. you can think of it as the 'Z axis' 

$\phi$ is a number that is sampled uniformly in the range $(0,2\pi)$ this means that rays will be generated around the reflected direction with no bias towards any particular direction. Generated rays will, however, be biased to have a direction close to $R$ due to the way that $\theta$ is sampled. When we generate rays in a backwards pathtracer we use the view direction to get the directions where the light is likely to come from, instead of throwing light around and seeing where it falls. The later technique is closer to photon mapping. 

After some testing and some research I have found that both the $\sin(\theta)$ and the $\cos(\theta)$ should be removed from the denominator. the reasoning is that, since we want a probability function of steradians instead of angles, the $\sin(\theta)$ factor (which is essentially used to convert from steradians on a unit sphere to radians on the two polar axis) should be dropped. Also, $\cos(\theta)$ (as Nathan Reed explains in this blog post) is there to convert from microfacet area to macrosurface area. Since we are generating microsurface samples, this should be removed. After these corrections we get the formula $$L_o(p,\omega_o)\approx\frac{1}{N}\sum_{i=1}^{N}\frac{L_i(p,\omega_i)FG}{4(n\cdot\omega_o)}$$ which is free of the artifacts of the formula on the question.