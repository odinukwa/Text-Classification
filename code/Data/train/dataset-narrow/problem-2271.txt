Tarjan showed that the classical disjoint set forest data structure with the union by rank and path compression heuristics takes $O(\alpha(n))$ time per operation, where $\alpha$ is the inverse Ackermann function. Fredman and Saks showed this is optimal: for any data structure there exists a sequence of $n$ union and find operations which must take $\Omega(n\alpha(n))$ time. 

The linear programming problem is solvable in (weakly) polynomial time. This seems very surprising: why would we be able to find one among an exponential number of vertices of a high-dimensional polytope? Why would we be able to solve a problem which is so ridiculously expressive? Not to mention all the exponential-size linear programs which we can solved by using the ellipsoid method and separation oracles, and other methods (adding variables, etc.). For example, it's amazing that an LP with an exponential number of variables such as the Karmakar-Karp relaxation of Bin Packing can be efficiently approximated. 

You can get references in the book chapter by Davenport, Duarte, Eldar and Kutinyok: look at the section Measurement Bounds on p.23. The result is that if an $m\times n$ matrix has the RIP property of order $k$, i.e. every submatrix of $k$ columns is almost an isometry as a linear operator, then $m = \Omega(k\log(n/k))$. 

Steps 1 and 3 are basically arithmetic based on the index of the clause of $f(x)$ and the index of the tableau cell it corresponds to. Both these indexes can be written in $O(p(|x|))$ bits and steps 1 and 3 therefore can be done in time polynomial in $|x|$. Since any function computable in polynomial time can be computed using a polynomial size circuit, we have poly size circuits for 1 and 3. For step 2 we need to just hard-wire $O(|x|)$ 3CNF terms into the circuit. Put all these together, and we're done. 

However, there two things that you must realize with respect to these links. One is that it is not clear that range spaces of HAPs are very natural. When do you expect to have input that is a multiset of integers and want to answer how many elements of a HAP are in the input? I cannot think of a situation when this comes up, but maybe I am missing something. Another thing that you must realize is that all these applications rely on the notion of hereditary discrepancy. This notion is more robust than discrepancy which makes it more tractable: there are stronger lower bounds available for it, it is approximable to within polylogarithmic factors, and it is approximately equal to the value of a convex optimization problem. The result Kunal talks about in the blog post (paper is here) and the construction by Alon and Kalai that Kalai wrote about in this blog post together essentially settle the hereditary discrepancy of HAPs. As Kunal explains, intuition for the lower bound on the hereditary discrepancy of HAPs came from the tight connection between hereditary discrepancy and differential privacy, together with prior results in differential privacy. However EDP is about the discrepancy of HAPs. Discrepancy is much more brittle than hereditary discrepancy, and that makes it harder to lower bound. This also makes it less useful in applications than hereditary discrepapncy. And this is why EDP is still wide open while the hereditary discrepancy question is fairly well understood. Let me finish with one approach to attack EDP that is inspired by computer science ideas. There is a way to relax discrepancy to a semidefinite program, see the survey by Bansal for details. The optimal value of the semidefinite program is lower bounded by the value of any feasible solution to its dual program. So one can attempt to prove EDP by exhibiting a family of dual solutions to this semidefinite relaxation of discrepancy, and showing that the value of the dual solutions goes to infinity. I see no reason why such an attack cannot work, in particular we do not know how to construct solutions to the semidefinite relaxation that have constant value for arbitrarily large instances. In fact a lot of the effort in polymath5 was centered around finding or ruling out dual solutions with particular structure. The discrepancy of arbitrary arithmetic progressions is $\Theta(n^{1/4})$ in the worse case: the lower bound is known as Roth's 1/4 Theorem. Lovasz gives a proof of the theorem using semidefinite programming (although by arguing directly about the primal rather than by constructing dual solutions). For the proof, see Section 5.2. of his SDP survey. 

Any language $L$ consisting of strings of length $2^k$ for nonnegative integers k has a succinct version: given a binary circuit $C$ that takes $k$ inputs and outputs a single bit, is the truth table (seen as a binary string of length $2^k$) a string of the language $L$. You can restrict any language to strings whose length is a power of two, or you can try other encoding tricks. 

The usual terms I know from the theory of posets and lattices are downset and upset (or upper set). Also as the empty set is a subset of every set, I think your first condition is vacuous as long as the system is not empty. Same for the first condition in the second definition. If a downset is closed under joins (in your case set unions), it is called an ideal. If an upset is closed under meets (in your case set intersections), it is called a filter. 

The complexity zoo is your friend! As Robin said, you have half the answer: any EXP-complete problem collapses NP to P, and therefore BPP to P. Buhrman and Fortnow constructed an oracle relative to which P = RP but BPP is not equal to P. This is more than what you asked for; I suspect there are easier constructions that separate P from both RP and BPP. 

Lower bounds in non-uniform models of computation, like boolean formulas and circuits, are proved using combinatorial arguments. Some examples are Krapchenko's method using formal complexity measures, Razborov's method of approximations for monotone circuits, the random restriction method, including random restriction + the switching lemma, and depth lower bounds using communication complexity via Karchmer-Wigderson games. You can find lecture notes on this material by Sudan, Kopparty, Buss, Zwick, among others. 

$$ \forall 1 \leq i \leq n: |qa_i - p_i| < \epsilon, $$ and $q< 2^{n(n+1)/4}\epsilon^{-n}.$ Dirichlet proved this without the $2^{n(n+1)/4}$ factor, but his proof is a pigeonhole argument (sometimes called Dirichlet's principle) and does not yield an efficient algorithm. 

If you want something more formal, you can think of a streaming algorithm as a Turing machine with read-once random bits tape and working tapes whose total size is polylogarithmically bounded. So, you can store and reuse random bits, as long as you don't exceed your polylog memory bound. From algorithms design perspective, designing and even understanding a streaming algorithm is usually a two-step process. First think of the streaming algorithm as a RAM that can magically reuse random bits, i.e. has access to a random oracle. Actually there are some papers that do propose algorithms in this random oracle model. But usually, the next step is to try to make the algorithm work with hash families with bounded independence rather than a true random oracle. The seed for such families is logarithmic in size, so you can afford to store it. For some fine examples of this, just check out the classical paper of Alon, Matias and Szegedy. Another, heavier hammer to use is Nisan's pseudorandom number generator, which also allows reducing the number of random bits needed to be stored down to polylogarithmic (but larger than you usually get from bounded independence). For an example of using Nisan's generator, look at Indyk's use of the Cauchy distribution to approximate the $\ell_1$-norm of a stream. Both papers are classic and good read. Refs: AMS: www.tau.ac.il/~nogaa/PDFS/amsz4.pdf Indyk: people.csail.mit.edu/indyk/stream.ps 

TL;DR The decimal expansion of a fixed rational number is not pseudorandom in the cryptographic sense, but irrational numbers (are conjectured to) exhibit some weaker but interesting forms of pseudorandom behavior. Roughly speaking, a sequence $s \in \{0, \ldots, B\}^n$ is pseudorandom with respect to distinguishers $\cal A$, if it cannot be distinguished (with non-trivial probability) from a uniformly random sequence sampled from $\{0, \ldots, B\}^n$ by any algorithm $A$ in a class of algorithms $\cal A$. Usually in cryptography, $\cal A$ is the class of polynomial-time algorithms or polynomial-size circuits. Then it is easy to see that any sequence that can be generated in polynomial time by a deterministic algorithm is not pseudorandom: the distinguisher just simulates the generator. For this reason, pseudorandom generators in cryptography are randomized, and we speak of a pseudorandom distribution rather than a fixed pseudorandom sequence. However, there are weaker notions of pseudorandomness (corresponding to more restricted classes $\cal A$ of distinguishers) that the expansion of an irrational number in an integer basis might satisfy. For example, $\sqrt{2}$, $e$, and $\pi$ are conjectured to be normal, which is a kind of pseudorandomness: each pattern of $k$ digits in base $B$ appears with frequency $1/B^k$, in the limit. This is like pseudorandomness with respect to distinguishers which can only ask "approximately how often does this pattern appear in the expansion of $\sqrt{2}$?" However, at the moment we are unable to prove normality for any of the above fundamental constants, in any base, even though it is conjectured they are absolutely normal, i.e. normal in every base. This is a famous open problem. Something that we understand better is sequences like $(k\sqrt{2} \bmod 1)_{k = 1}^\infty$ ($x \bmod 1$ just means the fractional part of $x$). By Weyl's ergodicity criterion, we know that for any Lebesgue integrable function $f$, the average $$ \overline{f}_n = \frac{1}{n-1}\sum_{k = 0}^{n-1}{f(k\sqrt{2} \bmod 1)}, $$ converges to the expectation $\mathbb{E}[f(x)]$, where $x$ is a uniformly sampled random number from $[0, 1]$. We also have deviation estimates: for $f$ the indicator function of an interval $I = [a, b]$, $a, b \in [0,1]$, the average $\overline{f}_n$ deviates from the expectation $b-a$ by $O((\log n)/n)$ in the worst case, and by $O(\sqrt{\log n}/n)$ on most intervals $[a, b]$ ("most" is measured by the natural product measure). Moreover, Beck has recently shown a central limit theorem, which quantifies the statement that numbers $n$ for which the deviation is large are very rare. For an intro to the pseudorandomness properties of these sequences, check section 2.3. of Chazelle's discrepancy book.