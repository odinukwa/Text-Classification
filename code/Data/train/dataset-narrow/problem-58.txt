I think it is fair to say that the reason there are so many niche variations of GLSL/HLSL/Cg/whatnot is because no programming language is nor will ever be a one size fits all tool. Different problems require different tools, so sometimes it is worth the effort of developing a custom built tool if it is going to pay off in the long run. Stock GLSL is by itself pretty much unusable. It hasn't even acquired much stability across versions, so for a program that targets more than one OpenGL version, some sort of preprocessing is a must. HLSL is a bit more stable across versions, but again, if targeting more than one D3D version, some work will need to done to get good portability. The kinds of things people usually do are pretty much what you said, like a adding support for basic programming features such as modules and uniform syntax across versions, or even portability across different APIs (GL/D3D) without having to rewrite the shader code. More sophisticated things include fully fledged material systems or things like generating shader programs on-the-fly. Shading languages will probably get better and more generic in the future, incorporating things that today are commonly hand-rolled as core features. The new GCN architecture is a sign of that. So shading languages will be more usable out-of-the-box a while from now, but custom built solutions will never go away because there's only so much you can generalize. 

Suppose you have a circle of radius $r$ (so its curvature is $1/r$), and you have two points on the circle, with their normals $n_1, n_2$. The positions of the points, relative to the circle's center, are going to be $p_1 = rn_1$ and $p_2 = rn_2$, due to the property that a circle or sphere's normals always point directly out from its center. Therefore you can recover the radius as $r = |p_1| / |n_1|$ or $|p_2| / |n_2|$. But in general, the vertex positions won't be relative to the circle's center. We can work around this by subtracting the two: $$\begin{aligned} p_2 - p_1 &= rn_2 - rn_1 \\ &= r(n_2 - n_1) \\ r &= \frac{|p_2 - p_1|}{|n_2 - n_1|} \\ \text{curvature} = \frac{1}{r} &= \frac{|n_2 - n_1|}{|p_2 - p_1|} \end{aligned}$$ The result is exact only for circles and spheres. However, we can extend it to make it a bit more "tolerant", and use it on arbitrary 3D meshes, and it seems to work reasonably well. We can make the formula more "tolerant" by first projecting the vector $n_2 - n_1$ onto the direction of the edge, $p_2 - p_1$. This allows for these two vectors not being exactly parallel (as they are in the circle case); we'll just project away any component that's not parallel. We can do this by dotting with the normalized edge vector: $$\begin{aligned} \text{curvature} &= \frac{(n_2 - n_1) \cdot \text{normalize}(p_2 - p_1)}{|p_2 - p_1|} \\ &= \frac{(n_2 - n_1) \cdot (p_2 - p_1)/|p_2 - p_1|}{|p_2 - p_1|} \\ &= \frac{(n_2 - n_1) \cdot (p_2 - p_1)}{|p_2 - p_1|^2} \end{aligned}$$ Et voil√†, there's the formula that appeared at the top of this answer. By the way, a nice side benefit of using the signed projection (the dot product) is that the formula then gives a signed curvature: positive for convex, and negative for concave surfaces. 

It is possible to do what you describe, but I'm afraid it is not a trivial process. Actually, this will be very tied to the Operating Systems you are targeting and possibly also requiring specific tweaks for the given game/application. I would start looking into DLL injection techniques, which should allow, for instance, intercepting calls to the Direct3D (Windows) or OpenGL APIs, which you can then use to copy de framebuffers of the application. A quick Internet search turns up this and this with detailed explanation. I once wrote a small-scale OpenGL interceptor on MacOS by injecting a custom shared module, but it was under very controlled circumstances and making a lot of assumptions, such as having root privileges in the system, so for a production tool, it would be a lot more complicated. You can find another very interesting project here about intercepting the D3D API to install an automated AI bot on Starcraft, which should touch on very similar concepts to what you are looking for. Once you manage to intercept the underlying rendering API, you could then perhaps hook your code into each or equivalent call to just copy the previous framebuffer before the swap, then save it (this is where something like would come into play). Copying and saving the framebuffer in an efficient manner is also challenging. The easy way is to just dump each frame as an uncompressed RGB image, but then you'll wind up with hundreds of gigabytes of data for just a couple of minutes of gameplay, so unless you have a nice HDD array sitting at the corner of your table , you'll need to look into compressing the frames somehow. The downside of compressing the frames now is that it takes a lot of processing, so a naive approach might turn the once interactive application you are trying to record into an interactive slide-show . So at this stage you'd have to start thinking about optimizations. Don't know of any projects out there providing a library for efficient frame capture for games, but this would certainly be something nice to have! I think one thing that might be holding such project back is, like I mentioned at the beginning, that this is a very system dependent thing, so cases of use will most likely be limited to a single platform or OS. 

Allocate a VBO with a reasonable amount of storage during startup, and update the data in it each frame. This will help the driver manage the memory efficiently. Don't destroy and recreate the buffer unless absolutely necessary; too much resource creation/destruction churn will put more pressure on the driver and can potentially lead to stalls. To be more specific: for data that will be updated every frame, you probably want to initialize it using glBufferStorage with (or, if you're on an older OpenGL version, glBufferData with ). Then, to update it each frame, use glMapBufferRange with . This should be an efficient, fast path in the graphics driver. 

It's unfortunate that people commonly recommend this. Blending between two (or four, etc.) translated copies of a noise function in that way is a pretty bad idea. Not only is it expensive, it doesn't even produce correct-looking results! On the left is some Perlin noise. On the right is two instances of Perlin noise, stacked and blended left-to-right. The difference is kind of subtle, but you can see that the second image has lower contrast in a vertical column running down the middle. That's where it's a 50% blend between two different instances of the noise function. Such a blend doesn't look like the original noise function: it just looks like a muddy mess. OK, so it's not quite that bad just looking at the raw noise...but if you then do any nonlinear transformations on the image, the nonuniform contrast can cause issues. For instance, here are those images thresholded at 60%. (Think of generating islands in an ocean, for instance.) Now you can plainly see how the image on the right has fewer, smaller white areas in the middle. Like you mentioned, for grid-based noise like Perlin, a better way is to tile the pseudorandom gradients at the grid points. That's easy and cheap to do, and then you can apply the interpolation algorithm to the gradients as usual (much like bilinear interpolation of a tiling texture). This produces tiling noise without any weird artifacts, because it works with the underlying noise algorithm rather than over the top of it. You can use a similar strategy with Worley noise (cellular noise) by tiling the random feature points it uses as a basis. With multiple octaves of noise it's not always so easy, though. If the relative scale between the octaves (aka "lacunarity") isn't an integer or simple rational number, then you may not be able to find a convenient tiling point where all the octaves' grids match up. You could tile each octave independently, but the overall noise would still not be tilable in that case. 

When applying multiple textures to a mesh, like for bump-mapping, I usually bind the textures to the first few fixed texture units, e.g.: diffuse = unit 0, bump = unit 1, specular = unit 2, then keep reusing those to each different mesh with different textures. But I've always wondered why supports so many texture units (in the previous link, it says at least 80). So it occurred to me that one possible way of managing textures is to bind distinct textures to each available unit and leave them enabled, just updating the uniform sampler index. That should improve rendering perf by reducing the number of textures switches. If you have less textures than the max texture units, you never have to unbind a texture. Is this standard practice on real-time OpenGL applications (I believe this also applies to D3D)? And are there any non obvious performance implications of taking this approach? Memory overhead perhaps? 

VT is still a somewhat hot topic on Computer Graphics, so there's tons of good material available, you should be able to find a lot more. If there's anything else I can add to this answer, please feel free to ask. I'm a bit rusty on the topic, haven't read much about it for the past year, but it is alway good for the memory to revisit stuff :) 

But we are still missing a key piece here, and that's how to determine which pages must be loaded from storage into the cache and consequently into the . That's where the Feedback Pass and the enter. The Feedback Pass is a pre-render of the view, with a custom shader and at a much lower resolution, that will write the ids of the required pages to the color framebuffer. That colorful patchwork of the cube and sphere above are actual page indexes encoded as an RGBA color. This pre-pass rendering is then read into main memory and processed by the to decode the page indexes and fire the new requests with the . After the Feedback pre-pass, the scene can be rendered normally with the VT lookup shaders. But note that we don't wait for new page request to finish, that would be terrible, because we'd simply block on synchronous file IO. The requests are asynchronous and might or might not be ready by the time the final view is rendered. If they are ready, sweet, but if not, we always keep a locked page of a low-res mipmap in the cache as a fallback, so we have some texture data in there to use, but it is going to be blurry. Others resources worth checking out 

// access part of a framebuffer's state. For reading the default framebuffer, this is or . However, the documentation doesn't say what it is for FBOs. The only thing I found was an offhand comment here stating that the default is GL_COLOR_ATTACHMENT0 (which is what I would guess). Can I rely on that? For writing, I didn't immediately find any claims what what color buffers are enabled. 

I've had great difficulty with a literature search on this. Here are some of the (few) papers I found: 

I've done some VR research; this comes up a lot since rendering the scene multiple times (especially at predicted VR resolutions) is expensive. The basic problem is that two views provide more information than only one. In particular, you have two slices of the light field instead of one. It's related to depth-of-field: screen-space methods fundamentally are incorrect. There has been some work in this area, most related to reprojection techniques that try to do some kind of geometry-aware holefilling in the final image. This sortof works. As far as I know, the best approach so far is to render the scene directly for the dominant eye, and then reproject it to the other one. 

Where does "face" fit in? Is my "layer-face" description correct? Or maybe it refers to all mip levels instead? Is my description of a "layer" correct? Similarly, does a "mipmap chain" refer to all mip levels of a single face, or to all mip levels of the layer? 

Overview The main reason for Virtual Texturing (VT), or Sparse Virtual Textures, as it is sometimes called, is as a memory optimization. The gist of the thing is to only move into video memory the actual texels (generalized as pages/tiles) that you might need for a rendered frame. So it will allow you to have much more texture data in offline or slow storage (HDD, Optical-Disk, Cloud) than it would otherwise fit on video memory or even main memory. If you understand the concept of Virtual Memory used by modern Operating Systems, it is the same thing in its essence (the name is not given by accident). VT does not require recomputing UVs in the sense that you'd do that each frame before rendering a mesh, then resubmit vertex data, but it does require some substantial work in the Vertex and Fragment shaders to perform the indirect lookup from the incoming UVs. In a good implementation however, it should be completely transparent for the application if it is using a virtual texture or a traditional one. Actually, most of the time an application will mix both types of texturing, virtual and traditional. Batching can in theory work very well, though I have never looked into the details of this. Since the usual criteria for grouping geometry are the textures, and with VT, every polygon in the scene can share the same "infinitely large" texture, theoretically, you could achieve full scene drawing with 1 draw call. But in reality, other factors come into play making this impractical. Issues with VT Zooming in/out and abrupt camera movement are the hardest things to handle in a VT setup. It can look very appealing for a static scene, but once things start moving, more texture pages/tiles will be requested than you can stream for external storage. Async file IO and threading can help, but if it is a real-time system, like in a game, you'll just have to render for a few frames with lower resolution tiles until the hi-res ones arrive, every now and then, resulting in a blurry texture. There's no silver bullet here and that's the biggest issue with the technique, IMO. Virtual Texturing also doesn't handle transparency in an easy way, so transparent polygons need a separate traditional rendering path for them. All in all, VT is interesting, but I wouldn't recommend it for everyone. It can work well, but it is hard to implement and optimize, plus there's just too many corner cases and case-specific tweaks needed for my taste. But for large open-world games or data visualization apps, it might be the only possible approach to fit all the content into the available hardware. With a lot of work, it can be made to run fairly efficiently even on limited hardware, like we can see in the PS3 and XBOX360 versions of id's Rage. Implementation I have managed to get VT working on iOS with OpenGL-ES, to a certain degree. My implementation is not "shippable", but I could conceivably make it so if I wanted and had the resources. You can view the source code here, it might help getting a better idea of how the pieces fit together. Here's a video of a demo running on the iOS Sim. It looks very laggy because the simulator is terrible at emulating shaders, but it runs smoothly on a device. The following diagram outlines the main components of the system in my implementation. It differs quite a bit from Sean's SVT demo (link down bellow), but it is closer in architecture to the one presented by the paper Accelerating Virtual Texturing Using CUDA, found in the first GPU Pro book (link bellow). 

Then unmap the buffer and issue the drawing commands, using a vertex shader to transform the points to screen-space as usual. 

This will do an okay job, but for higher-quality downsampling or resizing images to any size (not just neat integer factors), I suggest looking at the stb_image_resize library. It's quite easy to use, supports either 8-bit or floating-point images with any number of channels, and gives good results. 

First of all, you don't need position in the G-buffer at all. The position of a pixel can be reconstructed from the depth buffer, knowing the camera setup and the pixel's screen-space xy position. So you can get rid of that whole buffer. Also, you don't ordinarily need tangent vectors in the G-buffer either. They're only needed for converting normal maps from tangent space, and for parallax mapping; these would be done during the G-buffer fill pass (when you have tangents from the mesh you're rendering), and the G-buffer would only store normals in world or view space. Material properties like colors, roughness, and metallic are usually just 8-bit values in the G-buffer, since they're sourced from 8-bit textures. Same for AO. Height is also not needed in the G-buffer unless you're going to be doing some kind of multi-pass blending that depends on it, but if you do need it, 8 bits is probably enough for that too. Normals can be benefit from being stored as 16-bit values rather than 8-bit. Half-float is okay, but 16-bit fixed-point is even better, as it gives you more uniform precision across all orientations (half-float is more precise near the axes and loses some precision away from them). Moreover, you can cut them from 3 components down to 2 using octahedral mapping. So, at the end of the day, a minimal G-buffer might look like: 

Interpretation 1: Render an image that looks perceptually realistic. At the end of the day, your image still needs to be displayed somewhere. Here's the key: you want to render your image in such a way that when you *display* that image on a particular display device, it will produce the same sensation the original radiometric image would have produced. Here's how to unpack that idea. In the real world, radiometric spectra (i.e., real distributions of light) enter your eye and stimulate approximately1 four light receptors. The stimulations of the receptors produce the sensations of color we associate with images. In rendering, we don't have arbitrary control over the spectra we produce. Fortunately, since we (usually) have only three cones, each of which produces only a scalar value, color vision can be reproduced by using exactly three primaries. The bottom line is you can produce any color sensation by using a linear combination of three wavelengths only (up to a few colors that might have to be negative, in which case, you just use different primaries). You don't have a choice of primaries. Almost all color display devices use the sRGB standard, which provides three primaries (which actually usually don't have a single wavelength). That's fine because it turns out it's all abstracted and you don't have to care. To clarify the mess that is perceptually accurate rendering, here's the algorithm: