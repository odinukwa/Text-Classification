Given a quantum state $\rho_A$ chosen uniformly at random from a set of $N$ mixed states $\rho_1 ... \rho_N$, what is the maximum average probability of correctly identifying $A$? This problem can be turned into a two state distinguishability problem by considering the problem of distinguishing $\rho_A$ from $\rho_{B} = \frac{1}{N-1}\sum_{i\neq A}\rho_i$. I know for two quantum states the problem has a nice solution in terms of the trace distance between the states when you minimize the maximum probability of error rather than the minimizing the average probability of error, and I was hoping that there might be something similar for this case. It is of course possible to write the probability in terms of an optimization over POVMs, but I am hoping for something where the optimization has already been performed. I know there is a huge literature on the distinguishability of quantum states, and I've been reading through a lot of papers over the last few days trying to find the answer to this question, but I'm having trouble finding the answer to this particular variation of the problem. I'm hoping someone who knows that literature better can save me some time. Strictly speaking, I don't need the exact probability, a good upper bound would do. However, the difference between any one state and the maximally mixed state is quite small, so the bound would have to be useful in that limit. 

Valiant proved that the problem of finding the permanent of a matrix is complete for #P. See the wikipedia page on the issue. #P is the complexity class corresponding to counting the number of accepting paths of an NP machine. 

Great question. Actually, the same question popped up in something I was working on a few months ago (arXiv:1011.1217). It seems that any natural kind of decoherence leads to behaviour that looks initially balistic, but which becomes diffusive as time increases, so you are transitionioning between a $t$ regime and a $t^{\frac{1}{2}}$ regime. See figure 2 in the above paper for an example of this. This seems to be the natural behaviour as your state gradually loses coherence. This would seem to suggest that the variance only ever scales as $t$ or $t^2$, and hence the walk spreads as $t^{\frac{1}{2}}$ or $t$. However, exactly the same thing happens in quantum metrology when noise is introduced, but can be overcome to yield an intermediate scaling (see for example J. A. Jones et al, Science, 324, 5931 (2009), arXiv:1103.1219, arXiv:1101.2561, etc.). One way this can be achieved is by making intermediate measurements. Imagine you measure the position of the walker after every period of time $T$ collapsing the wavefunction, and allow free evolution in between. Now, imagine we want to evolve the system for total time $t=nT$. Then the variance in the position of the walker after this time will be $\mbox{Var}(x(nT)) = \sum_{i = 1}^n \mbox{Var}(x(T)) = n \mbox{Var}(x(T))$. In the absence of other decoherence we know the walker moves ballistically, and hence $\mbox{Var}(x(T)) = T^2$, and so the $\mbox{Var}(x(t)) = n T^2$. However, as $t=nT$, we can take $n\propto t^k$ and $T\propto t^{1-k}$. Thus $\mbox{Var}(x(t)) = t^{2-k}$. This way you can achieve any intermediate scaling, by chosing the measurement interval appropriately. 

UPDATE: (To answer the updated question regarding random walks on a grid or other lattice) One approach to the measurement issue you highlight with the spatial search problem is to simply make a measurement at each timestep such that it returns 1 iff the the vertex the walker is currently at (say $v_t$) is equal to $v^f$ and the current timestep t is the hitting time for that vertex. This should avoid the issue of collapsing the wave function, as the measurement is only made for each vertex once the hitting time is reached, and it only registers collapses onto a location if that location is the correct result. 

There are two quantum wikis which provide reasonably good list of research groups in QIP: Quantiki and Qwiki. Quantiki has better European coverage, while Qwiki has better US coverage. The geographic area I know best is the UK. In the UK there are large theory groups in Oxford, Cambridge, Bristol, University College London and Imperial College, among other places. In Oxford, where I have spent the last 5 years, QIP research is spread across a number of departments: Physics, Computer Science, Materials Science and Maths. There isn't much of a presence in Maths, although it is Artur Ekert's official affiliation. Computer Science has a growing group that mostly looks at category theory and quantum foundations. Physics has quite a number of different groups ranging from experiments to theory. Materials science is weirdly the department where I have been based (although I know little about materials) and there is both a theory group there and a fair number of experimentalists. Computer Science, Materials and Physics all have taught quantum computing courses which can be taken towards the course requirement of a DPhil. Hope this is useful. 

These kind of problems are in general very hard with the decission version of a problem often being NP-hard, so I doubt you will find an algorithm fast enough, unless your graph has some peculiarity which allows you to discard a large number of vertices. Since you restrict it to cliques of exactly 17 vertices then trivially there is an $O(n^{17})$ algorithm, but I'd hardly call this fast. To do this simply step through all subsets of 17 vertices in lexicographical order and check whether they form a clique. Note that this is exactly $\binom{n}{17}$ subsets. Note however, that for an input graph which is itself a clique, there are $\binom{n}{17}$ cliques of comprised of 17 vertices, and hence this trivial algorithm is optimal if we consider only worst case scaling, since simply reading a list of all such cliques takes $\binom{n}{17}$ steps. In order to do better than this, you need a graph with some structure. So for your specific case of $n=8568$ you potentially have $2 \times 10^{52}$ cliques of 17 vertices. Further since you have more than 12 million edges, this is enough for a maximally connected graph of 5000 vertices, which would have approximately $2 \times 10^{48}$ unique subsets 17 vertices which form cliques, so for the parameters you have listed it is entirely possible that you simply cannot enumerate cliques fast enough to give you a reasonable run time. 

Sure. The dependency sets arise from 'flow', which is indeed described in the paper you link to. This is, however, perhaps overkill for what we need. The idea behind the corrections is to insure that the same effective operator is applied independent of which branch you find yourself in after making a measurement. To do this in principle is fairly simple. Since all the measurements we make are in the XY plane, obtaining a 1 as the measurement outcome for a particular qubit $q$ of state $| \psi \rangle$ yields the same final state as as obtaining a 0 for the same measurement of the same qubit on a state $Z_q | \psi \rangle$. Thus to correct for obtaining a 1 rather than a 0 it is sufficient to find an operator $C$ on the output state such that $Z_q \otimes C | \psi \rangle = | \psi \rangle$. Now, this implies that $Z_q \otimes C$ is a stabilizer of the initial state. A stabilizer for a state is simply an operator which has that state as an eigenvector with corresponding eigenvalue $+1$. As it turns out, it is extremely easy to enumerate the generators of the stabilizer group for any graph: For every vertex $v$ in graph $G$ the operator $X_v \prod_{i\in\mbox{nbgh{v}}} Z_i$ is a stabilizer of the graph state, where $\mbox{nbgh{v}}$ denotes neighbours of $v$ in $G$. Thus in order to find the correction for the measured qubit it we can simply pick the stabilizer corresponding to a qubit neighbouring $q$ and multiply it by $Z_q$. This gives a set of $X$ and $Z$ corrections which, when applied to the output state, yield a state equal to the output of the process had the measurement result been inverted. We need one further requirement, which is that the correction set be in the future of $q$ (i.e. have not yet been measured). This obviously places restrictions on which neighbour of $q$ we choose. In the case of the brickwork state we introduce, this is satisfied uniquely by choosing $v$ to be neighbour of $q$ which is in the same row as $q$ but the next column on. This may sound arbitrary,but as it turns out, this is the unique choice satisfying the conditions I mentioned. Hopefully this answers your question. EDITED TO NOTE: You can propagate forward $Z$ corrections by applying the above procedure recursively, so that corrections on any qubit which is to be measured will be $X$ corrections. Whether or not an $X$ correction needs to be made to a particular qubit will then depend on the parity of the measurements for all qubits for which the correcting operator contains an $X$ at this location. To work out this set it is easiest to work the other way around: Simply calculate the correction operators for each vertex propagating all the $Z$ operators to the output qubits, and then once you have these operators work out which measurements alter the measurement at a given site.