To join two trees of equal height, first check to see if their roots are the same color. If so, sever from the left root its right-most child and from the right root its left-most child, then recursively join these two trees. The result will be a tree of the same height or one taller since the trees have the same flavor (see below). If the result of recursively joining the two trees has same height as the two severed children, make it the middle child of a root with the remaining children of the left root before it and the remaining children of the right root after it. If it is taller by 1, make its children the middle children of a root with the remaining children of the left root before it and the remaining children of the right root after it. If the roots have different colors, check to see if they have the same flavor. If they do, give them a new parent with the key and bit stream of the right root, eliding its first bit. If they do not, give each root a new parent with the key and bit stream of the old root (eliding each first bit), then recursively join those trees. There are two recursive calls in this algorithm. The first is when the roots have the same color, the second is when the roots have different colors and different flavors. The roots have the same color with probability $1/2$. The recursive call in this case always sees roots with the same flavor, so the second type of recursion never occurs after the first. However, the first can occur repeatedly, but each time with probability $1/2$, so the expected running time is still $O(1)$. The second recursive call happens with probability $1/4$, and subsequent recursive calls are always on trees with different colors, so the same analysis applies. To join two trees of unequal height, first trace down the left spine of the right tree, assuming the right tree is taller. (The other case is symmetric.) When two trees of equal height are reached, perform the join operation for two trees of equal height, modified as follows: If the result has the same height, replace the tree that was a child with the result of the join. If the result is taller, join the parent of the tree on the right to the root of the other tree, after it has been made taller by one by adding a parent for the root. The tree will be the same height with probability $1/2$, so this terminates in $O(1)$ expected. 

The Fractal Trees I have seen overviewed in Tokutek talks are Cache-Oblivious Lookahead Arrays (COLAs), which are at the intersection of two motivating storage layout ideas. If you're looking for similar layouts, you might be interested in them: 

By a famous theorem of Tarski, the first-order theory of real closed fields is decidable, as it admits quantifier elimination. Can this result be extended so that propositions can be quantified over functions on real numbers? That is, I know that there is a decision procedure for statements like $$\exists x \in \mathbb{R} ,\forall y \in \mathbb{R}, \exists z \in \mathbb{R}, x +y = zy$$ I'd like to know if there is a decision procedure for statements like $$\forall f \in \mathbb{R} \to \mathbb{R}, \exists g \in \mathbb{R} \to \mathbb{R}, \forall z \in \mathbb{R}, f(g(z)) = g(f(z)) = z$$ Undecidability results would be just as interesting! 

I am aware of three solutions to this problem that perform all operations in $O(1)$ amortized time. They all use multiplication. 

Yes! Use a two-level structure as discussed at the end of Section 2 of the Dietz and Sleator paper. For the top structure, use a scapegoat tree. By using a balance factor that can be implemented in $AC^0$ (like $2$), we get the result. See also exercise 8.12 from open data structures and Roura's "A new method for balancing binary search trees". 

A dynamic predecessor data structure supporting , , and over unique ordered keys in an unbounded universe in $O(\lg n)$ worst-case time can be stored contiguously using only $O(1)$ extra words of storage in addition to the storage for the keys themselves: "Optimal Worst-Case Operations for Implicit Cache-Oblivious Search Trees", Franceschini and Grossi The structure is complex (IMHO). Storing $n$ keys in an array that can grow dynamically requires at space for at least $\Theta(\sqrt{n})$ additional keys: "Resizable Arrays in Optimal Time and Space", Brodnik et al. Given the latter constraint, is there a simpler dynamic predecessor data structure that uses contiguous space for $n + O(\sqrt{n})$ keys? There were many implicit ($O(1)$ extra words) dynamic predecessor data structures that predate the result of Franceschini and Grossi: "Implicit Data Structures for the Dictionary Problem", Frederickson "Searchability in merging and implicit data structures", Munro and Poblete (does not support ) "Developing Implicit Data Structures", Munro I'm wondering if it is possible to simplify even these structures by using $\Theta(\sqrt{n})$ extra space. I am hoping to find a structure supporting all three operations in $o(n^\varepsilon)$ time. 

I think the best results so far are discussed in "Mantaining Dynamic Matrices for Fully Dynamic Transitive Closure". That paper discusses a randomized algorithm with $O(n^{0.58})$ query time and an $O(n^{1.58})$ update time. (This only covers the first version of your question, with and but without and .) 

It is not obvious to me how a slowly-moving $p(v)$ can keep nodes of height $\Omega(\log \log n)$ but $o(\log n)$ balanced. My intuition is that these nodes must be cared to more frequently than I can manage with the "one ancestor pointer" system. Any ideas on how to maintain $p(v)$? There has been some work on reducing the cost of updates at a known location in weight-balanced trees, but I'm not aware of any weight-balanced tree with $o(\lg n)$ worst-case updates. (Treaps are weight balanced and have $O(1)$ expected-time updates.) Mehlhorn's monograph is available electronically in an updated form that unfortunately leaves out III.9. 

Yes. Wegman and Carter's "New hash functions and their use in authentication and set equality" (mirror) shows a scheme meeting the requirements stated (almost universal, over $\mathbb{Z}_{2^b}$, sublinear space and randomness, linear evaluation time) based on a small number of hash functions drawn from a strongly universal family. This is sometimes called "tree hashing", and it is used in "Badger - A Fast and Provably Secure MAC" by Boesgaard et al. 

This is incorrect; see the comments. A function very close to this one was called "$\alpha^*$" and used in Pettie's "Splay Trees, Davenport-Schinzel Sequences, and the Deque Conjecture", in which he showed that "$n$ deque operations [in a splay tree] take only $O(n\alpha^*(n))$ time, where $\alpha^*(n)$ is the minimum number of applications of the inverse-Ackermann function mapping $n$ to a constant." This function is very slow growing, and is slower growing than $\log \alpha(n)$. Consider the function $f:\mathbb{N} \to \mathbb{N}$ $$ f(n) = \cases{1 & n = 0\\2^{f(n-1)} & n > 0} $$ This function is roughly as fast growing as $A(4,n)$, so is more slowly growing than $A'(n) = A(n,n)$. Now I'll evaluate $\log \alpha(n)$ and $\alpha^*(n)$ on $A'(f(n))$: $$ \log \alpha(A'(f(n))) = \log f(n) = f(n-1)$$ $$\alpha^*(A'(f(n))) = 1 + \alpha^*(f(n)) < 1 + \alpha^*(A'(n)) < 2 + \alpha^*(n)$$ Since $f(n-1) \in \omega(2+\alpha^*(n))$, $\log \alpha(n)$ is much faster growing than $\alpha^*(n)$. 

If randomness is allowed, you can get space $B + o(B)$ and time $O(1)$ (worst-case, with high probability), where $B$ is $\lg {U \choose n}$, which is the lower bound on how much space you need using any representation: Arbitman et al., " Backyard Cuckoo Hashing: Constant Worst-Case Operations with a Succinct Representation". For deterministic dictionaries, the best known bounds are discussed in Thorup's "Mihai Pǎtraşcu: Obituary and Open Problems" 

Dan Willard's "A density control algorithm for doing insertions and deletions in a sequentially ordered file in a good worst-case time" describes an algorithm for maintaining an ordered set in an array of size $O(n)$ with insertion and deletion in $O(\frac{\log^2 n}{B})$ worst-case time, where $B$ is the page size. The paper is 55 pages long, and its conclusion notes several improvements to the constants that the author does not describe for reasons of space. This makes me suspect that perhaps the constants aren't so galactic, and that this data structure would be of "legitimate utility", especially since it has been cited many times. 

has the type , while has the type . It is important to note that is a type, not a term. Another example: 

I think you need induction here to help you show that there is no term like . I also think you need to strengthen your induction hypothesis, because you don't just need to know that is uninhabited, but that is as well. I proved this using an auxiliary lemma 

When growing a hash table that is too full? When shrinking a hash table that is not full enough? When rebuilding a hash table that has too many "deleted" bits set? In $k$ different hash tables that may contain some keys in common? In $k$ different hash tables that contain no keys in common? 

One reason is that when there is only one set of slots, there will be some keys such that $h_1(k) = h_2(k)$. 

An operation can have higher amortized cost than actual cost if it adds too many coins (in the banker's method) or too much potential (in the physicist's method). The lower bound paper you linked to includes the line: 

Here's an idea for a structure that might meet these bounds. It uses an balanced binary search tree where each node is annotated with a pair of integers in $[-1,k-1]$ to indicate how much of the prefix of the key does not need to be compared again with the search key. Notation A shared prefix pair is a pair of integers associated with a key, called the focus, and a set of string keys not containing the focus, called the context. For a focus $f$ and context $c$, if $\forall y \in c, f < y$, then the left shared prefix (the left value in the shared prefix pair) of $f$ and $c$ is $-1$. Otherwise, let $g$ be the largest key in $c$ such that $g < f$. This is called the left neighbor of $f$ in $c$. Then the left shared prefix $i$ is the largest integer such that $f[i] > g[i]$ and $\forall j < i, f[j] = g[j]$. The right shared prefix and right neighbor are defined symmetrically. Search Annotate each node in a balanced binary search tree with a shared prefix pair, using the key at that node as the focus and they keys of its ancestors in the tree as the context. I'll first describe how to search for a string key (the needle, as in "needle in a haystack") in an annotated tree, then describe insertion and deletion. During search, maintain a shared prefix pair with the needle as the focus and the keys of the nodes inspected during the search as the context. By the definition above, we start with a shared prefix pair of $(-1,-1)$. Assume we are searching for the needle $d$ with shared prefix pair $(i,j)$ in a tree with root $v$ with key $z$ and shared prefix pair $(p,q)$. Since the search proceeds from parent to child, the prefix pairs share the same context and the same left and right neighbors, $a$ and $b$. Now, if $i < p$, then $z[i] = a[i] < d[i]$, and $z < d$. In this case, the search proceeds recursively to the right child of $v$. $z$ is the new left neighbor of $d$ and $d$'s prefix pair stays the same. If $i > p$, then $d[p] = a[p] < z[p]$, and $d < z$. In this case, the search proceeds recursively to the left child of $v$. $z$ is $d$'s new right neighbor and $p$ is the new right shared prefix of $d$. For $j \neq q$, proceed similarly. In the remaining case, $i = p$ and $j = q$. Assume w.l.o.g., that $i \geq j$. Find the first $i^{\prime} > i$ such that $d[i^{\prime}] \neq z[i^{\prime}]$ but $\forall n < i^{\prime}, d[n] = z[n]$. If $d[i^{\prime}] < z[i^{\prime}]$, then $d < z$, so proceed to the left child of $v$. The new right neighbor of $d$ is $z$ and the new right shared prefix of $d$ is $i^{\prime}$. If $d[i^{\prime}] > z[i^{\prime}]$, then $d > z$, so proceed to the right; $z$ is the new left neighbor of $d$, and $i^{\prime}$ is $d$'s new left shared prefix. The $j > i$ case is symmetric. Search Performance Letters are not compared unless $i=p$ and $j=q$. At each step in which letters are compared, either the sum of the shared prefix pair increases or only one letter comparison is made. We will discuss these two separately. If only one letter comparison is made, a child pointer is immediately followed. Since paths from the root to leaves are $O(\lg n)$ and $n \leq A^k$, fewer than $O(k \lg A)$ single-letter-comparison steps are taken. If $p > 1$ letter comparisons are made in a step, the sum $i+j$ of the shared prefix pair increases by at least $p-1$. Since the maximum possible sum is $2k$, $O(k)$ letter comparisons are made in comparisons that result in larger shared prefix pair sums. However, shared prefix pair sums may seemingly decrease in non-letter-comparison steps when $i > p$ and $j$ is replaced by $p$ or $j > q$ and $i$ is replaced by $q$. This is not the case, though: If $i > p$ then $d < z$, so $j \leq q$. If $j < q$, then $d$'s new right shared prefix is $j$, so $j = p$ and the shared prefix pair sum is not decreased. Similarly, if $j = q$, then $d$'s new right shared prefix is $\geq j$, and the sum is not decreased. Similarly, each step requires $O(1+p/B)$ block transfers if it makes $p-1$ letter comparisons. Insertion and Deletion To perform an insertion, first locate where the node with the key $d$ would be found if it were in the tree. After modification, perform the necessary $O(\log n)$ restructuring necessary to restore balance. Rotations might invalidate shared prefix pairs. Consider the case of a left rotation: