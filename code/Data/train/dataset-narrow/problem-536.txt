Once this is done, the next step is to actually sign this procedure, so whoever has rights to 'execute' the procedure will inherit rights from the certificate user. 

You could also use window functions which are usually faster,to get OrderNumbers that contain two rows, meaning contain both 'ghi' & 'abc' 

Just to expand on previous answer that was posted. Just like George said, shrinking in general is not something that should be part of maintenance job. Rather log size miscalculations, some unexpected scenarios (such as uncommitted transaction, large and intensive DMLs etc etc) or insufficient amount of log backups can cause excessive log growths. If your log size does not seem large enough, you should monitor it during busy hours, or during night time ETLs(If you have some) to see the average log size and change it if needed. Also make sure to set log size auto growth in specific MB size, which will mostly depend on your initial log size. More info could be found here Database log VLFs Now to answer your questions: 1) No in general. But in scenarios i mentioned above, it could be helpful. Which is the only time when it should be used - out of ordinary situations. 2) If you determined you want to shrink your log file, you should be aware that the log file is made out of VLFs(Virtual log files), which are gradually filled one at the time. Once all of them are filled,if you reached your log maximum size, log auto growth will happen and depending on size will grow in 4/8/16 VLFs. Once the log is backed up, these VLFs will become empty again (you will always have some in use, so it can track current LSN). To keep it short, once you backup you log, you can check used and unused VLFs using command 

The data will likely long outlive the application code. If the rule is critical to the data being useful over time (like foreign key constraints that help keep the integrity of the data), it must be in the database. Otherwise you risk losing the constraint in a new application that hits the database. Not only do multiple applications hit databases (Including some that might not realize there is an important data rule) but some of them such as data imports or reporting applications may not be able to use the data layer set up in the main data entry application. An frankly, the chances of there being a bug in the constraint are much higher in application code in my experience. In my personal opinion (based on over 30 years of dealing with data and experience with hundreds of different databases used for many different purposes) anyone who doesn't put the contraints in the database where they belong will eventually have poor data. Sometimes bad data to the point of being unusable. This is especially true where you have financial/regulatory data that needs to meet certain criteria for auditing. 

As Kris mentioned, you could create a stored procedure and run it as a job Here is a sample script that will accept Table name and Threshold(in KB) and send an email if table size exceeds its threshold 

When you use the nodes() method with the XML column from the table, you must use the APPLY operator. Example: 

Depending on transaction level, those queries will block writes. Serializable & Repetable Read transaction isolation level will hold S locks (for the whole duration of the transaction), which are incompatible with X locks that are required for writes(inserts/updates) . And yes it makes no difference, if you check execution plan you will see that they are exactly the same. So in order to prevent locking and blocking implementing different kind of objects wont give you no results, but performance boost(if used stored proc). Instead you should change isolation level 

There is a workaround for that issue. It will require an additional table,a trigger, and an agent job to be ran before transaction log backup job. So if you think its worth going through that than bare with me: You should create a table where you should add Database name,whether database was updated or not (you will need this for an agent), and some other info needed for backups such as : 

Alternative approach if you do not need to disguise the data for privacy reasons. All database changes and values for lookup tables should be in source control. We simply restore the last prod backup and then use source control scripts to add whatever dev changes haven't made it to Prod yet. If you have multiple databases, all should be refreshed at the same time. One advantage of this is that you have roughly the same number of records as prod which makes a huge differnce in testing your database code for performance. It also helps keep your devs using source control for changes and helps practice deployments before you move them to other environments. 

A trigger is the proper way to do this. We have a simliar process and when someone sets the value to 1, any existing records that are set to 1 are reset to 0. WHen someone deletes the one that is set to 1, it goes through a chain of rules to determine which remaining record to set to 1 (we must always have one and one one record set to 1). This process is more complicated than a check constraint to handle, thus it is in a trigger (this is the main purpose for triggers even existing). In a SQl server trigger, you need to be careful not to try to handle only one record at atime. Triggers operate on the entier set of data so make sure to write your trigger to do so as well. A 1,000,000 records might be changed in one update, you don't want to lock up your system while it gores through row by row. So no cursors and no scalar functions and no correlated subqueries. 

Havent really tried to debug the script, but i can see just from the parameters what it essentially does. There are many scripts online that you can use to do just that The one that i use pretty often is this: 

When you are all set and done, you can transfer those views from ViewSchema to dbo or some other more meaningful schema with command: 

After delete trigger is executed after the record has been removed from the table. Joining a table Emp will yield no result because record with that ID does not exist in that table. Also note that inserted table will always be empty in after delete trigger. 

See if you can find SOS_SCHEDULER_YIELD & CXPACKET waits. If SOS_SCHEDULER_YIELD waits are high you might have some very CPU extensive queries, which you should pay attention to. This: 

As specified ,returns table therefore SQL server checks the underlying structure of the table because it needs to know what kind of table is begin returned. How many columns it has,if any of the columns are without names(in aggregate cases) and inform you about it. You can test this by running the query above without referencing the column name( 'as user name' ) in case username table does not have a specified column name. Note: I had to tweak this two functions a little bit, to make them work. Hope this clarifies what is going under the hood for you 

When you are building complex queries, you should build them in stages checking the results as you go, not build one whole huge query and then try to figure out what is wrong. Here is what I do. First I list all the columns I want on a spearate line and comment out all the columns except those in the first table. Then I add the from clause and any where conditions on the first table. I note how many records are returned. Then I add each table one at a time and check the results again. I am especially concerned when the number jumps up really high unexpectedly or goes down unexpectedly. In the case of the number jumping up high, you may have a one to many relationship that needs further definition. This is especially true if the field or fields you are getting from the table are almost always the same. You may need a derived table or a specific where condition to resolve. You might even want to do some aggregation. Now I'm not saying it always bad if the record counts go up, only if they go up when you didn't expect them to or when the result set looks suspicious. In the second case, you generally have an inner join where you need a left join. The number of records went down because you did not have a matching record in the joined table. I often check each inner join with a left join to ensure that I return the same number of records. If it does then the inner join is appropriate, if it doesn't then I need to decide if it is filtering records I should be filtering or if I need a left join. When I am not sure why the record counts are off from what I expect, I use a select * (temporarily) just to see all the data, so I can determine why the data is off. I can almost always tell what the problem is when I see all the data. Do not ever use select * in a multiple join query like this or you will be returning much more data than you need and thus slowing the query as at a minumum the join fields are repeated (plus it is truly unlikely you need every field from 20 joins!). To find you issue, you are going to have to repeat this process. Start with just the from clause and the where condtions on it and add tables until you find the one (s) which are causing the query to eliminate all records. Don't stop with the first find, it is possible in a long query like this that you have multiple problems. 

Next step is to create a job that will check (before tran log backup job): Step 1. Whether there is a database with value of 0 (meaning not backed up) Step 2. Back it up taking values from the table (DB name,BackupName) Step 3. Update column to 1. Optional: Send an email informing DBAs about newly backed up database. 

Green rectangles are tables, red eclipses are attributes, blue triangles are tables that connect tables and a relationship descriptors so you could understand their relationship. Do not name triangle like tables as referred on the diagram, such as 'contains' , give them meaningful names. Purple eclipses are additional attributes of tables that connect other tables Numbers and letters next to triangles are telling you what kind of relationship it is Whether is : One to many - example Table Account has 1 and table Character has N which means one account can have many characters , but character can have only 1 account Many to many - example Character can "Ran into" many creeps and many creeps can ran into many characters What does it means? It means that depending on a relationship type, you will create foreign keys and constrains accordingly. If you are still in doubt what the foreign keys are, i suggest you to read some books/articles before you actually start designing database. Databases are base for any project, and it is not something to be taken lightly 

Are you backing up the transaction logs? Not the database but the logs? THe log will grow until it uses up the entire hard drive unless you back it up. I suiuggest at aleasta daily backup or more frequently if you havea lot of transactions. 

I think any of the major databases can handle the load if designed well. Sadly I would estimate that less than 1% of all databases are designed well. (I have personally dealt with data from literally thousands of different databases performing a wide variety of functions, so I think I have a good idea of the lack of quality that is out there inteh real world.) I would strongly suggest that you get some books on performance tuning for the database you choose and read them thorughly before begining to design. There are many things that will help your database perform better that should be designed in from the start. Just knowing how to write performant queries and design indexes is critical to getting a good design. This type of study and designed-in performance is not premature optimization. There is no reason at all to use known performance killing techniques in the design. Databases need to be designed for performance from the start.