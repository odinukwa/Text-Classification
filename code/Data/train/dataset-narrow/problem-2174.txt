Let $V = F_2^n$ be the $n$-dimensional vector space over the field of two elements. The $\epsilon$-noise distribution on $V$, denoted $\mu_\epsilon$, is a probability distribution on $V$ for which sampling a vector $w$ is done by setting each coordinate of $w$ independently, and a particular coordinate is set to $1$ with probability $\epsilon$ and to $0$ with probability $1-\epsilon$. Let $C \subseteq V$ be a linear subspace, and let $e \in V \setminus C$ be a vector not in $C$. Consider the following pair of events concerning a vector $x$ drawn from $\mu_\epsilon$: 

Solving this exactly this ends up being $\mathsf{NP}$-hard. The reduction I have doesn't pay much attention to the representation of the $f_i$'s. That said, the values of $f_i$ I end up giving can be computed efficiently, so this reduction works for any "black box" use of the $f_i$'s. I'll give the reduction in two parts. Composing the two pieces gives the whole argument. 

Boolean formulas are binary trees where internal nodes are gates and literals are leaves. Subexpressions correspond to choices of subtrees in which you pick a node and everything below it. Hence if you have $n$ literals, there are $n-1$ internal nodes in the tree, and hence $n-1$ total subexpressions, though there may be duplicates. If you want to count "bare literals" as subexpressions, then this climbs to $2n-1$ subexpressions with possible duplicates. Depending on how you want to handle duplicate subexpressions, one of two things happens, though the answer is the same. If you count duplicates separately, then $n-1$ or $2n-1$ is your final answer no matter the expression you start with. If you want to only count distinct subexpressions, but only care about worst-case guarantees, then you can just pick the $n$ literals to be distinct, and this will force all the subexpressions to be distinct. Hence you still get $n-1$ or $2n-1$ many subexpressions. 

I'd consider it unlikely that such a trick is easy to find and/or will give you significant gains, as it would give nontrivial satisfiability algorithms. Here's how: First of all, while ostensibly easier, your problem can actually solve the more general problem of, given a circuit $C$ and $N$ inputs $x_0, \ldots, x_{N-1}$, evaluate $C$ at all the inputs faster than $\tilde{O}(N\cdot|C|)$ time. The reason is that we can tweak $C$ into a circuit $C'$ of size $|C| + \tilde{O}(Nn)$ which, on input $0^i10^{N-1-i}$, outputs $C(x_{i})$. Basically, we just make a little lookup table that sends $0^i10^{N-1-i}$ to $x_i$, and wire it into $C$. Nontrivial algorithms for batch evaluation of boolean-circuits can then be used to make fast satisfiability algorithms. Here's an example in the simple case where we suppose we have an algorithm doing evaluation in time $\tilde{O}(|C|^{2-\epsilon} + (N\cdot|C|)^{1-\epsilon/2} + N^{2-\epsilon})$ for any constant $\epsilon > 0$. On input a circuit $C$, we can decide satisfiability by expanding $C$ into a circuit $C'$ of size $2^{n/2}\cdot |C|$ which is just the OR over all possible choices of the first $n/2$ inputs to $C$ (leaving the other inputs free). We then batch-evaluate $C'$ on all of its $2^{n/2}$ inputs. The end result is that we find a satisfying assignment to $C'$ iff $C$ is satisfiable. The running time is $\tilde{O}(2^{(n/2)(2-\epsilon)}\cdot |C|^{2-\epsilon}) = \tilde{O}(2^{n(1-\epsilon/2)}\cdot \textrm{poly}(|C|))$. 

Every directed graph can be partitioned into strongly connected components. Consider the "DAG of strongly connected components" in $\vec G$; call it the quotient graph. Note that $\Phi(\vec G)$ will have the same quotient structure, since $\Phi$ doesn't affect the edges between SCCs, and strongly connected graphs remain strongly connected when reversing all their edges. Additionally, if a SCC has more than one vertex, then all its constituent vertices have an incoming edge. If an SCC has just a single vertex and isn't a source in the quotient, then all its constituent vertices have an incoming edge. So to show $\Phi(\vec G)$ is admissible, it suffices to show that the SCCs which are sources in the quotient have multiple vertices. But this follows by the fact that every vertex in the component has an incoming edge, which must come from another vertex in the component since $G$ has no self-loops and the component is a source in the quotient. This follows from the fact that the quotient structure of $\Phi(\vec G)$ coincides with the quotient structure of $\vec G$. By admissibility, $\vec G$ has a cycle, and hence some SCC with an edge inside it. 

I won't actually prove this because of time, but it's not too bad to work out on one's own. That $1 \implies 2$ is straightforward. That $2 \implies 1$ is a bit more arduous, but it's not too bad when you reason with the automorphism of $G$ which sends each vertex in $L$ to its copy in $R$ (and vice-versa). The proof I have in mind directs the edges in $G$ so that all edges in a cycle go ``the same way around the cycle'' (each nonisolated vertex has in-degree = out-degree = 1), and so that the preceding automorphism of $G$ remains an automorphism of the directed version. $\pi$ is then chosen according to the edges that go from $L$ to $R$. You can phrase the above algorithm as a perfect matching question, and I imagine there are other reductions to 2-SAT. I don't see how to extend these approaches to the original problem though. 

For the very limited situation in which $k=1$ and we're only interested in functions whose domain is $\{0,1\}^n$, the VC dimension is $\lfloor \log_2(n+1) + 1 \rfloor$. The upper bound follows essentially from Sasho's argument: there are $2+2n$ 1-juntas on $n$ variables, and if there were more than $\log_2$ of that many inputs, we could find a function that avoids all 1-juntas. To see the lower bound, consider the matrix whose columns contain the $2^{\lfloor \log_2(n+1) \rfloor} - 1 \le n$ not-all-zero bitstrings of length $\lfloor \log_2(n+1) \rfloor + 1$ that start with a zero. The inputs to shatter are the rows of this matrix (padded up to length $n$). Every function of these inputs is either constant, or else it or its negation appears as a column, so it's computed by a 1-junta. 

A whole lot of fun things happen. Most of the ones I know of start with the IKW paper. There, the collapse $\textrm{NEXP} = \textrm{MA}$ is shown, and (I think) is the strongest literal collapse of complexity classes that we know of. There are other sorts of "collapses" though that I think should be pointed out. Most importantly, I think, is the "universal succinct witness" property (also from the IKW paper). For one, it gives you a tool from which many of the other collapses are straightforward consequences; for another, the recent circuit lower bounds (eg here and here) for $\textsf{NEXP}$ exploit this connection. Briefly, the property says that, for every $\textsf{NEXP}$ language $L$, and any $\textsf{NEXP}$-machine $M$ deciding $L$, every $x \in L$ has a succinctly describable witness according to $M$. Formally, there is a polynomial $p$ depending on $M$ so that for every $x \in L$, there is a circuit $C_x$ of size $p(|x|)$ so that the truth table of $C_x$ is a sequence of nondeterministic choices for $M$ that lead to acceptance on input $x$. The succinctness of the witnesses comes in handy, because you can straightforwardly rederive a lot of the other collapses from it. For instance, it gives you really trivial simulations of $\textsf{NEXP} = \textsf{coNEXP} = \textsf{EXP}$. For example, suppose $L$ is in $\textsf{NEXP}$ via a $\textsf{NEXP}$-machine $M$. The succinct-witness property says there's a polynomial $p$ so that $M$ has succinct witnesses of size $p$. We can then decide $L$ in $\textsf{EXP}$ by, on input $x$, brute-forcing all the circuits of size at most $p(|x|)$, and checking whether they encode a sequence of choices that lead to $M$ accepting on input $x$. You can combine this with the (previously known via interactive proofs) result that $\textsf{EXP} \subseteq \textsf{P}/\textrm{poly} \implies \textsf{EXP} = \textsf{MA}$ to conclude $\textsf{NEXP} \subseteq \textsf{P}/\textrm{poly} \implies \textsf{NEXP} = \textsf{MA}$. It's worth emphasizing that we get to pick $M$ and hence the form of the witnesses. For example, you can actually conclude from "$\textsf{NEXP}$ has universal succinct witnesses" that $\textsf{NEXP} = \textsf{OMA} = \textsf{co-OMA}$. Here $\textsf{OMA}$ is "oblivious-MA", meaning that there is an honest Merlin which depends only on the input length. It's easy to see that $\textsf{OMA} \subseteq \textsf{P}/\textrm{poly}$, so basically this is just giving a normal form for how $\textsf{NEXP}$ languages are computed in $\textsf{P}/\textrm{poly}$ under the assumption that $\textsf{NEXP} \subseteq \textsf{P}/\textrm{poly}$ in the first place. Here's one way to see the collapse to $\textsf{OMA}$: 

proof: Fix a variable index $i$; we'll show that $\frac{\partial F}{\partial x_i} \ge 0$ at all $x \in [-1,1]^n$. If this holds for all $i$, we're done. Since $F$ is multilinear, we can write this partial derivative as $\frac{1}{2}\left( F\restriction_{x_i=1} - F\restriction_{x_i = -1} \right)$, where $F\restriction_{x_i = a}$ means we assign $a$ to $x_i$ and leave the remaining variables free. So it suffices to show that $F\restriction_{x_i = 1} \ge F\restriction_{x_i = -1}$ for all $x\in [-1,1]^n$. Since $f\restriction_{x_i = 1} \ge f\restriction_{x_i = -1}$ on $\{-1,1\}^n$, and since $F\restriction_{x_i=a}$ is the multilinear extension of $f\restriction_{x_i=a}$, this reduces to the following claim: 

Sasho already gave you a yes/no answer, but here's an actual convex combination for you: If $B_\ell$ is the $k \times k$ matrix which is $1$ when $|S_i \cap S_j| \ge \ell$ and zero otherwise, then $M = \sum_{\ell=1}^{n'} \frac{1}{n'} B_\ell$. 

$\Phi$ is totally defined (every admissible orientation is mapped somewhere) $\Phi$ sends admissible orientations to admissible orientations $\Phi$ is an involution ($\Phi \circ \Phi$ is the identity) $\Phi$ has no fixed points 

The answer to your first question is yes by some linear algebra. Write down the equations for $L(m) = \sum_{x\in\pm 1} D(x)m(x)$ where $m$ is a monomial of degree at most $d$ (with no squared variables in it). You can write this as $L = MD$ where $M$ is a matrix with rows indexed by the monomials $m$ and columns indexed by the points $x \in \{\pm 1\}$, with values $m(x)$, and $D$ is a vector indexed by the points in $\{\pm 1\}$. $M$ has full rank, so you can find $D$ with $MD = L$ for any $L$. That any such $D$ satisfies the pseudodistribution properties follows from the fact that $L$ satisfies the corresponding properties. As for your second question, formally, it can be done for prime fields by just using equations like $x^p = 1$ in place of $x^2 = 1$. (I'm not sure what to do for general prime powers.) It might be helpful in characteristics greater than two to generalize the rest of the SOS theory to work over $\mathbb{C}$, since you're working with complex roots of unity. This can be done essentially by replacing "sum of squares" by "sum of products-of-conjugates" and making appropriate adaptations elsewhere. However, I'm doubtful that it's useful to think of $\{\pm 1\}$ as being "like" $\mathbb{F}_2$ in the context of SOS. Embedding finite fields as roots of unity doesn't jive well with sum of squares: addition in the finite field turns into multiplication over $\mathbb{R}$, and hence SOS requires high degree to reason about sums (in the finite field) of many unknowns, because they translate to products (over the reals) of many variables. An essentially canonical demonstration of this is the lower bound for solving linear equations over $\mathbb{F}_2$ (see section 3.1 here). In particular, you can trick low-degree SOS into believing that a certain system of linear equations over $\mathbb{F}_2$ is completely satisfiable, when in reality at most a $\frac{1}{2}+\epsilon$ fraction of equations can be satisfied by any true assignment. 

The circles represent vertices. You can also easily check that the graph is 3-regular and isn't $K_4$. Open circles define the maximum independent set which gives the counterexample. (Unfortunately, there are many other non-isomorphic maximum independent sets in this graph, and at least one of them does get 2-colored by some 3-coloring of the graph.) It's not too hard to see that each of the two "halves" that make up this graph can have an independent set of size at most three, and hence the graph as a whole has all independent sets having size at most six. Thus the independent set given here is indeed maximum. It's fairly straightforward to show that there's no 3-coloring that 2-colors the independent set. Let's try to color the vertices in the independent set Red or Blue, leaving Green as the third color, and derive a contradiction. We'll start at the bottom: the triangle there (with the red edge) needs to have one vertex of each color. Since the top vertex in the triangle can't be Green, then, by symmetry, we can assume wlog that the bottom left vertex is assigned Green. This forces the top of that same triangle to be Red-or-Blue, and the other vertex of the triangle to be Blue-or-Red (the opposite). The bottom right vertex being Blue-or-Red then forces the vertex above it to be Red-or-Blue. The two white vertices both being Red-or-Blue force the left vertex of the inner triangle (with all black edges) to be Red-or-Blue. This in turn forces the remaining white vertex in the bottom half to be assigned Blue-or-Red. The top two white vertices in the bottom half are then Red-or-Blue and Blue-or-Red, which forces the top-most black vertex in the bottom half to be Green. Repeating this argument for the top half, we eventually get that the edge that bridge the two halves is Green on both sides, a contradiction.