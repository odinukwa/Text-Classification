Train to avoid false negatives What your network learns depends on the loss function you pass it. By choosing this function you can emphasize various things - overall accuracy, avoiding false negatives, false positives etc. In your case you probably use a cross entropy loss in combination with a softmax classifier. While softmax squashes the prediction values to be 1 when combined across all classes, the cross entropy loss will penalise the distance between the actual ground truth and the prediction. In this calculation it will not take into account what the values of the "false negative" predictions are. In other words: The loss function only cares for the correct class and its related prediction, not for the values of all other classes. Since you want to avoid false negatives this behaviour is probably the exact thing you need. But if you also want the distance between the actual class and the false predictions another loss function that also takes into account the false values might even serve you better. Give your high accuracy this poses the risk that your overall performance will drop. What to do then? Making the wrong prediction and being very sure about it is not uncommon. There are millions of things you could look at, so your best guess probably is to investigate the error. E.g. you could use a confusion matrix to recognize patterns which classes are mixed with which. If there is structure you might need more samples of a certain class or there are probably labelling errors in your training data. Another way to go ahead would be to manually look at all (or some) examples of errors. Something very basic as listing the errors in a table and trying to find specific characteristics can guide you towards what you need to do. E.g. it would be understandable if your network usually gets the "difficult" examples wrong. But maybe there is some other clear systematic your network did not pick up yet due to lack of data? 

Normalisation helps your neural net because it ensures that your input data always is within certain numeric boundaries, basically making it easier for the network to work with the data and to treat data samples equally. Augmentation creates "new" data samples that should be ideally as close as possible to "real" rather than synthetic data points. Therefore: 

Complexity: Mixed complex sceneries with simple images. Result: The quality depends a lot on how difficult the task is. In my scenario the images to be classified where rather similar to the ones in the training data. In these situations all categories with 160 images and above are recognised rather reliably. I will do some more testing and update my answer. 

Having linear separable data means that an optimal solution exists. The support vector machine is an approach to identify the optimal solution with multiple steps through Lagrange multiplication. This method is guaranteed to give you the optimal solution if there is one. 

Manually create a small subset of cleaned labels Train the network on clean and noisy labels Use an additional network (or layer) to either create a mapping from noisy to clean labels or to learn about the noise distribution 

It depends. Actually there are research papers finding that neural network can sometimes cope very well with sparse or noisy labels. But basically when you apply any machine learning approach, you want your model to pick up patterns in your data. In order to be a pattern something needs to be repetitive and predictable. Outliers and errors in your data might not adhere to that. They rather break the pattern. It can still work but it is harder to recognize it. Making a model "learn" outliers/missing values etc. will only work, if these incidences also appear in a pattern which makes them unlikely to be outliers. As a rule of thumb it is therefore a good idea to remove this noise. Finding patterns in exceptions & errors As I mentioned some researches try to address the noise issue for example by classifying if there is random ("white") noise or structured noise. They not only predict the target value but also the type of noise and then try to make another prediction based on both pieces of information. So there are some active approaches to address noise but usually this is due to the fact that it would be to cumbersome or even impossible to remove the noise. 

I think in practice it sometimes actually doesn't matter in which order you do it. Depending on which transformations you apply to the images they might still be normalized afterwards (e.g. if they are mirrored). Effect on distribution A concern could be that the augmentation messes with the distribution of your data. However, it is likely that you perform linear transformations on your images. When your data is initially in a Gaussian distribution it will remain so after the transformation as explained by this paper. 

I am using a transfer learning approach. For this I followed the tensorflow for poets tutorial. I use a pre-trained InceptionV3 architecture trained on the Imagenet dataset. The last layer and the softmax classification is replaced and retrained, using a new set of 7 classes. Data Per class I have around 4.000 - 5.000 images. I tried multiple training parameters with an AdamOptimizer. The labels are noisy, about 15-20% of the labels are incorrect. The images show products of a certain category (e.g. cars) and the labels classify different type of a feature (e.g. 7 different types of tires, wheels). Parameters 

This work is a lot more recent (2016/17) than what you posted and comes with a nice tool set and a more detailed paper. Why using triplets aka Deep Ranking? As stated in the comments: Why should one use triplets for image similarity instead of learning feature vectors and calculate their distance? Triplets are a way of formulating the similarity question as a learning problem instead of learning feature vectors that basically do not care for similarity. This approach makes especially sense in cases where human-perceived similarity is important, which might differ from machine perception. Triplets work like this: You provide 3 images. One to compare to, one similar (close) and one not so similar (distant) image. This is your training/test/validation data. Training your network on those samples and predicting the correct order (classify similar from non-similar images) overall lets the network learn how to order images based on their similarity. All in all this approach is comparatively complex. It might be overengineered but you also asked for the best way to do this and Deep Ranking achieves very high precision values. 

Update A side note towards complexity: Recognising parts of objects also means multiple object detection (each party on itself). I wonder if this is better dealt with in separate e.g. to reduce amount of predictable classes or in a single one to benefit from learning of the other parts. 

Performance The test accuracy is 50%, train accuracy 68%. Visualising the learning rate, the network already reached 50% after 2000 iterations. What surprises me is the overall low performance as well as the lack of further improvement during the training time. Its also noteworthy that the network seems to make very hard to understand errors (not only mixing up similar classes but clearly distinguishable ones as well). Now I wonder: Is this potentially because retraining only a single layer is too limited to pick up the subtle differences in certain parts of the images? How would you go about it to improve?