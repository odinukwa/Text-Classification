(owner) has rights --> Use it for content modifier(s), ie developers in development environment, publishing tool in staging/production has rights --> Use it for content accesser(s) (ie Web server, backend applications, etc...), making sure all those belong to 

Use to validate your configuration Monitor your error log file while you issue to catch any error that might happen at runtime which are undetectable at configuration time. 

You set up 3 servers respectively listening for requests addressed to . When a request for an unknown domain kicks in, nginx serves it with the default domain. By default, nginx uses the first defined one, unless you explicitely specify another one with the flag on the directive of one of your servers. That is why your requests will always be served by the default (first) server. Now, what you wish is a reverse proxy. It is not what you asked nginx to do. You simply defined your backend servers to which you need to add another nginx server as frontend reverse-proxy. To do so, you will need to use the ngx_http_proxy_module along with the ngx_http_upstream_module. Specifically, you proxy requests from a with to your backend servers. Here is a simple reverse-proxy server configuration that might work for you: 

You need to include all of the parent directories down to the desired directory before using the exclude rule. For instance, I use the following in a backup script: 

We are looking into using BtrFS on an array of SSD disks and I have been asked to verify that BtrFS does in fact perform TRIM operations upon deleting a file. So far I have been unable to verify that the TRIM command is sent to the disks. I know BtrFS is not considered production ready, but we like the bleeding edge, therefore I'm testing it. The server is Ubuntu 11.04 server 64-bit release (mkfs.btrfs version 0.19). I have installed the Linux 3.0.0 kernel as the BtrFS changelog states that bulk TRIM is not available in the kernel shipped with Ubuntu 11.04 (2.6.38). Here's my testing methodology (initially adopted from $URL$ with modifications to work with BtrFS): 

Requests processing The base rule to remember is: nginx serves a request with one location (you could emphase even more: and one location only). Read: $URL$ matching Read: documentation Based on your configuration, nginx will first match on the prefix location, then on the regex location and will eventually serves the request with the latter. With the configuration you provided, the script should not be downloaded as a raw file any more but should rather by sent over to PHP. However, that does not mean your configuration is right: the request is not served by your location, which is useless at the moment. 

Your question is unclear. Do you wish to rewrite an URI (a location) to another ( to )? Or do you wish to serve content for with the files contained at ? nginx' configuration works differently from Apache's, that is a fact. Saying it is 'more confusion' is a personal feeling. I would object that, on the contrary of Apache, nginx allows to create lean, clean and order-independent configurations, guaranteeing readability, maintenance and scalability. It is normal to be lost because there is a learning curve for every new technology you need to handle. I suppose Apache's configuration is not always straightforward and when you started with it, it was not that obvious how to deal with it... Regarding , it is mixing content with configuration, polluting repositories with files you then need to protect to avoid serving them... nginx allows you as much granularity and modularity you wish through the use of the directive, which provides the nice feature of separating the configuration in multiple files, adding them manually where you wish inside the upper-level configuration and even include whole directories at once, allowing a per-server, per-whatever configuration file. If you need specific rules for a location, just create block for it. And if you wish to isolate a whole branch from the locations tree in a separate configuration file, you can. This is a cleaner/more readable/more maintainable way of doing what was doing wrong. 

After many failed attempts at verifying BtrFS on the server, I decided to try this same test using an old laptop (remove the RAID card layer). The initial attempts of this test using both Ext4 and BtrFS on the laptop fail (data not TRIM'd). I then upgraded the SSD drive firmware from version 0001 (as shipped out of the box) to version 0009. The tests were repeated with Ext4 and BtrFS and both filesystems successfully TRIM'd the data. To ensure the TRIM command had time to run, I did a before performing validation. One thing to note if you're attempting this same test: SSDs have erase blocks that they operate on (I don't know the size of the Crucial m4 erase blocks). When the file system sends the TRIM command to the drive, the drive will only erase a complete block; if the TRIM command is specified for a portion of a block, that block will not be TRIM'd due to the remaining valid data within the erase block. So to demonstrate what I'm talking about (output of the script above). This is with the test file on the SSD. Periods are sectors that only contain zeros. Pluses have one or more non-zero bytes. Test file on drive: 

I connected to and saw a cache hit (probably my request) displayed. What makes you think it is not working? 

To update content in Development, use the account or any tool using that user. To update content in Production, use the git tool rightly configured to push data at the corretc location, using the user. 

nginx is able to talk with any backend listening on a FastCGI socket. That includes PHP-FPM and HHVM, provided both frontend and backend are configured to find each other. Thus, it not efficiant to use 3 layers while you could only have 2 layers . You would need to decide which Web server you would prefer to use. I do not understand what might be unsafe, though, provided you use a setup suitable to be a server, up-to-date and properly configured. Security covers a wide range of different scopes, starting from network design, passing through OS configuration, up to individual Web server locations configuration. Provided you use stable software and an easy/automated method for maintenance/updates, I do not see why it would not be suitable for production. Anyway, the baseline is: you only install the bare minimum of stuff you need on a server. Security is helped by simplicity/cleanliness. 

We have a Sun 4140 running Linux (CentOS 5.5). A disk failed in a software RAID-1 array. We powered off the system and added a two new disks to empty slots in the chassis (we couldn't simply replace the failed disk due to some GRUB misconfiguration). Upon booting the system back up, we went to configure the new disks and add them to the array, but it wasn't found under the subsystem. The disk was found during booting (output of ): 

I recommend reading the revision number from a file rather than reading from an environment variable. There isn't an easy way to ensure that your current environment is the same a the environment that Apache is running under. Also this ensures that the revision number in the php code survives reboots, etc. You should set Subversion to ignore the svn-revision file so you aren't checking it in. 

Having the content enciphered before transmission to the TCP 443 port is thus a requirement per RFC. You cannot beat it. The reason for the requirement is an enforcement of the separation of OSI layers, TLS (SSL) being in the 5th one and HTTP in the 7th. There is no way they can understand each other. That is reflected in softwares, where processing of those parts are usually done by different code pieces (nginx handles HTTP, but defers TLS to an external library it has been linked with). 

That is worker recycling as the configuration directive sets. I (reasonably) assumed PHP-FPM would courrectly process any accepted connection/request before recycling the worker, as this task does not imply any kind of emergency. 

Mind the ending , replacing the original URI. If you wish to forward the original URI to backends, remove anything after the pair. 

One tool that you can try is sshfs (you can use this from the command line without needing a GUI). You do not need to do any setup on the server side. Connecting from the client is as simple as: 

I recently had a similar problem on a Debian Lenny box that was set to UTC when I wanted localtime. First you need to copy (or symlink) your correct zoneinfo file from to . For example I ran on my system. Second you need to edit to reflect your timezone as well. On my system, the file states . Once both of those files are taken care of, it is a good idea to restart crond to ensure the proper timezone is picked up. 

You may need to talk to you AD administrator to set up a service account for Apache to use to connect to AD (if your AD requires you to bind before performing a search). If you wish to allow anonymous reads while requiring users to authenticate to write, replace the line above with the following: