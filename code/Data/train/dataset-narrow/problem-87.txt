So I have a game and I am trying to fake glow inside of it by using the hardwares capability to downsize textures causing them to blur. The texture I wish to do this with is in a render buffer that is the size of the screen (not power of two) What would be the quickest way to get these lower resolutions created? Tricking the software to make minimaps? Another fbo with the texture rendered on it in smaller size? 

This packed array describes a series of disconnected triangles. Each triangle is the same color but each vertice within the triangle has a different alpha and texture coordinate (in addition to its position of course). How would you describe the layout with a MTLVertexBufferLayoutDescriptor in metal? I am not sure I could have described this with OpenGL attributes but perhaps this per triangle thing is possible with Metal? 

Does any of their rendering techniques involve rendering an area at a higher resolution just to scale it down in order to mitigate artifacts? I know they use ray methods rather than rasterization however ray methods still have artifacts even when done with a ton of rays.. right? I noticed a few moments in the film Moana that seemed to be poorly rendered because small details seemed fuzzy and sorta pixelated. Especially around the edges of a figure that was in focus with a slightly blurred background. Could they have used supersampling on those scenes to decrease the issue? 

Turns this was not an issue with the scale of the values passed in, just that the log function in glsl is base e. The solution was merely to do log(stuff)/log(10) and that got the result of a base 10 log. 

(Please note discoloration is due to a debugging thing in the fragment shader, not an error) In summary Cases 1 and 2 do not work as intended (they enlarge only part of the shape drawing and put it in the wrong proportion on the screen). Cases 3 and 4 show the individual complnents (drawing shapes, and drawing textures) working when everything is drawn to the front buffer without any 'glViewport' calls. The desired behavior is that an image like case 4 would be drawn, however the image would be an accumulation of previous frames (Notice how i never clear the layer_shapes fbo) so the shapes (that are moving) would leave a trail. The proportions are just being weird! And Incase you want to see it here is how I set up my texture fbo's. They get initialized and then have .load() called on them 

Explanation of variables: o_color: is the color that that particle is texture_coord: these are the exact same as if you were going to texture this, they go from 0-1 ratio_out: height / width. Will never be less then 1 Inside the main function to get the coordinates in screen scale we simply multiply the y texture coordinate by the scale factor. That makes the center of the circle part a 0.5, 0.5. rdistOut is the tiny area that the particle sometimes has when it is a square but still moving. Things should be shaded from 0.5-rdistOut. Could you please help me remove un-needed dot product computations and comparisons as well as get rid of branching? Please include an explanation of how you got the GPU to skip dot product calculations or why you did what you did. Next time I hope to be able to do this myself. 

I am trying to make a glow shader using separable gaussian blurring. I have recently been inspired by the short youtube video "computer color is broken" and I have messed with it with color interpolation and boy his suggestion is beautiful! A big thing the video talks about is that this principal should be applied to blurring, however I am pretty confused. I don't really know what to square when and what to sqrt when when values are being added. My current theory is each texture sample for the gaussian blur gets raised by the power of two weighted with a bell curve and added to a sum like usual. At the end the sum is square rooted, but i'm not sure if that is correct. Could someone please confirm? Would this make an appreciable difference that made things worth it? 

I am just curious why they appear to be there as I can see no mathematical reason why they should be there. Is it just the way I am displaying it, or is there some sort of computational precision issue? For reference here is the ShaderToy code that created this: 

I am trying to programmatically figure out how many "texture2d " calls the gpu can make before the fps drops below 60fps. My current way of doing this is to have a fragment shader with a for loop that will run a specified number of times based on a passed in uniform. So each frame the for loop will run more times, each time sampling the texture more times. Then on the cpu I can monitor the fps and see when it drops. My problem is if this shader runs on more then one pixel then the test is worth nothing. How can I ensure from the cpu code and vertex shader that the fragment program only runs once? 

It is a dial with the little triangle showing what part of the disk is being valued. The dial serves as a controller that sets a variable from 0-1. If the disk has 0 degrees facing left then it is 0, if the disk has 180 degrees facing left then it is 0.5. Somehow I will eventually find a way to express that so the disk will not be uniform. Anyway so the inner disk you see will spin meanwhile the outer disk with the triangle thing will stay in place. Something that would be nice is if the area of the spinning disk that is inside of the triangle would disappear. 

I am in awe at this app I just found. It was actually one of the first things I installed when I first bought my iPad 2 but now they have updated it to add an HDR effect. I have never seen a real-time glow effect that looked quite this amazing. I have no clue how the developer pulled off such a large radius in realtime graphics. 

Case #4 Proving that shape drawing works (This is what it should look like, except with an accumulation effect) 

I am trying to wrap my head around how exactly the GPU interpolates texture coords on a quad. I realize to the GPU a quad is two triangles but I find thinking in terms of the whole quad to be easiest. If we define texture space as the texture coordinates from X:Y{0-1} and world space as the actual coordinates of the quad how can I given a point p1 that is on the quad figure out what its texture coordinate would be? Given a texture coordinate point p2 how would I figure out its real world coordinates? Keep in mind that the quad can be any quadrilateral not necessarily always a parallelogram. As an extra note lets define the texture coordinates on the quad so that they are as such 

I recently watched this video that talked from a physics perspective how most of the ways we deal with color on the computer is incorrect because brightness is on a logarithmic not linear scale. Being a novice graphics programmer I want to know how to deal with the information to make my graphics even better. I figure ray-tracers but for rasterizing, blur, grading, and light adjusting like HDR what am I supposed to do with this information? So I guess I have some questions. (sorry its a lot) 

Ok, I have a simulation I am trying to make. The entire simulation is drawn by circles (with gl_Point) and rectangles (gl_triangles with indices). It would be great to add some anti aliasing especially as there scale is usually rather small. My problem is I have seen some formulas that smooth step the edges of a shape, however I do not know how to properly consider scale. Each sprite will have different scale to it, each rectangle will have a different width / height ratio, not to mention I am not yet passing in texture coords because they have been so far un-necesary. And this will run on multiple screen resolutions. I guess I have two questions: 

A thought is there was a line near the end (I put an arrow pointing to it) that I had to remove to get the fbo to work properly. Previously the code worked fine, however it is not working fine now. I do have a suspicion that since I have removed that line that caused nothing to work that the texture that is generated by it is the wrong size since the texture it is loading is of a non-device size. However I do not know why that call causes everything not to work. 

I have this GLSL function that I am trying to optimize because it is going to be ran on many pixels of an older devices GPU. There is no room for branching inefficiency. Essentially this function returns a 0 or a 1 based on the variables and 

I am having a lot of trouble understanding what the difference between a layout and a descriptor is inside of a MTLVertexBufferLayoutDescriptor object. I am hoping to create a layout descriptor that describes how the the following array is packed (in my actual application there is a new color for every 4 points and many more differences that would actually make not repeating the color for every vertex important TL;DR this is an example) 

The problem is I have no clue how to do this. Currently the rotation of the disk is done by a matrix that rotates the quad that is rendering this. I have two ideas 1. Edit the quad into a mesh cutting out the triangle, however this would not work as the mesh is translated in my current model. 2. Run costly trigonometric calculations to figure out where each fragment is in real space then figure out its angle and distance from the center of the circle. I know doing a little trig on each fragment might not sound like much, however just rendering the disk adds 1ms of rendering time when rendering on the lowest possible hardware this could run on. How do people achieve real-time masking in opengl? 

So I am trying to make my shader efficient by lowing the amount of texture lookups. If I had a line of code with something like. 

Is it even possible to have opengl assume that the Z is always 0? Is it possible to reuse the color like to hat? 

Where x is a value that ranges from 0 to 2 based on the fragment position. Would the GPU see when step evaluates to 0 and thus not ever calculate texture2D? Or would it still lookup the texture because the 0 is not constant? BTW I use the term "short circuit" because I have been told that is what happens when you have the computer sees the false and ignores the right. 

Usually I am fairly good at removing branching from GLSL code but I really don't know what to do with the range check for the state variable. 

Is it possible to anti-alias a rectangle in a fragment shader without knowing the UV of each fragment since I am not yet passing them in? Even if it was possible how would I account for different ratios of rectangles? What math can I use to ensure that no matter the device resolution, or shape size that the anti aliasing effects a maximum of 2 pixels (or whatever) from the edge of the shape? 

I am trying to make it so that objects leave trails behind them and if an object is in the back and tries to draw over a previous trail it is transparent because of it. The problem is whenever I do not GL_CLEAR the depth buffer after each frame the screen goes blank. Is there a way around this in Open GL ES 2.0? Diagram: 

Hello I have a simple application with some bright 2d mono-color shapes I am hoping to make appear emissive with a nice glow around them. To do that I am producing multiple gaussian blurs of varying kernels and resolutions and adding them all together to create a composite glow. I have seen many guides mention this and even mention what sizes and kernels they are using to produce their bloom however they never mention the math they used to put them all together. Right now I am assuming it was just regular additive blending. I was wondering what tips you might have to put this glow effect together effectively as far as if I should do anything other than just adding pixels together to combine the various maps. Should I do something with the weight on certain textures used in the composite? I am just trying to composite everything in an optimal manner. My main concern is whether or not I should gamma correct my glow map or if that is accounted with the weights from the gaussian sampling or source material. Should I gamma correct my blurred composite before adding it to the source image? 

The dot product is NEVER needed past the point where the circle part is done. If the scaleFactor is greater then 1 then the dot product is never needed past the center of the circle. 

It appears that the flickering ones have a constantly changing x result from "getGrid()" and most of the squares have an occasionally changing y result from the "getGrid()" call. I really have no clue why this is happening! It probably has something to do with the way glsl does math because id and everything else (I have checked) and they are constant. Incase it helps here is the tile map. The texture is actually 2000 x 2000 due to the size these need to be. It will be hard to see because the numbers are white and the background is grey. 

OK so I have a simple setup going on. Basically I am just drawing some textures and shapes. However sizing is going weird... let me show you how any maybe you can diagnose. Basically everything is broken into components that work fine by themselves, however when together it is disastrous. Case #1 Section of actual texture appears with an enlarged resolution in the bottom left corner. 

I have a scene rendered entirely with objects with additive blend mode. I have a value that is oscillating from 0, to 1. When it is 1 the object is supposed to be invisible, 0 should be entirely visible. However I have noticed that something seems off about it, specifically it stays dark for too long. It is not my oscillating function, I suspect it having something to do with light using a logarithmic scale. How would I convert my linear 0->1 value into a properly scaled value between 0->1 Fragment: