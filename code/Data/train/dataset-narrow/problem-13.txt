In summary, always set reasonable defaults, e.g. a software version that exists instead of using a non-existing one or empty variable and if there is a constant, put it in the vars directory. There is a number of galaxy playbooks that could be checked: 

One could create a tag that consists of multiple elements, e.g. a combination of a timestamp, git commit hash and semantic version. The latter has to be set manually, while the first two could be automated. Such a tag could look as follows: 

According to this documentation it is possible to run windows in docker. Yes it could make sense as this would be comparable to the positive effects of running linux in docker. Note that: 

I agree with this comment. Services should always be able to start. Imagine that one service goes down then another service will not work anymore. 

Install libpam-google-authenticator Run and scan the bar code Add to the bottom of Enable in Add to the bottom of . Ensure that login succeeds using ssh keys. in Restart sshd Open a new terminal and login to check whether the two-factor authentication works 

Results When a http site is recorded, then it is recorded, but when an https site then gatling records nothing Discussion Gatling can record websites, but when these are encrypted then one could define the following: HTTPS mode: 

$URL$ Seems not to be maintained. For example, an issue was created in May and no reaction. A new issue has just been created to ask whether this repo is still maintained. When is it allowed to fork someone else repo and to publish it to Ansible galaxy for example? 

Try to start with a clean sheet by removing the .vagrant.d folder or start to remove the corrupted zip. When the file or folder has been removed, one could run again and check whether it works. 

This documentation indicates that it is possible to add collaborators by trusting their key and that it is also possible to revoke access. Having said that, the answer to: 

It depends how the current configuration looks like that is used to deploy apps. $URL$ If one uses Jenkins pipelines, then one could replace the keyId with the one that is defined in Hashicorp vault. 

Pygradle is used to build a python project. Problem The problem is that the CI runs a different Python version, resulting in different build outcomes, e.g. the build succeeded on the laptop, but failed on CI. CI 

DevOps is about preventing silos and let teams work more together then let them throwing things over the wall. In an ideal situation every individual in a DevOps team is able to create programs, test them and make them available to the customer. According to me an important competence of a DevOps engineer is communication as this is required to get acceptance in a team and among teams for new tools and processes. The more I think about it the more I get doubts regarding the structure and responsibilities in a team. From a DevOps perspective, every individual should be able to do dev, qa and ops activities, but if you compare it with for example football, all players need to be able to kick a ball, but some of them are specialized in goalkeeping and others in scoring goals. So a DevOps team could exist of QA, Ops and Dev engineers, but all of them are able to test, create and deploy software and are a basically able to kick a ball like in football. As DevOps does not mean specialzed in dev, qa and ops, like goalkeepers will normally not score as much as a striker, is CD a profession or is it one of the DevOps tasks, or should it be everybody's responsibility, but why are Dev, QA and Ops engineer part of a DevOps team as well? From that perspective there should be a CD engineer as well, but CI is not part of CD. Should two professions, i.e. CI engineer and CD engineer be part of DevOps team as well? 

Back in the day this Q&A was created. ChaosMonkey is a tool that is able to stop random services on a platform in a certain time period. The goal of this is to get a stable platform that is fault tolerant. If this happens during the day when most of the Engineers are working, they get proactive and the infrastructure will be hardened. At the moment, more and more services are created in AWS, GCP and Azure. The services on these platforms are getting more and more critical. Now it is time to onboard more and more engineers. Especially most of the developers are still silo minded, i.e. some of them only want to develop, but this is shifting drop by drop. According to Wikipedia, ChaosMonkey was invented at Netflix and additional resilience tools were created as well: 

This slide share gives an idea how to handle CasC in Bamboo. The presenter advocates to use yaml, but according to him there are also downsides. 

If the code that is shown by the GAA will be entered, login should succeed. Warning: do not implement this immediately on the production systems, but test it locally first and discuss it with the team, e.g. it could be possible that some members do not possess a smartphone 

After reading the commands chapter and finding this sample, it looks like that the custom could be omitted by just passing the arguments directly: 

The Vault documentation indicates that there is a master key and that multiple keys are required to access the secrets, but does this mean that Vault is not suitable for personal projects if there is only one PC? 

What is the definition of completed? I wonder whether that is possible. What I have seen in several environments and on github for example, the build is triggered when the branch has been merged into master. 

and everytime when and subsequenlty is run the number of anonymous volumes doubles. What will happen if nginx will be pulled instead of influxdb and grafana? 

If person A generates a secret by executing how could person B check on his/her computer the value of ? 

Using clairctl it was possible to scan postgres an other images that are available on dockerhub, but scanning local images failed 

What is the definition of a microservice? One could also argue that running SQLServer and IIS are microservices if one compares it with running IIS and SQLServer and other Windows services on a Windows server instead of separately. If one compares it with running Linux in docker then one could use a windows server as a base image by defining in a dockerfile and run tomcat in a Windows container instead of running it on a dedicated server. 

Release software more often When the github flow will be applied then less merge conflicts will occur as when one creates a new feature then one has to create a branch from the master. As features are merged into the master, there is a bigger change that the commits in the feature branch will be behind master. Discipline Although workflows could be applied, the most important thing is discipline. Developers should ensure that if they create a Pull Request that their commits that reside in a feature branch are ahead of the master. Other developers should enforce that features will not be too big as these will be harder to merge and are harder to review. 

The problem is that the format docker seems not to be available when trying to configure a "Hosted repository" 

If one would like to use the same volume in multiple containers then one could specify the following when running each container: 

As @Tensibai indicated in one the comments, this could be caused as there is unsufficient CPU or memmory, but that is not always the case. For example, a helm chart was just deployed, it failed and the workload in GCP indicated that: 

$URL$ The as defined in the other answer was required (+1). The and were needed as well. Refresh and occurrences had to be assigned to a check. It was not needed to change the handlers. 

Based on this table one could also achieve also the same purpose by start to write some shell scripts that stop random services for example. Based on the information that was read, the definition of resilience-testing is basically testing if an app and underlying infrastructure is error prone in production. This is also confirmed by this blog post that refers to how resilience testing is done at AWS, but also describes IBMs view on this type of testing. According to IBM there are two parts that have to be taken into account, i.e. problem impact and service level. In an ideal world the customer will not notice any downtime at all, but if a machine dies (minutes) or a complete data center (hours) then the customer could experience downtime. The most important thing to be transparently as possible to the customer and to minimize impact. Currently, resilience-testing is not used in the company, but the more is read about it, it becomes interesting. Especially as it is also aligned with DevOps and the company tries to make Engineers DevOps minded. It looks like that resilience-testing promotes a DevOps mindset. Resilience testing is DevOps related Some people could argue that testing is not part of DevOps, but according to Wikipedia, tools like ChaosMonkey are aligned with the DevOps toolchain. 

I suggest that you do not reinvent the wheel and use a ansible galaxy role that has already solved the issues you are reporting. One could use Geerlingguys git role to install the latest git on CentOS7. It is possible to install this by issuing and subsequently include in the roles in order to apply the role on the nodes. 

Instead of storing the Ansible code centrally, one could extract the code and distribute it to every project. Another benefit of this approach is that config and code will be seen together and that developers will change config and Ops could fix e.g. bugs instead of working in silos. Running the code per project will prevent that the executions will take longer, longer and longer. 

No need to change the Dockerfile. Just define a certain user, e.g. jenkins and a certain uid, e.g. and ensure the same uid is used when a folder is mounted. 

In my opinion k8s should not be used to build docker images as it is an orchestration platform, but there is an open issue in the GitHub k8s repository. In conclusion, building docker images on a k8s cluster seems not to be possible at the moment. 

Each of them possesses unique qualities, i.e. diving, swimming, rowing (marine), diver (machine gun, driving), green beret (climbing, carrying barrels). All of them were able to take out enemies aka automation in DevOps. Whether Operations could be compared with the marine, driver of green beret does not matter. Operation, Development and Quality Assurance all have their specialties. Combining these elements is essential to release software more often. If for example one of the commandos died in the game, the game was over. All of them had to work together in order to accomplish a mission. I can remember that each of the commandos were isolated in the beginning of level 1 and had to take out enemies themselves, but they were dependent on each other as well. The marine was required to bring both the driver and the green beret to the other island as he was the only one that could row the boot. Once on the island the Green beret was needed as he was the only one that could move explosive barrels that were required to blow up the radio station. When they were working together there was a higher chance that they could survive as three shots were needed to take out an enemy. If they shoot together the enemy was taken out immediately.