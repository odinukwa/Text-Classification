Many of the best bounds in Ramsey Theory are done by brute forcing through sets of cleverly generated (non-isomorphic) graphs. Progress in Ramsey Theory generally fluxuates between mathematical and computational advances to the problem. In general, computer brute force is often used to get some evidence for conjectures when no proofs are known to exist. For example, the Goldbach Conjecture and Riemann Hypothesis have been verified by computer search up to very large numbers. 

As other answers mentioned, the decision version of this problem (like the Halting problem and a number of other problems like the Tiling Problem) is undecidable. However, I believe you are asking about practical ways to measure the randomness of a collection of bits. Standard practice here is to run the data through a series of randomness tests, like the Chi-Square test. 

Of course, we can add another bit of external state which tells us whether the last operation was a push or a pop. We can delay moving everything from one queue onto the other until we get two push operations in a row. This also makes the pop operation slightly more complicated. This gives us O(1) amortized complexity for the pop operation. Unfortunately push remains linear. All of this works because each time a push operation is done, the new element is put at the head of an empty queue and the full queue is added to the tail end of it, effectively reversing the elements. If you want to get amortized constant time operations, you'll probably have to do something more clever. 

I am a computer scientist taking a course on Topology (a sprinkling of point-set topology heavily flavored with continuum theory). I have become interested in decision problems testing a description of a space (by simplices) for topological properties; those preserved up to homeomorphism. It is known, for example, that determining the genus of a knot is in PSPACE and is NP-Hard. (Agol 2006; Hass, Lagarias, Pippenger 1999) Other results have more a more general feel: A. A. Markov (the son of the Markov) showed in 1958 that testing two spaces for a homeomorphism in dimensions $5$ or higher is undecidable (by showing the undecidability for 4-manifolds). Unfortunately, this last example is not a perfect exemplar of my question, as it deals with the homeomorphy problem itself rather than properties preserved under homeomorphism. There seems to be a large amount of work in "low dimensional topology": knot and graph theory. I am definitely interested in results from low dimensional topology, but am more interested in generalized results (these seem to be rare). I am most interested in problems which are NP-Hard on average, but feel encouraged to list problems not known to be so. 

Definitions: Consider a polytope $P \subset \mathbb{R}^n$ with a nonempty interior to be $P : \{x \in \mathbb{R}^n | Ax \le B\}$ for appropriate real $n \times m$ matrix $A$ and $m \times 1$ vector $B$. Furthermore consider a lattice $L$ to be all points spanned by positive integer linear combinations of a basis set of vectors $\mathbb{B} = V_1, V_2, ..., V_b$ where $V_i \in \mathbb{R}^n$, so that $L = \{\sum_{i}{c_i \times V_i} | c_i \in \mathbb{Z}^+\}$. Referenced work: Random points and lattice points in convex bodies I Bárány et al Sampling Lattice Points R Kannan et al Problem: The above referenced work illustrates how one might randomly sample integer lattice points within the body of a polytope and use the results to approximate the size of the polytope, which is known to be $\#P$ hard to calculate in the general case. Furthermore they give sufficient conditions for when the sampling procedure will output a reasonable approximation (within polynomial factors). These two papers assume the input lattice is the integer lattice $\mathbb{Z}^n$. For my application, I want lattice points spanned by a separate basis set. Question: While the general procedures used in the above papers to sample random integer lattice points seems adaptable to other lattices, the bounds necessary for nearly-uniform sampling may not. Do the same bounds and sampling procedures hold immediately for other basis vectors? More directly, I am seeking a quick (polynomial time) nearly-uniformal sampling procedure that can output points in $P \cap L$ with inputs $A, B, n, $ and $\mathbb{B}$. 

I can motivate the difference for you with attack scenarios. In a first preimage attack, we ask an adversary, given only $H(m)$, to find $m$ or some $m'$ such that $H(m')$ = $H(m)$. Suppose a website stores $\{username, H(password)\}$ in its databases instead of $\{username, password\}$. The website can still verify the authenticity of the user by accepting their password and comparing $H(input) =? H(password)$ (with probability of $1/2^n$ for some large $n$ for false positives). Now suppose this database is leaked or is otherwise comprimised. A first preimage attack is the situation where an adversary only has access to a message digest and is trying to generate a message that hashes to this value. In a second preimage attack, we allow the adversary more information. Specifically, not only do we give him $H(m)$ but also give him $m$. Consider the hash function $H(m) = m^d \mod{pq}$ where $p$ and $q$ are large primes and $d$ is a public constant. Obviously for a first preimage attack this becomes the RSA problem and is believed to be hard. However, in the case of the second preimage attack finding a collision becomes easy. If one sets $m' = mpq + m$, $H(mpq + m) = (mpq + m)^d \mod{pq} = m^d \mod{pq}$. And so the adversary has found a collision with little to no computation. We would like one way hash functions to be resistant to second preimage attacks because of digital signature schemes, in which case $H(document)$ is considered public information and is passed along (through a level of indirection) with every copy of the document. Here an attacker has access to both $document$ and $H(document)$. If the attacker can come up with a variation on the original document (or an entirely new message) $d'$ such that $H(d') = H(document)$ he could publish his document as though he were the original signer. A collision attack allows the adversary even more opportunity. In this scheme, we ask the adversary (can I call him Bob?) to find any two messages $m_1$ and $m_2$ such that $H(m_1) = H(m_2)$. Due to the pigeonhole principle and the birthday paradox, even 'perfect' hash functions are quadratically weaker to collision attacks than preimage attacks. In other words, given an unpredictable and irreversible message digest function $f(\{0,1\}^*) = \{0,1\}^n$ which takes $O(2^n)$ time to brute force, a collision can always be found in expected time $O(sqrt(2^n)) = O(2^{n/2})$. Bob can use a collision attack to his advantage in many ways. Here is one of the simpliest: Bob finds a collision between two binaries $b$ and $b'$ ($H(b) = H(b')$) such that b is a valid Microsoft Windows security patch and $b'$ is malware. (Bob works for Windows). Bob sends his security patch up the chain of command, where behind a vault they sign the code and ship the binary to Windows users around the world to fix a flaw. Bob can now contact and infect all Windows computers around the world with $b'$ and the signature that Microsoft computed for $b$. Beyond these sorts of attack scenarios, if a hash function is believed to be collision resistant, that hash function is also more likely to be preimage resistant. 

I'm looking to understand the complexity of finding solutions which optimize one of the two first goals in the list. Additionally, I'd like to know how hard it is to find some satisfying assignment (or prove that none exist) - without considering the optimization in either problem above. Does anyone have a reference to a similar problem that might be adaptable to these scenarios? A hunch or intuition about the correct way to approach the problem? Or (ideally) an answer to these questions with referenced papers or solid proof? 

The PCP Theorem states that every decision problem in NP has probabilistically checkable proofs (or equivalently, that there exists a complete and quasi-sound proof system for theorems in NP using constant query complexity and logarithmically many random bits). The “folk wisdom” surrounding the PCP Theorem (ignoring for a moment PCP’s importance to the theory of approximation) is that this means proofs written up in strict mathematical language can be checked efficiently to any desired degree of accuracy without the requirement of reading the entire proof (or much of the proof at all). I am not able to quite see this. Consider the second-order extension to propositional logic with unrestricted use of quantifiers (which I am told is already weaker than ZFC, but I am no logician). We can already start to express theorems which are not accessible to NP by alternating quantifiers. My question is whether there is a simple, known way of ‘unrolling’ quantifiers in higher order propositional statements so that PCPs for theorems in NP apply equally well to any level of PH. It could be that this cannot be done – that unrolling a quantifier costs, in the worst case, some a constant part of the soundness or correctness of our proof system. 

Quantum Computing models explicitly deal with noise and ways to make computations resilient to errors introduced through this vector. Quantum Computing, curiously, can be done forwards and backwards (by nature of QM Hadamard transforms and the time independence of the Hamiltonian) - "uncomputing" is one technique used to stem the tide of such errors. On 'real' computers - Enterprise servers - there is a small but feasible chance that a bit of RAM will be read incorrectly. The theory of error detecting and correcting codes can be applied at a machine word level to detect and fix such 1-bit errors (without very much overhead). And in fact many Enterprise servers that have critical operations invite a small parity bit on each word of RAM. While far from a proof it seems to me that standard error correcting coding schemes could be made to work with almost any theoretical automata (cellular automata are suspect) with only polynomial (in fact linear?) slowdown. 

I can't get the queues to implement a stack in amortized constant time. However, I can think of a way to get two queues to implement a stack in worst case linear time. 

I give people a concrete example. Specifically, I often motivate complexity theory with the same very illustrative (but simple) problem. I ask my audience how they would instruct a small child to discover whether his name is in an alphabetically sorted list of names (and tell them that it takes the child 3 seconds to compare any one name to another). It is often the case that the person/group will come up with the naive, linear approach. I force the conversation to turn to the logarithmic algorithm (I might use a different word than logarithm) either by asking the person for something better or by mentioning it myself. I show them how doubling the size of the list only adds three seconds of work for the child with this new approach. And I compare this directly with the linear version, which will now seem entirely silly. Of course, I bring it back to earth. I tell them that child in question is generally a computer but that it could be a child or really anyone in general. That the questions we ask aren't really about computers but are more about the amount of space, time and information you need to solve problems. And I motivate complexity analysis by analogy to the two different methods for solving the same problem. When I've got their attention - I bring out the heavy hitters. I ask them "can you prove that the logarithmic solution is the best you can ever hope to do or can you find something better?" and I ask them "are there problems that no process (algorithm) can hope to solve?" I've been surprised at how people try to tackle these questions when they don't have a TCS background.