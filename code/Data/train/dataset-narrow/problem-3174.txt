I am looking for a Python library that can perform segmented regression (a.k.a. piecewise regression). Example: 

Xudong Cao. "A practical theory for designing very deep convolutional neural networks". 2015. $URL$ explains how to compute the capacity of a layer: 

Typically they specify somewhere whether they talk about the forward (a.k.a. inference a.k.a. test) time, e.g. from the page you mentioned in your question: 

I am looking for benchmarks based on neural networks libraries (Theano/TensorFlow/Torch/Caffe/…) to compare the performance between different GPUs. I am aware of: 

I am trying to understand the training phase of the tutorial Using Keras and Deep Deterministic Policy Gradient to play TORCS (mirror, code) by Ben Lau published on October 11, 2016. The tutorial says: 

It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights. More details here. 

I would like to create some annotations on some texts using BRAT (brat rapid annotation tool). Can BRAT be installed on Microsoft Windows? If so, how? The installation instructions do not rule out Microsoft Windows: 

I am annotating a new corpus with BRAT. I have a set of files to annotate. Is it possible to configure BRAT so that if no file is found, then an empty file is created? Or am I supposed to myself provide the empty files? 

As Dawny33 says, TensorFlow is just getting started, but it is interesting to note that the number of questions on TensorFlow (244) on Stack Overflow already surpasses Torch (166) and will probably catch up with Theano (672) in 2016. 

Is there any Python library that provides ready-to-use metrics to analyze the performance of a classifier for a multioutput-multiclass classification task? scikit-learn doesn't have this option yet (as stated in the documentation and in the corresponding feature request on GitHub). 

However, on $URL$ I don't see any entities that existing services. How can I create a new project? I want to use the speech to text API. 

If no pair of your classes are overlapping and they do classify each and every element of the data set, then they constitute a partition of that set. So does your clustering. In such a case, I would suggest to use a distance metric to compare two partitions, instead of any bona fide index, even if it's popular -unless that index happens to be a metric as well. One such distances for comparing partitions is the $VI$ index, rediscovered by Maria Meila$\phantom{,}^\dagger$ a few years back, but its origin dating back to the 80's in a physics paper by Zurek$\phantom{,}^{\ddagger}$. How to calculate distance between two partitions P and Q: 

If I understood them correctly, both Jeremy and Edmund's (first) solutions are the same, namely, plain euclidean distance in a 4-dimensional space of IP addresses.BTW, I think a very fast alternative to euclidean distance would be to calculate a hamming distance bit-wise. Edmund's first update would be better than his second. The reason is simple to state: his 2nd update tries to define a distance measure by considering a non-linear function of the coordinates of a 4D vector. That however will most likely destroy the key properties that it needs to satisfy in order to be a metric, namely 

Injectivity: $d(IP_1,IP_2)=0 \iff IP_1=IP_2$, Symmetry: $d(IP_1,IP_2)=d(IP_2,IP_1)$, and Triangular inequality: $d(IP_1,IP_2)\leq d(IP_1,IP_3)+d(IP_3,IP_2)\,\forall IP_3$. 

The reason for using a real metric is simple: a metric satisfy the triangular inequality and our intuitions of "close" and "similar" heavily relies on that. Any ad-hoc measure of dis-similarity (distance) that doesn't satisfy the triangular inequality will eventually give you results like this: A is "$d$ away" from B (d being concrete number), with $d$ smaller than a given bound, say $D$. Then you find A close to C by an amount $d'<D$. If this distance were not a metric, it could then happen that the distance $d''$ between B and C is say $2D$. For a sound metric distance, it should be $d''\leq d+d'<2D$. In other words A would be "similar" to B and C, but the last two would be "different"/"not similar" from each other. This is counterintuitive. The downside is that the VI distance may be computationally more expensive than calculating say a Rand index. The upside is that it's mathematically sound and makes sense. You can use R to calculate the VI distance. You can also use Partanalyzer a program I wrote some time ago ($URL$ It needs a rewrite to make it more efficient, but it's open source, so you can modify it at will. $\dagger$: Meila, M. Journal of Multivariate Analysis, 98, 873 (2007). $\ddagger$: W.H. Zurek, Nature, vol.341, p.119 (1989). 

I am looking for a program that would allow me to fine-tune pre-trained word embeddings on my data set. Ideally, open source and working on Linux or Windows. 

What are the pros/cons of using external GPUs (e.g., connected through thunderbolt) vs. internal GPUs for machine learning? 

Can BRAT be used for text classification annotation? I.e., given the text, annotate whether it belongs to some classes? 

I agree that the current trend is to use Python/R and to bind it to some C/C++ extensions for computationally expensive tasks. However, if you want to stay in C/C++, you might want to have a look at Dlib: 

However they don't seem to give any instructions for Microsoft Windows. I don't want to use some in emulation of Unix such as virtual machines or Cygwin. 

I want to try the IBM speech to text API. I created an IBM cloud account and went to $URL$ I see the error message: 

Microsoft Cognition Toolkit (previously known as CNTK) has a Python API. Amongst other things, it is supposed to be good for multi-GPU: 

The second advantage, which is also very important for large databases, is that column-based storage allows better compression, since the data in one specific column is indeed homogeneous than across all the columns. The main drawback of a column-oriented approach is that manipulating (lookup, update or delete) an entire given row is inefficient. However the situation should occur rarely in databases for analytics (“warehousing”), which means most operations are read-only, rarely read many attributes in the same table and writes are only appends. Some RDMS offer a column-oriented storage engine option. For example, PostgreSQL has natively no option to store tables in a column-based fashion, but Greenplum has created a closed-source one (DBMS2, 2009). Interestingly, Greenplum is also behind the open-source library for scalable in-database analytics, MADlib (Hellerstein et al., 2012), which is no coincidence. More recently, CitusDB, a startup working on high speed, analytic database, released their own open-source columnar store extension for PostgreSQL, CSTORE (Miller, 2014). Google’s system for large scale machine learning Sibyl also uses column-oriented data format (Chandra et al., 2010). This trend reflects the growing interest around column-oriented storage for large-scale analytics. Stonebraker et al. (2005) further discuss the advantages of column-oriented DBMS. Two concrete use cases: How are most datasets for large-scale machine learning stored? (most of the answer comes from Appendix C of: BeatDB: An end-to-end approach to unveil saliencies from massive signal data sets. Franck Dernoncourt, S.M, thesis, MIT Dept of EECS) 

web_A= (1*(.037))/ (1*.041 + 1*(.0352) + 2*.047+ 1*.037 + 1*.081) web_B= (1*(.081))/ (1*.041 + 1*(.0352) + 2*.047+ 1*.037 + 1*.081) All other variables would have zero contribution for the purchase. A could make this a binary outcome, “purchased” or “not purchased” and use logistic regression, but I thought this approach would give a more granular view. Is this a sound approach? 

This equates to person_1 being served an add on desktop with digital_ctr_B on web_B. Count the number of purchases that this person makes before getting another ad ( they may only get one ad). If they do not purchase before they receive another ad they receive a 0. For example, 4 days later they receive another ad of the form 

All other variables would have zero contribution for this purchase. Suppose there was 1 purchase for vector_2. I would get contributions of 

meaning that they have now been served an ad on desktop and mobile, and seen digital_ctr_B twice, once on web_A and once on web_B. Again, count all the purchases they make before they receive their next ad exposure. Since this is sensitive the time between exposures I thought about modeling the problem as the average purchases per day between exposures. For example if person p_1 made a purchase on oct 2 and oct 3, then we would have y= 2(purcahses)/4(days)=.5 Could I get the coefficients for each variable and then for each purchase divide each unique coefficient-variable product by the sum of the coefficient-variable product for that exposure. Suppose the coefficients for the variables are 

Each time an ad is served to a given person they may have any combination of the above. For Example, on October 1, person p _1 may be servered with an ad with the following attributes: 

I’m working on a problem that is analyzing purchases at a store and determining the effect from advertising. There are many advertising variables, but to keep things simple, assume there are three types of media: 

Eventually you could play around with different weights and set a kind of normalization where you could fix the value of the maximal distance $d(0.0.0.0,FF.FF.FF.FF)$. Furthermore, this set up allows for more complex descriptions of your data where the relevant distance would contain "cross-products" of coordinates like say $g_{13}*(x_1-y_1)*(x_3-y_3)$. EDIT: While this would be a better "weighting" method than using those other I addressed, I realize now it is actualy meaningless: As Anony-Mousse and Phillip mention, IP are indeed 32 dimensional. This means in particular that giving the same weight to all bits in say the 2nd group is in general not sound: One bit could be part of the netmask while the other not. See Anony-Mousse answer for additional objections. 

The latter is key for later interpreting small distances as close points in IP space. One would need a linear (in the coordinates) distance function. However, simple euclidean distance is not enough as you saw. Physics (well, differential geometry actually) could lead to a nice solution to this problem: define a metric tensor $g$. In plain english, give weights to each pair of coordinates, take each pair difference, square it and multiply it by its weight, and then add those products. Take the square root of that sum and define it as your distance. For the sake of simplicity, one could start trying with a diagonal metric tensor. 

Determine $P\wedge Q, $the "intersection" (I always mix up the words join and meet here) of P and Q. This is simply the partition formed by taking the intersection of each cluster of P against each cluster of Q. For all three partitions $P$, $Q$, $P\wedge Q$ calculate their entropy $H$ defined as follows: $$H(P)=-\frac{1}{N}\sum_{i=1}^Np_i\log p_i\;,\;p_i\equiv\frac{n_i}{N}$$ where $N$ is the total number of elements you clustered/classified and $n_i$ is the number of elements within cluster/class $i$ of a given partition $P$. The distance $d(P,Q)$ between partitions P and Q is then given by $$d(P,Q)=2H(P\wedge Q)-H(P)-H(Q)$$