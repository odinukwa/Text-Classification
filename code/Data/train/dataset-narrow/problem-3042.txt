Neural networks are actually extremely effective at performing dimensionality reduction. A great example is word2vec, which applies a shallow neural network to reduce inputs on the order of several million features (i.e. unnormalized text) to a 30-150 dimensional vector via a process that is mathematically analogous to matrix factorization (which is the class of techniques PCA belongs to). Autoencoders function very similarly to word2vec: if you're planning on using an autoencoder to learn an embedding of your data, I wouldn't expect you'd gain anything from applying PCA first rather than using an autoencoder (or something similar) to learn a better embedding for your data that isn't constrained by the assumptions of PCA. 

Sampling is a totally good option, especially if the size of your data is bogging down the tool you're using to plot it. If that's not the problem, a common issue is that plotting opaque markers will show you where data is located, but will disguise density information. For example, imagine a situation where every pixel of your plotting area is associated with at least ome observation (i.e. you have a uniformly colored plot) but one pixel is actually associated with 99% of your data. A good technique for situations like this is to try to visualize the density of the data. A simple approach is to add transparency to your markers (often by adjusting the "alpha" parameter), or you can model the density more directly with binning (e.g. a histogram or hexgrid) or with a kernel density estimate. If you have discrete data, overplotting will likely be an issue but density might give you weird results. A good way to address this is to "jitter" your data by adding noise to one or more plotting dimensions to force your data to spread out more. If you have time series data, you can resample to a coarser resolution: e.g., if you have a data point for every millisecond, your data will probably be easier to visualize if you aggregate by hour, day, or week. Similarly, you can summarize the data by plotting a model. Plot $X$ vs $E[Y|X]$ instead of $X$ vs $Y$and throw in some error bands for good measure. All that said: just try plotting it first and see what happens. Your visualization tool might do some stuff under the hood to render at least some of this manual effort unnecessary. 

I can't tell you for sure without you describing your calculation more or showing code, but my guess is you're not actually calculating the posterior probability here. I bet this is just the conditional likelihood, or at best the unnormalized posterior. Remember: the posterior calculation has a division component. Does yours? You're probably forgetting to divide by the "evidence". 

And our mean proximities, again rescaled to [0,1] with 1 indicating the most similar pair of groups: 

If you use an optimization algorithm that naively shrinks the learning rate relative to the error, you're going to get stuck as soon as you hit the valley. On the one hand: congrats! You've achieved a low error solution. But you're not necessarily anywhere near the global minimum. So how can we do better? An approach used by modern gradient-based methods like Adagrad, RMSProp, and Adam is to separately assign learning rates to each parameter, and tie the learning rate to the magnitude of the respective parameter's update. The Stanford CS231n lecture notes explains: 

You need to use appropriate activations. If you were using softmax for those two components, they'd be constrained to sum to one. LeakyReLU not only doesn't impose this constraint: it can output any real value, so it isn't even constrained to [0,1] for those components individually. 

The normalization parameters you fitted in training are now part of your model. You fitted the model weights on the training data: the normalization step is part of your model now, and the "parameters" for that step are the mean and variance of the training data, not the test data. To help make this more concrete, let's pretend that this model is in production. You fit your model on whatever training data you had available, and now it's running as a scoring service. You can evaluate your model accuracy over time as data you've scored gets labeled to ground truth, but you're just evaluating the model not refitting it. How do you normalize these incoming predictions? You're going to need to use the mean and variance from whatever dataset you used to fit the model. If you'd rather do something like using the last K observations to estimate mean/variance, then that's the procedure you need to be testing when you fit the model. 

This is a common approach to address class-imbalance. You'll generally see it referred to as "upsampling" or "over-sampling". In addition to simply reusing existing data, you can "augment" your data by transforming it or adding noise, or generate synthetic data. A good demonstration of data augmentation is the winning solution to the kaggle Galaxy Zoo contest. A popular technique for generating new data is SMOTE. Keep in mind: if you change the class proportions in your dataset, your effectively changing the prior for those classes. Depending on your choice of algorithm and what you are trying to accomplish, you may want/need to apply a probability adjustment to the model's output. Another technique you can use is to just pick a different decision threshold. For example, if you're performing a logistic regression and don't care about performing inference on parameters, duplicating the data for one class won't actually change the slope of the decision boundary, it will just shift its location. You could equivalently use the decision boundary learned on the unmodified data and then pick a cutoff other than $P(X=1) \ge 0.5$. A good heuristic is to use the threshold which maximizes Youden's J, which can be trivially calculated from a ROC curve. If you have a specific model evaluation metric you are concerned with, choose your threshold relative to that. 

As far as I can tell, training neural networks is still more art than science and the amount of skill, patience, and care taken in training a neural network (depending on the architecture) can have as much impact on the model's performance as the network topology. Consider issues like mode collapse in GANs, vanishing gradient with ReLUs, or just getting stuck in local optima. Neural networks often need to be "babysat" during training, and consequently trial and error may contribute significantly to certain training process decisions. I'm sorry if that's not a satisfactory answer, but I have a strong suspicion that's what's going on here. If you post some specific examples, it might be easier to figure out concretely what's going on, but otherwise my guess is that this was just the result of trial and error. Regarding using parameter-specific learning rates generally, most modern optimizers do this, including AdaGrad, RMSProp and Adam. 

Rather than modifying your data, use great circle distance for those features' conmponent of the cost function. 

Contrast the results from your original model with the what happens when we replicate your dataset 100 times: 

The notation they're using is a bit funny. $f$ is just the dot product between the input and weights, $w^Tx_i$. The loss function is what differentiates those classifiers. Each loss function takes that dot product as an input and uses it in different ways. All of those loss functions compare the output of $f$ in some way with an associated $y$ value: you are computing your loss relative to a target variable to understand how well your model is performing, and then you can compute the gradient of the loss function to improve those predictions. If you ignore the loss function, you're just left with linear regression predictions (and even linear regression needs a loss function, MSE, to fit the weights) which aren't being evaluated relative to anything. 

You're absolutely right and yes, it is actually possible to overfit to your validation data if you're not careful. Some researchers at google published an interesting article about this problem and a way to address it called the Reusable Holdout. The general idea is that you only access the test set through a special intermediary algorithm. Obviously this isn't how most people work though. In practice, I think a common approach is to use several holdouts: use one for most of your prototyping, and then once you're satisfied you can extend your evaluation to one or more of your additional holdouts. 

These are just some ideas off the top of my head. Don't let yourself be limited by my suggestions. If you really only have the data you've shown us, a project you can try that would be more likely to give you good results would be to predict just the number of people who will visit the library on a particular day, rather than which actual people. If this sounds like it might be useful to you, check out Poisson models. 

For certain models, it can be important to make sure the inputs are linearly independent. I'm pretty sure this is never the case for the outputs. If we have more than two classes, it's extremely difficult if not impossible to output class-probabilities if we don't have a specific dimension for each class in the output vector. In other words, your suggestion has much more explanatory power than what was suggested by your interviewer, which is solving a problem that exists on the inputs for some models but not for the outputs. In multi-class problems, it is indeed the norm that each class has their own dimension in the output vector as you suggested. Either the interviewer was trying to look smart and confused input and output encoding (which still would only apply to certain families of models), or you aren't remembering the events of the interview properly. If you're confident that the interview went as you described, you should consider complaining to your point of contact that you were criticized for giving a correct answer. Feel free to cite the following as popular/canonical examples of multi-class classifiers: 

The problem here isn't your chosen neural network architecture, it's that you're working with literally unseparatable data. There's isn't any practical way to distinguish observations near the mode of multiple distributions that share a mode. The best you can do is approximate the distance from the mode at which the high variance distribution is more likely than the low variance distribution, and classify observations relative to that. You can do that analytically, and your neural network isn't going to do any better. Have you tried visualizing your data to better understand what you're tasking your model with here? This is a bit more extreme than "kind of overlapping" distributions. 

Depending on what you're working with here, one approach that might work would be to "jitter" the data. In other words: add noise. Your concern seems to be about people getting recognized from specific timestamp values, so jitter those. depending on the data, you could end up with new timestamps that don't align with any individuals in the data at all while minimally affecting the public's ability to draw inferences from the data.