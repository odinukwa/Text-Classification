You can adjust how often you want to dump the content of ip_conntrack by changing LOOP_COUNTER and LOOP_TIME accordingly. So to get it every 5 secs, it would be: LOOP_COUNTER=12 , LOOP_TIME=5. LOG_DIR is imply where the logs would be saved to. Afterwards you can use zcat to cat files you're interested in and use grep to filter source IPs/ports of your interest (or just use zgrep). will count whatever you're after. You can also use . 

So far the solution that worked best for me was to just grab the contents of /proc/net/ip_conntrack every 20 seconds, log that into a file with file name containing appropriate timestamp and using those as input to any of the filtering scripts, or even oneliners when necessary. To save you time you can use my script. I use crontab entries to make sure the script is ran every minute (it lasts for 60 seconds in the current configuration, feel free to modify it :-) 

Assuming you have just one network interface in that physical machine you will have to do a bit of juggling with routing and firewalling. That's because one NIC will actually have to communicate with 3 different networks: 1. Your ISP; 2. Your y.y.y.y being the pfSense router/gw for LAN; 3. Internal LAN-facing 192.168.0.0/16 subnet. You may look at vtun (Debian/Ubuntu) to create virtual networks. You've stated that: "pfsense can ping and traceroute resources in internet. But obviously is is unreachable from internet by its ip y.y.y.y". This indicates routing issues on x.x.x.x server which I presume is physical machine with Xen (and pfSense) virtual machine. Please check if you can reach y.y.y.y from x.x.x.x. Check if there is proper routing set up on the host and virtual machine (if they can reach other other) via tcp/udp (netcat comes in handy as well as tcpdump if you need to see what happens there). gives you the routing. If routing is missing you can it with: Another thing to check is the firewall on your x.x.x.x machine. Check if it lets the traffic pass. "net.ipv4.ip_forward = 1" is one thing but another is iptables filtering or DENY policy. In order for the whole communication to work properly, each machine needs to know how to reach another one. So test various combinations and if you get stuck somewhere, check the next hop i.e. if pfSense virtual machine cannot reach Internet, check if it has default route entry (most likely via x.x.x.x) and knows how to reach x.x.x.x at all. These links regarding routing may come in handy: $URL$ as well as $URL$ 

It is unclear to me how Azure Automation DSC can allow for such coupling. Sometimes just the account name is needed (to configure Sql Server), but sometimes the resource must be run as the owner, i.e. the owner credentials are needed. Unless there is a way for a DSC resource to run as the workstation owner without knowing owner's credentials. In short, it seems to me that Azure Automation DSC cannot be used to configure developer machines. And I am not even talking about shortage of out of the box and tested DSC resources like: 

Next, I have no idea how to push to a different branch. For instance, a developer works in the default branch and wishes to backup its local commits to the special backup repository shared by all the other devs. Of course, this backup repository is not the one from which developers pull the new changes. Anyway, the backup needs to go to a branch dedicated to that particular dev, as stated per options 2. How do I do it? How can I tell push that the changes are pushed to a different branch? 

I used ssh-host-config both on vm and srv to configure the ssh to run as a windows service. Besides that I did nothing else. Can anyone help me troubleshoot this issue? Thank you very much. EDIT The virtual machine software is VMWare Workstation 7.1.4. I think the problem is in its settings, but I have no idea where exactly. The Network Adapter is set to Bridged. EDIT2 All the machines are located in the company lab, I think all of them are on the same segment, but I may be wrong. Below is the output for each machine (skipping the linux server). I have deleted the Tunnel adapters to keep the output minimal. If anyone thinks they matter, do tell so and I will post them as well. In addition ping output is given to show that DNS is correct. Something else, may be relevant, may be not. Doing to srv works OK, whereas to vm failes with Access Denied. srv: 

Even though VirtualHosts log to their own error_log files, I found output from in . I ended up wrapping with the following shell script: 

Maybe the line-ending conversions in vsftpd were written inefficiently, and since binary mode is most commonly used, no one has bothered to improve the algorithm used in vsftpd. Or, it could just be that passing data from a tcp socket out to disk really is a lot faster than having to check every character for CR and LF. The check could introduce enough latency in the connection to reduce your transfer speed. Are you running tests locally on Ethernet (low latency, would be affected greatly by additional latency) or across the Internet? 

For one-shot deals, scp is handy. If it's a lot of files, then rsync is a good idea. If a connection is dropped, rsync can pick up where it left off. I knew that rsync had compression (), and have just learned that scp does as well (). 

I can't comment on SEO, but the typical use of the 204 response is to not refresh the current page. It's probably most appropriate with a request other than GET (like POST or DELETE): Excerpt from HTTP/1.1: Status Code Definitions: 

And continues logging after the rotate -- it's just named (and possibly other services) that aren't connecting to correctly. Here's /var/log/messages-20120212: 

Two weeks ago, I was notified by my VPS provider that my server (CentOS 5.5, yum is up to date) had originated "NULL byte/Directory Traversal" attacks agains some servers at DreamHost. I spent a few hours going over the server with a fine toothed comb and didn't find anything. Before logging in, I retrieved a copy of the sshd binary and confirmed that it hadn't been modified. I installed a rootkit checker (chkrootkit-0.49) that found nothing. I checked the web logs of the websites I host, looking for a hit that may have triggered a script on my server to initiate the attacks but found nothing. Checked /var/log/secure and /var/log/messages around the times of the attack but found nothing. Checked , but found nothing. Did a on key directories looking for files modified in the past 3 days, but nothing. What else can I do to to find the cause of the attacks? I wrote a script to check for outbound TCP connections on port 80, but only came up with legitimate connections (SpamAssassin and ClamAV downloading updates, Joomla checking its site for updates, etc.). Even if I did see an active outbound connection, would I even be able to dump data from the process (in the directory) to show me the account originating the attack? After watching the server for a few days, I gave up. Now I've received another complaint from DreamHost, so it's happened again. I've requested detailed logs from DreamHost, but then what? Where else can I look? If I can't find the source, is there something I can install to monitor the server and log data when it starts making outbound connections to tcp/80 in the DreamHost IP space? What would I log? Just get a of all traffic in that timeframe and try to sift through it manually? Update See my accepted answer for the solution I came up with. I'm still interested in options for logging the source of all outbound port 80 traffic -- a way to know what the source process is and perhaps its parent process (and the parent's parent). 

Suppose I have a product which I want to install on a non SSD drive, if exists. Otherwise, I want it to go on the drive C. For example, the following configuration is supposed to install the Sql Server 2016 in the default location: 

The Azure Automation DSC seems to know to integrate with GitHub, but it seems to expect only runbooks there. At least, this is what I understand from the following UI screen: 

Server Manager → Manage → Server Manager Properties "Do not Start manager automatically at logon" Server Manager → Local Server → IE Enhanced Security Configuration → Off 

However, I would like to check first if there is a non SSD drive and if present install it there. As I understand it, I cannot use Powershell code, because that code runs during compilation. I need it to run during the configuration. How do I do it? 

The problem is that the system temp directory is required when machine installs updates first thing after a restart. My RAM disk, however, starts through , which is probably read later in the process. My question is this - is there a way to redirect the system temp directory to a RAM disk in a way that this redirection is realized before windows installs the updates after restart? Use case: I have installed a RAM disk that gets initialized on startup. Everything works fine, including rebooting the server (it is possible to save the RAM disk image, making it survive the reboot). However, when I reboot the server as a result of Windows Update the update is undone, because Windows is unable to access the TEMP directory - the RAM disk has not been loaded yet. Truth to be told the problem is only with the system TEMP directory and there is not much harm not to redirect the system TEMP and leave it as is. Still, if I could load the RAM disk before the Windows Update resumes then I could place the system TEMP on the RAM disk as well. My question - is it possible? 

For your second solution, I think you can simply use "#" in your .qmail file instead of piping through . 

It's going to be almost impossible to get "varied" IP addresses for a single host. An ISP providing VPS services is going to have a block of IP addresses on the same subnet assigned to the machine hosting the VPS instances. You could get a few IPs, but they'll be contiguous. Also, it's unlikely the ISP will appreciate you running a honeypot on the VPS, especially if it's not set up correctly and hackers end up using it to attack other hosts on the ISP's network or the general Internet. Why not run the honeypot off of your home Internet connection? Set up your router to route all packets to a single machine on your internal network. You could even route them to a virtual machine running under VMware on one of your other systems. If you really want to have honeypots on multiple IPs, you should just get multiple VPS instances with different ISPs. 

CentOS 6.2, bind 9.7.3, rsyslog 4.6.2 I recently set up a server, and I noticed that named had stopped logging to after the logs had rotated. I thought that was odd, since all logging happens through and doesn't write directly to the log file. It was even more odd because I had HUPed after updating a zone file, and it still wasn't logging. After I stopped and restarted named, logging resumed. What's going on here? The syslog PID hasn't changed (/var/run/syslogd.pid matches the PID shown in ps). Is rsyslog opening a new socket when logrotate rotates its logs and HUPs it? /etc/logrotate.d/syslog: 

Are you sure you know everything the scripts are doing? Maybe the process is disk bound. Some Linux machines run every night and update the database used by . To do so, they scan the entire disk compiling a list of filenames. There's a lot of I/O but not much CPU usage. Maybe Windows does something similar? Compiling statistics by parsing the web logs? If you really are doing lots of SQL queries, maybe they're running on non-indexed fields of large tables, and MySQL has to scan through a lot of records. I think you need to take a closer look at what the nightly process does, in order to track down why it takes so long. 

EDIT 6 I followed all the instructions. At the end and after a few days I noticed I have it! Switched to advanced analytics and here I have it: 

I have uploaded a PFX file to the Azure Automation account - it is now a certificate asset over there. Now, I would like to have a DSC configuration that ensures this certificate is present in the LOCAL_MACHINE\My store on the target machine and its private key is accessible by a certain domain user. I found the resource, but it is unclear to me how I can use it. Any ideas? EDIT 1 My current solution is as follows: 

The following article explains how to generate a silent installation of VS 2015 - $URL$ The essential command is vs_enterprise.exe /Layout SomeSharedDirectory In the end it created the following folder structure: 

The ISO file itself is not small - over 7GB. But the folder is huge - 19GB, which are added to the same content of the ISO. My question - do I really need to keep this 26GB if I want to have a silent install of VS2015? Is it safe to delete the folder? And if it is, 19GB is still an awful lot. Is there a more space efficient way to arrange the silent install? 

Part of configuration is to run all the Windows Updates. I am trying to figure out how to express it with Powershell DSC. Seems like the best is to ensure that the windows updates are scheduled regularly. I found xWindowsUpdateAgent DSC resource, but it does not allow to specify the schedule itself, only to ensure that the updates are scheduled. So, is it possible to ensure a concrete windows update schedule? 

I am trying to grant a read-only access to my temp directory to another user from the command line using icacls.exe. Tried the following (PowerShell): 

Renamed SSMS-Setup-ENU.exe to SSMS-Setup-ENU.zip Peeked inside. The only text file is named "0" and this is an XML file. The very first element is from which I concluded that the exe was created with WIX burn. Renamed the file back. Found $URL$ Downloaded and extracted wix311-binaries.zip from $URL$ Ran , which extracted a lot of msi files from SSMS-Setup-ENU.exe to . Found $URL$ Saved the PowerShell script from the answer and modified it a bit to ease the piping (the script code is below). Piped all the msis through the script and identified the two msis with the Product Codes in question.