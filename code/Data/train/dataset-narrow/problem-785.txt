Look at the query plan for both queries, you will see that the "order by" clause will result in a sort of the data that is returned by the remainder of the query (which should be in memory, but could be paged if memory insufficient). The time that sort takes is related to the amount of data (it has to walk it at least once) and how well ordered the data already is (it may be correctly ordered if you are sorting on an indexed column, or data joined on a indexed column) 

Somehow you need to maintain a list of the stored procedures that relate to each software version. There will be a pile of ways to do this, but the version/procedures will need to be documented clearly somewhere or maintaining the code base will be difficult (expensive) and end up buggy. One method that springs to mind is to maintain a list of the procedures and their versions - as a database table or text list or web.config or compiled into your application. This list would have the procedure name and the version that it corresponds to (maybe with a base version) Then in the code you will need to write a method or class that will get the relevant procedures name as a string that you can return to the Data Access Layer (DAL) to use - I am assuming that the DAL know which query to run. To keep it simple, have a base list and then only put in "version" procedure entries where they are needed and get the most recent one. You could take this a step further, and dynamically create the procedures that you need by passing the query text or variables (procedure names, table names) to be substituted into your query in the stored procedures at run time. ie: if procedure foo needs to call procedure bar.vn the pass bar.vn as a variable to procedure foo. This allows you to dynamically take into account different versions, but wont allow the database engine to effectively create statistics for that procedure. Here is a example of how to code something like this up $URL$ The best solution is to not have multiple versions at once, try to have the different functionality as different execution paths in the application (code or stored procedure). It will mean a few IF..ELSE bits of code, and what looks like a bit of work, but is much easier to maintain because the alternatives are in front of the developer - they do not need to know about which version needs which procedure. 

Sounds like a messy thing to do from a management point of view. Just how do you plan to backup that many databases? with a script that loops though each one? Unless you have a really good reason, why not just have one database where the structure is designed so that all the data will link to back to a customer ID. Add Indexes/Foreign Key/Primary Keys based on this field which will ensure data integrity. Then you just need to have a where clause in all your queries to access only one customer ID. This will be much simpler to maintain and is just as easy to develop (cause in either case you need to allow for the customer identification) 

This is something you can use views or queries for. Have one table with the raw data and another with the conversion ratio that you need to apply - maybe with a date or some such other information if the ratio to apply is time or situation sensitive. Then create a query or view that does the conversion you need by joining the tables together. This way you can change the conversion ratio value as required and recalculate. Simple example (PostgreSQL) for a hypothetical scenario, yours will be different: 

Some thing not to look at are the disk sizes of tables and indexes as they may be different in the restored database (I would expect smaller in the restored version, but it depends on a lot of things) 

Once you have normalised you need 2 queries, one to load parents then one to load children (you can only insert into 1 table at a time using sql) It sounds like you do this ofter, so why not create a file with the sql commands that you use, and each time you do this task open the file up in your databases sql editor window and run (may-be first modifying) these commands. If it always the same sort of task, you could write a script that connects to the database and runs the sql statments you need - then it is a one click or command task. 

You can use the import data tools that are part of access to import data from one database into the other. It is under the "External Data" tab. This should work as long as you have network access to both databases. Some more info : $URL$ Before doing this, have a little think about your table design - do you need to have a flag to indicate which store the data came from? If the data is normalized, you may have have issues importing data that may be resolved by the order that the data is imported in. Also, remember to regularly backup these database files before trying to import. This way if you break something you can go back and start again. Also store them on physically different disks just in case.... 

From your question, the key is how you intend to replecate the data, and where you will store that. How you do that would depend on how you update your database engine, hardware and performance requirments. You could use your database engine to create backups (differencial or full) that you restore into another database instance, possibly on different hardware (periodical update). You could create trigger functions (real time update) or a regular batch process (periodical update) that port the data across to your reporting instance - either a different set of tables, or in a difference table space or a different database (engine dependant). You state that the data will be logically isolated from the active data, so you wont get any logical contention (table locks) between the main and reporting parts of the application but you could have issues with disk and CPU between the main and reporting parts of the application that would affect the main applications performance if the reporting data is on the same hardware as the main application database - but that would depend on load and hardware spec. 

When it comes to databases, less is more. You are doing the right thing by archiving off old data as this will speed up the performance and reduce the maintenance time of the active database. The archived data needs to be considered. Keeping it in a separate database instance will be beneficial because backups/restoration and other maintenance activities will be completed separately - reducing the effect on the live data. You should consider keeping the archived data on a physically different disk to the live data - this will avoid conflicts if both are being used at the same time. Also consider the type of disk this the archive is on as this has cost implications - does it really need to be on a raid type array or is a single disk and backup tape/dvd all that is needed? Storage may be relatively cheap, but it is still a cost in $, IO, time and network. Next take a look at the archived data - do you really need to keep all the records? do you really need to keep all the columns of data? Would changing the structure of the data result in a smaller disk footprint/faster read query time? Can you summarize the data? Archived data for data mining does not always need to have the same structure as live data. Keeping separate months data in separate databases may be a good way to go, but there other options that may make the analysis easier/quicker. Your options here include keeping the data in one database instance, but with table per month. Another is to have one set of archive tables and use table partitioning (read the manuals). 

For what you want to do, I would recommend the following (which is pretty much what you were thinking). 1> Create history tables for the historic data you have - keep the schemas as similar as possible. Split up by some logical grouping (such as year/month) based on how they are going to be queried (say you need to report with in month/year as well as all). Do not worry about the table size of the splits unless they are getting into the TB size range (your dbms should handle it) just make sure that they are appropriately indexed for the queries that need to be run. You should consider putting these onto a different disk to the active data if performance is an issue. 2> Create a routine to move data from the active table to the relevant historic table. Run this periodically. As a practice rebuild the indexes on the table that has had the data removed from it, and maybe update the table statistics. Easiest way to do this is to write a sql script. 3> Consider the reporting you want to do. If you want to only have to deal with 1 table when writing queries, create a view that joins the archived tables together. Create indexes on all the tables to suite the view. This way if you want all the data, select from the view. If you want data from a specific year/month, query that table. The view will look something like: