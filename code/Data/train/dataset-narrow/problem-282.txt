Small packets aimed at hammering the application of the server. Large packets aimed at exhausting available bandwidth. 

Enable IPv6 throughout the network, until IPv6 capabilities and connectivity are on par with IPv4. Wait for the day to come when IPv4 is no longer relevant. 

There is of course also the possibility of using large packets if the attacker knows that a well-crafted packet can cause additional processing time on the server in excess of the additional bandwidth cost. Simply dropping UDP packets below a certain size is far too blunt of an instrument to really be commendable, not least because this may well just result in your attacker changing strategy. For example, if the focus was to hammer your servers, the attacker could switch to an DNS amplification attack that aims to exhaust your available bandwidth. 

Given the prudence and importance of address conservation, the general approach to using a /31 should be "if it works, use it". Of course, you could take this a step further and start using private space for your point-to-point links, but this obviously can be problematic if you're going to run traceroutes from across the internet rather than within your own network, although even that can be mitigated somewhat by configuring your router to issue ICMP errors with a specific source IP address. In short, do whatever you can to waste as few addresses as possible (within the limits of best-practice and feasibility, don't start throwing NAT concentrators up everywhere) 

At layer 2, all load balancing is, at best, done by an XOR or hash of the source and destination MAC, and if you're lucky, it may even read into layer 3 and hash that data too. At layer 3, however, where we're basically talking about multiple gateways (so, effectively, two physical links with a unique next-hop across each) you can max out the bandwidth across the links IF you're prepared to do per-packet balancing. Before I go on, per-packet balancing is generally a bad thing due to the fact that it can result in out-of-order packet delivery, this can be especially terrible with TCP connections, but that of course comes down to the implementation and most modern stacks can tolerate this relatively well. In order to do per-packet balancing, obviously one requirement is that the source and destination IP addresses are not at all on-link to the devices that have the multiple paths since they need to be routed in order for balancing to be possible. Redundancy can be achieved via a routing protocol such as BGP, OSPF, ISIS, RIP, or alternatively, BFD or simple link-state detection. Finally, there is of course a transport layer solution - protocols like SCTP support connecting to multiple endpoints, and TCP already has drafts in the making that will add options to do similar things. Or... you can just make your application open multiple sockets. 

I'll toss out for consideration what most will consider a bit more unorthodox solution. Consider solving this with Layer 3 instead of Layer 2. Put both switches into place, and DON'T interconnect them. Connect the router(s) to both switches. Connect your HP servers to both switches. Use two different IP blocks internally for the servers to talk to the router(s)...one block on each switch (and therefore interface on the router(s) and servers). Put the IP addresses that you actually use to communicate with the servers on a loopback interface. Put quagga on the servers and run OSPF (at your scale, just throw everything in area 0, no biggie)...make sure the loopback addresses/interfaces are included in the OSPF config. Put OSPF on the router(s). Voila', the router(s) learn about the addresses that you're actually using to talk to the servers via OSPF as host routes...if a switch dies, the relevant adjancies go away and traffic gets rerouted over to the other switch. As a bonus, if you use different IP address for the various services that you run on your Linux servers, you can move the services and their associated IP addresses around seamlessly and the network adapts cleanly and easily. No danger in this setup of having bad behavior from a split-brain situation if the link between the two switches fail...no danger of bad behavior from a FHRP like VRRP, HSRP and the like...no danger of the switches falling back to inefficiently flooding traffic if you run into an asymmetric situation. I use this solution in a much larger sort of environment and its works EXTREMELY well, is amazingly robust and resilient to both equipment failure and human configuration mistakes. 

This is basic subnetting/binary logic. There's 2 /25s in a /24, 2 /26s in a /25 (ergo 4 /26s in a /24) and so forth. The closest match to the requirements is to use which will leave you with one spare subnet. You cannot by definition have an odd number of subnets exist from any given subnet size. Either your teacher actually intended this and the 7 was a red herring, or somebody failed hard at math. 

Backpressure refers to what is essentially concentration of traffic. E.g. I can have 10 x 1Gbit links internally that are all feeding into a 1Gbit link that provides me with Internet transit. at saturation point, the router can store packets in its buffer and/or drop them - with no particular configuration, a router will generally fill its buffers and then tail drop, this gives rise to two problems: buffer-bloat and tcp global synchronisation. The first refers to a case where the buffer is constantly filled due to constantly saturated link utilisation. The second refers to the issue of hosts re-transmitting dropped packets all at the same time, thereby causing a burst of traffic and thus, more drops, more retransmits, ad nauseum. RED was conceived a long time ago as a means to deal with this issue; namely by randomly selecting packets to drop during times of congestion. This however required careful tuning according to the properties and expected behaviour of the link. Fortunately things have moved on and AQM (Active Queue Management) is now the cutting-edge of the industry. A top-notch example of AQM is CoDeL - this is an algorithm that focuses purely upon the sojorn of a packet through the system and aims to ensure packets are passed within a specific time rather than caring about whether or not a certain amount of bandwidth/buffer is being utilised. 

A closed port is a port that doesn't have any software listening on it, so an attempt to make a connection to that port on that system will result in the system sending back a TCP RST packet. A filtered port, on the other hand is typically a port that is blocked by a firewall in the network path, so an attempt to make a connection to that port on that system will result in nothing coming back at all...not even a TCP RST...so a connection attempt will sit there until TCP times out the attempt to connect. 

OK, so I have revisited this issue several times, and now have working IPv6 with TWC via my SRX. I wish I could mark several answers and comments as correct as several of you gave me parts of the overall solution. Thank you all, for that. I've upvoted the answers that contributed to give as much as I could to contributors. As it turns out, the DOCSIS cable modem mini-pim was part of the problem. It seems that TWC will not send IPv6 RA messages to you if you have an "unsupported" modem. So the solution to my problem was to go by a TWC supported modem (I got a Motorola SB6141), and connect it to one of the gigabit ports on my SRX. My interface config now looks like: 

Our network isn't even all that big and we utilize this network design pattern. It just makes manageability easier. Scale-ability just completely goes away as an issue. And ultimately it becomes a little easier to grok what's going on in the network. The general idea is that the network infrastructure uses OSPF (or IS-IS, or other sorts of IGP routing information), while "customer" routes are carried by BGP. Where "customer" there could be IP blocks at the edge of the network, routes from upstream providers, routes from 3rd party partners and VPNs. The result being that you can look at the source of the route...if its from an IGP, you know that its describing the internal network topology of your network, and if its a BGP route, you know its a "source" or "destination" network (at least from the perspective of your internal network). We go so far as to have Quagga or other routing protocol implementations on some of our Linux servers...they're peering using eBGP to their upstream routers (typically 2), and inject routes for IP addresses on their loopback interfaces into BGP. Using BGP allows us to put a lot of policy enforcement on the edge routes to only allow the Linux servers to advertise certain specific IPs. Overall this use case works extremely well, reliable and hugely scale-able. It does take a bit of mind-set change from typical Enterprise network design, but its well worth it, IMO. 

Using a /127 isn't terrible, but letting it go into your backbone as a /127 is. The reason for this is that, essentially, most modern router TCAMs can typically only handle up to 64 bits of address width at a time - this means that if you're in a situation where all routes are /64 or shorter, lookups can occur in a single cycle. Anything longer and it has to perform another lookup operation. Even on a TCAM that only has 32 or 48 bit width, going beyond /64 is obviously still significant. So, my personal recommendation is to allocate a /64 for every P2P link even if you only use a /127 on the wire - that way, when you bring up your routing protocol, you can then aggregate the /127 to a /64. My personal favourite, however is to allocate a reasonable chunk of your IPv6 space purely for facilitating P2P links (in my case, I reserved a /48) - this /48 is then blocked on all network edge interfaces at ingress as a destination. In this way, you're free to just go ahead and use a /64 on your P2P links and still have traceroutes, ICMP errors et. al work, but you are not vulnerable to NDP attacks from outside. Obviously not everyone is going to care about this and if the additional cost of using longer prefixes is acceptable to you (or you have super-duper 128 bit TCAMs) then you can of course ignore everything above. How scalable do you want your network to be? 

I am sure this is a common question but would like to know the best practices. I have a pair of Juniper routers connecting to a IRFed HP 10k core switch. My HP core runs stp and we are tyring to extend the L2 of HP core to router as well, for that we would need to enable the rstp on the Juniper mx routers. Does RSTP and STP on core works well or do we need to move to rstp on the core as well? 

We are planning to connect a pair of QFX 5200 (QFX is a WAN device where ISP connections terminates) and a pair Fortinet 3800. Fortinet further connects to my LAN core switch. For redundancy purpose we are planning to connect QFX and Fortinet in criss-cross so the each QFX will connect to each Fortinet. Now, Fortinet that runs in HA mode (active-active), should be able to talk to the same gateway IP in QFX. Each QFX has L3 separation no L2 link between them. So I cannot run VRRP on them. In this case, how do I provide redundant out going gateway in QFX. In each QFX I can have an aggregated interface and assign a IP to it. But I cannot have two WAN outgoing gateway IPs in the Fortinet without vrrp. In Fortinet HA mode, it is just one single config for both units. It internally syncs the config to other firewall. Any thoughts? 

We are planning to put a Juniper QFX 5200 series switch which has 40G QSFP+ ports in front of the firewall which has QSFP+ 40G ports. They should work well with the 3800 Firewall's QSFP+ 40G port right? Actually my question is general. Irrespective device types and vendor, 40G QSFP+ will work across the devices right? So this will work without breakout cables and give 40G bandwidth on each link? 

That's called recursive next-hop lookup (or something along those lines) by most vendors. Most equipment will handle that just fine, but its not a particularly good practice to get into for static routing. FWIW, iBGP typically works this way. The "next-hop" carried in an iBGP session is the next-hop on the far side of the autonomous system and some sort of IGP (typically OSPF, less commonly, IS-IS, RIP, or even static routing) is used to recursively resolve that iBGP next-hop to an immediate next-hop. If you do too much of this, the gear is having to do more and more layers of recursion to find the real next-hop. I've never really tested any gear to see how many times it would recurse to find a real next-hop, but I wouldn't be surprised if the limit were 3 or 4. 

First off, recognize that TCP/IP Illustrated...while a great book (I have a copy as well), is quite dated at this point. In modern networking, talking about IPv4 address classes as anything other than an historical footnote is going to get you into trouble really quickly. So, think of the example this way: A site is allocated 128.32.0.0/16, and the site administrator assigns network numbers* within the site based on a 24-bit mask, or 255.255.255.0 in dotted-quad notation. Note that the internal networks need not all use the same network mask length, but we'll stick with them for this example. A 24-bit netmask includes the first 3 octets in dotted-quad notation in the network portion of the IP address, so any set of IP addresses that have the first 3 octets in common are part of the same network. This means that 128.32.0.x is a network, 128.32.1.x is a network, 128.32.2.x is a network, and so on, because all IP addresses in each network with different values of "x" still have the first 3 octets in common which is what the 24-bit netmask signifies. Note that, in the example, 128.32.0.0/16 is never actually assigned as a /16 network to any network segment, only various networks with 24-bit network masks are.