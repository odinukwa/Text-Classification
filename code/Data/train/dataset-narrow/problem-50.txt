This is integrated with the Physically based Refraction and it gives interesting results but I still don't know how to get rid of the high parallax scale issue which appear at grazing angles... Any idea how to get rig of that ? 

Huge thanks to @MJP who answered this. The aim is to avoid the simplification made when using tangent space normals. Here is the paper : Blending in detail But only implement equation (4) which gives you this. 

This gives the standard tiling/stretching and offset behaviour when you tile/stretche a clamped texture as you can see in the image below. First is normal, second is offset and last is stretching. 

As I am working with Unity, I found an interesting way of calculating the parallax offset inside the engine. It's not physically based but it gives better results than the classic parallax refraction. 

In my vertex shader I am using a function to offset vertices. But as I change the frequency of my function I notice some "flickering" or "jumps". I guess that this comes from the fact that the phase is not synchronised any more. 

I was messing with glowing lighting when I noticed an odd artifact in my first try. For some reason I can see three defined edges inside of the glowing pattern. I downloaded the image and marked where I see the edges. 

At least on iOS every frame you are likely to get a completely new texture that you need to draw to due to the OS switching between a couple textures each frame. My question is essentially if the OS feels the need to use multiple textures should we also be using multiple textures for some reason when doing full screen post processing? Is there ever a time where switching between off screen textures can improve performance? 

Right now I am just taking the oscillating float time and passing it in directly, but eventually I will put it on a function so it fades in, temporarily is extra bright, then goes to the source texture. Edit: I solved my problem, to my surprise the GLSL log function is in base e rather then 10. 

I am making a 2d game in opengl es 2.0 Inside are tons of rectangles defined by 4 points and one 4 component color. I am using vertex buffer objects, and I have heard that it is efficent to interlace the data. So like traditionally you would do 

I am trying to scale and repeat a Cubemap with Latitude-Longitude mapping layout just as you would do with classical UV mapping but without any interesting result. This should be used as a skybox. This comes from the fact that the coordinates are in 3D space and we can't apply this simple formula How would you handle such features : scaling which involves tiling and offsetting. 

I am trying to achieve a special texture stretching effect in my shader. Here is how I define my uv before sampling my texture, nothing really special. 

I think that I got a solution but I would gladly know if there are some optimizations possible. My UVs and local coordinates values are corresponding. I mean that they are in the same range value. That said, I can use my XY vertices values for sampling the occlusion texture. The main problem is that the local coordinates are dependent to the rotation of my object which doesn't solve anything... To solve that I am converting my local coordinates into world space using a objectToWorld matrix (inverse of current world matrix). Then I am converting it back into object space using the inverse TRS matrix but without taking into account the rotation of the object. It's a bit hacky and I think that I could avoid some steps but this is working. Any advice is welcome :) 

When you have to code a software to support multiple graphics libraries how do you generally do it? Do you loose any efficiency with the technique you use? 

I am running this fragment shader on every pixel on screen. I am trying to make it as efficient as possible. BTW I am running open gl es 2.0. The code segment below is only a sample from the code, there are about 56 different calls to Gaussian() spread across different functions. I am wondering wether it would be valuable to replace the calls to Gaussian() with there appropriate resulting float value. I know a lot of times stuff like this is pre-calculated on compilation of the shader, or calculated only once because the gpu realizes it is the same calculation for all fragments. So would it be worthwhile for me to manually calculate each of these and replace them with their values? 

I have a game with similar unprocessed graphics to his. I have been trying to get a glow effect like this working but it has been near impossible. I am currently using separable convolution gaussian blur methods in addition to downscaling my blur mask. Still, it barely runs in real-time. It is nowhere near as big as a radius as this app pulls off and it certainly isn't as good of quality. Is there some method I am not thinking of? I am open to the possibility (but doubtful) that they are using really large textures with a falloff distance or that lighting is computed from the distance from a fragment to light sources. If you play with it and notice when the FPS drops it it just doesn't seem to have the right FPS to particle amount and HDR on/off quality to it to be that. 

The full code for something that writes into a buffer to be used as a compressed texture looks like this: 

You cannot avoid a memcpy: You cannot write directly into texture storage allocated for a compressed texture using only OpenGL API calls. This means that you cannot avoid the call to with a bound PBO. That being said, you may be able to use a 16-bit RGBA texture and a GLSL image type in your compute shader. You do need to synchronize memory: In order to make sure that your compute shader finishes writing to your storage buffer, you must make sure that all reads and writes to it finish. This is done by calling with . 

I've heard a lot of people working on VR talk about scanline racing and that it's supposed to help improve latency for motion-to-photon. However, it isn't clear to me how this can be done with OpenGL. Could someone explain how scanline racing works, and how it can be implemented on modern GPUs. 

I have a couple of compute shaders that need to be executed in a certain order and whose outputs depend on previous inputs. Ideally, I'll never need to copy a buffer client-side and do all of my work on the GPU. Consider I have two compute shaders compiled and linked as and . Suppose I also have a that contains the data that is written to by and read by . Can I simply do the following: 

But I want to avoid the deformation behaviour when stretching, I want to keep margins when stretching my texture or simply cut it in the middle and stretching it. Here is an illustration below. 

I would like to blur the content of those 4 spheres using the same offset no mater their position. If I apply the same blur on all objects, far object's content will appear more blurry than ones on the foreground and I want to avoid that. I think that the depth make could help but any precision would help me. If I blur the whole image and apply the result on the sphere, the white background will bleed onto the sphere shape and I want to avoid that. I also don't want that the blue (3) and yellow (4) sphere merges with the red (1) and green (2) ones. But I would like that the green and red ones merges. Again This could be done using the depth but if you have more precision about how to do it it would be interesting.