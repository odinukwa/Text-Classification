If you want to have a computer be assigned to more than one user, you're going to need a junction table between the two to store the relationships. This was not stated in the requirements but it seems like a logical approach given the project scope. 1. Account 2. Computers 3. Account_Computer_Assignments. The third one is your junction table that stores the relationship between the other two. It's primary key would consist of IDAccounts and IDComputers using your naming convention. A foreign key relating back to their parent tables would also be advisable. This structure will also allow for multiple users to link to the same computer. I don't know if that is the intended behavior but it will allow for that with this design. If that is not intended, and a single computer can only be monitored by one and only one user, then a single field on the Computer table for the AccountID is all that is needed. No junction table. 

You're question is very broad. But in general the answer is yes. A single instance has to share resources across all databases unless configured specifically not to. By default, SQL uses all available CPU's for all operations. You can configure MAXDOP (max degree of parallelism) on the database so a single query will only have access to that many CPU's. IO is another tricky issue. Generally it's best to seperate your workloads on physical hardware whenever possible. Logs, data and tempdb all on seperate physical storage. In addition, if you are trying to create a seperation of resources for the databases, you can use filegroups to assign the objects or the entire database to a separate storage device. There are many, many other answers but again. The question of 'how can I avoid that?'(in terms of cross database resource allocation) is way too broad for a specific answer. If you are looking at one aspect, say IO, or CPU usage, it's an easier, more specific answer. 

How can I work around this? Do I need to explicitly set the value of in conjunction with ? New inserts into will be rather sporadic and minimal, but will be a constraining key across other tables. This is why I decided to investigate this implementation method so to dynamically alter the partition function. 

I am attempting to update a query that utilizes the operator within a clause predicate with to compare potential performance improvements and better understand what is happening behind the scenes when the two are interchanged. It is my understanding that in practice, the query optimizer treats and the same way whenever it can. I'm noticing that when the query is ran with the operator, it returns the desired result set. However, when I replace it with the equivalent, it pulls in all values from the primary table I want to filter. It is ignoring the provided input values passed to and returning all possible distinct values. The use case is relatively straightforward: the query accepts a pipe delimited string that can contain up to 4 values, e.g. or . From here I the input into a table variable to determine the corresponding internal for the . The result set returned is then filtered to exclude the that were not passed. To illustrate, here is a similar table definition: 

If you want to see all values from Table2 and see all records from table1 where there is a matching email, do this; 

If going down the view approach which would limit disk IO, be more normalized and MAY perform better(depending on the use of the value), it would look something like this; 

Once you have the anchor statement, you can UNION below it for all iterations down the chain. In fact, I'm not going to bother rewriting everything Pinal Dave said since it's so right on point in this case. 

I think given the requirements and that at some point you may want additional information that wasn't listed here, I would go with a fully normalized approach. 3 Tables: Guests, Episodes, Episode_Guests Then depending on if you want to do this for more than one show, another table for Shows(or series). As Paparazzi mentioned, the Guests table should contain sex. The Episode table should contain a date. Also, if you are going to do this for multiple shows, the Episode table should also have a foreign key back to the Shows table. The Episode_Guests table should record every instance of a Guest appearing on an Episode so all it would need is a foreign key relationship back to guests and another for Episode. 

We are running two servers, a PRIMARY and REPLICA within an AlwaysOn Availability Group. The VMs are housed on Azure and have 8 logical processors, 28GB of memory, and all SSD drives. The data file layout looks like: 

*Option 3 If I do not place the unique indexes on the partition scheme, the DDL operation succeeds, but as one would expect it places the indexes on the specified file group directly. In my mind, this defeats the entire purpose of the partition scheme as we cannot leverage the performance benefits of the partitioning column. But I may be wrong on this and would like to be corrected if so. 

I have looked into INSTEAD OF INSERT but have not had any success. The trigger fires once and updates the with a value of 0 (implicitly converted from NULL). I believe this is due to the not being properly captured in the scope of the transaction. 

Based on my understanding of what I have delved into, the composite key is only a good idea if we will always be looking up the data on all three columns always. Since we are wanting to isolate the data, I cannot foresee instances where we wouldn't want to look-up the as well as the before seeking to the . Perhaps, however, I am misunderstanding the pros and cons and am better served utilizing only for the primary key, coupled with an index against and . Is my thinking flawed? I'm still in the infant stages of the schema's development, so I'll be running plenty of performance tests with large quantities of dummy data once I get the initial sketches completed. But as a general practice, what is recommended in this scenario? Furthermore and generally encapsulating multi-tenant database architecture that must ensure high levels of data isolation, has there been any significant movement forward that doesn't lean itself towards utilizing a two-valued key combination? I have read and watched a good deal on the topic, primarily referencing Salesforce's Mulitenant Magic Webinar and Google's F1 white paper. More recent articles still tend to follow the concepts they've outlined even in their age, and while I am building a schema for a database that will not be anywhere close the scale of Salesforce and AdWords, I find myself leaning towards the principles that they have resonated. 

As long as it is run as a single transaction, the value will be re-used on each line, not recalculated. I'm not 100% sure about Postgres or SQL Lite but I'm fairly confident (98%) they will work the same. In the past when I was more confused about this question, I would pre-fetch the date value to a variable and then reuse the variable in the where clause. Now I realize that was pointless unless I have more than one place in a longer set of statements where I would like to use the value. As a test, move the calculated date value to the select clause and execute against a large table that takes more than a few seconds to return. You will notice that the first line will have the same date/time value as the last record. 

The 'UPDATED' column from this query can be used to do a quick find and replace on whatever database or table name might be found in the view definition. You will want to make sure you do the same for every possible version of the text you are searching. For instance a database called 'db' might be written as [db] or db. Once you have what you want from the query just copy and paste to a query window and execute the whole thing. Please make sure to backup the database prior to running it. Also carefully proof read the output text so you can verify that the replace worked correctly and did not accidentally change more than you intended. BTW, the answer WEI_DBA gave is perfectly acceptable also. It's just a different approach to the same thing. In his example, the GUI will aide in getting all of the object definitions to the query window. Then you would just use Find/Replace(ctrl+h) to replace the database name. To each his own in this case. I don't know that there's an advantage in either approach except maybe that my approach will allow you to limit the query only to certain objects (views in this case) with additional filters added if needed in the where clause. Generate scripts will allow you to filter to only views, stored procs, etc, and then individually select objects but additional filters will be limited to whatever the UI can do. 

We have been experiencing an odd behavior in our application where various modules will begin to timeout in SQL Server 2012. Each time we stumble across this issue, we find that the statistics require update and that running fixes the issue. After updating index statistics, the timeouts go away. However, the frightening issue is the frequency in which we have been experiencing the need to update the statistics, and the fact that nearly all of the statistics are showing a need to be updated across all of our tables related to order processing. Due to the frequency, we have setup a job to run prior to business hours at 7:30 AM. This seemed to have calmed the issue while we continued to investigate until it occurred again today. Not 10 minutes after business opening, they were receiving timeouts. I immediately ran and the timeouts disappeared and application function returned to normal. The order volume since business opening was low (less than 20 rows added to the primary order table), yet the statistics became so bad between 7:30 AM and 8:10 AM that we began experiencing time outs. A couple of notes: 

This answer will require some dynamic SQL and potentially could make use of a while/do loop. Basically the idea will be to figure our how to define your list of tables and column names. If you really are just looking for a list of tables where a column name exists, you will probably want to make use of the sys.syscolumns table to query the column names. 

SSIS is going to be your best bet in this case I believe. If you already license SQL Server, it should be included as well. An SSIS package has the ability to access and utilize multiple ODBC or OLEDB sources and destinations including MySQL. It can be run using a SQL Agent job or just using DTEXEC from a Windows scheduled job. Also you can easily build logging, error checking and validation. 

Ok. A literal, not a lateral value... Gotcha. A literal value in really any programming language is basically just a value that is defined within the code and does not change programatically. In SQL I might use that by simply defining a varchar variable with a set value. For example, at the start of a command, 

I have a table that captures the host platform that a user is running on. The table's definition is straightforward: 

I have an issue where within my SQL Server 2016 Standard instance, is set to but within the file, it is capturing . Once I observed the behavior, I thought that the reason for this was that I had an trigger enabled. This trigger was created to prevent usage of a SQL Server credential when connecting with SQL Server Management Studio that originates from a that has not been delegated access. This was added to allow access from specific terminals outside of the domain (but within the same network) that the SQL Server is hosted on. This is the trigger: 

Login to the vSphere Web Client for your VMware cluster, and browse to the Virtual Machine that hosts SQL Server. Your VM must be offline in order to adjust CPU and memory configurations. Within the primary pane, go to , click the button in the top right-hand corner. You will open up a context menu that has . For reference, the below image is the incorrect configuration. Note that I have set to . Given the limitations of SQL Server Standard Edition, this is a bad configuration. 

First off, the sort operation on a simple query like this will probably always be the most costly. That's because it's an operation that has to run after it has all of the data returned. That being said, there are no clustered indexes on the tables. That means they are heaps and they are storing the data totally unsorted on the disk. Without adding a clustered index to those tables where your sort is applied, it's unlikely that any changes to your query will improve performance. The only other thing that might help(might being a very big might), is move the 'WHERE type = 4' part into the corresponding join statement that it belongs to. So instead of WHERE type = 4 it would be added to the JOIN ON for ArchiveConfig (I think that's where type comes from at least) as AND type = 4. The other ON clause for that join seems redundant. Keep in mind, I doubt very much that this change will actually improve performance but it will make your code a bit cleaner. As the others have said, I don't know if it's a good idea to use the (NOLOCK) hint. I doubt it will improve performance in this case and might result in bad data being returned. 

The fix is as simple as adjusting the value. In our case, we set it to so that we have . This allows SQL Server to utilize all 12 processors. 

The outputs two rows as desired. The execution plan (PasteThePlan) shows it performing a to filter the appropriate rows. If I change the clause to utilize , I am greeted with all four possible outcomes even though I have set to return only two. 

Maintenance plans have not changed. Each night I run an integrity check, a index rebuild/reorganization (based on fragmentation level), a statistics update, a full backup, and log backups every 15 minutes before and after working hours from 6:00 AM to 7:00 PM. As far as volume goes, Tenant A sees roughly ~10,000 orders each day. With Tenant B added, we now see a combined ~25,000 orders each day. The volume has increased dramatically. The schema does not differ between the two tenants, business logic is the same save for tenant parameterization, etc.. With all this, I am a bit perplexed how we are seeing an increase in database performance with much more volume. I am going to continue to monitor and gather statistics throughout the week, but so far, both tenants are incredibly happy with the performance of the system. The tenant that was already on the database has brought up how the application is behaving much more quickly than before. Has anyone noticed such an odd case before, or perhaps know of anything to pay mind to?