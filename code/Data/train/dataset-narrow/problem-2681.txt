I should also mention that the SDL_Event 'localEvent' is part of the GameState parent class, while this event handling code is part of a SplashScreenState subclass. If anyone knows why this is happening, or if there is any way to improve my code, It'd be helpful to me! :D I'm still a very new programmer, trying to learn. UPDATE: I added a std::cout line to that the code runs multiple times with a single KEYDOWN event. I also tried disabling SDL_EnableKeyRepeat, but it didn't fix the issue. 

However, if sampling is acceptable, the question becomes how accurate of a simulation are you going for? From what I remember, Valve's Source engine has a relatively similar method of combining sound samples for the same/similar effect (when a plastic prop collides with a metal one, they generate a 'hybrid' sound). If that's acceptable, then you will need to: 

I'm a little bit stuck on how I might go about programming dynamic, interesting water in a game. To qualify that a little bit, I'm interested in creating water with broad, rolling waves that affect collision, physics, and the visual shape of the mesh. As an example, here's a screenshot from Wave Race 64, a 1996 jetski racing game for the Nintendo 64! 

I'm working on a 2D game project right now (using SFML+OpenGL and C++) and I'm trying to figure out how to go about choosing a resolution. I want my game to have a pixel resolution that is around that of classic '16bit' era consoles like the Super Nintendo or Neo Geo. However, I'd also like to have my game fit the 16:9 aspect ratio that most modern PC monitors use. Finally I'd like to be able to include an option for running full screen. I know that I could create my own low-res 16:9 resolution that is more-or-less around the size of SNES or NeoGeo games. However, the problem seems to be that doing so would leave me with a non-standard resolution that my monitor would not be able to support in fullscreen mode. For example, if i divide the common 16:9 resolution 1920x1080 by 4, I would get a 16:9 resolution that is relatively close to the resolution used by 16bit era games; 480x270. That would be fine in a windowed mode, but I don't think that it would be supported in fullscreen mode. How can I choose a resolution that suits my needs? Can I use something like 480x270? If so, how would I go about getting fullscreen mode to work with such a non-standard resolution? (I'm guessing OpenGL/SFML might have a way of up-scaling...but..) 

If you can't use harmonic analysis of real world samples for some reason, the scope of this problem will become a lot bigger - as sound synthesis usually relies on combination/modulation of fundamental waveforms, you'll need to use physical modelling synthesis instead. 

I'm trying to make a smooth day/night cycle for a game project that I've been working on. I've already set up a clock that adds deltaTime to seconds, wraps seconds to gameMinutes, etc. The issue that I'm having is that I want a signed normalized float to represent day (+1) and night (-1), and I'm not exactly sure what the most effective way of doing that is. Here's my time class in pseudocode: 

When you say 'synthesis' do you mean pure analog/additive/FM synthesis from scratch, or would a sample-based approach be acceptable? If you can't use combinations of real-world audio samples then this is more complicated process. Trying to generate truly realistic sounds through synthesis isn't the standard way that most game/virtual instruments/sound designers work. If you do need something that's based on real waveform synthesis, like what you linked to, you probably need to perform a harmonic analysis of a large variety of sounds. Performing a harmonic analysis of various sounds will allow you to break real world sound samples down into fundamental sine waves. Keeping a data structure that associates each sound with its harmonics will probably allow you to interpolate new sounds semi-realistically. Using all this harmonic data, you can then construct the sounds again through additive synthesis. To do this from scratch you will probably need: 

So I've come across an issue in the game I'm working on, but it seems to be a pretty fundamental thing that probably comes up in a lot of games. My game requires that some gameplay function occurs at a very specific point in time during a character animation. And so, as the title says, I'm wondering what are some good techniques and fundamental strategies for syncing gameplay-related events/functions/actions to specific points in a character's animation. Here are some simple examples of what I'm talking about in various types of games: 

In Wave Race, not only do the waves move visually, but the choppy water pushes your vehicle around and creates bumps that give the game a really fun and unique feeling. I imagine that this type of thing is pretty standard in water simulation and physics, but I think the 'arcadiness' of the water physics in Wave Race makes it a pretty decent example. I'm not really sure what steps need to be taken to create this effect though. I imagine that the visual effect of waves would be pretty simple to do create using a vertex shader, but I'm uncertain on how to program the physical 'entity' of the water. Here are some of my exact sticking points: 

Using an orthographic projection, characters are accurately aligned with their collision data regardless of position. However, the desired visual effects of perspective are lost. Their Solution: The developers at Arc System Works solved this issues by creating a 'partial perspective' projection - a non-uniform mixture of perspective and orthographic (maybe parallel is more accurate) projections. On the horizontal axis, the camera's projection is said to be a '70% orthographic and 30% perspective' hybrid. On the vertical axis, the projection remains 100% perspective. Example 3: Models using 'hybrid' projection. 

In short. Figure out specifically what you need in terms of audio-minutes, revisions, fileformats (midi? audio?), time, etc. When you've figured out what exactly you need, and when you need it, contact various musicians that you like, and find out if they are willing/able to provide commissioned service under these terms. Good luck! :D 

I'm going to start with some background, so jump to my question at the bottom for the TLDR version. Problem: In a Japanese 4gamer article (english translation via Chev on Polycount) explaining some of the technology and design behind Guilty Gear Xrd (a highly-stylized, 2.5D, toon-shaded fighting game) they detail some of the challenges concerning the game's development. One of the challenges had to do with the use of an orthographic 2D collision detection system on top of 3D character models that were rendered in perspective. In short, rendering character models through a perspective projection would naturally result in the characters looking wider towards the edges of the camera. As such, at various points on the screen the visual representation of the characters would fail to accurately represent their 'hitboxes' and 'hurtboxes'. Example 1: Models using perspective projection. 

Admittedly, my particular case is most like the second example, in which I have created an animation where my turn-based character lunges forward during an attack and I want the damage to be applied at the exact moment that the animation seems to be making contact. Because my game using a turn-based system (imagine something like Final Fantasy or Fire Emblem) I want the damage/healing/magic/etc. to be applied at the correct time during each character animation even though I'm not actually using collision/hitboxes. I should mention that I'm making my game in a popular game engine, and that right now I'm handling this by using their animation events or notifies to achieve something close to the desired results - my character performs a certain command and triggers a command-specific animation (i.e.: 'attack_command') and the animation assets for each one of my commands must include an animation event/notify 'callback' into my characters ExecuteCommand function. In other words - the character tells the attack animation to play, and then the attack animation emits an event/notify callback into the character at the exact moment during the animation when the damage should be dealt. Honestly, this works for now, but it just feels wrong - like I'm missing some part of the bigger picture here! Part of the reason that this method feels wrong is that it couples the game logic with the animation assets; if my animation asset forgets to include an ExecuteCommand() event/callback, the command wont execute properly and extra code is needed to check if a command animation finished without executing the command. It's messy and it means that my gameplay has a strange dependency on its assets. Of course I want the damage to occur at a specific point during my attack animation, but I feel really weird about calling gameplay code inside animation assets.. So what am I overlooking here? What are some good general techniques for handling these types of situations in which you want certain important gameplay actions to occur at specific times during animations? Edit: To clarify, this isn't an engine-specific question nor am I looking for engine-specific designs/techniques. I'm interested in general animation/gameplay synchronization techniques that one might use in your game projects regardless of technologies used. 

The character model for Player 2 (orange) more closely matches his collision data because of his location towards the center of the screen. Example 2: Models using orthographic projection. 

I have some "starting point" ideas, like possibly using a 2D or 3D vector field for the water. But I'm still fuzzy on the general implementation of fluid simulations, especially as a gameplay element! 

Texture budget will depend on what hardware you're targeting, as each device will have a different maximum texture size. Look at the range of devices that you plan on supporting (mobile devices, consoles, PC hardware, etc.) and try to find out the lowest common denominator. If you have two characters, you probably don't want to have their sprites on the same atlas due to the fact that only one of them might be needed at a time. 

This likely depends on the size of your target platforms RAM/VRAM, and how much you'll need to have loaded at one time. It's probably something that you have to fine-tune. Another thing to consider is non-volatile storage (media/disk space, etc.). 

You should probably find a musician online who's style that you like, and then contact them about commissioning a certain amount of minutes/songs. With freelance artists and musicians, unless they work with you in a studio setting, it's very difficult to work out hourly/weekly payment methods. Going down that road may lead to potential issues/disputes later. It's a much better idea to pay them for 'x' minutes of finished music, or 'x' number of completed tracks. Another important thing to consider is working out some kind of deal for an amount of revisions. This is important so that you can arrange to listen to the finished/nearly-finished tracks and have them adjusted or revised to your personal taste. Please also make sure that both you and any musician/artist that you work together with have a very firm and clear understanding of the ownership plans. Are they making the music for you to use as you please, with a one-time fee? Just make sure that everyone is on the same page.. -- Also, when it comes down to the exact price, there really isn't a standard. It varies from person to person. I'd suggest this: __ date. (You can try to suggest a price/minute of rendered audio, OR you can ask them how much they charge per minute of rendered audio. 

Right now I'm working on a simple SDL project, but I've hit an issue when trying to get a single keypress event to skip past a splash screen. Right now, there are 4 start-up splash screens that I would like to be able to skip with a single keypress (of any key). My issue is that, as of now, if I hold down a key, it skips through each splash screen to the very last one immediately. The splash screens are stored as an array of SDL surfaces which are all loaded at the initialization of the state. I have an variable called currentSplashImage that controls which element of the array is being rendered on the screen. I've set it up so that whenever there's a SDL_KEYDOWN event, it triggers a single incrementation of the currentSplashImage variable. So, I'm really not sure why my code isn't working correctly. For some reason, when I hold down a button, it seems to be treating the held button as a new key press event every time it ticks through the code. Does anyone know how I can go about fixing this issue? [Here's a snippet of code that I've been using...] 

Mixing perspective and orthographic horizontally allowed them to preserve collision alignment and visual spacing (an important aspect in competitive fighting games). At the same time, the 'fully perspective' projection on the vertical axis allowed them to achieve extra visual depth when the characters jump. My Question: How would one go about achieving this kind of non-uniform, semi-perspective, semi-parallel camera projection? When I think about this, I can imagine a camera frustum with a wide vertical FoV and a somewhat more narrow (but not necessarily parallel) horizontal FoV. Is this the correct interpretation of what they're describing, or am I visualizing it incorrectly? Typically how would you generate a projection/modelview matrix like this? Is there a way to ask libraries like OpenGL and Direct3D for non-uniform matrices like these, or is there some way to combine an orthographic matrix and a perspective matrix in a non-uniform way through matrix math? 

I've never attempted this so I've only given it a little bit of thought, but from what I understand this is how most commercial virtual instruments create realistic sound performances. When 'VST' makers try to recreate the sound of a violin, they usually do so through extensive sampling of a real violin - sampling various notes on each string played at different velocities, using different playing techniques, and so on. Creating realistic sounds through pure waveform synthesis is interesting, but it's certainly not the standard practice in game engines at the moment. Good luck. Edit: I didn't touch upon the physics engine part of your question, but I'd assume that most physics engines should be able to detect collisions for you and present you with information about those collisions (force/object mass/direction vectors/etc.). 

It's a trade off. Switching UV indices within a single texture is a lot faster than binding a different texture. However, having textures that are too big or being forced to load more than you need at any given time is also not ideal and can bog down performance. It could be that the time to bind a different texture is relatively minimal, in which case multiple texture atlases might be the way to go. But if your character switches animations very frequently, and each animation is only a few frames, then you might find it works better to pack as many frames into your atlas as you can until you reach your decided max texture size. Good luck. 

So, my GameTime has been tested and seems to be working fine; when the value in seconds is >= the value of SEC_PER_MIN it adds another minute and stores the leftover fraction of a second. It then goes on to check minutes, hours, days, etc. I've been a little bit stuck at UpdateDayNightCycle() though. I want to be able to convert my time into a smooth decimal value where -1 represents absolute night and 1 represents absolute day. I've normalized values between 0~1 and -1~1 before, but I've never tried to use a normalized value to represent a repeating/periodic value like night~day before.. I tried drawing a simple graph where time of day is on the x-axis and daynight value is on the y-axis. What I get is essentially a graph of a triangle wave. Sadly my math skills are a little bit underdeveloped and even after looking into formulas for triangle waves, I've been having trouble implementing it in my code.. So, am I even on the right track? How can I smoothly convert my game world's time into a single decimal value between -1 and 1? 

Performance will likely not be an issue for you if you only plan on making a simple 2D RPG. If you do end up having performance issues, it's MUCH more likely that you've made a poor decision in your algorithm. The biggest effects on performance isn't the slight differences between languages in small-scale programs. Often, it's not the language, or the library, or even the hardware! Many times, if you want to optimize your program, you should first check to make sure that your algorithms aren't wasteful/illogical. Basically, I think that you would be able to make the type of game that you want to make in any of those language/library setups. It really depends what you feel comfortable with! The mantra that I've always been told is, 'don't worry about optimization until you have WORKING code'. Pick a language, pick a library, start learning and plugging away, get your game working on a basic level and see how it runs. If it isn't running as fast as you'd like, then check you code and see if you can rewrite your algorithm in a smarter, more efficient way! From the options that you suggested, I personally think that Java is the most 'middle of the road' and it kind of lies between C++ and Python. But I think you can do what you want to do in any of those languages. :D Good luck!