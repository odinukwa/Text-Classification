I see there is a column for 'Is DST', so I am assuming the server is tracking that. So can I safely assume sql server will adjust the time as needed during DST? 

This returns the expected value, but I have not been able to determine if this will still work during daylight savings time. By querying the time zone info DMV: 

Using SQL Server 2008R2. I have a single table with a bunch of columns. I have built a new database with multiple tables to copy the data too. How do I copy the data, and still keep the relationships? For example, I have a table with columns such as: StudentFirstName, StudentLastName, TeacherName, Class1, class2, class3, class4, AdvisorName, etc. I want to copy that to multiple tables, such as: Student - StudentId, FirstName, LastName Teacher - TeacherId, FirstName, LastName StudentTeacher - Id, StudentId, TeacherId etc. When I insert the data into the Student and Teacher tables, I need to grab the ID fields from them (StudentId, TeacherId) and load those into the the StudentTeacher table, in the correct order. Is this best done with SSIS? If so, will I need a bunch of data flows because of the "one source, one destination" thing? If I do it with a script, I'm thinking the OUTPUT clause might help, but I have not yet figured out how to make that work. I have tried this: 

I have to convert UTC dates for some reports. We are using SQL Server 2016 and all my report queries are SPs, so I was able to do this: 

Turns out I had to run as VS as an administrator. I guess because of the remote nature of the server. 

But that doesn't load at the same time, so I get multiple rows, one with a studentid, one with a teacherid. How do I get them to load at the same time? Just looking for some direction 

I am moving an Access app to SQL Server. I have confirmed the data in the existing PK is unique so I want to copy it into the new PK Identity field. The field is currently set as IDENTITY(1,1). I know I can set the IDENTITY_INSERT for the insert period, but do I then need to adjust the IDENTITY start point so the next insert does not start at 1? Example, the newest row in the existing data is id 4132, so do I insert the data then alter the column to IDENTITY (4133,1) ? 

I added a report project to my existing solution in VS2015, built some reports and now I am trying to deploy the project to a report server. I have gotten the URL from the config manager ( $URL$ ) and put that in the report project properties as the target server URL. When I click Deploy I get an error saying "The specified report server $URL$ could ot be found. I have no idea where it is getting this localhost reportserver URL from, that is not what is saved in the properties. The Report project is inside a solution with an MVC project. Do I need to change something at the Solution level? 

will repeat the headings every 100 rows rather than every 15. You can set the value to (just about) whatever you'd like. If you want to ensure that the headers are only written once, you can set it to a number much greater than the number of rows that the query will return. 

The data you're getting from Toad appears to be incorrect or, at least, misleading. If you are using a locally managed tablespace with automatic extent allocation, Oracle will determine your initial and next extent sizes automatically. In 11.2, the first 16 extents are going to be 64k in size (for a total of 1 MB). The next 63 extents are going to be 1 MB in size. So if you have 26 extents, that implies that the table occupies 11 MB of space on disk. An initial extent of 1.44 GB makes no sense and a size of 4.18 MB seems rather low if you're saying it hasn't shrunk. What does 

Is it possible? Sure. Is there likely to be an improvement in performance? No. If there is a change in performance (barring cases where you discover that a join is missing or otherwise fix a query), it'm more likely that the old implicit join syntax will be more efficient. But that's pretty unlikely. Behind the scenes, when you have a query using the SQL 99 \'left outer join` syntax, Oracle actually transforms that to use the old style syntax before optimizing it. When Oracle first introduced the SQL 99 syntax, there were occasionally bugs in this transformation process. By the time you get to 10.2.0.4, the odds that you'd encounter one of those bugs is pretty low. But if you look at enough queries, you might find one that is more efficient using the old syntax. 

This is realistically going to depend on a large number of factors that aren't mentioned here and would probably require more analysis than is realistic in this sort of a forum. First off, "good hardware" means very different things to different people. You'd need to analyze how much data needs to be read, how much needs to be written, and what sort of bandwidth your disk subsystem provides. A RAID-5 array of a few disks in a mid-level server will obviously have very different characteristics than a machine that has hundreds of GB of solid-state disk backed by a high-end SAN with tons of cache where you're spreading the I/O across hundreds of spindles. A "read request" that fetches a single row will obviously produce very different load than would a "read request" that asks for all 1 billion rows. Second, the design of your application will be terribly important. Do all of your reads need to lock the row, for example? Or is this a data warehouse where you can use a on all your queries to potentially allow dirty reads because you won't be running loads at the same time you're running queries? Will your read requests be for different rows or will large numbers of users be hitting the same set of "hot" rows over and over? In order to make any sort of reasonable projection, you'd need someone that understood exactly what your application was going to need to do, the physical data model that you'd implement, how SQL Server 2012 operates, and how your specific hardware would behave in various situations. That's probably outside the scope of this sort of forum-- it's more likely that it is something that you'd a consultant to come in and help with (your hardware vendor may be happy to provide a pre-sales resource to help you size an appropriate server for the task).