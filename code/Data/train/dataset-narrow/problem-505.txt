are unable to use indexes because of the wild carding. You can switch to using FULLTEXT matching. The LIKE pattern is not scalable and will cause full table scans all the time. 

You may have fragmentation in the table meaning that you could reclaim space by running the optimize command on the table. Check the free_data column from . Further reading; $URL$ If you are using the innodb storage engine (and you generally should with narrow exceptions), then you could consider using innodb row compression. Caveats here are that it's not recommended on a CPU-bound workload and you need to ensure that you're working with innodb_file_per_table enabled. Further reading; $URL$ 

errno: 13 should be enough of a clue here. 13 is generally permission denied. Check the .so file in your plugindir. It will likely have privs that will prevent the user running MySQL from opening it. 

In that case, go to McAfee audit plugin as it's compatible with MySQL from version 5.1 to latest GA. This plugin outputs in json nicely for you and you can filter nicely on the events you do and do not want. 

Only filtering, aggregating or ordering involving the left most column will be valid for this index. i.e. 

When seeking a dump which provides binlog position you will instruct MySQL to call Flush Tables With Read Lock. The acquisition this lock can block. This where you could impact the system. If you have long running transactions you may see problems but you can check this before starting the dump. Your command looks fine to me. 

You could investigate the MariaDB storage engine 'CONNECT'. It's in beta but could simplify your process. It is not OSX, however. $URL$ $URL$ This storage engine permits you to read and write to 'other' data files. Take a look as it's a very interesting component. 

It's likely the disk space is the root cause. Watch the space free in the tmpdir location whilst the ALTER is running. 

MySQL can run as a service and I hope that windows is stopping the services gracefully or you are likely to have more problems outside of MySQL. If you want to be careful, stop the service by hand. This should allow MySQL to flush all buffers before stopping. I would guess from your comment that you're getting corruption that you are using the MyISAM engine; my advice would be to migrate your tables to the InnoDB engine to make best use of the durability of the engine. In the event of a crash or ungraceful shutdown, MySQL will better recover data. 

Track the activity on your server with one of the off the shelf audit plugins available. McAfee Audit would be my recommendation here sincemyou likely just want to know calls filtered around the stored procs. 

You do not need to change the sort_buffer_size from default. You misunderstand it's use based on the question. You should begin by examining the SQL to see if you are able to tune it and satisfy ORDER BY/GROUP BY conditions using an index. It will generally be a composite index. Further: $URL$ 

I suspect you're suffering 'out-of-the-box' syndrome. For the whole world to know... MySQL config out of the box sucks. Go to $URL$ and create better configuration based on the questions asked in the form. This should provide you with a better starting point. Once the output is generated, add it to your configuration file (/etc/my.cnf) and restart MySQL; review the mysql error log to ensure that your mysql instance started cleanly (beware of paths, permissions and the innodb_log_file_size size differences since that will require disposal of the old ib_log* files before new ones can be created). Once done, try your import again. 

If you want to learn more on MySQL indexing check out resources such as $URL$ Indexing in general is well detailed by Markus Winand $URL$ Your queries using 

Syntax issue in the . I've not written any procs with this notation so for me it rang alarm bells. I checked both with and without and dropping the 'OR REPLACE' allowed the proc to be created. You can 'DROP PROCEDURE' in advance of creating is for the same effect. Try with; 

Using InnoDB and the Barracuda file format permits you to deploy the Dynamic row_format. This configuration will aid you due to the way it will store the BLOBs off page. Queries not using the columns with the BLOBs shouldn't need to touch their pages; $URL$ 

Use pt-archiver tool. It can delete data that it moves. Alternatively use drop & create after the select into outfile. 

It's possible. You will potentially lose capacity to serve all data requests from in memory buffers but that depends on how large your data is. select ROUND(SUM(data_length+index_length/pow(1024,2))) as TotalSizeMB, SUM(table_rows), engine from information_schema.tables where table_schema not in ('mysql','information_schema') group by engine to see your distribution per engine. You'll be able to judge whether all data and indexes fit inside the innodb bufferpool or indexes within the myisam key buffer. Assuming you set these buffers up yourself in the my.cnf config file and you're not running with the defaults. 

My full conclusion here is that you should spend some time learning about indexes on BTREE data structures. The implementation in MySQL isn't very complicated and the sharp edges are evidently unknown. I've added some resources to my original answer and if you're serious about the performance of this and future applications you write/optimise with MySQL you should do your homework. Feeding you the fish here will not help you longer term. 

Looks like an issue I have encountered recently. With MySQL stopped, move the ib_logfile* from the data directory (perhaps to /tmp to be safe). Then start mysql and watch the log for progress. I didn't check for bugs yet but I'd wager something has been logged. Let us know how you get on. 

You could also use MySQL events (like internal cron) and select into outfile. Advantages of this method? Your backups will contain this logic. Both approaches are valid IMO just depends on your preference. 

Using the INSERT (SELECT)...ON DUPLICATE KEY UPDATE should permit you to visit these rows and update them with original values. Also you're not correct about REPLACE. This is the mechanism used by pt-table-sync to correct data drift. The documentation details a REPLACE == DELETE AND INSERT but that's not done within the scope of AUTO_INC. 

Check the version you used to backup the data is the same as the one you are trying to use to restore. This can bite. 

The best performing way to delete a lot of data is to chunk your activity. There are tools to complete this such as pt-archive and oak-chunk-update. This avoids the build up of undo and performance issues. 

The query cache is only helpful for low write, high read workloads and should be pretty handy on a blog with this access pattern. If you issue any DML then you invalidate the QC for that table and repopulation would be necessary. Have you calculated the QC efficiency based on your local status? $URL$ The table cache is a different animal. It caches tables in memory so that MySQL can avoid the overhead of opening them. There's a recipe for tuning this, which is something along the lines of but I doubt this us causing you trouble. It's likely that you would be better off tuning the key_buffer (MyISAM's cache for indexes) or the Innodb_buffer_pool_size (InnoDB cache for data + indexes) to reduce load on the server. $URL$ 

This will ensure that MySQL will use a file per table to store data and therefore easier to recover space moving forward. Drop all the data and restore logically from the backup file. This will populate each table per file and future tables are going to be created in their own file. 

The duration of the creation of the index will be directly related to the size of the index. I don't think it would be wildly different for the 2 options provided. Under the hood MySQL is going to copy the table to recreate it in the new structure, so consider the amount of time MySQL will take to read the data and write the data. The process is single threaded too. If your storage is slow this will take some time. Tips: - Ensure that fast_index_creation is enabled - Group multiple ALTERs - use pt-online-schema-change tool to perform it online. This will take longer but will permit you to continue to use the table. 

My $0.02; perform the calculation app side. It will scale better. Its possible to perform the calculation in an or an statement. 

Alternatively you can install and deploy the Maria DB audit log to enable a trace on the objects and access you require. $URL$ 

I wouldn't say this is impossible (MySQL is open source so hacking the source code could bring about this change) but I wouldn't advise it. Information_schema is an ANSI standard and isn't limited to MySQL. It contains virtual tables or views describing metadata about the server. More detail here. $URL$ Hopefully this will enlighten you to why it's there and why it's managed by the server not the user. 

Using Xtrabackup for full backups every 15 minutes is overkill IMO. You should ensure that you have binary logs enabled and then mirror them off to another server using mysqlbinlog from MySQL 5.6 (it's backwards compatible) so that you can perform point in time restores. The binary logs contain all changes that occur on the instance whilst a full xtrabackup would contain the entire dataset. This will have obvious overhead. Alternatively look at the incremental backup capabilities of Xtrabackup and if you're having any issues using it head over to Percona's forums for some assistance. 

Your data is corrupt. You might attempt starting the server iterating through the $URL$ documentation. If it starts you can attempt to save the large table (mysqldump/mydumper export) or drop it if it's not needed. 

There is no core method for gathering this information. It could be sought by utilising an audit plugin. By collecting all of the queries occurring in the server you could analyse the logs. Using the McAfee Audit plugin you can filter the events to only long on certain schemas and that might make lighter work of the analysis. 

Use the information_schema.columns table i.e. select table_name,column_name from information_schema.columns 

It's tough to optimise Aggregation (AVG), Group By and Order by. These operations even if well indexed will generally cost internal temp tables and regularly on-disk temp tables for grouping and sorting. It's widely accepted to cache the results for re-use rather then rerunning this at any level of concurrency. You may have heard of summary tables to record such values. Worth a read, google is your friend.