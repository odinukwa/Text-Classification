Assuming $\mathsf{P} \ne \mathsf{NP}$ can we show $\mathsf{P} \ne \mathsf{P/poly} \cap \mathsf{NP}$? Obviously this would be the case if $\mathsf{P} \ne \mathsf{NP}$ and $\mathsf{P/poly} \supset \mathsf{NP}$ but this is unlikely. This is also true under the assumption $\mathsf{P} \ne \mathsf{BPP}$, however AFAIK it is commonly believed $\mathsf{P} = \mathsf{BPP}$. Can we prove $\mathsf{P} \ne \mathsf{P/poly} \cap \mathsf{NP}$ under a weaker assumption? What evidence can be provided for/against? Is there a specific language which is believed to be in $\mathsf{P/poly} \cap \mathsf{NP}$ but not in $\mathsf{P}$? 

If the answer is positive, can $f$ be polynomial? What is the growth rate of $g$ (clearly at least exponential under ETH)? If the answer is negative, can polynomial $f$ exist if ETH is wrong but $P \neq NP$? 

Motivation Consider some $L \subseteq \{0,1\}^*$. Suppose Alice gives Bob a machine or oracle $M$ that purportedly decides $L$. If Bob has only polynomial time in their disposal, then they cannot directly test $M$ unless they already know a polynomial time algorithm for $L$. However, if $L \in \textsf{PSPACE}$ then Alice might be able to convince Bob via an interactive proof protocol. That is, Bob can generate random instances of the problem of size $n$, run $M$ and have Alice demonstrate the correctness of the answer. If they repeat the procedure sufficiently many times then Bob knows that, with high probability, $M$ gives the correct answer on most instances of size $n$, so it is at least "average-case" valid. Now, if Alice is also limited to polynomial-time, then in general they won't be able to supply an interactive proof. Indeed, if it is possible for both the verifier and the prover to be polynomial-time, it follows that $L \in \textsf{BPP}$. The motivation for the question is: what happens if $L$ is in $\textsf{BPP}$, but Bob doesn't know the algorithm for solving it? In this case, it is possible that Bob knows an interactive proof protocol for some class that is known to contain $L$ and Alice is able to fill in the role of prover. For example, if Alice tells Bob a polynomial-time algorithm for solving graph isomorphism then Bob can easily convince themselves of its validity (for given input size) because there is an interactive proof protocol where the prover can run in polynomial-time given oracle access to the problem. However, this example requires using a protocol specifically tailored to graph isomorphism, whereas I would like a protocol that can work for any problem in a large class (optimally all of $\textsf{PSPACE}$). I am interested in all results that are relevant to the above discussion, but to make the question concrete, I will suggest a specific hypothesis. Question Given a polynomial-time computable mapping $f: \{0,1\}^* \rightarrow \{0,1\}^*$, we can consider the language $f^{-1}(\textsf{TQBF}) \in \textsf{PSPACE}$. We say that such an $f$ is easy when there is a polynomial-time algorithm that implements an optimal strategy for the corresponding game. That is, given $x \in \{0,1\}^*$, $k \in \mathbb{N}$ and any assignment of the variables corresponding to the $k$ outermost quantifiers in the formula represented by $f(x)$, our algorithm can decide whether the resulting formula (with quantifiers over unassigned variables) is true. In particular, in this case $f^{-1}(\textsf{TQBF}) \in \textsf{P}$. 

It is not hard to see that a positive answer would imply $\exists A \in \mathsf{TALLY}:\mathsf{R} \subset \mathsf{E}^A$ ($A$ is s.t. $U$ is Cook-reducible to $A$ and we have $\mathsf{R} \subset \mathsf{E}^A$ since it is possible to apply $U$ to the sequence $\chi_L(n)$ to decide $L$ in exponential time). This implication seems surprising but I don't see why it's necessarily false EDIT: Actually such $A$ exists: $\lbrace w | \exists i,j \in \mathbb{N} : i \in L_j$ and $|w| = f(i,j) \rbrace$ Here $f : \mathbb{N}^2 \rightarrow \mathbb{N}$ is a bijection computable in polynomial time and $L_j$ is the $j$-th recursive language (I'm using an arbitrary enumeration of the recursive languages) However a universal predictor in $\mathsf{P/poly}$ implies a stronger statement, namely, that there is $A \in \mathsf{TALLY}$ and some fixed polynomial $p(n)$ such that all languages in $\mathsf{TALLY} \cap \mathsf{R}$ can be computed in time $p(n)$ given an oracle for $A$ EDIT: The answer to the first question is positive. Showing this is equivalent to constructing a Turing machine with a special tape on which some fixed (input-independent) infinite string is written in the initial state s.t. this machine computes a universal predictor in polynomial time. To do this, imagine the special tape to be 2-dimensional. In row $i$ of the tape write infinite computable sequence number $i$ (i.e. computed by program $i$). The predictor then works as follows. Given an input of length $n$ it compares it to the first $n$ infinite computable sequences. The first sequence which matches the input is used for the prediction. If no sequence yields a match the predictor outputs $0$. 

Consider any language $L$. Define $s(L) \in {\lbrace 0, 1 \rbrace}^\omega$ (an infinite sequence of bits) by the recursive formula $$s(L)_n=\chi_L(s(L)_{<n})$$ Here $\chi_L$ is the characteristic function of $L$ i.e. $\chi_L(w)=1$ for $w \in L$, $\chi_L(w)=0$ for $w \notin L$ A language $U$ is called a "universal predictor" when $$\forall L \in \mathsf{R} \, \forall n>>0:s(L)_n=\chi_U(s(L)_{<n})$$ A famous example is Solomonoff induction It is easy to see $U \notin \mathsf{R}$ by considering $L = U^c$. The question is 

Observations Reductions Let's call a Karp-reduction $f$ of $(L, X_n)$ to $(M, Y_n)$ semi-invertible when there exists $g: \lbrace 0, 1 \rbrace^* \rightarrow \lbrace 0, 1 \rbrace^* \cup \lbrace \bot \rbrace$ computable in polynomial time s.t. $$\forall x: g(f(x))=x$$ $$\forall y: g(y) \ne \bot \rightarrow f(g(y))=y$$ Given $A$ optimal for $(M, Y_n)$ and $f$ a semi-invertible reduction of $(L, X_n)$ to $(M, Y_n)$, $A \circ f$ is optimal for $(L, X_n)$. In particular, if $(M, Y_n)$ is $\mathsf{sampNP}$-complete s.t. the reductions can be chosen semi-invertible (such $(M, Y_n)$ exists since examining the construction of a complete problem in Ben-David et al we see the reductions are semi-invertible), we get optimal algorithms for all problems in $\mathsf{sampNP}$. In particular, $A$ yields an average-case polynomial time solution for any problem formulated as a problem in $\mathsf{sampNP}$ which is "secretly" in $\mathsf{sampP}$. This is similar to the properties of Levin search, but Levin search only solves instances with a positive answer. Uniqueness Consider $A$, $B$ estimators for $(L, X_n)$. Define the "metric" $$\delta_n(A, B; L) := E_{X_n}[\chi_L |\ln{A} - \ln{B}| + (1-\chi_L) |\ln{(1-A)} - \ln{(1-B)}|]$$ If $\delta_n(A, B; L)$ vanishes for large $n$ with superpolynomial speed, then $\epsilon_n(A; L) - \epsilon_n(B; L)$ also vanished for large $n$ with superpolynomial speed. Suppose $A$ and $B$ are both optimal. Then it can be seen that $\delta_n(A, B; L)$ vanishes for large $n$ with superpolynomial speed (via considering $\frac{A+B}{2}$ and using the convexity of the logarithm). So optimal estimators are unique up to "small" deviations. 

Clearly this algorithm has $O(1)$ update time. To verify that this algorithm is correct we must prove the following claim. Claim. With high probability, for every $\text{id}$, the count of $\text{id}$ in $S'$ is close to the sum of the weights of $\text{id}$ in $S$. Let $w_\text{id}$ be the "true" weight of $\text{id}$ in $S$ and $W_\text{id}$ the weight of $\text{id}$ in $S'$. Let $w=\sum_\text{id} w_\text{id}$ be the total weight of $S$ and $W=\sum_\text{id} W_\text{id}$ be the total weight of $S'$. Our claim is that $|W_\text{id}-w_\text{id}| \leq \varepsilon w$ for all $\text{id}$ with high probability. Clearly $\mathbb{E}\left[W_\text{id}\right]=w_\text{id}$. It remains to show concentration bounds. To this end, we use the following result. Bernstein's Inequality. Let $X_1, \cdots, X_n \in \{0,1\}$ be independent random variables. Then $$\mathbb{P}\left[ \left| \sum_{i=1}^n X_i - \mathbb{E}\left[X_i\right] \right| > t \right] \leq 2 \cdot \exp\left(-\Omega\left(\frac{t^2}{t+\sum_{i=1}^n \mathsf{Var}\left[X_i\right]}\right)\right)$$ for all $t > 0$. Thus $$\mathbb{P}\left[ \left| W_\text{id}-w_\text{id} \right| > \varepsilon w \right] \leq 2 \cdot \exp\left(-\Omega\left(\frac{\varepsilon^2 w^2}{\varepsilon w+w_\text{id}}\right)\right) \leq 2 \cdot \exp\left(-\Omega\left(\varepsilon^2 w\right)\right).$$ Note that if $w_\text{id}=0$, then $W_\text{id}=0$. So we need only consider the $\text{id}$s that appear in the stream. In particular, we can take a union bound over at most $N$ $\text{id}$s: If $w \geq O\left(\log(N)/\varepsilon^2\right)$, then the weights in $S'$ are close to the weights in $S$ with high probability and the claim is verified. What about when $w \leq O\left(\log(N)/\varepsilon^2\right)$? Then we can first repeat each weighted update $T$ times. This increases the weight to $Tw$. The good news is that $S'$ is only length $O(Tw)$ and transforming $S$ into $S'$ takes $O(1 + Tw/N)$ (amortized) time per update with high probability. So we just need to find a $T$ with $ O\left(\log(N)/\varepsilon^2\right) \leq Tw \leq O(N)$. 

The obvious thing to try is setting $h_\pm(x,y)=f_\pm(x) \cdot g_\pm(y)$. Unfortunately, it doesn't seem to work out (essentially the signs and inequalities don't go the right way). I'm also interested in generalisations of this question, where $X$ is, say, small-biased, rather than $k$-wise independent. 

We're interested in additive approximations to #3SAT. i.e. given a 3CNF $\phi$ on $n$ variables count the number of satisfying assignments (call this $a$) up to additive error $k$. Here are some basic results for this: Case 1: $k=2^{n-1}-\mathrm{poly}(n)$ Here there is a deterministic poly-time algorithm: Let $m=2^n-2k = \mathrm{poly}(n)$. Now evaluate $\phi$ on $m$ arbitrary inputs (e.g. the lexicographically first $m$ inputs). Suppose $\ell$ of these inputs satisfy $\phi$. Then we know $a \geq \ell$ as there are at least $\ell$ satisfying assignments and $a \leq 2^n - (m-\ell)$ as there are at least $m-\ell$ unsatisfying assignments. The length of this interval is $2^n - (m-\ell) - \ell = 2k$. So if we output the midpoint $2^{n-1} -m/2 + \ell$ this is within $k$ of the correct answer, as required. Case 2: $k=2^n/\mathrm{poly}(n)$ Here we have a randomized poly-time algorithm: Evaluate $\phi$ at $m$ random points $X_1, \cdots, X_m \in \{0,1\}^n$. Let $\alpha = \frac{1}{m} \sum_{i=1}^m \phi(X_i)$ and $\varepsilon = k/2^n$. We output $2^n \cdot \alpha$. For this to have error at most $k$ we need $$k \geq |2^n \alpha - a| = 2^n |\alpha - a/2^n|,$$ which is equivalent to $|\alpha - a/2^n| \leq \varepsilon.$ By a Chernoff bound, $$\mathbb{P}[|\alpha - a/2^n| > \varepsilon] \leq 2^{-\Omega(m \varepsilon^2)},$$ as $\mathbb{E}[\phi(X_i)]=\mathbb{E}[\alpha]=a/2^n$. This implies that, if we choose $m=O(1/\varepsilon^2) = \mathrm{poly}(n)$ (and ensure $m$ is a power of $2$), then with probability at least $0.99$, the error is at most $k$. Case 3: $k=2^{cn + o(n)}$ for $c < 1$ In this case the problem is #P-hard: We will do a reduction from #3SAT. Take a 3CNF $\psi$ on $m$ variables. Pick $n \geq m$ such that $k < 2^{n-m-1}$ -- this requires $n = O(m/(1-c))$. Let $\phi=\psi$ except $\phi$ is now on $n$ variables, rather than $m$. If $\psi$ has $b$ satisfying assignments, then $\phi$ has $b \cdot 2^{n-m}$ satisfying assignments, as the $n-m$ "free" variables can take any value in a satisfying assignment. Now suppose we have $\hat{a}$ such that $|\hat{a}-a| \leq k$ -- that is $\hat{a}$ is an approximation to the number of satisfying assignments of $\phi$ with additive error $k$. Then $$|b-\hat{a}/2^{n-m}| = \left| \frac{a - \hat{a}}{2^{n-m}}\right| \leq \frac{k}{2^{n-m}} < 1/2.$$ Since $b$ is an integer, this means we can determine the exact value of $b$ from $\hat{a}$. Algorithmically determining the exact value of $b$ entails solving the #P-complete problem #3SAT. This means that it is #P-hard to compute $\hat{a}$. 

Here is a better bound on the sample complexity. (Although the computational complexity is still $n^k$.) Theorem. Assume there exists a subcube $S$ of size $2^{n-k}$ such that $|\mathbb{E}_{x \in S}[f(x)]| \geq 0.12$. With $O(2^k \cdot k \cdot \log n)$ samples we can, with high probability, identify a subcube $S'$ of size $2^{n-k}$ such that $|\mathbb{E}_{x \in S'}[f(x)]| \geq 0.1$. Note the small loss in parameters ($0.12$ is optimal versus guarantee of $0.1$). Proof. Pick $m$ points $P \subset \{0,1\}^n$ uniformly at random and query $f$ at each $x \in P$. Fix a subcube $S$ of size $2^{n-k}$. We have $\mathbb{E}[|S \cap P|]=m 2^{-k}$. By a Chernoff bound, $$\mathbb{P}[|S \cap P| < m 2^{-k-1}] \leq 2^{-\Omega(m 2^{-k})}.$$ Also $$\mathbb{P}[| \mathbb{E}_{x \in S \cap P}[f(x)] - \mathbb{E}_{x \in S}[f(x)] | > \varepsilon] \leq 2^{-\Omega(|S \cap P| \varepsilon^2)}.$$ By a union bound over all ${n \choose k}2^k$ choices of $S$, we have $$\mathbb{P}[\forall S ~~ | \mathbb{E}_{x \in S \cap P}[f(x)] - \mathbb{E}_{x \in S}[f(x)] | \leq \varepsilon] \geq 1 - {n \choose k} 2^k 2^{-\Omega(m 2^{-k} \varepsilon^2)}.$$ So by picking $m = O(2^k/\varepsilon^2 \cdot k \log n)$, we can ensure that, with probability at least $0.99$, we can estimate $\mathbb{E}_{x \in S}[f(x)]$ to within $\varepsilon$ for all subcubes $S$ of size $2^{n-k}$. Setting $\varepsilon=0.01$, we prove the theorem: picking the subcube with the largest $| \mathbb{E}_{x \in S \cap P}[f(x)]|$ will, with high probability, satisfy the requirements. Q.E.D. 

If you want $Y$ to have entropy less than $0.99 n$ bits, the answer is no, by the uncertainty principle: Either $Y$ has high entropy or its Fourier transform has large support. Theorem. Let $H(Y)$ be the Shannon entropy of $Y$ and let $F \subset \{0,1\}^n$ be the support of $\hat{Y}$. Then $H(Y) \geq n - \log |F|$. Proof. Consider the collision probability of $Y$ (the probability that two independent samples of $Y$ are the same). By Parseval's identity, $$CP(Y) = \sum_y \text{Pr}[Y=y]^2 = 2^{-n} \sum_s \hat{Y}(s)^2 \leq 2^{-n} |F|,$$ as $|\hat{Y}(s)| \leq 1$. On the other hand, $CP(Y) = 2^{-H_2(Y)}$, where $H_2(Y)$ is the Renyi entropy of $Y$. Noting that $H_2(Y) \leq H(Y)$ gives the result. Q.E.D. If $|F| = \mathrm{poly}(n)$, then $H(Y) = n-O(\log n)$. So, if $Y$ is the output of a pseudorandom generator, the seed length is at least $n-O(\log n)$. 

Note that, for $d=1$, we can set $f$ to be the median. So this shows how to generalise the median for $d>1$. Before proving this result, note that it is tight: Let $n=d+1$ and let $x_1, \cdots, x_d$ be the standard basis elements and $x_{d+1}=0$. Any subset of $d$ of the points is contained in an affine space $G$ of dimension $d-1$ (which is uniquely defined by those points). But no point is contained in all of those affine spaces. Hence there is some convex $G$ that contains $n\cdot d/(d+1)=d$ points but doesn't contain $f(x_1, \cdots, x_n)$, whatever value that takes.