Different solutions are going to cost different amounts per month, so some maybe be more or less expensive than others - choosing the minimum needed infrastructure to meet the SLA's is often the best choice. Outside of the other technical requirements (let's just keep it SQL Server at this point), you already have an FCI on site and setup. Depending on the version of SQL Server, you can do Availability Groups if you need that level of DR or BC. There is also Log Shipping, Transactional Replication, Backups (not even to an instance, just to a storage area), depending on the database feature usage there is also a bacpac which could be imported to IaaS or PaaS solutions. 

Probably my least favorite option but here for completeness. The logins could be logged to a table, but why re-invent the wheel? Trace and extended events will do this for you. $URL$ 

Creating a snapshot of a database causes the snapshot to run recovery - just as if you were to start up any database in SQL Server. Since there is 10 GB of log to go through... it's going to take a while to run recovery. Pair that with adding of more log to an already behind database and you have yourself a great recipe for never catching up and always being behind. From BOL: Uncommitted transactions are rolled back in a newly created database snapshot because the Database Engine runs recovery after the snapshot has been created (transactions in the database are not affected). 

A single log backup doesn't hold everything that ever happened, you'll need every log backup ever taken. If your database was ever in the simple recovery model, or if it was in the full recovery model and anything was accomplished in it before a full backup was taken then you're not going to get everything back. Period. Just because there are 100k records in the table doesn't mean there were 100k transactions, either. It could have been one big transaction, it could have been 2 transactions that inserted 200k and then a 3rd transaction that removed 100k, etc. ad nauseam. 

Then either change your edition or go without. You can write your own tracking using triggers, but it's going to make your application run extremely slow. You really need to look into items that do this, Golden Gate, CDC, CT, etc., and go from there. If you can't spend any money then you need to make a business justification case for it and if still nothing, you can write it using triggers. Note that using triggers is still spending money on development... so either way money is being used. 

Free ones of this would be a program and they are generally paid for. There are no free ones that I know of. Additionally the SQL Server Transaction Log is not documented as it is for internal use only. 

The "simple restart" just masked the problem. If you restarted a primary replica, then the primary would have changed - probably to the one with a working mapping. If it was a secondary when restarted, then the readable connection (if using read only routing) would have been disconnected and a reconnect would have went to your other secondary where the mapping was probably fine. The restart didn't "fix" anything, you got lucky and "masked" it. This is completely dependent on the exact state of each replica at the time you completed this. 

That depends on your connection string, DNS infrastructure, and availability group setup. If it's a single subnet, there shouldn't be an issue. If it's multi-subnet, you'll want to instruct the client driver to resolve all ips in parallel using the keyword in older version of .net/java/php/etc. Make sure you're using an updated client driver version that supports said keyword. There are additional windows clustering private property settings for net names that can influence how these things work, but it doesn't sound like you've made any manual adjustments to these (defaults used) and I won't muddy the waters with it but did want you to be aware that there are additional settings. 

Always On Availability Groups does not inherently need larger log files. Generally it ends up being the cause because most databases are in the simple recovery model before the change is made to Availability Groups and thus log managements is a new concept to that database. If the database was in the Simple recovery model and the log grew to X GBs, then the minimum log size will be X GBs regardless of recovery model, Always On, etc. However, since log management isn't automatic anymore a random number (generally 30 minutes or an hour) is arbitrarily chosen and log management happens on those intervals. Now the log will need to grow to accommodate the changes in log management automation. Much as @Shawn Melton and @RDFozz have stated in the comments - if you don't want the log growing too large and all of your replicas are properly in sync, then have your log management happen more frequently. 

You really don't have to be since you're not looking for HA or DR. That's my opinion, though, and won't translate well everywhere depending on the resources and familiarity with Windows Server. 

I would totally expect that, there would be more latency to get to the SAN than DAS. This probably (other than latency) won't effect the outcome too awful much, assuming the SAN has the same or more cache than the local disk controllers and the latency isn't too terrible. 

If the database replica that needs to be taken offline is a primary, check for the current secondary to be synchronized. Set the database to be "taken down" as a non-readable secondary. If the database replica was a primary, failover. Do any other items you'd like here, such as removing it from the AG, etc. 

If your min/max memory settings are set properly then SQL Server shouldn't be the main cause of memory pressure according to Windows, especially since you said it's dedicated with nothing else running. It could be internal memory pressure, where clock hands ae sweeping caches (inside or outside hands) and that would be due to workload/amount of memory available. That should be investigated. How do you know there is pressure? Could you update your post with what you've found? 

I want to point out something. When a failover occurs, any client using the listener to connect will have its' connection closed. This doesn't matter if is set or not. What MultiSubnetFailover helps with is the client driver to spin up multiple connection threads (1 for each IP) without the application knowing and connecting to the currently active IP transparently to the app. Having said that, since on a single subnet this setting isn't required or needed (but best practice is to still put it in the connection string as you don't know if it may be needed later or not) there isn't anything needing to be done. 

A hidden snapshot is taken and crash recovery run on the databases to bring it to a consistent state so that checkdb can run against the hidden snapshot copy of the database. This is how it works when checkdb is run online. There is the tablock option which is called 'offline' as it takes locks where the online way doesn't. 

Since you're using SQL Server 2014 I would use resource governor to limit the amount of IO. Obviously this will make the processes take MUCH longer and you still may have timeouts. There is no way to disable the IO messages and potential issues except to get a better IO subsystem or do less IO intensive items. 

This way there is always an odd number of votes which is ideal. Even if this is used, there are still scenarios in WS2012R2 that could cause the cluster to shut down due to quorum - many of these have been addressed with WS2016. 

No, Microsoft does not release the hashing algorithm. Additionally, hashing happens at a different layer than original query text - so even if you had the algorithm, you'd still need to normalize like SQL Server does, first. 

I've seen this with earlier version of SQL Server where rollover happened and you'd get negative numbers because the values are signed. I took a look at the source code and indeed these are 8-byte signed values. Since this is all C++, I created a simple repro: 

I would do the opposite. Have the GIS team change their connection string to the listener and set multisubnetfailover. It should, then, work... again with previously said assumptions. 

SQL Server Errorlog Application Event Log System Event Log Cluster Log/Clustering Event Log Hypervisor Logs/Database Internal Lists of Downtimes/Upgrades/Patches/Etc. 

This is not, by far, an exhaustive list but more a guide to give you something to look at and think about. You'll need to meet with your business owners and decide what is and is not acceptable. Maybe they don't need seamless integration, maybe they just need best effort. We don't know, but this is a start. 

No, there isn't a way to get the "original query text". I quote this, because the log records record what changes happen in that database. Since select statements, cte's, etc., do not generate* log records there is no way to get that information. Additionally, the log records describe the changes that have happened, of which a query could be generated to create the same changes again, reverse them, or show what was changed... however there is no way to find what the larger part - if any - it may play in the batch. Let's take a quick and simple example: Suppose we wanted to find all of the accounts using gift cards and give them an extra $1 if they made a purchase over $5 in the last 7 days. The log records would show the $1 increase to any of the accounts that met the above logic, but there is no place for us to see what that predicate was... we'd just see that some accounts (if any) were updated to have 1 added to a column (or whatever your model was). Thus we could re-create the UPDATE of the individual record, but no way to figure out what the original query was, however we could look at the entire transaction and make a logically equal batch. What if you wanted to do this in the future - to know all of the queries that have happened? Native to SQL Server there are various ways to log all of the queries that happen. Additionally there are 3rd party services and applications that can log this as well. 

You cannot eliminate log backups for the most important reason of transaction log re-use. It's not possible to know when an issue will occur and thus having multiple ways to get to the RPO is useful and sometimes necessary. Differentials aren't a substitute for log backups, they will lower the RTO as they are much faster to apply than log backups but ultimately aren't worth much on their own (spanning broken lsn points, etc, is useful) How are you going to know which ones you need and don't need ahead of time? 

This is troubling as it should definitely not be the case. Certain workloads are not for in memory tables (SQL 2014) and some workloads lend themselves to it. In most situations there can be a minimal bump in performance just by migrating and choosing the proper indexes. Originally I was thinking very narrow about your questions regarding this: 

No, databases involved in an availability cannot be restored or recovered unless first removed from the availability group. You can log ship to a database NOT in an availability group, however, just fine. 

Until the old principal server (now the mirror server) is upgraded to 2016 you will not be able to resume data movement - no data will flow until you upgrade the older instance to 2016 and manually resume the data movement. 

That's a good question. If you want to know what applications are causing this in a physical environment and even to a good extent a virtual environment then the following performance counters can be used: 

Yes, you're correct; they do require Windows Server Failover Clustering. However, I don't see how that's a show stopper. Does it take a little extra configuration - yes. Does it require a little more understanding about your actual failure scenarios and overall architecture - sure... but it also beats having to deal with mirroring. In fact, there were many, many, updates done to availability groups in 2016 that it is much more performant than mirroring. 

Yes, but it'll require some extra infrastructure setup - still easily doable. Setting up an AG, though, would cost more than log shipping for the additional usage of components (at least in Azure - I work for Microsoft) and require much less setup. Things to think about. 

Number of database Number of needed replicas for HA and DR Log generation rate CPU/Disk/Memory usage 

This can happen for a variety of reasons. There are some subtle nuances that make it seem like it isn't actually up. Let's take a look. 

Since you only need the successful logins, there is an event "sqlserver.login" that can work well for you. Send it to an event_file target and it's fairly simple as well. There are also functions to load the files and do your analysis. $URL$ 

Let's also assume that the cluster is setup with defaults. This means it'll take a minimum of 5 seconds to notice that something is wrong. It may take longer, depending on how the node fails up to a maximum of the SQL Server healthcheck interval (default 30 seconds). You can already see that finding the failure can take anywhere from 5-30 seconds, by default. The next step is initiating the failure and arbitrating for resources. This should take a trivial amount of time, a few seconds at most. This is where the listener is moved and becomes responsive to new login attempts. Additionally there is the time is takes for the databases to redo and become available - especially if there is a default database specified that is part of the AG (versus one that isn't, say ). This can take anywhere from near instantaneous to many minutes depending on use and hardware subsystems. Let's assume, for this, it only takes 3 seconds. Adding this all up we have: 

It seems that the Log Shipping process is somewhat of a mystery here so let me explain that and then answer the questions you have. Log shipping works by taking transaction log backups of a primary database and applying them to secondary database(s) through the use of jobs and a shared file location. In order to apply log backups, the databases that will have the log applied to them must be ready to accept the log restore (in NORECOVERY or STANDBY and must be FULL or BULK_LOGGED recovery model) and must be able to have the log restored to that database (LSNs must match or be within the ranges). Thus, when the databases were brought online in read/write mode and the DR test was run - the databases are no longer synchronized through LSNs as new operations, etc, were completed on the databases. This means you can no longer restore transaction log backups (simply put). 

Any log lower than the LSN currently requesting to be flushed will be flushed as well. Log records are put into the log buffer in order and all transactions are serialized to the log buffer for that database. Note that individual log records aren't flushed, they are put into log blocks and log blocks are flushed. 

There is one proper way to pre-stage the listener and one way to allow the cluster to create the listener itself. Please note that YOUR account is not what is used to authorize to AD to create the listener when creating it through FCM/Powershell or SQL Server, the CNO is used as security context. The official pre-stage way 

I spoke to people much smarter than I and we will be documenting this soonâ„¢. The actual definition of this, in the interim is: 

Only objects at the database level will be replicated as part of the log stream. System databases cannot currently be put in availability groups. Thus, any server level object or objects not contained in the user databases will need to be replicated. Examples include: Logins, Jobs, Server Level Certificates, Proxies, Server Level Configurations, Linked Server Definitions, Security Audits, etc. 

Don't try to be smarter than SQL Server. Use the available constructs that fit the need - in SQL Server 2012 (major version 11) has both identity and sequences. Use them. Purposely kill your concurrency by either using serializable or lock hints such as XLOCK. This will on purpose block other sessions which means you'll have less performance... and you're doing it on purpose... so yeah that seems bad but a possible solution. Pre-create values (again trying to be smarter than SQL Server) and use XLOCK + READPAST to get a little better concurrency from a different table. Blah. Let me reiterate #1, don't try to be smarter than SQL Server [in this case].