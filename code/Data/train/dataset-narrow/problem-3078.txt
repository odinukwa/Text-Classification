You actually convert the output of your algorithm from continuous to categorical. I see many reasons of why you would want to do that. A simple case for this would be when you have very long time-series data that take a lot of space to be saved. In this case, it is convenient to convert the timeseries in a histogram representation, which actually is a number of predefined bins where all the values fall into. This will greatly reduce the memory requirements of your algorithm because you just increase the count of a bin (integer number) when a measurement is taken that falls in it, instead of storing the measurement itself as a float number. This of course has pitfalls, such as losing the "time" dependencies between the measurements as well as reduction in resolution of your dataset. BUT, it can be really useful when forecasting measurements because instead of having the continuous R space as output of a predictor, you actually have (e.g.) 10 bins (classes) that the next measurement is predicted to fall into, making it easier for your model to be trained. FYI, this is exactly what is done in the automotive industry. EDIT - (Added sources) Sources: $URL$ (check the algorithm) $URL$ (the previous algorithm is applied here for trucks) $URL$ (another algorithm based on histograms of truck data) 

The main difference between supervised and unsupervised learning is the following: In supervised learning you have a set of labelled data, meaning that you have the values of the inputs and the outputs. What you try to achieve with machine learning is to find the true relationship between them, what we usually call the model in math. There are many different algorithms in machine learning that allow you to obtain a model of the data. The objective that you seek, and how you can use machine learning, is to predict the output given a new input, once you know the model. In unsupervised learning you don't have the data labelled. You can say that you have the inputs but not the outputs. And the objective is to find some kind of pattern in your data. You can find groups or clusters that you think that belong to the same group or output. Here you also have to obtain a model. And again, the objective you seek is to be able to predict the output given a new input. Finally, going back to your question, if you don't have labels you can not use supervised learning, you have to use unsupervised learning. 

For an intuitive explanation I also found this IMHO, for your problem with n=10 you can use d<=2, for n=1000 d=3 (maybe 4, at most 5). Why d=3 for n=1000? Roughly speaking this would correspond to 10 points along each dimension (10^3=1000), which is reasonable to fill the 3D space. For d=5 it is like 4 points in each dimension, which is not so good but not a disaster. IMHO, you should try to reformulate your problem and significantly reduce dimensionality (maybe try to use SVD or PCA). This may automatically solve your problem of noisy data. 

The size of each tree depends very much on its depth. Thus, change the maximal depth (). Try to set it to finite number (as opposed to the default "None") and then try to reduce this number. In addition (or as alternative) try to increase or . You can also analyze you features and keep only important ones. The simplest way would be to have a look at the of your forest. (In general, finding important features is an art and science on itself.) Exclude non-relevant features and rebuild the forest. 

converts strings to integers, but you have integers already. Thus, LabelEncoder will not help you anyway. Wenn you are using your column with integers as it is, treats it as numbers. This means, for example, that distance between 1 and 2 is 1, distance between 1 and 4 is 3. Can you say the same about your activities (if you know the meaning of the integers)? What is the pairwise distances between, for example, "exercise", "work", "rest", "leasure"? If you think, that the pairwise distance between any pair of activities is 1, because those are just different activities, then is your choice. 

Microsoft announced a couple of weeks ago virtual machines in Azure with GPUs. They use K-80 NVIDIA cards. The biggest machine has 4 GPUs and 224 GB of ram. Good to play with deep learning :-) 

I think mxnet is one of the best options if you code in R. They have an R wrapper but the core is in C++. They have several examples in the web. One of them is the character recognition with MNIST database. They have support for multi-gpus and also for Spark. 

This is the typical value function of Reinforcement Learning. The discount factor evaluates the importance of the accumulated future events in your current value. The smaller the number, the less important are the future events in the current action. Usually this number is selected heuristically. I usually select 0.9. If I don't want any discount then I would select 1. 

The model I would use is the one that minimizes the accumulated quadratic error. Both models you are using, linear and quadratic, looks good. You can compute which one has the lowest error. If you want to use an advanced method you can use RANSAC. It is an iterative method for regression that assumes that there are outliers and remove them from the optimization. So your model should be more accurate that just using the first approach I told you. 

I think, even before doing LDA, you should remove words which appear in more than "x" percent of your documents. Try different "x" starting from 80% and then going down. The logic is that if the word is common for many documents, it does not distinguished those and should be neglected. 

First, I would like to emphasize, that cross-validation on itself does not give you any insights about overfitting. This would require comparing training and validation errors over the epochs. Typically you make such comparison with your eye and you can start with one train/validation split. Question 1: By getting validation error N times, you develop a reasonable (whether it is very or not very good is a question) understanding of how your network will perform (= what error it will give) on the new unseen data. Often you do cross validation as a part of grid search of hyper-parameters. Then averaging errors at step 6 is mainly for choosing the best hyper-parameters: you believe that the hyper-parameters are best if the corresponding network produces the smallest average validation error. Simultaneously this error is your estimation on what error the final model will give you on the new data. If you want, you can proceed with your exploration and compare validation errors (for one and the same hyper-parameter set) with each other, calculate standard deviation in order to get further insights. The concept "model generalizes well" is more related to the absence of overfitting. To make this conclusion, you need to compare train and validation errors. If they are close to each other, then the model generalizes well. This is not directly related to cross validation. Question 2: The only purpose to take the whole dataset is to train the model on more data. And more data is always good. On the other side, if you are happy with one of the models produced during cross-validation, you can take it. Question 3: You can take the number of epochs as one of the parameters in grid-search of hyper-parameters. This search usually goes with cross-validation inside. When at step 7 you take the whole data set, you take more data. Thus, overfitting, if at all, at this stage can only be reduced. If it bothers you, that each chunk is small, replace K-fold cross validation with, for example, K times 50/50 train/test splits. And I would never worry about leave-one-out. It was developed for small (very small) datasets. For Neural Net to be good, you typically need large or very large dataset. 

A good library for machine learning with GPUs is mxnet. The package is mostly deep learning though, so if you are looking for specific machine learning algorithms you might not find them there. However they have a good set of deep learning algorithms. 

Yes, they can connect natively. You can manage data and then put it all in different services like it is showed next. As you can see you can use SQL database, blob storate and also PowerBI. 

Just to add some more resources. Recently there was a paper studying the differences between several packages of neural networks and deep neural networks. Here you can find the information. It looks that Torch and TensorFlow are the winners. Note: not all of them are in python. However, I posted it to open the discussion. 

I would try to set a multilabel classification algorithm and make the output standard by adding zeros. So if your data is like this: <1, 1>, <2, [1, 1]>, <3, [2, 1]>, <4, [1, 2, 1, 1]>, <5, [1, 1, 1, 2, 2, 1]>. The maximum number of output is 6. So you could transform your data into something like: <1, [1,0,0,0,0,0]>, <2, [1, 1,0,0,0,0]>, <3, [2, 1,0,0,0,0]>, <4, [1, 2, 1, 1,0,0]>, <5, [1, 1, 1, 2, 2, 1]> Another option that occurs to me is to add the limit dynamically. Let say you have your training and test set. You can search for the biggest length and create an algorithm that adds the zeros to both datasets. Then let's say a new data you want to predict has a bigger length, then you'll need to recompute all training and test with for this new prediction. You can even check how extending the limit affects your model. 

You are using RandomForest with the default number of trees, which is 10. For around 30 features this is too few. Therefore standard deviation is large. Try at least 100 or even 1000 trees, like 

No. There is no similar mechanism for continuous variable. If it worries you, that , you can 1) demean the price, that is subtract mean price from all price values. Then negative values will clearly show below-average and positive above-average prices. 2) after demeaning you can divide values by the standard deviation of the price. (1) and (2) together is called "standardization". Alternatively you can 3) rescale your price to the range of values you want. Usual choice is (0,1) range. If you do this for one feature (price in your case), then it makes sense to do the same for other features. Whether or not this will help to get better prediction results depends on the model. Some models, a typical example would be SVM, do require such transformation. 

With (d=256, n=10) as well as with (d=16000, n=1000) you are under the curse of dimensionality. The essence of the curse (quoted from Wikipedia) 

The question 'is this a good model?' is essentially a business question. There is always a precision/recall tradeoff and you decide based on your business goal what model to choose. Data science can help you with the question on how to compare classifiers. Let us start, that each (or nearly each) classifier can predict not only the outcome (Class2 or Class3), but also the probability of this outcome. Your and all your metrics are suitable for making business decision, but, from data science point of view, have two problems 1) they do not take into account the agreement by chance 2) they are based on the threshold probability 0.5, which is not necessarily the optimal probability for deciding about the class. To deal with (1), have a look at Cohen's kappa To deal with (2), use metrics based on probability. The simpler one is Area Under the ROC Curve, see also here. Even finer, but not so straightforward to interpret is Logarithmic Loss 

Can somebody answer that? It would be good if the answer comes with evidences or some research paper. I'm not asking for opinions 

I think there might be a problem in the way you are stating the problem. You say that you test data doesn't have two fields, but that can not be correct. You have to take all your data and split it into 2 groups, the training set and the test set. In a proportion of 80%-20% or 70%-30%. Then you train your algorithm with the data in the training set, and test the accuracy of the model with the data in the test set. The accuracy you get is the probability that your model is correct. Or said in another way, the next time you use your model to predict a sale, the accuracy is the probability that your prediction is real 

A good way to measure the difference between two probabilistic distributions is Kullbak-Liebler. You have to take into account that the distribution has integrate to one. Also you have to take into account that it's not a distance because it's not symmetric. KL(A,B) not equal to KL(B,A) 

I would suggest Recurrent Neural Nets. They are good for time series, however they need a huge dataset to get good performance. Here you can find an implementation in torch.