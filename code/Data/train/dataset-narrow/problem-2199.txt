I guess we could try to see this as a modelling problem: how can we re-phrase the question so that it becomes computer science and not physics? I'll try to give a simple, concrete example of how we might try to do this, to get things started... 

Reductions of type 1 are common in the context of positive results, and these are certainly good reasons to feel optimistic. However, if you consider hardness reductions that we encounter in the context of, e.g., NP-hardness proofs, they are almost always of type 2. Note that even if you do not know anything about the computational complexity of problem A or problem B, you can nevertheless tell if your reduction is of type 1 or type 2. Hence we do not need to believe in, e.g., P ≠ NP to determine if we should feel optimistic or pessimistic. We can just see what we have learned thanks to the reduction. 

Models are just models. I wouldn't push it too far; they tell something about some aspects of your algorithms, but not the whole truth. I would suggest that you simply use the standard word RAM model in your analysis and implement the algorithm and see how well it performs in practice. (Actually just implementing your algorithm without ever running it tells you already a lot about it... For one thing, it is then provably implementable.) 

At least two of the literals $x$, $y$, $z$ are satisfied iff at least one literal in each pair $(x,y)$, $(x,z)$, $(y,z)$ is satisfied. Therefore it is a special case of 2SAT, and there is a polynomial-time algorithm for solving it. 

Very often, if the running time of an algorithm is a complicated expression, the algorithm itself is also complicated and impractical. Each of the cube roots and $\log \log n$ factors in the asymptotic running time tends to add complexity to the algorithm and also hidden constant factors to the running time. Do we have striking examples in which this rule of thumb fails? Of course it is easy to find examples of algorithms that are very difficult to implement even though they happen to have a very simple worst-case running time. But what about the converse? 

Here is a simple example from the field of distributed algorithms. Typically randomness helps tremendously. Randomised distributed algorithms are often easier to design and they are faster. However, if you have a fast deterministic distributed algorithm, you can mechanically convert [1, 2] it into a fast self-stabilising algorithm. In essence, you will get a very strong version of fault-tolerance for free (at least if the bottleneck resource is the number of communication rounds). You can simplify your algorithm design by focusing on fault-free synchronous static networks, and the conversion will give you a fault-tolerant algorithm that can handle asynchronous dynamic networks. No such conversion is known for randomised distributed algorithms in general. 

Often, when we take part in TCS conferences, we notice some little details that we wish the conference organisers would have taken care of. And when we are organising conferences, we have already forgotten it. Hence the question: Which small steps we could easily take to improve TCS conferences? Hopefully, this question could become a resource that we could double-check whenever we are organising conferences, to make sure that we do not repeat the same mistakes again and again... 

This question is about quadratic programming problems with box constraints (box-QP), i.e., optimisation problems of the form 

The fractional chromatic number is at most $k$. The fractional chromatic number is at least $k^{\log(k)/25}$. 

I think one of the safest approaches is to come up with multiple independent proofs. Then you can be confident that your main result is correct, even if you have a mistake in some details of a proof. 

(These are formulated as tree problems, but you can generalise them to arbitrary graphs. Then the above formulations are obtained as the special case when you restrict your input to trees.) 

Suresh mentions OmniGraffle and Ross mentions Graphviz. Actually, OmniGraffle is Graphviz with a GUI (and much more). You can draw a graph (or import it from a file), then use a Graphiz-based layout engine to automatically layout the graph. You can tweak the parameters of the layout engine, and finally you can switch off automatic layout and fine-tune the placement of the nodes manually. That said, I still tend to use Xfig for most illustrations in my papers... The possibility to embed arbitrary Latex code in your illustrations is often essential, and that's exactly where Xfig excels (at least until you need to deal with publishers who expect self-contained EPS files). 

I think the classical definition of the approximation factor emphasises the first interpretation. We classify problems according to how easy they are to solve fairly well. The differential approximation ratio seem to put a bit more weight on the second interpretation: we do not want to "reward" trivial algorithms (e.g., algorithms that just output an empty set, or the set of all nodes). Of course, both are valid viewpoints, but they are different viewpoints. 

No. Construction: Take two copies of $K_{3,3}$, one with the nodes $\{a,b,c\} \cup \{a',b',c'\}$ and the other one with the nodes $\{d,e,f\} \cup \{d',e',f'\}$. Remove the edges $(c,c')$ and $(d,d')$. Add the edges $(c,d')$ and $(d,c')$. 

Now it might be interesting to see if there is a non-trivial answer to this question. For example, which CAs admit computers that have size smaller than half of $W$? 

Does anyone recognise the following problems? Do they have names? Are they hard? If we were looking for an exact match (0 mismatches), these would be solvable in polynomial time (using e.g. standard algorithms for rooted tree isomorphism). But what about the inexact case? Variant 1 Input: 

However, what is ambiguous is something like "the running time is $O(n)$". As you have observed, there are two sources of ambiguity: 

If you want something that cannot be easily reduced to the maximum-weight matching problem, here is one example: the stable marriage problem. One interpretation is that in the stable marriage problem, $f_v$ is the "stability" of the vertex $v$; it is $0$ if $v$ is incident to an unstable edge (blocking edge) and $1$ otherwise. Then the objective is to find a matching that maximises $\sum_v f_v$. (And this can be solved by using the Gale–Shapley algorithm; the optimum is always $|V|$.) A crucial property of this $f_v$ is that it depends not only on which edges incident to $v$ are matched, but also on the neighbours of the edges incident to $v$. (Edit: The above property is essential in order to get something that isn't just the maximum-weight matching problem in disguise. Note that if feasible solutions are matchings and if $f_v$ only depends on which edges incident to $v$ are matched, then we can define the weight $w(e)$ of an edge $e = \{u,v\}$ as follows: how much $f_u + f_v$ increases if we replace an empty matching $M = \emptyset$ by a matching $M' = \{e\}$ that contains just the edge $e$. A maximum-weight matching w.r.t. these weights also maximises $\sum_v f_v$.) 

Now it follows that there exists an $i$ such that $-1 \le g(i) \le +1$. Hence we can construct an $(n+O(\log n))$-bit string $y$ as follows: concatenate $f(x,i)$ and the binary encoding of the index $i$. The absolute value of the imbalance of $y$ is $O(\log n)$. Moreover, we can recover $x$ given $y$; the mapping is bijection. Finally, you can add $O(\log n)$ dummy bits that reduce the imbalance of $y$ from $O(\log n)$ to $0$. 

Background: I have some fairly small box-QPs that I would actually like to solve, and I was a bit surprised to see how poorly some commercial software packages perform, even for very small values of $n$. I started to wonder if there is a TCS explanation for this observation. 

Let's replace the "universe" by something that is very discrete and simple (and finite!). Let's say that our universe is a finite cellular automaton. In particular, the whole world $W$ is an $n \times n$ grid. Assume that the initial configuration of the world $W$ is arbitrary. Now the question seems to be the following: Can we choose a strict subset $C$ of $W$ ("computer"), and an initial state of $C$, that satisfies the following conditions: 

Background (This section can be safely skipped.) The question is related to the foundations of distributed computing, and in particular to local algorithms. What we would like to understand is the following: in which situations the existence of a total order helps with local symmetry breaking in a distributed system. Intuitively, each node $v$ of $G$ has to produce an output that is a function of $(G_v,\le_v)$, i.e., a function of the local neighbourhood of $v$. If an edge $e = \{u,v\}$ is bad, there is some local symmetry-breaking information available near $e$, and nodes $u$ and $v$ may produce different outputs; if the edge is good, then nodes $u$ and $v$ are locally indistinguishable and they have to produce the same output. For many classical graph problems it is known that a total order does not help (much weaker relations provide essentially the same amount of symmetry-breaking information), but some cases are still open – and a general result that covers the case of all high-girth graphs could be a breakthrough. This might be a win-win question: regardless of the answer, we learn something new. If the answer is "yes", we might be able to derive new, stronger lower-bound results; if the answer is "no", we might be able to design faster algorithms that exploit the local symmetry-breaking information that is available in any $(G,\le)$. Of course in the real world we do not have a total order on $V$; we have something more: each node $v \in V$ has a unique label $\ell(v) \in \mathbb{N}$. But bridging the gap between a total order and unique labels is usually more straightforward; often a Ramsey-like argument shows that (in the worst case) the labels do not provide any information that is not available in a total order.