My question is pretty straightforward. Using OpenOffice or Microsoft Office, is it possible to automatically lock a file for edition on the Alfresco file system via Samba? For example, if a user goes to the samba directory, click on a file for editing it, it would be locked. When he finish to edit it (close the document), this document would be reopen to everyone. Thanks for your help :) 

My server distribute two main websites, says : www.google.com & www.facebook.com (yeah I know :p) I want them to be distributed via https. Using Apache, I defined a vhost file in sites-available/enabled containing this : 

I really have a server called db1.mydomain.com but for a reason I can't find, it's relayed to localhost.mydomain.com. Of course, I configured aliases : 

I'm having some security issues regarding my uWSGI configuration. Here's the current issue: I have a front server, called api.domain.tld, that have NGinx installed with that points to the uWSGI instance using a file. This uWSGI instance on the frontend server is configured that way: 

What does that mean ? I did the same command with a smaller file and I got a , so clearly, the first try didn't succeed completly, but I don't know what went wrong neither what to do to make it work completely. Is there a possible solution to that ? Thanks for your help ! 

I'm sure CIFS is working because if I connect with a local machine, it works well, but not via Internet. Thanks for your enlightenment! :) 

does not work, with the error that "/usr/share/phpmyadmin/phpmyadmin/index.php" was not found. So is my actual configuration secure enough regarding the access of ? Thank you for your help! 

stsadm -o backup and stsadm -o restore. This moves site collections and not sites. Lots of people are talking about sites when they really mean site collections. This is usually the easiest method and can move data at a rate of about 15 GB/hr. If your site is the only site in the site collection, this is your best bet. stsadm -o export and stsadm -o import. This moves sites. Not as easy to use as backup/restore, but does the trick. There are gotchas when it comes to permissioning, so make sure the site is accessible as expected after the move. You cannot import a site into a site collection where it already exists (which shouldn't be a problem in your case). You will need to make sure all customizations (templates, web parts, features, solutions) are deployed to the new site collection you are importing into otherwise it's error time. stsadm -o mergecontentdb. This can move a site collection from one database to another. This seems more complex at first, but is easier to use than backup/restore. This moves a site collection within a farm. Create a new content database and move it to the new db, detach, and reattach the database in the other installation (farm). stsadm -o gl-moveweb part of Gary Lapointe's stsadm extensions (every farm should have this installed). This moves a site within a farm. (gl-movesite moves a site collection). Most 3rd party SharePoint backup applications have a feature to backup/restore a site to another farm. 

I want to sysprep a Windows Server 2008 R2 SP1 machine that has SQL Server 2008 R2 SP1 installed (for reference, SQL Server 2008 R2 has a new sysprep feature that allows the instance to be sysprepped). On the server is a SQL Server client alias that points to the default SQL Server database engine instance. For reference, the alias is called Alias-SQLServer and has been configured in both 32-bit and 64-bit cliconfig versions (that is, both registry keys exist) The alias points to the local instance as the image will be used to create development VMs and the installation script for the application that is being developed will use the SQL Server client alias in order to generalize the installation scripts. I can't seem to find information about whether the sysprep tool will update the SQL Server client alias's registry keys with the server's new name once it's unsealed. My guess is that it is not; how is sysprep to know that the server name the alias points to will be different for each image? Right? Perhaps if the alias points to localhost instead of the server name this will work? 

And that, in the same server. I thought about installing only Apache on the server, that would redirect request to the dedicated Docker instance based on the server name, but then I would have to install Apache (again !) andMySQL on any Docker instances. Is this possible and moreover, is this interesting in term of performance (or not at all)? Thank you for your help. 

I'm not quite used to IPTables and I'm trying to run an iptables script to allow only ssh connection from all and connection to mysql server only from specified IPs. I made a bash script for this, which is lister under, but when I run this, my master-master replication stops working. For information, here's my network structure : 

I ask this because I will setup multiple servers and it would be faster to do something like this. Thanks in advance. 

But still, the problem persist. I don't know what I missed and where to fix it. And everything I tried to search on Google doesn't really helps me (it's about /etc/aliases most of the time). Thank you for your help. (if you need more details, please ask, I'll add them) 

Well, I'll answer myself on that one. The problem is not from NGinx but from the backend, here, PlayFramework that returns a 404 when a HEAD is requested and the routes files does not contains HEAD. A bug has been opened for that : $URL$ 

What is wrong with this configuration ? Why a HEAD returns 404 instead of 200 ? Thanks for the help :) 

My question is similar to Powershell Remoting: One way trust, however there are differences and the resolution (adding the server to the trusted list) does not work for me. Scenario: I have two domains. DOMAIN and DOMAINDMZ. DOMAIN has an incoming trust from DOMAINDMZ. That is DOMAINDMZ trusts DOMAIN, but not vice versa. I have an administrative user who is a member of the Administrators local group on several servers in the DOMAINDMZ domain: , , , etc. I can log into these servers with DOMAIN\myadmin from the console or RDP. I am attempting to log into SERVERA and run a PowerShell script on SERVERB using PowerShell remoting. Remote Management is enabled on SERVERB. I start an elevated PowerShell session on SERVERA, and then attempt to use the cmdlet. I receive the following error: 

On Windows Server 2008, it is event ID 5136 (Directory Service Changes). See also event IDs 5137 (create), 5138 (undelete), 5130 (move). Event ID 4662 contains the old-style audit event (see below). On Windows 2000 Server and Windows Server 2003: 

One way: add xerox_1600n to your network's DNS infrastructure (the internal/local DNS servers your client machines use) . It won't "magically pop-up," but it will resolve when entered. 

Copying the files in 12\TEMPLATE should do the trick, assuming the "templates" you are talking about are site definitions. You'll need to do an IIS reset on the server you copy to once the files have been copied. Usually site definitions are "site collection templates," so you would see them when adding a new site collection. If they are "site templates" you'll see them when adding a new site. There are no .STP files involved with site definitions. Best practice is to package and deploy a site definition as a solution as this simplifies the process. 

I manage servers where users have their own websites on it that can be accessed by FTP (like an hosting company) and instead of working on isolating LAMP stack processes, I was wondering if it was possible to implement Docker and use an images per website. From what I understand, you can expose Docker instance via their ports, so if you run two docker instance on the same server, you'll have to expose two different ports. But is it possible to export not ports, but server name, like : 

(The ip is the IP to my server). I also tried this (interesting tool) : $URL$ The only problems are related to HTTPS, everything else is good. Having "everything else good" is what bother me : if it's good, why Google still reject my emails ?? What did I do wrong ? Do I have to add a SPF also to mail.mediafins.com, my MX DNS entry ? Thank you for your help, I really appreciate ! 

I want to execute a local (or remote (via http)) script when someone hits one of my website with GET request. I saw the mod_actions module for Apache which would be great if it would also work for simple GET requests : 

All my users in /home/{user}/ have a specific error_log file in it that may grow overtime. So I was thinking about using logrotate to implement some kind of file reducing on it : when the file reaches 500kb, we remove the first lines to reduce it to lower than 500kb. It's not important to keep what is removed, so keeping the old lines is not necessary. I took a look at logrotate, and I came to this configuration file, but since I'm new with LogRotate, I was wondering if it would work. 

I have set up a lab with a number of Windows Server 2012 R2 machines. The lab has an Active Directory domain (DFL: Windows Server 2012 R2, FFL: Windows Server 2012 R2) and these machines are joined to the domain. By default if left unattended these Windows machines will automatically lock. I do not want the machines to lock automatically. I do not have any security concerns with having the machines remain unlocked as this is an isolated lab. I have created a group policy object that sets a number of configurations and the machines still lock. I have verified that the GPO has been applied to the machines. The GPO configures the following settings: 

On the suggestion of checking the Power Management settings by @joeqwerty I created a new Power Plan with the following settings: 

You may need to use the ADScpRegister utility from (included wtih RMS for Windows Server 2008, located in "C:\Program Files\RMS SP2 Administration Toolkit" ) to re-register the the URL: Here is a guide for installing RMS on Windows Server 2008, that discusses the ADScpRegister utility. 

If I provide a credential object containing a user in the DOMAINDMZ domain, the error goes away and the scriptblock executes as expected: 

First, no matter what you do, you should run the SharePoint Prescan tool, to determine any possible issues you may have with upgrading. Here is a guide to running the prescan tool. There are many different migration tools and methods to migrate from SharePoint v2 to SharePoint v3. Microsoft has a TechNet article about the different (Microsoft supported) methods available, pros and cons, and how to perform each: Determine upgrade approach [Windows SharePoint Services]. There is a SPS to MOSS version available as well. Generally the database migration upgrade method is the easiest (assuming you are not using a lot of customizations). This involves migrating the SQL database (if necessary) and attaching it to your new SharePoint instance (either through Central Administator or with stsadm -o addcontentdb). SharePoint will upgrade the database automatically. In place upgrade will probably not suit you, considering you are not upgrading the WSS instance, but moving the content to a new farm. A commercial tool that I have used is metalogix SharePoint Site Migration Manager and it usually does the trick. It uses the v2 and v3 APIs to copy content from your old instance to your new instance. 

But on Play Framework, asking for the client IP (request.remoteAddress) results in a . I'm sure it's because of the proxy, but I don't know which parameter to set to make this works. Thanks for your help ! 

I saw the directive, that is exactly what I want, but do I have to specify for every location {}, the proxy_* configuration ? I have a specific message to display if the user reach the quota allowed, in my app (located at /errors/413). If I go directly using my browser, it works. But using the configuration below, I've got a "504 Gateway Time-out". Why? 

Every three months, my Let's Encrypt certificate expires, and my customers get an invalid https certificate. So I recently placed the following cron task : 

Is this possible? Would it work if instead of a socket I would allow a local (127.0.0.1) only access ? Thank you for your insighs. 

I'm looking to set up a webmail server that will be used by a lots of users that will receive and send emails. They will also have the possibility to forward emails they receive. I'd like to know which steps are recommanded/required to indicate to others Mail services (GMail, Outlook, etc) that my server is not used as a spam sender (disclaimer : IT's NOT ! :p) but a legitimate one. I know I have to define a SPF TXT records for example, but what others steps would you recommend me to do ? For example, is there a formula like having a proportional number of servers based on the amount of email sent (for having a different IP address) ? (something like sending a maximum of 1M emails / per IP / per day ?) Something else I'm missing ? I tried to search online, but I mostly find how to avoid emails sent with scripts (like PHP) being put in the SPAM folder. I'm looking for a server/dns configuration side. Thanks a lot for your help/tips, I appreciate !