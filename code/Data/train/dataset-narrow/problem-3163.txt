Another way of looking at 600 columns is having a vector with 600 dimensions. So, the answer to reduce the 600 dimensional vectors to fewer dimensions without losing lot of information will be dimensionality reduction. Principal component analysis(PCA) is a commonly used one. T-distributed stochastic neighbour embedding (TSNE) has been the state of the art for reducing higher dimensional vectors. Data visualisation using TSNE To get the quick intuition about the vectors, the best thing is to reduce it to 2 or 3 dimensional vectors and plot them in a graph. Data visualisation always helps to get an overview of the data you are dealing with. 

Have you tried word2vec models ?. One of the main application is computing similar words, as they are very closer in the feature vector space. Thus apart from and , also , may also be obtained as closer words. The important thing is that a huge amount of sentences are needed for this unsupervised learning model. Ref: gensim, word2vec in java 

The original papers written by Thomas Mikolov are word2vec and doc2vec. Lecture notes given by Richard Socher is simple to understand. word2vec explained gives an intuition how the negative sampling in word embedding works. Not the whole algorithm though. Nevertheless, there are implementations in various languages and libraries explaining how it works in their docs. Python - gensim, Tensorflow java - deeplearning4j 

I'm not familiar with Torch, but since basically word2vec and doc2vec are considered, These models learn from each sentences and so there is no need to have all the sentences in the memory. You could iterate via each sentence in the corpora and let the model learn from each of the sentence. And that is probably how people train on huge corpora with or without high computation machines. A short example in python: 

In , If I train a set of sentences multiple times with change in order (as it increases the vector representations), will the frequency of a word get changed due to it.? For example, if I have the word "deer" in my corpus 4 times and If I set the to be 5, does training the model 3 times repeatedly count "deer" with frequency 12 and will be included in the model ? If it knows it is the same corpus then how it is possible to differentiate, if I retrain the model with a new corpus. 

Both the data could be used for a better model. As your aim is to classify whether the employee will quit or continue, use each person's monthly entries and construct them(marital status, experience, daily working hours, age, salary) as features with the output quit/continue. Train a classifier with the available data and use it to predict new data. 

I have a confusion matrix with 7 classes and would like to represent the matrix in a graph. Something similar like a confusion wheel. Mainly I need to show, the correct observations in each class and incorrect ones with other classes in a pie chart. Any python implementations would be great. 

The iterator provides each data from the files line by line, rather than loading an entire document into the model. It is not a memory efficient approach when you have few documents that could fit into your memory. 

The words "Earth" and "earth" may have the same meaning, but according to word2vec algorithm, it derives the semantic information from the position of the words. Thus commonly, "Earth" will appear most often at the start of the sentence being a subject and "earth" will appear mostly in the object form at the end. So, the closest adjacent words may differ, but on overall both the sentences may contain the words such as "pollution, climate, water, countries". In conclusion, I guess with a larger window size, it seems to preserve the same semantic information with a little changes where the "Earth" will have some subject information and "earth" will have object information. So, averaging wont affect much and seems to be a possible case. But with lower window size, there is a high probability that it could have different meanings. 

I haven't read the paper you linked, but I have followed a lecture notes by Richard Socher. So, basically that mapping matrix is called weight matrices. There are two weight matrices and for input and output mapping. Thus for each word, vectors in both the matrices are updated via backpropagation. To answer your question, a word is represented by one-hot sparse vector which has the size of (V is size of vocabulary), with a value of 1 in one of the position. and when this vector is multiplied with the weight matrix with size , the corresponding embedding vector of size (N is size of the required embedding vector) is used in the hidden layer. Document has no one-hot represented vectors. So, basically there is a document matrix of size (d is number of documents) where each column represents a document. In other words, the matrices and need the one-hot representation only for mathematical steps, other than that they are representing each word in each column just as the document matrix . 

In the world of Big data, the data is already there and there are more than enough ML models currently but constantly there are improvements and applications, the main reason for that is how the features are defined from the raw data. In your case, introduce a new feature as penalty for the bias present in the data. For ex. in the presentation bias, high penalty is given to the top items and low penalty is given to the items farther down. similarly, introduce weight feature with less weight for men products when most of the products are male products and vice versa. ML still needs human intuition in the form of features ;-) 

I believe that you misunderstood the word2vec concept. Basically for words, the feature vector for a word is learnt from the surrounding words. - Firth.J.R In your case characters have been used, so the feature vector for each character depends upon the adjacent characters present. your example might work, if you have the following training sentences. 

I have read that distributional representation is based on distributional hypothesis that words occurring in similar context tends to have similar meanings. Word2Vec and Doc2Vec both are modeled according to this hypothesis. But, in the original paper, even they are titled as and . So, are these algorithms based on distributional representation or distributed representation. How about other models such as LDA and LSA. 

Tensorflow has implementations for a pool of machine learning algorithms, so it should be comfortable if your application needs to build something on top of word2vec. Gensim is mainly intended for topic modelling techniques, but pretty robust as its their main work. If you want to get a clear grasp of how the algorithm works, then implementing manually makes sense. Else, just go with one of the implementations. Google word2vec model is pretty good and covers most of the English words. Use it, if you do not have computational power or time to train a model. Manual training gives you the freedom of choosing domain-specific dataset, window size, cbow or sg, length of vector. If there are Out of vocabulary(OOV) words, it will throw an error.