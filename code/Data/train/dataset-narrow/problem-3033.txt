Why not just use quantile regressions? Usually, in regressions, the coefficients are estimated for the average case: $\min_\beta \sum_i||X_i\cdot\beta -y_i||^2$. If you want, you can estimate your coefficients so that they tell you things like $P(R=y)>p$. You give a probability $p$, and the model gives you $y$. You can even build probability density function out of these estimation techniques. In quantile regressions, you solve this optimization problem for every quantile $\tau$ that you want, $\min_\beta \sum_{i|y_i\geq X_i\cdot\beta}\tau|X_i\cdot\beta-y_i| + \sum_{i|y_i< X_i\cdot\beta}(1-\tau)|X_i\cdot\beta-y_i|$. This is a more robust approach to modeling the revenue distribution. Usually people use quantile regressions because software is readily available, but they can be implemented for other models as well. I work in an institute that does exactly that. 

I disagree with the other comments. First of all, I see no need to normalize data for decision trees. Decision trees work by calculating a score (usually entropy) for each different division of the data $(X\leq x_i,X>x_i)$. Applying a transformation to the data that does not change the order of the data makes no difference. Random forests are just a bunch of decision trees, so it doesn't change this rationale. Neural networks are a different story. First of all, in terms of prediction, it makes no difference. The neural network can easily counter your normalization since it just scales the weights and changes the bias. The big problem is in the training. If you use an algorithm like resilient backpropagation to estimate the weights of the neural network, then it makes no difference. The reason is because it uses the sign of the gradient, not its magnitude, when changing the weights in the direction of whatever minimizes your error. This is the default algorithm for the package in R, by the way. When does it make a difference? When you are using traditional backpropagation with sigmoid activation functions, it can saturate the sigmoid derivative. Consider the sigmoid function (green) and its derivative (blue): 

You might want to interpret your coefficients. That is, to be able to say things like "if I increase my variable $X_1$ by 1, then, on average and all else being equal, $Y$ should increase by $\beta_1$". For your coefficients to be interpretable, linear regression assumes a bunch of things. One of these things is no multicollinearity. That is, your $X$ variables should not be correlated against each other. Another is Homoscedasticity. The errors your model commits should have the same variance, i.e. you should ensure the linear regression does not make small errors for low values of $X$ and big errors for higher values of $X$. In other words, the difference between what you predict $\hat Y$ and the true values $Y$ should be constant. You can ensure that by making sure that $Y$ follows a Gaussian distribution. (The proof is highly mathematical.) Depending on your data, you may be able to make it Gaussian. Typical transformations are taking the inverse, the logarithm or square roots. Many others exist of course, it all depends on your data. You have to look at your data, and then do a histogram or run a normality test, such as the Shapiro-Wilk test. These are all techniques to build an unbiased estimator. I don't think it has anything to do with convergence as others have said (sometimes you may also want to normalize your data, but that is a different topic). Following the linear regression assumptions is important if you want to either interpret the coefficients or if you want to use statistical tests in your model. Otherwise, forget about it. Applying the logarithm or normalizing your data, is also important because linear regression optimization algorithms typically minimize $\|\hat y - y\|^2$, so if you have some big $y$ outliers, your estimator is going to be VERY concerned about minimizing those, since it is concerned about the squared error, not absolute error. Normalizing your data is important in those case and this is why scikit-learn has a option in the LinearRegression constructor. 

I am not saying your are committing this fallacy about thinking more trees can cause overfitting. You clearly are not since you have asked for a lower bound. This is just something that has been bugging me for awhile, and I think it is important to keep in mind. (Addendum: Elements of Statistical Learning discusses this at page 596, and is in agreement on this with me. «It is certainly true that increasing B [B=number of trees] does not cause the random forest sequence to overfit». The author does make the observation that «this limit can overfit the data». In other words, since other hyperparameters may lead to overfitting, creating a robust model does not rescue you from overfit. You have to pay attention when cross-validating your other hyperparameters.) To answer your question, adding decision trees will always be beneficial to your ensemble. It will always make it more and more robust. But, of course, it is dubious whether the marginal 0.00000001 reduction in variance is worth the computational time. Your question therefore, as I understand, is whether you can somehow calculate or estimate the amount of decision trees to reduce the error variance to below a certain threshold. I very much doubt it. We do not have clear answers for many broad questions in data mining, much less specific questions like that. As Leo Breiman (the author of random forests) wrote, there are two cultures in statistical modeling, and random forests is the type of model that he says has few assumptions, but is also very data-specific. That is why, he says, we cannot resort to hypothesis testing, we have to go with brute-force cross-validation. 

EDIT: @Emre said I might have misunderstood the question. Indeed, I thought the problem was slicing an already imported CSV, not importing the CSV file. will replace your strings by . If all you want is to remove that columns, then that is good enough. But if you want to keep them, you will have to convert them to integers or some such. I suggest using : 

Comment in parenthesis is added by myself. The work being cited is another thesis, and it indeed says that, for instance, 

is not used as much as because is much older and is shipped with r-cran. But has more training algorithms, including resilient backpropagation which is lacking even in packages like Tensorflow, and is much more robust to hyperparameter choices, and has more features overall. 

It's hard to give a very good answer to such a broad question. Model interpretability is a big topic, and usually depends on the model. Simpler models, such as logistic regressions, are easier to interpret than neural networks. It's easy to say things like "if I can increase feature $X_i$ by 1, then the odds of $y$ happening will increase by $\beta_i$". Likewise, individual decision trees are easier to interpret than random forests. Yet, some people try to interpret random forests by computing "feature importance", which can be computed several ways, one of which is the number of splits that include the feature relative to the number of samples it splits. You want a way to treat your model as a black box and be able to interpret any model? I can think of two ways: 

I decided turning my comment into an answer. If you want to go pro, use a framework such as scrapy. Personally, I find them overly-cumbersome and I have been successful using the following approach. I think your use case is simple enough for it to be of use to you as well. Assuming you are using Python3 as well, you can grab a webpage easily, and then get what you want using XPath notation. 

Usually, when talking about decision trees, were are referring to the C4.5 algorithm. That is, we build a binary decision tree, where at each node we need to make a decision where we split the data using the rule "variable < value" (if the variable is categorical, you can have an = sign). How do know what the best split is? Simple, we test all possible combinations of variable and of value, and then choose the split which splits the data in the most egualitarian fashion (closest to 50-50). How do we know if the data is being well split? We try to minimize metrics such as entropy or maximize metrics such as the Gini coefficient. You want to reduce the original entropy down to zero. When do you stop? You either stop when you have a single sample in a node or when all samples have the same value. Alternatively, you can define a maximum depth for the decision tree (this is known as pre-pruning) or try to reduce your decision tree afterwards (this is known as post-pruning). Why are decision trees so fast to create if they test all combinations of variables and values? Well, I lied to you. They don't test all combinations. The metrics used (entropy or gini) can be computed incrementally, so what the algorithms do is to sort the values for each variable, and then incrementally see whether including one more sample improves or reduces the score. But this is a technicality you need not be aware of. Anything more? You should keep in mind that decision trees do one-ahead optimization. They do not find the global best decision tree. They are myopic. Therefore, if you think two variables are correlated, you should feature engineer a new variable that uses both. My decision tree is highly variable? Decision trees are know to be very different if the same changes a little bit. This is why people build ensembles of decision trees like random forests by resampling the data to make them stronger. You do lose interpretability: you can no longer draw a decision tree, if you use many of them. See this question for more information on this. Furthermore, I recommend these resources: