Soundbite answer: DNA computing does not provide a magic wand to solve NP-complete problems, even though some respected researchers in the 1990s thought for a time it might. The inaugural DNA computing experiment was performed in a laboratory headed by the renowned number theorist Len Adleman. Adleman solved a small Traveling Salesman Problem -- a well-known NP-complete problem, and he and others thought for a while the method might scale up. Adleman describes his approach in this short video, which I find fascinating. The problem they encountered was that to solve a TSP problem of modest size, they would need more DNA than the size of the Earth. They had figured out a way to save time by increasing the amount of work done in parallel, but this did not mean the TSP problem required less than exponential resources to solve. They had only shifted the exponential cost from amount-of-time to amount-of-physical material. (There's an added question: if you require an exponential amount of machinery to solve a problem, do you automatically require an exponential amount of time, or at least preprocessing, to build the machinery in the first place? I'll leave that issue to one side, though.) This general problem -- reducing the time a computation requires at the expense of some other resource -- has shown up many times in biologically-inspired models of computing. The Wikipedia page on membrane computing (an abstraction of a biological cell) says that a certain type of membrane system is able to solve NP-complete problems in polynomial time. This works because that system allows for the creation of exponentially-many subobjects inside an overall membrane, in polynomial time. Well... how does an exponential amount of raw material arrive from the outside world an enter through a membrane with constant surface area? Answer: it's not considered. They're not paying for a resource that the computation would otherwise require. Finally, to respond to Anthony Labarre, who linked to a paper showing AHNEPs can solve NP-complete problems in polynomial time. There's even a paper out showing AHNEPs can solve 3SAT in linear time. AHNEP = Accepting Hybrid Network of Evolutionary Processors. An evolutionary processor is a model inspired by DNA, whose core has a string that at each step can be changed by substitution, deletion, or (importantly) insertion. Further, an arbitrarily large number of strings is available at every node, and at each communication step, all nodes send all their correct strings to all attached nodes. So without time cost, it's possible to transfer exponential amounts of information, and because of the insertion rule, individual strings can become ever larger over the course of the computation, so it's a double whammy. If you are interested in recent work in biocomputation, by researchers who focus on computations that are real-world practical, I can offer this book review I recently wrote for SIGACT News, which touches briefly on multiple areas. 

This should be a comment to Neel's answer, but it's a bit long. Prompted by a hint from Rasmus Petersen, I found the following in MÃ¸gelberg's thesis (emphasize mine): 

To solve the question asked here, make the vertices be employees and draw an arc $x\to y$ of unit cost when $x$ would like the job of $y$. Note that employees are now vertices rather than edges. The nice thing is that an employee can say "I really want the job of $y$, but that of $z$ would do too". Solution: 

Background. I bumped into this conjecture while trying to prove a lower bound for a problem discussed in a program analysis paper, [Yang, Grigore, Abstraction Refinement Guided by a Learnt Probabilistic Model, 2016]. 

C.A.R. Hoare, An Axiomatic Basis for Computer Programming. From the abstract: In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. It has six pages that are quite easy to follow. 

Most automatic theorem provers handle EUF in some form. In particular, provers based on the Nelson-Oppen architecture do so. For big systems, however, EUF is usually not enough. One needs specialized decision procedures for arithmetic, arrays, bit vectors, and so on to get better performance. Take a look at SMT solvers. If you follow the links to solvers and where they are used you'll find many applications to real world problems. For example, Z3 is used in VCC, which in turn is used to fully verify Microsoft's (baby) hardware simulator Hyper-V. 

This is not an answer, but too long for a comment. I'm trying to explain why the question, as posed, may be hard to understand. There are two ways to define computational complexity for a device X. The first and most natural way is intrinsic. One needs to say how the device X uses the input, so that we may later look at how the size n of the input affects the run time of the device. One also needs to say what counts as an operation (or step). Then we simply let the device run on the input and count operations. The second is extrinsic. We define computational complexity for another device Y and then we program Y to act as a simulator for X. Since there may be multiple ways for Y to simulate X, we need to add that we are supposed to use the best one. Let me say the same with other words: We say that X takes $O(f(n))$ time on an input of size n if there exists a simulator of X implemented on machine Y that takes $f(n)$ time. For example, an intrinsic definition for NFA says that it takes n steps to process a string of length n; an extrinsic definition that uses a RAM machine as device Y says that the best known upper bound is probably what David Eppstein answered. (Otherwise it would be strange that (1) the best practical implementation pointed in the other answer does not use the better alternative and (2) no one here indicated a better alternative.) Note also that strictly speaking your device X is the regular expression, but since the NFA has the same size it is safe to take it as being the device X you are looking at. Now, when you use the second kind of definition it makes little sense to ask how restricting the features of device X affects the running time. It does however make sense to ask how restricting the features of device Y affects the running time. Obviously, allowing more powerful machines Y might allow us to simulate X faster. So, if we assume one of the most powerful machines that could be implemented (this rules out nondeterministic machines, for example) and come up with a lower bound $\Omega(f(n))$, then we know that no less powerful machine could do better. So, in a sense, the best answer you could hope for is a proof in something like the cell probe model that simulating an NFA needs a certain amount of time. (Note that if you take into account the conversion NFA to DFA you need time to write down the big DFA, so memory isn't the only issue there.) 

Gurevich in formulating the conjecture about a logic that could capture $\mathsf{P}$ requires the logic to be computable in two ways: (1) the set of sentences legally obtainable from the vocabulary $\sigma$ has to be computable, given $\sigma$; and (2) the satisfiability relation needs to be computable from $\sigma$, i.e., ordered pairs consisting of a finite structure $M$ and a sentence $\varphi$ such that all models isomorphic to $M$ satisfy $\varphi$. Also, significantly for comparison with this randomized logic result, the vocabulary $\sigma$ has to be finite. (A vocabulary is a set of constant symbols and relation symbols, for example, equals sign, less-than sign, $R_1,R_2,\ldots$) This is a paraphrase of Definition 1.14 of this paper by Gurevich, which is reference [9] in the quote Kaveh gave. The paper about BPP and randomized logic presents a significantly different framework. It starts with a finite vocabulary $\sigma$, and then considers a probability space of all vocabularies that extend $\sigma$ with some disjoint vocabulary $\rho$. So a formula is satisfiable in the new randomized logic if it is satisfiable in "enough" logics based on extensions of $\sigma$ by different $\rho$. This is my butchering of Definition 1 in the Eickmeyer-Grohe paper linked to by Robin Kothari. In particular, the vocabulary is not finite (well, each vocabulary is, but we have to consider infinitely many distinct vocabularies), the set of sentences of this logic is undecidable, and the notion of satisfiability is different from the one put forth by Gurevich. 

Suppose we have an orthogonal polygon with holes (all walls are axis-parallel). All vertices can be on integer coordinates, if that helps. Partition the polygon into rectangular rooms. I would like to find the best room to start from, to visit all the rooms (rectangles). There's a limitation on my movement: in any room, I can only leave by two directions, say north and west. (Here best means there would only be one source in the plane dual graph with directed edges showing how to walk from room to room. If more than one source is required, I wish to minimize them.) I have been looking at art gallery problems, and at VLSI papers on building rectilinear floorplans from network flows, and they are all tantalizingly close but far. Can anyone provide suggestions so I can focus my search/proof construction? EDIT to fix problem pointed out by Peter Taylor. I can choose two directions per room. (probably they need to be adjacent, so NE is ok but NS is not.) If I enter one room northward, I am automatically choosing South as one of thst new room's directions. (so only two in or out directions per room) If I choose a direction, and there are multiple rooms adjacent in that direction, I can enter all of them (and all of them then have the reverse direction assigned as one of their two directions), so the naive greedy approach would be to choose the direction that maximizes the number of rooms I can enter at that stage. I hope this is now complete, and understandable. 

This works in $O(m+\lg n)$. Compare with "traverse the tree and insert in the result", which takes $O(n+m\lg m)$. Update, in response to the request for $o(m)$ auxiliary space: I believe you need $\Omega(\lg m)$ auxiliary space to build an almost complete binary tree given that you are told the size in advance and then you receive the elements in-order one-by-one. Draw the complete final tree and imagine you received the first $k$ elements. These elements are separated from the others by a vertical line that cuts some edges. Since the left extremities of those edges need to be connected later, you need to remember them. In the worst case the vertical line cuts a zig-zag with $\lg m$ edges. I didn't work out the details, so I'm not sure how the extra bookkeeping affects the running time. Second update: JeffE's comment below says how to do it with $O(1)$ auxiliary space and within the same time bounds. (For mutable trees, at least.) That means that my waving hands argument above about $\Omega(\lg m)$ space is wrong. 

Since I was myself somewhat confused, I begin by clarifying a few concepts in the question. Collection. I see no reason to spend time rigorously defining what "collection" means when we can simply ask what happens for data structures in general. A data structure occupies a piece of memory and has some operations that may access that memory and that may be invoked by users. These users may be distinct processors or just different threads, it does not concern us. All that matters is that they may execute operations in parallel. Lock-free. Herlihy and Boss say that a data structure is lock-free when a crashing user does not prevent further uses of the data structure. For example, imagine one pours water on a processor that is in the midst of inserting a node in a sorted set. Well, if other processors try later to insert into that sorted set, they should succeed. (Edit: According to this definition, it is the case that if a data structure uses locks then it is not lock-free, but it is not the case that if a data structure does not use locks then it is lock-free.) With these definition, I think Herlihy and Boss basically say that the answer is to turn critical regions into transactions. But, you may ask, does this have the same complexity? I'm not sure the question makes sense. Consider . Is this a constant time operation? If you ignore the locking operation and hence other users then you can answer YES. If you do not wish to ignore other users, then there really is no way to say whether push will run in constant time. If you go one level up and see how the stack is used by some particular algorithm, then you might be able to say that push will always take constant time (measured now in terms of whatever happens to be the input of your parallel algorithm). But that really is a property of your algorithm, so it doesn't make sense to say that push is a constant time operation. In summary, if you ignore how much a user executing an operation waits for other users, then using transactions instead of critical regions answers your question affirmatively. If you don't ignore the waiting time, then you need to look at how the data structure is used.