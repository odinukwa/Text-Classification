I would do this: Table: Customers Columns: CustomerId Table: ChangeSets Columns: ChangeSetId | Comment | Committer Table: CustomerVersions Columns: CustomerId | ChangeSetId | AllOtherCustomerData We typically need two kinds of FK references, both a more permanent one to Customers (this is the employee's laptop/iPhone etc) and a more volatile one to CustomerVersions (this is the employee's current position, location, etc.) 

** The constraint CHK_IntegerSettings_PreviousFinishedAt_NotAfter_StartedAt guarantees exactly that. See for yourself: 

This works for me. However, if I want to keep the results on my screen, I have to keep the transaction open. When I execute COMMIT, my result set is discarded. When I execute both FETCH and COMMIT at the same time, the first result set is discarded. Is there a way to commit the transaction but keep the result set? The version of PgAdmin is 1.18.1. 

This is a very simple solution - it took me 10 minutes to develop. A recent college graduate can come up with it. On the database side, execution plan is a trivial merge join which uses very little CPU and memory. Edit: to be realistic, I am running client and server on separate boxes. 

As such, at the time of the original release of SQL 2008, I decided against using MERGE. I am using MERGE a lot now, on 2008 R2, and I think it is a really great feature. Edit: here is the list of defects in SQL 2012 that were recently fixed. Hope it helps. Another edit: I have chosen MERGE for more detailed analysis, because it is a very important improvement. In fact, it is a major step in catching up with Oracle, and it does improve our productivity. As such, MERGE has been marketed a lot at the time of SQL 2008 release. Yet it was not completely ready to use in serious production systems when it was originally released, and there was no easy way to know it from the presentations/articles/blog posts and such. Similarly, snapshot isolation is an awesome new feature which just works, but invoking scalar UDFs in CHECK constraints does not work in all cases and as such should not be used in production when we need data integrity. However, both new features were recommended in "What is new in SQL xxxx" presentations, as well as in books, articles etc, and with similar enthusiasm. We need to be very careful with new features - not all of them are going to be useful/reliable/performant. 

Of course, this query is only correct if the data in Events table is valid. We can use constraints to ensure 100% data integrity. I can explain how if you are interested. Another alternative is to just load your raw data, your periods, into a client application - your problem is absolutely trivial in C++/C#/Java. Yet another approach is to use an RDBMS with fast cursors such as Oracle - that will allow you to just write a simple cursor and enjoy good performance, but still not always as good as my first solution. 

I would not bother with persisted columns at all. I would just define a computed column and have it recalculated every time we select, so that I save considerable storage and have my selects do less reads, at the expense of the negligible amount of CPU needed to calculate volume. Of course, I did some benchmarks in the past. 

We can use trusted constraints to make sure each PrescriptionId covers a range of dates without gaps and overlaps, so that one interval stores as one unbroken sequence of dates. Note: I know you are using DISTINCT in your first subquery, so you are assuming that one person can take one drug on one day from more than one prescription. I am not mkaing this assumption, for simplicity. Are you sure it is a correct assumption? If yes, we will have to change the design. Once we have this table, we can essentially materialize your first subquery as an indexed view: 

I know I am late for this party, but I would index exactly the expressions used for locating rows, such as substring(col3,10,1). If the whole col3 is ever used, I would index CHECKSUM(col3) (understanding that there could be collisions of course). 

Currently I am running 2008 R2, but 2012 solutions are very welcome. I have posted my C# solution as an answer. 

The following documentation describes how to see the refcursor returned from a function, here, like this: 

I am not sure if "the highest customer in the chain" for a leaf node means closest to the leaf or closest to the root. If you meant "closest to the leaf", you may want to denormalize and have a FK make sure your denormalized data is always correct: 

We have used this approach many times to change large live production tables without downtime, with no issues at all. 

You can create a covering index on the following columns (message_type_id, hidden, message_id). Try out different order of columns in the index: that might make some difference. I would rather not use a filtered index: it is quite likely that the optimizer would fail to use it. 

I had a similar problem in the past. I needed to send up to 100K numbers to SQL Server 2005 as fast as possible. After lots of benchmarks I think the fastest approach in my environment was to pack numbers in binary format. That needed less network packets and parsed very fast using very little CPU. Erland Sommarskog included the C# code I used to pack numbers into a CLOB in his article Arrays and Lists in SQL Server 2005 , in the section entitled "Passing Numbers as Binary". 

We've been doing this for almost five years, and we think that explicitly testing modifications is definitely doable, but it is quite slow. Besides, we cannot easily run such tests concurrently from several connections, unless we use separate databases. Instead, we should test modfications implicitly - we use them to build up at least some of the test data, and verify that our selects return expected results. I've written an article entitled Close Those Loopholes: Lessons learned from Unit Testing T-SQL, as well as some blog posts Regarding your question "Is there a treshold in complexity where it gets completely hopeless?", complex modules need tests much more than simple ones. To simplify maintenance, we generate expected results, and we store them in separate files - that makes a huge difference. 

The answer can be platform-specific. Although the results are the same, your performance might be very different. I would suggest another approach, because it seems to clearly document the following intent: calculate some aggregates, then decorate them with another field(s). Here is an example: 

I have a 1.2Tb SQL Server 2008 R2 database which is more than 50% empty, because I migrated a lot of functionality to PostgreSql. The database is still in use, and still somewhat growing, but I do not think it will grow to use all 1.2Tb in the next 3-5 years. At the current rate of growth, we are not going to use up 1.2Tb in more than 14 years. More to the point, we keep migrating functionality to PostgreSql whenever we need to make serious changes. In a very dynamic agile environment, this means we frequently migrate data away out of this SQL Server database. So, this database is likely to grow even slower in the future: it is being phased out, eventually. As such, we'd rather use this unused empty space for other databases and such - moist likely it is not going to be needed for growing tables/indexes ever. I recall that the rule of thumb used to be "never shrink databases". Does it apply in this case? Currently the server is running 2008 R2 EE, but we plan to upgrade to 2012 very soon. I think because this shrink functionality exists, there should be valid use cases where it makes sense to shrink. Otherwise why does it exist at all? We can afford a few hours of downtime during a weekend. Edit: the problem we are solving is as I said above: "we'd rather use this unused empty space for other databases". Also we'd rather prefer a solution that does not require investing too much time in learning the technology we are migrating out of. 

** The constraint UNQ_IntegerSettings_SettingID_PreviousFinishedAt ensures exactly that. The first interval does not have a previous one, which means that PreviousFinishedAt IS NULL. The UNIQUE constraint guarantees that there can be only one such row per setting. See for yourself: 

This business rule can be enforced in the model using only constraints. The following table should solve your problem. Use it instead of your view: 

The following is just a few examples regarding "actual evidence for or against reliability in the first version of any new release", as requested. This is not meant to be a complete analysis, but rather a suggestion on what you might want to research. You can google up "List of issues that are fixed by SQL Server 2008 Service Pack 1" and "List of issues that are fixed by SQL Server 2008 Service Pack 3" on MSDN website. Compare the number and severity of issues in both lists. IMO the first list is longer, and it has more items that could ruin my day, such as: 

This is not a complete answer, I do not have the time now, so let me just share a few thoughts. The complete answer would be huge, and I am not sure you want to know the details. I have been working with various temporal queries for several years already, and learned a lot in the process. As such, I would rather not have to optimize your query in my production system. I would try very hard to avoid solving it with T-SQL. It is a complex problem. Itzik Ben-Gan has written about "gaps and islands" several times, including a chapter in his latest book on OLAP functions. Your problem is a variation of gaps and islands. First, I would consider reading all the data to the client and solve it there using loops. I know, it requires sending data over the network, but fast loops in Java/C++/C# work very well for me most of the time. For instance, once I was struggling with a query involving time series and temporal data. When I moved most of the logic to the client, the C# solution was several times shorter and it ran 20,000 times faster. That's not a typo - twenty thousand times faster. There is another problem with solving such problems in T-SQL - your performance may be unstable. If a query is complex, all of a sudden the optimizer can choose another plan and it will run many times slower, and we have to optimize it again. Alternatively, I would consider storing data differently. Right now I see two possible approaches. First, instead of storing intervals we could use this table: 

This is an answer to Kirk's question 'why not use it (HierarchyId)'. As compared to materialized path, in some important cases HierarchyId seems to be both less performant and less convenient to work with. The reason is simple: quoting from Microsoft comment on Connect, "The problem is that CLR calls, including hierarchyID's methods, are opaque to the query optimizer. This is by design. However, it means that the cardinality estimate for them can sometimes be quite wrong." On the other hand, implementing materialized path is very easy the first time we need to do it, and next time it is essentially a copy-and-paste task. So, we get a more versatile and better performing solution with very little effort. So I completely agree with Paul Nielsen, who wrote in his excellent book entitled "Microsoft® SQL Server® 2008 Bible" as follows: "The new HierarchyID is not without controversy. It’s new and gets plenty of press and demo time, but I’m not sure it’s a problem that needed another solution." 

The language keeps evolving. A few decades ago the literate people used "indices" instead of simpler "indexes". As we switched to "indexes", we eliminated an unnecessary complication and made the language more useful. The need to memorize a plural for "index" was pure overhead - it did not in any way help us communicate. Make no mistake, there used to be grammar Nazis who enjoyed correcting those who switched to "indexes". Of course, grammar Nazis lost. This is how Occam's razor eliminates useless details if the whole thing stays relevant long enough. So let us take it easy - knowing the difference between rows and records adds absolutely nothing to our ability to develop and maintain databases. Many excellent professionals use rows and records interchangeably, yet develop awesome systems. As such, Occam's razor should eventually eliminate the distinction, and the next generation will have to learn one less useless fact. If, of course, SQL is still relevant at that time. 

I would actually verify how it scales up: set up a test environment, expose to to the workload you are expecting, and see what happens. That done, describing your tests would be easy. If you skip this essential step, your write-up might look good on paper and be completely useless. 

I think the best approach for you would be to actually expose your module to high concurrency and see for yourself. Sometimes UPDLOCK alone is enough, and there is no need for HOLDLOCK. Sometimes sp_getapplock works out very well. I would not make any blanket statement here - sometimes adding one more index, trigger, or indexed view changes the outcome. We need to stress test code and see for ourselves on case by case basis. I have written several examples of stress testing here Edit: for better knowledge of the internals, you can read Kalen Delaney's books. However, books may get out of sync just like any other documentation. Besides, there are too many combinations to consider: six isolation levels, many types of locks, clustered/nonclustered indexes and who knows what else. That is a lot of combinations. On top of that, SQL Server is closed source, so we cannot download source code, debug it and such - that would be the ultimate source of knowledge. Anything else may be incomplete or outdated after the next release or service pack. So, you should not decide what works for your system without your own stress testing. Whatever you have read, it can help you in understanding what is going on, but you have to prove that the advice you have read works for you. I don't think anybody can do it for you. 

Because update trigger is invoked once per row being updated, it is not invoked at all if no rows are going to be updated. See for yourself: 

Here is your description of the problem: "The CTE portion of the query returns almost instantly, only when we start joining to the clients does the performance issue appear". One possible, and likely, explanation is as follows: the optimizer fails to estimate the cardinality of your subtree, and chooses an inefficient plan. With your way of storing hierarchies, this is no surprise. How would you yourself estimate the size of a subtree without actually retrieving it? Can you use materialized path? Getting a subtree using materialized path is essentially one range scan, fast and simple, and the optimizer can have a good cardinality estimate off the statistics on one index. In my experience, your way of storing/reading hierarchies does not scale up. I have never been able to make it work fast and use resources efficiently. 

Be careful: when we turn on READ_COMMITTED_SNAPSHOT, we can break existing code. I wrote a few examples here: When Snapshot Isolation Helps and When It Hurts 

This is doable, but inconvenient. Are there other ways to return dynamic column lists? Edit to protect against SQL injection, I typically split the comma-separated list and join it against a system view. Anything that is not an actual column name is not returned. I did not mention that originally, just to keep the question short. 

In another tab, COUNT() should always return a multiple of 10, but the select at the bottom returns all COUNT() that are not multiples of 10: 

I have not yet seen a single project that used extended properties. IMO the reason is this: even if we want to store documentation in the database, which is usually not the case, there are alternatives. Usually extended properties do not do exactly what we want. On the other hand, rolling out our own solution that does exactly what we need is so easy, so why bother? 

I regularly do such refactorings, without shutting the system down, and without stopping modifications against the table being refactored. I am moving data in small batches. Copied from my blog: Refactoring large live OLTP tables without downtime Refactoring tables does not have to be such a big deal. We do not have to shut modifications down, migrate all data into new structure, deploy modified modules, and do it all at once, while the system is down. Doing all migration at once may be risky, and if we make a mistake, there is no easy rollback to the old version. This is why whenever we need to refactor a large table, we are using an alternative, low-risk, no-downtime, incremental approach. The method I am going to describe has been used in practice several times, and we did not have any problems with it. All our transitions from old table structure to the new one were smooth and incremental. More to the point, we were able to set aside the migration at any time and switch to some other more important task, or leave for the day, or enjoy the weekend, all while the database was fully functional. Typical refactoring scenario I am totally making this up, I have never worked on bug tracking systems. Suppose that we are storing tickets in a table dbo.Tickets, which, among other columns, has a column named AssignedTo. As such, each ticket can be assigned to only one person at any time. However, the next version of our system will allow to assign a ticket to more than one person. Our plan is to create another table, dbo.TicketAssignments(TicketID, AssignedTo), migrate existing data to it, and change appr. 50 stored procedures affected by this change. To minimize risks, we are going to finish all database changes at least a few days before the new version of our system is released. This means that the current version of our system is going to run against our refactored database exactly as it did against the old one. BTW, the table dbo.Tickets is quite large, and is heavily used all the time. Before the migration. Prerequisites. We shall need some extra disk space, approximately as much as the old table uses up. Besides, we need a server that is not struggling with its current workload, so that it can withstand some additional work for the duration of our migration. Also we shall need a good test coverage on all modules using the table, including unit tests, stress tests, and performance baselines. Typically in our system we already have solid test coverage. One more thing: because table structure is going to change, inserts directly into the table are not going to work any more. As a result, all test data should be populated via stored procedures that continue to work against the new table structure. For example, we might use a stored procedure dbo.SaveTicket that has a parameter @AssingedTo to populate test data for our unit tests. Creating new tables, changing modifications. As our first step, we create two new empty tables: dbo.TicketsV2, which has all the same columns as dbo.Tickets, except it does not have AssignedTo column; dbo.TicketAssignments(TicketID, AssignedTo) We also change all the procedures which modify dbo.Tickets, so that they write to both old and new tables. This biggest risk in this step is introducing concurrency-related problems. We need to stress test our modifications well. If we have any problems, however, we can just run a rollback script, changing all the modifications back to their original version. Such rollback takes just a split second. In fact, we never actually had any problems at this step, because our stress testing harness is quite solid. Of course, our modifications get slower, but we have made sure that our hardware can handle it. We had not had actual problems with slow modifications either. Migrating existing data At this stage all the reports are running off the old table. As all new changes get saved into both old and new tables, we are also moving over all existing data to the new structure. We want this migration to be non-intrusive, so we typically just run one WHILE loop, moving over like 1K-10K rows at a time, so that our migration does not hinder OLTP activity and reports. Modifying the reports. While the data is migrating, we can take our time changing our stored procedures to read from new tables. Because we have good unit testing coverage, we can refactor procedures with confidence - if we break something, we shall know it right away. Because not all data has been migrated yet, we do not deploy the modified procedures. Verifying that migration completed. To verify that all data migrated correctly, we need to write a SQL query. To my best knowledge, there is no GUI tool that can efficiently compare large tables. Writing a query, however, is not that difficult. This query is going to use a lot of resources. We need to be careful not to bring the server to its knees while the query is running. There are several ways to accomplish that. Deploying modified reports. Once we have complete and correct data in the new tables, we can start deploying new procedures. We do not have to deploy them all at once - we can deploy them five or ten modules at a time, even if some other procedures still access the old table. There is one more problem our team need to be very careful with - our test server is not exactly identical to our production one. As such, we encounter a risk that our procedure runs fast in test environment, but is slow in production. This is why we first deploy our changed procedures into a different schema, which (schema) is not exposed to our user. For example, instead of altering procedure dbo.GetTicketsForDay, we create a new procedure Internal.GetTicketsForDay. Only developers have privileges on Internal schema, so users cannot execute it yet. Once we have executed Internal.GetTicketsForDay in production environment and are happy with performance, we can deploy it as dbo.GetTicketsForDay. Our only risk at this stage is that we can deploy poorly performing procedures. Our rollback strategy is simple - we just roll back to the original stored procedures that read from the old table. Finalizing the migration. Once all the reports access the new table, we can change our modifications, so that they no longer write to the old table. The old table can be archived out and dropped, reclaiming the additional storage which we needed for the migration. Conclusion As we have seen, we can refactor an OLTP table without downtime and with low risks, even if it is big and data migration takes a lot of time. One more thing: a common reaction to such Agile war stories is a recommendation to "do it right the first time". It is so common that I would like to address it right now. Of course, at the time when the previous version of bug tracking system, it was quite obvious that eventually we might need to add the ability to assign a ticket to more than one person. Of course, if we had the table dbo.TicketAssignments from the very beginning, we would not have to go through this complex refactoring. In other words, we should "do it right the first time", should we not? In general, "doing it right the first time", developing a flexible database structure that we should not have to change later, makes a lot of practical sense, but not under all circumstances. More specifically, when the previous version of bug tracking system was being designed and developed, there were hundreds of brilliant ideas, lots of great features that might be useful later on. Implementing all these hundreds of brilliant ideas would take years, and a much bigger team. It was just not possible. More to the point, it was not what our customers wanted - they wanted us to take care of their most important problems first, and quickly. They did not want to wait until we could provide more features. Instead of trying to add as many features as possible and as a result delivering a half baked low quality product, the team concentrated on developing only must-have features, on performance, and on very high quality. As a result, the product had a very limited set of features, but it delivered everything it promised, and it was simple, very fast, and rock solid. As such, it was a success. It would be quite easy to provide examples of over-engineered projects that tried to "do it right the first time" and miserably failed, but this is beyond the scope of this post...