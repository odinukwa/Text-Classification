means that you'll only see rows where there are matching records in A and B. If you want all the rows in A and matching records in B, you could change to . Conversely, if you want all the records from B and only the matching ones from A, use . Finally, if you need everything from both tables, matching or not, you can use . 

The query does not break ACID compliance. The rows/pages/table (depending on your database) are locked for the duration of the execution (or actually, until the outer-most statement). It's: 

Note: will be deprecated and replaced by where n is the number of decimals of the seconds. See also: CAST and CONVERT (Transact-SQL) 

.. tells SQL Server to always choose the index , which can absolutely kill your query if the index doesn't properly cover all the data you want to retrieve, including the columns , , as well as the huge column list in the SELECT clause and the columns in ORDER BY. Right off the bat, I would try creating indexes like these to try to resolve the problem. 

As other answers have noted, overloading is not supported for SQL Server stored procedures. One workaround could be having your stored procedure accept an xml variable as parameter, so your application could send a "set" of arguments in an xml blob: 

is a server-level role that allows its members to create, alter, drop, and restore databases on the instance. Intuitively - and without having seen your dacpac - I would grant the executing user in question membership in the database role, so it can view all objects, as well as , so it can create, alter and drop objects in the core schema as needed. From there on, I would monitor what types of permission errors you encounter running the dacpac. Judging from your question, I'm guessing that it wants to alter or drop/recreate the database - if that's the case, see if you can fix that when you're building the dacpac. 

If you're on SQL Server 2012 or 2014, this is an aggregate that you can calculate with an ordered window function. 

... and the plan looks a tiny bit better as well (look for the red at the bottom of the original plan, that is now gone.) 

Without touching on the obvious hardware possibilities and HA solutions, I would consider building a "staging table" which is minimally indexed or even a heap, where you could offload incoming transactions with maximum performance. A scheduled/recurring process could then asynchronously move that data into the main fact table, which could have indexes that are more suitable for reporting. The same process could also maintain aggregates in another table, so you could build reports directly on those aggregates. The key is asynchronous, so I wouldn't use triggers or indexed views, but rather something like a SQL Server Agent job that runs a stored procedure over and over. Pros: 

This is a "kitchen sink" query, for which SQL Server MVP Aaron Bertrand has a good video on how to optimize using dynamic SQL. A few points to get you started on the performance of your query: 

I have intentionally not pivoted the results because the PIVOT syntax needs you to hard-code the names of the columns, which in this case are dynamically defined in the UnitType table. While we're at it, you may wish to consider a few revisions to your schema: 

Now all we have to do is calculate the number of days between weekStartDate and weekEndDate for each row. Divide this number of days by the total number of days for the Actual (=12), and you get a distribution key: 

Yes, placing a clustered index like you suggest will give you excellent query performance for those two particular types of queries, but it may spell disaster for most other SELECT queries on that table. I would offer an alternative solution: I would suggest adding a basic clustered index on a single, unique column like : 

Based on your question, I would index the column with the clustered index. And to make the index unique, just make sure to include the identity column in the index definition: 

A dynamic management view, like the name implies, isn't a table with indexes and statistics, but rather a view which could potentially use a large number of system tables. To generally speed up complex DMV queries, you could try dumping the contents of some of the larger DMVs into a temp table or table variable and then use those. You can control the indexing on the temp tables, and it also reduces the number of join operations that the server has to perform. For the same reason, if your solutions allows for it, consider using in order to minimize locking on those system tables, which could otherwise affect your entire database or server. 

If it's any use to you, I've written a script that compiles all of the permissions in a database. It's available on the Downloads page of my blog, and it's completely "as-is", without any warranty or guarantee. 

All of this serves to simplify the understanding of, and accessibility to, the business data for people who aren't too comfortable writing SQL queries or using pivot table tools. Some of the greatest pitfalls in data warehousing and BI involve users not understanding the structure or meaning of the data, producing incorrect results and subsequently not trusting the BI solution. 

Using like this returns one record for every unique occurrence of (cart, barcode) and the totals of counted_in and counted_out for each. 

To clear things up, language-wise: In SQL Server, the login is global for the SQL Server instance (a "server principal") and allows a client to log on to the server with very limited permissions. Once you've set up a login, you can assign users to this login. A user is the login's security principal in a specific database ("database principal"), which means that a login can have multiple users, one for each database where he holds permissions or role memberships. You might have added a login, but not the user, or you may not have assigned the user enough permissions in the database. To view the definition of database objects, your user will probably need , either on the database, the schema or on the individual object. These permissions are inherited. If you want to be able to read the data of tables, add permissions. Membership in the database role gives the user complete read-only access to all tables and views in the database. I've recently written a series of blog posts on SQL Server security that might give you a little more details. 

The common table expression returns a list of TicketID, CreatedMoment and MAX(StatusChangeMoment). Using this result set, we can apply a second aggregate in the SELECT statement below. What I'm doing there is a conditional SUM(). I'm adding 1 for each row that meets the criteria and 0 for each row that doesn't. NB: I've simplified your criteria a bit, I hope it's equivalent to what you're looking for. 

If you want to see the number or purchases per buyer and date, you would add the column to the SELECT as well as the GROUP BY: 

Which solution is best for you is dependent primarily on how much data you have. Try the different solutions. 

Because I've put the two conditions in a single clause, the two queries only scan the c_contact table once, making them much more efficient. In particular, an optimal index for this type of query would probably look something like this: 

Check the DMV, which contains all granted and denied permissions. The will point to the in , the points to the in (if applicable). The and columns point to the granted and granting user ID respectively, which you can find in . Remember that permissions can be assigned not just to users, but also to server or database roles, which the user can inherit. Also, granted and denied permissions can conflict, in which case the more restrictive of the two rules applies. I wrote a blog series a while back, which goes into more details on permissions. 

Ridicule and disdain aside, the following query would return all possible permutations of your integers. 

They're two slightly different animals that can be used for the same purposes, as in your example. CROSS APPLY is your only option for "joining" table value functions and "expanding" xml documents, though. Some queries, particularly parallel queries, can exhibit vastly improved performance using CROSS APPLY, provided you have the requisite processor threads and indexing strategy. Microsoft MVP Itzik Ben-Gan elaborates a couple of great examples in this talk 

To put things very simply, if the column is in the SELECT clause, it has to either appear in the GROUP BY or inside an aggregate function. In your case, you're probably looking for this: 

Normally, I would recommend putting tempdb on its own disk(s) and size the database from the start, so you'll avoid the autogrow thing entirely. Autogrowing by 10 percent is often really bad practice for two reasons: 

Look for files on your drive called (database files) and (log files). The filenames don't neccessarily have to be the same as the name of the database. At a very minimum, there will be one of each type for any given database, but it's quite common to have multiple database files, often spread over different volumes. Move those files to their intended locations, then attach them by right-clicking "Databases" and choosing "Attach" and adding all the files. In my experience, adding the first file will often suggest the remaining files, but I don't know this for a fact. 

Even though swapping an inline function for a multi-statement function comes with its own set of advantages and drawbacks, an important difference in your case may be that the output is stored in a temp table with a clustered index before the join happens. If you've properly aligned the clustered indexes on the two functions, joining their output tables should be a speedy affair. Here's an idea of what kind of query plan you could expect (excluding the inner workings of the functions, obviously). 

So you can see that there are 3 rows with a=X and 2 rows with a=Y. The clause comes into play when you want an ordered window function, like a row number or a running total. 

The orders every row by size, then assigns a row number, starting at 1. This row number is assigned a "group" (the column) on a round-robin basis. First row is group 1, second row is group 2, then 3, the fourth gets group 0, and so on. 

The technical answer To my knowledge, there's no problem in using the same user for all "client" machines. The same user account can connect to a SQL Server as many times as the licensing of that SQL Server allows. The security perspective However, you may want to review the security aspects of this setup - do all the remote machines have the same access requirements, or should they have different users for security reasons? Could you perhaps even use windows/AD authentication? Remember that when you hard-code an account into the linked server, pretty much anyone who can log on to the remote server will gain the privileges on the "central" server that you've given to the user account defined on the linked server, which may lead to a privilege escalation. Tread carefully with linked servers and permissions, is all I'm saying. 

Like others have said before me, there is no way to restrict the use of built-in functions in SQL Server, and there really would be no point to it. Instead, good security practices dictate restricting access to data rather than programmatic functionality. In essence, if you can't use in SQL Server, you could extract the data and perform in Excel and achieve the same results. 

When you add a clustered index to an existing table, SQL Server needs space to build the new index structure, in essence the entire table. If space is an issue, you could create a new table with the clustered index, then migrate the data in chunks, drop the original table and rename the new one. It goes without saying that moving 1.4 TB if data like this is really only realistic if you're in recovery model, since you'd fill the log with that data otherwise. You could probably use partition switching to avoid renaming, as renames carry some risks. That would involve truncating the original table, adding the clustered index to it, and finally using . Note that this operation will interrupt users, and you'll also have to be mindful of foreign key constraints and other schemabound objects. However, creating a clustered index on a TB+ table will also block the table for a long time, so it's your call. I would probably just ask my SAN admin for more disk space if that's an option.