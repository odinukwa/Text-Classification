Depends on the volume efffect. Uniform volume effects that do not belong do scattering can be simulated by just calculating ray enter and ray exit distances. Otherwise you need to do integration of the ray path, also known as ray marching. To avoid needing to shoot secondary rays the raymarching is often coupled with some sort of cache, like depthmap, deepmaps, brick maps or voxel clouds for light shadowing, etc. This way you dont necceserily need to march the whole scene. Similar caching is often done to the volume procedural texture. It is also possible to convert the texture to surface primitives like boxes, spheres or planes that have some suitable soft edged texture. You can then use normal rendering techniques to solve the volumetric effect. The problem with this is that you usually need lots of primitives. Additionally the shape of the primitive may show up as too uniform sampling. 

What you see in the image called a UV map. That is, it is simply texture coordinates to be looked up encoded in a image. Same thing happens in all texture lookup in 3D there is a underlying sampler that picks where to pick texture color from. 

Image 3: The shape was derived using an old draftsman's trick. Please note: Most applications polygonize NURBS or Bézier surface for rendering. Also trivial root finding does not really work well as result should not be pixels but a curve. Let us to avoid this, as I am perfectly capable of discretisizing the solution and project the edges on the back face culling edge, and doing a secondary fitting, or even fitting on the NURBS underworld and then to 2d*. That is exactly the same solution as I am using now. Seem to me there should be a somewhat analytical transformation possible. 

Image 1: No ambient light (left) and ambient light (right). Both look artificial. We can approximate ambient light by a constant factor. But this looks slightly washed out as ambient light is not constant over the scene. To make the ambient look better we can use a trick. If we make a hemispherical probe for object coverage (occlusion), we get an estimate of how much the pixel lies inside a cavity. The less you live in cavity the more likely the ambient light is going to hit here. 

So in the case #2, the diffuse is left out, which is equivalent to a 0%, pure black, diffuse color. In case you haven't checked it already, this presentation is an excellent code explanation of SmallPT, dense but thorough: smallpt: Global Illumination in 99 lines of C++. 

From my understanding, the specular color usually refers to the amount of light that is reflected when the surface is lit at normal incidence, and is noted $F_0$ or $R_0$. Moreover, for non metal materials, this value is calculated from the index of refraction of the material $n$ with the formula deduced from the Fresnel equations (in which 1 is the index of refraction of air or void): $$F_0 = \frac{(n - 1)^2}{(n + 1)^2} $$ According to this list of refractive indices on Wikipedia: 

You cannot do something like that. is a 3x3 matrix: without a 4th column and a non-zero homogeneous coordinate, you can only transform directions, not positions. You need to first get the camera direction relative to the vertex, then transform that direction. 

Reduce shading when possible Lens distortion Part of the NVidia VRWorks SDK is a feature, Multi-Res Shading, that allow to reduce the amount of shading, to take into account the fact that some pixels contribute less to the final image due to lens distortion. In Alex Vlachos' GDC 2015 presentation, he also mentions the idea of rendering the image in two parts of different resolution, to achieve the same goal. Foveated rendering Finally, like your question mentions, foveated rendering aims at scaling the resolution to take into account the fact that eyes have more precision in the fovea and less in the periphery. There is some research on the topic, but this requires eye tracking like in the Fove HMD. 

Make sure you still take the paths that hit nothing into account when averaging the color of the pixel, otherwise this will also introduce a bias. As joojaa said in the comment, you should try using a simpler scene: a few spheres or cubes, no texture. You can also reduce the number of bounces to one or two to see only direct lighting. If necessary, maybe even use a constant sky dome first instead of a light area. 

I have no knowledge of the literature on the topic, but I did something very similar to what you're asking some time ago: I wanted to generate lathe meshes and bend them according to a spline. I think the same technique could be adapted to your case quite easily. First you would need to define what your default axis is: if the input mesh corresponds to the case when the spline curve is a straight line, where is that line in 3D? It could be defined arbitrarily as the Y axis, the PCA of the mesh, or some manually defined axis. The algorithm is then: 

Now a bump map is pretty hard to draw with a paint application. Because its hard to see even if you were correctly linear. So you end up using such things as looking at values. Now here is the catch: 

What app to choose depends on your exact needs*. For example do you need to retarget a existing animation to the new bone structure? Many of the game engines also suppprt FBX and possibly limited tweaking within their editor. Atleast Unreal and Unity support fbx. And if you do not try to do anything really invasive you might be able to use these tools. * using these apps is outside the scope of this forum. 

Image 2: Indexed bitmaps store a lookup array from where they take the actual color. Note that some formats may have pretty sophisticated transforms, or the data is compressed further by some more elaborate compression scheme. But even so a compression of $2/3$ and especially the indexed $1/6$ is pretty good for something that naturally tends to take up space. Off course if we compare with a ASCII pixmap those compression ratios are already quite significantly higher. There are many such formats that you could type in with a text editor if you must. However, since your dealing with individual bits its much easier to do this with a hex editor instead as the data is essentially binary. In practice it is easier to write a script to dump full values in the file, except perhaps in the 1 byte is one channel scenario where a hex editor works fine. 

Image 1 Scaled version of original JPEG picture at (1 048 198 bytes) and corrupted one (610 922 bytes) side by side. Original image by Maruf Mostafa image available here This is easy for you to test take an image and just simply delete some bytes out of it (like with a text editor). Some files are easy just open them, like the JPEG in question that just opens up in many editors without a hitch. PNG required some slight special handling to uncompress. It did't open in Photoshop or windows image preview. But was still relatively straightforward to open as the web browser happily opened even partial PNG files to the point it was defined to. 

Addendum To use depth or normals, you would need to save them in a texture if that's not done already. When you create the frame buffer for your regular rendering pass, you can attach various textures to it (see ) and write other information to them than just the color of the scene. If you attach a depth texture (with ), it will be used automatically for depth. If you attach one or several color textures (with ), you can write to them by declaring several outputs to your fragment shaders (this used to be done with ; either way, see ). For more information on the topic, look up "Multi Render Target" (MRT). 

To be able to give a good answer, we need to know what is the compiler error you are referring to. At first sight your shader looks ok, although: 

Back to the problem So we want to know the color of the points of an object, we know that to compute it we need to integrate light in all directions, and we have a cube map that represents light coming from all directions. So far so good. A problem though is integrating basically means we need to go over every single texel of the hemisphere (so half the environment map: $(64 \times 64 \times 6) / 2$ texels), do some math, and add that to the result. That's a lot of computation for each point, which we'd like to avoid if possible. We know that the contribution of one light depends on the light direction. If we consider the environment to be static (the lighting doesn't change), then we can isolate the part of the computation that only depends on the light and surface normal (and not on the material or the observer), pre-compute it and store it to use later. For the diffuse term, that's the diffuse irradiance mentioned earlier, and it typically looks like the center figure of the illustration. Each pixel represents the irradiance term for a given surface normal. The right figure is the specular environment map, computed in a similar way but with a different integral. 

This approach should give the expected result, but the problem with it is the first part, which is not trivial at all. Idea B: Approximate the mesh with fat particles, that are big enough to hide points in the background. 

Traditional rendering solutions do not do account for secondary light bounces (called indirect light). Even with strategically placed fill lights you still have areas where none of the direct light hits. Ambient light tries to solve this problem by shining by a constant amount in all directions. In practice this means that light position or surface normal has no meaning, one just adds some of the shaders color multiplied by ambient light color to the shading result. Ambient light has a tendency to look artificial when overused. But the opposite problem is that surfaces look like they are on outer space. Ambient light also makes the difference between dark materials and light materials more apparent. 

You can then use something like poisson disk sampling to reconstruct the image (maybe the didsk should be longer ovals the further away they are form the center line and less long as you progress further into the spiral). Now that I think of the problem a bit more. A simpler approach could be to map the image on mesh, using UV mapping, then unwrap the mesh. A triangular mesh might exhibit some artifacts so using a B-spline interpolation could be better. Several image mesh algorithms exist and you even have these in Photoshop. 

Image 2: a flat shaded surface (back), Smooth shaded (middle) and a mapped smooth normal. The illusion of a wavy surface breaks because the edge plays so prominent part in the image, you could instead increase the normals 2) does the smoothing at least increase the memory allocated Hard to say definitive things about the underlying graphics engine. The normals need to be emitted to the graphics card anyway, most likely this data is cached, but could be calculated on the fly (in both cases). Since Max uses smoothing groups it seems to me that memory usage is constant regardless. Hard to say, even if its not cached then it wouldn't make a big difference. It makes the shader tiny a bit more complicated, but only just most likely this complexity is present use it or not. 

That works, but you can simply avoid all of this at creation time because you understand that separate planes are working differently. So only the curved sides need normal direction merging. And in fact you can just directly calulate them from the underlying mathematical shape. 

I assume a similar rule will apply to the OpenGL equivalent, Uniform Buffer Objects, since they map to the same hardware feature. What about vanilla uniforms though? What are the rules that apply when declaring uniforms? 

The problem of reducing noise is still under active research though, and there is no silver bullet. At this point you will need to read what are the latest developments. 

At CEDEC and GDC in the early 2000s, Masaki Kawase has presented a series of fast post-processing based lens effects, including large bloom. This presentation from GDC 2003, Frame Buffer Postprocessing Effects in DOUBLE-S.T.E.A.L (Wreckless) (see slides 15 and upwards: "Bloom"), gives a first version of the bloom. It consists in doing a Gaussian blur by sampling at exponentially increasing distances. In this presentation from GDC 2004, Practical Implementation of High Dynamic Range Rendering (see slides 44 and upward: "Glare generation"), he updates the technique. Instead of varying the sampling distance, he uses downsized versions of the original image, using a carefully crafted equation to achieve a large yet spiky Gaussian blur. 

This being said, you're adding to yourself a lot of trouble by sticking with fixed pipeline instead of using GLSL. 

Another possibility is to do a random jitter (instead of the matrix based one above), but you then soon enter the realm of signal processing and it takes a lot of reading to know how to choose a good noise function. The idea remains the same though: consider the pixel to represent a tiny square area, and instead of shooting only one ray that passes through the center of the pixel, shoot many rays covering the entire pixel area. The more dense the ray distribution is, the better signal you get. P.S.: I wrote the code above on the fly, so I'd expect a few errors in it. It's only meant to show the basic idea. 

So you simply need to make sure texture coordinates generation is disabled for the color texture, but enabled for the shadow texture. As you guessed, this is done with , by writing for example: 

The well known Schlick approximation of the Fresnel coefficient gives the equation: $F=F_0+(1 - F_0)(1 - cos(\theta))^5$ And $cos(\theta)$ is equal to the dot product of the surface normal vector and the view vector. It is still unclear to me though if we should use the actual surface normal $N$ or the half vector $H$. Which should be used in a physically based BRDF and why? Moreover, as far as I understand the Fresnel coefficient gives the probability of a given ray to either get reflected or refracted. So I have trouble seeing why we can still use that formula in a BRDF, which is supposed to approximate the integral over all the hemisphere. This observation would tend to make me think this is where $H$ would come, but it is not obvious to me that the Fresnel of a representative normal is equivalent to integrating the Fresnel of all the actual normals.