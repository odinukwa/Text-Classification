You should choose mean or average over median. Let me explain why, specifically in your case since you're checking for expected computational time. Mean or average could be one of the three cases. 

The following image shows a scatter plot of my data. The Y axis points are the labels, labeled from 1 to 6 and X-axis are dimensionally reduced values of all my features. I reduced them for better visualization. 

The paragraph you mentioned explains a the parametric procedure of creating training data and testing data from a given data set. Let us take an example let us consider that the distribution of a certain dataset follows Normal distribution (Gaussian) This means that 68% of the data lies near the mean of the dataset. Also since the dataset has been identified as gaussian we also know the expected probability function (pdf) of the data set assuming we know the mean and variance of the given dataset. $P(x) = \frac{1}{\sqrt{2 \pi \sigma ^2}} e^{\frac{-(x-\mu)^2}{2 \sigma ^2}}$ Now that we have the formula we can use random variate generation techniques on this formula to create training and test data separately which can be used for the model to learn and test its efficiency. To learn more about random variate generation I'd direct you to this resource here. It has a great chapter which can help you with understanding the statistical technique behind it. 

I have two independent time series with the same length. I want to input them together into a Keras LSTM. That is, suppose I have: 

I want to input the set {A1, B1} for time 1, then {A2, B2} for time 2, etc. A Keras LSTM takes input in the form To enable better generalization, I intend to break these sequences up into subsequences of length as recommended here and here, which means (e.g. if n is 5 and my window of time, k, is 3, then there are 5 - 3 + 1 = 3 sequences total). I have time steps divided into separate windows, so (I think?). Question: what should the values of and be, and how should I shape my input data? 

If you have imbalanced classes (for example, if you have 3 classes and 100 examples of class 1 and 1000 examples of class 2 and 5000 examples of class 3), then yes, I would weight the loss function (I would use weighted categorical cross-entropy). If you mean some classes have a higher probability than others, then this is normal and expected behaviour. For example, if you were doing a 10-class classification problem like on MNIST, and you're trying to predict a given image, if the image has some rounded sections then it's much more likely to be a 3 or an 8 than a 1. 

Check the correlation of the variables or features, the ones with the least correlation can be discarded and the ones with highest must be considered to perform prediction. However, least and highest is subjective to your requirement 

There is very little information in this question. I will try to answer this in the most generic sense. Let's start by defining Noise. Noise here as you probably know is unwanted data. Any data which you are not looking for while evaluating a problem or scenario can be considered as noise. Examples for amplifying noise: Amplifying noise might occur in cases and scenarios where there is a small data set and you are trying to supersample the dataset or another example could be while working with waveforms. In order to detect weaker signals. Disadvantages of Amplifying noise The biggest disadvantage of amplifying noise from a data science perspective is that the model used to perform various operations on the data such as Regression, Classification etc will be less efficient. For example having noise based on supersampling in Classification may affect the model. If we were to use decision trees for classification you might create a bias in the algorithm which just pertains to noise while training. So your accuracy for classification also decreases. Similarly, in regression when you train with noise you might choose a wrong model because the noise alters the goodness of the fit. 

Context / big picture: Two events are independent if they have no influence on each other's outcomes. For example, if event A is "I go get coffee" and event B is "it's raining outside", then events A and B are independent, because I'm a coffee fiend and I don't care whether it's raining - I'm getting that coffee anyway! Logistic regression example: Say you have blood pressure samples from 50 different individuals at a hospital. You want to classify these as high or low blood pressure. In this example, your X (input variable) is the blood pressure from the 50 individuals, and your Y (binary response) is yes/no to the question 'is this blood pressure high?'. Each person's blood pressure is independent (if we assume they are unrelated strangers). If you were to sample the same person's blood pressure twice, like before and after physical exercise, then you would no longer have independent samples. Why: We need independent samples in logistic regression because otherwise the degrees of freedom of the model are not what we expect them to be, and this affects further calculations. For example, in your dataset, you have 50 independent samples. You use 2 to estimate your intercept and slope, which leaves you with 48 degrees of freedom to work with. If you didn't have independent samples, then you would not have 48 degrees of freedom. EDIT: a bit more detail on why independent samples are important. The assumption of independence ensures that each sample contributes the same amount of information to the experiment. With independent samples, this is the case. A blood pressure measurement from one individual is not dependent on the blood pressure of another individual - they're independent. However, if you measure the same person's blood pressure before and after exercise, a different amount of information is added. You can no longer ask the same questions as you could with independent samples. Let's pick a simpler model to describe what's going on. We will follow standard hypothesis testing principles to illustrate the practical difference between independent and dependent samples. I will round to two decimal places for ease of reading. Suppose we have two groups of people: those with blood type A and those with blood type O. (Aside: notice people cannot have both blood types at once). We want to investigate whether those with blood type A have a higher resting heart rate than those with blood type O. We have 12 people with blood type A and 10 people with blood type O. To answer this question, we'll conduct a two-sample hypothesis test. Null hypothesis: The mean resting heart rate of those with blood type A is the same as the mean resting heart rate of those with blood type O Alternate hypothesis: The mean resting heart rate of those with blood type A is different from the mean resting heart rate of those with blood type O. We measure each person's resting heart rate, and we get the following results: Type A: 60, 65, 70, 62, 55, 80, 70, 72, 66, 81, 77, 78 Type O: 61, 64, 70, 72, 59, 62, 66, 78, 69, 75 We then calculate the sample mean and sample standard deviation of each group: mean(Type A) = 69.67; standard deviation(Type A) = 8.35 mean(Type B) = 67.6; standard deviation(type B) = 6.28 We can then calculate a test statistic, which will follow a t-distribution. We may decide to use either Welch's method or a pooled variance test - in this case, I'll show Welch's method, which doesn't require the population variances to be equal (unlike the pooled variance test). The test statistic is calculated via: standard error: sqrt((8.35^2/12) + (6.28^2/12)) = 3.02 test statistic = (69.67 - 67.6)/3.02 = 0.69 The degrees of freedom here are difficult to calculate, but a generally accepted approximation is "the lesser of the degrees of freedom in the two groups", that is min(df_1, df_2). In this case the degrees of freedom of the first group is 12 - 1 = 11 and the degrees of freedom of the second group is 10 - 1 = 9. We subtract 1 here from each sample size, because we have estimated one parameter for each group (the sample mean). If we wanted to continue this example, we could look at a t-distribution on 9 degrees of freedom and decide based on the test statistic whether to reject or not reject the null hypothesis. In this case, we're more interested in the degrees of freedom than the result, so we'll move on. Dependent samples Suppose we have a that same group of people who all have blood type A. Instead of measuring their resting heart rate just once, we’ll measure each Type A person’s resting heart rate twice. This introduces dependence between the samples. Null hypothesis: the mean resting heart rate of people with blood type A is the same as the mean resting heart rate of all blood types Alternative hypothesis: the mean resting heart rate of people with blood type A is different from the mean resting heart rate of all blood types We measure each person's resting heart rate before exercise, and we get the following results: Before exercise: 60, 65, 70, 62, 55, 80, 70, 72, 66, 81, 77, 78 Then we measure their resting heart rate after exercise, and we get the following results (notice that person 1 in the previous list of heart rates is the same as person 1 in this list): After exercise: 90, 85, 70, 72, 76, 88, 89, 92, 96, 93, 84, 85 Now we have two observations for each individual. We can’t ask the same question, unless we want to throw out half of our data! Instead, we’ll ask whether there is a difference in the resting heart rate before and after exercise. We’ll use a paired-difference t-test. To conduct this test, we have to take both observations from each individual into account. We calculate the differences between each person’s before and after measurement, and treat this set of differences as our actual data set: 60-90, 65-85,70-70, 62-72, … Then we can calculate a sample mean, a sample standard deviation, etc. Our degrees of freedom would be 12 -1 = 11. What would have happened if we had assumed that those dependent samples were independent? Notice that when we had dependent samples, our degrees of freedom was 11; when we had independent samples, our degrees of freedom was 9. If we had assumed that the dependent samples were in fact two independent samples, then we could have used the wrong degrees of freedom. ** What if we had had only 10 measurements instead of 12 in the ‘dependent samples’ section? Wouldn’t we have had the same degrees of freedom? ** Yes, but it still wouldn’t have meant the same thing. We would have had: Before exercise: 60, 65, 70, 62, 55, 80, 70, 72, 66, 81, 77, 78 After exercise: 90, 85, 70, 72, 76, 88, 89, 92, 96, 93 And we would have had to ‘throw away’ the last two ‘Before exercises’ measurements and use only the first ten, as the last two had no partner measurement. The meaning has still changed, though. With dependent samples, you cannot ask the same kinds of questions as with independent samples. 

Let's start by answering your first question. Is it required to balance the dataset? Absolutely, the reason is simple in failing to do so you end up with algorithmic bias. This means that if you train your classifier without balancing the classifier has a high chance of favoring one of the classes with the most examples. This is especially the case with boosted trees. Even normal decision trees, in general, have the same effect. So it is always important to balance the dataset Now let's discuss the three different scenarios placed. Choice A): This would be what I explained all along. I'm not saying necessarily you will have a bias. It depends on the dataset itself. If the nature of the dataset has a very fine distinction with the boundaries then the chance of misclassification is reduced, you might get a decent result but it's still not recommended. Also if the data does not have good boundaries then the rate of misclassification rises a lot. Choice B): Since you are placing weights for each sample you are trying to overcome the bias with a penalty. This is also called as an Asymmetric method. Normally these methods increase the accuracy of a model by a slight margin but that mostly depends on the machine learning algorithm you are using. In examples like Adaboost such a model the effectivity of the model increases. This method is also called Asymmetric Adaboost. But this might not necessarily work with all algorithms. Choice C): Assuming you have weighted the samples accordingly it should do the same as either choice A or choice B. I'll leave this for you to extrapolate based on my previous explanations. 

In more traditional statistical learning methods, such as logistic regression, the coefficients on the pre-selected features can be interpreted as increasing or decreasing the log-odds of success. For example, say we want to use logistic regression with a single predictor variable to predict whether a student will pass the exam. We can estimate the parameters in the model: $log(Y) = \beta_0 + \beta_1$studying_hours where $Y = 1$ if the student passes the exam and $Y = 0$ otherwise, and studying_hours is a variable with values 1, 2, ..., 100 hours. We can say that a one-unit increase (a one hour increase) in the number of hours studying for the exam increases the log-odds of passing the exam by $\beta_1$. It stands to reason that we should be able to make a similar argument when the features are learned instead of pre-selected. But then we would need to understand exactly what the learned feature is. In my example above, instead of pre-selecting to be the feature we want to use to predict whether a student passes the course, we would learn some informative feature instead. I've heard of Hinton diagrams, and of visualizing the feature map for CNNs, but I don't think either of those techniques really works here. A Hinton diagram would just tell me the magnitude and sign of the weights. A feature map would not really be informative. I don't expect to understand what all of the features are, because the whole point is that the algorithm can do a better job of learning features than I can do designing features. But even so, at least some of the features should be human-intepretable, like a 'straight line detector' in a CNN. How can we interpret the features being learned by a multilayer perceptron for logistic regression? Not really sure what to tag here, so please, if there's an edit you would recommend, go ahead! EDIT This is my attempt at starting an answer, but clarification/other opinions would be appreciated. Let's take a case of binary classification, and start from the simplest possible case: one scalar input, one hidden unit with a relu activation, and one output unit. Assume the hidden unit has a bias of 0. We can express the function that describes this network as: $log(Y) = \beta_0 + \beta_1 max\{wx, 0\}$, where 

Now if it is equal to the median there is no problem in choosing whichever value but in real time it's most likely to follow the other two cases. The reason the mean is greater or lesser than the median, in this case, is because it skews to either higher computational time or lower computational time. What this skew represents is you could say where the most likely frequency of computational times would be. This implies it gives a more normalized approach and it shows where you can expect the computational times of most of your execution runs expected to be. So choose mean over median. 

This depends on the nature of your data. If you can effectively simulate 435 samples using any given oversampling methods such as SMOTE or ADASYN for instance, then I would say oversampling would be better. Because it would provide data for various scenarios. But if exact replication is not possible and if replicated there might be an inherent problem with the model then you should choose to undersample. A good example for undersampling would be that for example if you're conducting a scientific experiment and the data you have are limited to a few scenarios and the other data is yet to be fully observed, you would choose to undersample. Oversampling would be the reverse case, where you can effectively simulate data for various classes based on some parameter and the generated data can mimic actual data and scenarios, then you should oversample. 

Although the previous answer by @Imran is correct, I feel it necessary to add a caveat: there are applications out there where people do feed a sliding window in to an LSTM. For example, here, for framing forecasting as a supervised learning problem. If your data are not very rich, then you may find that any LSTM at all overfits. There are a lot of parameters in an LSTM (in fact, there are $4(mn + n^2 + n)$ parameters, where $m$ is the input length and $n$ is the output length, according to this post). Since LSTMs do not require fixed size input, they can find the optimal lookback number on their own. However, if you've done a prior autoregressive analysis and decided that, for example, the current time step is most correlated with the 10th previous time step, and not correlated with the 11th or any time steps further in the past, then you could perhaps save yourself some training time by feeding in fixed-length sequences. However, that kind of defeats the purpose of an LSTM. If your data are not rich enough for an LSTM, I would recommend trying something much simpler, like an autoregressive model, and working your way up. EDIT (responding to a comment): Overlapping sequences are used as input, especially when the sequence is very long (although, of course, 'long' is relative). Although LSTMs are better than a vanilla RNN for long sequences, they can still have some trouble remembering time steps from the beginning of a sequence if the sequence is very long. That led to things like the bidirectional LSTM, which reads the sequence forwards and backwards, improving the exposure of the network to the beginning and end of each input sequence. The principle is the same with overlapping sequences, although I would argue that overlapping sequences is more intuitive. 

This should mostly do the job. Use the arr1 ,arr2,arr3 in the function you mentioned. They are the 1d array of the columns you split 

Some of the features which are being used now can't be used later because they might not be known. Will it affect the model? If they can be used, what type of algorithm can be chosen? 

The question is poorly phrased, I've tried to edit it to the best of my abilities. However here are the problems you've stated, 

Parametric methods in simple terms follow a particular distribution. The most common example would be that of Normal Distribution, where 64 percent of the data is situated around +-1 standard deviation from the mean. The essence of this distribution is the arrangement of values with respect to their mean. Similarly, other methods such Poisson Distribution etc have their own unique modeling technique. Parametric Estimation might have laid the foundation to some of the most vital parts of Machine Learning but it is an absolute mistake to think supervised learning is the same thing. Supervised Learning may include approaches to fit the aforementioned parametric models but this might not always be the case. More often, the data scatter is quite spread out. It might not just be fitting one parametric model but a hybrid of more than one. Supervised Learning also takes into account the error which most parametric models don't consider unless incorporated manually. You could say that supervised learning is an evolved and more robust version of parametric methods which is highly flexible. 

The standard deviation and mean of a categorical variable is not meaningful. It looks like the original data are from a range of [0, 20), and the space has been discretized. Now, instead of ranges, you have ordered categories 1 through 4. The individual numbers within each category have lost their meaning. For more details, see the accepted answer here 

I've seen discussions about the 'overhead' of a GPU, and that for 'small' networks, it may actually be faster to train on a CPU (or network of CPUs) than a GPU. What is meant by 'small'? For example, would a single-layer MLP with 100 hidden units be 'small'? Does our definition of 'small' change for recurrent architectures? Are there any other criteria that should be considered when deciding whether to train on CPU or GPU? EDIT 1: I just found a blog post (possibly outdated? It's from 2014): "...Most network card[s] only work with memory that is registered with the CPU and so the GPU to GPU transfer between two nodes would be like this: GPU 1 to CPU 1 to Network Card 1 to Network Card 2 to CPU 2 to GPU 2. What this means is, if one chooses a slow network card then there might be no speedups over a single computer. Even with fast network cards, if the cluster is large, one does not even get speedups from GPUs when compared to CPUs as the GPUs just work too fast for the network cards to keep up with them. This is the reason why many big companies like Google and Microsoft are using CPU rather than GPU clusters to train their big neural networks. " So at some point, according to this post, it could have been faster to use CPUs. Is this still the case? EDIT 2: Yes, that blog post may very well be outdated because: Now it seems that GPUs within a node are connected via PCIe bus, so communication can happen at about 6GiB/s. (For example: $URL$ about 35 minutes in). The speaker implies that this is faster than going from GPU1 to CPU to GPU2. It would mean the network card is no longer the bottleneck.