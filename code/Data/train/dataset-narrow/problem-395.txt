You are mixing your granularity inside your CASE statement. You cannot compare an aggregate with a non aggregate unless they come from two different datasets 

One reason why it is taking so long could be the AutoGrow settings on the database files and log files. If that is set to a very low value, you might end up with many many, synchronous, auto grow events. You can avoid this by pregrowing the data/log files and/or increasing the AutoGrow settings. 

One possible reason could be locking/blocking. If there is lots of activity going on in your 'Slow' DB, your queries might be waiting for long time before being able to acquire the required locks. 

The only way I can think of is, using "Application Name" in the connection string. That is how SQL server recognises different applications. I think, you can append application name in the connection string at runtime using your v$session column. 

I am working on the optimization of a SP which contains some business logic using looping. I have removed the looping and converted those piece of code into some simple insert/update statements. Now I've to do benchmarking and compare old and new code in terms of execution time and logical/physical reads. My problem is because of the loop in my old code, how can I determine what is the total no of logical/physical reads. Because in SSMS, I can see thousands of IO stats statements like: "Table 'Employee'. Scan count 1, logical reads 3, physical reads 0, read-ahead reads 0, lob logical reads 43, lob physical reads 0, lob read-ahead reads 0." 

I would say that your second suggestion, haivng a TechSpecID in the Vehicles table, makes the most sense. A way of thinking could be, is a Vehicle implementing a TechSpec or is a TechSpec implementing a Vehicle? I.e. what comes first (the Technical Specification) and what is 'using' your Technical Specifications (the vehicles). But, in the end, it's all down to your context and what makes the most sense in your particular situation. You have a very broad question so it is difficult to answer specifically what you might be looking for. One thing I'd like to point out, however, is that most people don't prepend tg_ or fwk_ to their table names. You could use schemas if you'd like to differentiate them some... 

Page write is just an indicator of logical writes as well as physical writes. Logical write is an operation that happens when the page is being written in the buffer(dirty buffer) and physical write is an operation that happens when the same dirty buffer is converted to clean buffer and is being written on the physical disk. Definitely, this is not a single metric to judge memory pressure. You need to combine different metrics for coming to a conclusion. But in your case, it doesn't seem to be a problem as your page life expectancy is quite high which indicates a page will remain in the buffer for around 37k secs. So you don't need to worry about memory pressure I think. 

I'd suggest the first thing to determine is whether or not the server workload actually struggles during those spikes. Do you see for example IO being maxed out, or other queries being blocked (due to the splits increasing the time the write transactions take to complete) or slowed? If not, the page splits are not an immediate concern, but still may be worth looking into. The question is, are these "good" or "bad" page splits. This link might help you determine what you're seeing. Logging your index fragmentation levels before and after a spike might also be a simpler way. "Good" page splits are simply inserts to the end of an increasing index (clustered or otherwise)that require a new blank page, so it's really not a page split as generally thought of, even though SQL Server counts it as such--presumably because there is some overhead, but probably not more than the inserts cost in general. "Bad" page splits are updates or inserts to the middle of an index that overflow the page, and are the ones that cause both internal and external fragmentation, with external not much of an an issue with SSDs and/or shared storage, and internal being of more potential impact due to the IO and cache memory they waste. It could be that you've got a mix of good and bad, perhaps good into the clustered index and bad in multiple non-clustered indexes. That's pretty much unavoidable, and you'll just need to consider your index maintenance and possibly a specific fill factor on indexes that are frequently affected. But read $URL$ first. However if you find your clustered index is being bad-splitted, then it may be worth considering whether a clustered index that better supports your inserts would be in order. Or if the splits are caused by updates adding more data during the life of a record, a specific fill factor might be in order, but really only if the updates are evenly distributed throughout all your data, since a fill factor to support only your recent data would waste a lot of space/IO/cache if most of your data is static over time. The ideal clustering config really depends how your table is used overall though, not just on how it's written to. 

I am connecting to a database server(MSSQL 2008R2) using SSMS 2016 and for simple queries even like "USE DBNAME", it's taking 4-5 secs. However time stat shows "CPU time = 0 ms, elapsed time = 0 ms", not sure why? On the other hand, If I connect the same server using SSMS 2008R2, it gets executed instantly. I compared client statistics for both SSMS2008R2 AND 2016 but both shows almost same stats. Why my SSMS2016 is behaving like this? 

Just by looking at the query and predicate ("income in (32,33,34) and status_id = 6;"), you can't be sure that SQL will use covering index. It all depends on cost that SQL thinks is the lowest one. Not sure how many rows your predicate will have, but obviously, using Clustered index scan is cheaper than covering index. 

Well, I think this is a good example when SET operations might not be the best tool in the box. I'd go for a simple cursor here! :) Both easier to read, maintain and scans the table only once! An RDBMS does not really have the concept of 'ordered table' unless you explicitly specify it with an ORDER BY. I do, however, guess your table is 'ordered' by < EMPID | Name , PunchDate, PunchTime>. If not, and the visualised order above is just a coincidence, I have used an additional column called RowNum. 

(I know that the Application Name can be spoofed but if they really want to connect, that is OK) Are there any other ideas? Edit: The fully story is that the SQL Authenticated user in question, is a user used primarily from some Java applications. I cannot change the permissions without a full test of all applications. Also, unfortunately the Java developers knows these passwords and uses this user willy-nilly. So, if we change the password, the developers need to know. Windows Authentication from Java is a can of worms I'd prefer not to go into right now. And again, if they manage to find a way to connect anyway, it's not the end of the world. I just want to make it complicated enough for them to reconsider. (They have personal Windows Authenticated users which they can use and sometimes do). 

I'll always prefer to store that as a single column unless there is some specific business/application demand. Below are my points - 

I am tuning an SP and re-written that to optimize. Now I am comparing old code and new code in terms of timing using "setting statistics time on/off". While setting time statistics off, my new code is performing well at least 4 times better than old code (Old code is taking 4 secs and new code is taking 1 secs to execute), but when I am setting time statistics On, my new code is taking approx 12 secs and old code is taking around 7 secs. Why new code is performing badly after setting time stats on? Looks like there is some cost of time stats also and in my new code, that cost is very higher in comparison to old code. Am I right? If so what is that?