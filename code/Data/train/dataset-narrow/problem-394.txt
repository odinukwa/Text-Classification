Try the JDBC driver or ODBC driver(Install/Config Info per platform) supplied by Microsoft for Linux clients. I've found that I'm often more successful when using the drivers provided by the vendor than those that are included with the product (e.g. in this case DBeaver). I would think all the other settings remain the same, just swap out the driver url to point the MS supplied driver instead. 

Now, to audit those logins that are in the SYSADMIN fixed server role, you can run through the following process: 

Since both and are DDL statements that have Server Scope, I would suggest setting up a Server DDL Trigger to capture when your EE is stopped/dropped. This generates a simple notification when either DDL statement is executed, but feel free to alter it to your liking: 

I feel dirty providing this as an option, but if you choose to segregate the FILESTREAM data into its own database, you could maintain RI between the tables in the separate dbs by way of triggers: 

The beauty here is any user that is part of the role will have privileges to any new view you place into the schema automagically which is what I assume you're looking for. 

You likely need to perform some string manipulation to get it to convert properly. If all of your strings are fixed width and follow the same pattern as provided in your example, the following is a possible way to get the conversion to work: 

We've got a number of 4.2.3.2 Greenplum databases that are running PostGIS 1.4. We are trying to get these databases onto a more recent version of Greenplum, such as 4.3.8.1 or the re-released version of 4.3.9.1, but we're running into a mess of issues when attempting to upgrade PostGIS to version 2.0. We've attempted a number of different ways to upgrade the database without compromising data and functions that rely on PostGIS 1.4, but it looks like when we run the gppkg utility with the new PostGIS extensions we only get errors, similar to the following: 

This is a bit kludgy as I was unable to get a windowing function (i.e. DENSE_RANK, NTILE, RANK, or ROW_NUMBER) to work like I had hoped, so I had to "design" one via COALESCE and some customizations within the table structure. The basic premise here is you first need to join these tables to an intermediary table that contains a list of all days (e.g. an additional table I created, named #tmpDates). This is the foundation for the output of the report as we will be grouping records to get the output you specified. Then we work in the custom windowing function (using COALESCE here) so the steps between "maintenance" days and normal days are enumerated, but to make it work properly, I had to define an Identify Field in the Events table with a skipping increment value. The code in all its horrendous glory is below. If you have questions, let me know and I'll do my best to clarify, but this is garbage... working garbage. I'm hoping someone can get this working with a ranking function instead of my awful hack-and-slash fest with COALESCE: 

Regarding FQDNs vs NETBIOS names, I will recommend FQDNs as they aren't as prone to problems if you run up against random DNS server issues. Lifted from my blog post on the matter, formats should look as follows: 

These are not hard-line requirements, but good practice recommendations and the reason behind them is that all activity that completes during the period where the database is in bulk-logged mode (once a minimally logged operation occurs) will either have to be redone if you roll back to the tlog backup taken in step 1 or will be committed if you restore the tlog backup taken in step 5. The yellow window identified in the infographic is an all-or-nothing sort of process, and you are unable to do any point-in-time recovery during that time-frame. The moment you convert the database back to the full recovery model and take another tlog backup is the moment you can once again utilize point-in-time recovery. Finally, there are some restrictions to using the bulk-logged recovery model, such as considerations for databases with read-only filegroups and online recovery scenarios, so do some testing to make sure you're not causing yourself more headache than necessary. I've used the bulk-logged recovery model for years and it's quite helpful so long as you understand how to use it properly. 

Basically the source tables in their entirety are trying to get pulled over the linked server and all the joins and filtering are happening locally. When you're not working with millions of records this is often transparent, but when you include large tables you can immediately feel the pain. In your situation, I suggest you push a copy of the table up to your Azure DB and then run this UPDATE statement from a connection made directly to the AzureDB. If you need to initiate the process from a different server, stuff the update into a Stored Procedure in the Azure DB and then execute the SP remotely. Alternatively, if you don't want/can't push that table to Azure, you could convert the query to utilize OPENQUERY for all the remote join operations. This will quickly get ugly, but it's possible. OPENQUERY will force execution of whatever query you pass to it remotely, therefore allowing the join logic against that large table to be done remotely so only whatever records are returned from that query travel back over the linked server. These would then be joined against the local copy of the to identify what records in would need to be updated. I really don't like this approach just to update a table on a remote database, but it's an option. 

Again, my Identity Seed is configured so I won't have any ID conflicts between the two tables. The on both tables should also enforce this. Let's confirm that Partition Elimination is still occurring: 

The reason is because that field is still a datatype. If you want the output to only show the date, you'll need to CONVERT the output to a (or in this case), such as: 

We run RMAN backups via CRON jobs, but find the out-of-the-box emails with CRON to almost do what we want. I know we can channel the RMAN output to /dev/null so no email is sent, which for any successful run of RMAN is the desired outcome, but if the RMAN job runs into an error, I would like the entirety of the standard output sent via email to the DBA team for review. I suspect I can wrap my RMAN scripts with a bash shell script that will pipe the standard output to cron only in the event of a failure, but is there a better way to do this? To summarize: 

What's the downside of using an NCCI? It's important to know that after a NCCI is created, any data stored within a RowGroup effectively becomes read-only. Changes are not applied to the RowGroup itself, rather a record is marked as deleted and the updated data is stored in a different RowGroup within the NCCI. These changes are cataloged in the Deleted Buffer and the Deleted Bitmap so the engine knows what's valid in a given RowGroup. Proper maintenance will help minimize the performance costs of changing data, but if you have a lot of activity occurring on the newest data within a table (e.g. the "hot" data) between maintenance windows, a filtered NCCI may be the best way to segment the less volatile/warm data from the more volatile/hot data. Why the assumptions are important? If the first assumption is false and you will see more than 1 million records inserted into a table per tenant per week, then you can forgo a filtered NCCI and should instead create the NCCI using the additional keyword. There's a query MS recommends you run before doing this, found here, that will give you a better idea of what this value should be, but the maximum value is 10080 or 7 days time. This will keep a delta store open until either 1 million records are accumulated or the specified period of time passes and then that delta store will be converted into a NCCI RowGroup. This basically allows the NCCI to automatically work like a filtered NCCI with a sliding window. However, if you don't have enough incoming data to periodically fill a RowGroup to take advantage of this use-case, a filtered NCCI may be the better approach. If the second assumption is false, and you don't have a column that can easily define what data is warm vs hot, a filtered NCCI becomes no better than a normal NCCI. Again, use the keyword with an appropriate value and go from there. The cost of a filtered NCCI Filtered NCCIs don't come without a cost; the main issue being the definition of the filter. As hot data eventually becomes warm data, the filter may also need to be adjusted so this additional data can be included within the filtered NCCI. You are unable to change the filter of a NCCI using , so changing the definition of the filter requires that you to drop the index and recreate it with the updated filter clause. This obviously will carry ramifications in that the index won't be accessible during this change, so if your database doesn't afford a long enough maintenance window for this sort of operation, don't use a filtered NCCI. Filtered NCCIs also have some other technical limitations as fully outlined here. The benefit of a filtered NCCI will outweigh its cost when your new data is very volatile and a NCCI isn't providing a good performance boost over its row-store index counterparts. Testing is the name of the game here, but in the right situation, a filtered NCCI sitting on top of a partitioned table for your multi-tenant database could translate to big gains in performance. Hopefully that does a better job of explaining my comment to David's earlier answer.... and again, this is not an answer, just a very long-winded comment.