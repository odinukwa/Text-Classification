Since group_concat_max_len has a default of 1024, you may need do two more things to accommodate results longer that 1024 First, login to MySQL and run 

Your Question The indexes of your table may not be loaded into memory if the MySQL Query Optimizer decides not to use. If your WHERE clause dictates the a significant amount of row have to be read from the indexes, MySQL Query Optimizer will see that when constructing the EXPLAIN plan and decide to use a full table scan instead. Parallel I/O operations on a MyISAM table is unattainable because it is unconfigurable. InnoDB can be tuned for increase performance like that. 

Subquery A will bring back all listing_ids that have Ford, Exhaust, or both. Doing the GROUP BY count within Subquery A gives any listing id that has a COUNT(1) of 2 has both Ford and Exhaust taxon ids becasue BB.listing_id would appears twice thus HAVING COUNT(1) = 2. Then Subquery A has an INNER JOIN with listings. Make sure you have the following indexes 

Just hearing the question makes me think of two aspects: ASPECT #1 : Functions are supposed to be DETERMINISTIC If this so, this implies that a function should present the same return data consistently for a given set of parameters, NO MATTER WHEN YOU CALL THE FUNCTION. Now, imagine a function that produces a different answer because of gathering data at different times of the day based on static SQL in the function. In a sense, that can still be considered DETERMINISTIC if you query the same set of tables and columns every time, given the same set of parameters. What if you could change the underlying tables of a function via Dynamic SQL? You be violating the definition of a DETERMINISTIC function. Notice that MySQL added this option in /etc/my.cnf 

Performance will decrease for sure. Under what circumstances ? You should only be doing backups on nodes. The already has enough work performing inserts and writes. If your read preference is or , will compete with all and operations. If your read preference is or , you need to designate one of the nodes as a hidden member. This should make this node not be selected to perform any queries but would still just perform replication. This makes a hidden node a prime candidate for doing backups. If you do not change the into a hidden member and you launch a against it, all queries to that node will slow each other down for sure. Please read the MongoDB Docs on Hidden Secondary Members on how to set up a hidden secondary. 

SUGGESTION #3 After performing the mass delete, the index statistics need to be recalculated. does it for youm but you can do a as a separate step, like this: 

You can see global variables and status in the mysql instance using SHOW GLOBAL VARIABLES; and SHOW GLOBAL STATUS; You can also see session variables and status in the db connection using SHOW VARIABLES; and SHOW STATUS; Once you have successfully connected to the mysql instance, you have a wide variety of session and server options that you can set for the current session or permamnently for all new sessions. 

The one thing you can probably do is to increase the sensitivity of the acknowledgement Please look for this variables in 

If the character set's collation is case insensitive, then using the BINARY operator is not necessary. It would only appear to work fine in that instance. For more clarification on collations being case insensitive, please read the MySQL Documentation section "The _bin and binary Collations" 

there should never be foreign key violations. What you may need to do is cleanup every server's view of the auto increment values Here is something to try 

Of course, you could issue where #### is the process ID of either the I/O Thread or SQL Thread. You would be totally respsonsible for reestablishing replication at the risk of losing the correct log file and position if the command misses any communication because of an unnatural stoppage of a replicaton thread. 

This mechanism does not exist in MySQL for Windows. If the Windows service for MySQL crashes, that's it. You must check the error log. 

This should coalesce the LSN of the incremental after making the incremental. For more information, please read Preparing the backup from the XtraBackup Docs 

Whenever mysql writes to binary logs, it has to encode all the operations. There exists a base64 constant at the head of every binary log : . This is referred to the binlog magic number. This value is registered in in the mysql source code. Other events are also encoded in : 

When you divide 1747097889216 by 212294240, that means for every row you update, there are 8229.6 rows that are being read. This goes along with the 2245.46 (33165.95/14.97) SELECTs for every INSERT, UPDATE, or DELETE. I can see writes squeezing into the Buffer Pool. This makes me feel that the Buffer Pool, the Log Buffer, and the Redo Logs should all be increased. 

I can't tell if this involves Aurora, but here is something worth noting: There are occasions when InnoDB will leave lock lying around. How does this happen ? About six years ago, I answer this question : Are InnoDB Deadlocks exclusive to INSERT/UPDATE/DELETE?. In that post I mentioned the following from the MySQL Documentation: 

Explanation for Proposed Solution #2 only works with unique indexes and primary keys. In your case, a unique index on was needed for . After making such an index, you can insert all columns except . You could also include id like this 

Please read the MySQL Documentation in full for , then ... GIVE IT A TRY !!! If it does not work on the first try, do this 

Try removing the index so as to speed up INSERTs and UPDATEs. ALTERNATE SUGGESTION Try out your temp table solution using another method STEP 01) STEP 02) Do your bulk INSERTs into STEP 03) STEP 04) Perform count on the latest bulk INSERT batch 

Step01-b) After STEP01, you should see mysql-bin.000001 and mysql-bin.index in /var/lib/mysql STEP02 : Perform mysqldump On the Master, you can mysqldump the data and record from what point in time it happened. 

Notice I used the regular expression operator. This searches "from the beginning of the string". As for your question, why does fail to get all b's? Look at what you are asking for in the query: Every manu that is >= 'a' and <= 'b'. 

If scaling out DB Servers is not your cup of tea, then Amazon RDS is OK to use because all bells and whistles come with it. Those who simply want moderate HA, backups, and scaling out benefit a great deal. On the flip side, if you want to scale up hardware, that is out of the question for RDS. What if you want to scale up MySQL's capabilities? Unfortunately, that is out of the question for many aspects one would want. For example, did you know that two fields are capped across all seven(7) RDS server models? 

You will note that the will have an blank username and some host. GIVE IT A TRY !!! UPDATE 2015-11-17 16:24 EST The column known as in no longer exists in MySQL 5.7. It was renamed . Proper Approach What you should have done is run 

By default, InnoDB is set to autocommit = 1 or ON. Once committed, they cannot be rolled back. You would have to do one of two things to disable it going forward: OPTION 1 : Add this to /etc/my.cnf and restart mysql 

and start mysql and see if it comes up UPDATE 2012-05-15 15:59 EDT Since it fails to start up, you may need uninstall MySQL 5.5.24 and go back to MySQL 5.5.23 or whichever version you were last running with. UPDATE 2012-05-15 16:06 EDT Please run on the mysql error log and paste that in the question.f it is not configured, go to the data folder and do to find it. UPDATE 2012-05-15 17:54 EDT Based on the display you pasted in, you evidently had MySQL 5.1 running before. It just so happens that MySQL 5.1 has a table called mysql.plugin. It may have been populated before. It probably had something concerning the InnoDB plugin. MySQL 5.1.38 introduced the InnoDB plugin which has new features that are now native to MySQL 5.5's InnoDB. In other words, MySQL 5.5 does not need any plugin to run the new InnoDB. Please run this query: 

VIEWPOINT #3 : Retrieve only the columns you really need I see you have SELECT * and you have four tables (hits, stadiums, players, games). You will have a lot of duplicate data to drag into the query, particularly when dragging the gameName column from all four tables. You should reorganize the query to bring only one gameName column: 

Step 05) on ServerA, ServerB, ServerC Step 06) Create GRANT for replication user on ServerA, ServerB, ServerC 

Please be a little conservative on the number of rows you choose to batches together. You will take much longer to parse the mysqldump because of the mysqldump will be much longer. 

If you want the triggers for table A to launch but not to table B, then add the code block only to table B's trigger. 

Even though you have data in RAM, mysqld will always hit the .frm file to check for the table existing as a reference point, thus always incurring a little disk I/O. Proportionally, heavy access to a MEMORY storage engine table will have noticeable disk I/O. 

Those files are called binary logs. They contain a list of all completed SQL statements you have executed. Your mysqld process created them and autorotates at 1GB. Any restart of mysqld or issuing will close one binary log and open another They got created because there is a line in my.ini that looks like this: 

If you know the number will not surpass certain values, you can change the column definitions to accommodate less diskspace. At the very least, let PROCEDURE ANALYSE() suggest it for you. OPTION 3 : Diskspace can be a bottleneck The more house guests, the more housekeeping to accommodate. If a host can take care of 20 house guests, keep them all 20 happy, and the host is happy as well, the host can always care 20 house guests over and over again. Add one more guest, and the host's quality of services goes down hill. The same goes with indexes. If your queries are fast enough to warrant keeping indexes around, you need to select which indexes are really necessary, and eliminate indexes that are redundant (unwanted house guests). The host must make sure the each house guest's request (query) can be satisfied (done with covering indexes) Nice Links on Covering Indexes