Have a look at the proceedings of the conference "Fun with Algorithms": they should provide you with a good selection of "Fun" problems to work on, and a venue where to submit your results for feedback. Check the publications of people known to consider fun problems and problems for fun. For instance: 

Concerning technical writing: Check spell your paper, review your notations (even though an index of notation cannot be included in a conference submission in general, doing it early helps for later, longer versions, and for the clarity of the early versions), have someone else than those involved in the writing to read it and check it is understandable. Concerning content: Justify all your choices. When you have several related results, find what they have in common, describe it as a question, and explore alternative answers to this questions that you did not consider before, eventually adding new (minor?) results or justification for discarding alternate answer. When you have a single result, a single answer to one specific problem, describe what are the other possible answers to this problem, in the literature or based on futuristic results. Do not limit your paper to "I did this, and look, it works!". Explain why it works and why or how other choices would work worse or would not work. 

One approach to proving your conjecture would be to try to use the Szemerédi regularity lemma, similar to the way the triangle removal lemma is proved (see e.g. here). I don't know if you'll get the right constants from this approach, though. 

Some work in progress: I'm trying to prove a lower bound of $4^k$. Here is a question that I'm pretty sure would give such a lower bound: find the minimum $t$ such that there exists a function $f:\{S \subseteq [n], |S|=k/2 \} \rightarrow \{0,1\}^t$ that preserves disjointness, i.e. that $S_1 \cap S_2 = \emptyset$ iff $f(S_1) \cap f(S_2) = \emptyset$. I'm pretty sure a lower bound of $t \ge 2k$ would almost immediately imply a lower bound of $2^{2k}=4k$ for our problem. $f(S)$ roughly corresponds to the set of nodes the NFA can get to after reading the first $k/2$ symbols of the input, when the set of these $k/2$ symbols is $S$. I think the solution to this question might already be known, either in the communication complexity literature (especially in papers dealing with the disjointness problem; maybe some matrix rank arguments will help), or in literature about encodings (e.g. like this). 

It's possible to build the suffix array of $s$ in linear time from $s$ and $BWT(s)$ in a somewhat easy way. You do need to build a rank-select data structure on $s$ in order to do this. To see how to do this, look at Ferragina and Manzini's paper "The FM-Index". The LF mapping they describe also essentially computes the suffix array. 

There is a whole community working about Kolmogorov's complexity and its variants, and another community working on loss-less compression (the example on integers that I used has equivalent on many other data types), I barely scratched the surface, and others might add precisions (Kolmogorov is really not my specialty), but I hope that this might help you clarify your question, if not necessarily give you the answer you were hoping for :) 

Have a look at my (modest) proposal of "fun" problem below. If you work on it, get in touch with me! 

Let $u$ denote the size of the union of the sets in $S$, $n\leq 2^u$ denote the size of $S$, $h\geq n$ denote the size of the output, and $S_i$ the set of sets resulting of the intersection of $i$ sets from $S$ (in this sense, $S=S_1$), maintained in lexicographical order. For all $i\in[1..n]$, $|S_i|\leq h$. $S_2$ can be computed in time $un(n-1)/2\leq uh(h-1)/2$: each binary intersection takes at most time $u$, and there are at most $n(n-1)/2$ of them. $S_4, S_8, \ldots$ can be computed in time $uh(h-1)/2$ each in a similar way. Other $S_j$ which are not power of two can be obtained by combining those results, up to $S_n$. The total running time would be within $O(unh^2)$? 

A Hufman code associates an "integer" number of bits to each message: on some distributions this can sum up to a linear cost in the number of messages being sent (hence the $1$ in the $n(1+H)$ upper bound for the size of the sequence of bits produced. My understanding is that arithmetic codes overcome this weakness, at the cost of a more complex algorithm to compute the codes, and encoding tables (disclaimer: I am rusty on this, anyone should feel free to correct or complete), reaching $nH+O(1)$. The difference between $n(1+H)$ and $nH+O(1)$ is more sensible for $H$ small of course, so arithmetical codes are prefered for low entropy distribution, if there are no constaints on the cost of encoding/decoding. 

I suspect you won't get a closed form solution for the distribution you're looking for. Think of the seemingly easier problem where $k$ is always chosen to be exactly $2$, and where you get the set $\{1\}$ "for free". This problem is just like asking for a closed-form distribution of the cardinality of the connected component in the Erdos-Renyi graph $G(n,s)$ that contains a specific vertex $v_1$. I'm pretty sure that there's no known closed-form description of this distribution, and that in fact the lower order terms are poorly understood, especially around the critical value $s \sim n \log n$. 

Here is one algorithm. It runs in time at most $O(|S|*|w| + c)$, and probably runs much faster for many inputs. We create a directed graph $G$. The graph $G$ has a vertex $(s,i,j)$ for any $s \in S$ and any indices $i,j$ such that $s=w[i..j]$. Then the graph has a directed edge from $(s,i,j)$ to $(s',i',j')$ whenever $i'=j+1$. Finally, the graph has two additional vertices, a source and a sink. There is a directed edge from the source to any vertex of the form $(s,0,j)$ and from any vertex of the form $(s,0,|w|)$ to the sink. Now notice that the directed paths from the source of the sink correspond exactly to the parsings of $w$. So it's enough to enumerate all (source,sink)-paths in $G$, which can be done in time $O(|G'|+c)$. Note that the graph $G$ kind of represents all the parsings, so for many applications you wouldn't even have to enumerate them explicitly. 

Element distinctness can be solved deterministically in the RAM model in time within $O(n\log\log n)\subset o(n\log n)$ time: Sort in time within $O(n\log\log n)$ your $n$ numbers of $w$ bits using the sorting algorithm described by Han in STOC 2002 ("Deterministic sorting in $O(n\log\log n)$ time and linear space"), then scan in linear time for collisions. As far as I know, that is the best result known to this day. 

The concept of "witnesses" or "checkable proofs" is not totally new: as mentioned in the comments, look for the concept of "certificate". Three examples came to mind, there are more (in the comments and elsewhere): 

This kind of "laws" are usually labelled as Pareto principle, or 80–20 rule: Answering specifically your question(s) 1) This law is true in the real sense, or is just an observation, a presumption? 

It took a few years (five!), but here is a partial answer to the question: $URL$ Optimal Prefix Free Codes With Partial Sorting Jérémy Barbay (Submitted on 29 Jan 2016) We describe an algorithm computing an optimal prefix free code for n unsorted positive weights in time within O(n(1+lgα))⊆O(nlgn), where the alternation α∈[1..n−1] measures the amount of sorting required by the computation. This asymptotical complexity is within a constant factor of the optimal in the algebraic decision tree computational model, in the worst case over all instances of size n and alternation α. Such results refine the state of the art complexity of Θ(nlgn) in the worst case over instances of size n in the same computational model, a landmark in compression and coding since 1952, by the mere combination of van Leeuwen's algorithm to compute optimal prefix free codes from sorted weights (known since 1976), with Deferred Data Structures to partially sort a multiset depending on the queries on it (known since 1988). 

When $p^q>3k$, then there is an easy randomized algorithm for this problem (with error probability zero and polynomial expected running time). The algorithm prints "IMPOSSIBLE" if there is $i$ such that $B|w_i$ has more than $n-|w_i|$ rows that consist entirely of zeroes. Otherwise, the algorithm prints "POSSIBLE". It then chooses uniformly random matrices $A$ until it finds one that satisfies the requirements. Below I'll prove that a random matrix $A$ satisfies these requirements with at least constant probability. If the algorithm printed "IMPOSSIBLE" then it is easy to see that indeed there is no $A$ that satisfies the requirements. Now, consider the case that the algorithm printed "POSSIBLE". A known result states that a random matrix over a field of size $|F|$ is full-rank with probability $\ge 1-2/|F|$. To see this consider the columns one by one, and compute the size of the linear subspace that they have to "dodge". Work from the last column to the first. You get that the chance that the matrix is full rank is $(1-1/|F|)\cdot(1-1/|F|^2)\cdot\ldots$. In fact, the same proof can be seen to work even if some of the entries are fixed to zero, as long as no entire row is fixed to zero. QED. 

I agree with @usul. I've also never seen the term empirical mutual information mentioned, but I've seen the term empirical entropy quite a lot, especially in the compression community. The definition of empirical information is $-\Sigma p_i \log p_i$, where $p_i$ are the empirical probabilities, i.e. the fraction of the time that each value appears in your samples. To compute empirical mutual information given samples $(x_1,y_1),\ldots,(x_n,y_n)$, I'd just compute the empirical entropy of the $x$'s separately, and of the $y$'s separately, and of the pairs together, and then I'd use the standard equation $I(X:Y)=H(X)+H(Y)-H(XY)$, where all quantities on the right hand side are replaced by their empirical analogue. I don't know if this is equivalent to the equation that @usul gave, and I don't know if this is the quantity referenced in the articles you're reading, but this seems like the natural interpretation to me. 

A SIGMOD 2014 paper from Microsoft Research states that the "importance of sorting almost sorted data quickly has just emerged over the last decade", and goes on to propose variants of Patience sort and Linear Merge, and measure their performance on synthetic "close to sorted" data. It seems to me that this description matches the theme of "Adaptive Sorting", covering algorithms taking advantage of existing preorder in sequences to be sorted, which has been the topic of various publications (albeit in the community of theoretical computer science rather than databases) from as early as 1979: 

The concept is not always usefull for error checking: there are problems where checking the certificate takes as much time as producing it (or simply producing the result). Two examples come to mind, one trivial and one complicated, Blum and Kannan (mentioned in the comments) give others. 

Of course, for us in academia, most problems are fun anyway: we would not be there otherwise ;) But I understand that you might be more interested in problems which do not require much background and/or which are related to mundane topics like social games. Note that it might be harder (and more important) for you to find someone willing to comment on your solution and help you to improve it than to find a fun problem itself. For this reason, asking a professor in your department might be a better starting point. That said, here are some suggestions as to where to find ideas, and a modest proposal of mine. 

Here is a wrong answer: it outputs some vertices that are part of non-simple paths from $s$ to $t$ and that are not a part of any simple path from $s$ to $t$ of length $\le \ell$. The answer might still be relevant to the asker's application, so I'm leaving it here. Here is an algorithm that runs in time $O(|V|+|E|)$ (and actually is faster than this when $\ell$ is small). The algorithm runs a BFS search from $s$ that terminates at depth $\ell$. This BFS gives a set $V_s$ of all vertices reachable from $s$ with a path of length at most $\ell$, and it also computes the distances $dist(s,v)$ for each $v \in V_s$. Then I'd do the same from $t$ and get the set $V_t$ and distances from $t$. Finally, the vertices you're looking for are exactly $V_{solution}=\{ v : v \in V_s \cap V_t, dist(s,v)+dist(t,v) \le \ell \}$. The edges are exactly those edges in $E[V_{solution}]$ ($=(v,u) \in E : u,v \in V_{solution}$). The running time of this algorithm is surely $O(|V|+|E|)$ because it just does two BFSs. But the running time is actually $O(|V_s| + |V_t| + |E[V_s]|+|E[V_t]|)$ which will be much smaller than the size of the graph when the $\ell$-radius neighborhoods of $s$ and $t$ are small. Edit: there's probably a somewhat faster algorithm in practice that does a BFS from $s$ and $t$ of depth only $\ell/2$ rather than $\ell$. This discovers all the paths, and then with a bit of bookkeeping you can find all the vertices. This cuts the running time by a square root for the case of a large random-looking graph when $\ell$ is small. 

I found the result hidden in an obscure 4p technical report: I share my results here in case others are interested. 

I named this problem "Binary Sorted Min Sum" for lack of a better name. The computational complexity of this problem in the worst case over instances of size $n$ is clearly $2n$ evaluations, $n-1$ comparisons, and $n$ additions, all within $O(n)$. A linear number of instances of this problem occur in the core of my colleague's solution, which complexity he analyzes as $O(n^2)$. Inspired by the fact that many instances of this problem can be solved in sublinear time (e.g. when $f$ is increasing much faster than $g$ is decreasing, one can certify that the minimum is at index $1$ in one single comparison, 4 accesses and 2 sums), I got some interesting adaptive results, showing tight bounds within $O(\delta\lg(n/\delta)$ for some parameter $\delta\in[1..n]$ measuring the difficulty of the instance, implying both the $\Theta(n)$ bound in the worst case and the $\Theta(\lg n)$ bound in easy instances: it is actually quite similar (but not reducible, it seems) to the "Binary Sorted Intersection" problem which I studied previously. A quick search on "Binary Sorted Min Sum" did not yield anything meaningful, yet the problem is quite simple, so I am wondering if someone introduced it before under another name? 

Proof: Let $G$ be a graph which is not strongly connected. We will prove that $\delta^+(G)+\delta^-(G) < n$. Write the decomposition of $G$ into strongly connected components. Let $S$ be a strongly connected component which is a sink (i.e. no edges go outside of $S$) and $T$ be a source (i.e. no edges go into $T$). Since no edges go $S$ to outside of $S$, then $\delta^+(G) \le \delta^+(S) \le |S|-1$. Similarly we get $\delta^-(G) \le |T|-1$, and taking these two things together we get $$\delta^+(G)+\delta^-(G) \le |S|+|T|-2 \le n-2 \ .$$ QED. 

@Chandra, @Emil, and myself solved the question in the comments. The solution is $$f(n) = 2^{\Theta(n \log \log n / \log n)} \ .$$ To see the lower bound, apply the recurrence definition $\log n$ times, to get $$f(n) = 2f(n - \log n) + f(n - \log n - 1) + \ldots + f(n - 2 \log n) \ge \log n \cdot f(n - \log n) \ .$$ Use this inequality $n / \log n$ times, and we get that the solution is $2^{\Omega(n \log \log n / \log n)}$. To get the upper bound, use the recurrence $\log n$ times and get that $$f(n) \le (\log n + 1) \cdot f(n - \log n) \ . $$ Use this inequality $n / \log n$ times, and we get that the solution is $2^{O(n \log \log n / \log n)}$. 

For more details, see the section on "Trans-dichotomous algorithms" from the wikipedia page for "Integer Sorting". 

The library Leda is the most general effort (that I know of) toward making deterministic certificate-producing algorithms the norm in practice. Blum and Kannan's paper is the best effort I saw to make it the norm in theory, but they do show the limits of this approach. Hope it helps... 

Does it matter in the 3Sum problem if the numbers to be summed belong to the same set or to distinct sets? Let's define 

During talks, display electronically the time left to the speaker (see videos of TED talks for an example). I used a simple java application in full screen to do so at the last conference I chaired: it would have felt stupid to do it "by hand", keeping an eye on the clock and using papers with "10mns left", "5mns left", "1mn left", "FINISH NOW" written on it. Bonus: display the overspent time in color flashing after the timer is done. 

Many times I use /Xournal/ under Linux on a tablet pc "lenovo X61". It allows me to write my slides by hand, yet copy paste the structure from one slide to the other, redo the stuff till I am happy with it, and give it a particular slide. I was pointed out that in some groups it was perceived as a lack of professionalism or effort (which I feel is not true), for those groups I make some slides in /Beamer/. On occasion I have used a mere /orgmode/ file under /emacs/, unrolling and rolling the sections as needed: I like how you see the overall structure and then zoom in, a bit like in /prezi/. It can display LaTeX formula, but the size of the fonts sometime is an issue.