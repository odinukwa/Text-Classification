After that, it would be up to you to disable the Mesh Colliders in the child components, or not, as your program requires. See this documentation for more information: docs.unity3d.com/ScriptReference/MeshCollider-sharedMesh.html docs.unity3d.com/ScriptReference/Mesh.CombineMeshes.html 

You never stop dragging the block; it simply appears to the user that the block has stopped dragging while it would intersect with an obstacle. 

In short, what you're doing is moving around some reticles right in front of your camera to overlap invisible objects you put in the player's line of fire. If you don't want to have the reticles sitting in front of your main camera at the near clipping plane, which is usually pretty close, you can consider these options: 

Decide how high some obstacle on the plane has to be in order to block your dragged object (where "high" means removed along whichever axis is appropriateâ€”I'd assume the one that points toward the camera). Each update, save the position of your raycast hit. Each update, before saving the position of the current raycast hit, check the position of the current raycast hit against the position of the previous raycast hit. If the difference >= blocking height, stop your moving object at the previous raycast hit position. 

In the case when equals 1, unless I comment out the content of the second block, it assigns an another value to . Also while equals 1, when I remove the second 'if' block and change to like in the sample code below, nothing happens (which means that really equals 1). 

First of all I must say, that I have read a lot of posts describing an usage of cubemaps, but I'm still confused about how to use them. My goal is to achieve a simple omni-directional (point) light type shading in my WebGL application. I know that there is a lot more techniques (like using Two-Hemispheres or Camera Space Shadow Mapping) which are way more efficient, but for an educational purpose cubemaps are my primary goal. Till now, I have adapted a simple shadow mapping which works with spotlights (with one exception: I don't know how to cut off the glitchy part beyond the reach of a single shadow map texture): >>>glitchy shadow mapping<<< So for now, this is how I understand the usage of cubemaps in shadow mapping: 

Setup a framebuffer (in case of cubemaps - 6 framebuffers; 6 instead of 1 because every usage of framebufferTexture2D slows down an execution which is nicely described >>> here <<<) and a texture cubemap. Also in WebGL depth components are not well supported, so I need to render it to RGBA first. 

You might check to see if the triangles in each index variant are still listed in the same order (for instance, sometimes it's A0 B1 C2 D3, sometimes it's B0 C1 D2 A3, but always the same triangles come before and after each other in the index). If that's the case, you might be able to calibrate when you instantiate your gameobject by shooting a raycast at the triangle you want to treat as index 0 and noting the difference between 0 and its actual index. You could infer the index of other triangles from there. 

Add two empty Game Objects to your player object along the line of fire (in front of the ship), one farther away than the other. 

You could, at runtime, assemble your children's colliders into one big collider that you add to the parent. In order for this to work the way I've done it below, the children would have to have Mesh Colliders, even if they were box-shaped: 

Make two reticle objects that look however you like. Make them children of the main camera. Place these reticles anywhere on the X and Y axes, but keep their positions on the Z axis right in front of the main camera, exactly at the near clipping plane. On the main camera or the Game Controller, write a Reticles Manager script. This script should watch the two empty game objects you placed in front of the ship and use Camera.WorldToScreenPoint to obtain their screen positions relative to the main camera. Once it has those positions, the Reticles Manager should move your two reticles to those two X and Y positions (keep the Z fixed at the near clipping plane). 

Let's say, that I have 3 programs, and in each of those programs there is a view matrix uniform, which should be the same in all those programs. Right now, when my camera moves, I need to re-upload the modified matrix to every program separately. Is it possible to create some kind of global uniforms which are constant for all programs linked to it, so I could just upload the matrix once? I tried creating a globalUniforms object which looked kinda like this: 

Finally, by trial and error, I managed to get this thing working. The problem was that the cubemap texture is mirrored, so I had to scale the regular projection matrix by (1, -1, 1) in order to un-flip it. Also, the right vector to read the cubemap is . Now everything seems to be working nice! 

Setup the light and the camera (I'm not sure if should I store all of 6 view matrices and send them to shaders later, or is there a way to do it with just one view matrix). Render the scene 6 times from the light's position, each time in another direction (X, -X, Y, -Y, Z, -Z) 

It sounds like you've done a lot of math in 3D space to try and support this feature, taking into account the position of the ship and the camera, etc. But unless I'm missing something, I think there's an easier way. This should work whether you want the game to be in first-person or third-person view. 

It's difficult to detect collision without any rigidbodies involved, but since you're using cubes you may be able to use Bounds.Intersects. Each update, check if the dragged object's bounds intersect with any obstacles' bounds. If it does, then you could try this: 

It seems to me that if you'd like your object to drag along a slanted plane of an obstacle, you should simply have its transform continue to follow the position of the latest raycast hit. 

Disable the Mesh Renderer for the dragged object to make it invisible (but continue to drag it). Drop a visible, placeholder object identical to the dragged one at the last position before bounds intersection (now it looks like the object stopped, but really you are still dragging the original). Each update, check to see if the bounds of the object you're still dragging intersect with an obstacle's bounds. When the dragged object's bounds no longer intersect with an obstacle's, delete/deactivate the placeholder object and re-enable the Mesh Renderer for the dragged object to make it visible again. 

You're calculating the up vector (and also the rest of them) from a matrix unrelated to the matrix, in which you have stored the new rotation. Usually, what I do is multiply an old rotation matrix by the new rotation matrix, and then calculate the up vector from the outcome matrix. Hope this helps. 

Also, when I manually create an int variable (which is not an uniform) like this: and replace with it in the if-else branch, everything works fine, so I guess it have something to do with the fact that is the uniform. 

In a second pass, calculate the projection a a current vertex using light's projection and view matrix. Now I don't know If should I calculate 6 of them, because of 6 faces of a cubemap. ScaleMatrix pushes the projected vertex into the 0.0 - 1.0 region. 

I have read that in order to optimize WebGL application, one should reduce an amount of draw calls. But does it mean that computing a one big mesh from all single meshes on CPU by modifying vertices position (which I heard is much slower than GPU while talking about matrix multiplications) and calling the draw call once would be faster than drawing every single mesh using model matrix inside GPU, calling the draw call for each one of them?