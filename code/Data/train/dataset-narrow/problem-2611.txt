When in doubt, always talk to a lawyer. People on this website are not qualified to give legal advice. That includes me, so don't come back and sue me for giving you incorrect information. That said, you have to copy something to be breaking any copyright laws. If you're using the music files in-place on the player's computer, you can't be infringing on any copyrights. (There have been arguments made that, when you open a file, you are copying the file to memory and should be subject to those copyright laws. I don't think they were taken very seriously, though.) Even if you copy the files to another location on the same device, I can't see that being a problem. Just don't move the player's music collection to the cloud. That can get you in trouble. 

According to the (yet-in) complete guide to Soya 3D, the only supported model formats are .blend files or its own custom format. I've found a couple places that said you can import .obj files by placing them in a certain directory, but I'm not sure if it's true, and at least it's not in the documentation. So to use your files, I recommend importing them with Blender, saving as a .blend file, then importing that with Soya. Either that, or use a separate .obj parser. This file may work, or I'm sure you can write your own, since .obj files are pretty easy to parse. 

I'm setting up a system for people to load up extra levels. Since I don't know how many levels are going to be loaded I have a UI prefab that gets added to the main Level Load UI. However, the Text components of the prefab are not being bound by the scrollrect mask. The panel, images and button all are, and they are all children of the panel getting added to the panel that is the scrollrect's content. 

So, nothing that should be affecting the button that I can see. I thought that the button was getting resized, but when checking in the scene the button is stretched all the way across the prefab. Any ideas why the button isn't working properly? 

I have some textures that are used in multiple scenes and I want to keep them in memory to speed up load times between scenes. I know about Resources.Load, but does that always keep it loaded? If a texture is loaded with Resources.Load, will instantiating a prefab with that texture recognize that it's already loaded or will it load its own instance of the texture? It would be better if it's a method that I can use in code so that I can make a loading screen and not freeze everyhing up. I'm limited to an earlier version of Unity so I don't have access to LoadLevelAsyc. 

Using libgdx here. I've just finished learning some of the basics of creating a 2D environment and using an OrthographicCamera to view it. The tutorials I went through, however, hardcoded their tiled map in, and none made mention of how to do it any other way. By tiled map, I mean like Final Fantasy 1, where the world map is a grid of squares, each with a different texture. So for example, I've got a 6 tile x 6 tile map, using the following code: 

Given the random nature of the environment, for loops don't really help as I have to start and finish a loop before I was able to do enough to make it worth setting up the loop. I can see how a loop might be helpful for like tiling an ocean or something, but not in the above case. The above code DOES get me my final desired output, however, if I were to decide I wanted to move a piece or swap two pieces out, oh boy, what a nightmare, even with just a 6x6 test piece, much less a 1000x1000 world map. There must be a better way of doing this. Someone on some post somewhere (can't find it now, of course) said to check out MapEditor. Looks legit. The question is, if that is the answer, how can I make something in MapEditor and have the output map plug in to a variable in my code? I need the tiles as objects in my code, because for example, I determine whether or not a tile is can be passed through or collided with based on my TileTyle enum variable. Are there alternative/language "native" (i.e. not using an outside tool) methods to doing this? 

It works great in the editor, but when I made a build and tested it in Chrome none of the buttons had any response. Further testing revealed that it did work in Firefox. Rather than telling people to change their browser if they want to play, I want to make the button code work. How else can I get the buttons to know when they're being pressed if the built-in stuff isn't working? 

Another related problem, each of the prefabs also has a button on it. I've given it some alpha because I find that once it's in the scrollrect only the top half is clickable. You can see below that it only highlights (as red) if your on the top half of the button. The exception is that the final button will work if your mouse is anywhere on the button. This isn't an issue for the buttons if they're just placed in the scene. It only happens inside the scrollrect. 

I have a script that controls how the particles in a particle emitter should look. (XNA example here) Yet it doesn't seem to update correctly. With an orb that should have 50 particles only a few show up. I tried to debug it to see what was going on and it all started working again. I removed the debugging and I was back to only a few particles being visible. I put in a simple debug message (Debug.Log("hi")) and it worked, but it was a little slow since I was putting out a debug message 50 times per frame. I wanted to know if I wasn't getting the correct number of particles so I set the starting color to something with full alpha instead of 0 and all of the particles showed up even without the debug message, but they just pop in instead of fading now. This means that somehow it's not fading in correctly unless there's a debug statement. It makes no sense to me. The issue seems to be in the "if (myParticles[i].ChangingColors)" section of Update. Here are my start and update functions. OrbBase - Handles the number of particles. RotateParticleFinal - Holds the data that I want to set to the particle. 

You can call isActive() on each btCollisionObject. It just returns a boolean, so it's easy to work with. Alternatively, you can call getActivationState() which returns an int representing one of these values, but you shouldn't need to. To optimize, if you really need to, I'd just add all collision objects to a list. Loop through this list every frame, removing each inactive object you come across until you find a single active one, then break out of the loop. Or just loop through all objects every frame. Premature optimization is usually not worth it. 

The bottleneck for games is usually the bandwidth and latency between cpu and gpu, not the number of calculations you're doing. Using your example of 3,000 sprites, the reason it's so slow is because Game Maker draws them all individually, which means 3,000 draw commands going from the cpu to the gpu. Even though each draw command is really simple, every new command has a slight delay to it as it works its way through the driver and then through the wires to the gpu. Particle systems also draw sprites, but they can draw thousands or even millions of them without hurting your fps any. This is because the information for all those sprites is combined into one buffer and drawn with one command. It comes with some limitations, though, especially if you want each sprite to have a different texture or behave differently. It's hard for a game framework to generalize this technique, so it's usually limited to particles unless you can write your own shaders. When it comes to 3D, drawing 10 boxes is pretty close to drawing 10 sprites. The calculations are a little more complicated, but you're still using a miniscule fraction of the gpu's power. The number of draw calls is the same, so you'll get about the same fps. Once you add lighting, shadows, anti-aliasing, and so on, then you might get closer to that computational limit, but since you're still using the same number of draw calls, you'll stay under the bandwidth limit, which lets you do a lot more. 

I have a GameObject that I want to have multiple sprites. I tried adding an extra Sprite Renderer, but it tells me I can't add the same component multiple times. I'd really rather not have to have four GameObjects just to display this thing correctly. For example, I would want to have a tile background, a tile artwork, a tile character, and a nice shine on the top. If I were to make that all a single image, I would have to export nearly a thousand images to get all possible combinations. Tile Background: Tile Artwork: Tile Character: Shine (Barely visible): Everything put together how I want it in-game: 

So, in RotateParticleFinal I have a part that updates the alpha. That's only using floats. But when I check the actual particle to see if the alpha is at 1... well, it's a byte now. It was working better with a lot of debug calls because it slowed everything down, meaning I got a lot more alpha on my deltaTime based fade-in. Been wrestling with this for a while now and I didn't figure it out until right after I finally posted this >_< 

I'm instantiating UI objects with buttons on them into a scrollrect. For some reason, all of the buttons will ignore their bottom half with the exception of the last prefab instantiated. Below is a gif to show the problem. I have it set to turn red when the button is highlighted. When the mouse isn't in a highlight position clicking will do nothing so I know it's not just a highlighting issue. 

You can do the trig functions yourself if you want, but it's a lot easier to use a rotation matrix. In the background it will do the exact same sin/cos stuff, but it's already programmed for you, so why redo it? I'm not too familiar with Irrlicht (or C++), but adapting some code I found on their forums, it'll probably look something like this: 

This isn't the only solution. The alternative is creating a new copy of an already existing tetrahedron model and positioning that. I do think this method is easier, though. 

The texture2D() function returns a vec4 containing the rgba values of that texel. You can then manipulate this just like any other vec4. For example, to make everything darker, you can do... 

Unity has three kinds of lights that work in real-time: Directional, point, and spot. Point lights just sit somewhere and emit light in all directions, which sounds a lot like the sun, but you would have to put the light so far away from everything and make it so strong that it may be unreasonable. Spot lights are the same, except they emit light in a cone instead of in all directions. Directional lights, however, don't have a position. Well technically in Unity they probably do, but the position isn't used in the light calculations. The only factors are the direction and the strength of the light. This means the light effectively has a distance of infinity, but the brightness does not fall off. Of these three, the directional light is the most reasonable to simulate sunlight, even if it doesn't exactly sound right. For some pictures to further clarify things, here's Unity's page on the matter: $URL$