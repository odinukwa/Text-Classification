Indications from my searches are that this is some kind of connection issue or timeout. I'm working to improve the proc performance, but the timeout is not set on the Linux machines (so should default to 0). Is there something I can do to stop this error? Here is the log from FreeTDS of a bad run: 

With the change to package configurations in 2008 compared to 2005 when I specify /ConfigFile something.dtsConfig on the command line, variables defined in the package are keeping their design-time values instead of using the settings from the config file. I'm not quite sure I understand HOW to get the external config file to be used at all. I've read articles that say that only design-time configurations that are set will overwrite the load of the external file. Does this mean I can change the variables to blank strings and then they will get overwritten? I can't delete the variable completely! What about integers? I've seen articles that mention turning OFF using package configurations in the package. I can use the SSIS Package Editor or an XML editor to change the configuration file path in the package, and then it will use that file's settings "last" (regardless of the external /ConfigFile option), but I don't want to be changing the package. I want one package with Test.dtsConfig and Production.dtsConfig and be able to swap back and forth without changing the package. What is the recommended way to do this now? 

With your sample data, there are some unmatched pairs - this solution only gives the interval for the matched pairs, and more than two entries for each event, which can cause an cross join between the start and two stops. $URL$ 

Not 100% what you are getting at, whether you're talking about the order of developing a data warehouse or how the data flows into the warehouse and on to other things. There is a camp that models all the data into the data warehouse (using dimensional modeling techniques, and only generally informed by the users roles and domain expertise) and then the end users build their analytics on it. There is another camp that gathers end-user requirements and then builds the warehouse to meet that, identifying the data in the source system. I tend to be more in the first camp, because users only know what they want once they play with the data and see what's available. 

At the very least, you should have an index on WardCode on uk_pc. If you also add Latitude and Longitude it will become covering for this query and so the table will not need to be used at all. Your datatypes aren't really the most efficient, so I'd definitely revisit your table designs. narrower columns (Latitude and Longitude are better off as decimal and would never be 100 characters long) will store better in the tables (and of course in the indexes as well). The width of varchar columns might seem to be something you don't neet to worry about, but it does affect what the database engine sets aside while performing queries since it does not know if it will encounter larger strings in the data. In general, limiting the working set like this can only help, especially when it is easily foreseeable. 

The problem with your first example is the tri-link table. Is that going to require one of the foreign keys on either report or recommendations to always be NULL so that keywords link only one way or the other? In the case of your second example, the joining from the base to the derived tables now may require use of the type selector or LEFT JOINs depending on how you do it. Given that, why not just make it explicit and eliminate all the NULLs and LEFT JOINs? 

It can be very general, just a collection of data and structures. The system for managing a database can be as simple as a file system or as complex as a federated system like DNS. Generally in modern usage, when one says database, one does imply both the data storage and the structures and an accompanying database management system, and because so much theoretical work has been done on the foundations of relational databases, these are still the most popular so that often when one says database, one is often implying a relational database. With the rise of NoSQL/non-relational databases, the term database has returned to being more general, and potentially more ambiguous, since a shared model for understanding the data cannot be assumed. Prior to the foundation of relational theory, the modeling of data in other systems varied from system to system and did not have shared guiding principles as the relational model has - other kinds of databases such as hierarchical databases and network databases were used. 

Because it's a system, you will have a LOT of interacting subsystems, including power, environmental controls, communications, hardware, and then on top of that the data management and everything else. There are facets which cut across all these layers, like security, redundancy, business continuity, etc. Just from the database side, you would need to consider the transactional load, the architecture for balancing the load, the storage requirements, the response time requirements. Everything depends upon the requirements - of which there will be many from regulatory bodies, clients, users, business partners. As to whether cloud-based databases, I don't think any of those can be viable if your organization has to be responsible for the system. Any time you outsource anything, you limit your ability to be responsive to requirements. Some things you delegate are easier to mitigate than others. For instance, you decide to use a software library as a component in your system - you can always modify the library if you have the source, or rewrite it yourself or swap it out. Or picking a RDBMS like Oracle or SQL Server where you can get a vendor to do a proof of concept and have a high level of support because the sale is large. That's not to say by relaxing your requirements, you couldn't use Google Apps BigTable or Amazon or Azure or whatever, but the simple fact is that even with a contract, you have a limited amount of control over another organization's system and you are relying on them. All systems are built by balancing tradeoffs. 

Given the information you've provided, and with just general normalization being the goal, I would probably simply add nullable columns, but you haven't given enough information about how the data will be used to know what the best way to model the data is. Depending upon how you are really using this data, you may want to consider a different data model. If you are putting this data for reporting, you might want to be looking into a dimensional model, which can be more efficient for certain types of reporting - for instance time-of-day analysis works well with a date and time dimension split out. For answering analytic questions, like "what is the most popular time of day for visits from campaigns like X" or "what day of a campaign do we see the most visits per hour", a single data-time column is not going to work very well (but this can even be split in a relational model), and there are a lot of cases where you might treat the IP address as a dimension (perhaps with some kind of geography data in a snowflake). 

Is the reason you can't use IDENTITY because there are already foreign key relationships between separate tables you are loading? And there is no other natural key for you to be able to link them up in an operation from a staging area to the production area? For that reason, I'd like to know a little more about how they are currently "linked" in the source system before you bulk copy? Do multiple source systems simply use their own sequences and have the possibility for conflicting sequences when brought into a shared database? The COMB ID/sequential GUID technique is one which I am familiar with, and it is workable any time you effectively need that global uniqueness assigned outside the database - it's effectively a usable row identity both inside and outside the database. For that reason, in highly-distributed environments or disconnected scenarios, it's an OK choice Except if you really don't need it, Because that extra width difference is significant when the size of the data grows and these keys are in every index and the working sets for a lot of queries. Also, with generation distributed, if the rows don't actually come in the order of the GUID column, the issues with using this for the clustered index key (narrow, static, increasing) potentially causing some fragmentation compared to clustering on an IDENTITY still remain. 

If you are already talking about splitting and computing, don't store this as an array. Regardless of the relational theory and traditional normalization rules and dogma, it's simply a design which gives you MINIMAL flexibility. Make each exam result a row. I'm not trying to anticipate everything, but there are a very large number of things which this more granular (and, yes, normalized) and only ever so slightly more space expensive design facilitates which you may or may not need now and may or may not need in the future: 

A lot of problems with what you are trying that I just don't think this particular approach is going to work (I think the goal is interesting): - this is basically schema-bound. You can't alter the schema of the returned table - I see later you are going to attempt to "ALTER" this variable - that's just not going to happen. Even if you could, this part: 

Does the data actually need to be replicated in the same model? What will you be doing on the MySQL side with the data? I would strongly consider putting either another database or instance which you control on the SQL Server side to be able to take snapshots on that side so you pull less data down (comparing the snapshot to live data should be quick) and also look at what you want to do with the data on the MySQL side, since it is possible that you actually want to transform the data during the extraction to better facilitate the load. I think you will find that having a sandbox database or instance which you do control collocated with the vendor database will give you a tremendous amount of power, while still allowing the vendor to operate without interference. This can also help in reducing the bandwidth (unneeded columns, denormalizing usually increases data size, but not always) and reducing the load times or the complexity of the load process. A different model on the MySQL side which corresponds to the queries you will actually be running can also improve performance of reporting or even the load. Having views against the source data can also help mitigate against changes - although you might not be able to use DDL triggers and schemabinding to stop changes from breaking dependencies, you could possibly catch schema changes when the extraction fails instead of after the extraction has neglected to bring down some new columns. 

I expect this simple solution would work fine in many cases although it doesn't particularly scale (but neither does the example in your question using CHARINDEX): 

Is there an option to tell it to stop prompting me to split a file which contains multiple objects when opening .sql files from an edit window? I never want to split the file! 

Typically in relational databases, we have columns and rows, not fields and records. In relational database theory, the term relation is used to refer to a table. Rows in a table are equivalent to tuples of attributes (equivalent to columns) which are "related" to various candidate keys, one of which is identified as the primary key. 

Not sure about your specific domain, but typically in most inventory management it's going to be a little more complicated than that. Most businesses will take a physical inventory count on some kind of schedule and this may be as infrequent as annually. However, that physical will then become the new stock value, generating some kind of adjustment for the variance. In addition, sometime there is a separate release and receipt procedure which ensures that the change in inventory is booked to the correct inventory period. You should keep that in mind in any design, regardless of whether using triggers or not. 

Partitioning is only at the table level and is for managing partitions of that table. Typically the advantage of partitioning is in swapping in and out data and for getting additional control of the granularity of storage and backup. To some extent it can also help with performance if there are shared partition columns in a join, but that probably shouldn't be a reason to partition. I understand your thinking about the child tables, but if you want that data also partitioned by country (for some of the above reasons), you would have to add a column because the partition function is restricted. What is your thinking about why you want to partition in the first place? $URL$ 

The data is being compared from the local version of the table to one in the contracts linked server. I'm not certain what the goal of this code is in the first place, but I'm trying to get a handle on simplifying it a little. 

I don't seem to have a script like this in my toolbox, but found some others in my search. Have you already seen this script? $URL$ It uses a cursor, which is probably not strictly necessary. But I typically can clean them up to avoid a cursor by using the FOR XML and potentially a quirky update to concatenate the strings. Also found these: $URL$ $URL$ 

isn't going to affect the @result in the outer part - these dynamic SQL parts have their own scope and can't get to the variables in the calling part this way And then it really comes down to the error you are currently getting - which is sp_executesql can't be run inside a function anyway. Anyway, to aid you a little in making progress, may I suggest you read the following two articles which are are tangentially related (and I have used recently) regarding parsing CSV and JSON data. Both use his hierarchy idea (an unpivoted name/value thing) and repivoting and you might find some useful techniques there: $URL$ $URL$ If you can use the CLR, this article which was published might also be useful: $URL$ 

You can't have more than one primary key on a table - I assume you mean foreign keys. There is no best way to do this, because it depends upon your workload. In some indexes one column may come first, while another may have a completely different selection of columns, let alone order. In the end it doesn't really matter, because Netezza doesn't have indexes in the first place, so I'm not sure what you're getting at. That is a composite primary key made up of all those columns. Together they are "unique" (Netezza doesn't enforce this) and uniquely identify the row. This is typical in a dimensional design. The FKs of a large number of dimensions uniquely identify a fact - usually a date/time for the snapshot, and something like a stock ticker or customer or whatever. 

I expect I would have a drive_usage fact table with a link to a snapshot time dimension, a drive dimension, a computer dimension and the various numerical facts about the drive at that instant in time. There should probably be nothing regularly changing in the drive dimension - I guess it depends upon your definition of drive - is it a physical drive or a logical unit or what. Perhaps your "C" drive has a serial number, and it's replaced - then the dimension will expire and a new dimension is added. These things about a dimension are not really "facts", they are attributes. This wouldn't affect reporting because the data for computer X, drive C has continuity. Similarly if computer X is upgraded from dual core to quad core and so there is a change to the dimension (assuming something beyond number of cores is not tracked in a fact table, like a motherboard revision). The capacity of a drive would be in the fact table, so changes to that over time are just new facts with new dates. Sometimes you can even model changes to membership as facts. i.e. if physical drives 1-5 are in logical drive C one day and then physical drives 1-6 are in logical drive C the next, that might just be a fact change in the physical drive membership fact table. These are what some people call factless fact tables, since the only fact is the existence of the row shows membership - there is not much to be done except totalling or counting. When you get into folders, modeling the hierarchy can be a lot more tricky depending upon what you are trying to achieve with rollups. There is a lot of art to DW modeling in domains which aren't run-of-the-mill scenarios. 

We've done a lot of this, and (administrative) users were allowed to fix the translations live. (You still might want a caching layer, but I'm totally down with driving this with a real database and not resource files - it gives you a ton of power to query and find things which need to be translated, etc). I think your schema is probably fine, so I'll just pass on some stuff we learned in the hope that it's useful. One thing you have left out is phrases with insertion points. In the example below, the order is reversed and the language is still English, but this could very easily be two different languages - pretend this is just two languages who normally put things in a different order.