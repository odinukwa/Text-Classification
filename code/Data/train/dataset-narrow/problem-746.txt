It adds the switch /X86 to the Dtexec execution string. (please note that this switch will be ignored if you directly run it from the command line.) However, be aware that in that case, you are running the package in 32bit while not making use of the 64bit benefits. This shouldn't have to be a problem, just something to keep in mind. 

Okay, there is a reason why tools that can do this are expensive. It's by no way easy to do. However this blog post can help you. How to create undo update statements Since you have found the transaction, you can filter the result by using your transaction id. 

After running the script, start the snapshot agent and you should be done. Transaction Log corruption When you hit transaction log corruption and you rebuild the log file by using in Emergency mode you'll see that the publication and subscription are gone. You have to recreate these. 

For the most part of the time, the task is running on one core only. In this case the green one. This last graph is basically how it was in pre windows 2008. Questions: 

I currently have Mirroring set up between two production SQL Servers. One of these servers is a hardware SQL server (principal) and the mirror is a VM (on different server). I need to migrate these servers to Availability Groups (HA cluster NOT using shared storage). None of this is my choice. I would personally prefer two VMs, or completely identical systems. My question is: has anyone come across issues for AG/clustering between a hardware SQL Server and a VM SQL Server? Or is there any issues that you can imagine with that setup? I have had no issues with Mirroring between a hardware & VM. As long as the servers are very similar (ie: same version of SQL Server, same RAM given, data files are named & stored in the same locations, etc). My concern is AG utilizes windows failover clustering - an unfamiliar territory for me - so I'd like to know if there's anything additional to look out for. 

What is a good way to manage developers' data changes? I am using RedGate SQL Source Control to monitor changes to tables and stored procecures, so that when we do a production build, I can include all the changes in a SQL update script. My question is: what is a good way to consolidate and monitor changes made to the data? (As this is something RedGate Source Control cannot track) For example, new records included in lookup tables? And new records included in tables used for routing procedure calls. Thank you! 

That makes using application roles not an option. However, here's the link anyway since it's good to know: Application roles Your second option would be using logon triggers. Link for more info What you do is, once somebody logs in, you check which application name they are using and if it's not your EXE, you deny access by aborting the logon.. However, this is not bullet proof since the "application" string can be spoofed. It might however be "good enough" for stopping the curious people. Security should be designed in such a way that people that are able to log in to a database can only do what they are allowed to do. So your best option (but the most complicated to implement at this stage) Would be to revoke the DBO permission an give more granular permissions. If you need business logic to be forced your only option is to implement that in stored procedures and only give execute permissions to those stored procedures.. However, that might again mean chaning the EXE. 

Does anyone have preferences or biases? For myself, I use most (4): I create a new empty database using SSMS, then populate the schema of new database using RedGate. Perhaps, it makes no difference. But I would like to know from the more experienced DBAs what their opinions are. Thank you. 

I am using Availability Groups (database level) SQL Server 2017: I have set my availability group to "prefer secondary". I have allowed read access on replica. I have created a maintenance plan with full backups on all databases on replica using "copy only" and then also on primary. But now I'm wondering does AG actually make any decisions? Both Mainenance plans will run and from what I can tell won't change "copy only" to primary's maintenance plan if there's a failover. Besides just preventing me from running backups if I change the setting..... besides enforcing rules for what [another] dba may or may not be able to do: what is the point of this? Or, did I setup my backups totally wrong? 

No, members of the Sysadmin role or the database owner role bypass the permission checking algorithm. So no matter what permission are granted or denied , they are able to always access the objects. 

What where the exact steps you took, from the moment you found a problem until the moment you discovered that replication didn't work? What was broken, what was repaired, what was lost? All of this information is available in the output of . What exactly do you mean by: "Replication don't work?" Did you get any errors, or are tables not the same? Is data missing? 

Ever since Windows 2008 I've noticed the following behavior: I'm using a windows 2008R2 VM with two vCPUs but this also works on a physical server. I'm using SQL server 2008 R2 for this example. Make sure you have processor affinity set to AUTO (the default): 

However, if you want to open symmetric key by decrypting with the certificate the dbuser opening the key would need CONTROL permission on the certificate: 

I am attempting to setup a Linked Server from MS SQL Server 2012 to PostgreSQL 9.3 via Linked Servers & ODBC driver from PostgreSQL. Everything works, until a given query invokes MSDTC, at which point I get an error like this on the SQL Server machine, and the query utterly fails: 

I am trying to understand best uses of PostgreSQL replication and how it works so I can troubleshoot in a production environment. I am having a hard time understanding the differences between these 2 types of replication in terms of (1) Configuration (2) How the 2 servers Master/Slave perform in each scenario Replication on PostgreSQL (9.2+) is essentially XLOG files of 16MB in size (depending on frequency settings for creating each file) are being created on Master and sent by some method to the Slave. My Setup (for purposes of this question) Configuration of Postgresql.conf on Master archive_command= 'rsync -av %p postgres@[SlaveIP]:[wal_archive_folder]/%f' Configuration of Recovery.conf on Slave to read log files restore_command = 'cp [wal_archive_folder]/%f \"%p\"' primary_conninfo = 'host=[MasterIP] port=5432 user=postgres' My question is what part of this configuration makes this "streaming" replication versus "log shipping"? My master is configured to use rsync to send logs to the slave (is this log shipping?) My slave is configured to be able to connect to the master in recovery.conf (is this streaming?) Second part of the question: What is happening? I understand there is another protocol on PostgreSQL via WAL_sender & WAL_receiver. But I am not clear if this is used for streaming only and if so, how is the rsync being used in the Master? :) Thank you!! And sorry if this is an obvious question. I've been doing a bunch of reading blogs/books but having a hard time understanding. Postgres wiki is so in depth that it takes a long time to get through it all (and I have deadlines) 

Initial size is not just 3MB, it is taken from the model database (if not specified during the creation of your user database.) So assuming you haven't specified a initial size during the creation of your user db and you haven't altered the model database file sizes after you have created your userdb you can do the following: 

That way, SQL Server is able to parse the statement and will notice that it's a command that contains sensitive information and will not show it in any trace output. it will show up as: 

If you create symmetric keys that's encrypted by a certificate (that is created by another db user) for example: 

It's by design. I can understand why it could be confusing to be able to see partial meta data. However, the idea behind the data role db_datareader is to grant a user read access to both user data as well as column information. (you wouldn't be able to create a SELECT statement if you didn't have access to the column names as well.) So with that in mind, showing only the part of the index definition that contains column info would be perfectly within the boundaries of the datareader permission. The filtered index clause however is considered non column information so therefore you need more permission to be able to see this part of the definition as well. You need VIEW DEFINITION permission on that object, or VIEW ANY DEFINITION to be able to see that extra bit of meta data. I think it would be nice if they would give a warning when you generate a " incomplete" CREATE script. You can test it yourself: 

I have installed postgresql 9.3 on ubuntu 14.04 using "apt-get install postgresql". Everything was going well until I discovered that I do not have access to the "pg_ctl" commands. Installing the postgres-xc literally breaks postgresql server. The package links become broken. I cannot re-install and have had to revert my VM and keep working without pg_ctl. My question is, to get access to this package, should I start from scratch or uninstall and re-install postgresql? What should I have installed in the first place? Alternatively, is there something wrong with my path? (I am new to postgresql and have been tossed into it for work purposes) 

After enabling TDE Encryption my database backup file .bak is 3x larger than before. Is there a way to improve this? Is this due to compression not being as efficient on an encrypted file? 

The DLL is in fact that location, so the registry seems to be pointing to the right file. The ODBC driver is 64bit and so is my OS. "File=%2" is pointing to something on the d drive, which doesn't make sense to me, since d drive is a DVD. MSDTC is running... what am I missing? I have toggled Linked Server Properties "Enable Promotion of Distributed Transactions for RPC" to both "True" and "False" and this doesn't change the issue and does not produce a different error. Otherwise, scouring the Internet has brought me nothing. Last thing to point out, my query isn't actually doing any updating - it is just pulling data. So I'm not sure why MSDTC get's invoked in the first place... 

Welcome to MS SQL :-) To start with your question: Yes there is. First you make a full database backup: 

Keep in mind, that you should evaluate this over time. Your workload, (data mod) might change over time. Also keep in mind what your current problems are. Page plits cause extra load on the log file (potentially a lot). Extra writes to the data files (higher checkpoint peaks). And read ahead reads become less effective because of fragmentation. However, low fill factors, eat up buffer space, increase read load. One thing to be very careful about is that the pagesplits/sec counters Aren't really reliable. They keep track of all page splits. (sql 2012 is better) What I mean by all is that when you insert a record at the very end of an index, effectively you need a new page added to the index to continue. The adding of the new data page is counted as a pagesplit as well. But in reality nothing is split. So in order inserts on a cluster index will still have page splits when you look at the pagesplit/sec counter, but they aren't bad. So only look at this counter in relation with other facts. For example: Unexplainable high trans log load together with high out of order inserts together with high page splits/sec. One last word of warning. Especially on large tables. Always check what the current page density is of your index and calculate what the size impact would be when you rebuild the index with a different fill factor. (Rebuilding an index with a 100% page density with a new 50% fill factor will effectively double the size of the index)