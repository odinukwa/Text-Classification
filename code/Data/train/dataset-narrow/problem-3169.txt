Assuming that the rows are in same order that you wish to merge all of the dataframes, you can use the concat command specificying axis=1. 

If the row index for each of the data frames are different and you want to merge them in the current order, you can also apply ignore_index: 

By running a logistic regression model, the objective is to get the chance of a binary outcome based on the different predictor variables. Since the result is a chance, what is modeled is actually: $$ log \left(\frac{P_{+}}{1 - P_{+}}\right) = \beta_{0} + \beta_{1}x_{1} + ... + \beta_{n}x_{n} $$ So the coefficients you get from R are the $\beta$ values. The function you have so far in Excel computes correctly the right hand side (RHS) of the above equation, but to get the probability $P_{+}$, you need to solve the equation for it. Let's call the RHS, $\mathbf{Bx}$ $$ log \left(\frac{P_{+}}{1 - P_{+}}\right) = \mathbf{Bx} \\ \frac{P_{+}}{1 - P_{+}} = e^{\mathbf{Bx}} \\ \frac{1}{e^{\mathbf{Bx}}} = \frac{1}{P_{+}} - 1 \\ \frac{e^{\mathbf{Bx}} + 1}{e^{\mathbf{Bx}}} = \frac{1}{P_{+}} \\ P_{+} = \frac{1}{1 + e^{-\mathbf{Bx}}} $$ To your results, apply this transformation and you should get the same results as the predict function in R. 

If you choose to build a model where one of the categorical features such as a gender plays a big role in the outcome, then the overall model results will give a large weight to the gender, and other features will have a smaller weight. While your model will be applicable for the unsegmented population, the predictive power might not be very good. Now, suppose that you wanted to identify if the weights of the other features differed for males vs. females and if the segmented models had better predictive power, then by segmenting the population and creating models for each gender, you could explore changes in the magnitude or direction of the weights of each feature & evaluate the goodness of fit of your models to see if there is an improvement in the segmented population models vs. the overall model. If performing logistic regression, you could scale the coefficients or Z-values returned in the model parameters for each model and check if the relative magnitude & direction of the overall vs. the segmented models gave you similar or different results. You could also look at the confusion matrix and use various accuracy measures to assess if the segmented models performed better, worse or the same as the overall model. In this way, you could determine whether for the outcome independent gender-based models are more appropriate for your question than a single overall model. But, this leads to the question of if I have "n" features, should I create multiple models by segmenting the "x" categories in each of the "n" features? This really depends on the data question being asked. In some cases, it is valuable to run multiple models where each feature is dropped one-by-one to assess feature importance. In some cases, just evaluating the weight & relevance of each feature from the overall model is sufficient. Going beyond, one could look at optimizing models by choosing which of all "n" features should be segmented to yield the model with the best predictive power. This paper from FICO describes one methodology. This article details an approach and comparison of various models. 

I am assuming that you are referring to this Stackoverflow post that mentions to add noise to the data since the error seems to be coming when there is one (or small) instance of a class in the dataset. Is that the case with the training data? If what you're trying to predict is a rare-event, then a suggestion might be to balance the training data by oversampling the rare class (hence adding noise). Provided the above is not working, another suggestion is to remove infrequent terms in your term-document-matrix using the function . Going beyond, given the amount of training data you have, it would be good to evaluate if the term document matrix with the words it contains or frequency of specific words is sufficient to differentiate the classes. If not, you should consider adding new features to describe the dataset. Few suggestions: 

One of the references I mentioned in the OP led me to a potential solution that seems quite powerful, described in "Privacy-preserving record linkage using Bloom filters" (doi:10.1186/1472-6947-9-41): 

The #1 most important thing is to explicitly document your process. Different people working on different data make different choices. Any scientists who claim that their work is entirely objective and rational are in denial about the nature of science; every single model we create is informed by our biases and perspectives. The key is that we each have different biases and perspectives, which is one reason that science is the pursuit of a society, not of individuals, and underscores the importance of peer review. You have already identified some trade-offs between an algorithmic solution and individual/case-by-case corrections. If we have training as scientists, we may be used to thinking that everything has to be applied as an overall rule, so we may bend over backwards to find correlations and fit curves to data that just won't reasonably accept it. Applying an algorithm to the entire data set can be powerful when it works, but it's not the only tool at your disposal. Sometimes you need to use your judgment as the scientist. But, if you use your individual judgment to make exceptions and to manually correct your data, be prepared to defend your judgment and argue your case. Be prepared to show that you considered all the data. If you're going to manually correct 8 observations out of a set of 100,000, you need to do more than justify those 8 manual corrections - you also need to justify not correcting the other 99,992 observations. You can still take a methodical approach, perhaps by classifying your data in a systematic way, and choosing which subset of the data to apply your cleaning algorithm to based on that system of classification. And when you do this, you document it, you make your case, and you respect the judgment of your colleagues in the field. On the other hand, why do all this extra work before you know it's necessary? Plenty of "dirty" data sets will still produce useful results. Perhaps 0.5% of your data is "dirty" in a way that you know will have a bad influence on your end product. Well, any analysis is subject to error, and the more you obsess over that 0.5% the more you will start to think of it and treat it like it was 5%, or 25% - much more significant than it truly is. Try to apply your analysis while admitting you know there is some error, and only do extra work to clean the data once you can show for certain that your analysis fails or is not useful otherwise. For example... Say you have a set of test results from a survey of wells, showing the concentrations of certain dissolved substances, hourly over the course of a year. And you observe in this set certain spikes, of short duration, and orders of magnitude higher than the surrounding data. If they are few, and obvious, and you know that the sensors used to produce the data set occasionally malfunction, then there's no reason to apply an algorithmic solution to the entire data set. You have a choice between excluding some data or modifying it. I recommend the exclusion route whenever possible, since you are making fewer assumptions when you don't have to additionally choose a "correct" value. But if your analysis will absolutely fail with a discontinuity, there are many options. You could choose to replace "bad" values via linear interpolation. You could hold constant the previous value. If there is some great previous literature to apply, and you really have a strong case that the "bad" data is purely due to equipment malfunction, perhaps there's an established model that you can apply to fill in those regions. There are a great many approaches, which is why the most important thing is to document your process, explicitly and exhaustively. Arguing your case is important too, but not strictly necessary; the community is just more likely to ignore you if you don't make a full effort to engage the review process. 

I got a similar task. Maybe you can tune your trade-off by designing an appropriate cost function. Or, see F Score on wikipedia. Reinforcement learning could also have tools you are looking for, namely, means of rewarding 'acting only if sure'. See e.g. the intro in Sutton, Barto Or, try searching so called Conformal Prediction. Venn Prediction is an extension of the original Conformal Prediction framework... 

I am building a deep neural network based binary classifier, with single output. The loss function I actually want to minimize is $$ \mathcal L(\hat y,y) = \begin{cases} 0, & \text{if $\hat y$ = 0} \\ 1, & \text{if $\hat y$ = 1 & $y$ = 0} \\ \gamma \approx -0.7 , & \text{if $\hat y$ = 1 & $y$ = 1} \end{cases} $$ where $y \in \{0;1\}$ is sample's label, $\hat y \in \{0;1\}$ - classifier's output and $\gamma$ - a hyperparameter. I think this is a case of assymmetric loss. (One can see it as betting: no reward when not betting, stake 1\$, payout 1.7\$ if bet wins) From what I know by now, this loss function is probably not suited well for backpropagation and gradient descent. Question: Is there a better-suited formulation? The often used cross-entropy loss doesn't allow for trade-off tuning between precision and recall. LINEX and LINLIN are assymetric by design purpose, but I coundn't find an example of a deep NN trained with them. An alternative could be leaving the loss function as it is and resorting to SPSA, but I'd like to keep it simple if possible. Edit: I came up with $$ \mathcal L(\hat y,y) = - (\gamma \hat y)^{y}(-\hat y)^{1-y}$$ At the moment, I have no clue if it is going to work for NN learning. I'm concerned (maybe unnecessarily) that, without logarithm, it is not convex regarding NN weights. (Last layer has sigmoid activation.) Here is the log loss shown for comparison. $$ \mathcal L(\hat y,y) = - (y\log \hat y + (1-y)\log(1-\hat y)) = -\log[\hat y^y (1-\hat y)^{1-y}]$$ 

Intuitively, I'd say that these two applications are quite different and, due to the purpose stated above, the classifier has to be tuned differently, too. In the second case it should be more inclined to 'cherry-picking'. Question: Are there ways to encount for costs of type I and type II errors and to maximize an arbitrary performance measure when learning a classifier? Is it possible to incorporate the abovementioned performance measure into the cost function, while keeping NN training computationally realistic? Reinforcement learning sounds like an option, too (agent has possible actions "do nothing" and "classify" and seeks to maximize his total reward), but I wonder if I can solve this just by means of supervised learing. I also thought that unequal reward magnitudes for true and false positives call for boosting correspondingly. Is this viable and described somewhere? 

What techniques are currently available for e.g. locating the ZIP code on the envelope and cutting it to single digits prior to feeding them to digit recognition algorithm (trained on MNIST dataset)? Or imagine a task of automatic recognition if a phone call recording contains a sound of a train passing a track switch (just yes or no). If present at all, it can be gentle or loud, depending if a window was open. It can be slow or fast, depending how quick the train was going through the switch. I found so far Spatial Transformations Networks mentioned in this comment 

Imagine a [neural network] binary classifier. Presented person's photo, it should output "positive" if person is wearing dark eyeglasses and "negative" otherwise, including 'beeing unsure'. Tricky part: there is a positive reward for correct classification and a negative reward for a wrong one. The purpose is to maximize the reward-to-variability ratio Now imagine two possible applications. First one: 

Padding the front and back with spaces seems to be generally optional when constructing n-grams; the examples given in the paper that proposes this method use such padding. Each n-gram can be hashed to produce a Bloom filter, and this set of Bloom filters can be superimposed on itself (bitwise OR operation) to produce the Bloom filter for the string. If the filter contains many more bits than there are hash functions or n-grams, arbitrary strings are relatively unlikely to produce exactly the same filter. However, the more n-grams two strings have in common, the more bits their filters will ultimately share. You can then compare any two filters by means of their Dice coefficient: 

If you expect (or find) that nodes are requesting the same data more than once, perhaps you could benefit from a caching strategy? Especially where some data is used much more often than others, so you can target only the most frequently-used information. If the data is mutable, you also need a way to confirm that it hasn't changed since the last request that's less expensive than repeating the request. This is further complicated if each node has its own separate cache. Depending on the nature of your system and task(s), you could consider adding a node dedicated to serving information between the processing nodes, and building a single cache on that node. For an example of when that might be a good idea, let's suppose I retrieve some data from a remote data store over a low-bandwidth connection, and I have some task(s) requiring that data, which are distributed exclusively among local nodes. I definitely wouldn't want each node requesting information separately over that low-bandwidth connection, which another node might have previously requested. Since my local I/O is much less expensive than my I/O over the low-bandwidth connection, I might add a node between the processing nodes and the remote source that acts as an intermediate server. This node would take requests from the processing nodes, communicate with the remote data store, and cache frequently-requested data to minimize the use of that low-bandwidth connection. The core concepts here that may be applicable to your specific case are: 

Where is the number of bits that are set to 1 in both filters, is the number of bits set to 1 in only filter A, and is the number of bits set to 1 in only filter B. If the strings are exactly the same, the Dice coefficient will be 1; the more they differ, the closer the coefficient will be to . Because the hash functions are mapping an indeterminate number of unique inputs to a small number of possible bit indexes, different inputs may produce the same filter, so the coefficient indicates only a probability that the strings are the same or similar. The number of different hash functions and the number of bits in the filter are important parameters for determining the likelihood of false positives - pairs of inputs that are much less similar than the Dice coefficient produced by this method predicts. I found this tutorial to be very helpful for understanding the Bloom filter. There is some flexibility in the implementation of this method; see also this 2010 paper (also linked at the end of the question) for some indications of how performant it is in relation to other methods, and with various parameters. 

The article goes into detail about the method, which I will summarize here to the best of my ability. A Bloom filter is a fixed-length series of bits storing the results of a fixed set of independent hash functions, each computed on the same input value. The output of each hash function should be an index value from among the possible indexes in the filter; i.e., if you have a 0-indexed series of 10 bits, hash functions should return (or be mapped to) values from 0 to 9. The filter starts with each bit set to 0. After hashing the input value with each function from the set of hash functions, each bit corresponding to an index value returned by any hash function is set to 1. If the same index is returned by more than one hash function, the bit at that index is only set once. You could consider the Bloom filter to be a superposition of the set of hashes onto the fixed range of bits. The protocol described in the above-linked article divides strings into n-grams, which are in this case sets of characters. As an example, might yield the following set of 2-grams: