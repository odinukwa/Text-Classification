I suggest training/testing your classifier on separate splits of the original dataset, and then printing a confusion matrix: $URL$ This is a way of seeing how many of the 'true' classifications your classifier predicted correctly or incorrectly, and the same for 'false' classifications. This will give you more information than just 'accuracy', because a model trained on data where most of the classes are 1, for example, will predict 1 most of the time because it will probably report reasonably high accuracy in doing so. A confusion matrix is like a sanity check for this. 

I think it makes sense to stick with Classification here, since you already have examples of fraudulent and non-fraudulent calls that you can train on. It might also be beneficial to train several models for different regions based on IP, as well as your 'global' model, and apply the region specific and global models to incoming calls. Just a few ideas. From what I understand, real-time learning would require immediate feedback, which most fraud detection systems can't provide. For example, it may take a few days or weeks to have a case of fraud resolved (labelled fraud/not-fraud), and therefore take some time for the learning system to receive the feedback on it's prediction. 

The concept of state-action values $Q$ is to denote how good is to be in a particular state and perform a particular action in terms of expected future reward. From what I understand from your question, you are interested in the problem of model uncertainty (uncertainty on the dynamics of the system). In other words, our artificial agent interacts within an unknown environment (transition dynamics $T(s,a,s')$ and reward dynamics $R(s,a)$ or $R(s,a,s')$ are unknown). The framework you should take a look at is Bayesian Model-based RL. I outline an approach so you can have an idea: Modelling Transitions First assume that we have uncertainty on the transitions of the environment $T(s,a,s')$. To tackle this we will assume that our agent maintains a distribution over possible transitions. Without getting into the theoretical math, I will illustrate this by using a simple Dirchlet-Multinomial model: The states are sampled from a Multinomial likelihood $s'\sim Mult(p_{ss'}^{a})$ and we assume a prior over the transitions $p_{ss'}\sim Dir(\alpha)$, where $\alpha$ is set to $1/|\cal{S}|$, where is $\cal{S}$ is the state space. The posterior over transitions will be also a Dirichlet because of the conjugacy of the likelihood and prior distributions. To update such a posterior you need to perform simple algebraic calculations and maintaining the counts of each transition. The Algorithm The agent does two processes: 

Example: I am currently trying to include RunnerX-RunnerY past race data by counting all the races that RunnerX and RunnerY have run together and normalizing them on a scale from to ; indicating RunnerX lost all past races against RunnerY; and indicating that RunnerX has won all past races against RunnerY; and indicating. And indicating an equal number of wins and losses (or no past races against each other). For instance, if RunnerA is racing RunnerB, and RunnerA has beat RunnerB in the past, then I want the algorithm to know that (denoted by a on the RunnerB column of row RunnerA); same for vice versa. Taking it another step further, If RunnerA is racing RunnerC (but the two have never raced each other in the past), and RunnerA has beat RunnerD in a past race, and RunnerD has beat RunnerC in a past race, then I want the algorithm to learn that RunnerA should beat RunnerC. I say beat here, but I mean an "average beat" for any RunnerX-RunnerY combinations when data for more than 1 past race is available. I have set my data up as: 

I might be a bit late for answering but hope it helps!I assume that you are familiar with RL so I will omit lots of details (please if you are still interested comment so I can help you). Neural Networks and RL: You have two options. The first one is to use a network which you will have as input your feature vectors (states) and output probability of each action. This is called a policy network and you can find a very detailed tutorial with Python Code in order to implement it by A. Karpathy. Your second option is to use the Q-Network approach. Your input will be again the same but the output will be values of your Q function for each action you have ($Q(a_i)$). You will use the Q-learning equation $Q(s_t,a_t)=Q(s_t,a_t) +\alpha[r_t+\gamma \max _a'Q(s_{t+1},a')-Q(s_t,a_t)]$. The details of the implementation can be found in the paper of V. Mnih. Also do not worry about the delayed rewards as the discount factor $\gamma will "help" your agent to be affected by the future rewards. In order to calculate your states, I would suggest you to create a simulation of the environment and a step function. You don't mention what kind of game you are dealing with but the general idea is that the step function will take as input your current state and current action and output the next state and reward (don't mind about the continuous space of your features as you can discretize it - you can use Kalman filters or other models to have a better state estimation as well). My advice would be to choose your approach (Policy net or Q-net) and read the blog or the paper, create a simulation of the environment and a step function for your game. You can find tons of implementations of the Deep Q-net although I would suggest you to start with a very simple network so you don't get in trouble by tuning the Deep net. 

I'm not a business analyst so I guess you'll have to take what I have to say with a pinch of salt. From my understanding, Business Analysts responsibilities are focused around improving processes within a company, for example how certain technologies could be implemented to improve a workflow, they are expected to understand how these technologies might improve the workflow, or a product etc, and manage these improvement projects. This seems to differ from Data Science on an abstract level in that it is exploring known unknowns ("can our process be improved? what technologies/methods exist that could improve it?"), whereas Data Science is great for exploring unknown unknowns. For example, why is this better for our workflow/product specifically? Data Science is great at throwing up results you don't expect, which is one reason why it is so valuable. I may be wrong in saying this, but Business Analysis seems to be relatively free-form depending on the company and the needs, whereas Data Science has a less subjective methodology. With this in mind, perhaps Data Scientists could be used to better inform your Business Analysts decisions? But the other way around, BAs could perhaps be used to better inform Data Scientists of business processes, or maybe your BA could focus on improving processes to make life easier for your Data Scientists such as data pipelines in a non-automated environment (example: how can we gather more useful data from our vehicle showrooms? What technologies do the DSs need and how can we implement them?). 

I am not very sure what do you mean as input. In Supervised Learning the learning signal comes from the difference between true response and model's response ("teacher's supervision"). In Reinforcement Learning the learning signal comes from the reward which might come delayed, sometimes not at all in that particular trial (but in another yes) etc. Deep-Q learning which basically is Q-learning with function approximation is a Model-free RL. It means that at the end you want your system to learn a mapping between stimulus and response. Think of it as a reflex elicited by a stimulus. The reward cannot be the input to your system as it is your learning signal. If you are referring to the experience replay, as I mentioned to you, the reward sometimes doesn't come "on time". So we need to decorrelate states,actions and sequences and that's why we don't update the network at every single time step. Instead we prefer to build a buffer with experience and sample from that. So as you stated, you want to learn to avoid enemies not locations and for this if you sample experience from the buffer the network's training will be more "intuitive". 

I'm fairly new to machine learning, but I'm doing my best to learn as much as possible. I am curious about how predicting athlete performance (runners in particular) in a race of a specific starting lineup. For instance, if RunnerA, RunnerB, RunnerC, and RunnerD are all racing a 400 meter race, I want to best predict whether RunnerA will beat RunnerB based on past race result information (which I have at my disposal). However, I have many cases where RunnerA has never raced against RunnerB; yet I do have data showing RunnerA has beat RunnerC in the past, and RunnerC has beat RunnerB in the past. This logic extends deeper as well. So, it would seem that RunnerA should beat RunnerB, given this information. My real concern is when it gets more complicated than this as I add more features (multiple runners, different distances, etc), and so I'm turing to ML algorithms to help my predictions. However, I am having difficulty figuring out how to include this in my row data that I can train (after all, correctly formatting data is 99% of proper machine learning), and I am hoping that someone here might have thought along the same lines in the past and might be able to shed some light. 

To my extent of my knowledge, RL is used as a model for attention mechanism in object detection field (particularly the REINFORCE algorithm which is a 'flavor' of policy gradient methods). You can take a look at one of the papers that first proposed the method: RAM 

I think your best approach is to use Imitation Learning. Many techniques in imitation learning use Supervised Learning so you do not need to use emulator. Check DAGGER which is used in continuous action scenarios or the recent AggreVated algorithm (just ignore the theoretical parts of the paper). As a start you can use Supervised Learning just for experimentation and then use the above algorithms. I would suggest though to use even a poor simulator just to have an idea of how your implementations behave after the. Bare in mind that RL tries to solve an optimization problem (maximize an utility/cost function) whereas the Supervised Learning methods try to optimize the difference between model's prediction and ground truth. Just be cautious of the algorithm's behavior before you go live. 

Stochastic Policy Gradients (SPG): Output is a probability over actions. For your algorithm, the output would be the parameters of a pre-specified distribution (usually Gausian) as Neil described. Then you sample that distribution in a similar way you sample the Boltzman distribution. Deterministic Policy Gradients (DPG): Output is the value of an action (e.g. speed, height etc).