I'm not sure why you want asynchronous, but a couple of indexed view sounds like just the ticket here. If you want a simple SUM per some group that is: define running total. If you really want asynchronous, with 160 new rows per second your running totals will always be out of date. Asynchronous would mean no triggers or indexed views 

In addition to Shawn's answer and your comment.. Tables that are maintained by stored procedures or views only do not require any explicit permissions. "Ownership chaining2 states they are not checked: this includes a DENY. I do prefer schemas though but I also disallow direct table access. For a given schema (Desktop, WebGUI etc) then I assume all clients with permissions need to use it. Within that, I'd have some application security too though eg based on SUSER_SNAME() or such to track users. 

OBJECTPROPERTY is local to the database the query is run it. So the passed in is resolved against : but the object_id comes from So here you have case 2. On my server, I have 37 matching object_id values between and . But the names are different. 

What do you expect your real life volume of data to be? For 10 million rows, I wouldn't bother with partitioning. The overhead far outweighs the benefits: partitioning isn't a silver bullet to cure performance issues. To answer, Point 1: on the first run, data needs loaded into memory ("buffer pool") and will stay cached until evicted based on memory pressure and usage. Personally, I'd test with the cache filled because you'd expect your app to require that data very often, especially if you think partitioning is the solution to some problem For point 2, what queries do you expect to run in production? The queries should be representative of this production load. However they should test different realistic filter combinations with and without partition key at least. Edit, some reading, after comments below: 

This applies more to other trace flags (eg 610 for minimal logging) that affect the optimiser and important behaviours/optimisations You should be OK with the deadlock trace flags 1204, 1205 and 1222 

Just migrate the objects with the "ssrs_migration.rss" script. Much easier than database keys and restores 

Next, downoad Red Gate Compare tools or something like Embarcadero SQL Change manager. You can't easily migrate without it. The cost is trivial to the amount of time saved. And most importantly, the scripts generated make changes in a single transaction which means a clean deployment Now, 

This forum or ServerFault. In the 13 years i've been working with SQL Server since 6.5 I've frequented several (SQL Server Central, SQL Server Performance etc) but it's hard to discern quality answers on these. The reputation system on the Stack Exchange sites is a powerful tool to ensure higher quality answers as well weeding out the poor ones. I can't understand your worries about upgrading: unless you have some code that (ab)uses sp_OA% or such you should be OK. The Upgrade Advisor will assist you here 

You can't delete the LDF file: it's essential. For the NDF, you need to show that it's not used to SQL Server. You'd use DBCC SHRINKFILE with EMPTYFILE. Substitute xxx based on sys.database_files 

All you need is persistence of your XML. Use a NoSQL solution or the file system. There is no benefit in using an RDBMS, unless you want to use it instead of NoSQL or the file system. 

Requirement 2 requires the data be stored "normally": that is, you can't use encryption in the client for all data. Encrypting data in the client also contradicts requirement 1 Requirement 3 requires the media or the actual backups are encrypted. Encrypting the HDDs isn't reliable because once someone has the actual media then it can normally be unencrypted by a sysadmin. So, I'd suggest using a 3rd party tool like Red Gate SQL Backup Pro or LiteSpeed by Quest to secure your backups. Points 1 and 2 are satisfied because the on-line database is unchanged 

Note: UQ1 is a unique constraint used by the FKs in Cat and Dog. It is a "super key" on Pet that allows sub-types via the FK Again, you can have a link table PetOwner (multiple owner) or OwnerID as an FK in Pet (single owner). Note, PetType can be a constant derived column in Dog and Cat Edit: added tables for Pet, Cat and Dog. I assume Owner is obvious... 

"a function on a WHERE predicate" means indexes won't be used If you batch it (say on UPDATE TOP (10000) ... AND costPercentage IS NULL) then you need an index on costPercentage and this assume you are setting it. The only solutions I see are 

Finally, consider how you use addresses. You add or update them. You don't look for an existing address and link to that. Each address is per source only. 

The CREATE INDEX is unit of work that follows ACID. So when it completes, it has completed (or failed) completely ("Atomic" in ACID). No further work is needed to populate or otherwise prepare the index ("Consistent" in ACID) Whether it improves performance depends on it's suitability for the queries you are running, data distribution etc. That is a whole new area... 

None of performance, stability, optimization are true. Does anyone have a solid argument or reference article why these would be true? Resources are not allocated to a database: the SQL Server Instance balances resources so it makes no difference You lose: 

In addition: - added or fk should be first - ID is first = not much use Try changing the clustered key to and drop . You've already tried . If nothing else, you'll save a lot of disk space and index maintenance Another option might be to try the FORCE ORDER hint with table order boh ways and no JOIN/INDEX hints. I try not to use JOIN/INDEX hints personally because you remove options for the optimiser. Many years ago I was told (seminar with a SQL Guru) that FORCE ORDER hint can help when you have huge table JOIN small table: YMMV 7 years later... Oh, and let us know where the DBA lives so we can arrange for some percussion adjustment Edit, after 02 Jun update The 4th column is not part of the non-clustered index so it uses the clustered index. Try changing the NC index to INCLUDE the value column so it doesn't have to access the value column for the clustered index 

Sorry to say it, but best practice is to always qualify object names with the schema anyway. This is mandatory for things like schemabound views. Why is it best practice? See this by Tibor Karaszi or this by Midnight DBA or just trust me or the MS SQL Server Best Practice Analyzer After comment... Have you considered synonyms to make OtherSchema.mycode point to [DOMAIN\Glen].MyCode? 

For more info about on-disk structures, see Inside the Storage Engine: Anatomy of a record. Interestingly, where is the 4 byte row header? This appears excluded from the 8060 bytes. Tested on SQL Server 2008 SP1 and SQL Server 2005 SP3 

No experience of this and don't know of any alternate reference, sorry. I tend to stick with DMVs because DTA you needs db_owner. On prod (as a Developer DBA, not a Production DBA) I don't have permissions to run this... 

Unfortunately, I can't remember the exact details now. It's the only time in 20 years of SQL Server I've... 

You could also use computed columns to add a column that says "department" or "division" Another way: mandate that each department has at least one division... 

Does it matter, unless it is sucking resources? As I understand it (from some searches), it will run anyway. There is an MS Connect item for SQL Server 2008 for CPU usage, but it looks unresolved 

Use SQL Server Integration Services. It is that simple. We can't show you how here: it's too general 

An example, separation of duties On a web server, the password for the service account is only known by one team. It isn't required to lie around in web.config or source control. 

I've implemented both: it depends on access patterns and usage. If I'm reading all versions in one go or JOINING onto it, then one table. If "previous" data is only on demand then tables. In your case, can you defer the previous versions until a user clicks to load it? Is it rare to actually need previous versions? If yes to either, then I'd suggest 2 tables. This way, you only shift and render less data only for current versions. 

You either recreate as above or restore it. The info above is similar to MSDN, see "Creating a New msdb Database". Note: if this came from Paul Randal's blog or the SQL Server Storage Engine then you can trust it 

That is, try not to let SQL Server process the data locally. Note: On my 2nd example above, the filter is sent to the linked server because it just a string constant to SQL Server. The real WHERE clause in the first example may or not be sent remotely. Finally, I've also found that staging the data into a temp table and joining that onto local tables is often better then joining directly onto the OPENQUERY. 

Have a look at "free data models" on databaseanswers.org. For example, "Contact Management" Your question is too broad: you can hire me to do it of course :-) 

Unless you've badly coded it, then no. It is probably less then 10 writes per second. Even very write heavy databases are over 90% reads and under 10% writes. Consider though that such things can be replaced by, say, an indexed view on this table that looks like the separate table. Or the DML can be done in a stored procedure so it's all in one place for clarity. If you do hit issues, consider snapshot isolation so that the writes do not block reads. Summary: should be OK, but there are things to mitigate if not. Edit, read:write ratio