I think the big difficulty with the phonetics-phonology divide is not only that linguists don't even really agree on the difference but also that there doesn't exist a good analogy with any other pair of subfields. This is the way I've seen it (cards on the table, although there are more extreme folks, I'm fairly far on the "phonology doesn't exist" camp, and that is probably influencing my answers)... Phonology is the study of the cognitive processes that turn words into instructions to hand down to the physical body parts that produce the sounds. These instructions, personified into human commands, might sound like, "close your lips, now move your tongue to touch your alveolar ridge; begin lowering the diaphragm at a normal rate and constrict the vocal chords to this degree". On the acoustic side, phonology's role is much harder to specify (at least to me), but I would say that the "phonology" center takes in sequences/matricies of interpreted linguistic features, for example "between 442-488ms, palatalization level 2". Phonology would then turn that into the abstract "underlying" representations that can be mapped to morphological parsers and the lexicon. Phonetics is the study of how the "commands" end up translating into specific articulator and vocal tract movements. For instance, how the command to retract the tongue at some particular time "really" maps to minute physical details like exactly when tongue section X touches mouth section Y and then in turn how that affects parts of the resultant acoustic signal. Phonetics also makes observations of how certain groups of instructions can cause very specific consequences. On the acoustic side, phonetics turns the mental spectrogram we receive from the nerve endings in our cochleas into feature sets and timings of the sort that it received from the phonological center during articulation. Articulatory phonology is an attempt to consolidate the two, that, as far as I can tell, is basically phonetics taken one level deeper to receive underlying segments as inputs. And articulatory phonology moves a lot of what was in phonology proper as cognitive processes into physically motivated processes during articulation. In short, nobody really knows the difference, but the broad agreement is that phonetics is lower-level and more articulator-centric and phonology is higher-level and more cognition-centric. 

In the movie Prometheus, David, an Android tasked with maintaining the functions on the space ship while his human comrades are sleeping in cryostatic chambers, is learning PIE during his off-duty time. In one sequence he is repeating after his holographic teacher a text in PIE. One sound in the word akwunsez sounds odd, it sounds like some one is beat boxing, a sound generated with probably only the larynx and its epiglottal membranes in a loud way. You can hear it here, about the time stamp of 1:10 David is beat boxing very ancient lyrics Is this sound one of the exotic ones I asked about , are there more ? 

Do you know futher examples of languages with free word order, highly flectional languages like Balto-Slavic, Ancient Greek or Sanskrit or agglutinative languages like Korean and Japanese with a free word order except for the predicate or others that use word order to convey some additional meaning? 

I understand the field or fields have to be quiet extensive, therefore I don't ask for a description, but rather a list of the names of the linguistic subfields, theories and significant people dealing with it, maybe with short annotations. 

As for the languages I know I think to believe, that a subordinated clause comes in a chunk and not scattered throughout the main clause. For instance: 

The question points in the future and so I see the problem of having evidence but an educated guess or quotations of linguists would do, I guess. It is no secret any longer, though some nationalist Turks I know, passionate advocates of the Sun Language Theory, see it differently, languages have changed through time. Not quiet randomly, but by rules which historical linguists have discovered. However, times are different know : We ve have Jackie Chan, Star Wars, oppressive governments, prescriptive language advocates, the Kings James Bible, constitutions, TV series and all the things which urges us to speak their way either for entertainment to understand what Yoda said in plain English, for our constitutional rights to know how to read the laws and thus to know how to sit on a leather chair sponsored by welfare money instead the electrical one and all other modern reasons which keep our language in a specific shape like dinosaurs eyewitnessing mosquitoes in amber. So, what is the opinion on this the scientists came up with? 

That's actually a slightly different case in my mind. The prosodic characteristics of at (being a very weak unit) in at shoe may cause segmentation issues. While I still haven't read anything that's totally convinced me that phonemic affricates exist in English, I suspect that proponents of phonemic affricates might argue that the resegmented form is proof that /tʃ/ the cluster and /tʃ/ the affricate are distinct. Rambling aside, this doesn't appear to prevail in my dialect and the fricative in at shoe emerges like a vanilla fricative: At shoe 

You have very good instincts, because this statement is halfway to the answer. An exploration of the topic is best started in a discussion of the differences between the words shoe and chew, which differ only in that the former has a fricative initially and the latter has an affricate. The period of silence (caused by voiceless stop closures like [t]) that characterizes stops and the "stop" portion of affricates is indistinguishable from regular silence. We only ever "identify" the presence of "stoppiness" as well as the nature of the stoppiness (e.g., bilabial vs. velar) by the influences that the silences have on neighboring segments. In the case of the postalveolar fricative versus the postalveolar affricate, in the postalveolar affricate, the stop asserts its status by altering the nature (non-technical term used here; I will elaborate) of the following fricative. Let's compare my totally amateur recordings of shoe versus chew. Shoe 

What do you notice? There are two major differences that linguists have narrowed down as the cues our ears and minds use to differentiate shoe from chew: 

At this point I don't want to explain my personal crackpot theories on how names for numbers emerged and I assume that anything remotely connected with the origin of language is highly speculative and hard if not impossible being researched empirically. Nethertheless, centuries of speculation might have culminated into a few theories which might be considered an educated guess or even more. Here I limit the theories onto the origin of the names for numerals, what probably doesn't make it much easier. 

Now, gerund is being described as a verbal, which is a derived noun,adjective or adverb from a verb,more precisely a present participle which can take a few specific roles in a sentence. Is a gerund just another category of part of speeches , is it something beyond the tripartite system of levels or are there more than three levels with gerund being within one of it? 

I was told here several times, that a part of speech is not universal, but specific for each language as much as the A,T,C and G's are in everyone's genome. Nethertheless, occasionally the same terms are used for in different languages for practical reasons, e.g. Adjective in English and Japanese, even though they are similar but not equal in all respects. But what if you compare the parts of speeches of thousands of languages? Is it still the case, that no two languages have identical parts of speeches or could it be, that there is a limit for specific ones? To draw an analogy, each language possess a subset of all phonetic sounds possible across all languages as described in the IPA chart, but the number of all sounds possible would have a limit, even if millions of languages existed. Is there something like an IPA of the parts of speeches or another approach to describe parts of speeches with a closed set of universal features? 

You can even test this out for yourself by doing some basic audio editing in Praat or Audacity. When our brains hear catch it or cat shit, it is applying these two metrics to figure out which one it heard. Slow ascension to maximum amplitude and relatively long frication noise? Must be shit. This only really tells part of the story, though. Let's explore the production side a little. When an oral stop happens, the tongue (or lips) completely constrict the oral pathway. No air (and consequently sound, to an extent) can get in or out. That being said, in the transition process when a non-obstruent like a fricative or an oral sonorant follows a stop in the same syllable (this does not apply all the time, but the story is really complicated), what happens is that in the last few dozen milliseconds air starts coming out from the bottom of the vocal tract. Air pressure behind the stops exceeds air pressure outside the stop. And when the stop is released, air molecules quickly crack from behind the stop to in front of it relatively quickly (on releases of all oral stops, aspirated or not, especially into vowels, you can feel a marked burst of air on your lips or if you put your hand in front of your mouth). This is what we call the stop burst. Now, many different sorts of things can happen following the stop burst. In the case of the fricatives after stops, the closure relaxes slightly, causing the air to burst, but it remains relatively tight, and air traveling turbulently as it goes through and comes out of that tight corridor is what causes frication noise. So the story of the affricate (stop into fricative) is that significantly higher than atmospheric pressure builds up behind the closure. When the stop bursts and the tongue goes into constriction position for the fricative, that high pressure of air is released over the early span of the fricative, causing a high amount of noise. Compare this to what happens when you pronounce a bare fricative. In those cases (e.g., shoe), your tongue goes into fricative position (small constriction), and then your lower vocal tract just starts pushing air through the constriction. The noise does not begin amidst extremely high pressure behind the constriction. That lack of an air pressure gradient causes it to be quieter. The quicker climb of the affricate case causes a "critical noise/vibration" level to be reached more quickly and the tongue to retreat from its constriction position, hence the shorter duration of fricatives in general (the critical level hypothesis is not that thoroughly explored to my knowledge, but it seems to explain a lot). So that's the story of fricative versus affricate. Bring it all the way back to your case, what happens when there's a "syllable break" is important. When there is a syllable break after the stop, the pressure buildup behind the stop closure releases (sometimes audibly, depending on the dialect). Then, the tongue assumes the fricative constriction position during a period of neutral pressure, and then the air starts flowing, and the sound occurs as a regular fricative. I'm sure this is poorly edited, so I welcome volunteers who would be kind enough to correct me on all the errors I may have made. @aedia asked in a comment a very interesting question that I'll address as an edit: 

Let me clarify the question, There are traditional grammars to describe the working and structure of languages, mostly with the purpose of teaching someone to speak the languages. So, it is approach is simply educative and theoretical inconsistencies within the grammar used are neglected , it only has to work as a teaching instrument. Then, there are descriptions of languages with the purpose of describing them for further linguistic evaluations. Maybe to have a common system of description to compare languages with one another, to group them according to some criteria, to follow their development through time by observing its change of grammar, to describe their structure precisely for computational evaluation and so forth. Now, at this point I don't know how deep an answer to this question can get, if it is too comprehensive, please try to point out some relevant differences. 

In korean, consonants are divided into three categories of articulation or coarticulation, one of them is called Tenseness. In korean orthography, tensed consonants are written with a reduplicated consonant sign. In IPA orthography, they are written with a pair of vertical parallel lines at the bottom of the letter: 

Now I don't know if this contradicts some basic principle in linguistics, but a friend of mine,Aslan, who admittingly tends to be a crackpot, but has also proven to be smart, claims, that this is the case in one of the Balkan languages he speaks. The example I provide is necessarily just wrong grammar I had to make up, to exemplify what I mean. 

There are also prosodic differences that influence our judgment of dialects, but I'm not experienced enough with them to advise on it intelligently. In summary, differences between dialects are marked in so many ways including both vowels and consonants. There is no "primary" difference. That question is as unanswerable as the question "What is the primary sound in languages? Vowels or consonants?". 

Is there a specific question about whether a particular candidate can be considered a phoneme in sign language? Perhaps if you frame your question in terms of the research you've already done on the field (and it doesn't take much to start), people can build on that. That way, you can take advantage of the diverse array of backgrounds here to help you arrive at new knowledge. I feel the way the question is phrased now, it might be banking entirely on one of the 50-or-so private beta-ists on knowing sign language phonology, which might be unlikely. That being said, what a phoneme is is no different in sign language than in natural language. The fact that word phoneme is derived from a root meaning sound doesn't affect the more abstract meaning that it's taken in academic linguistics. It's the underlying representation of a minimally contrastive unit. If changing the location of a gesture and nothing else changes the semantic meaning produced (in ASL, and all other full sign languages as far as I know), then each location (or some more abstract construct that includes or influences location) is mutually contrastive. If changing the hand shape and unilaterally change the semantic meaning, then each of those shapes are mutually contrastive. Once we know all the mutually contrastive locations, movements, and hand shapes (and other things), we discover relationships between them and rely on native speaker intuition to wean this down into a candidate list of phonemes, whose compositions can deterministically produce every word in the lexicon.