The notion of a word as a reference to a set of objects fails for obvious reasons best described by Quine's discussion of Natural Kinds. Late Wittgenstein proposes that instead of a set of objects, a definition is a token in a game that transfers power in the form of knowledge. The power that a definition gives is the ability to predict the behavior of the world around you, including other people. The notion of love is no different from the notion of dog in this respect. Dogs behave a given way, they cause us to experience various things, and the people around them behave a given way because the dog is present. So does love, it 'behaves' in certain ways, increasing or decreasing and evolving in character, it causes us to experience various things and people behave in given ways when it is present. So the right way to model both in an ontology is in terms of potential and expected effects. 

I find your question confusing because Gestalt psychology usually means almost the exact opposite of what you ask for. Its contrasting approach, Structuralism, is the one that "models topics such as the self, free will, and the mind using organizations of individual mind components?" Gestalt psychology rebelled against analytic forms like Freud's and Jung's or cognitive approaches that start by looking into the mind and isolating separate parts to find out how they interact. The point of Gestaltisms in general is that the analysis into components loses so much of the character of the original that it must always be viewed with suspicion. A great example of a balance point between them is laid out in Ken Wilbur's approach to different layers of the self in "No Boundary". The analogy in philosophy seems to be between atomism and something like Leibniz or Whiteheads monadism. The former emphasizes parts and separations, the latter emphasizes that a mind is an irreducible whole that merely reflects multiple influences upon it, without incorporating them or being actually changed by them. 

There are at least three different ways to believe something, (loosely following the Lacanian version of three worlds) only one of which is to believe that it is true. (In the 'real' world.) The most practical definition of belief is ethical. A belief is a statement with consequences that one is willing to act upon. This requires no notion of truth, only an attachment to consequences. (In the imaginary world:) The most strict definition of belief is logical. Logic generally treats belief as the acceptance of the truth of a fact. And obviously this requires a standard of truth. Of course, the logic itself has to back up the notion of truth in order to make this meaningful. (In the symbolic world.) Between these is the customary, everyday definition of belief as holding a position in a 'language game'. One believes something if one is willing to reason from it, at risk to one's standing, self-image and power positions, whether or not the reasoning ultimately leads to action. This also does not require a notion of truth. What matters is various players' positions within the game and the contribution to the evolution of the game's ultimate purpose. Religious and political beliefs are the form of belief that most obviously have this flavor. Religious and political ideologies have their own language games in which adherents negotiate orthodoxy and challenge or support different freedoms of faith. The game allows individuals to trust one another and seek effects that they communally want. This last form of belief, although the most complex, and hardest to understand, is also the most common, because it is interactional and we are both obsessively social animals and highly dependent upon environmental feedback for survival. Such beliefs are, in many ways, the only kind of truly new information. We do not know facts, we merely believe them, and we only believe them within a context that gives us feedback about them and their 'truth'. That context is a language game we enter into either in our internal monolog, or with others. We seldom deal with this form of belief, as a belief, 'in the raw', however. Instead, once we conceive of something as a potential belief, we tend to internally model it in terms of the first form of belief by projecting it onto the second form, so that we can create abstract models and make predictions, or we do the opposite, and embrace it as the second form, acting with the risk of unplanned consequences so we can project them onto the first form as data about the outside world upon which to base an internal model. 

This is a basic content fallacy where one is using human psychology instead of logic. It is akin to an ad-hominem fallacy, where the quality of a person as a human being is taken to bear rationally on the quality of his arguments. And in quality, it lies alongside observations like "He keeps arguing with you because he likes you." (and not because he believes what he is saying is true.) The problem is that as reliable as some emotional reactions might be, they are logically off-topic. In this case, Z is imagining X's logic is actually defensiveness, due to some quality of its presentation, perhaps its length or tone, and one only defends oneself vigorously against arguments one considers likely to have an element of truth. So he presumes Y's argument is a valid threat to X, and therefore that at least part of it must be true. But X may be legitimately scared or angered by some element of Y's argument instead of defensively so. Or X's own opinion of the arguments' relative strength may be flawed. Or X may simply be coloring his arguments emotionally for rhetorical effect, or for other reasons entirely. Whatever the reason for the observed emotion, if it is even present, and not Z's imagining, it is not logically connected to the validity of X's argument. 

Famously, the continuum hypothesis, the axiom of choice, and a set of other potentially optional improvements of Zermelo-Frankel Set Theory are potentially true, but unprovable within the formalism of the theory itself. In such cases, effective proof is absolutely impossible for either the statement itself or its negation. Since one side or the other is classically true, we have candidates for true-but-unprovable results, but, without any agreed criterion for truth beyond language, we can only narrow them down to opposing pairs, and never pick one that is definitely true and unprovable. These, especially the axiom of choice, seem intuitive to some people, but can be used to produce counterintuitive results, so others feel we have not found the correct intuitive interpretation of them. And still others find other things intuitive, like the determinacy of matrix games, which would contradict the axiom itself. Since we cannot even agree on a criterion for what is intuitive, we cannot possibly imagine that this is a set, much less a recursively enumerable one, because the element operator is ambiguous. In his Intuitionism, Brouwer sets the bar quite high on what is intuitive. Heyting recasts some of Brouwer's 'basic intuitions' in the form of sets of axioms, and his success here indicates the remainder can probably also be captured that way, though, to some degree, putting them that way defies the intention of the program. From there we only accept what can be constructed. That means the true statements are recursively enumerable, because only stated axioms and the resulting proven statements are allowed to meet the criteria for intuitiveness. So by at least one school that attends closely to intuition, you could consider the intuitive propositions recursively enumerable. 

Not at all. First, God aside, we cannot attribute human characteristics to non-humans. Do you think it would be really boring to be a paramecium, or a cow? I see no reason one should. And I don't think the reason is that we are somehow superior to them. I don't find humans who crave excitement to be more intelligent, or even more interesting, than couch-potatoes with vivid imaginations. The former constantly imagines the latter is bored. Even from the standpoint we are God's image, we cannot attribute too many of our biological drives onto him. An image is not a duplicate, it has attributes of its medium that the original lacks. But in the end, there is a good reason for humans to get bored, we have limited lifespans and cumulative memories. Filling up that memory with experiences that are not all related increases your likelihood of discovering things and moving us all forward. That would not apply to God. In particular, from a traditional Roman Catholic point of view God causes time, and would not be subject to it, by the likes of Augustine, who translates 'eternal' as 'extra-temporal' he wouldn't even experience it. That would remove the possibility of boredom in a very complete way. For certain other forms of Christianity, like C.S. Lewis' "Mere Christianity", at some point God had to be bored, but that can't be too common. The need to be everything good is why it was necessary for God to be incarnated. Otherwise, relative to humanity, he would have the "Mary" experience from "Mary's Room" (q.v. $URL$ but not the "real" experience. And to be truly omniscient, he must fully relate to all forms of experience. (The natural Gnostic extension is that this is why Satan is also necessary, and is also God. It is quite mandatory, from a pretty common human point of view, that God understand completely what it is like to be evil, if he is going to judge evil humans fairly.) So in some sense, God was just dying to get bored, it must have been new and exciting for him! 

It is odd to see this attributed to an individual. It actually goes way back to religious Neo-Platonism and Gnostic Christianity variants like Bogomilism. They had the experience from extended harsh asceticism that acceptance is the way of the weak-minded and renunciation is the best, perhaps the only, way to have free will. Therefore, Satan must be in control of the world, since humans become their best only when they renounce it. So, the creator of our world, the entity that we think of as God, is really and basically evil. Not only did they notice this problem, they acted on it -- any Church who considered the Creator worthy of worship needed to be abandoned, and they left and prayed in the fields, they wrote secretly and instructed their disciples individually. Their later Neo-Platonic apologists proposed what I see as a very creative solution. In the light of independence as the root of the true Will of God, Satan's renunciation of God's authority was the assertion of his own free will, which he had as an aspect of his being the very best and most complete of the Angels. The others could remain good only because they lacked something essential to real good. In that framing, not only does the Satan, our evil God, exist, but his creator must also, against whom this world is an act of rebellion. If there were not some disagreement between Gods, or within God, good and evil would have no clear meaning. They deduce from this apparent contraction that there are layers of worlds where basic evil in one is a central aspect of good in the next one up. (And in their writings on this we have the Apocrypha of modern Satanism leading up to Crowley, Thelema, Sethianism and all that. The original, "unelaborated" Apocrypha were rediscovered at Nag Hamaddi, and said something quite different. But it is traces of this more creative form we find in the Zohar and the Tarot of John Dee.) I think of this as the ultimate seed of Hegelian dialectic. We attain a better, more complete notion of good by relativizing what our world sees as good and what it sees as evil in the right way. That always means that what was once evil is, in a better way, really good. 

What makes you assume that human ontology is maximal? Kant surely did not. He assumed we are bound by the forms of intuition to almost always merely approximate reality, whereas a divine being would be able to know a deeper reality more directly. Presumably a semi-divine being would see some intermediate approximation much better than what we might attain. I would claim that we are even more restricted by our biology than he imagined. Our physics is starting to show us a reality where we have a very hard time going. They are basically the same places Kant imagined we would find walls. 

Only concepts have opposites, a single actual referent involves too many conceptual frameworks to avoid having some of those not defining opposition. 

If the causation of something is what makes it real, then the thing is not real until the causation process is completed. At the same time, the causation process is not real without the thing, as then it would not be a causation process, as it would have nothing to cause. So the causation exists only as an attribute of the thing caused, and not as any independent thing. But the causation process itself has attributes, among them, the thing it is supposed to be causing. But the value of that attribute cannot actually be the as-yet-nonexistent thing to be caused, it must be some special version of or reference to that thing, with a subordinate less-real status of existing. So this leads to a hierarchy of degrees of subordinate existence. The thing being caused does not yet exist, yet it must exist already for the causation itself to be well-defined in any way, so it exists 'subordinately', and the cause exists subordinately to it, and so forth, and so on. So if there is any existence involved, there are infinitely many separate layers of existence. Unfortunately, this is a descending tower, we cannot find the most basic way in which either the causation process itself or the thing it results in would exist. So how can any of this pile of causations get started? So if causation exists, it is not a real thing, only an attribute of the caused thing. And thus all causation is the causation of the real by the unreal. 

From way out in left field we can look at Computation Theory as a model of the brain. The first and most basic result in Computation Theory is Turing's result on the halting problem. Basically, if the brain is a computing machine of any variety without specialized access to truth other than data, (Turing's Thesis), then there are real questions about its own behavior it cannot address. Specifically, it cannot propose a limited set of rules that will determine what questions it can and cannot answer definitively, in general. 

I would argue that this is a false dichotomy, so arguments for and against it would be misleading. The observation is basically an overstatement, and it is due to our current confused notion of Kuhn's three modes of science: pre-science, paradigmatic negotiation, and normal science. From his perspective, these are successive stages in a cycle, we ascend from each into the next and then occasionally fall back to the earlier one when times get tough. But in reality, I would argue that this is an overstatement: they are all taking place all the time, especially in the younger sciences. Starting at the 'top', in 'normal science' the rules are stable. Anything that does not follow them must be worked out, logically dodged, or put aside for consideration when other results elucidate the approach. Questions are the important thing, then, because great insight is in finding out where the rules might be hard to apply or where working out some part of the canvas of explanation in detail might allow them to be applied to an open question. If the person working there knew why he couldn't proceed, he could probably proceed. He just hasn't had the insight to diagnose some gap, and if you just ask the right question, he can answer it and move on. But at the same time, there are positional questions where things can go either way, where each of a number of alternatives must be worked out. Most obviously, these are the basic paradigmatic questions, where we spend whole centuries assuming one answer, say continuity of substance, and then find out it really loses to another, modified atomism, which then loses to another, 'vibrational' occupation of space by empty particles. Obviously, the right answer there, or the right support for it, would save everybody a century, and that can hardly be considered unimportant. At the same time, these fork-and-find-out questions don't only arise at paradigm crisis points. They are a part of day-to-day science that really does not get credit in the classical definition of normal science as puzzle solving. There is always value in pursuing the less-favored options inside your paradigm. Working out those details can give a reframing or alternative approach that is still within the paradigm, but broadens everyone's toolset. When you are going down those paths, answers matter. Closing tributaries or finding alternatives are done with answers, not questions. At the same time, parts of various sciences always remain in the prescientific state. There are underlying assumptions that simply fail to make intuitive sense, or to accord with natural philosophical notions. These questions are longstanding, almost perennial, they ask themselves, so the insight is not in the asking. There too, creative answers bring things like the duality of wave and particle, or the underlying mystery of quantum state into the realm of application. As long as 'only wave' and 'only particle' were fixed for under-analyzed philosophical reasons, the duality remained a spooky, almost religious concept. A lot is gained in creative answers like those of QFT, which attach properties in new and interesting ways, or investigations that prove the underlying philosophical alternatives consistent with one another by injecting alternative notions. So, yes, overall normal science is normal science, and the best approach to normal science is the most productive answer to what to do next. But there are payoffs for doing the exact opposite in both of the lesser modes of inquiry.