Question: Given the error I suspect the issue is related to the trust and Kerberos, but I'm not clear what I can do to resolve. Should I be setting an SPN for DOMAIN\myadmin or SERVERB? Is there something else I can try? 

I have set up a lab with a number of Windows Server 2012 R2 machines. The lab has an Active Directory domain (DFL: Windows Server 2012 R2, FFL: Windows Server 2012 R2) and these machines are joined to the domain. By default if left unattended these Windows machines will automatically lock. I do not want the machines to lock automatically. I do not have any security concerns with having the machines remain unlocked as this is an isolated lab. I have created a group policy object that sets a number of configurations and the machines still lock. I have verified that the GPO has been applied to the machines. The GPO configures the following settings: 

This sounds like it could be related to a recent Microsoft update: MS16-072: Security update for Group Policy: June 14, 2016 This update fundamentally changes how GPOs are applied. The cause is summarized well in this snippet from the Microsoft Directory Services Team blog entry: 

Is your DFS-R staging area on the same volume as the DFS-R folder? For performance reasons, it should be. If not, then DFS-R is having to copy the file from staging volume to the destination volume rather than doing a straight move. Here's where the speculation kicks in. It may be that during this copy operation, DFS-R is creating a sparse file, and then filling the blocks, and "unsparsing" it when complete. If something interrupts this process (like antivirus, Undelete, or some other file filter driver program scanning the DfsrPrivate folder), then you may end up with a temporary sparse file that doesn't get filled with its contents. You can test for this behavior by using Process Monitor on the files that are replicating properly and seeing if they are marked/unmarked as sparse at any point in the process. I'm not a fan of mixing 2008 and 2003 when it comes to DFS-R. I was soooo glad to get the last 2003 machine off our DFS tree. 

We enforce a 90-day password expiry on everyone here, (including ourselves.) Mostly because it's simply best-practices. Chances of someone using a "weak" password, vs. a stronger one is greater and the longer you leave it the same would possibly result in a long-term, undetected security breach. 

After reading all the descriptions of what's going on, (greyed out controls, unable to RDP, etc.) I'd bet a soda pop that your RPC service is having great difficulty, (and/or is possibly affecting your server service.) If this is the case, none of the pstools will work, (psexec, pslist, etc.) And if time is of the essence, suggest wrangling a user on-site to perform a hard power down/up. I'd probably also have the user watch for RAID errors, during boot process. Good luck, mate.. I've been where you are, before :) 

Try restarting the netlogon service on your DC? (Or probably in your case, just giving it a good reboot wouldn't hurt) 

Think of SPF and DKIM as ways to validate the mail path, and think of DMARC as an extension that also validates the message sender. Think of this as delivering a FedEx letter. It's easy to validate where the envelope was shipped from, and that the courier was legitimate, but it doesn't provide a way to prove that the letter inside the envelope is really from the person whose name is printed on it. Your webserver is a valid SMTP server for mywebserver.com and that your Sender address is legitimate, but that's not enough for other servers to trust that you have permission to send as website-user@gmail.com . How does GMail know that your server hasn't been hacked or otherwise used for malicious intent? Gmail's servers aren't going to blindly trust you to send mail as one of their users -- unless maybe you are hosted by them, and then you'd probably have trouble sending to Yahoo. To address your first part of the question, yes, it's very likely that this is why GMail is categorizing it as spam. The oldest forms of spam center around spoofing the "From" address. This is what most users see when they get a message, and is the primary field they want to trust. When a message from a legitimate mail server is sent using a From address that doesn't belong to that mail server, it's still a red flag. As you mentioned, DMARC operates on the From address as part of the specification. Granted, it makes it harder to write web apps that send on someone's behalf, but that's sort of the point. As to why they do it - well, that's up to the designers of the specification, but it's a trade-off. They are taking the high road and making a system that works very well if you stay within that limitation. Perhaps future mechanisms will find a way around this. The unfortunate solution is to only use addresses that you have control of. To address your third question, sign your messages with your domain name, and mention in the body that it was sent on behalf of website-user@gmail.com. Otherwise you will have to request that your recipients add the address to their whitelist. It's not much fun for a legitimate web app developer, but it will protect the sanctity of the recipient's inbox. You might have luck using the Reply-To header with the web user's email address. There is a discussion of this limitation on this DMARC thread. In the mean time, you can try to make sure that your server isn't blacklisted on any RBLs. It could be that you can fail DMARC but still get through some spam filters if you have good enough reputation... but I wouldn't rely on it. 

Two things. First, make sure that the content database is set to online (Central Admin -> Web applications -> Content Databases) and not offline. When offline user name changes do not propagate. Since you said this is MOSS, make sure your SSP user profiles are set to synchronize on a schedule (they are not by default). Set a schedule if not set up. You can also try to force a user profile synchronization them by running 

My question is similar to Powershell Remoting: One way trust, however there are differences and the resolution (adding the server to the trusted list) does not work for me. Scenario: I have two domains. DOMAIN and DOMAINDMZ. DOMAIN has an incoming trust from DOMAINDMZ. That is DOMAINDMZ trusts DOMAIN, but not vice versa. I have an administrative user who is a member of the Administrators local group on several servers in the DOMAINDMZ domain: , , , etc. I can log into these servers with DOMAIN\myadmin from the console or RDP. I am attempting to log into SERVERA and run a PowerShell script on SERVERB using PowerShell remoting. Remote Management is enabled on SERVERB. I start an elevated PowerShell session on SERVERA, and then attempt to use the cmdlet. I receive the following error: 

We use the default Microsoft quota system on the file servers, specifically when it comes to home drives. Other than that, one of our admins down south of us has a tendency to let the users' project spaces fill up, then encourages them to run duplicate finders and large-file/last-accessed programs such as "Doublekiller", "Easy Duplicate Finder" and "JDisk Report", which is one of my personal favorites. Another solution we're looking at implmenting is Symantec's "Enterprise Vault" especially if you're doing disk-disk-tape or have the tape storage available to shuffle files off to. 

Yes, "We" can see what you're doing, long story short. Here, we use Altiris to enable Helpdesk to administer your machine from afar, as well as perform application metering, maintaining configuration standards in the guise of rapid image deployment. On our network and in our company, Skype is a banned application. If you install it, thanks to app metering, we'll know within 24/48 hours. Altiris also comes with a remote control function that can be hidden from the logged-on user. (At least the taskbar display icon, etc.) If the bosses-that-be decide you need to be monitored, after our system emails us that you've installed Skype, then we'll do so. Remember that "Use of electronic company equipment constitutes consent to monitoring", etc, blah, blah? Maybe it was worded differently in your company. Either way, I'll bet you a can of soda pop that your sr. IT, HR and Legal management have agreed on this policy. Let's say that you install Skype anyway and we don't catch it for a while. I'm still a pretty good network admin though, I'll probably catch the traffic from it on one of my wireshark or LanHound monitoring laptops. Or, you could just whip out your cell phone and call whomever it is you want to talk to, completely independent of your work-network. At that point, all you need to worry about is the NSA. :) And you're not doing anything wrong, right? 

First, no matter what you do, you should run the SharePoint Prescan tool, to determine any possible issues you may have with upgrading. Here is a guide to running the prescan tool. There are many different migration tools and methods to migrate from SharePoint v2 to SharePoint v3. Microsoft has a TechNet article about the different (Microsoft supported) methods available, pros and cons, and how to perform each: Determine upgrade approach [Windows SharePoint Services]. There is a SPS to MOSS version available as well. Generally the database migration upgrade method is the easiest (assuming you are not using a lot of customizations). This involves migrating the SQL database (if necessary) and attaching it to your new SharePoint instance (either through Central Administator or with stsadm -o addcontentdb). SharePoint will upgrade the database automatically. In place upgrade will probably not suit you, considering you are not upgrading the WSS instance, but moving the content to a new farm. A commercial tool that I have used is metalogix SharePoint Site Migration Manager and it usually does the trick. It uses the v2 and v3 APIs to copy content from your old instance to your new instance. 

I would venture to guess that the users who are affected have this update installed in their workstation or VM pool. If you have clients with this update, they will be unable to apply user settings if their workstation does not have Read (not necessarily Apply) permission to your printer assignment GPO. The default security settings include this, but if you customized the security and removed Authenticated Users from the security, you will experience problems. I have actually seen that practice recommended in some older books as a measure to optimize the GPO, but it's time to rip that page out now. The simplest thing to do is to grant Authenticated Users read permission to the GPO. If you have a lot of them to fix, Microsoft has a PowerShell script which can help you update them. As usual, weigh this information with your particular needs. If you aren't comfortable with adding this permission to all Authenticated Users, then you will need to find or make some other group to assign read permissions to the workstations. 

First of all, you've got good backups, right? :) Always a good idea to make sure that you're all backed up, just in case a disk decides to not spin up, etc. when you bring everything back online. Nothing like breathing easy after a power outage then realizing that one critical SQL server never came back. :) Second, you're right, gracefully shut down and turn off all servers, etc. If it's not a modern building and/or you don't have a UPS in-line with surge protection, (which it sounds like) or anything like that, it's always better being safe rather than sorry, and I would suggest unplugging equipment. Bringing power back online could result in a power spike. Also, a little FYI; bring your core servers down last, (such as DNS, etc.) You don't want to bring down your DNS servers and find yourself unable to resolve the rest of the servers if you're shutting them off remotely :) When bringing everything back online, bring the network equipment up first, then your core critical servers, then finally your app/file servers. Good luck! 

... using only one line per IP address, and adding all the FQDNs to the list. Make sure to add the appropriate DNS entries so that other computers can connect to this server (if that is needed). Once you have this in place, you can tweak your Apace VirtualHost entries on how to handle requests for each domain name. You might want to start with a single VirtualHost listening on *:80 just to test your FQDNs out. After that, you can add or replace it with specific VirtualHost entries for each domain name that you want to handle. 

You have installed the trust list. Now it's trying to grab the CA certificate. You're probably going to have to grab that certificate manually as well. It appears that this is the certificate chain it is trying to obtain: $URL$ Download it and import it into your computer's Trusted Root Certification Authorities. It is mentioned in this TechNet blog: $URL$ 

DISCLAIMER: I'm assuming that you've already checked your replication, made sure everything else is working properly, etc. This is intended to be a quick-check option. 

Ah. After much gnashing of teeth and troubleshooting, it appeared that during a maintenance period when I ran a remote, "psshutdown" command on the server, it somehow held on to the temporary service that psshutdown creates and kept re-running it over and over. What confused me was the fact that when I would shut down for the day, it roughly corresponded to shutdown times on the server. Weird. Anyway, thanks to all for great answers, much appreciated.. 

If you're good on the above, a too-deep file/folder path might be giving you some grief. Try navigating several folders towards the root, then cutting and pasting them to reduce some of that path. Alternately, you could rename the folders in the path to "1", "2", "3", etc. Cutting down on some of the extraneous characters.