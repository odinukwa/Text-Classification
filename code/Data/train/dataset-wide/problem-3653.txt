It appears you are missing the column in your users table. You will also need and fields for this query. Dovecot may be able to determine these values by other means. Dovecot may be able to use other mechanisms to determine these values. 

Your latest revision indicates that outgoing traffic on port 25 is being blocked. Your firewall may also block traffic ISPs often do this for domains that should not be sending email to the internet. It is difficult to validate your configuration when you don't provide your domain. You can get an idea of where you are being blocked using a tcptraceroute program to see where you get blocked. In the mean time try sending via your ISPs email relay. Original Response: Normally, you would configure your local network (likely something like 192.168.1.0/24) as trusted. If the host at 50.244.27.204 is truly one of your servers you might want to trust it. Given that this is an internet routeable address you do risk becoming a limited open relay if this is not your server. The usually method for allowing access from hosts not on the local network is to use the Submission port (587) to accept authenticated requests. You should configure authentication to require a TLS encrypted connection started with STARTTLS. You will see this kind of message from untrusted address, as there are bots out there that will try to use your serve as an open relay. 

does not automatically push the domain search list to the hosts getting IP addresses using DHCP. There is DHCP parameter for this that needs to be set. You should also to push the address of your router as the DNS server. Check the dnsmasq documentation for some of the data that can be sent by DHCP. 

The simple solution is to configure one address in the definition, and use it's domain name in the helo message. If you use SPF, allow this address to send mail for all three email domains. Its domain does not need to match any of the web servers. records are for incoming mail, and your MX does not need to be the address your email originates from. Automated mail servers tend to be poorly configured and cause me no end of problems. I applaud your efforts to get it right. Please make sure your outgoing email is properly formatted with all the necessary headers. Often the simplest thing to do is to use the MX for the web server's domain as a for the web server. It is fairly easy to routing rules for different domains. In this case you would use the to select the routing. 

Logrotate is a good tool for log rotation and compression. It is available for most Unix flavors. I find the defaults used on Ubuntu to be a good starting point. When changing rotation frequency, you should change the rotate count. Keep logs as long as they may be useful. Archive to non-disk storage if necessary. For business systems there may be legal requirements directing retention and/or destruction of log data. Depending on the data and availability of data in another log data may be kept for a week, a month, a quarter, or a year. Only duplicated data is deleted after a week. A monthly backup retained for a year will give you a years worth of most log data. A centralized log server generally serves a different function than log retention. Comparing the centralized logs to host logs may detect log file tampering. 

Create a new domain for the new MX. Assuming you have as your MX, you could created or for the new MX. You may want to add the new MX with a different priority until you are ready decommission the old MX. The priority will control which MX will be used as a first choice by properly behaving MTAs that are sending you mail. There are a number of methods that can be used to forward mail to the appropriate MX. It is not unusual for the MX to forward mail for final delivery. 

It may be faster to run rsync directly on the server. You have about a million files to access over the network. There are a couple of minimal installs of that you can run. I've setup BackupPC on Windows this way. You can run a full Cygwin install, or the minimal cygwin-rsycnd install available in the BackupPC project. 

You may want to use something like BackupPC to do the backups for you. There are various other tools which will do rsync backups of this sort. A tool like can create aa recovery DVD or CD to begin a bare bones recovery. 

Besides deploying the certificate, you may need to create a symbolic link for the certificate. The utility is provided for this. You can also use a command to get the hash value for the certificate. The symlinks have a suffix. Alternatively, you can add the certificate to your LDAP configuration. This could be . Older versions also had settings in . EDIT: The handshake error may be a result of disabling SSLv3 or otherwise restricting the available algorithms. I recently disabled SLLv3 due to the Poodle vulnerability. I included the commands I used to test connections at the end. In your case try a commands like: 

The sending client should handle this case. Postfix should not accept the message for the invalid recipient. From the dialog you have included it is behaving correctly. It is the responsibility of the sending software to provide the appropriate notifications. If it has accepted the message, the sender may be fake so you do not want to send a bounce message. If you do send a bounce message after the message has been accepted. Sending such backscatter spam is bad practice, and may reduce the reputation of your server. Servers that receive backscatter spam may refuse your mail or assign it to the spam box. 

You can do everything you want with one instance. This is an example based on a configuration I have used. 

Look at a tool like Shorewall to build your iptables rules. Configure your VPN and the Internet into different zones. You can then allow ports on the VPN zone that you don't allow on the NET zone. The Shorewall documentation is extensive and includes how to modify your configuration when adding a VPN. Start with the example 1 interface configuration and add in the VPN. 

If you have a lot of jobs defined like that you are asking for trouble. Unfortunately these are often long running system jobs. I also tend to schedule jobs at different hours throughout a batch process window. (23 to 05) hours. I like the new cron specification used on Ubuntu. This has several directories to specify jobs to run. They get run in sequence rather than parallel limiting the load. You should be able to see what is scheduled in the files located in . Reading these files will require root access. If it is users who are causing the problems discuss the problem with them. You could also check for CRON entries to see what is being run when. 

It is managing the move. This includes finding the files, creating new files, and copying them. All this takes some CPU time. Due to the way files are buffered, it may be possible to this very quickly with the file writes occurring asynchronously to the copy operation. If the files aren't buffered, then the CPU utilization can be quite low while files are accessed. 

Cache times are built using the current system time. Time jumps such as you experienced are expected to create strange behavior. is just one of several programs which can behave this way. These issues generally clear, once the times catch up. Try running an daemon process rather than periodically running . One advantage of ntp over ntpdate is that for smaller adjustments it will slew the clock rather than jump the clock. Once your clock is in sync, ntp will query its servers roughly every 15 minutes (1024 seconds). If you know your clock is significantly off, then you can manually adjust the clock in small adjustments over a long period of time. This could be scripted and run as a cron job. Once you are within a minute or so of the real time, then you can start your daemon. 

To limit the damage they can do to your system make sure that the apache process can only write to directories and files that it should be able to change. In most cases the server only needs read access to the content it serves. 

It is good practice to use a separate zone for your internal network. If you do that, then you can just forward your requests off to your forwarders or forward directly to GoDaddy. Doing split configuration of a zone is difficult when using two different providers. It may be simpler to just copy the few entries in from GoDaddy. You may find you want to use internal addresses rather than external addresses for your servers. If you are allowing DHCP to update your DNS it is more important to use a separate zone. Otherwise users can replace the entries for your servers. 

It looks like you want a DNS service for your local area network. Many routers will not support this but some do. Those based on DD-WRT or OpenWRT should allow you to add local entries. It is incorrect to reply with a private network address to a query originating from the internet. Possible solutions are or running on a Linux server. Configure the router to use this for DNS lookups. If you are already running you can use split DNS to provide different answers to the Internet from those provided the local area network. The simple solution is to add a file entry to any devices that need to connect by domain. This works for both Linux and Windows although the files are located in different directories. 

Exim4 is character set agnostic. It just provides the transport and delivery mechanism for your email. It handles UTF-8 and a wide variety of other character sets. Character set selection must be specified. The content should specify utf-8 if it is using it. A content-type header can be used to indicate utf-8 content: 

I don't believe DKIM supports reassembling strings. If you are using a multi-line format verify it is reassembled into one piece. If it is served with the extra quotes, it will likely break. Your DNS lookup shows that this is the case. All to often I see SPF records showing up as , which is read as . This record should be delivered as but would work as . SPF does specify support for quoted parts by concatenates the parts as is without introducing extra spaces. This allows lines to be broken mid-word. EDIT: I am testing using a key formatted in multiple strings. However, I will need to wait until DNS updates. check-auth@verifier.port25.com tells me my new key does not exist. The format I get from bind does not match the format the documentation leads me to believe I should get. The documentation indicates that I should see a single string concatenating the fragments I entered. Instead I see the fragments as entered in the zone file. Testing with bind indicates the text fragments can be as large as 255 characters. Anything over that needs to be split. In any case two such fragments will likely exceed the capacity of a UPD packet. A review of the RFC indicates a 2048 bit key size is likely the practical limit. There is a warning to implementers that the may have to deal with TXT fragments and reassemble them in order. 

Typically, the web server would front the application. The application would exist on a path different from the site root, for instance $URL$ It is also possible to use a different domain name for the application. This provides some isolation of the application from the Internet. The web server can also serve up static content for the application such as graphic, javascript, and CSS files. WordPress on Apache handles this quite well. I have a variety of non-WordPress content in paths mixed with a WordPress site. Consider using the Apache security plugins to edit the requests being passed back to the application. It is also a good idea to limit the URLS being passed back to the application. 

If it works on another server, then that server is the one hosting the mysql database. Your connect string needs to refer to it instead of localhost. The command 'netstat -lnt | grep 3306' will tell you if mysql is running and listening on the port. If it is only listening on '127.0.0.1', you will need to run your Java application on the same server, or tunnel the connection. If it is only listening on an address other than 127.0.0.1, try substituting localhost with that address. Edit: From what I see your mysql configuration should be good. Check the port to see if it has been moved to a non-standard port. There are two locatiosn where port is defined, so check both. To see if it is listening run the netstat command above. If it isn't listening you won't be able to connect. Debian by default should listen on TCP address localhost:3306. It is typicall to allow remote connections. To enable it, change the listen address to 0.0.0.0 in your mysql configuration and restart. I use DBVisualizer to browse databases. It is a Java application which uses JDBC to access the databases. It provides reasonably good output on connection failure, so you may want to try it. Use a basic connection string without options first. Then add your date option to see if that is the problem. MySQL login security can include the originating hosts in addition to user and password, so logins may fail for that reason. Passwords can vary depending on origin. 

Add a file in named . Check the options for pipe transport (chapters 24 and 29 in the specification). You will need to set the options which don't have a value specified. Also set the correct command which can have options if required. 

I have used a custom log format with the %T (service time) specified on an ongoing basis. se I replaced %l (identd name) in the format. This can be used to identify slow pages for tuning. Large responses on slow lines can cause false positives, as can network retries. I used a custom log summary script to identify pages with slowest responses and average response time. These may be good targets for optimization. Comparing reports over time helps identify upcoming problems. Client side tools can provide good stress benchmarks and verify fixes haven't broken the site. There are factors which can provide benchmarks which do not match what you get in production.