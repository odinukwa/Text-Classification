Click Ok (on permissions entry window) Click Ok (on Advanced Security Setings window) Click Ok (on OU properties window) Add service accounts running SQL Server services to the OU (Optionally) Restart SQL Server Services running under said account(s) Enjoy Treats 

Max's answer provides some good context about how you can perform cell-level encryption, so I'm not going to re-hash that; rather, I'm going to add additional context I think you may be looking for. 

Since Precision is a measure of how exact the number you wish to represent is and we know we're limited in how much we can store, you have to sacrifice precision for scale. So basically the smaller the scale the larger the final value can become. Scale may best be explained with how we look at maps. A map of a state or province shows a much larger scale of area with less detail, whereas a map of a city shows a smaller area at greater detail. Scale can therefore be thought of as the amount of detail. The more detail we want to convey comes with the cost of showing less actual stuff. I don't know if that helps you at all, but hopefully that helps provide some context behind the definitions of these terms. 

When you restore a database that has encrypted data to a different instance, with a different Service Master Key (SMK), the data won't decrypt properly because of SQL Server's Encryption Hierarchy. The "restore" of the master key you mention is really decrpyting the data using the Database Master Key (DMK) password and then re-ecrypting the data with the new instance's SMK: 

What this does is pull all successful logins from the current error log, identify the login name, and pass it into the IS_SRVROLEMEMBER function you were already tying into with your Login Trigger approach. This will potentially balloon your ERRORLOG, but it won't block/slow logins like a failing trigger may cause. It will also provide login times and if you have some SQL Auditing in place, you may be able to correlate these with activity, etc. At any rate, I think this will give you what you're looking for. Final note: you don't have to audit the failed logins, but I find that information more useful than not. 

Now, check the shortcuts took after pressing Ok and your shortcut keys should work as expected. As you can see, I assigned mine to and , but you can use whatever you want: 

However, it sounds like you have a log header that doesn't want to wrap back around to the start of the physical log file and continues to fire off auto-growth events instead. This can occur for any number of reasons, but the most common is a vlf in front of the logical log has a DBCC LOGINFO status of 2 for whatever reason, even after switching recovery models as you're doing now, but the way to fix it is the same regardless of the reason. Simply perform the following steps: 

These are not hard-line requirements, but good practice recommendations and the reason behind them is that all activity that completes during the period where the database is in bulk-logged mode (once a minimally logged operation occurs) will either have to be redone if you roll back to the tlog backup taken in step 1 or will be committed if you restore the tlog backup taken in step 5. The yellow window identified in the infographic is an all-or-nothing sort of process, and you are unable to do any point-in-time recovery during that time-frame. The moment you convert the database back to the full recovery model and take another tlog backup is the moment you can once again utilize point-in-time recovery. Finally, there are some restrictions to using the bulk-logged recovery model, such as considerations for databases with read-only filegroups and online recovery scenarios, so do some testing to make sure you're not causing yourself more headache than necessary. I've used the bulk-logged recovery model for years and it's quite helpful so long as you understand how to use it properly. 

In older versions of SQL, the threshold where VLFs started to affect recovery times were lower, but with newer versions of SQL you may not see any ill-effects until you get past the 5k mark or even higher. A lot of it really depends on your hardware configuration and the version of SQL Server you're running. For example, about 10 years ago I administered a SQL 2005 instance containing a 200+ GB database with 500+ VLFs and after lowering/reconfiguring their amounts/sizes recovery times significantly improved. A similar database with a similar configuration on SQL 2012 years later didn't have any issues, and this was likely because of the newer version of SQL as well as the updated hardware. For now, I suggest you look at recovery times for dbs with 1000 VLFs to start with. If that number of VLFs doesn't seem to adversely affect recover/startup times, adjust your threshold to 1500-2000 VLFs before you start worrying and further adjust as needed . Really, your configuration is going to dictate the threshold you should be concerned with. 

The functionality of the delayed start is best described by CoreTech's post from the SuperUser forum: 

The beauty here is any user that is part of the role will have privileges to any new view you place into the schema automagically which is what I assume you're looking for. 

If you detach a database from an instance, you will need to perform an OS-level delete of the file. The safer approach is to drop the database instead. What I suggest is taking a final backup of the database after you put it into Read Only mode (as this will ensure no activity is occurring during the backup), after which remove it from you system by way of a Drop Database command. The full set of commands would look similar to the following: 

One final note on the process. If your tlog is highly active, you may need to perform this a few times or during a window of reduced activity. High activity may result in another errant auto-growth event occurring before you execute the second log backup. This basically turns what you thought was step 3 back into step 1. Generally Steps 1 and 2 go by quickly, and step 3 takes the longest to complete. Make sure to follow-through with steps 4 and 5. 

As the last sentence alludes to though, there are ways to simulate this behavior by using connection settings (e.g. and for memory) or OS level tricks with the help of extensions (e.g. prioritize for CPU prioritization). If you've got a budget, there are customized Postgres engines that claim to provide this functionality as well, one of which is offered by EnterpriseDB. I haven't used it nor do I have any idea how well it performs, but it's another option if you're looking for more alternatives. 

all tables containing PostGIS related data for every database on the server drop all tables containing PostGIS related data (same that were just backed up) for databases that had PostGIS tables in step 1 run script in against databases that had PostGIS tables in step 1 Uninstall PostGIS 1.0 gppkg () Upgrade GreenPlum database via the gpmigrator/gpmigrator_mirrors standard process Install PostGIS 2.0 gppkg () run script in against each database that had PostGIS tables in step 1 all tables exported in step 1 

Check out the linked article as Kimball goes into more detail on why this approach is necessary for data warehousing. 

Rebuilding the clustered index doesn't automatically rebuild any non-clustered indexes associated with the table. To do this, you would need to specify the keyword instead of the clustered index name. Per MS: 

If you want to assemble the statement at runtime, you're locked into Dynamic SQL as far as I know. I don't like sp_MSforeachdb as well because it's undocumented and I've ran into bugs with it, so here's an alternative that doesn't require its use: 

Even the execution plans are the same, so why the different result sets between a temp table and a formally defined table? Finally, a shout out to Joe Obbish as I gratuitously ripped off his CROSS JOIN approach to build large sets of test data as it's quite efficient! 

One potential cause could be that the default database listed for the user account being used to connect to SQL Server is pointed to the wrong database. Adjust this on the SQL Server side and see if that resolves your issue. 

If you suspect the query does support a critical process, I would start reaching out to users who have profiles on the originating server in question and start asking them about processes they have used from there. Finally, if you can't identify any useful information from users and you don't want to go so far as to create the logon trigger, I would then track it down by process-of-elimination. Start this by logging into the Server where this query originates from and slowly stop any running services (i.e. services.msc) that look suspect as well as review/disable any suspect Windows Scheduler jobs. Eventually I would hope you turn off the proper process and eventually figure out what's going on. Hopefully this gives you some ideas on routes to take. Good luck and hopefully you figure it out. 

Add a proxy account for Operating System (CmdExec) jobs and run the Ola jobs under that. This way the proxy account can have proper access to only the backup shares, etc. and you don't have to elevate permissions on the SQL Agent account (and inherently any other jobs running under the SQL Agent security context). Principle of Least Privilege for the win! :) 

Reorganizing an Index should be done when you have elevated amounts of white space within your index (i.e. column in the sys.dm_db_index_physical_stats DMV). Many people (and even solutions) in the SQL Server community say you need to look at Index Fragmentation levels (e.g. the in that same DMV), but this is not as much a concern depending on the type of underlying storage (e.g. anything that's RAIDed fragments your data for redundancy purposes regardless). Others also have come to the same conclusion. The downsides to Reorganizing your Indexes when not needed is that this operation will purge portions of your cache so it can have sufficient room to perform the reorg within memory. Reorgs aren't as destructive to your cache as Rebuild operations, but they will reduce your Page Life Expectancy (PLE) and potentially purge useful data that you will have to re-read from disk (e.g. increased I/O operations). Additionally, Reorg operations do NOT update Index Statistics. Updated Statistics do more to produce optimal executions plans than defragmented indexes, so if you're going to mindlessly run a nightly operation I would say Updating Stats is the better choice over Reorging instances. The right answer though is to have a solution check your White Space thresholds and reorg/rebuild indexes when needed. Have another process check your statistics and update when needed. There are a few solutions out there, such as Ola's maintenance solution, Minionware's Reindex solution, etc. But I often find myself customizing solutions to fit my needs. 

When the installation is complete, be sure to back out these permissions as they are not the most secure, by running one of the following: 

Now that you know where this query is coming from, the next step to take will be based on if you think this process is critical or not. If I were in your shoes, I would setup a Login Trigger that won't allow this connection in and see who complains or what tool starts erring out. Just a word of warning, a poorly defined Logon trigger can make your life maddening, but hopefully this example will do the trick. I modified an example of a different restrictive trigger based from the one found in this post: