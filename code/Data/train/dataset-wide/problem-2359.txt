Checking our production logs from yesterday, we discovered a period of about 5 minutes when a whole bunch of really simple queries were timing out. Further investigation on the server logs showed a huge spike in disk activity, which led me to the conclusion that an automatic CHECKPOINT was being run on the DB at that time. That's something I really don't want to happen during peak hours. So I was thinking of scheduling a daily CHECKPOINT every day during off-peak hours. Is that a good idea? Bad idea? Waste of time? If not that, then what? 

I have a query that was generated by C#/Linq-To-Entities code. As with this kind of query, it looks seriously ugly and I doubt it would help to include it here. But I have run it through a query profiler and found that 50.5% of the processing time is happening in a "Nested Loops (Inner Join)" step, which is joining a clustered index seek on (0.3%) and a clustered index scan on the same table (13.1%). I confess I don't truly understand what this "nested loops join" is doing, but why would the query be doing a scan on the table right after doing an index seek, when the fields it's looking up are available directly through another index on exactly the required fields? And yes, I have run , to no effect. Not being an expert in understanding the query plan, I'd appreciate if you could prod me for answers to relevant questions; I might just not know enough to have supplied you with enough useful info. Thanks! 

You want to increase the speed of your cursors? Wrap them in a transaction. If you are processing millions of records and don't want/need them all in one transaction, you can commit it on occasion to reduce resources. I did this with a cursor that took an hour to run (this is an extreme case) and afterword it ran in 1 1/2 minutes. I know that does not answer you question but it might help you avoid doing the conversion until your more familiar with SQL. The answer to your question is... experience. There is no magic site or book you can read, it just takes time learning to do more and more in a single statement. 

I have started doing some long-term tracking of various activities on our 2012 SQL Server and I have noticed an increase in Batch Requests during our incremental backups. To give an idea, normal day-to-day we are around 10-20 batch requests per second but during the time our differential backup runs, it jumps to 100-130 per second. I know this is not a lot but it made me curious as to what is going on that it increases 10-fold. Not sure what info you might need to help troubleshoot this issue. 

I noticed that the first transaction log backup of the day, at 0700 is like 3GB. We run a full backup each night at 22:00. We are a 9-5 shop so there is no real activity overnight. We take TLog backups hourly during the day and they average around 30 MB. How can I find out why this first log backup is so big? Thanks! 

When running the disk usage report, it's showing the "Total space reserved" as 275 GB but the "Data Files Space Reserved" is 279 GB and the "Transaction Log Space Reserved" is 1.7 GB. Why isn't the "Total space reserved" closer to the total of "Data Files Space Reserved" + "Transaction Log Space Reserved" ? Thanks! 

Learning sql server over here and I'm trying to figure out how to see the securable permissions I have added via T-SQL , in SSMS For example, I have a database user called "LoginUser1" in Database1 and I granted "grant create table, create view to LoginUser1". I'm trying to see this permission in SSMS. I go to the properties for the user in this database but i'm not seeing anything. What am I missing? Thanks! 

After monitoring the active sessions and queries during the high activity period, I have come to the conclusion that the third party software we use (LiteSpeed) to perform backups is the culprit. There seems to be a lot of queries to get information about the maintenance plan, updating backup statistics, getting information about the server and the database, etc. This server is not only the one being backed up at this time but it is also the "Central Repository" for all our servers. It appears I jumped the gun by posting here but at least this might prevent others from spinning their wheels on something that does not appear to be a real issue or one you can do anything about, other than not using the third party software. If our environment becomes bogged because of this, I would bet moving the "Central Repository" to another server would reduce the load on this one. 

The thing that slows down INSERTS more than anything is indexes. When you insert data, the indexes must be maintained so they are correct. If you dont have indexes, there will be no overhead in maintaining them. Some other things you can do if you are still not getting the performance you want is to: 

A user is opening a page in our application which takes around 35 seconds. I want to see what is happening in the database during this time. What's the best way to do this, doing a trace? Thanks! 

I ran the DBCC UPDATEUSAGE command on our Dev box to see if it fixes the 'disk usage report'. It ran quickly ( 10 seconds) and ran successfully but did not fix the report. Any ideas about what the issue could be? Thanks! 

In my recent studies about Auto Growth, I took a look at one of our DB's at work and see the Transaction log has many Auto Growth events. My understanding is that the Auto Growth event is a blocking event and should be avoided. So my goal is to stop this recurring Auto Growth. Is this happening because the reserved Log space is too small and the transactions fill it up immediately triggering the Auto Growth? Also, In this DB, they are taking TLog backups every hour on the hour. Here is a pic from the Disk Usage Report. Thanks! 

Indeed, that was the answer. I set the db recovery model to "Simple", waited a minute for the filestream data to clear up, and then I could remove the filestream file and filegroup. 

I have been given a DMP data pump export file to import into my local Oracle instance. I've tried running this command line: 

And no data is imported. From what I've Googled, one possible cause of this is that I need to specify . But I have no idea what the name is of the schema in the dmp file. Any easy way to find out? EDIT: I didn't find a solution to this question, but I did find a workaround... I tracked down the guy who made the DMP, and beat got the schema name out of him. Specified according to his definition, and Hey Presto! 

I have an intermittent problem with a software installation package that installs our product (written using InstallShield/InstallScript). During the process of the installation, we restart the SQL Browser Service. Most of the time this works fine. But occasionally - and I have not worked out how to reproduce this predictably - the service fails to restart, and I find in my "Services" manager that the service status is set to "Disabled". Any ideas what would be causing the service to be disabled, and how to prevent it happening? 

If you dont know what fields they will be searching on, you should not create indexes for all the fields. They can still search them. Nothing says the query will not perform well. The reason for the guideline is that this is generally what SQL Server will do when looking at your statistics to determine if it should even use the index you created. If you are always querying these fields, it is a good idea to do a covering index on the additional fields (Name, Current_State) so SQL Server does not have to then lookup the records that match the date, it already has the addition data it needs. (with name, it might not be a good idea) Ultimately, you should run a few queries (with various indexes) and review the statistics to see which performs the best according to your terms. 

The biggest choke point for databases is the HD. Do what you can to it, and the database will follow. 

I have a user which is supposed to have privileges on several databases. I thought I had set it up right: 

Now I want to create an index on LongValue, so that I can easily look up on serialized values that represent numbers. 

These all qualify as nuisances, but if I really want to work around them in order to gain the performance benefits, I can make a plan. The real kicker is the fact that you can't run an statement, and you have to go through this rigmarole every time you so much as add a field to the list of an index. Moreover, it appears that you have to shut users out of the system in order to make any schema changes to MO tables on the live DB. I find this totally outrageous, to the extent that I actually cannot believe that Microsoft could have invested so much development capital into this feature, and left it so impractical to maintain. This leads me to the conclusion that I must have gotten the wrong end of the stick; I must have misunderstood something about memory-optimized tables that has led me to believe that it is far more difficult to maintain them than it actually is. So, what have I misunderstood? Have you used MO tables? Is there some kind of secret switch or process that makes them practical to use and maintain? 

I'm wondering how I can restore a single table in SQL Server 2016. For example, I backup the table using SSMS task script option. Delete a row from that table. Now I want to restore the table to bring the deleted row back, How can I do this in SSMS? Thanks! 

I have setup sp_whoisactive as a report in SS Reporting Services so people less familiar with SSMS can run it and see what is happening when needed. I have it working good and BUT I can't seem to figure out how to show the "sql text" column as a hyperlink in SSRS as it's shown in SSMS. Anyone have any ideas? Thanks! fyi, we ended up doing this: 

Quick question. I'm using Ola's solution and with the full backup job for user databases, is there any issue scheduling these at the same time? I saw another thread on here that mentioned that it will just backup each DB one after the other but wanted to confirm? Thanks!