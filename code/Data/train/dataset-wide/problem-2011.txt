The query retrieves all the rows with unique fields (by ), prioritising the version you specify, or the draft one (which has its priority bumped to the top). If you want the draft or last version, then use instead of . 

The following schema is what I came up as a fully denormalised set of tables. There are pros (e.g. flexibility) and cons (e.g. user interface complexity) to this approach, but you may find it useful, or it may shed some insights: 

Your design seems almost correct. I can pinpoint the following problems though: You're using the table for two purposes: 

Ensure first there's an index for , or better still, ; If this doesn't preform well enough, you may benefit from creating a temporary table: 

Without knowledge of your schema, query attempted and statistics from explain analyze, any response can only deal in generics. In this sense and in terms of SQL, there are generally two commonly used strategies for dealing with finding missed relations: and . NOT EXISTS: 

You can also use . Ensure that your local user can ssh passwordlessly to your remote host () and that its public key () is included in the remote hosts authorized_keys files (). Once you (as user) can ssh to the remote host, update the following in your : 

This will really depend on many factors, beginning with your choice of RDBMS, and followed by your application language and framework. The choice will be very different if you use some form of RAILS and mysql (where database design is driven by class design in your application), than PHP and PostgreSQL (where you can choose to use create a CONTACT composite type, and keep contacts as an array of contacts in a column on each of your databases, without a separate table for it). 

Ensure has an index (or key much better if applicable) on ! You run the function several times for each row, this is likely taking a significant toll. I've replaced it with string_to_array, and then fetch the individual pieces as needed. I also didn't understand what you intended to obtain for the field using reverse? The query below returns the last element for it. Your query returns many million rows. Even if PostgreSQL processes the query reasonably quickly, your client application (especially if you use PgAdminIII) will struggle allocating enough memory and receive and format the results, and probably be what takes the most time. So you may want to create a temporary table with the results, and then query against the temporary table: 

The purpose for this UNION is to return all data for a timeperiod, and ALSO to return data for the same timeperiod for 12 months previously EDIT: Added a missing "CATCH-ALL" EDIT2: Added a second Â½ of the UNION statement EDIT3: Corrected the GROUP BY to include some other necessary elements 

I have a need to see a trend of data for "month to date" So I want to pull the 1st x number of days of each month for the last 14 months, and the n aggregate this. My data has approximately 10k data points each day So far I have only been able to figure out how to do this by writing a double while loop - the outer loop counting down the months and the inner loop selecting each day and aggregating the data for the day - then storing it in a temp table. Once I have stepped through each month and each day I then selet the data from the temp table and aggregate this to give me monthly data summed for each contract type. For various reasons I need to run my select 4 times for each day. This all ends up meaning my SQL takes around 120 secs to run. This is sub-optimal - as I am hoping to have this used by SSRS to pull a report on demand when a user wants to see it. Making them wait 2 whole minutes? Not Desirable - especially considering primary target audience is the exec team Here is my SQL (NB I've changed a couple of table/ column names ) 

I have a user account in a database that can create tables and Stored Procedures. I would like to confirm if this user account can Create Schemas - WITHOUT trying to do it first. (I am part of finance not IT) If I CAN create Schema's then I'll ask IT for permission to go ahead and do so. If I CAN'T then I'll just continue to wait for IT to do it on the Service Desk request I put in some time ago sigh Is there a command I can run in SSMS that will tell me this without actually creating a new Schema? I have also taken a couple of screenshots of this user's permissions - maybe they will have the answer? 

The is for compressing the transfer, which is something you probably want. For further compression, you may use instead: 

Note also that the character in regexp has a special meaning, and although can deal with it unescaped (because it is implied), other regexp functions (such as regexp_replace or regexp_matches) will likely fail, so you may want to always escape them, as a general practice: 

JSON documents are ideal for this purpose. MySQL can store JSON documents via its JSON datatype, and it has a variety of function for extracting information in such documents. However, MySQL doesn't currently have an effective native way of indexing JSON documents for fast search. Googling for "mysql json index" will render several strategies for doing so, some using triggers and external tables, others using indexes on computed columns. PostgreSQL, however, has very good support for JSON datatypes including indexing, opening up documents as recordsets (using LEFT JOIN LATERAL in combination with is very powerful), and GIN indexes that support JSON documents (for both tags and contents). Unless you're heavily invested in MySQL already, PostgreSQL would be a great option for your application. Alternatively, you may want to consider PostgreSQL instead of MongoDB if you'd use it for this purpose alone. Additionally, PostgreSQL can easily access MySQL tables with via Foreign Data Wrappers, it has good support for regex text manipulation, and you can do all sorts of magic on JSON documents through standard perl libraries, using pl/perl stored procedures. 

Your queries are not the same. The first one is required to perform a complete left join and then sort, in case the results of the left join render fewer records than the limit (in which case it would includes as many records from as needed, in no particular order). Your second query effectively cancels the left join and turns it into the equivalent of an inner join, which in this case is completely pointless because you will obtain the exact same result as: