You contradict yourself. First you state you would like to treat humans and organization as one group and then you state using one table would cause queries to require more code. Three tables with one master: With two tables rather than funnel to, the constituentID is the master the [human].[id_number] and [organization].[id_number] would just be FK. So you would insert into constituent first to get the master ID. With this it would be a pain to enforce the same ID not in both [human] and [organization]. If you are going to put a type in constituent to enforce the two other tables then you might as well combine in the first place. Two tables: You could have two independent [human] [organization] tables and combine them with with a view with a union but that would not be my preference. One table: Not having a last name is not a big difference. They both have address, phone number, and EIN/SSN. I would share name / last name and let first name be null. A person is still valid without a first name so do you really need to enforce it. ID is the PK and is an identity. Type - person or organization is not part of the PK. Maybe SSN/EIN as PK but then that would not allow for one constituent to have more than one account. Then you can have views for person and organization. In summary I don't think a null (enforced or not) is that big of a deal. Compared to separate tables. 

I could be wrong. I think every condition that need to pass does pass. Not sure if something that should fail will get through. It fails on This might work 

all that join does is make sure it is in Profile but you are not reporting anything from profile and how is that not ? 

I know you are going to think that disable index will slow it down Index maintenance is also slowing it down What is the harm in trying 

Let's say Col1 is clustered index you should sort on col3 even if inserting all This is not a name conflict that order by is table_name2 

I would leave the PK clustered index alone create two non clustered indexes on [CountryCode] [AddressFormatId] as for query that was added late there is no ItemNo in the table definition since you are joining on TaxRegionId index on TaxRegionId 

The fact that when you finally ran the second query and it ran fast is a very strong indication you have a volume and not an index issue. That count(*) had to touch those rows. Even if JobSubTypeId is Code that is clearly not a sufficient join 

this is the revision - not 100% sure about it but (maybe) give it a try even one OR is probably going to be a problem this would break on ActiveDirectoryUser null 

Before you vote this down look at the question before the major edits This is valid (and good) answer to the original stated question What you have is just messed up Why use a merge for an update only? Why use a CTE when it is the whole table 

I don't know about a best but at a small number there is no real value. There is some overhead to using the index. Yes an index seek is faster than a table scan but there is some overhead to using the index. Index maintenance clearly has overhead. If a table has a PK then you should use that as a PK and typically clustered. Consider a table of USstates (50 rows) ID PK identity tinyint Name varchar(20) Region tinyint Region would be use to group states like NE, SE, ... I personally would never use indexes on Name or Region - a table scan is still very fast. Region would be a FK but that does not automatically create an index (to my understanding). The whole table is right at the 2K page size. If sort on State.Name is used a lot then yes that index would be used but I just don't think you could even measure the performance gain. Over a million rows then yes start building indexes up front. Between a thousand and a million then consider building indexes on a case by case basis. Even at 10,000 rows there are going to be a lot of cases of obvious indexes. A column like AddDate that is not likely to change and would be used a lot for search and sort I would index and maintain (de-fragment). A table with more than 10,000 rows that reference State as a FK I would index that column up front. But since you are asking the question maybe wait and optimize for real life queries. I would not want you to take the other extreme and put an index on every column as it might be used. An index has overhead. An index will slow down insert and update. A highly fragment index can be slower than a table scan. I get a lot of the users on this site want to optimize up front and have theoretical discussions. This is real life advice for a newbie in DBA. 

How is client ID and name sufficient? Don't you have address, phone, ...? Client ID is required to support additional data (today or in the future). CliPrice does not need an ID. 

an index on försystem should help This index on the view should help person_id, termin_fakta, läsår_fakta Do you need to use the view? Try going straight to the tables. You may be doing stuff in the view that you don't really need for the delete. Optimize the select then try it on the delete But the index on [sko].[stage_närvaro] may help the select but hurt the delete. Index adds overhead to the delete. 

You will need to go bigint as 11 digits is too big for int. You will save space (once complete). Downside is that it will take time and space to convert. If you have leading 0 or not all exactly 11 digits then sort will be different. 

One of these is not like the others - expanse Entities: groupID, ID, name composite PK on groupID, ID 

Server side would typically have no UI. Server side is the business rules, data layer, and database (if required). Server side will typically have authentication. Client side is the UI and will call the server objects. Client will typically have some business rules and validation. Server and client can talk using WCF (Windows Communication Foundation), REST, raw socket, and others. Often the server program will operate as a Service. You can also have server and client in the same program. The client just creates a server object and calls it directly. You might do this for the initial development and then later move it out. A web site might host the server code also and just create the server object. The browser client would not access the server object directly. This separation protects the data from direct access by the client. 

The query planner is more efficient with #temp. On a table variable it only considers the first few rows. Your table variable (and #temp if you use one) would likely benefit from declaring a primary key. Put a key on #AutoData and only populate with necessary rows. Sort by key as you add rows. I suspect below can be optimized with a row_number()