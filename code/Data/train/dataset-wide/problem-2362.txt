This will allow aggreagtes in th result. A purely existential check can be performed with and a correlated sub query. 

I think you may be mixing up syntax and semantics. It is syntactically correct to join any two tables on any columns that are type compatible. For example, you could join Utilizadores and Áreas on . It would be legal SQL to do so but it would be meaningless. In other words, syntactically valid but semantically invalid. The table Permissões is there for a reason. Its existence records meaning. A row in that table connects a user to an area for a reason. It may be that user manages that area, or sells into that area or is banned from entering that area or any other possibility. The real-world situation that your database models will determine what the meaning is. There may be several reasons to join two things and those reasons should be modeled separately. Take the example of a person and a car. The same person may be the registered owner of the car, he may have paid for it and he may be driving it right now. Those are three different facts, however, none imply the other, each have different semantics and should be modelled separately. By joining to Permissões you are using the semantics built into that table. Any other join will have different semantics, so even if it returns the same values it will have different meaning and so will not be equivalent. 

.. plus the others I skipped from the example hash calculations previously. Now a query comes in. The predicate is "a=foo". We take the value and hash it, getting 2085198204. We read table 2 . We get two rows: id 1 and id 2. We read table 1 and get our full rows. A second query arrives. It has predicates "c=baz and b=bar". We order the values in column sequence getting "barbaz" which hashes to -1478044229. Reading table 2 for this hash value gives id 1. If values could occur in more than one column -- perhaps "foo" could be in column d as well as column a -- the naive scheme above could return false positives. Secondary checking of the data rows would be cumbersome. Better to include the columns in the hash, I think, so "a=foo" hashes to a diferent value than "d=foo". 

The PIVOT pattern would work for this. It converts rows' values to columns in a single row, according to their common key. There are a few ways to implement this. Some require only a single table scan. After the PIVOT you would have a table with one row per postcode and a column per value. The remainder of the query would be written as though it referenced a single table. 

Relational modelling starts from the identification of classes of "things." To these we apply normalisation to remove data update anomolies. The results are implemented as tables with a column for each attribute we identify. We do not start from a bunch of domains, look around for "things" with the same bunch of domains, shoehorn them into the same table and call it an Entity. This is an anti-pattern generally called "One True look-up Table". Implementing it compromises what constraints can be defined & how foreign keys can be established. Tables can become bloated with NULL-able columns. This increases the IO required to fulfil queries. Index depth increases, slowing even key lookups. Now, at some stage in every project the science of normalisation bumps up against the art of model design. Decisions have to be made and compromises have to be accepted. In your case I can see one of three models emerging. The first is the normalised one you have currently. This is how RDBMS's are supposed to work. Mappings are obvious; queries are fast. Exansion, however, requires schema changes and these can be slow to implement, depending on your company's abilities. The second is a super type / sub type model. You treat Company, Contact etc. as specialisations of another type, say LegalPerson. This super type contains the common attriubtes. Each sub type has the attributes specific to it. There is a one-to-one relationship between the super type and the sub type i.e. a given "thing" must have a row in the super type and exactly one sub type. This can be difficult to enforce in current SQL products. This allows recognition of common columns between entity types. Relationships between the types are held in specific foreign key columns or intersection entities. The insertion and manipulation of data is more complex. Again, expansion of the schema will reqire development work. Finally there is the entity-attribute-value (EAV) model. Here you are effectively building a database-within-a-databse. The objects you model are "Table" and "Column" instead of "Customer" and "Company". Thus your application can ammend the logical schema at will. The price is application complexity and run-time performance. EAVs have a bad reputation. There are many posts explaining why perdition will descend upon you simply by uttering the name aloud. Others defend the practice. I myself have an EAV systems holding 300M values. It works well for us. But I did this intentionally, in full understanding of the implications, with a DBA team to support me, to solve a specific problem we could not address otherwise. It was not done because some of the tables looked a bit like others. 

Apologies for the slightly Heath Robinson presentation. Hopefully the below will give you some pointers nonetheless. I'm using a "numbers" table. There are any number of aricles on the interwebs explaining what this is, why it's a good thing and how to get one efficiently. I take the columns to be and as . Other types would work with suitable adjustments to the respective functions. You don't say what version of SQL Server. I've written for 2008R2 since that's what I had to hand. Newer versions have nicer syntax for some of the things I've done. I've laid it out as a series of CTEs simply because that's how the ideas came to me and what I could manage in the time available. Undoubtedly further thought would give a prettier, simpler, more effieient structure. This would not be resilient against anomolies in . Any gaps or overlaps would cause all sorts of problems with my solution. Your examples had a job lasting, at most, overnight. This will cover a job of any duration. Anyhoo, thanks for the challenge! 

Having a primary key per se will not speed up queries. Primary key constraints are usually accompanied by a unique index. If this index matches query predicates or join conditions then those queries are likely to run faster. 

There has to be some invariant between two versions to say the versions relate to the same thing. This is the job of the primary key. It exists to differentiate this thing from that thing and to say that all that stuff over there relates to this one thing over here, via the foreign key of stuff.thing_id. If you change the primary key - Name, or GroupID in your case - you have a new thing, not an old thing with a new name. This is one of the reasons we have impersonal identification like social security numbers. You have a couple of options. A) re-design your tables so there is an invariant identifier. An integer surrogate key works well for this. B) add two columns and . Populate these as you accumulate values for the new version. Use them in your join. "A" would be better. You may also want to think about using a partitioned table and / or views rather than move data around between TableA and TableB yourself. 

Each table will have its own little bit of disk. These are unlikely to be contiguous. Every query will have to hit every table, even if they're indexed. It is unlikely to be fast. Once again this query must change every time the list of regions changes. It can be hidden from the application somewhat by enclosing it in a view, but the maintenance task remains. As a starting point, I would maintain a single Users table with a foreign key to showroom, just as you've shown. If testing proved to be a problem in practice at the anticipated scale, on production-sized hardware, I'd consider propagating the key of Region into SubRegion and Showroom, giving them compound keys. Users table can then be indexed / clustered by Region to satisfy the given query. I may even use the natural key for this, rather than the surrogate, to remove further steps from the execution plan. The foreign key constraints will ensure only correct Region_Id appear in the child tables. This could also be achieved by demnormalization. Region_Id column is added to Users but not to SubRegion or Showroom. The value is copied in when a row is written to Users. Assuming a user never switches Showroom/ Subregion / Region you're done. The system cannot help you ensure Users.Showroom_Id and Users.Region_Id correspond to each other. 

Fragmentation occurs when the storage engine can't physically place a row in its correct logical position. So you need a table with no internal space, then insert rows in the "middle". The easiest way to fill pages is to ensure only one row will fit 

Although Cassandra allows the definition of "columns" within a "table", these are much less strict than a relational database schema. As it says here 

There are other DMVs that can help you quantify this. has columns , and amongst others. While you're running your plan-getting query in one session you can interrogate this DMV in a second session to find how expensive the first is. Pleasingly, you can also quantify how expensive your second session is at the same time! 

Sadly row 4 has been eliminated entirely since it has only NULLs! It can be conveniently re-introduced by injecting a dummy value into the source query: 

For this I would create a second monitoring job to look at in msdb. This will work best if your current job typically runs within a fairly well defined time interval. If it doesn't then you cannot schedule the monitoring job until quite late in the day, by when it may be too late to do anything meaningful to correct the situation. Be aware that holds a limited number of rows. If you have other jobs which run very frequently they may push this job's history out of the table before the monitoring job can see it. If this is a problem create a custom logging table in an application database and get your current job to write to it. I haven't yet figured out how to get informed if the monitoring job doesn't run :-)