Something like "is it hard to read" is a really subjective question and I'm not sure how well its going to be received in terms of allowing for an objective answer. Perhaps something that might help illuminate the question is that this book is a piece of professional philosophy by a professional philosopher that is intended for (not strictly professional) philosophers. This differs from things that are not professional philosophy but still written by professional philosophers with an intended audience of beginning or non philosophers, for example. It assumes background knowledge of the work that it discusses, because much of it is Dummett's critique of the ideas of Wittgenstein, Frege, Davidson, Quine, and Tarski, among others, and his offerings of solutions to the problems discussed therein. In regards to "what sort of content does this work cover and what will I need to know to understand it", this work uses a lot of background from the general lexicon of 20th century philosophy of language and metaphysics. This book is Dummett giving a very intricate evaluation of the debate between realism and anti-realism, but it strongly goes through its points in light of conversations about meaning, intension and extension, definability of truth, etc. which are central to logic and the philosophy of language. The Internet Encyclopedia of Philosophy summarizes it this way: 

The second sentence of the SEP's article on mathematical intuitionism gives a pretty good explanation of why it is named as it is: 

In the Phaedo, Plato's main argument is that the soul is immortal and undergoes the process of metempsychosis, or transmigration of the soul, after death. He makes four arguments that he believes are so sound that any philosopher would see that they are true and would therefore not fear death in the same way Socrates did not. The four arguments are (paraphrased from the Internet Encyclopedia of Philosophy): 

The Terminology of Faith vs. Assumption First, I will try to explain why the word assumption is better than the word faith in this context. In the definition of faith you have, it ends with "for which there is no proof" and this, ultimately, is the issue. Faith, being defined in that way, means that it applies to things that cannot, under any circumstances, have a proof. However, the axioms we use in mathematics do not fall under that category. Axioms are relatively unprovable. This means that a statement, such as "parallel lines never intersect on a flat surface," might be assumed without proof in one context, but is completely provable in another context. I'll explain this with an example below, but the point to take away is that there are proofs of mathematical statements that we assume as axioms, but those proofs are in systems that do not use those statements as axioms. The reason why the statement "we take it on faith that our axioms are true" is less true than "we assume our axioms are true" is because faith says "we are saying there is no proof that parallel lines on a flat plane never intersect but we believe it anyway" when assumption says "we are assuming that parallel lines on a flat plane never intersect without proving it". We don't take it on faith that Euclid's postulates are true, we assume them without proof because they are arguably self evident, or even definitional. The difference comes down to the difference between "there is no proof" and "without proof". An Example of the Relativity of Proof As I said above, a certain mathematical statement may be assumed as an axiom in one context (in one specific theory) or it may be derivable as a theorem in another context. Let's take for example Euclid's first postulate: 

In this context daimonion is the Greek word for demon which in ancient Greek culture had a much different meaning than the meaning in, say, Christian theological contexts. From the Online Etymology Dictionary's entry for demon: 

Philosophy mostly deals with the definition (a) given above and this question concerns the definition (b). So, if person A tries various things, such as putting themselves in person B's shoes or having person B explain their reasoning, then perhaps person A could understand person B's logic. But again, this is a different use of the word "logic" than the use most common in philosophy. Consider the situation where person B has more information than person A. If that is the case, then person A might be drawn towards a different conclusion than person B. If person B explains their perspective and person A finds out the missing information then it would be completely possible for person A to understand person B's logic, even though they originally had different conclusions. But again, this is using the second definition of "logic", not the first. Conflation is the situation wherein someone treats two distinct things as one thing due to them sharing some property. This happens a lot when words have a technical definition as well as a common colloquial use. In this case you should be careful not to conflate the first definition of "logic" with the second. 

So, functions are viewed as a mapping from symbols to symbols. Once they are given an interpretation they make meaningful statements about actual numbers. Russell showed us that it is logically equivalent to replace functions with predicates that map to a truth value instead of a variable. Both of these are logically equivalent ways of thinking about the formal language. The successor function is what is referred to as a primitive recursive function. These are a set of functions that are crucially important in the foundations of mathematics and computer science. You might be interested in reading on their history and why they are considered to be "primitive" or "irreducible" functions that are used to compose more complex functions. *As a reference I have been using Introduction to Mathematical Logic, Sixth Edition (Discrete Mathematics and Its Applications) by Elliott Mendelson. Answers to the subquestions of the question 

As a quick aside/introduction, this answer might be better served as an answer to "what is meant be 'intensional context' in the philosophy of language?" because I go over more than just the titular question about substitutivity. However, due to a comment you made about the question of modality and the number of planets, it seems to me that a larger explanation of this topic is what you're really looking for or at the very least it will help clear up your confusion on the topic. Intensional Transitive Verbs Intensional contexts are brought about when you have what is called an "intensional transitive verb" acting in a proposition. A transitive verb is a verb that transfers action from one object to another, for example "carry" transfers the action from the subject, that which is carrying, to the object, that which is being carried. A intransitive verb is the opposite, a verb that does not transfer action. An example would be "sleep", as in "I sleep," where it is clear that there is no object that is being transferred the action of "sleeping". The three criteria that make a transitive verb intensional are outlined in the Stanford Encyclopedia of Philosophy's (SEP) article "Intensional Transitive Verbs" (in the quote, "VP" means verb phrase), and in order for an intensional context to arise it is sufficient for at least one of these criteria to be met: 

It's very hard to understand what a connection between the analytic/synthetic distinction and computational complexity could mean. As an example objection to the idea: if you are saying that synthetic statements are somehow equivalent to an NP-complete question, how would you handle the statement: 

Terminology has changed over the past 100 years, and some conventions in PM are no longer used, such as writing "(x)" for universal quantification or delimiting syntax with periods and colons (thank God). Unique existential quantification has also changed (I believe partly due to the want to separate it from it's close connection to Russell's theory of descriptions). For example, in Kleene's Introduction to Metamathematics (1952) (page 199) he writes: 

This is important because Aune has just shown that he is now using "PM" to represent physically possible, and "LM" to represent "logically possible. The lines that show that his weird use of symbols in the beginning are exactly the same as these lettered abbreviations are: 

It feels as though the idea of identifying this distinction with complexity classes (especially classes that haven't been proven to be distinct yet (although to be fair, of course most sane people believe its true)) is very arbitrary and unfounded. Also, I think that your example of triangles might be a little misguided. You're right, the reason the statement 

This directly supports the definitions given above for a language of thought. The systematic way that the programs cluster the languages together by the internal semantics of the statements, if completely true, can be thought of as a physically realize, linguistic system contained in the structure of the neural networks. However, the paper goes on to state results that this semantic clumping was not perfect: 

Those two fields, which are part of mathematical logic, study exactly what you are talking about per your comment 

What is NP-complete about that statement? Does an algorithm that decides the answer to that question run in NP-complete time? How would you go about verifying any synthetic statement in relation to a specific complexity class? An issue that you will run into many times when trying to formulate this comparison is how you choose what complexity class fits each description. Let's say that we verify that synthetic statement by looking up a table of Louisianan cities and their area. If it is sorted by smallest to largest and we could process one entry per second, then verifying that statement would run in linear time proportional to the size of the list. However, what if we verify it another way? What if we went to Louisiana and measured all of the cities by hand? There is a chance that the algorithm we use to measure would be of a different complexity class than the one we used on the list. This is not good if you are trying to associate synthetic statements with complexity classes. How are you going to prove the hardness of statements such as: 

Again, first-order logic is the same as propositional logic in the sense that "first-order logic" refers to a specific syntactical system; a theory of first-order logic means a specific set of sentences that are axioms of your theory, rules of inference, and theorems that you derive from the axioms (if any can be derived). What I've said so far is the modern logic of your question. The other terms refer to ancient/medieval/Aristotelian logic and I will let someone else explain them in detail, but I will give a brief overview. Aristotle's logic is called syllogistic logic because it focuses on what are called syllogisms. This is something of the form: 

These are all incredibly comprehensive articles and do a great job condensing the information into one source. You would probably be well off using this as a guide and then following the bibliography and reading primary sources and articles if you want to really understand the material. For additional sources, if you want them, the prominent analytic philosopher Scott Soames as taken it upon himself to author multiple multi-volume series on this subject. So far there are: 

If you are trying to develop Euclidian geometry, this is one of the assumptions you need to make. This, along with his other postulates, are assumed without prior proof and are used as the starting points for logical deductions to be made. We start off assuming that a collection of things are true about lines and points and we then use deductive proofs to prove theorems. Some examples of theorems that Euclidean geometry proves (things like the Pythagorean theorem) are found here. In current foundations of mathematics we use the axiomatic system ZFC set theory. ZFC is also a list of axioms equipped with a proof system that lets us derive theorems, however ZFC is much more powerful than Euclid's postulates. ZFC tells us that sets, a certain amount and kind of them, exist and that those sets have certain properties. The method of using set theory as a foundation for mathematics is done by treating the sets that ZFC talks about as numbers and that the properties that the sets have reflect the properties of actual numbers. Doing this, ZFC is able to prove (almost) all mathematical statements about geometry, algebra, number theory, analysis, and so on. What ZFC cannot prove is its own axioms. However, ZFC can easily prove Euclid's first postulate once a suitable definition of "line" "point" and "straight" are defined in the theory. Doing an actual deduction in ZFC is very tedious; you can see here for the details of as well as the actual proof of 2+2=4 in ZFC. So hopefully it is clear now why the relative assumption of an axiom in one case doesn't mean it is "without a proof" in all cases. But, this also sounds like it might end up in circular reasoning, so let me show how that is dispelled. Obviously, if we say "well in one theory we have axioms A and B and those axioms prove C and D, and in another theory we have axioms C and D and those axioms prove A and B, so we have a perfect set of theories!" then we have circular reasoning. While those statements may be true, it would be dishonest to use them to justify each other. We need to pick one theory as a bedrock and have those axioms assumed in all cases, otherwise everything would be circular. Philosophical Justifications for Axioms If we agree that it is essential to have a bedrock of assumed axioms that will never be justified in our theory, how do we choose them? In mathematics, especially after the formalization that happened after the foundational crisis around the turn of the 20th century, philosophers, logicians, and mathematicians have spend considerable time discussing the axioms we eventually settled on (ZFC) and how we can justify what they say. Honestly, a lot of these ideas are not new and go back to the ancient Greeks. A main idea is that the axioms are self evident, meaning that when we think about what they mean, they seem plainly obvious, even definitional. This is what Euclid thought of his postulates. There are translations of the Greek word "axioma (ἀξίωμα)" that include "self evident" in the definition. Now, take for example the ZFC axiom of extensionality. This axiom states that if two sets have exactly the same elements then those two sets are equal to each other. This axiom, although it cannot be formally proven within ZFC, seems almost definitional to the idea of equality, doesn't it? It also is a perfect candidate for something that is self evidently true. Is it possible for two things to be made of exactly the same things and yet be different? It seems, at least to our rational intuition, that that cannot be true. A contemporary defense of the ZFC axioms, as pointed out in the comments, is Believing the Axioms by Penelope Maddy. In it, Maddy even argues that the axiom of extensionality seems to be the most definitional of all of the ZFC axioms. The Validity of a Proof In regards to your final question, the answer is no, there is no relationship between proof in mathematics and faith. Mathematics uses deductive reasoning, in the form of a formal proof system which are objects studied in proof theory. During a mathematical proof, you start with some statement and then apply a rule of inference to gain a new statement. In mathematics, we start with axioms and use our rules of inference to derive theorems, and often times we then can use those theorems to derive more theorems and so on. However, this method of inferring conclusions from premises has nothing to do with faith or with assumptions. In proof theory, a set of rules of inference can have two important properties: validity and soundness. A valid argument, where argument means a specific instance of some premises and a conclusion, is one where "if the premises are true, then the conclusion must also be true." An example of a valid argument is: