Yes, move it. The extra join overhead will be minimal (if the right indexes are in place!) and you will get a performance improvement for all the other queries which don't use addresses because you will get more rows per page in and . Down the road, any master data management (MDM) or data clean-up will be easier with the values all in one place. Sticky points to look out for are common definitions of column semantics (is the city, say, for absolutely every possible address?). This could be important for reporting, for example, or summarising sales by region. Uniqueness could be difficult to define for any natural keys you may choose to create on . 

One concern with a large column counts is to do with the number of rows which can fit in a page. DBMS performs IO one fixed-size page at a time. If any part of a page is needed, all of it is read into memory where the required bit is processed. So, if there are a small number of short columns many rows can fit in a single page. Conversely tables with many, long columns can only fit a few rows per page and more IO is required to retrieve a given number of rows. Is this good or bad? Well, it depends. (If you're new to database design you'd better get used to hearing that phrase!) If you only ever look for a single row at a time it would be better to have all the columns in single table so one IO will return all the values to memory. They are likely to stay there for a while, meaning subsequent actions on the row will not have to do physical IO. This is great for a typical transaction processing workload where an item goes through a number of steps until it reaches completion. If your workflow is more BI oriented, where you need sums and averages and whatnot, then a query will retrieve many rows and will benefit from having as many rows per page as possible. Ideally, those rows should only have the columns required to answer the query. Other columns would be better held in a secondary table. Sadly, few systems are this cut-and-dried. Often there is a mix of OLTP and BI work. No one query is so much more important than the others that it drives the DB design. The additional IO cost of referencing a secondary table can be troublesome. This is where the art takes over from the science of being a DBA. You have to understand the intended usage and juggle options accordingly. For my two cents, without any understanding of your actual workload whatsoever, I'd suggest you keep everything in a single table for now. Anything else is premature optimisation. When you run into an actual problem you can't solve another way, that's the time to split tables. The additional complication of keeping the secondary tables synchronised with the primary can be surprisingly tricky. Before then covering indexes will help a lot. Materialised views are useful, too. Neither are free lunches, of course, so understand the costs and benefits before deploying either. Should you decide to split over several tables I'd suggest keeping together those columns which are used together in a single business action. Mixing read-only and updatable columns in a table is not a problem. I can think of no advantage of separating them on those lines only. 

Is Azure DB mature enough? As I understand it, it is SQL Server, but tailored to a cloud environment. Rumour has it it is actually vNext and new code is first deployed on Azure, then released as on-prem CUs. So the question becomes is Microsoft's cloud ready for production, but that will be a common factor in any of the options you're considering so is moot. Currently I work for a small start up. All our environments are on Azure and work well, though our current requirements are quite modest. Previously I've worked as DBA and developer for large multi-nationals. I would have no hesitation suggesting a heavy workload be deployed on an Azure DB. 

Sum the AccountTransaction tables for your participant ID - these accounts record the units assigned to you and, presumably, sitting in your warehouse. 

Each participant needs an account per product since there is no direct translation from e.g. sheet to circle, according to your description. Even from circle to rolled circle there may be wastage during handling so sending out 100 units does not mean 100 units are always returned. It's a lot like owning some dollars and some Yen - you can convert each to the other, but the point-in-time exchange rate fluctuates. Each must be held in its own bank account. You yourself will have an account for each of the five products you deal with. This represents the items sitting on your shelves at any point in time, waiting for further work or to be sold. 

When the referenced data exists independently from the referencing data. In this example, is it useful in the problem domain to know what the possible life states are even when there are no rows in . When the values can only be drawn from a known, finite set. This is often referred to as "reference data". A foreign key constraint can enforce this. When referential integrity must be maintained between two (or more) normalized entities. This will be for transactional data where the values will not be known in advance. For example where an application allows the addition of new users at run-time, and has on each table. Again a foreign key will enforce this. When additional domain values are discovered that depend sole on the life_state. This is the usual key dependency normalization. When meta data is required e.g. for "soft deletes" instead of removing the value from the system entirely. 

The query optimiser should be considered, too. Its role is to take your declarative SQL and translate it into procedural steps. To find the most efficient combination of procedural steps it will examine combinations of index usage, sorts, caching intermediate results sets and all sorts of other things, too. The number of permutations can get exceedingly large even with what look like quite simple queries. Much of the calculation done to find the best plan is driven by the distribution of data within the tables. These distributions are sampled and stored as statistics objects. If these are wrong, they lead the optimiser to make poor choices. Poor choices early in the plan lead to even poorer choices later on in a snowball effect. It's not unknown for a medium sized query returning modest amounts of data to take minutes to run. Correct indexing and good statistics then reduces this to milliseconds. 

A better design would be to have a single table to hold all the parties to a payment. If necessary, sub-type this table so companies, proprietors and customers have their own, unique sets of columns (or a separate, related table). The payments table will then have two foreign keys to , let's call them and , along with amount, date etc. This way all permutations of money transfer can be tracked. To find all the monies a person has received use 

Just for giggles I started a VM with SQL Server 2017 RTM, default installation. I set it to . The user names were created from a SEQUENCE. It took a while, and the creation rate decreased over time, but it created over 270,000 users before I gave up. If there is a limit, it's greater than that. 

So let's think forward a little. The application goes live, the tables are created and everything's lovely. Then, you have a new requirement and the tables must change. How do you code that? There's the CREATEs for environments where the code's never run, plus a bunch of ALTERs for existing environments. Now another requirement comes along. This time you have to migrate data for some reason or other. Now there are the CREATEs (for new environments), the ALTERs (for existing environments) and a whole bunch of DDL and migration logic. But wait - there's more. For really good business reasons you want the column holding the migrated data to have the same name as an existing column. How do you now tell if the database has been migrated or not? No longer do you have a metadata-only lookup. No, now you need a flag or version table to show what DDL has to run at each and every execution. Yeah! You're a great success. Zillions of people want to use your service. Venture capitalists wheeling barrows of money to your door. Third-party organisations are begging to partner with you. The dev team's doubled (nay, trebled!) in size to handle the business opportunities and customisations. Oops. These all have to be coded in the API start-up code. Each. And. Every. One. You no longer have an API. You have the mother of all migration scripts with a fragment of business code tacked on. :sad-face: 

It will pick the cheapest one. It is the optimiser's job to examine various physical implementations of the query (use and index, scan all rows, sort the data etc), apply heuristics to assign a cost to each permutation and deliver the cheapest (in terms of estimated elapsed time) to the execution engine as a query plan. Note that the cheapest need not be the least complex in Big-O terms. Big-O notation shows how the effort required changes as the number of items approaches a very large limit. Most databases, however, do not have an infininte number of rows. The optimizer has information about the actual number and distribution of values within the dataset. These are stored in internal statistic objects. Let's take an example where we have a BTree on M which is 3 levels deep. The full table occupies 1,000 pages on disk. Remember that all data movement happens one page at a time. If the given query were likely to return only a single row it would be efficient to use an index on M to read four pages in total (3 from the index on M and one from the table to get the corresponding value of A). On the other hand, if the statistics suggest that most of the data pages will be read it will be more efficient to ingore the index and scan all data pages, ignoring the ones which do not match the predicate. I have never heard of binary search implemented in a DBMS. BTrees offer the same functionality with greater performance. Linear search (also known as a table scan) will be used either when no index exists, or when the overhead of using the index is more than the benefit of identifying individual rows. BTrees are not the only type of index. Single-value lookup and table scan are not the only possible algorithms. 

Normalization is a set of techniques to avoid certain problems viz data update anomolies. In a perfect world you would not store structured content but split it into its constituent parts across however many table are required and re-construct each complex object at runtime (1st normal form). However .. if pre-parsing the object and storing the interesting / frequently used bits somewhere else resolves a performance issue then do this as long as you do it with your eyes open. If you, and everyone else who may code against the database, understands that extra work must be done in the application in order to achieve the necessary response times then that's a design choice and necessary in the real world. 

It sounds like there are two classes of query - one to understand which locations lie within the current view window and a second to deliver the desired statistic for those points. My suggestion is to use separate, specialised tools for each. I'm assuming all measurements relate to the same set of 75Bn points. These lat/longs, once established, are therefore static. They can be grouped, aggregated and indexed at a one-off cost. Therefore I would suggest sharding by region and zoom level. The size of each shard will be driven by the performance that can be achieved from each GIS instance. The GIS will return a set of points that are passed to a time series database. This holds the measured values and performs aggregates. KDB is one I'm aware of. It targets securities trading, which will have fewer keys but more data points per key than your scenario. There will be a cost to transferring the key values from the GIS server to the timeseries DB. My hypothesis is that this cost will be paid back by the faster processing in the task-specific timeseries DB. From the wording of the question it seems that a single instance will not be able to hold all data so some cross-server traffic seems inevitable. Given the relative speed of the components it seems likely sending a keyset to a remote server which has the data cached will be faster than reading the data off local disk. If the point-finding and value-calculation parts can be local to each other then of course I would expect response to be faster. My (limited) understanding is that finding the N closest neighbours to a given point is a non-trivial task. This is why I suggested using specific software to perform it. If the point-finding can be reduced to 

I would definitely advise using intersection and lookup tables on the staff side. On the parent side it is more of a judgement call. Still, I'd use the intersections. Although you only have two parents you loose almost nothing by using the intersetion table (a little complexity) and gain flexibility. You should be able to hard-code the ParentStatus into whatever populates the 'mother' and 'father' fields (not an Access guru, sorry) of the example form, thereby maintianing the specificity of the original schema. You will be able to code other screens to allow for the muliplicity of blended families, for example. If it's important that each child has at most one "mother" and one "father" you can put a UNIQUE constraint on and . 

I'm not familiar enough with Access' syntax to give you a cut-and-paste answer. In pseudo-code, though, you can use something of this form: 

Overnight maintenance creates a new data table, drops the old ones and re-generates the definition of the view. Read processes never have to change. Write processes may be trickier, depending on MySQL's abilities (I'm SQL Server centric). Option 1 - dynamic SQL to always write to the current table. 2 - constant name for the current table, and rename during maintenance. 3 - write to the view and let the RDBMS sort it out. 

I would be tempted to deploy all of the techniques you've mentioned. Let me explain. The normalised Service/ City/ State/ Country is great for OLTP processing. So retain it for that part of the application and treat it as the actual data store. As you mentioned, this is likely to make search complicated so denormalising would be advantageous. Unlike your proposal I would denormalise into a separate table which exists only to assist search. It is an explicit acknowledgement that a real DBMS on an actual computer does not have the performance characteristics the theoretical computer scientists would like to imagine. By putting it in a separate table one isolates the compromised physical necessity from the preferred, normalised design. Finally I would combine all the search terms in a single column in this denormalised table. One way would be to together the Service Name, City Name, State Name and Country Name, space delimited, and place a full text index on that column. Another would be to have three columns - , and . The first two come from the data values listed in the question. says which of the normalised tables this comes from. Sample data, taken from the question, would be: 

This is fairly ugly. You'd be lucky to avoid two scans of . Ideally you'd want filtered indexes but IIRC these aren't availble until SQL Server 2008. The applicaiton would have to know how many rows there were per , set flags accordingly and maintain those if the number changed. Just use the n:m approach. 

The partioning column must be in the clustering index. Therefore, if the value of IsDeleted changes the row has to move from one bit of disk to another. If your intention in using the flag was to defer the IO cost of the delete by performing an UPDATE instead of a DELETE this solution is counterproductive. With it you will be perfoming both a DELETE (from the row's current location) and and INSERT to its new location. If the intention was to isolate archive data then you are onto a winner. I would suggest you use filtered indexes on the active partion of your table, too.