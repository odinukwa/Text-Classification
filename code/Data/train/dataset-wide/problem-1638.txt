I don't really think I fully understand the first part of your question. Redshift is measured by comparing the wavelengths of a redshifted pattern of absorption or emission lines with the wavelengths they would have in an object at rest. All lines are shifted by the same factor of $1+z$, where $z$ is the redshift. Gravitational waves cannot be used to obtain a redshift because the intrinsic wavelength of the waves is not known. An inspiralling binary emits waves with a frequency equal to twice the orbital frequency. This in turn depends on the mass, but the observed frequency is also redshifted. There is thus a "degeneracy" between the mass and redshift. What you can do with gravitational waves is estimate the physical distance to the source using the wave amplitude and how quickly the amplitude changes with time. If the host galaxy of the source can then be identified, then you can use the redshift of that galaxy to get an estimate of the Hubble parameter (e.g. see Abbott et al. 2017). 

When the diagrams were constructed, it was not at all clear what the sequence of spectral types or spectral type indicators actually meant. It turned out of course that the sequence (in modern day parlance O,B,A,F,G,K,M) actually corresponds to decreasing temperature. Astronomers have simply stuck with this convention to the present day, there is no particular reason for that. Most HR diagrams are now plotted with temperature (decreasing) along the x-axis, although that is not what the original HR diagram was. 

HR diagram showing the location of the subgiant and red giant branches on the evolutionary tracks for stars of different mass. The tracks begin at the main sequence and evolve to the terination of the main sequence, across the subgiant branch, turn up the red giant branch, which terminates when core He burning commences and the stars become less luminous and hotter. 

The convective overturn time is the typical timescale for a convective cell to rise in a gas. Imagine a "lava lamp" - it's the time for one of the blobs to rise from its lowest to highest point. I am most familiar with its use in stars, where convective energy transport is modelled using a mixing length. This posits that the typical height travelled by an adiabatic convection cell is some multiple $\alpha$ of the pressure scale height $H_p$ (the typical length on which the internal pressure changes significantly). The convective overturn (or turnover) time is then $\alpha H_p/v$, where $v$ is the velocity at which convective cells rise. 

It is a bright star. You can see the "diffraction spikes" that are the signature of a point-like object. The star will be in our Galaxy, so is certainly closer than the galaxy in the image. However, this is not why it appears to be big. The image appears big because the starlight will be spread by the telescope objects over a finite area - the point spread function. In addition, when a star is very bright, the central CCD pixels saturate and charge can spill into adjacent pixels. Even if that were not the case, there is enough light in the "wings" of the point spread function to produce a large image. 

It's not completely clear what you are asking, but if this is a multi-choice quiz, then the only option that could be correct is (a). (b) Is not correct, because a white dwarf that just passes the Chandrasekhar mass is comfortably below the maximum mass that is supportable by a neutron star. So neutronisation followed by neutron degeneracy pressure and the strongly repulsive nuclear force between neutrons at small separations ought to be capable of preventing black hole formation. (c) Is not correct because by definition, type II supernovae result from the collapse of massive stars. Observationally, they are distinguished by hydrogen absorption in their spectra, but since a white dwarf will contain little if any hydrogen, then this is not possible. (a) Might happen. As mass is added to the white dwarf it will become smaller and denser. It is possible that nuclear reactions (carbon fusion) might begin. Because the white dwarf is supported by temperature-independent electron degeneracy pressure, then the nuclear reactions take off at constant density, but the increasing temperature leads to runaway nuclear reactions that detonate the star as a type Ia supernova. 

Maybe someone can give chapter and verse on this, but my understanding is that bound objects are unaffected by the expansion for the case of dark energy represented by a simple cosmological constant. But a "big rip" will occur in a finite time if the dark energy equation of state $P = w \rho$ is characterised by $w < -1$. In this scenario, the expansion accelerates so quickly that the scale factor becomes infinite in a finite time and structures must disintegrate when the size of the observable universe becomes smaller than that structure. The present state of play is that $w$ appears to be very close to -1 (a cosmological constant). But given that dark energy is not understood, who can really say whether it might change in the future... 

A supernova may actually be necessary in the creation of a stellar black hole. At the ends of their lives the cores of massive stars are made mostly of iron-peak nuclei from which you cannot extract more fusion energy. To support their weight, these stars rely on electron degeneracy pressure - the pressure caused by the Pauli exclusion principle allowing no more than one electron to share the same quantum state. In principle a star might be supported by degeneracy pressure forever as it gradually cools - this is the fate of most white dwarfs. However, the core of a massive star is just too big for that to work. The density increases until all the electron are moving at close-to the speed of light and that's as high as the degeneracy pressure can get. If the core exceeds the Chandrasekhar mass, it will collapse and as it does so, the rest of the star collapses with it (a little more slowly). The collapse is triggered by the removal of electrons by electron capture into nuclei to form neutrons. At some point enough neutrons are produced for neutron degeneracy pressure to halt or at least slow the collapse. This and the release of a lot of gravitational potential energy are ultimately what power a supernova explosion. But if the collapse is not halted then even neutron degeneracy pressure will not support the star and collapse to a black hole becomes inevitable. A black hole status is reached once a proportion of its mass is compressed inside its Schwarzschild radius $r_s = 2GM/c^2$. i.e. once its density achieves $$ \rho > \frac{3M}{4\pi r_s^{3}}$$ i.e. when a central mass $M$ has a density that exceeds $$ \rho > \frac{3}{32\pi} \frac{c^6}{G^3 M^2} = 1.8\times10^{19} \left(\frac{M}{M_{\odot}}\right)^{-2}\ {\rm kg/m}^3$$ This is a ball park figure and assumes spherical symmetry and neglects any detailed GR treatment, but is more or less correct - a few times higher than typical neutron star densities. In other words it is the density of the material that largely determines whether something becomes a black hole. The mass is only an indirect parameter. 

Emission measure is (usually) used in X-ray and EUV astronomy, though I suppose also in cases of optically thin radio emission. It is defined as the square of the number density of free electrons integrated over the volume of the plasma. $${\rm EM} = \int n_e^2 \ dV$$ The flux of optically thin emission from a plasma (e.g. thermal bremsstrahlung) is then directly proportional to the emission measure of the plasma multiplied by a temperature dependent cooling loss law. In other words, when you measure the flux of X-rays from an unresolved optically thin emitter, there is a degeneracy between the electron number density (squared) and the overall plasma volume. When you fit an X-ray spectrum with an optically thin model, the emission measure (divided by $4\pi d^2$, where $d$ is the distance to the object), is a multiplicative free parameter. Your question about calculation is extremely difficult to answer. Suppose I measure a count-rate of $N$ X-ray counts per second using some X-ray telescope (I can only assume that's what you mean by a "X-ray light curve".). The count-rate received at the telescope depends on: the emission measure (as defined above) multiplied by a term that depends on the temperature (or temperatures) of the source, the chemical composition of the source and the adopted emission process (is it free-free thermal bremsstrahlung, a thermal plasma or something else). It is then attenuated by any intrinsic absorption in the source and any absorption between us and the source and by the distance to the source (assuming the radiation is isotropic). Finally what is detected is determined by the response of the X-ray detector to X-ray photons as a function of energy. 

So that is why I claim the Sun can be classed as a "third generation star" - it contains atoms/nuclei that must have been inside at least two previous stars. But you should not take this too literally. There are grains of material trapped inside meteorites that consist of solids that were already present in the pre-solar material. These are important because these grains were thought to have formed in individual stellar events and their isotopic compositions can be studied. These tell us that the Sun formed from material that has been inside many different stars of different types. Stellar evolution and nucleosynthesis calculations tell us the same story. For example, whilst most of our oxygen was made in massive stars that underwent a core collapse supernova, such events do not produce that much carbon. The C/O ratio tells us that most of our carbon comes via the winds from intermediate mass AGB stars. Heavy elements like uranium may be dominantly produced in neutron star collisions, but others like barium and strontium are not. The details of how many ancestors have contributed to the Sun has no simple answer. Much of the solar hydrogen and helium could be pristine; some will have been through more than one star. Heavier elements (bar some lithium) will have been through at least one star. The fact that we have s-process elements like Ba, Sr, La and Ce, which are formed by neutron capture onto iron-peak elements, tells us those have been through at least two stars. However, these are vast under-estimates. Mixing in the interstellar medium is reasonably effective. The material spewed out from supernovae and stellar winds 5-12 billion years ago has had plenty of time to mix throughout the Galaxy before the Sun's birth. Turbulence and shear instabilities, driven by the winds ans supernovae from massive stars, should distribute material on galactic length scales in a billion years or less (Roy & Kunth 1995; de Avillez & Mac Low 2003), though local inhomogeneities associated with nearby recent events can persist over $10^{8}$ years. If this is the case, then the Sun is the product of the $\sim$ billion stars that died before it was born. The reason you are confused with your lifetime argument is that you have ignored the possibility of the Sun being made from stars that lived at the same time in different parts of the Galaxy. The material that they ejected near the end of their lives has just been thoroughly mixed up. $^1$ The rest are produced by the s-process in intermediate mass AGB stars; through nova events on white dwarfs; or perhaps in the case of the heavier elements, through the collision of neutron stars (see this Physics SE question). $^2$ A rough expression for the lifetime of a star is $10 (M/M_{\odot})^{-5/2}$ billion years. 

The straightforward definition is in terms of where a star lies on its evolutionary track in the HR diagram (see below). The subgiant branch stars are those which have exhausted their hydrogen core and which are burning hydrogen in a shell but their He cores have not begun to contract significantly. The distinct upturn in luminosity marks the beginning of the red giant branch. This occurs when the core grows significantly more massive, cannot support itself hydrostatically and begins to contract. At the same time the envelope expands and becomes convective and the H-burning shell move inwards and increases in temperature and luminosity. The tip of the red giant branch is where He is ignited. This takes place "explosively" in a degenerate core if the star is above about $2M_{\odot}$ (not $5M_{\odot}$), but begins smoothly in higher mass stars. This causes the core to expand, pushes the H-burning shell out and reduces the luminosity. 

The HR diagram is an observational diagram. Whilst neutron stars could be placed in the HR diagram in the same way as white dwarf stars are, it turns out to be impractical to do so because the photospheric luminosity and photospheric temperature of neutron stars is next to impossible to determine. The reason for this is two-fold: (i) Neutron stars start off very hot (interior temperatures of $\sim 10^{10}$K and photospheric temperatures of $\sim 10^{7}$K, but they cool very rapidly. Within 10,000 years after the originating supernova they will have cooled below a million degrees, then photon cooling takes over from neutrino losses and they cool to a few thousand degrees within 10 million years (e.g. $URL$ ). There are many uncertainties and unknowns in these processes. (ii) the photospheric emission is usually dwarfed by emission from the magnetosphere or luminosity due to accretion from a companion or the interstellar medium. One can theoretically work out where neutron stars should be by assuming that the emission is like that from a blackbody and that the radius $R \sim 10$ km. In that case neutron stars lie on a locus defined by $$ \frac{L}{L_{\odot}} = 1.9\times 10^{-9} \left(\frac{T}{10^{4}K}\right)^4 $$ So, contrary to what you you say in your question, most neutron stars could be cool and very, very faint and spend the majority of their (cooling) lives at the bottom-left or even bottom-right of the HR diagram. There is actually a huge uncertainty over where neutron stars would appear on this locus. Their very low heat capacities means that any "reheating processes" could very effectively raise their temperatures. Such processes include Ohmic dissipation of the magnetic field, some kind of thermalisation of their rotational energy or accretion from the interstellar medium. For the latter, luminosities of $10^{20}-10^{21}$ W may be possible, implying effective temperatures of tens of thousands of Kelvin. A neutron star at the same temperature as Sirius would have an absolute visual magnitude that was about 22 magnitudes fainter, $M_V \sim 23$. Another way of visualising this is that the neutron star cooling sequence is roughly parallel with the white dwarf cooling sequence but about 13 magnitudes fainter. You never see this locus shown on an HR diagram because it is usually way off the bottom of the plot. 

The stars in the Galactic bulge are predominantly metal-rich (by that I mean have a metallicity similar to the Sun or even a little higher). Even though these stars are predominantly old, the bulge is thought to have formed extremely quickly and the interstellar medium from which the stars were formed would have been enriched with metals very quickly. Here is a plot from Zoccali et al. (2009) (which I recommend reading). It shows the metallicity distribution of many stars in the Galactic bulge, measured using high-resolution spectroscopy. It shows that the highest metallicity stars are towards the middle (it is very hard to get samples right in the middle because of extinction) and the averge metallicity falls as you mover further from the centre (the samples with more negative Galactic latitude $b$.). The percentages in the plots are the estimated contamination from the disk population in each sample. 

The minimum mass of a "planet" forming from a gas cloud (definitions of what a planet is are rather slippery, and some would say this is not a planet at all) is not determined by the time available. The collapse process is rapid - less than a million years. There is a minimum mass though, and what you are referring to is something known as the fragmentation limit. A cloud becomes unstable and collapses if its mass exceeds the Jeans mass. The Jeans mass depends on temperature to the power of 3/2 and inversely on the square root of the cloud density. $$ M_J \propto \frac{T^{3/2}}{\rho^{1/2}}$$ When a cloud of gas collapses, its density increases. If it is able to radiate away heat efficiently, then its temperature can remain more-or-less constant and so the Jeans mass decreases. This allows the cloud to fragment into smaller pieces. The minimum mass that can collapse in isolation will therefore be set by the smallest value that the Jeans mass can attain as the collapse proceeds. This fragmentation limit is in turn set by the cloud becoming opaque to its own radiation, which occurs when the density becomes large enough. At this point, the cloud can no longer efficiently get rid of all the heat that is generated in its interior by the work done by gravity in squashing it. The temperature rises and the Jeans mass stops decreasing. Now, the cloud may still collapse, but it won't break into smaller chunks. The fragmentation limit is difficult to calculate with any accuracy, because it depends on the 3D turbulent dynamics of a collapsing cloud and also on whether the cloud is spinning. It is generally thought to be in the range one-to-a-few times the mass of Jupiter (e.g. Whitworth & Stamatellos 2006). This is far below the minimum mass for hydrogen fusion of about 75 Jupiter masses or deuterium fusion of about 13 Jupiter masses. Evidence that such objects may exist can be found in answers to the related questions How are rogue planets discovered? and Is there any hard evidence that rogue planets exist?