This analogy happens to have the bonus side-effect that, as most arguments so far have been based on the magnitudes of ontological contents rather than the magnitudes of ontological expanses, it now accommodates this concern by having built into it the altogether irrelevance of the magnitudes of ontological contents. So, to answer the original question, there's nothing to worry about. In comparing modern humans to dogs or to early humans with tight extremophilic analogies, ultimately no great filter to superintelligence or beyond is implied. 

The propositions ~C, ~D, and ~A were not contradictions of any propositions that you derived. For example, up to those points, you had not derived C, D, or A. If you would have, you would have arrived at a contradiction sooner than when you arrived at B and ~B. For your first question, here's a much a simpler problem, and with words, so it's easier to get the idea. 

A dog's ontology is presumably quite limited. A human's ontology is apparently maximal. While our average intelligence is quite a bit more than it is for dogs, it's not "godly" by comparison (maybe "demi-godly" but not godly, since for instance we can't create dogs from raw materials right now). But our ontology is maximal. If the analogy should carry over, as it seems it should, as in 

It can be done in one step. The first man sees two white hats and says that he knows. But, you say, the problem says that the first man says "I don't know"? Exactly. The problem's definition is also specified by the second man saying "I don't know." Therefore, if you can break the definition of the problem for your two steps, I can break the definition of the problem for my one step. ... 

It doesn't seem difficult to determine the appropriate nature of our mind-to-world intentional states. Their appropriate nature is that they match accurately and precisely to the world, perhaps maximally so (except where perhaps this doesn't infringe excessively on most others' privacy). It feels as if this is an obvious and undeniable logical definition. However, it seems much more difficult to determine the appropriate nature of our world-to-mind intentional states. Their appropriate nature is... what? While one good first answer might be that their appropriate nature is that they reflect end-states that already have well-defined routines for satisfying them, we know this answer comes up short. We regularly see people solve problems that at least in large part had no known well-defined routines for satisfying them. This meant that they posed a problem that hadn't been posed exactly like it before (or finally solved one that had been such an open problem). What exactly is the permissibilic mechanism for people to pose unique (previously unsatisfied) problems, unique world-to-mind intentional states? Sometimes people pose unique problems and other people like them for it, and sometimes people pose unique problems and other people hate them for it. Many people act as though they have a thoroughgoing familiarity of the permissibilic mechanism, that here too there's an obvious and undeniable logical definition in effect. But where's the manual – or what is it like – on the necessary and sufficient constraints on posing previously unsatisfied problems, on formulating previously unsatisfied world-to-mind intentional states? 

I can see the power behind the argument because to get around it a person would either have to stretch the imagination into areas that might seem implausible or have to solve a lot of academic problems, if the person is to be satisfied that Mary has all the facts and that all the facts are physical facts. Foregoing the route of solving a lot of academic problems for now, I think we can continue to think about Mary in a way that derives directly from your formulation, RECURSIVE FARTS. The original Mary is characterized with the standard scientific kind, as having descriptive knowledge. If it's implied that she has procedural knowledge, which standardly tends to be an engineering kind, it's not very much. In domains where descriptive knowledge is enough, interfacing with things around you tends to be enough. You don't need to begin an approach to knowing the inner workings of things to the extent of knowing how to reconstruct them or something like them. That is, you don't need to begin an approach that's pointed to knowing things in themselves. At most, you'd need to know enough to create simulations, which, while a lot of times very sophisticated, is not the same. But descriptive knowledge isn't always enough in all domains where physical knowledge is required. Additional knowledge is required to assemble basic components (which are usually for larger systems). Without getting into whether or not Mary has the "right kind" of knowledge about colors, we can already say she lacks some physical knowledge. That is, she doesn't know how to assemble basic components. Or, at least, she doesn't have the facilities to assemble basic components and gain the knowledge of herself going through the process of assembling the basic components. Indeed, this would be just like you said, this would be a posteriori knowledge, RECURSIVE FARTS(). Now that we may begin to question the sobriety of the original caricature of Mary, we may be able to close in on settling the issue. Even letting alone the question of the life-changing subjective quality of strolling through the wilderness, we know that as it stands Mary neither has enough knowledge ab intra nor has enough knowledge a fortiori. However, provided proper education and further facilities, whether industrial or surgically integrated with her body, she would simply demonstrate her sufficiently complete knowledge of the physical world. First she would make basic nanoconstructors. Then she would make physical replicas of everyday inanimate objects. Then she would make physical replicas of archaea. (...) And finally, for now, she would give you a neural implant to complement hers for bidirectional communication of art works and anything else that could be seen in virtual reality. Of course, the tests she had to perform to get such magic right must've involved having knowledge of the physical world to such a degree that it went much beyond descriptive knowledge, and into its superset of acquaintance knowledge. Having physical descriptive knowledge is simply not the only kind of physical knowledge to have. 

I imagine this being filled with the Lord is a state of "extreme passion", but not "selfish attachment" from which one is detached. 

One way of looking at guilt is to acknowledge that the experience of moral guilt is real, but that it does not mean anything. That is, the human moral faculty is not reliable although it is tempting to believe there is more meaning there than there is. Alternatively, if one wants to take the experience of guilt more seriously then everyone is guilty as David Simpson describes how the character, Clamence, experiences guilt in The Fall: Internet Encyclopedia of Philosophy 

Although that statement makes sense based on the principle of sufficient reason, it doesn’t lead to theism unless somewhere in that event chain of causes there is more than impersonal event causation. There must be agent causation somewhere in that chain for a God to exist. This agent makes a free choice. See pages 64-67 of William Lane Craig and Quentin Smith’s Theism, Atheism, and Big Bang Cosmology where Craig uses Kant and al-Ghazali to argue not just for a cause of the universe, but for a personal Creator. To say “the world is a sum of collections of events” also asserts determinism which is an extreme way to assert atheism since it not only denies the agency of God, but also our own agency. Even if one includes indeterminism, which is often how quantum collapses are described, that randomness description side-steps the existence of agents. If the world is truly the sum of collections of events then so are human beings. That means we cannot choose to believe or not believe. That we actually do believe and disbelieve many things is evidence that determinism is not correct. See Richard Taylor, "Freedom and Determinism" in his Metaphysics for one libertarian approach to agency. 

If one looks at scientific law as a way to describe why something happens the way it does and not some other way, then having a multi-verse where everything happens in some universe would not ground why something happens in this particular way in our universe except to say that it is the manifestation of one of many random possibilities. In Reinventing Gravity John W. Moffat, a physicist who developed modified gravity, a relativistic and deterministic gravitation theory that does not require dark matter, wrote (page 98): 

There may be other ways around the problem, but just restricting omnipotence to doing only what is logically coherent is all that is needed. 

One could view calling someone a “bloody idiot” as rhetorical satire rather than a fallacy. See this discussion of “Satire in Rhetoric”: However, as that article mentions such satire could succeed as an “effective rhetorical tool” or it could be “misconstrued as offensive to certain audiences depending on the topic and the situation”. That your rhetorical efforts received objections and were described as a logical fallacy suggests that the audience was not amused, and most importantly, that they were not convinced. If that was the case, and assuming you were truly interested in convincing the audience of some position and not just engaging in verbal abuse, it would be advisable to try an alternate rhetorical approach besides satire. 

Aristotle’s four causes are not obsolete. To get more information on this with an emphasis on how modern these causes are and examples of the other three see the first half of Jack Sander’s Philosophy of Science Lecture 8: Scientific Explanation. To read more about Aristotle’s four causes see the Stanford Encyclopedia of Philosophy entry, Aristotle’s Natural Philosophy: Explanations are for our use and they are not complete. We sometimes think that if someone can explain something that they have given us a complete explanation of something. As you can see in the example above, explanations are at best useful and always partial. 

Consider the specific questions. Question 2: Has the question of agency been studied in philosophy of mind and philosophy of artificial intelligence? Regardless of what has been done, one can start to question it now by defining the choice of a freewill agent and the state of a deterministic machine. The following definitions are provisional especially the use of "something". Define the choice of a freewill agent as something for which there does not exist a complete explanation. Define the state of a deterministic machine as something for which there exists a complete explanation. An answer to the first question follows from these definitions. Question 1: Does this autonomy/agency aspect of a program really make it possible to distinguish intelligent AI from "just software"? Since there does not exist a complete explanation for choices made by a freewill agent but there do exist complete explanations for states of a deterministic machine, deterministic machines are not freewill agents. That implies there is no way, based on agency, to distinguish intelligent AI from just software. Both “intelligent AI” and “just software” are deterministic machines and not freewill agents. Question 3: How is agency (as described above) related to intentionality and to Daniel Dennett's intentional stance? Dennett, based on the provided quote, apparently needs a “rational agent” to get intentionality and perhaps agency. Are agents in general “rational”? Do agents in general even have a brain? These questions are not merely rhetorical. Note that John Conway and Simon Kochen’s “The Strong Free Will Theorem” proved that if humans have free will then so do quantum particles. Quantum particles do not have brains. They are not rational. A brain is not necessary for agency. When Dennett restricts agents to “rational agents” I suspect he is simplifying the problem so he can intelligibly explain it with his theory. Such simplifications may produce results that do not adequately reflect reality. That is, Dennett’s explanations may be neither correct explanations nor complete explanations although they may be useful for AI. The correctness and completeness of his explanations for human reality require justification. In the next references to Alan Turing and Jonathan Haidt, I will argue why such a justification may not be possible. Turning’s “Computing Machinery and Intelligence” expresses a concern about extrasensory perceptions: 

That Harris, or someone else, states something confidently does not mean it is true. You will have to make a choice and decide for yourself if the assertion is true or not. There are two cases to consider: Either we have free will or we do not. Case 1: Suppose we have free will. If we have free will, then the answer to the question is: “Nothing”. We already have free will. The claim that what we experience now does not include free will is false. To help see this more clearly, suppose someone stated confidently that we do not have fingers even though we are using our fingers to type messages back and forth. They assert confidently that our experiences of having fingers are illusions. They might even argue, if they notice that we think they are nutty to suggest that we don't have fingers, that our brains are in some vat or we are part of a simulation. That often works now-a-days to get otherwise intelligent people to doubt their experiences. To further make their point that we do not have fingers they might even ask us, “Suppose you did have fingers, what would that give you that you don’t have now?” Well, it would give us nothing that we don't have now, because we already have fingers. Case 2: Suppose we do not have free will. If we do not have free will, then we could not answer the question. Why is that? There are many ways to answer any question. At the very least the word order we might use could be different. We have to make a choice among all the possible variations of each answer we could give to pick that specific answer we actually do give. That requires a choice, but we are assuming in this case that we do not have free will. That means, we cannot make that choice. So, we cannot answer the question. If we do answer the question, then case 1 is true.