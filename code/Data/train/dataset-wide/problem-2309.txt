/netonly tells it to just use those alternate credentials when talking over the network. Change the OTHERDOMAIN\OtherUser to match the username needed to connect to the remote system. And then the last part is the path to Visual Studio 2015. Once that's done you should be able to deploy without supplying alternate credentials again. By the way, I would highly recommend you install the free BIDS Helper in order to leverage the Deploy SSIS Packages feature which makes it easier to deploy single packages. You still need runas.exe though. 

In SQL Server Management Studio you can connect Object Explorer to SSAS, expand to the table, right click, choose Partitions, then click to process one or more partitions and do a Process Clear: 

I would study this white paper and the AsPartitionProcessing sample for ideas. Ignore that it targets Azure Analysis Services as it should work against your SSAS instance if you use that servername instead of asazure:// as the servername. It uses the Tabular Object Model (TOM). 

After pasting the hex text into Sublime I would search for and see what characters were next. If it was I was in good shape, time to move on to the next step of the process. In the above example it is not the character i'm expecting () instead it is . This means the data is bad in the database. After ensuring the data is correct in the database the next place it goes is the filesystem. I used vim to follow a similar process to verify the character is what I need: First I export the data to a file on disk: 

I feel the problem is in the import because the data looks correct when I'm importing it, but I'm not sure what is up... 

If you close Visual Studio then open it from a command prompt with the following command it will prompt you for your password needed to connect to the other server. 

But mainly I would recommend upgrading to SSIS Enterprise edition and using this connector. Here is the one for SSIS 2014 but you can search for other versions easily. The performance of that connector is much improved. 

If your drillthrough command includes a dimension attribute from a many-to-many dimension then it behaves as described: showing a row zero to many times. The easiest solution is to create a drillthrough action and mark that action as the default drillthrough action and specify the columns you want to include in the drillthrough results. If you don't include the many-to-many dimensions then it will stop skipping or duplicating rows. 

After hours of trying to figure out what was going on I finally solved the issue. The issue in my particular case was that the import statement I was using for test: 

Then I needed a hex viewer. Sublime has one, but it didn't work well (search was input in hex and didn't seem to search the whole buffer for me, and I had to have another window open to help me compare the expected text to) so I used vim. To put vim in hex mode I used the command after opening utf8.sql. Vim is nice because it shows the hex on the left and unicode on the right. Additionally allows searching by unicode! So I can input my search term and quickly see the next character, looking for . If I didn't see and in the previous step it was then that means the problem was with the export. This is what the line looks like in vim 

What happened is that developers added new data sources instead of adding tables from an existing connection. In the future go to the Model menu... Existing Connections... Open. Then choose another table or query to import. This process is documented here. I don't believe there is a UI way of switching an existing table to use a different data source. The best approach is probably to just delete and recreate that table with the process above. You should be able to delete the extra unused data sources from the Existing Connections dialog. 

My recommendation is not to attempt to do the sequence logic in MDX. You will be disappointed in performance. I would recommend the following approach: First create a view (or a physical table if you prefer) which returns one row per part/customer/person per day showing the currently effective value. The view would look something like this: 

In my case this was useful for dealing with although I couldn't paste into my sql terminal (however it worked from save command files) 

How I figured it out was by viewing the binary at each stage in the process to ensure it was what I expected. First, the way I discovered there were discrepancies in the first place was by using git. By adding my sql file to git then when I do an export I can do and to see if the data changed. Using this method I isolated one table, then one row, and then one character that was failing to use as my test case. The rest of these steps focus on ensuring that character is right. How to find the character. Look in your data. In my case I had "Spartyâ€™s\" that would look incorrect after export and import. By using a hex viewer (discussed below) I used the y and s characters as the endpoints and looked at what was in the middle to determine the character. Starting with the database. I would select the one column from the one row with the issue as hex like this: 

The only time you need to use Schema Generation is if you did top down development and have never generated the schema before. Now that you have previously generated the SQL schema, you can just script out all the objects in SSMS and provide that script along with the SSAS project to other developers. Then the new developer would deploy the SQL scripts and edit the data source in the SSAS project to point to their database. This is the most straightforward approach I believe. 

You need to process D1 and D5 in the same transaction. Then both will complete processing before measure groups are processed. Make sure you mark Transaction=true (the default unless you explicitly set it to false) and Parallel. 

For some reason, SSMS doesn't provide this New Database menu option for SSAS Tabular. But you can easily create a new empty or shell database with the following XMLA script. Just click the XMLA button in the toolbar and then paste in the following. Edit the ID and Name property to the desired database name. And edit the DOMAIN\GroupName to set the group that has admin permission. This will allow members of that group to deploy over this database without being SSAS admins for the whole instance. The following script is for SSAS 2012 SP1 or 2014 Tabular models: