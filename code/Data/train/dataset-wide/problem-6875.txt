Martingale is a very broad term, sometimes just basically meaning "the future is independent conditioned on today". Of course, any random walk has this property. So do Markov chains. "Martingale" also usually refers to a real-valued random variable that changes over time, but whose expectation is always equal to its current value. An unbiased random walk also satisfies this because, given that the current location of the random walk is $x$, the expected location after $50$ steps is still $x$. It is hard to see how a Markov chain relates to this property in general because its state space might not be the real numbers. The idea of the Doob martingale is that these two cases are almost the same: any time you have a Markov-chain type structure where the future is independent conditioned on the present, and you have a real-valued random variable $X_t$ that is changing over time, you can construct a new random variable $Y_t = \mathbb{E}[X_T | X_t]$, where $T$ is the time that the process stops. Now by conditional independence, $Y_t$ is a martingale. Finally, a Bayesian agent's expectation of a random variable is a martingale, as they receive new information over time. To see this, consider a two-stage process: The agent begins with just a prior distribution on $Z$, then receives a signal $A$ and updates to a posterior on $Z$. Naturally, the posterior will be $\mathbb{E}[ Z \mid A]$. But the prior will just be $\mathbb{E} Z$, and the law of iterated expectation says that $$ \mathbb{E}_A\left[ \mathbb{E}_Z [Z \mid A] \right] = \mathbb{E} Z. $$ You can see this by working it out for yourself using the definition of expectation. 

So for each $i$, we sample $m$ permutations randomly and calculate the average marginal contribution $\bar{X}_i$. For example, if $K=100$ and we want an accuracy of $\epsilon = 0.01$ and a probability of failure of $2e^{-50}$, then we need $2m\epsilon^2/K^2 = 50$, so $m = 25K^2/\epsilon^2 = 2.5$ billion. So we need to sample $2.5$ billion permutations to "guarantee" accuracy of $0.01$, except with a miniscule $2e^{-50}$ chance of failure. As comments have mentioned, there are big improvements for many special cases. 

I think your definition is incorrect, or at least incomplete.** Usually, in a Bayesian game, there is assumed to be a prior distribution on $T$ (where $T = \times_i T_i$). This distribution is called the "common prior" and it is assumed to be common knowledge that types are drawn according to this distribution. In this case, each player $i$'s belief $p_i$ is given by Bayesian updating on $T_i$ and this prior; and in BNE player $i$ must be best-responding to this belief. The assumption of a common prior and Bayesian update rule (which is what gives this solution concept its name, after all) mean that players cannot be wrong, merely underinformed. In other words, a player with posterior belief $p_i$ is correct about the distribution of types of other players conditioned on her own, even though she does not know which realizations they have. ** Edit. Osborne and Rubenstein's text mentions that it is possible to define a more general game in which each player has a different prior distribution (there is no common prior). So your definition does match their most general definition. I suppose that in such a case two players may hold incompatible views, hence you could say that someone must be incorrect. That all being said, the vast majority of Bayesian games assume a common prior. 

The sampling approach rigorously would look like this. For each player $i$, we want to estimate the expected marginal contribution, where the expectation is taken over a the subset of players that precede $i$ in the permutation ordering. So for each $i$, we do the following. Let $X_i$ be a random variable equal to the marginal contribution of $i$, when we draw the permutation of players randomly. Let $\mu_i$ be the true Shapley value of $i$ (which we do not know). Then $\mathbb{E} X_i = \mu_i$. Now, if we sample many independent copies of $X_i$ and average them, this average, call it $\bar{X}_i$, should be very close to $\mu_i$, and the closeness is given by e.g. Hoeffding's inequality, which says 

Red ball solo For any red ball in some urn, the probability that another red ball misses that urn is $1-\frac{1}{U}$. The probability that all other red balls miss that urn is the product of the chance they all miss that urn, or $\left(1-\frac{1}{U}\right)^{R-1}$. Red ball super ball The balls are symmetric, so $\Pr[\text{a red ball is a superball}] = \frac{\mathbb{E}[\text{# superballs}]}{R}$. The expected number of superballs is the expected number of urns with a nonzero number of red balls. The probability that an urn has $\geq 1$ red ball is $1 - \left(1-\frac{1}{U}\right)^R$, because the chance that every red ball misses the urn is $\left(1-\frac{1}{U}\right)^R$. The expected number of urns with a nonzero number of red balls is therefore $U$ times this probability. So $$ \Pr[\text{a red ball is a superball}] = \frac{U}{R}\left(1 - \left(1-\frac{1}{U}\right)^R\right) . $$ 

This definition nicely summarizes much of the framework rational choice theory is embedded in. It contains instrumental rationality, in so far as that humans have goals (denoted as preferences) but only limited resources (constraints) and they choose so as to maximize their utility. It also hints at methodological individualism because it places human behaviour at the center of its investigation. Note, however, that Robbins again only summarized a research paradigm that was already well developed. (He says so himself on the same page.) Still, I guess it is fair to say that rational choice theory as we know it today took off after WW2. Here different names come to mind, depending on which focus you want to have. Arrow/Debreu for general equilibrium theory, von Neumann/Morgenstern if you want to stress expected utility (and game theory). A bit later, Becker is a good choice if you want to stress that rational choice theory is an approach that is driven by its methodology (rational behavior, methodological individualism) and need not investigate an economic topic at all. But yeah, asking for a single father of rational choice theory would really stress things a bit too far. 

Historically, the distinction between GDP and GNW goes back (at least) to Adam Smith. Before Smith, it was quite common to measure a country's economic power by its wealth. For example, this was very characteristic of the Mercantilistic school in the 17th century. Following the motto "you are what you have", mercantilists suggested that countries should try to accumulate as much wealth (for them: gold) as possible. This was to be achieved by a combination of an export oriented economy and political measures to limit imports such as tariffs. Selling goods abroad and keeping one's currency (again, gold) they hoped would result in having more money and thus being more wealthy. This idea is reasonable for the middleages where the difference between having and not having money can decide about which country is able to hire more mercenaries and has the potential to decide wars. For more modern societies a country's wealth (read assets) becomes less important. Adam Smith was the first to realize that the "Wealth of Nations" is not so much about how much money they have but rather about how much economic welfare they can generate. This is measured in the number of goods and services produced in a given year, nowadays known as GDP. From today's perspective, why do we use GDP rather than GNW: 

The answer is there is no single father of rational choice theory. The reason is that rational choice theory is not so much a theory but rather a coherent framework (others would call it a paradigm) resting on the key foundations of methodological individualism and instrumental rationality / self interest. Both of these aspects played important roles in the marginal revolution in the late 19th century. Important names here are, of course, Walras, Jevons, Menger and Marshall although none of them can be regarded single father of Economics in the modern sense. Some decades later, Robbins (1932, p. 15) famously defined Economics as 

Neither in the normal nor in the extensive game form. Different types of a player can be only be used if they are mutually exclusive. So suppose the citizens (one player!) can either be poor or rich and this may affect their pay-offs and thus actions. There is no possibility for the citizens to be poor and rich at the same time (while represented by the same player). Your only option is to follow Sub-Optimal's solution and model different types of citizens as different players. As Bayesian pointed out, you can then model simultaneous decisions by citizen 1, 2 and so forth by setting their information sets accordingly. 

This is not true. A subset of a game can be any subset of nodes that (1) starts with a single initial node, (2) includes all subsequent nodes and (3) makes sure that the information set of any node in the subgame are completely included in the subgame. Since the actual game does meet all three conditions it is a subgame of itself. This being clarified, the definition of the Subgame Perfect Equilibrium (SPE) does make sense. An SPE must be a Nash Equilibrium (NE) in any subgame, including the whole game. Note, that it then logically follows that every SPE is a NE (but not vice versa). (Also, for further reference: The set of subgames excluding the complete is called the set of proper subgames. Note, that the definition of the SPE only refers to subgames without specifying that they have to be proper ones.)