The basic technique is quite straightforward. When you read the record you take a note of the version or timestamp column, e.g. 

In Relational algebra, projection means collecting a subset of columns for use in operations, i.e. a projection is the list of columns selected. In a query optimiser step, the projection will manifest itself as a buffer or spool area of some description containing a subset of the columns from the underlying table or operator, or a logical view based on those columns that is used by subsequent operations. In a view, a projection equates to the list of columns selected in the query underlying the view. 

Comparing dmp files like that is probably quite difficult to do, as the format is quite complex, with snippets of embedded SQL and all sorts. Having said that, although it is a binary format, it is a stream format (i.e. no internal pointers), so it's quite amenable to processing with sed. I think loading the contents of he dmp files into two schemas and comparing the two is by far the easiest way you're going to do this, barring the existence of some tool that I'm not aware of. 

I can't see a lot of use for this in an operational system. In an analytic system you might want a separate date dimension with a grain of 'days' so the date links to reporting rollups of date, and perhaps a 'time' column if you want to do analysis by time-of-day for some reason. You can also present date and time as calculated columns based on a core datetime column. 

When you go to write out the record you filter the write by the timestamp/version so that the write writes nothing if the timestamp/version has changed. This makes the write atomic, e.g. 

cmd.exe actually has significant pipelining capabilities, although not as rich as a unix shell. You can get Win32 ports of tee and grep from the GNU Win32 collection, or from Microsoft Services for Unix (SFU). The GNU Win32 ones are better because they use msvcrt.dll and understand native Windows paths. SFU places an emulation layer over the base O/S in much the same way as cygwin, so it emulates style paths that behave in a more unix-like manner. An alternative to this would be to use WSH, powershell or even a small .Net console app to implement a small utility to read the output from the log file and present your prompt to the user. Reimplement the process in a way that allows the control to be automated. If you can agree business rules for this control it can be run automatically and simply log the result, generating a status report when the process has finished running. It seems quite strange that you are developing a server-side process that requires interactive input from an operator. If this process has to be run interactively, is there any reason why the interactive parts could not simply be run from somebody's PC? This would probably allow you considerably more flexibility in installing third party components or bespoke client-side code. 

It seems you want to aggregate location based statistics over time for rainfall. A database structure like the one below would let you do that. The 'data source' could be just a filename, or some indication as to where it came from. 

Replicate your database onto another server and move the reporting sprocs onto it. Reports are run off the replicated server. This is the least effort and can re-use your existing reports and stored procedures. Build a Data Warehouse that consolidates the data from your production systems and transforms it into a form that is much friendlier for reporting. If you have a lot of ad-hoc statistical reporting that could be done acceptably from a snapshot as of 'close of business yesterday' a data warehouse might be the better approach. 

It really depends on whether the developer has any support responsibilities. If they are on the hook for third line support then they will probably need to look at the production database to do this. Generally it's a bad idea to do anything on a production server unless it's really necessary to do it there. For most development purposes, mirrors or snapshots of the production database will be adequate, and probably better than the live production database. If you are doing anything involving integration then you will want stable database environments where you can control what's in them. Anything involving reconciliation will also need the ability to look at a controlled point in time. If the problem is that you don't have production mirror environments or any means to put a copy of production data somewhere for your developers then this is a somewhat different question. In that case your developers really need at least one mirror environment. If you can't see what the problem is in the data then it's kind of hard to troubleshoot it. 

Year to date sales is a different measure. Depending on your platform (OLAP systems, for example) it may be purely calculated on the fly. If you have an appropriate unit of analysis to do so, store it, otherwise calculate it on the fly. 

How up-do-date do your read queries need to be? You could partition the database by time if the map just needs to show the most recent measurement. This would reduce your query load for the map. For the history of a given point, you could hold a second store by x and y showing the history. This could be done with a nightly refresh/update as the the historical data won't change. Then you could pre-compute averages at more coarse resolutions for integrating with maps at different zoom levels. This would reduce the number of points to retrieve for large map areas (zoom out). Finer resolutions would be used for more zoomed in maps which were querying smaller areas. If you really need to speed this up you could compute tiles as blobs and interpret them in your application. Because these would involve some re-computing of aggregate information there would be some latency in query results. Depending on how much latency was acceptable you could use this sort of approach to optimise your reads. OK, so your points need to be computed averages over time. With this computation I guess your actual queries come down quite a lot from 22 trillion items as the raster values can be pre-calculated for querying. 

I did something like this once with a document parser tool I wrote. It would strip out specially formatted comments from a DDL statement and produce a data dictionary. This was renedered with LaTeX. However, I still had to reverse engineer the database into Visio to make E-R diagrams. For a limited subset of SQL needed to parse create table statements it wasn't too hard to write the parser. I had it working in a couple of weekends. 

Oracle tends to be finicky about requiring specific versions of system libraries. Linux is generally fairly amenable to installing these without breaking the existing libraries, so this is normally fairly straightforward to overcome. Many of the HOWTO documents explain how to obtain and install these libraries. I've certainly had Oracle running on unsupported platforms (Fedora, Debian for example), and once the correct libraries are present you can just use the standard installer. This link is to an OTN article that describes the libraries required to install Oracle 11gR2 on Ubuntu. 

Microsoft make a SQL Server ODBC driver for Linux, although it's binary only and only runs on certain 64 bit RHEL builds.1 Anything that can consumbe ODBC (through the unixODBC driver suite) can use it. For scripting, you could use python, perl, or anything that plays with unixODBC to automate tasks. The package also comes with versions of bcp and sqlcmd. Sqlcmd will support interactive querying or allow queries to be embedded in shell scripts. 

If you have a frequently used query that returns items in descending order then you might want to create the index with a column ordered DESC. An example of where this might be useful is with date stamped transactions or history where you frequently issue queries where you want to see the most recent ones first. However, in most cases you probably want to use the default, which is ASC. 

You could try GRASS or QGIS., open-source GIS applications. You might be able to get what you want from one of them. EDIT: I see you're after a data modelling tool with support for spatial data types. I can't see any specific reference with basic google-fu. You may be stuck with building your database model using statements and then reverse engineering them into a modelling tool for documentation. If you have Visio Professional, the database modelling feature of that has a passably good database reverse engineering feature that will work OK with anything that supports an ODBC driver. Another option might be to get an extensible tool such as Sparx Enterprise Architect and frig the metamodel. However, this might be more work than building your database by hand. 

The process is slightly roundabout in MDX because it does not have a 'WHERE EXISTS` operator. You can do it in terms of set operations though, as demonstrated in the snippet of MDX. Some OLAP front-end tools, such as the late and much lamented Proclarity would allow the user to define sets in the tool and then do the operation manually. If you don't have access to that I can't easily think of a way that materially improves on MDX queries like this one. One option would be to build a report that takes the combination of products you want as parameters and returns the list. You could also implement that in SQL. You could also possibly do something with a dynamic named set in SSAS2008+ where you calculated the set of products not present in the customers' preferences. This would be computed as the total set of all products minus the total set of products filtered by the preferences in the set of customers. You can calculate the sets of customers by selecting those with a particular preference or set of preferences. Showing the non-empty members of the non-preferred list, perhaps filtered by some specific subset of interest would allow you to show them along a different axis. This would adjust to producing a list of products where no customers in the selected set were considered to be leads. Unfortunately I don't have a running SSAS2008 instance to hand to demo this, so it's left as an exercise to the reader. 

The correlated subquery does not have an alias as it does not participate in a join as such1. The references and for are both available to the subquery as correlated subqueries share their namespace for aliases with the parent. 

There is pl/proxy, which provides a basic federation capability, but it's not an engine optimised for this sort of work. Greenplum is a commercial product that does have a shared nothing engine that might do what you want. However, it's not cheap - less expensive than Oracle but more expensive than SQL Server. There are no pure open-source engines of that type for PostgreSQL. However, you might get what you want from PL/Proxy. 

The SSIS Send mail task connects directly to a SMTP server - it doesn't use database mail. If you want to use database mail, you would invoke it through an 'Execute T-SQL Statement' task that calls a stored procedure that wraps the database mail facility, or something similar. 

3 letter ISO currency codes are relatively small, so there is little overhead in using them as the key for the reference table. Having certain items such as dates, GL codes and other analysis codes and currency codes directly on the tables is quite convenient for people reporting off the transactions and balances. Many accounting packages - even big players like Oracle Financials - do it for this reason. 

Although Access has data bound controls, by and large you don't really want to use them for any sort of non-trivial application - partiuclarly if you want to do significant data validation before you save records. You can use ADO through VBA on Access, much as the same as through VB. The code to populate a form from a record set is pretty straightforward, and Access comes with a usable set of controls that can be used on a form. If you're using ADO, the life cycle is that you read the record by issuing a query to the database. Then you can populate the form from the recordset - this is straightforward to do with VBA. The 'save' button onclick method calls a validate and save procedure. You can add whatever other UI code you need to populate dropdowns, enable/disable controls etc. You may also want to separate data access into a separate class, depending on the complexity of the application. This lets you have a M:M relationship between formns and data items with the data access modularised into one place (i.e. one form can gather data from two or more queries by using the appropriate modules). Beyond this we're getting into the realms of application architecture, which is probably a bigger scope than you had in mind for this question. There are plenty of books on the subject, including ones specifically about application development with MS-Access. 

If the relationship is strictly hierarchical (i.e. a portfolio can have several sub-portfolios or projects, but a project cannot appear in more than one sub-portfolio) then you can model portfolio and project using a subtype pattern, e.g. 

On the whole, I would rate VS2010 about a B-. It was clumsy on a relatively simple database project with two fact tables and around 15 dimensions. The largest data model I ever did was for a court case management system, which had around 560 tables. I would not recommend VS2010 for a project that size (I did that on Oracle Designer). Essentially it's trying to be clever and the paradigm doesn't really work all that well. You're better off with a best-of-breed modelling tool like PowerDesigner or just using create table scripts by hand. SSMS is simple and reliable but manual. You have pretty much infinite control over how you want to manage the model. Combine it with a schema manager such as Redgate SQL Compare and maybe a decent modelling tool such as PowerDesigner and you have a much better package than VS2010. Summary I'm not sure I could quote any killer features or benefits apart from (perhaps) integration with VS solutions containing other projects. If you already have VS2010 premium or ultimate then you get a somewhat flawed database development tool thrown in with your .Net tool chain. It will integrate DB projects into your VS solution, so you can use it to make sproc deployment scripts at least. However, VS has no modelling tool to speak of, so PowerDesigner or even Erwin is better on that count. Redgate's schema management is much better and SQL Compare Pro is quite cheap (about £400 IIRC). IMHO SSMS works much better for T-SQL development but you can certainly do it with VS2010. VS2010 premium isn't much cheaper than a best-of-breed database modelling tool and VS2010 ultimate is at least as expensive. At the expense of tight integration with your VS project you could probably do better with third party tools. An alternative I guess one shouldn't slag off VS2010 too much without suggesting at least one alternative and outlining its pros and cons. For the purposes of this I'll assume a large project. Although I mainly do A/P work these days I have been involved in a 100+ staff year project where I did the data model (and some development work) and a couple of others in the 10 staff-year range, where I mainly worked as an analyst or a developer. Mostly I work on data warehouse systems these days but the larger projects were mainly applications. Based on my experiences with various tools, here are some suggestions for an alternative tool chain: