then it is cannot prove its own consistency. If your formal system can prove its own consistency, it must either 

I am surprised that you are asking about algebras for this functor, rather than for the monad of which it is naturally a part. Classically. the algebras are complete atomic Boolean algebras (Lindenbaum and Tarski, 1930s). In a topos, they are full powersets and indeed the category of algebras is equivalent to the opposite of the topos. This was proved by Bob Par√© in 1973. It was an inportant part of the development of elementary toposes at the time because it meant that colimits could be deleted as a requirement (in fact Christian Mikkelsen had done this a bit earlier.) Another example of such a dual equivalence arising from double exponentiation of an object $\Sigma$ is the category of locally compact locales (or spaces, with Choice.) There is also an abstract construction that takes a category that has products and powers of a particular object and yields a category with this monadic property. My programme called Abstract Stone Duality bulds on these ideas to develop a computable theory of general topology. 

Since Tom Leinster queries my reference to actual/completed versus potential/incomplete infinities, maybe we should ask a philosopher whether I am using these terms in the standard way. In any case, I am not doing metaphysics. I am just describing the way in which it appears to me that mathematicians actually work, in contrast to the way they say they work because they have been trained to say such things. When you compare my remarks with the others on this page, please note that they are based on thinking about these things for myself over 25 years, originally from a categorical perspective but increasingly influenced by symbolic logic, and not on reciting bits of textbooks. To do ordinary arithmetic, you may need very (arbitrarily) large numbers, but you don't all of them together. So far as I can gather from history, mathematicians up to the mid-19th century managed very well to deal with things in this way, for example defining functions as expressions. Post-Cantor, 20th century mathematicians got into the habit of introducing the completed infinity before the structure. For example, we say "a group is a set with...", relegating the essence of symmetry to second place. This is like saying that humanity is a collection of pieces of flesh, onto which faces are painted as an afterthought. Categorists, being part of the pure mathematical culture, did the same thing, in the vast majority of cases with great profit. However, when it comes to foundations, treating the universe first as a completed infinity (and only afterwards containing products, function-spaces, powersets or whatever other structure you require) inevitably leads into the set-theoretic trap. By contrast, type theoretic methods build up the universe by means of the actual operations that you actually want to consider, just as the symmetry group of the Rubik cube is built up from individual rotations. Moreover, despite the fact that type theory looks completely different from category theory or algebra, it is an accurate underpinning of the actual methods of reasoning of mathematics. See, for example, my discussion of the idiom "there exists" in my book. This is not dogmatic Finitism or Logicism and is readily adaptable to considering the object $\bf N$ along with individual natural numbers, an internal category $\bf Set$ along with individual types, and so on. Now let me consider the other approaches to this question. First order logic. This was the first usable general technique in mathematical logic. Like other disciplines, it starts with the completed infinity and adds properties to it. Does it presuppose a set theory? Well, yes, in the same sense that a boot-loader presupposes a primitive operating system. I would be more convinced that first-order logic is independent of set theory if there were a branch of model theory that had examples of structures whose carriers were topological spaces or algebraic varieties. In fact, first order logic can be set up in the type-theoretic way that I have described above. But if you're going to do that, you may as well set up the type theory that you actually want to use. If we're looking for a metalanguage specificaly for categorical logic (say, in which to construct toposes) then first order logic is not the right structure. It is easy to describe an internal category in a category with all finite finits, and, by adding more diagrams, we can talk about internal toposes too. However, it's much more interesting to consider free internal structures, for which we need an arithmetic universe, although unfortunately there is next to zero literature on this topic. Fibrations, 2-categories, etc. None of what I have said contradicts the use of these categorical techniques. I personally consider that fibrations, and especially hyperdoctrines, are obfuscation, but other people find them useful. However, they organise the world, but they do not bring it into existence, which was the thrust of the original question. 

It is completely inappropriate to use a discrete two-element set for the "truth values" of such a proposition. We can swap $0$ and $1$. We cannot swap the statements "the $n$th Turing machine halts" and "the $n$th Turing machine runs forever". They are fundamentally different: we can just wait for the first to happen and then if it does we know that it is true. We can never know that the second is true. Granted, you can add an oracle to your Turing machine that can say whether the $n$th ordinary Turing machine halts, but it does not answer the question for the enhanced machine. A similar argument applies to "Turing machines" for (finding proofs in) stronger logical theories, which I guess is what the second question and Noah's answer are about. However, the point that I want to make here is that a gratuitous confusion is introduced in the meanings of words like decidable and recursive by imposing two-valued logic on them. At least say semidecidable and semirecursive (or, better, semicomputable). In the case of computable functions, the Kleene normal form theorem says that any semicomputable predicate $\phi(n)$ is of the form $$ \phi(n) \iff \exists h.T(p,n,h) $$ for some code $p$, where $T$ is a (standard) decidable ternary predicate. This means that, in the appropriate truth-value object for such predicates, which I call $\Sigma$, any truth value is a computable disjunction of decidable ones ($\bot$ and $\top$). Reconsidering the second question, trying to prove that a statement follows from a given theory is like running a Turing machine to search all possible proofs. (This is a pretty daft way of doing mathematics, but then computability theory was traditionally written in a similarly infeasible manner. With small modifications it can be made to look much more like modern practical computation. Using binary trees instead of integers makes a huge difference.) If we are going to discuss which symbols to use for true and false then I suggest the following. In the second question there is an object language (statements that might be deduced from theories) and a metalanguage (searching for proofs). We might think of the object language as a kind of algebra, so it makes sense to write $0$ and $1$ for its truth values, especially if the theory under discussion is a classical one. In the metalanguage success and failure are not interchangeable ideas, so my $\bot$ and $\top$ are preferable and they certainly belong to the space $\Sigma$ and not a two-element discrete set. In other words, the confusion to which the original question referred is slightly sloppy language based on fundamentally the same idea. Imposing two-valued logic on termination of computations is also a misuse of language, but in my opinion one that seriously obstructs understanding. 

I still can't see what the question is. You seem to be asking when a prefactorisation system (an orthogonal pair $(E,M)$ in which both sides are closed in the Galois connection) actually admits factorisation. You need some finite limits or colimits for this, not just filtered ones. Example 5.7.10 in my book (Practical Foundations of Mathematics, CUP 1999) shows a poset with six points that is not a lattice together with a prefactorisation system that is not a factorisation system. It is followed by a version of the special adjoint functor theorem that shows how to get a factorisation system. 

Thanks first to Andrej for drawing attention to my paper on the IVT, and indeed for his contributions to the work itself. This paper is the introduction to Abstract Stone Duality (my theory of computable general topology) for the general mathematician, but Sections 1 and 2 discuss the IVT in traditional language first. The following are hints at the ideas that you will find there and at the end of Section 14. I think it's worth starting with a warning about the computable situation in ${\bf R}^2$, where it is customary to talk about fixed points instead of zeroes. Gunter Baigger 

What is this construction called, how can we rigorously define it, and where can I read about it? For starters, it's a category and not usually a groupoid because the morphisms need not be invertible and even for a theory with a single sort $X$ you need objects standing for $X^2$, $X^3$, ... in order to take account of binary, ternary, ... operations. This category is known as a Lawvere theory and was introduced in his PhD thesis c1963. Such a category has finite products, although it is sometimes presented as its opposite, having coproducts instead. The hom-sets $Law_T(X^n,X)$ of this category, ie the sets of all expressions in $n$ variables, had previously been known in Universal Algebra under the name of clone. I think this idea was due to Peter Hall. The universal property of the Lawvere theory $Law_T$ is that models (algebras) for the theory $T$ in any category $C$ with finite products correspond naturally and bijectively to product-preserving functors $Law_T\to C$. We can generalise from finite products to finite limits and the corresponding notion is called an essentially algebraic theory. A leading example is that of categories. However, such theories are often better described as dependently typed. For example, a category qua essentially algebraic theory has a single type of all morphisms, with conditional composition, whereas it is more natural to take the dependent type $C(x,y)$ where $x$ and $y$ range over the type of objects. Dependently typed theories with singletons and equality types are equivalent to essentially algebraic theories. Without these, these is a class of distinguished display maps that needs to be closed under pullbacks. These were introduced in my PhD theses and described more (accurately and) fully in Chapter VIII of my book Practical Foundations of Mathematics (CUP, 1999). A similar construction also lends itself to type-theoretic constructors such as quantifiers, as is described in Chapter IX of the book. I describe the category as that of contexts and substitutions.