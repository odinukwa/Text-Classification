You can simplify your code a lot, by letting the recursion stack keep track of the number of items, rather than doing it yourself explicitly: 

You can optimize this quite a bit, e.g. by breaking early out of the loop if both and are false. And you can make your code more explicit by using explicit statements, rather than operators, which may feel a little too terse. 

It should be obvious that the following, code, that doesn't need to explicitly check for even entries, also gets the job done: 

And even the largest problem in your challenge can be solved in a reasonable amount of time, not sure if it will be fast enough though: 

I think your algorithmic part is exceedingly verbose. One of the nicenesses of Python is how it can lead to code that reads almost like English. And for this particular problem, even leaving performance considerations aside, I think iteration is clearer than recursion. So I would rewrite things as: 

There are two almost identical blocks in that function that should probably be refactored into a common external function, but I felt it would better show what was going on to keep them separated. Anyway, when you run this you get the expected output: 

That last code is a little too clever for its own good, so it should it should be heavily commented. By my timings, for your target size, is 5x faster than your implementation, and about 2-3x faster. 

There is not much need for a class for what you are after. Storing all the conversion factors in a dictionary, you could simply do something like: 

In your case, you want to take into account extra contributions from multiples of the factor squared, cubed, etc... This is relatively straightforward to do also: 

Know the language conventions! Python's PEP8 dictates format for function and variable names. Avoid unnecessary explicit checks. Your function should work on any iterable, there is no point in checking for and not for, e.g. an or a , which would also error out when you try to compute their . If you want to be pedantic about it, do something like: 

It still manages to handle values of , or in the tenths in the blink of an eye, but there has to be a better way for this. 

and that subsequent rows can be filled out by initializing them to all zeros, and then use the adjacency graph to add the values at a given column of the previous row, to all the columns that are successors of that one in the graph. Once the array is fully filled out, the sum of the last row is the total number of numbers that can be created. Because we only care about the sum of this last row, and each row only depends on the previous one, we only need to keep two rows of the array in memory at a time. Putting it all together into code: 

The following outlines a couple variations of a numpy solution. For the problem sizes in your post, they all clock faster than 100us, so you should get your 30,744 calls in about 3 seconds. 

And if you want to squeeze those last drops of performance, the speed of the following code is marginally, if at all, slower than , and is also much more Pythonic and readable: 

I think the accepted solution is not taking full advantage of the known bounds to the problem, so I'm going to give it a shot as well. One of the nice things of factorization is that, despite the initial bounds not being too optimistic, you can typically improve them as you go. If you do trial division of a number \$n\$ by the sequence of increasingprime numbers, you will know that the number is itself as prime if it is not divisible by all primes smaller than or equal to its square root. Similarly, if you remove all smaller prime factors from a number, you know that what's left is a prime itself, once the next trial prime is larger than its square root. So despite the fact that the largest possible prime factor of a number is the number itself, you only need to do trial division by primes smaller than or equal to the square root of the remaining factor. If you couple this with a sieve of Erathostenes like algorithm, you will find yourself only needing to sieve for prime factor smaller than the quartic root of the number. The whole idea can be implemented very compactly: 

Counting and enumerating are two different problems. Sometimes there is no better way to count than actually enumerating all the possible values, but that is often not the case, and this is one of those cases. To build an efficient algorithm, we are going to use dynamic programming. We will build a 7 rows by 10 columns array. Using zero-based indexing, the item in row and column is the number of phone numbers of digits that have as the last digit. It should be obvious that the first row of that array will be 

If you were to time each of your iterations advancing the board one move, you would notice that they get progressively slower, as more and more cells are non-zero and contribute to their neighbors. One way of mitigating this effect is to, rather than completing all moves from the source towards the destination, do half of them like this, and the rest from destination to source. If there are \$m\$ ways of getting from the source to a certain cell in half the moves, and \$n\$ ways of getting from the destination to the same cell in the remaining moves, then there are \$mn\$ ways of getting from source to destination. You could implement this like: 

Also, note that I am computing as , not . This is not relevant in Python, because integers never overflow. But in other languages, the form I have used will never overflow if and areboth positive values. This is a more or less famous bug, that was shipped with standard libraries for years, until someone figured it out. 

I wasn't aware that scipy's sparse matrices did not support broadcasting. What a shame that you can't simply do ! Your construction of can be sped up quite a bit, but you need to access the internals of the CSR matrix directly. If you aren't aware of the details, the wikipedia article has a good description of the format. In any case, you can construct as follows: 

You are searching over the full set of permutations, which leads to a time complexity of \$O(n!)\$, which is rarely a good thing. This seems like the type of problem that would benefit from a dynamic programming approach. I don't find it entirely satisfactory, as it uses a top-down approach, but it is much more efficient than your implementation, so here it goes: The basic idea is that, if you take a subset \$A\$ of \$m\$ items, and compute the \$m\$ values \$t_{A,j}\$, being the "best taste using the items in \$A\$ and ending in the \$j\$-th item, you can compute \$t_{A^*, m+1}\$, where \$A^*\$ is the set \$A\$ augmented with an extra item not in that set, easily. This is kind of confusing, I know, but below I am indexing memoized solutions by , and building the total taste by adding one of the remaining items at a time. It may be easier to look at the code: 

There are cleverer ways of computing a quotient of factorials, but that is left as exercise. You also have to handle all the possible permutations with less than the full possible digits. There may be a better way of not having to brute-force your way through this, but I'm out of cleverness right now, so I am going to go with something very unsophisticated: 

rather than iterating over , which creates a list of the lines in the file, we iterate over , which produces the same result, but as a generator, rather than first creating a potentially huge list of lists of strings, then converting it to a list of lists of ints, only a single row is instantiated at a time. 

Descriptive variable names are good, but code that fits in a monitor's width is even better, and I think you are erring on the too long side. E.g. is probably descriptive enough, and for a method of such a class, is probably also good enough. variables can be compared directly, no need to go through the cumbersome process of finding their index in a list. I think your code is not doing what you want it to, as you are forgetting all history and characterizing the string by whatever order the last two characters follow. 

When you go on to calculate , you are actually negating and then dividing it by a number that turns out to be , so you can calculate as 

This has a dreadful performance, \$O(n)\$ space and \$O(n^2)\$ time, because we are continuously rescanning the full array for the maximum. If we are only after the value of the maximum, and not the indices of the neighbors that produce it, we can minimize the overhead by keeping three numbers only, the previous maximum plus the last two entries: 

This function returns the first position in which you could insert , and keep the sorted, which will also be the first occurrence of in if there are repeated values. The neat twist is that, by replacing a single with a you get the following: 

Explicit looping in Python is (very) slow, and you have a double loop, which isn't helping. Your inner loop is effectively setting to a slice of of the form , and you can take advantage of that to speed things up considerably. The only tricky part is figuring out how many items you have to set, to properly build the right hand side of the assignment. It is not hard to figure out that if you have a list with items, and you set every -th item starting with , you will in total set elements, which can be computed directly as . Putting it all together: 

Of course the proper, fast way of checking if a number is a power of two is with a little bit twiddling hack that give \$O(1)\$ performance: 

and then replace with , you will spare yourself a huge amount both of computations and memory accesses, which should boost performance by a lot. 

This is pretty much the same you are doing, modulo a nicer syntax. Now to the meat of the problem... A naive approach like the one you have implemented works: 

I would also refactor your code to have the output creation happen in a single, common place, after some manipulation of the arguments: 

Since you have decided to go the way of the specialized object for your board, there are two obvious improvements. One is to use Python magic methods to have a more pythonic notation. The other is to offload the bounds checking to this specialized class. For instance: 

What exactly do you want to do with your array of booleans? If you used numpy, you could instantiate your boolean array as an array of one eighth the size of the boolean array, e.g. 

There are a number of obvious optimizations that would probably make the code more obscure, like keeping only odd primes in the sieve, or dynamically updating the limit up to which sieving is done. But they would not affect the overall complexity of the algorithm. 

You can make your current \$O(n^2)\$ algorithm run in \$O(n \log n)\$ time by first sorting the input intervals by interval start time, then do a linear scan of the sorted array: 

Actually, although not immediately obvious, your solution is \$O(n)\$... The sorting algorithm used by Python is Timsort, thus named after its inventor, Tim Peters, author also of The Zen of Python. It has many subtleties and refinements, but at a very high level, Timsort works by scanning the array to find sorted runs, i.e. contiguous subarrays already in sorted order, then merging them into larger sorted subarrays. When run on an array made out of two concatenated sorted subarrays, it will identify both of them as runs, then merge them into a single sorted array, all in linear time. That's one of the beauties of Timsort: that it can beat the famous \$O(n \log n)\$ bound if the array is highly structured. Your particular case happens to be one of the ones that run in linear time! 

Also, you can call the split method directly, without the statement, as it will return a single item list with the full string inside if the character is not in the string: 

Boolean values are converted to 0 or 1 when used in an integer setting, so your adding of values is typically written as: