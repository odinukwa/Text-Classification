@Complex on twitter found it for me. The issue is that there's a maximum field length setting that was getting in the way. As found at $URL$ you go to Console, then Configuration/Settings, and alter Maximum Field Length to a more reasonable number, and it starts working shortly. 

I don't guarantee that this code works right (most especially the line that starts with "RUNNING=", but substitute some test of your own that can return a string if it's running and no string when it isn't. 

The strong-armed solution for something like this is 802.1X, where a user has to authenticate to gain access to a network port. Implementing this is non-trivial. How large is your userbase? If it's reasonably small (or if you have a good up-to-date inventory of user machines), you can configure your DHCP server to only hand out addresses to known machines. What are you using for your DHCP server right now? 

For the drives that spin up and can be addressed, use the following command to "zero" out the drive: dd if=/dev/zero of=/dev/harddrive bs=1M If you're really paranoid, do it 8 times, but to my knowledge, no one has ever recovered from one without pulling the platters and examining them microscopically. There's actually been an ongoing challenge since 2008: $URL$ If the drives DON'T spin up, 5 or 6 good wacks with a hammer will take care of the platters. 

If not, use the error to help troubleshoot (or post it here) Step 5 - Does SMBClient let you list the shrares remotely 

From the prompt, I'd guess it's a csh builtin. And from reading "man csh", that appears to be the case 

$URL$ To copy and paste: SIDs can sometimes change. The SID for a Group object won't change. The values of other object properties can change, but the Object-GUID never changes. When an object is assigned a GUID, it keeps that value for life. 

I would say that the "big boys" do SSL offloading onto clustered front end load balancers, since that's what I do and I'm no where near a "big boy". I found these instructions on httpd.apache.org on removing the passphrase dialog, which you have likely seen if you googled the problem, which I assume you have. The problem with revocation is that you're at the mercy of the signing CA to hurry up and deal with your problem. If you pay a lot for your certs, I'm sure their service is very positive. I'm not certain how it would be for some of the smaller wholesalers. Maybe someone else could chime in. 

Have you looked at mod_gunzip? I'm too new to link to it, but a google search should point you in the right direction. 

Evan Anderson is going to give you the really good answer on this one, but until he shows up, I can give you some stuff to research. Basically, you've got an Active Directory "tree". It's your domain. Your new company has one, too. Their "tree" is their domain. What you want to do is put both "trees" in the same "forest". I'm not being cute, these are the actual terms. I only know enough technically to know that I don't know the answer. Google provides some good looking results, but definitely talk to someone who knows before you do it. Is there anyone in the other company with more Windows experience? Also, I'd recommend picking up an AD book for the version of Windows Server you use. They're enlightening, and as a Linux guy myself, seeing their mindset is a little unsettling at time, but it usually works. It's just different. Good luck! 

You cannot make scripts SUID. Fortunately. You may be interested in the SUID-wrapper program here, though: $URL$ I should also add, please please please make sure that you really need to do this before you do it. SUID binaries can be a great big gaping hole in your system. 

For rsync, you basically take a look at the mirror list, select one that lists rsync as an available option, and use the command: 

Although I haven't implemented it yet, I'm planning on moving all of my log-generating machines to rsyslog, and implementing a bastion-type server which will function as the collector of syslogs. From there, I think the free version of Splunk can do everything I need to pull out information. Now just to implement it... 

As Gleb said, using mysqldump is the easiest way (depending on the traffic on the database. Very heavy and mysqldump won't pick up some changes). With apache, just rsync the directories across before-hand, then do it again when you're ready to do the switch. That way you minimize the size of the sync when it's crunch time. You'll also want to do it before hand to make sure that your various configuration files were picked up and synced correctly (and that you got them all). This isn't too complex, but it is good practice for bigger live migrations. 

In terms of reliability and security, probably CIFS (aka Samba) but NFS "seems" much more lightweight, and with careful configuration, it's possible not to completely expose your valuable data to every other machine on the network ;-) No insult to the FUSE stuff, but it still seems...fresh, if you know what I mean. I don't know if I trust it yet, but that could just be me being an old fogey, but old fogeyism is sometimes warranted when it comes to valuable enterprise data. If you want to permanently mount one share on multiple machines, and you can play along with some of the weirdness (mostly UID/GID issues), then use NFS. I use it, and have for many years. 

Chances are really good that you'll at least get a response from your gateway, 10.x.x.1. Anything past that means your gateway is routing traffic to you. If you don't get responses, that may indicate a network firewall causing the problem. Of course, there's still the chance that you're getting traffic, but that your gateway is filtering ICMP packets. It would be diagnostic to try telnetting to google and pretending to be a web browser: 

Step 1 Securely configure the hosting web server to have a "staging" directory, which will hold the clients' uploaded files until you can retrieve them. 

You need to setup an SSH gatekeeper. This allows openssh to permit multifactor authentication. Here's a great link: $URL$ Essentially, you use the ForceCommand directive to run a script when the user logs in. That script then prompts the user for the password. I'm currently looking for a method to verify a given password against the system password, but I'm coming up (understandably) blank. If the user account is stored in an LDAP directory, you could attempt to bind to the directory using those credentials, but the problem is going to be that the program running will be running as the user, not as root. I don't know the security implications of writing the compiled code and setting it SUID. Hopefully someone will give you a better answer. but since I've typed this much, are you in an ultra-secure site? Because that's really the only reason for this. Normal public keys with passphrases should be more than adequate for 99% of cases out there. 

By default, CentOS is pretty restrictive in its package selection and slow in the updates to new packages because it literally is a repackage of RHEL, and RHEL is slow and steady for reliability sake. That being said, you have the ability to add other repositories which feature a wider selection and newer packages. Check this link for more possibilities: $URL$ I myself have used EPEL to a decent amount of success. 

So, some research has been done on this. According to SSD researchers Michael Wei, Laura M. Grupp, Frederick E. Spada, and Steven Swanson, who presented the paper, "Reliably Erasing Data from Flash-Based Drives" (PDF warning), quote: 

Lots of us have Storage Area Networks (SANs). Most of us, probably. How do you go about optimizing for speed? How many spindles do you have? iSCSI or FC? How do you break up your array? And if you have an EMC SAN, what do you do with the first four drives that share the stupid Flare OS? Looking for input from anyone who has put effort into speeding up their SAN to make disk access more timely and reliable. 

Whenever I'm screwing around with machines that are going to be in production as a name separate than they are now, I make liberal use of lying-through-hosts-file-manipulation. The only problem will be if you start to use certificates, the IP / name combo in the certificate may not match. I've also seen some licensed software that isn't fooled this way. On the machine, make sure that your entries in /etc/hosts accurately reflect reality whenever the things switch over. Then, when things change, make sure to take out the host file entries, because things will change in the future, and you don't want a host file screwing you up while at the same time giving you a false negative error indication. 

It said that there is no logical volume because you created the volume using the other slots. I'm willing to bet that particular PERC keeps drive information entirely on the controller, and in a per slot configuration. I'd recommend either getting a third disk and using it as a hot spare or removing it entirely and putting in an empty carrier. After recreating the array, of course. 

You really need to implement an email archiving solution. Red Gate software has one (disclaimer: I've received money from them for writing articles in a magazine they publish), but Global Relay is a solid service that you can outsource it to, if you don't have the high availability stuff it requires. 

There isn't a great way. The reason that effective package management was created was to solve this very problem. Upgrading and uninstalling source-compiled things is hard. I agree with Tom and David. If this is a one-off case, then re-compiling from source is probably your best bet. If it's on an array of machines, it's definitely time to move to the supported package management. 

I'd guess that it may be related to warranty prices being static regardless of how many returns or repairs you call in, whereas insurance premiums can go up if you have a lot of claims. There's also the possibility of a bulk loss through disaster, you can't recover via warranty, but you need to claim those on your insurance. All of our equipment at my last position was covered under business insurance, and the production equipment had warranties on it so it could be replaced. Does this answer the question you asked? 

In terms of flexibility, you can't beat the snmp plugin. It's behind nearly every check I run, and if that isn't, the TCP connect is. 

Authenticating is absolutely simple using Likewise Open. $URL$ Nearly my entire Linux infrastructure has centralized authentication and user management thanks to Likewise Open. It's stunningly simple to install and implement. I cannot possibly say enough good about it. As a note, UIDs and GIDs are assigned according to a hash function, so they are identical across the entire infrastructure, so NFS mounts work perfectly. 

Is there a UPS attached to the machine? Are you sure it's not telling the machine to shut down because it thinks it's dying? 

GFS2 was until 5.3 a "technology preview", even though the documentation for RHCS gave specific instructions for it. I can't speak to it in 5.3 because I gave up on it. I have heard good things about Lustre and OCFS. I have also gathered from listening to a lot of people that (if you have the disk space available) DRBD is an excellent way to go. Unfortunately, I'm yet to try it. 

It never has been in the past. This thread leads me to believe that it isn't in RHEL6 either. I suspect that you would either need to run your own Satellite (or Spacewalk) server to push out applications. If you wanted to pull them from the client, you'd need an internal YUM repo and a configuration management solution (like puppet, chef, or CFengine) to have them install the the packages and keep them up to date. Someone else may know: is it possible to run arbitrary commands on servers subscribed to certain channels using RHN? If so, you may be able to make a server pull from an internal YUM repo without a 3rd party configuration management solution. 

I vote no. Allow me to enumerate my reasons. 1: Reliability. Having each server machine rely on dhcp in order to have its networking stack come up correctly adds another potential fault. In a server environment, where you're trying as hard as possible to achieve maximum availability, adding another moving part is not a good idea 2: Security DHCP essentially hands anyone plugging into the switch a valid lease. Yes, you can specify that only known MACs get leases, and everyone else is denied, but a better place for this is dynamic VLANs. 3: Documentation Having a central DHCP pool which assigns addresses willy-nilly is insane for a server block. Assigning a server a specific IP via DHCP is less insane, in the sense that having 3 imaginary pink elephants chasing you is less insane than 5. 4: Management Not only to you have to specify in the DHCP server what each machine is assigned to, you have to keep documentation of it. And you have to update ALL of the documentation any time anything changes. New network card? Update documentation and DHCP server and DNS, etc. Simple is better.