Are there other approaches... CCG is not the only logic-based grammar. There are others that closer to mathematics, in that, their derivation trees for sentences that look exactly like natural deduction proofs. I think the ACG guys have a working parser. I am not sure about the others like TLG, CVG, etc. 

I have no idea about the specific person that you mentioned, or how to go about identifying his country of origin. My answer addresses the general aspect of it. Yes, there are linguists/computational linguists who are working to create software to identify aspects of the author, be it country of origin, or even personal identity. Speech is definitely an aspect that many are working on (example). There are many others who are focusing on the errors of non-native writers (example). The field, in general, is called stylometry or computational stylistics. If you want to try out some such software, stylo and JGAAP are good apps to start with. 

When you write down the semantic representation of a sentence (lambda calculus) you see that the determiner "consumes" the noun. Hence, it is the semantic head of a noun phrase. SEOP has a comprehensive overview of the phenomena that led to this, but Wikipedia is not bad either. In syntax, the noun is considered the head of the noun phrase because it is the more "contentful" part, in the way people understand sentences. In HPSG, the CAT derivation is somewhat like the regular phrase-structure derivation, with heads having a correspondence to phrase-structure (PS) heads. (Though, AFAIK, there is no need for the concept of "head" in PS.*) The CONT derivation is somewhat like the derivation in Montague semantics. These are not two different types of parsing in HPSG (or anywhere else, AFAIK), but two (of the many) aspects of parsing with a rich grammar like HPSG. Benefits: I don't think you can compare one with the other for benefits... if you go through the references I gave for quantifiers, you'll see that the mathematics would become very cumbersome and ugly (or maybe even impossible... I don't know) if you try to make the semantic heads correspond with PS heads. * Though some parsing techniques find it useful to have heads of a PS marked. 

Examining this list, I can only see 4 semantic units: "all right", "come on", "get out" and "have to". Would you agree with this? What strategy might I use to find all the semantic units in such lists? 

I think folk artists tend to embrase their local accents, indeed exagerating the nuances of these accents. On the other hand, pop singers tend to favour a more "international" and polished accent, which makes it more difficult to locate where the singer is from. Clear Dublin (male) and clear London (female) accent: 

Let's say you're teaching someone the sentence "I got in the car". This is a tricky sentence for beginners since they might ask: eh, what did you get in the car? Such sentences are easier to teach when there is a context associated with the sentence. That is, if you see someone getting into a car. So, I'm wondering if any of you know of a tool or database which can be used to generate pictures or animations based on simple sentences. Google Images does not help when you enter "I got in the car" into its search engine. However, if you enter "enter car" you do get some useful matches. But perhaps someone has developed software that you know about which can generate animations and or images by inferring the meaning associated with the sentence. 

I do a little linguistics as a hobby while working of my own software development projects. I am wondering if there is a database or online api where I can easily establish the possible "parts of speech" for a given word from within my code. So for example: 

Note that dependency graphs are (1) not a formalism and (2) not standardized, so each researcher may re-define what happens in dependencies. Some of those dependency graphs are not even trees. Refer Generating Typed Dependency Parses from Phrase Structure Parses. 

In some languages (e.g. Hindi) the schwa is treated inconsistently. One needs to study not just letters for phonemes but the various forms it can take. Contrast these four forms of /r/: रस्ता कर्ण कृपा प्रदेश (the last word appears incorrectly for me on Chrome, but correctly on Firefox), or these three forms of /u/: उत्तर कुत्ता रुमाल. These are particularly troublesome on computers, where programmers have to create a large look-up table for all possible combinations of phonemes. Indians readily borrow words from English, but most of their vowels can't be notated. This may have been ameliorated if there had been a concerted effort to add new symbols. 

This book by Pang and Lee is a classic reference on the topic. To get a good overview of the topic, look at the book's website which has links to presentation slides. I see that you want to implement the software on your own, and you want to (re-)invent algorithms too. The reasons behind these is not entirely clear. If you are writing a research paper or a thesis on the topic, you would need to do both. Otherwise, you can (1) look for free software for sentiment analysis; if you can't find any, or the licenses are unsuitable for your use, (2) learn techniques from published papers that demonstrate good results for the kind of classification you wish to do. There are a few workshops on this topic, and the website keeps changing from year to year, based on who is hosting it. In general, you could look for papers at "Workshop on Computational Approaches to Subjectivity and Sentiment Analysis" (WASSA-13, for example) or at "Practice and Theory of Opinion Mining and Sentiment Analysis" (PATHOS-2013, for example). 

When I learned Swedish I noticed I went through two phases of learning with regard to understanding the language. First I had to learn the meaning of common words. For example, "mening" means both "meaning" and "sentence". Secondly I had to associate meanings with groups of consequtive words - idioms, phrasal verbs etc. So the difficulty for me involved a word or a small group of consequtive words. Once I knew them, I could understand the sentence. Is this the same with all languages? For example, if I learned the meanings associated with every word, idiom, phrasal verb etc, would I understand every sentence? Or can there be other complexities at play which effect the meaning of the sentence? Word order differs accross languages but when learning Swedish I still heard a command when I heard "Nu skall vi gå!" instead of a question "Now shall we go?". This is due to how we tone the words in the sentence. But maybe this toning is different or missing in certain languages. Anyway, my guess is that the word ordering is so fundamental to any language that you will learn it at the very beginning and that it will be consistent thereafter. 

What is the technical term from a group of consecutive words with a single associated meaning? For example, phrasal verbs like: "get out" and idioms like: "on the other hand". 

More folk songs sung in English: Clear Irish Midlands accent: - $URL$ Clear Irish Midwest accent: - $URL$ Clear Irish Northwest accent: - $URL$ Clear Dublin accent: - $URL$ Clear Scottish accent: - $URL$ 

I wanted to write this as a comment, but it was too long. Anyway, I don't speak Telugu, I speak Kannada. While I can't tell you about the differences between what's indicated in the script and what's actually spoken in Telugu, I can tell you about that in Kannada. This might help you come up with analogous examples for Telugu. (1) Regional dialects may demonstrate their own peculiar deviations. While ಎಲೆ ("leaf") indicates /ele/, and that's mostly how it's pronounced in most dialects, in the northern dialects it is pronounced /eli/. (2) Contemporary informal registers tend to omit sounds indicated by the script. While news readers or literary scholars might enunciate all the sounds of "ಮಾಡುತ್ತಾರೆ" ("will do", 3rd person plural) and pronounce it /maaDuttaare/ most native speakers would read it as /maaDtare/ or /maaDtaare/. 

There are many actively researched aspects in the field of natural language parsing. Nearly all of this activity is on statistical parsers. Consequently, there is no "standard" computational model for dependency parsers -- there are many competing and collaborating methodologies. 

You could go the BNF way too, if you prefer. (The NLP community prefers to call it CFG (Context Free Grammar)). You can find some online demos of these too. NLTK comes with an implementation you could play with. 

mean very different things to me, and each has an unambiguous meaning -- not just in text (with punctuation) but in speech too (with intonation). I have pasted a couple of graphs below. To make these, I fed the sentences to the Stanford Parser and used my own software to generate the graphs. But you can verify it for yourself at the Stanford Parser's demo website. 

But even in more pop like forms you will hear people embracing local accents: Very clear English Northeastern accent: - $URL$ English Northern: - $URL$ Irish: - $URL$ 

What are the steps invoked in producing a dictionary? I am primarily interested in understanding the role software plays in the production process. Obviously a corpus for the language is first produced and this corpus needs to find a reasonable balance between technical terms, fiction, slang etc. Then I'm guessing the size of the dictionary needs to be decided on and consequently the number of words it should contain. Then I'm presuming the most common words from the corpus make it into the dictionary and the rest are ignored. But what happens then? Suppose I need to make the entry in the dictionary for the word "of" and I have 20,000 entries for this word in the corpus. How do I proceed? How do I keep track of the senses covered? 

In Swedish you use "Ja" to positively answer a positive question and "Jo" to positively answer a negative question. Nej is used otherwise. $URL$ 

So peann is lenited when demonstrating male ownership. In Scottish Gaelic: When addressing a person: 

I'm helping some native English speakers to learn Swedish. I have a large list of sentences which I wish to organise by linking each sentence to its associated set of meanings. For example: 

Does anyone know of a database or tool that can be used to produce or find images that help describe or reinforce the meaning of a given sentence? For example: "The car was going very fast." 

I hope from all that I have explained above, it's clear that Shannon's definition of entropy was the beginning of the field of information theory, and not the end of it. In short, Shannon entropy sets a lower bound for a random sequence of symbols, but we know natural language is decidedly not a jumble of words. With better models, we get lower entropy figures. "Semantics" does not have just one, static meaning. For the context of entropy, we could use distributional semantics to predict the next word or phrase in the sequence "pass me the ______". Ngram models would only predict the list of objects that they have previously seen in the context of these words in their training corpus. Models based on word meaning in context could do better and predict a candidate from all the words or phrases that denote tangible objects that can be held with one's hand, even if they had not been seen in the context of "pass me" before. Models based on Frame semantics could use the context of the utterance "pass me the ______" to predict the missing word. In the context of a garage, it could be tools like hammers, screwdrivers, etc; in the context of a dining room, it could be salt, pepper, spoon, etc. 

The Trie data structure is commonly used in NLP. This, and its descendants, Suffix Tree and Generalized Suffix Tree provide both, an efficient way to store commonly occurring sub-sequences, and a fast way to search for sub-sequences. 

Despite the fact that most parsers (and I ) tend to treat these in a manner identical to the Stanford Parser, I do not claim that most speakers of English would see this difference between these sentences. @FumbleFingers: I don't know about a UK/US difference in this case, but having done classes in linguistics with a few classmates from the US, I have come to the opinion that in matters of subtle interpretation, not all people from the same region tend to have the same opinions. How do you interpret the first sentence? How do you interpret it in speech? Do your RL friends from your region interpret it your way too? 

will increment the score associated with "set aside" instead. This is because I hardcode my parser with "set aside" and increment its counter each time I observe it in my corpus. I will eventually hardcode the parser with the most common semantic units in order to establish their frequencies. Far from perfect, but my understanding is that my parser would need to be much, much more complicated in order to catch non sequential semantic units like "turn the radio down". However, if you have suggestions as to how I might solve this problem, please advise. Although I'm giving English examples in my question, I am actually working with Gaelic. Here is a list of consecutive words which are very common in my corpus: 

I am producing exercises for my language students and I would like to complement the sentences involved with visual cues. Google Images is useful but I'm interested in exploring alternatives. 

How did the word "they" come to represent "he or she"? For example, "They forgot their coat" can be used to represent a single person of either sex. 

What theories do linguists have with regard to the best methods for learning new words for a seconds language? Previously I have used mnemonics which I think are ok but when I learn new words in English I do so without even thinking about it. Words like facebook, Obama, noob, bling, pimped, twitter, ISIS, wasted, Pussy Riot and linked-in. I didn't consciously learn these words. 

Furthermore, artists from non English speaking countries might well "adopt" an accent depending on where their music style is based: American sounding Swedes: - $URL$ English sounding Swedes: - $URL$