According to Draga Zec in The Cambridge Handbook of Phonology, the older model of onset, rime, nucleus, coda— 

As you can see, my system too is fully Unicode and has the relevant fonts. I think this is a bug; Praat probably uses a UI toolkit to let the system draw fonts, and for whatever reason it's not using the font fallback stack properly on Linux. There are font options for Pictures on , but nothing for annotations except font size. The manual entry for annotation objects specifically claims that you can use Chinese characters. We should report this bug to Praat devs. 

That depends on what you're trying to do. Are you interested in word-forms (inflected words), or lemmas (words abstracting infections, as in dictionaries)? Do you want "eaten" to count as an instance of "eat", or not? For example, suppose you're trying to measure how often the suffix -en occurs relatively to -ed. Then you want to count things like "eaten" and "devoured" as independent. But suppose you're interested in measuring how often the verb "eat" occurs relatively to the verb "devour". Then you'll want to increment the "eat" counter—let's call it EAT—whenever you see "eaten", "eats", even "ate"; while DEVOUR will count "devoured", "devours"... To do that, whenever you see a word-form like "eats", you'll want your code to convert it to EAT. This is called lemmatization. Note that this is orthogonal to the type/token distinction. You can count types or tokens of word-forms or lemmas, in all combinations. How many words are there in the sentence "I eat apples because she eats apples?" 

These consonants are traditionally described as "retroflex" ("bent backwards"), meaning the tongue tip curls back towards the throat. However, Ladefoged & Wu (1984) (also discussed on Ladefoged & Maddieson (1996)) have carefully studied X-ray photographs of native Mandarin speakers, and showed conclusively that they're produced with a retracted tongue blade, either flat or a little bit convex, with no curling back of the tongue tip: 

In /s,z/ the sides of the tongue are slightly raised, creating an u-shaped channel or groove, known as sulcus. You probably already do that unconsciously when you say a Korean /s/ (unless you have a lisp). Sulcalization (grooving) concentrates the airflow, making it sound more strident as it hits the top front teeth. In /θ,ð/ the tongue is comparatively flat, without sulcalization; this makes the airflow spread, resulting in a softer sound. An easy way to do that is to make the tongue tip peek out beyond the teeth. Many native speakers do that (if you pay attention, sometimes you can spot a tongue tip in a singer's /θ/ in a music video). Depending on the dialect and phonetic context, other speakers don't pop out the tongue at all; they can produce /θ,ð/ sounds with a flattish tongue entirely inside, tongue tip behind the top teeth. 

I argue that numerals are true ideograms (or "semantograms", meaning-symbols). Ask yourself: what does the symbol '1' stand for in the following examples?: 

The official Chinese language isn't "supposed to" be monosyllabic, at all. That's a misconception. Chinese languages are polysyllabic and that's it, including the putonghua standard (the pīnyīn orthographic standard, for example, includes rules to space the letters by polysyllabic words). The confusion arises because Chinese morphemes are usually monosyllabic, so that most (not all!) syllables are also morphemes (source: Packard, The Morphology of Chinese). This feature is reflected in the traditional writing system, which is syllabic, and thus lends an impression of "monosyllabism". But morphemes are not words, and syllables being morphemes isn't the same as the language being monosyllabic. In the past, some people (Karlgren) have argued that Chinese used to be monosyllabic. According to this old hypothesis, the syllables used to be more complex, and were used as independent words; as the phonetic system simplified, they supposedly became too similar to one another, so that new, polysyllabic words had to be coined. However, in reality polysyllabic words occur throughout the historical record; they're not new at all (see: Mair, Buddhism and the Rise of the Written Vernacular in East Asia, & forthcoming; and Dong, The Prosody and Morphology of Elastic Words in Chinese). It's not hard to see why languages aren't monosyllabic. English has well over 100,000 words; so does my copy of the CEDICT Chinese dictionary. Even with tones and unusually complex syllables, the number of possible syllables hardly reaches the low-tens of thousands. So a language that was monosyllabic would have to deal with an unusually restricted vocabulary—and the words would sound confusingly similar to one another, to boot. 

For a non-ASL example as requested: I don't know the Brazilian sign language (Libras) at all, but it was easy to find articles claiming that yes, they have idioms distinct from Brazilian Portuguese. One example is "expensive eye" = attentive onlooker vs. "cheap eye" = distracted person; another is "to squeeze your belly" = to laugh a lot. In other cases, idioms are direct calques from Brazilian Portuguese, like "wood face" = shameless, brazen.[1] Albres et al. [2] have a short quantitative study where they classified 243 metaphors and idioms, and compared them between the two languages: 

Several reasons: English pronunciation isn't easy Don't think that, just because you find it easy, most people in the world will; English pronunciation is actually quite complex by any measure. The language has something around 10 vowels (not counting diphthongs) and 44 phonemes; well above the average, and more than double Japanese's 5 vowels and 17 phonemes. What's more, English syllables are unusually complex, and may have long sequences of consonants (as in "lengths") and consonant-only syllables (as in "bottle"). Even people in Spain or Italy will unconsciously add vowels between the consonants in order to simplify English syllables—and Japanese syllables are even simpler. People generally have trouble with foreign sounds You say you find it easy to pronounce sounds not in your native language, and I don't doubt you. However, you're being unfair to the Japanese by singling them out; just listen to second-language speakers anywhere in the world, and you'll find that people generally have trouble distinguishing sounds not in their languages. For example, the Chinese speakers I known find it difficult to distinguish sounds like b/d or e/ɛ when learning my native Portuguese. English gringos have trouble with the nasal "ã", or even with saying a simple /o/ without turning it into a diphthong [oʊ]. I have trouble with English t/th; anyone from a non-tonal language has trouble with Chinese tones, and so on and so forth. Of course, people can learn these sounds (if properly instructed); but it's a well-known fact that many don't, even after years living in another country. This is interesting, because babies always learn all the sounds people speak around them (well, except for a few cases of "speech dysfunction", like lisps; but these are unusual). Adult foreign learners have a lot more difficulty. English is incorporated into the Japanese sound system Did you know that around 58% of English words came from Latin and French? But they pronounce them very different than actual Latin or French. The word "Latin" itself, for example, is like [ˈlæ.ʔn̩] rather than /ˈla.tin/. Et cetera is /ˌɛtˈsɛtɹə/ instead of /ɛt ˈkeːtera/, and so on. My point is, when you incorporate lots of words from a foreign language into your own, you adapt the pronunciation to fit yours. It would be a bother to keep changing from English sounds to Latin sounds all the time. Japanese has something like 30% "foreign" (gairaigo) words (not counting Chinese), most of which are English. Just like English speakers adapt words like "Et cetera", "Paris" or "Mexico" to fit their own sound system when speaking English, Japanese adapt English words (and others) to fit their own sound system when speaking Japanese. So "hamburger" becomes hambāgā, "cut" becomes katto, etc. Unfortunately, this means Japanese people have an easy-available but inexact "version" of English already in their minds, and this makes it harder to learn actual English pronunciation. When English people learn Latin, they have to begin by unlearning the way they pronounce Latin words and Latin letters, and get used to actual Latin sounds. Japanese people, too, have to learn to set aside their native gairaigo pronunciation. If the Japanese person needs to actually speak English, for example to live in England, they will learn it; but if they're just living in Japan and talking to other Japanese people, they'll just use the Japanized English words, for the same reason that English speakers use an Anglicized pronunciation of Latin words when talking to each other. English classes in Japan usually suck Japan has mandatory but atrocious English education, which is worse than no education. Richard Schmidt has pointed long ago that foreign learners won't acquire non-native sounds unless you draw conscious attention to them first. However, the typical English course in Japan doesn't try to teach the basics of articulatory phonetics or how to enunciate the sounds; they just keep doing grammar drills and such. I'm living in Germany, and I got a Japanese student to pronounce German-only sounds like "ö" and "ü" in one afternoon by explicitly explaining tongue and lip positions to her. I'm sure that, if a Japanese person is actually taught English phonology, they'll be able to pronounce English without major issues. I've met plenty of Japanese individuals with better English pronunciation than mine (admittedly, they usually have lived overseas, and therefore had incentive to learn actual English pronunciation, as opposed to their native Japanized English). 

Do Chinese characters stand for morphemes, or phonemes? You can't read a Chinese character without producing a sequence of phonemes. Some have argued that they're just a complicated notation for sequences of phonemes, making them no different than alphabets. However, in their most basic, systematic principle of usage, there's a fundamental difference between phoneme-based notation, and Chinese-style notation: you have to change Chinese characters not only when phonemes change, but also when morphemes change (yīn 'cause' is written as 因, yīn 'sound' as 音, yīn 'flourishing' as 殷…) This is what we mean when we call Chinese characters "morphograms". It's not that they don't represent sounds; they do. It's just that they represent sounds and meanings together, as units; that is, morphemes. Likewise, an ideogram (or semantogram, semasiogram etc.) isn't a symbol that cannot be put into words and will never represent words, ever. It's a symbol whose usage systematically points to a given concept, which can then be expressed as words according to context (words which, in turn, are expressed as phonemes, just like morphograms/logograms). 1 doesn't systematically denote the word 'one' or any other particular set of words, but the mathematical concept of unity, however it's expressed. (This is unsurprising when we consider that it's not originally a linguistic symbol, but a math symbol.) If the difference is still confusing, compare it with a Chinese morphogram (or, to keep with your example, let's pick one that works as a logogram, though Chinese symbols are basically morphograms). The Chinese symbol 狗 always represents the word gǒu, 'dog'; so it can be said to be a logogram for gǒu, 'dog'. However, like English with 'dog' and 'hound', Chinese has other words for 'dog', like quǎn; and quǎn isn't written as 狗 but rather 犬. So 狗 cannot in any way be said to represent the idea or concept of 'dog'; it represents the specific morpheme gǒu (which, in turn, is realized as a sequence of three phonemes and a tone). The symbol '1' does not work like that; it represents the concept of 'oneness', no matter if it's realized as the words "one", "ten", "fir(st)", cien… 

IPA is a representation of sound; therefore "transforming to IPA" implies converting text to sound. That task is harder than it seems, because writing systems are underdetermined – they don't include all the relevant information for pronunciation. You need language-specific knowledge to pronounce things: 

Notice that the output has some idiosyncrasies; it marks stress before vowels, but in the case of Chinese this feature is pointless/a bug; it marks so-called "retroflex" (backed) sibilants with a , rather than IPA /ʂ/ etc.; and it uses pīnyīn tone numbers, rather than IPA tone marks. These could be easily solved in post-processing. More problematic is the fact that TTS isn't perfect, and errors may creep in (/əː1/?). I find it's generally better to try and find TTS systems specifically made for that language, rather than broad ones like . 

There's no cut-and-dry line. Languages differ between each individual, but if they can understand one another very well, we still say they speak "the same language" (even if you use "haters be like" and they don't, and you say caught like cot and they don't). Sometimes languages differ in a consistent way among a certain geographic or political line, or community (e.g. Afro-Americans) etc. If these differences are pronounced enough that they have some trouble understanding one another, but can still mostly understand it, we call it a "dialect". If understanding is low or zero, we say it's "another language". That is, the most common criterion linguists use is mutual intelligibility. This is a fuzzy, subjective measure, prone to disagreements. Oftentimes things are defined as "languages" or "dialects" out of nationalism, politics or tradition. So, for example, many varieties of Japanese and Chinese that their governments call "dialects" are considered to be "different languages" by scientists, due to being clearly unintelligible to standard speakers (so that linguistics books talk of "the Ryūkyūan language family" where the government says "the Ryūkyūan dialects"). On the other hand, I speak Portuguese and can mostly understand Spanish without ever studying it; perhaps Portuguese would be considered a "dialect" of Spanish, were it not for the fact that it's a different nation and they'd be pretty annoyed at the idea. This led to the oft-repeated quip that "a language is a dialect with an army and a navy". 

But let's say you're not worried about contextual tone phonetics like that. You want to know about good standard canonical cases where high tones are higher in pitch, dammit. So we do a modified experiment 1': 

More on the phonetics than phonology side, but Ladefoged’s Vowels and Consonants is very accessible and the author is a standard reference. From there you should be able to follow The Sounds of the World’s Languages for discussion on a wide range of languages. I’d also like to recommend, as a companion to whatever you choose, Catford’s A Practical Introduction to Phonetics. This is an unusual book; it’s very short but intense, because it’s all about practical exercises to get you to perform sounds of all kinds—including the more exotic things like ejectives and whatnot. It will give you an entirely different kind of hand-on knowledge about the possibilities of speech. Be warned that, while working on it, you will catch yourself unconsciously making funny noises in public and attracting attention. 

—which can account for everything the notion of 'rime' could, and more. It's mostly language-independent, but there's a distinction in that some languages have weightless C codas (as in (c) above), while in others C codas carry weight (as in (b)). For example, in Cairene Arabic both CVC and CVV attract stress, so CVC falls under structure (b), while in Khal Mongolian only CVV attracts stress, so that CVC are monomoraic as in (c). 

In the 1920s, the Japanese linguist Hattori Shirō wanted to investigate local dialects. At the time, the two most notorious dialectal areas in the region were Tokyo-style or "Eastern", and Kyoto-style or "Western" Japanese. The two areas differ in vocabulary: For example, the copula "to be" in Tokyo is da, while Kyoto prefers ya. They differ in grammar: for example, to conjugate the verb taberu, "to eat", into the negative, "not eat", Tokyo goes tabe-nai while Kyoto uses tabe-hen. And they differ in pitch accent patterns; in Tokyo HAshi means "chopsticks" and haSHI means "bridge", while in Kyoto it's the other way around. But Hattori wanted to map out precise borders, so he took the train, and stopped at every stop to talk to people and listen. Turns out that exactly up to the east margin of the Ibi river in Mie one finds Tokyo-style pitch. Cross the river and people on the other side clearly talk in Kyoto pitch. 

The rest of the verbal morphology uses both bound prefixes (ta-pe-só "you may go") and suffixes (xe-só-reme "if I go"). It also features a form of noun incorporation; the direct object, or a pronoun agreeing with it, must be tacked on before the verb as follows: 

Similarly wide uses of -te can be found elsewhere in the Kojiki, the Man'yōshū etc., meaning they go as far back as we have data. Pefective -tu/te makes a pair with perfective -nu; they occur in Old Japanese more or less with the same function, but with -tu/te preferred for transitive verbs and -nu for intransitives. They're thought to be both derived from copulas, -te from to and -nu from ni. Frellesvig thinks connective -te, perfective -te, copulas -to, -tu, genitive -tu and continuative -tutu all trace back to some proto-element with a -t-; but this is reconstruction, we don't have documentation of this common ancestor. 

Does experiment 2 show that languages have something specific in the phonetic realization of tones? Assume for the sake of argument a restricted universe where all tonal languages realize tone in exactly the same way: /H/ is always a simple level high-pitch, and /L/ is always a simple level low-pitch. Let's postulate a non-tonal language, say language B, with the following phonetic behavior: aspirated consonants cause high-falling pitch, unaspirated stops cause medium-level pitch, and voiced consonants cause low-rising pitch, except that the first syllable of a word is always low-rising. From this data, I can't even tell whether B is tonal or not; from my point of view, pitch seems to be all over the place. And yet, in this universe, every single tonal language has exactly the same phonetic realization of tones. So the answer is again no; I can't conclude anything about tone universals (or lack thereof) from experiment 2.