Personally, I would start with DBA_HIST_ACTIVE_SESS_HISTORY, and look at all statments that contain that type of hint. From there, you can pull the index name coming from that hint, and then do a lookup on dba_indexes to see if the index exists, is valid, etc. You should be able to do this via PL/SQL if you want to make it really fancy and do it all in one step, otherwise a few pieces of SQL and a spreadsheet application can be your friend. 

It's not necessarily the number of transactions, but the timeout. The parameter is set to 60 seconds by default. The purpose of this parameter is to avoid having distributed transactions in a long running wait status while something else is performing work on that row; the transaction will wait 60 seconds, then Oracle kills it. You can modify this parameter (requires an instance restart) to whatever you want (in seconds). 

Don't get hung-up on "cloud" vs "local" - what you're doing is establishing a TNS network link between two databases. It is nothing more than that. In order to do this, you will need to have the appropriate tns entries configured in your tnsnames.ora (unless you are using LDAP or you pass in the entire connect string). If you don't know how to do this, the netca application will create one for you using a wizard. Once you have this tns entry, you can simply create the database link as you listed above, but replace the USING clause value you have with the tns entry alias. $URL$ 

OPTION (RECOMPILE) allows constant folding of @Step and @FindNo in your second query. Since @Step is known to equal 1, the line will be optimized away. Notice that this affects estimated rows. If you want to see a more drastic example of this, try the below code. Notice how constant folding (enabled by option recompile) allows the second query plan to completely avoid the union, since the optimizer knows no rows could be returned. 

Bad news: the plus approach in your script completely flattens out the XML document, and will need to be reworked if you have any multiple-X-per-Y structures in your document. If the dynamic SQL is a requirement, then please provide an example to test that aspect more easily. Another approach might be to build queries you need manually. If your fundamental problem is including parent info, then you might modify my demo SELECT query below. (N.B. Mikael Eriksson's answer has an improved query. Please refer to that.) Key ideas: is the context node, and gives you the parent is a different kind of XML function that belongs in the FROM section of the query and is usually seen with CROSS APPLY. It returns a pointer per match, and is what allows working with multiple rows. Read more here. is one of several methods for letting SQL Server know the value method only has one piece of data to work with (fixing the "requires a singleton" error) 

I suspect this may have something to do with you starting impdp as sysdba. Please see the note in $URL$ "Do not start Export as SYSDBA, except at the request of Oracle technical support. SYSDBA is used internally and has specialized functions; its behavior is not the same as for general users." Try your command as system. 

I completely agree with dartonw's comments about reading through the guide. That's the only real way to start understanding the concept of indexing as it relates to Oracle and the Optimizer. One thing I will add to it though is this - Philosophically, as a DBA, I rarely decide what columns to index on my own. What I mean by this is that I go back to the developers and ask them for the use cases of searching that table. If the table has five columns, and the dev tells me "we'll be joining this table to that table by columns a and b to retrieve columns c,d,e", that tells me that I need the index on columns a and/or b. The Developers/business users and their SQL should dictate what columns are indexed, not what you have read about in a book. Each app is different, and your indexing should be built specifically around data storage and retrieval of that app. I hope that makes sense. 

According to Paul Randal in his Pluralsight course "SQL Server: Logging, Recovery, and the Transaction Log" the LSN is composed of three parts: 

I'm fighting against NOLOCK in my current environment. One argument I've heard is that the overhead of locking slows down a query. So, I devised a test to see just how much this overhead might be. I discovered that NOLOCK actually slows down my scan. At first I was delighted, but now I 'm just confused. Is my test invalid somehow? Shouldn't NOLOCK actually allow a slightly faster scan? What's happening here? Here's my script: 

Short answer: start SQL Server with the -m flag from a local admin account. Step-by-step instructions here: 

The most promising exploration comes from removing the trash variable and using a no-results query. Initially this showed NOLOCK as slightly faster, but when I showed the demo to my boss, NOLOCK was back to being slower. What is it about NOLOCK that slows down a scan with variable assignment? 

This demo shows that a delete can produce a very unbalanced b-tree, with practically all data on one side. 

Note how the first open transaction (Transaction ID 0000:000002fa for me) isn't committed until the end of the REBUILD ALL, but for the index-by-index rebuilds, they are successively committed. 

This gives me a block-for-block recreation of the production level database, and depending on how many threads you give RMAN, can be completed pretty quickly. From there, I have a set of environment specific scripts I run that change passwords, null out email addresses, etc in the DEV database to configure it the way it needs to be. 

Looks like Tom Kyte has the answer: $URL$ Gandolf, you were technically correct according to what Tom responds with, but for whatever reason (bug? user error?) I couldn't get that to work. So I tried the workaround: 

Technically, you could use Oracle's SUM function (good explanation here: $URL$ Your code would look something like this: 

This might be over simplifying it, but could you use an block around the call to the extra procedure? As in, "check to see if the variable I just wrote to is null, if not, do more work, if it is, specifically call my custom error package? If you've built your own exception handler (and kudos to you for not falling into the "catch all the errors generically" trap), you don't HAVE to call that in an exception block. You can call it whenever you want, and trap the errors whenever they occur.