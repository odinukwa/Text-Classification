In my opinion option 3 is the best. Option 1 has the problem that you always have to use a like on the tags field to find the articles. Option 2 will cause a rewrite of the tag each time that an article is added or deleted. When there are a lot of articles for the tag then the record becomes long. Both option 1 and 2 will end up with 'big' rows which means less rows in a physical block which means more physical reads and 'slower' result. Option 3 has a simple setup. It is the way to present an M-to-N relation. Put an index on the name of the tag to help to speed up the finding of the tags (will not help much if you have a very limited number of tags). Adding an article or adding a tag to an article will be fast too. 

So change the DBID and unique name (Check the documentation on these subjects) and check the configuration files of sqlnet (normally in the directory) for references to server A. 

SQL*Plus is a client that is already available for a very long time. If you use then it does not use the . It connects to the local database that is defined in the environment variable. If you use then it will search in the file to find out the location of the database. 

The length of the key would be big. The data would be stored in the Person table too. Streets can sometimes be renamed or renumbered. 

Do not think that there is a script for this. It needs to be decided/defined before the data is entered. Otherwise it might block your application from functioning. Normally a table needs 1 field to be unique. Only if you create a table to link 2 different tables (for an N-to-M) relation then you need both the M and the N key to make it unique. There are exceptions but it is up to the designer to decide what field(s) make up the unique key. The data can grow and so can the uniqueness. 

I have a 2 node cluster (NODE-A & NODE-B) with 2 SQL instances spread between them. INST1 prefers NODE-A, INST2 prefers NODE-B. INST1 started generating errors then, failed over to NODE-B. Migrating INST1 back to NODE-A generates the connection errors after it logs a "Recovery is complete." message. Win 2008 R2 Ent. SQL 2008 R2 Ent. Errors from the Event Log after first failure: 

$URL$ shows that -Output expects a path, not an integer as the error message says. Pub & Sub are both v9.0.4211, Dist is v10.0.2723 My Script (run at distributor): 

Edit: I know PostgreSQL uses Sequences instead of IDENTITY, I don't recall the correct syntax off hand, so translate accordingly. 

You can, but I wouldn't. You would always have to wrap the DB name with square brackets such as [MyApp.Sales]. So to recap: if you value your sanity, don't do it. 

If your database is in FULL or BULK_LOGGED recovery mode, you need to backup your database and log files on a regular basis. If your database is in SIMPLE recovery mode, then you only need to backup your database on a regular basis. Please read the following articles for more info: 

You could do this. It would allow good performance for normal duration games, while allowing you to also store long running games. 

I know LINQ queries are composable, but have you tried playing with the order of LINQ operations to see if it could affect the query generator? This question may be better served over on Stack Overflow. 

You're pretty much going to have to use networking tools to do the monitoring. Another option is to query the size of the tables involved directly on the linked server and that will give you a rough estimate. If you're joining across linked servers, all the data must be brought over, then filtered down, so you're probably transferring a lot more data than you think. 

This gives a baseline to compare against. Notice that after the query completed, a statistics object was created (_WA_Sys_00000003_1FCDBCEB). The PK_PersistedViewTest statistics object was created when the clustered table index was created. Next, the filtered view and clustered index on that view are created: 

If using an in-memory table in SQL Server 2016, can I define a column in that table as a foreign key from a non in-memory (or disk bound) table? I know that the foreign key does need to be the primary key of the referenced table. I'm just wondering about the limitations and reliability of in-memory tables and how well they work with disk bound tables in SQL Server 2016. 

This execution plan looks quite similar to the one that was produced with the nonclustered index given in Max Vernon's answer. But, this one is done with one less (nonclustered) index and one less statistics object. It turns out that the NOEXPAND option has to be used with the express and standard versions of SQL Server to make proper use of an indexed view. Paul White has an excellent article that expounds on the benefits of using the NOEXPAND option. He also recommends this option be used with the enterprise edition to ensure the uniqueness guarantee provided by the view indexes is used by the optimizer. The above analysis was done with the express edition of SQL Sever 2014. I also tried it with the developer edition of SQL Server 2016. The NOEXPAND option does not appear to be required with the development edition to achieve the performance gains, but is still recommended. Less than 5 months ago, Microsoft made the developer editions free. The license restricts the use to development only, which means the database can't be used in a production environment. So, if you have been looking to test out memory optimized tables, encryption, R, etc. then you no longer have the no-license excuse. I successfully installed it on my computer a few days ago along side SQL Server 2014 Express with no issues. 

Create an SQL that does the same as the would do. Remove the rows from the corresponding ~50 tables in the right order before you remove the users. This is some work but it keeps your data safe from an application error that does an accidental remove of a user with (lots of) messages. 

When you know the names of the tables and fields then you are able to craft attacks on their database. The famous 'SQL injection' becomes easier when you know how the data is stored. At the same time you see what information they store. Like they say: Data is one of the biggest assets of a company. 

The solution is a 'normal' solution to generate a unique key. It is used when the database has no sequence or auto-increment option. You can also use it to become database independent. 

I do not think that you want to test Oracle 12c itself but if the applications still function. In that case I propose that the developers (or test team) perform there 'normal' application tests to see if the applications still function correct. The only thing that the DBA might give is information on the differences between Oracle10g and Oracle 12c at the handling of the data at SQL-level. 

If you have multiple control files then you can try to copy another one over this corrupted one. Otherwise you need to restore it from an (RMAN) backup. 

Take into account that the size you get also contains the empty space that is allocated to the table. When the table was just extended and only 1 new record is written in this extension then the information is less correct. But given the fact that you have nearly 2M of rows per day the space per row will be quite accurate. 

You can only reference servers that are listed under Server Objects -> Linked Servers as well as the local server via what you get back from @@SERVERNAME. Four part naming does not trigger a NETBIOS / DNS lookup. If you are referencing the local machine anyway, why not just use three part naming? 

I had written a process back in 2005 to use bcp to dump the data out then pg_import the data and do all the schema scripting and conversion necessary. Procs are slightly different because of syntax discrepancies. 

Have you tried using Adam Machanic's sp_whoisactive? There's an option to get the outer command to see if it really is within a proc. It could be the application is holding open a transaction instead of committing it. Try looking at DBCC OPENTRAN as well. 

I would suggest adding a (set of) staging tables to the destination or an intermediary that can be better controlled and more stable. Place the tracking info there. Then do all the transformation from the staging to the final destination, either carrying the tracking info with it or discarding it. There are several ways you can generate a tracking key from multiple systems, as long as they all follow the same algorithm, it doesn't have to be an INT. It could be a 2 char prefix with a sequential number. For that matter it doesn't have to be just one column. 

You could also try at the beginning of the proc, setting isolation level to SNAPSHOT. More info available at: $URL$ You will incur some cost in tempdb for the row versioning. 

You could use a Document-oriented database for this. You could then create a program in your preferred language to import the existing documents into the db, parsing the folder structure for the metadata (customer, job#, etc). 

The following execution plan (with no view / index view) is produced after the following query is run against the table: 

This is an attempt at improving Max Vernon's work around. In his solution, he suggests using 2 indexes on the view and a statistics object. The 1st index is clustered, which is actually required since unlike a nonclustered index on a table, an error will be generated if creation of a nonclustered index on the view is attempted without first having a clustered index. The 2nd index is a nonclustered index, which is used as the index behind the query. In the comments section of his answer, I asked what would happen if a clustered index were used instead of a nonclustered index. The following analysis tries to answer this question. I'm using his exact same code, except I'm not creating a nonclustered index on the view. I'm also not creating a statistics object. If you are following along and using SQL Server Management Studio (SSMS) to enter the code below, you should be aware that you may see some red squiggly lines - which look like errors. These are (probably) not errors, but involve an issue with intellisense. You can either disable intellisense or just ignore the errors and run the commands. They should complete without errors. 

If the new plan is to be believed, after the addition of the view and clustered index on that view, the statistics appear to indicate that the time required to execute the query has now doubled. Also, notice that no new statistics object was created to support the new index after the query was run, which is different from the query on the table. The query plan still suggests that creation of a nonclustered index would be quite helpful in improving the performance of the query. So, does that mean that a nonclustered index has to be added to the view before the desired performance improvement can be obtained? There is one last thing to try. Modify the query to use the "WITH NOEXPAND" option: