Also, it was hell finding the commands again, in the end I had to just install LTSP on my workstation and run . For my own sanity, and other looking for the extra-help message on Google, I'm pasting it here: From : 

I have noticed that in most web browsers if they can't make a connection using domain.com, then they will automatically (and near instantaneously) jump to www.domain.com. Make sure your server is actually answering the request for domain.com. Do the following to test, and please leave a comment with results (this assumes the use of a Linux workstation, maybe someone else can post a Windows/Mac version): Make sure the domain resolves when used by itself: 

Basically it pings the SSH server once per second until it can connect, then it mounts the SSHFS share. My question is: Is there a more direct and less "hack-ish" way to make "/etc/fstab" wait until there is an active network connection before attempting to mount "/home"? An alternative idea I had, was adding the "sshfs root@10.0.0.200:/home/ /home/ -o transform_symlinks,allow_other,nonempty,hard_remove" line as an "post-up" script in "/etc/network/interfaces", but that still feels wrong. Environment: Server OS: Ubuntu Server Edition 10.04 Client OS: Ubuntu Desktop 10.04 

Edit Sat Oct 27 17:03:42 CDT 2012: While it's true that you can work with the RAID 10 array before it's finished re syncing, I figured that I would answer the question about how to pause during the re-sync process anyway. This is how you pause the script for the re-syncing process (IMO/AFAIK/etc.): 

Sounds like it's not using basic authentication... perhaps cookie session based... what are you hitting it with? wget? curl? perl? There are perl modules that can emulate a browser and you can write a script to navigate thru the login and fetch the XML you want. Google 'perldoc WWW::Mechanize' Would love to hear of solutions that require less module installation, however. 

to get a rolling 1hour average (or whatever looks like a good smoothing interval). use RPN to compare that to your current value and use another CDEF to highlight the spike. I have found limited value in percentage based banding around a moving metric. I've built interactive cgi's to help explore up/down spike detection using RRDtool and jQuery. 

It depends on your application and tolerance for failures. If you are running an oracle database for a financial business, you want expensive servers with hot-swappable parts and built-in redundancy (power supplies, disks, even cpu and memory). If it's a web server or compute servers with NAS storage, go cheap (on the server, not the NAS) as long as you can tolerate the loss of a box without much impact. Dont go so cheap that you are constantly replacing bad hardware. The general rule of thumb for me has been to use raid to protect your important disk-based data, but buy cheap commodity hardware for compute and web farms. Get a good load balancer that can detect when a webserver is not responding and mark it offline. Real life experiences: Bad: Running oracle on commodity hardware was a cheap solution that we were able to put together very quickly, but a bad CPU fan caused a server crash which forced us to restore Oracle from tape (ugh!). Good: We replaced 2 high-end heavily redundant machines with 70 commodity rackmount servers. We were able to drop maintenance on the 2 machines and started just buying $2500 'spares'. Over about 2 years, I think we only ever used about 6 of the 'spares' (the real challenge was avoiding deployment of spares for other purposes). 

Can's answer requires a user to run the script and then type the path manually, the following will allow you to use the syntax you specified in your OP: 

There are others that I am either forgetting, or omitting for simplicity's sake (SAS especially has a few extra, but complex, features). 

Looks like it was a matter of the at the end of the file name. I could swear I've named scripts in /etc/cron.* with a or suffix before, so it might be a bug... In any case: 

This will search through the process name for . ETA: You can also test what processes will be grabbed by using . is the sister command to and accepts most of the same CLI flags, but instead of killing the command, it just prints info about the match. In your case you would run 

I have a rabbit mq ha/cluster, and a virtual ip shared between the two nodes by pacemaker. The problem is that pacemaker will only failover if the entire active host goes down, but not if rabbit mq goes down on the active host. I can't just add rabbit mq as a resource because pacemaker will stop the service on the passive host, which will stop rabbit mq from syncing messages... How do I make the virtual ip failover if rabbit mq goes down on the active host? (I'm traveling, so I had to fill this out on my phone. I apologize for any typos.) 

ACK! several folks have suggested the wonderful date +%Y%m%d_%H%M%S style solution but nobody has mentioned the major caveat of '%' in crontabs... '%' it is equivalent to '\n' so your cronjob will likely fire and fail mystereously! You'll more likely want to simply escape it with backslash like this (and I also like to get some kind of inventory or other output to check that it ran). 

Not saying I know specifically how tomcat performs shutdowns... I would expect a pidfile associated with the process, or a control port that tells the application to shutdown. Barring those, however, it's common for scripts to 'hunt-and-kill' by looking at 'ps -ef' output (or similar). In these cases, it's easy for the kill scripts to be too aggressive and kill off all matching pids (or just the parents of those pids). Cant tell you how many times I've been editing a script in 'vi' only to be killed off by an aggressive 'stop' command someplace. 

The number one tool I wish I had when running a small site is 'push-button' builds. It makes patching, updates, and rebuilds easier, which can address a myriad of other problems in the future. No ssh properly installed on all boxes? no curl/wget/vim either? what about other in-house tools you'd like to have on each box? Having central management of your servers is one of the first tools you should have working to make future efforts much easier. 

Is there a way to have Zabbix place a call to a given user's cell phone in case of high priority issues that happen at 3am? My original plan was to use Asterisk and Festival as part of a Zabbix alert script, but it has proved to be to complex for me... Does anyone know of an easier way? 

Following the steps above, Zimbra now works again, the problem is that all the emails that were in the mail queue from a whole day of broken Zimbra, are now GONE! And I don't just mean that you can't see them in the GUI, they're not even on the file system anymore. There were at least 75 emails that were in the queue when I attempted to fix Zimbra, and this is the state of the spool now: 

Obviously it doesn't have to be exactly the above syntax, it just has to include a minimum of the real user (eg. root), the sudo user (eg. ksoviero), and the full command that was run (eg. yum install random-pkg). I've already tried , but it did not include the variable. 

We have a Dell 1950 with a DRAC. The DRAC is assigned an external IP and we don't have access to the firewall to create any rules to secure it. Assuming the password is secure enough to avoid brute force attacks, can the DRAC have an external IP like this? My gut says no, but I've always worked with the OS, never with the hardware. Thanks! 

I use this to keep two machines in sync, or to keep to subdirs in sync (like backing up to a USB drive). As one of the other posts stated earlier, the 'checksum' may actually be forced OFF if you are dealing with local drives. In some rare instances, I've had to add additional parameters to account for changes in login accounts across remote machines, changing ports, and even specifying where 'rsync' lives on the remote host... but those are not directly applicable to your question. 

I always run the above to make sure it works, then remove the 'n' flag that once I'm happy with the results. The key features of the above combinations: 

GomezNetworks or Keynote are good 3rd party payware services that can provide performance data and handle the javascript nature of the website. dotcom-monitor.com seems to be another service that might help, but the key point is that you likely want 'transaction' monitoring, not just 'hit this URL' (though dotcom-monitor can do a form POST directly), and you want them to have full browser emulation (ie: javascript), not just simple HTML POST/GET etc try googling 'web transaction monitoring service' 

You might consider using `date +%w' as part of your tarfile, so you have a tar file for each of the last 7 days and dont have to worry about purging old copies. 

Caveat! Every single time I think I understand the rules for how authentication works, I have to futz with the config repeatedly until I get some nuance correct. Use this only as a starting point. Re-read the apache documentation on mod_auth and mod_access in particular, paying special attention to the Order directive. Therein lies your answer. Hope this helps, and please post your working example if it doesn't match this one, as this is a pretty good recipe to have in an apache cookbook. --edit-- Testing the above shows that restricted area is forbidden to all except for those from the IP address, who must provide authentication. It is not clear from your question if users from other IPs need unfettered access to this 'restricted area' or if they are simply forbidden? 

Let's say I bought two Intel Xeon's and installed them into server class hardware... If one CPU failed would the other still function and pick up the slack, therefore providing fault tolerance? This does not seem very likely, but I figured I would ask instead of making any assumptions. 

The result is very quick, and certainly no where near the limit of three whole seconds. Also, this doesn't happen on all hosts, for example, it happens on virt1, virt2, and a VM called nas, but not on any of the other VMs. Hopefully, there's a Zabbix guru here who can help. Thanks! ETA: Here are the stats that asaveljevs was talking about: 

The reason I'm doing it this way instead of uploading the script into the container and executing that way is that this is just the proof of concept for a much more complex application we're building that has to take commands over stdin and run them in the container. It seems to work great, except for one thing. It moves onto the command while postgresql is still installing, i.e., 

We have a cron script meant to run Salt in our environment along with several other steps, but for some reason it isn't executing it at all. 

How would one go about objectively benchmarking different x86 server for their potential as x86 routers? One idea I had is to setup two subnets, 10.0.0.0/24 and 10.0.1.0/24, on opposite sides of a router. Then setup an Iperf client and server on the two subnets. However, this would only test raw throughput, and not how services like NAT and firewall rules act under load. What is an objective way to test the throughput a router could handle in reality, eg. test NAT, firewall, and thoughput all at the same time. EDIT: Is there a way to test a routers throughput directly without dealing with the bandwidth limits of the CAT6 cable in between. 

But when I try to map port 80 to a pid I get nothing: When I try seeing what sockets that specific pid is using I get: 

wiki's tend to have revision control built-in, and many are file-based (vs stored in a database), so rsync should work perfectly fine. I know folks who do this for 'TWiki' for replicating their installations to mulitple servers. Perhaps you only have 'ftp' access to your wiki files? you might consider 'wget' to pull from ftp (rather than the http interface) with the recursive (-r) and timestamping (-N) flags set so that it only transfers file that are 'newer' (which isn't exactly a diff). Once you have a 'copy' of what is out on the ftp server, you'd mark the update time somehow (often with just a 'touch' of a specific marker file). You would then edit normally via your local installation of the same wiki, then use 'find $dir --newer touchmarkerfile' to identify the updates for ftp and transfer them over via a script around an ftp delivery tool. I have used such a solution before (though I had the advantage of sucking the changes back to the main server via 'wget', so just used the recursive timestamping approach again. In hindsight, if I had 'ssh' access (I didn't), I would have simply used 'rsync -globtru[n]cv' to simply pull (or push) the files in each direction. 

I have several hosts I want to connect directly to my WAN subnet, eg., 1.1.1.1/29 (web, mail, and router for LAN). I have a web managed switch (DGS-1210-24), and was wondering if there were any security concerns with creating an untagged VLAN for this purpose. 

The closest equivalent to Active Directory for Linux is FreeIPA. FreeIPA is made by Redhat, and provides both LDAP and Kerberos based authentication to a Linux network... 

There are so many options for how to do this. My recommendation is to use the flag from to search through the process name for your file. For example: 

If anyone has this problem in the future, this is how you repair it. You may end up with more than a few duplicates, be that's better than losing an important email... To find the messages that got queued up, run: 

I can easily script everything listed above, I just don't know where to put the scripts to do this... QA: "Why not use NFS/CIFS/other-nas-protocol?" High latency network, and spotty connection at times... 

This script will take all CLI arguments given to it as source directories/files, and send them to the server. 

The "share resources amongst each to act on behalf of a single operating system" is throwing me. The only thing I know of that can do clustering of resources at an operating system level would be something like: $URL$