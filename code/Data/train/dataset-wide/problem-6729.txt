I would interpret it as, "I have knowledge about the same things you have knowledge about." On the other hand, if I heard: 

UPDATE: I stumbled upon this great website, the Interactive Atlas of Romance Intonation, that includes clickable maps with audio and video data collected from an intonational survey. The vocative tune was included among the elicited utterance types. The survey made a distinction between "vocative" and "insistent vocative", and these category names were functional rather than descriptive. That is, the survey provided a scenario for the speaker to imagine while calling out a person's name. As a result, not all speakers chose to utilize the descending third tune that is the subject of this post. But many did! 

The short answer is no, there is no "universal" question intonation. I think a few points in your question need to be addressed, though... First, the intonation to which you are referring is most likely the intonation for yes-no questions in English, or possibly echo questions, but not other types of questions like wh-questions (questions that start with who, what, when, where, why, and how), which don't rise at the end. Just the fact that not all questions in English have the same intonation is a hint that it is an acquired behavior. Second, stress is a grammatical construct (not an acoustic one) that does not even play a role in all languages (Japanese, e.g.). In English, the stress pattern of a sentence is independent of whether the intonation (i.e. "tune") of the sentence goes up or down on any given word or syllable. Generally, yes-no and echo questions in English can be characterized by a final rising pitch contour starting on the last stressed syllable in the sentence (not the very last word). For example, in, "You got me a BIRTHDAY gift?" the rise starts on the first syllable of "birthday", which is the last emphasized word in the sentence, but not the last word. Finally, although typologically there does seem to be a tendency across languages for yes-no and/or echo questions to end with a rising pitch--see Ladd (1981)--there are many counterexamples. Lee (2008) lists some: varieties of Hungarian and Romanian, Bengali, Greek, Basque, and many African languages. Also, in many languages with lexical tone, including Mandarin and North Kyeongsang Korean, if the tone on the final syllable is falling, the pitch falls at the end even in yes-no and echo questions. Ladd, D. R., 1981. On Intonational Universals. In T. Myers et al. (eds.), The Cognitive Representation of Speech. Amsterdam: North Holland Publishing. Lee, Hye-Sook, 2008. Non-rising questions in North Kyeonsang Korean. In P.A. Barbosa, S. Madureira, and C. Reis (eds.), Proceedings of Speech Prosody 2008 Conference, Campinas, Brazil, 241-244. 

I'm guessing by "on the fly" you mean you can be speaking into a microphone and seeing a waveform, spectrogram, pitch contour, etc. produced as you speak into the microphone? Praat cannot do this, but WaveSurfer (another free speech analysis program) can. You can decide ahead of time what types of display windows (called "panes" in WaveSurfer) to have open, and then as soon as you hit the record button all the displays start scrolling across in real time. As soon as you start speaking into the microphone, you can see the various panes responding accordingly. When you hit "stop", the scrolling stops and the displays become static and you can interact with them largely in the way that you would in Praat. Praat is great for certain things, but WaveSurfer is great for demonstrations in classes when first introducing the concept of a spectrogram. It shows very quickly what different types of segments look like in a spectrogram, as well as the relationship between loudness and waveform amplitude, pitch and glottal pulse frequency, etc. 

Assuming you mean phonologically voiced (because if you're whispering there can't be any phonetic voicing), this indeed has to be a language-specific question, because how phonological voicing contrasts are realized phonetically differs from language to language. For English, there are often several other phonetic manifestations of phonological voicing other than phonetic voicing (in fact, quite often phonologically voiced consonants are not phonetically voiced at all). What these manifestations are depends on the consonants in question and on their position in an utterance. For example, word-initial voiceless stops (/p t k/) tend to have stronger bursts than their voiced counterparts (/b d g/), where "stronger bursts" means longer and more intense periods of noise. They are also characterized by a relatively long period of aspiration (especially if they are in a stressed syllable) whereas the voiced stops lack such aspiration or only show a bit of it. Aspiration may be distinguished from whispered phonation by being more intense (because the air coming out is not as impeded by the vocal folds). Also, the period of aspiration usually adds duration to the total duration of a syllable (this relative duration difference is usually not perceptually relevant in my experience, but it can be seen in spectrograms when comparing minimal pairs side by side). As for syllable-final stops and fricatives, the relative duration of the nucleus is dependent on the voicing of the coda. All else being equal, a nucleus before a voiced coda will be longer than a nucleus before a voiceless coda. Try recording yourself whispering minimal pairs like seat and seed to convince yourself of this (or have a native speaker do it if you are not one yourself). For phrase-final stops, released voiceless stops often display a stronger burst than released voiced stops. Finally, if the consonants in question are in positions where their closure durations can be measured (stops in utterance-initial position and final unreleased stops do not fall under this category), then all else being equal, the closure duration of a voiceless consonant tends to be longer than that of its voiced counterpart. This is true for fricatives, too, where the period of frication is longer for voiceless fricatives (like /s/) than for their voiced counterparts (/z/ in the case of /s/). Try recording yourself whispering E.C. and E.Z. to confirm this. 

I started writing another comment, but I figured I'd just bite the bullet and turn it into a response... It's not quite true that there's no way to figure out which dialect is more conservative; it's just that there's no way to figure it out with the one data point you've provided. As a phonologist I'd look for more data to support one rule vs. the other. If there are other examples of [χ]-initial words in D1 that don't contain uvulars later in the word, and those words are pronounced with an initial [h] in D2, then I'd go for the second rule. If, on the other hand, there are in fact words that start with [χ] in D2, then I'd go for rule 1. Note that by saying "χ-initial words are disallowed" you're basically asserting that the former is true, in which case the data point you gave does not provide strong evidence for uvular harmony in D1. Simpler to assume that the underlying form is just /χeqepo/. But if there is a word that starts with [k] in D2 that is pronounced with an initial [q] in D1, that would be stronger evidence for uvular harmony: 

You could invent a tonal system for Russian from scratch. This is what @guifa and @user6726 seem to be suggesting. You'd have to decide to what unit the lexical tonal patterns would apply in Russian, and you'd have to assign a pattern to every single word in the Russian dictionary. I agree with @user6726 that the normal speaking rhythm of Russian doesn't lend itself to a Mandarin-style system, so I'd go for something more along the lines of the systems found in languages like Yoruba or Japanese, where many words are polysyllabic and thus the tonal pattern, made up of a series of relative high and low pitches, is assigned to the word. One of the most complicated aspects of the tonal system that would need addressing is how the lexical tones in this new version of Russian would interact with the already existing intonational system of the language, since both tone and intonation would dictate the pitch patterns present in the complete utterance. Tone-intonation interactions are completely language-specific and unpredictable, so you'd have to decide, for example, how the tone on a given word is produced when the word is part of a yes-no question versus when it is part of a declarative statement. You could manipulate Russian utterances such that the syllables in them were characterized by durations and pitch contours typical in Mandarin. I can't stress enough, however, how much this wouldn't be "speaking Russian with Mandarin tone". This would basically just be like singing in Russian--imposing melodies and rhythms on the words and phrases--and the melodic contours would not be contributing anything lexical or grammatical to those utterances. There would be several ways to achieve this effect. For my money the most straightforward way would be to find a native Russian speaker who is fluent enough in Mandarin to take a crack at it. I know they're out there, because when I took Mandarin in grad school there was a Russian student who was learning Mandarin in order to be able to speak the native language of his future wife, who was Chinese! If I were to try to use speech technology to generate such utterances myself, I wouldn't use outright speech synthesis; rather I'd take recordings of carefully articulated Russian and use Praat's manipulation functions to alter the durations and fundamental frequency contours of all of the syllables. For both the Russian human and for me, the decisions about what contour to give to each syllable would be completely arbitrary (other than, perhaps, avoiding too many strings of the same contour in a row and also avoiding sequences that don't generally occur in Mandarin). You could forget the Mandarin-Russian example and focus instead on two languages that both have lexical tone. In this way, you could bring the task more in line with the English-with-a-French-accent analogy. Ideally you'd want two languages that have similar tone-bearing units, like the syllable in both or the word in both. You could then come up with a straightforward substitution rubric--for Tone 1 in Language A replaces Tone 3 in Language B, Tone 2 in Language A replaces both Tone 1 and Tone 5 in Language B, etc. In such a scenario, you'd also have to decide what to do about that pesky tone-intonation interaction. The easiest thing to do would be to adopt the intonation of Language A along with its tones.