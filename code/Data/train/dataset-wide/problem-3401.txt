I agree with TomTom regarding the memory requirements. 24GB will be insufficient to host 10 virtual machines unless they happen to be very limited in functionality. We just purchased a Hyper-V server and loaded it wtih 96GB of RAM in anticipation of running 10 to 15 virtual machines. As for the CPUs, real cores are always better than virtual cores, but the 1.8GHz Xeons are on the rather slow side making it a tough call. If you can manage to get a pair of 2.4 to 2.6GHz Xeons, that would make the choice much easier. 

One group within our IT department wants to eliminate all drive mappings from our network and replace them with simple Network Location (UNC) shortcuts. The group claims the drive mappings consume too many resources on the servers and hurt performance. Another group believe the convenience of being able to relocate files as needed and alter the drive mappings through login scripts overrides the resource consumption concern. Does either method, drive mappings or network location shortcuts, have a significant advantage over the other? Is the resource consumption a legitimate concern? I realize that some applications may not be able to handle UNC paths; we will need to deal with that on a case-by-case basis. We have about 500 client PCs each having on average about 5 mapped drive connections. Thanks for your thoughts. 

I have a virtual hard disk file (VHD) created using Microsoft Virtual PC 2007. Can this file be copied to a host running Microsoft Virtual Server 2005 R2 and be used as the basis for creating a new virtual machine? Essentially, are the VHD file formats between the two products compatible, or do I need to create a new VHD from scratch? Thanks. 

As I recall, the retail license for Windows Server 2008 R2 EE comes with four VM licenses. That doesn't mean you can't create more than four VMs, but it does mean that you will need to purchase licenses for the number you create beyond four. You could purchase a license for each VM beyond four, but as explained in this blog on Microsoft TechNet $URL$ You can assign two EE licenes to the host to obtain 8 VM licenses, or three EE licenses to get 12 VMs. It all comes down to how many VMs you need as to which OS you need to purchase. And don't forget to purchase lots of RAM in your server. Hyper-V loves RAM. 

In my company we have high employee turnover, and hence our helpdesk receives about a dozen requests per week for new Active Directory accounts. Currently, we receive these requests simply via e-mail or voice-mail, and rarely do we have all of the information necessary to create the account. I would like to find a web application that can be used by a manager or supervisor to formalize the requests they make for AD accounts for new employees under their command. Ideally, the application would prompt for all of necessary information, and allow the helpdesk to review the requests and approve or deny each one. If approved, the application would take care of creating the account and send an e-mail to the manager. I have found several application on the Internet that handle self-service account management (i.e., password resets or update contact info), which is also nice to have, but nothing that streamlines the new account request and creation part. Can anyone make suggestions on such an application? Thanks. 

Have you tried mod_evasive? It may be exactly what you are looking for, depending on the block time you want. 

To completely block traffic you could do an iptables drop command for the outbound traffic to that dst address. ie 

In the USA there are OSHA standards with regards to ambient noise. If the ambient noise exceeds a certain threshold you are required to wear ear protection. I am sure that any EU countries have even stricter standards. Even if it's not immediately "ear-piercing," prolonged exposure to elevated noise levels can damage your hearing. $URL$ 

(Modified from this config example on monit.com) This could play into the ulimit option mentioned earlier as well. Restarting the service is a bandaid. You should instead try to find out why it's leaking memory. 

As it sounds like your company is running a captive portal this will be quite an issue. If you log on to the captive portal are you then able to use the proxy normally, or do you have to keep that portal session active? 

To connect directly to the desktop. This will automatically SSH to servername.domain.com, forward a port to the destination VNC server, then connect to that port. Replace desktop with the internal DNS name or IP of the desktop, and of course the rest should be obvious there. As an alternative there's also X forwarding, which you can bounce through a tunnel, but XLib does not make any attempts to optimize for bandwidth. 

I've been fairly happy with RackMonkey. It allows me to define the apps that run on the bare metal, what hosts do what functions, where Staging is for the app, where Development is for the app, etc. It's also just perl/sqlite3 so it's easy to extend the functionality. 

By far the simplest way to do this is to install VNC Server on the Desktop machine. Then you can do the following from your laptop with RealVNC Viewer installed: 

The name argument directly after is freeform and can be anything. The name is supposedly only used internally inside Monit for purposes of notification. As long as you specify the pid file, monit does not care what the name of the process is. See their docs (search for procmatch): $URL$ 

So you could specify any number of check cycles between checks for your expensive check. Configure Monit to have the check interval you desire. 

A bandaid approach would be to listen non a non-privileged port, then redirect with xinetd from the privileged port. 

This is one of the areas where Puppet is helpful. Since Puppet is self-documenting, you can easily expand your network and recover from outages. Puppet collects certain information about the hosts it manages, the processors, RAM, disks, SSH keys, etc. The basic premise is to kickstart enough to get EPEL installed and onto the local PuppetCA, then your puppetmaster can take care of the rest of the configuration. Rolling endless crap into kickstart is not a solution. A software that you can use to identify changes in an environment is Blueprint, which can also play into Puppet configs. The end result is that your servers should be recipes. Predictable, reproducible recipes that you can test and deploy at will. The question here for you is the size of your environment. How many physical hosts are you managing? How many virtual? If it's not a lot, Puppet might not be worth the effort. 

You can use the nc_cmd configuration option in the template, as stated in the comment just above your last comment to the bug report on the Google Code Cacti template project, where you linked to this question. In addition, you should know that you are using outdated templates. As stated on the Better Cacti Templates project, that project has been discontinued and is now part of the Percona Monitoring Plugins. See $URL$ and note that this software is fully covered by Percona support or consulting contracts, which is a great way to get help with installation. Or, in the unlikely event that there's a real incompatibility that won't let you get the templates to install as-is, you'll be able to get bug fixes to the software. 

At a 5GB data size, you almost surely cannot and should not use a dump (myslqdump, phpmyadmin, etc) as a backup. The reason isn't the backup. The reason is because the restore will take a long time, potentially many days depending on the table structure and your server hardware. You need some type of file backup. Whether you use LVM snapshot or Percona XtraBackup or rsync or something else will depend on the storage engine you use, the hardware, filesystem, and a number of other factors. This is much too complex of a question to answer without a lot more detail, but I can simplify as follows: 

I would suggest the so-called user_statistics feature that Google originally developed. It is available in Percona Server and MariaDB, and there is a similar feature in the PERFORMANCE_SCHEMA in the upcoming MySQL 5.6 release. All of them make it rather trivial to determine what tables, indexes, etc are used. If this is not an option, then I would choose pt-query-digest as suggested by sreimer. 

The problems you're seeing could be caused by dozens of different root causes. You definitely should not guess; you certainly need to measure and diagnose carefully. If you gather enough information, the true cause will be evident, and the solution will be obvious (provided that you also understand enough about the server's internals to know what you're looking at). If you guess and try to do things such as reconfiguring the server, my experience shows that you can make the problem much worse, or cause other problems, and you'll never really know whether any specific change helped or not. I would suggest using the pt-stalk tool from Percona Toolkit to capture a set of diagnostic data from the server when one of the spikes of slowness happens, and ditto for when it's running more quickly. There should almost certainly be enough information there to understand what's happening. If you are not comfortable making a diagnosis from it, any competent MySQL support provider should have no problem if you tarball the samples from pt-stalk and send them over. I don't mean to be too repetitive or insistent, but again please don't use trial-and-error on this one. 

In this situation you should probably try to duplicate the volume rather than trying to duplicate the individual files. Does your iSCSI target have a way to clone the volume? 

By specifying you tell netstat to use the port number instead of the service name. If you look at the output of you will see that the port numbers are instead human-readable service names. These are mapped from , so if you are listening on port 80 you will see or , and if you are listening on port 8080 you will see or . From the netstat man page on Fedora 16: 

Your "most preferred" package is handled by . The command can give you information on the alternative packages and their priority. will display all of the editor entries and their priority, in a machine-parseable format. There is also a utility called that accepts input and returns the first usable entry. 

Really the best solution here would be APCUPSD, you ought to get something that has monitoring capabilities (USB,Serial,Ethernet). 

Mount the OS root partitions in /mnt/osA and /mnt/osB would give you some output similar to Solaris' You could then diff the more concerning files closely, like sysctl.conf, httpd.conf, etc. And how could I forget Blueprint! With Blueprint you can run against a system and get a recipe of what has changed from the default install. 

It's been my experience that things like these die as soon as your shell is terminated by idle activity. This is one of the circumstances where you should be using something like GNU Screen. With GNU Screen you log into your server, type , and then begin your work. You can spawn additional terminals inside that screen session with Ctrl+a c , and cycle between them with Ctrl+a n and Ctrl+a p. To disconnect from the GNU Screen you can type Ctrl+a d. To reconnect you can log in and run . A Howto from Kuro5hin is available here: $URL$ 

I found that once I had any kind of ACL or routing rule in place on a WRT54G that the processor limited me to approximately 12mbps of inspected traffic. I suspect that this will become your problem before any issues that might arise with the 2.4 band. 

This is an area where I've been met with a great deal of frustration in Solaris. One unreasonable way I've found that could work for Solaris global zones would be to create differential flars (one from the beginning, then one afterwards), then extract the differential flar's cpio archive and pass it to a DIY monkeyscript that would create a Puppet module. When you have ZFS root pools you can just create a new BE, make your changes, lucompare and send that to DIY monkeyscripts which can spit out your Puppet modules. But again that doesn't help with non-global zones. 

In your second scenario you could possibly NAT the entire second network to a 10.0.0.0/24, IE 10.119.0.10 would go out your natty router, be sent to their natty router, then NAT to 192.168.1.10 on their side. If it even works it will be as much work as just changing the network enumeration on either end really.