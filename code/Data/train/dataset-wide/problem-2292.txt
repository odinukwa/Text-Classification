and the distribution database (if you use replication) are also system databases that you do not mention by the way. 

The difference seems to be that simplifies further and as a result you get more accurate cardinality statistics going into the next join. This is an inline table valued function and you are calling it with literal values so it can do something like. 

This means you may get the results in key order but equally may not. Specifically see When can allocation order scans be used? 

You should do this with a foreign key constraint. You can create a check constraint with the following definition 

This appears to be incorrect! It seems from my testing that the value it actually uses for the in the second case is the of the default schema for the executing user rather than an identifier for that specific user. Running the four statements again under a different login with default schema "dbo" gives. 

Yes. If you only have one column in the table then storage uses a but up to 8 columns can be stored in the same byte so the next 7 are "free" in that respect. There is also a 1 bit per column storage need for the (again rounded up to the next byte). In the data pages this contains a for all columns irrespective of whether or not they allow (with the exception of nullable columns added later as a metadata only change via where the row has not yet been updated) 

The main difference here is that the predicate can now match multiple different colours. This means the matching index rows for that predicate are no longer guaranteed to be sorted in order of . For example the index seek on might return the following rows 

My question is: What is the reason for the better performance of the table variable version? I've done some investigation. e.g. Looking at the performance counters with 

Logical fragmentation occurs when the logically next page is different from the physically next page. In the case of the leaf level of an index with a monotonically increasing key this can happen if extent allocations for the index become interleaved with the extent allocations for other objects. Even without this a small amount of fragmentation will ensue due to the first page allocations coming from mixed extents and pages for different index levels being interleaved in the same extents. (Edit: Plus of course effect of updates and page splits as per @gbn's answer) 

Then you see a seek. BTW: The 20% estimated cost of the scan is likely to be an underestimate in this case. The FK validation is under a left semi join and SQL Server costs it as though only a partial scan will be needed and it will find a matching row and the delete will fail. Presumably the rows you are actually deleting will succeed more often than not and so a full scan will be required in order to validate that there are no conflicting rows. Using trace flag to turn off row goals 

Both of these pages being latched belong to (different) non clustered indexes on the base table named and . Querying during the runs indicates that the number of log records added by the first execution of each stored procedure was somewhat variable but for subsequent executions the number added by each iteration was very consistent and predictable. Once the procedure plans are cached the number of log entries are about half those needed for the version. 

SQL Server and PostgreSQL return 1 row. MySQL and Oracle return zero rows. Which is correct? Or are both equally valid? 

There is no built in declarative support for non updatable columns (except for specific pre defined cases such as ) This Connect item requested it but was rejected. Add DRI to enforce immutable column values An trigger would probably be the most robust way of achieving this. It could check and raise an error and rollback the transaction if true. 

You have and so you are introducing the same table twice without giving either an alias. Some of the join conditions appear probably wrong as well. 

Somewhat surprisingly no one has mentioned that this is built into SQL Server Data Tools yet. Though the functionality is basic when compared with Redgate for example. Some details in Compare and Synchronize Data in One or More Tables with Data in a Reference Database 

SQL Server maintains statistics on substrings in string columns in the form of tries that are usable by the query but not by the . See the String Summary Statistics section for more about this. A couple of important caveats are that any escaping of wildcards must be done with the proprietary square bracketing technique rather than the keyword and that for strings longer than 80 characters only the first and last 40 characters are used. 

The addition of the trailing wildcard has now caused an index scan. The cost of the plan is still quite low though for a scan on a 20 million row table. Adding shows some more information 

But you can see that under SQL Server's cost model this plan is given a higher estimated cost than the competing CI scan (roughly double). 

So you are looking for Employees that earn below the average in their current department but above the average in their prospective new department. One possible way of getting all employee transfers that would meet this would be 

My theory is that there there is some optimisation available when doing bulk inserts to local temporary B+ trees that only applies when it does not already have any pages allocated. I base this on the following observations. 

gives the closest estimated subtree cost to at . Either way it is clear that it is basing the costing on the assumption that each scan will only have to process a tiny proportion of the table, in the order of hundreds of rows rather than millions. I'm not sure exactly what maths it bases this assumption on and it doesn't really add up with the row count estimates in the rest of the plan (The 236 estimated rows coming out of the nested loops join would imply that there were 236 cases where no matching row was found at all and a full scan was required). I assume this is just a case where the modelling assumptions made fall down somewhat and leave the nested loops plan significantly under costed. 

You can't. You just need to disable it again. There is no syntax that prevents this. The disable trigger topic explicitly calls out that 

There's no need for copy only differential backups because you can just take a normal differential backup. Taking a differential backup doesn't break anything as each such backup is cumulative and contains all changes since the last full backup. If you were to take a full backup without that would potentially be problematic as it resets the differential base (clears the bitmaps containing information about which extents have changed). 

For at least the standard (FixedVar) record format it does not make any difference to how much space the table consumes (it might make a marginal difference to indexes as discussed later). Both a null and an empty string are stored in exactly the same way. The only way they are distinguished is whether there was a or in the null bitmap. They both take zero length in the variable length column data section and both can also avoid taking up two bytes in the variable column offset array if they are not followed by any columns that do contain data. One of the comments says