Most of the studies in this diagram have uncertainty intervals that cross the 0 line; Most of them cannot be certain whether HIT or traditional training is better, even if they find an advantage for HIT. The meta-analysis is also careful to not overstate the certainty of their results. A few months ago, I decided to switch from continuous training to HIT. This evidence is strong enough to convince me to keep doing HIT, but I wouldn't be too surprised if it turned out to be wrong. Science is plagued by statistical shenanigans and bias. Meta-analyses are the best tool for cutting through this, but they are still limited. 

In my opinion, "potentially helpful and unlikely to be harmful" is a great summary of this answer. There is some evidence that implies that the claim is plausible, but I don't see anything that directly supports or contradicts it. 

This is probably a low estimate of the number of papers on the topic. If I allow the word "antibacterial" I get an additional 50 papers. This search does not include papers that are about AMR but do not have these words in the title, (for example Colistin resistance gene mcr-1 harboured on a multidrug resistant plasmid.) It would be possible, but time consuming, to compile a list of the authors of those 2720 papers. Some scientists certainly published multiple papers on AMR in a single year, and duplicate names would have to be removed. On the other hand, most of the papers on that list have multiple authors, usually 3-5. I expect the number of papers to be of the same order of magnitude as the number of authors. A single scientist may have a handful of authored papers in a year, but share authorship with a handful of people. If we define a "scientist studying antimicrobial resistance" as someone who has authorship on a paper on the topic in the past year, I find the claim very hard to believe. A more exclusive alternate definition that would make the claim true exists. 

This claim is pretty hard to specifically pin down. What is a "scientists studying antimicrobial resistance" exactly? Do we limit the definition to professors who have been exclusively focused on AMR for 10 years or more? Does a grad student who is doing their masters thesis on a topic tangentially related to AMR count? Either way, I cannot do a world census of scientists who work on a particular topic. What I can do is examine the number of papers published yearly on AMR. According to Google Scholar, 2720 scientific papers were published in 2016 that had the word "resistance" in their title as well as either "antibiotic" or "antimicrobial." 

This scientific paper examined "cancer resistance in large and long-lived organisms, including elephants." Every time a cell in your body divides there is a tiny chance that its DNA will not be perfectly copied, a mutation. A larger animal has more cells, and therefore more cell divisions and more chances to have cancer causing mutations. Longer lived animals have more time to build up cancer causing mutations. Therefore it makes sense that elephants, being large and long lived, should have cancer all the time, but this is not the case. This contradiction between the intuitive explanation and actual observations of cancer rates is known as Peto's Paradox. The linked paper suggests that elephants may be more cancer resistant because they have additional copies of the gene TP53. 

Artificial neural networks (ANNs) are a machine learning tool that can be trained to recognize patterns in data and make predictions based on those patterns. Their structure was inspired by the structure of natural neural networks, brains. Like a natural neural network, their decision making processes are a black box. ANNs make a prediction (whether the individual is gay or straight) based on input data (a set of photographs), but they do not explain their reasoning. The researchers speculate on what the ANN might have been picking up on, but they don't really know. 

Although this data supports a broader version of your claim, it is not particularly high quality. The effects of car seat installation on injury rates was not the primary objective. The data here has a small sample size, because it is a subset of a larger broader data set. I suspect that the assessments of accident and injury severity are difficult to accurately measure. This quote supports the claim indirectly. "Many restrained children survived very high-speed crashes without injury to the neck or other parts, with deceleration injuries confined to bruising from belt loadings." A seat belt is supposed to spread the force of the crash over its area. I would think that a twisted belt has lower area, and therefore higher pressure, and probably worse bruising. 

Some scientific evidence shows that although dehydration might possibly trigger migraines, it is definitely not a common trigger. Drinking gatorade is supposed to rehydrate you. The scientific evidence is fairly muddy. This book chapter reviews evidence from many sources. "Taken together, virtually all aspects of life have been suspected to trigger migraine or TTH, but scientific evidence for many of these triggers is poor. . . possibly dehydration may trigger migraine and TTH in some patients." Dehydration is a "possible" trigger for migraines, not a definite one. When reading scientific research I see different versions of these two statements a lot: Dehydration can trigger migraine in some people, but a huge variety of things can trigger migraine and triggers are usually specific to individuals. If this claim is true, it is probably only true for certain people, possibly only the ones for whom dehydration is a trigger. It is also worth noting that migraines can trigger increased urination which can lead to dehydration. I personally suffer from migraines, and this is what happens to me. No matter how much water I drink, I piss it out and become dehydrated again. This is only tangentially relevant to the claim. The claim implies that hydration can prevent the migraine before it gets to this stage. This study, examines water deprivation headaches, but implies they are distinct from migraines. Their research method involved asking people if they got headaches when they were thirsty, which is not the most robust method for a scientific study. To their credit, their discussion and conclusion section did not overstate the strength of their evidence. This paper states that dehydration may contribute to migraine. This study, surveyed migraine sufferers about common migraine triggers. Their survey did not include dehydration. I believe the researchers made a list of migraine triggers and then asked the survey participants if they experienced the trigger. Although it is possible that some survey participants had dehydration as a trigger and noone asked, it seems likely that the researchers did not believe that dehydration was a common trigger. This seems odd because I found the paper about water deprivation headaches from reading this paper. At best, this is weak evidence against the claim. This study also surveyed migraine sufferers about common migraine triggers. The researchers chose 15 common triggers for migraines, and sent out a survey asking migraine sufferers whether they experienced those triggers. A key difference in this study, is that they left free space for participants to write in their own triggers. Although dehydration was not on their original list, 10 of the 522 people surveyed, 2%, wrote it in. This review paper, reviews the research on migraine triggers. Because it focuses on how people cope with their migraines, I don't expect them to go into depth on the less common triggers. They mention in passing that dehydration may lead to headaches, but do not cite their source or go into any detail. They also have a word of caution that I think applies to the study discussed previously, 

Summary: This seems to be an open question. I don't see anyone actually making the claim that raptors are running around spreading fires. The authors of this paper provide evidence that people believe that, which is a subtly different claim. The authors probably believe it; They say they are planning to invest time and money into proving their hypothesis. 

Since the advent of cheap DNA tests, several companies have sprung up which will tell you ancestral origins according to your DNA. 23andMe claims, 

I found evidence that appropriate installation of car seats is better than inappropriate installation. I did not find any data that addresses your claim about twisted belts specifically. I did a rather frustrating search of the scientific literature. Your claim is fairly specific. A lot of the literature talks about inappropriately installed car seats, but they don't usually go into detail on exactly how each car seat in their data set was inappropriately installed. I frequently ran across the claim that inappropriate use of safety seats is both extremely common and dangerous [1,2, and others]. When I followed the sources cited to back up these claims, I ran into paywalls despite my university credentials. This paper pointed out a confounding factor in studies that compare injury rates with and without proper car seat use: 

Conclusion The findings of this article are supported by some previous scientific articles and opposed by others. Economists cant really agree about when and if lobbying is helpful or hurtful to a corporation. 

This gold is present in only trace amounts. A penny sunk in a ton of water is about 2.5 parts per million. The value for cow liver is roughly 1/10th of that. Because we know that there is gold in a cow's liver, it is not surprising that it can also be found in cow urine. This scientific paper found 0.005673 ppm gold in cow urine. This is roughly 1/36th the concentration found in cow livers by Bertrand (quoted in the previous report). I have to give a tip of my hat to DavePhD for finding this source. As discussed previously, the concentration of gold in urine probably depends on a lot of different things. The paper does not describe in any detail how the cows were kept or held, or how much trace gold contaminated their food. The authors actually weren't interested in gold at all. 

Note: Although they try to eliminate any dependence on things other than fixed facial features, I am not sure if they succeeded. I am not deeply familiar with facial recognition algorithms or neural networks. The ANN in the paper used a training set with half gay and half straight faces. The ANN would be presented with a single gay face and a single straight face, and asked which was which. When the faces were made using 5 photographs, it was able to correctly choose which one was gay 91% of the time. This is quite different from any real life application, where roughly 10% of the population is gay. 

Note: This report was published in 2015, when the FBI numbers I just linked were not available. Based on the HRC's discussion of the challenges of collecting data, I believe their numbers are also an undercount. I applaud both their effort, and their openness about the weaknesses in their data. The HRC states that "in 2009, the FBI began tracking bias-motivated crimes based on the victim’s actual or perceived gender identity." So not only are the FBI numbers an undercount, they don't go back very far in time. This article, published by an advocacy group for LGBTQ issues, reports the number of murdered transgender people in 2016, but does not say where there data comes from. It also lists the names, and a few details for each murder. It repeats the claim, that 2016 was the deadliest year for trans people, but does not attempt to prove it by presenting any historical data. In summary, I found what appears to be semi-quality data going back a few years that comes from independent organizations. The FBI, which should compile authoritative crime statistics, appears to be severely deficient on this count. The claim that 2016 had more anti-trans murders than any other year ever is not based on historical data. If the claim were revised to say that 2016 had more trans murders than any year since 2013, that claim is grounded in some evidence. In recent years, trans people have gotten more organized, and gotten more media attention. It is entirely possible that the rate of trans murders is flat, but more of those murders are being recognized for what they truly are. The claim could be true, or it could be a case of reporting bias. 

The Bloomberg Article mostly faithfully summarizes the main conclusions of this scientific article, The economics of corporate lobbying. This article concludes that in most cases, lobbying is correlated with poor company performance. 

This page, written by the United States Geological Service, makes a claim about the improbability of an imminent eruption and then says that their numbers are basically made up. 

The original study, Massage of preterm newborns to improve growth and development, was published in 1988. The author, Dr. Tiffany M. Field still a professor and her lab is called the Touch Research Institute. The website for her lab repeats this finding on the home page. She apparently still believes her original finding, and is still doing research in this field. In the years since publication, the study has garnered 118 citations. These citations are part of post publication peer review by other research groups, which is the ultimate test for scientific concepts. These two papers are follow up studies that Dr. Field worked on. It is great that Dr. Field can repeat her own work, but I will be more convinced if someone else can replicate her findings. In 2003, Dr. Field wrote a review paper about Preterm infant massage therapy. She describes 4 replication studies, done by other people, that followed her method and got very similar results. She also briefly cites a Cochrane review of the subject from 2000. Cochrane "produces and disseminates systematic reviews of healthcare interventions and promotes the search for evidence in the form of clinical trials and other studies of interventions." They are a third party who's mission is to summarize the evidence so that practicing doctors can quickly make evidence based decisions. Both the 2004 Cochrane review she cited, and the the latest Cochrane review of the subject (published in 2015) say that there is some evidence of benefits but the evidence is not strong enough to support widespread use. 

This article looks at the frequency and recurrence rate of smaller volcanoes. It concludes that mega volcanoes do not follow the same pattern as smaller volcanoes, and we don't really have enough data to make real conclusions about giant volcanoes. Their focus is on the global occurrence of volcanism, not the reoccurrence patterns of individual volcanoes. They also limit their data to more recent volcanoes, for which we have more reliable data. 

This wired article discusses how this is not a good way to dispose of a phone that contains secret information. They suggested using a blender and fire. 

The Centers for Disease Control has applied for and been assigned patents for a number of vaccines. Vaccines are a great tool for controlling disease. The CDC licenses out their patents. I cannot find any actual discussion of licensing fees, but the form that you have to fill out to get a license asks for a market analysis. Presumably, so this is so they know how much they can charge you. They are not making billions from this. Their entire budget is around 7 billion page 13. This line item break down of the CDC budget lists one budget item in the billions, Vaccines for Children. This is a taxpayer funded program that provides vaccines to "children who might not otherwise be vaccinated because of inability to pay." In 2016 the CDC made 137.8 million from royalty income. Their total royalty income is slightly less than 2% of their budget. Presumably some fraction of that comes from its patents on vaccines. Summary: The CDC makes a small percentage of its budget from licensing vaccine patents that it owns.