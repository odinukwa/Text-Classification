The last two left joins are not used and should be deleted Have index on all ON conditions I would try materialize in a #temp with a PK 

Since no one is advising 3 tables I will advise 3 table Join on pk to indexed fk is very fast OP said would search details some times It would avoid dual updates of the data (if in two tables) 

Reorganize and Rebuild Indexes There are scripts out there that will go though and reorganize verses rebuild versus skip. I don't agree with Kin this is a knee response that will be a least help. Yes it looks like some smaller tables but defragmenting indexes is good starting point to looking at performance problems. Again you most likely have some big indexes that are fragmented. You can slow down fragmentation by using a fill factor < 100. But your index will also use more space. On a small table just leave it alone. You can also slow down fragmentation by inserting data in the order of the index. After that look at your worse preforming queries one at a time and address performance. 

Don't use here is going to be an arbitrary order. Are you sure that is what you want. without and order by is . You are partially batching but then you exclude partial batches. This seems wrong to me. You can just use in the final select. 

1000 transaction per minute = 16.67 / second = 480,000 / 8 hr day 16.67 / second is not the fast. I am getting over 100 / second on just a regular active big table. Pick your PK or at least one index that you can sort the incoming data by so you have minimal fragmentation of that index. If you can hold records to insert 100 or 1000 at a time and insert them sorted. One insert of 100 records is much faster than 100 inserts of one record each. Have timer that they are insert at least every x seconds. On the other indexes pick only what you need. Give them a fill factor of like 50. You would be amazed at how much slower fragmentation takes place if you leave some space with a fill factor. Perform index maintenance daily. Yes you may very well need to get more exotic but 1000 / minute is not that big. Even if you do get more exotic index design that minimized fragmentation is still a good thing. 

Almost every minute new rows are inserted. So, after one hour, I have several rows not ranked and I need to update the whole table again. However, it takes too much time. Is there any way to optimize or a different way to update the table faster? Maybe a different approach? Partitioning the table? 

Every minute the status of each player is checked and the table is updated accordingly. The number of players online is normally between 500 and 1200. Which means that data is written (with ) every minute and it can be hundreds or thousands rows a minute. What about reads? Anyone (player or guest) can access the stats of any player (as long as the player doesn't hide it). The site has between 8,000 to 10,000 hits a day. To avoid reading a table that is constantly being written, I cache the main query (that pulls stats from last 6 months) for 1 hour. Caching with PHP, saving the array to a file. Meaning that to access the last 6 months of a given player, it will be only one read per hour. However... there is AJAX. Players, not guests, can pick any day or any range of days, even older than 6 months, through AJAX. These queries are not saved into a file. Which means, if someone decides to check ranges or days multiple times, he will be hitting the table every time he pick a day/range. This table has 16 millions of rows. There are other tables like this one with even more rows, but let's stay with this one. Querying the table sometimes takes a long time (about 15 seconds). This is why I cache the queries. And obviously, as this is MyISAM, the table gets locked every time a query hits the table. So, considering all this, I came up with the following optimized version: 

I have a simple table with million of records (14,000,000) and for a simple query it is spending too much time "sending data". The table 

I would start here and add stuff in to see where it breaks. And defragment the indexes - probably not the core problem here but good practice and will make debugging faster. 

For sure you should have an index on PP.OBJECT_CD Make sure it is not fragmented Start with the above and examine (and post) the query plan Consider an index on #TEMP.price but that is going to slow down the insert More important why are you even using #TEMP in the first place? 

If they are willing to work with day old data then a table of the view updated nightly is an option. 3.2 million roww is a lot but unless very wide then manageable. In a transaction 

this needs a lot more information they are going to have registry, church, and a LOT of other stuff you need to read a book on database design or do some tutorials as your design is not very good starts with weddings plural they are not going go want a separate database for each wedding if everyone needs to logon then you need one table for that 

And then a FieldName table with (same) ID as the PK You could also add ID to valuesHierarchyTable and then would have ID11, and ID12 in that table. Then you don't need to hard code ID in the join. Two possible master tables depending on you you want to manage 

I only now SQL Server With 5 or more joins I have observed the query optimize will tend to go with loop joins more often. And a loop join can perform poorly. But you just need to deal with it. You can get good performance with 5 or joins - some times it takes a bit more work. 

8 minutes is not that bad. Is this a query you need to run often? If so a view might be more effective. 

Let's just look at TableOne, TableTwo, and TableFour Most of the condition in the "in where" in are already covered in the join And suggest you use join on for all 

I'm running a PostgreSQL 9.4.x instance on Amazon RDS. I can successfully connect to that server using SSL using a command line similar to this one: 

Suppose there is a PostgreSQL server running and it has SSL enabled. Using "standard" Linux and PostgreSQL tools, how can I examine its SSL certificate? I'm hoping for output similar to what you would get from running . And I'm hoping for a one- or two-liner command line answer so I don't have to resort to running a packet sniffer. I do not have access to the PostgreSQL server, so I cannot look at its configuration files directly. I do not have a superuser login, so I can't get the value of the setting and then on it. Using doesn't work because PostgreSQL doesn't seem to want to do the SSL handshake right away. From a quick look at the documentation, I could not find a command-line parameter that makes it show that information on startup. (Though it does show me certain cipher information.) 

As far as I can tell, on Amazon RDS you can't edit , so I can't put entries in there. This AWS blog post suggests how it could be done with MySQL, but I can't find links to similar information for PostgreSQL. My motivation is to prevent myself (or other users) from accidentally transmitting sensitive information unencrypted across the network. 

I'm using poorly documented third-party software that does esoteric things with PostgreSQL 9.3. At some point during its setup, it attempts the following: 

How can I force clients to use SSL (to make eavesdropping harder)? The following should fail to connect: 

It turns out there's a (somewhat hidden) permission for users, namely whether they have permission to "update system catalogs directly," which is specified by the column in the system table: $URL$ This other answer talks about how to set it: $URL$ Anyhow, once I set it to for my user, the was allowed. That column in appears to have gone away as of PostgreSQL 9.5.