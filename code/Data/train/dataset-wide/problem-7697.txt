Let $(X_t, t \in \mathbb{N})$ be a martingale, and let $a \leq b \leq T \in \mathbb{N}$ be constants. Is there something like Doob's inequality for $\mathbb{E} \sup_{a \leq t \leq b} X_t(X_T-X_t)$, i.e. is it possible to bound this supremum by something involving just a deterministic variance? I am hoping for one of the following inequalities to be true: $$ \mathbb{E} \sup_{a \leq t \leq b} \left(X_t(X_T-X_t)\right)^2 \leq C\mathbb{E} \left(X_b(X_T-X_a)\right)^2 $$ or maybe even $$ \mathbb{E} \sup_{a \leq t \leq b} \left(X_t)(X_T-X_t)\right)^2 \leq C\mathbb{E} X_b^2 \mathbb{E}(X_T-X_a)^2 $$ for some deterministic constant $C$. In my application, $X_t$ is the sum process of iid mean-0 random variables; perhaps that makes life easier? 

A friend pointed out that (in the opposite direction, i.e. going from an ODE to a discrete approximation) this is essentially Euler's method, and convergence theorems for it can be found in any reasonable numerical analysis textbook (e.g., Chapter 7 of Bradie, A Friendly Introduction to Numerical Analysis). 

I have a Lipschitz function $X=X(t)$ with the property that, at all points $t$, the right derivative $\lim_{\epsilon \downarrow 0} \epsilon^{-1}(X(t+\epsilon)-X(t))$ exists and is given by $f(X)$ for some (discontinuous) function $f$. (Of course, at all regulat points, i.e. almost everywhere, this is then the ordinary derivative.) Is it true that such $X$ is necessarily unique? Of course, if I just specify that a Lipschitz function satisfies $X'(t) = f(X(t))$ at all regular points, the solution doesn't have to be unique; but I'm requiring that this hold for the right derivative at all times, which intuitively seems like it ought to work. 

Pick two points $(x,0)$ and $(0,y)$ (say $x>0$ and $y>0$). Pick a unit vector $u = (u_1,u_2)$, $v = (v_1, v_2)$, and attach one to each of the points. Provided $u$ and $v$ are "nice" ($v$ needs to lie "between" $(u_1,u_2)$ and $(-u_1,u_2)$ in the appropriate sense -- in their convex hull if $u_2 > 0$, outside of it if $u_2 < 0$, and $v_1 > 0$ if $u_2 = 0$), there exists a unique ellipse with major axes parallel to the coordinate axes, passing through the two points, and with the unit vectors as its unit outer normals. (I think.) Is there a generalization of this statement to higher dimensions? 

Is there either a closed form (in terms of the moments of $X_1$, say) or good bounds on $$ \mathbb{E} \sup_{k \leq n} \frac{1}{k} \sum_{i=1}^k X_i, $$ where $X_i$ are iid and arbitrarily nice? (In my specific application, $X_i$ are given by $(B_i - p)^2$, where $B_i$ are iid Bernoulli variables with mean $p$.) I am particularly interested in the correct functional dependence on $n$; e.g., is there a constant bound that holds for all $n$? 

Consider the sequence of stopping times $\tau_{1,k} = \inf\{s \geq t, X_s = k\}$, $\tau_{2,k} = \inf\{s \geq \tau_{1,k}, X_s = k\}$, $\tau_{3,k} = \inf\{s \geq \tau_{2,k}, X_s = k\}$, etc. Then $\cup_{i \geq t, k} \{\tau_{i,k}\}$ is all the natural numbers bigger than $t$, and also $E[R_{\tau_{i,k}} g(X_{\tau_{i,k}}) | \mathcal{F}_t]$ is decreasing in $i$ for each $k$. I think these give you your result, i.e. $$ \sup_{t \leq \tau < \infty} E[R_\tau g(X_\tau) | \mathcal{F}_t] = \sup_{i \geq t, k} E[R_{\tau_{i,k}} g(X_{\tau_{i,k}}) | \mathcal{F}_t] = \max_k E[R_{\tau_{1,k}} g(X_{\tau_{1,k}}) | \mathcal{F}_t]. $$ 

It turns out that to find a whole slew of results of this shape I needed to know that (a) this sort of statement is called "strong invariance principle" more often than "strong approximation"; (b) the result as requested above is the subject of the book by Walter Philipp and William Stout, Almost sure invariance principles for partial sums of weakly dependent random variables, Memoirs of the AMS vol. 2 no. 161, July 1975. 

Let $G$ be a directed graph on $n$ vertices. Let $H_1$, ..., $H_k$ be marked subgraphs of $G$. (Specifically, each $H_i$ consists of a subset of the vertices of $G$ and a subset of the edges of the induced subgraph.) I am interested in an algorithm for finding the maximum number of edge-disjoint paths from $s \in V$ to $t \in V$, subject to the constraint that none of the $H_i$ is entirely contained in the union of the paths. Without the constraint, this is a standard max flow problem. With the constraint, one could modify the Ford-Fulkerson algorithm to conform to the constraint at all steps and give a lower bound, but there's no reason to expect it to be tight. I am primarily interested in the setting where $|H_i| = O(1)$ and $1 \ll k \ll n$. Ideally, of course, I would like an algorithm that's polynomial in all of $n$, $k$, and $\max_i |H_i|$. I am willing to make some assumptions about the $H_i$: one natural assumption is that each $H_i$ is either a totally disconnected set of vertices, or the entire induced subgraph. I am also willing to assume that $G$ is directed acyclic. I don't know whether either of those help. 

Is the distribution of the last time Brownian motion crosses a line y=a*x known? (Equivalently, the distribution of the last time a Brownian motion with downwards drift hits 0.) It's not hard to give bounds on it, but it would be nice to have an exact distribution if it's known. 

Let $X$ be a Markov chain, with countable state space $I$ and transition probability matrix $P$. $X$ is irreducible, but need not be recurrent. Let $S$ be a fixed subset of $I$. Define a subset $K$ of $I$ to be "nice" if there exists $\epsilon = \epsilon_K$ such that for all $k \in K$, $P_{kS} \geq \epsilon$. (Here, $P_{kS} = \sum_{s \in S} P_{ks}$.) Given: with probability 1, there exists a nice set which $X$ visits infinitely often. (Note that the set $K$, and therefore the value of $\epsilon_K$, may be random.) Want to show: with probability 1, $X$ visits $S$ infinitely often. It seems like it ought to be either trivially true or trivially false, but I'm failing to determine which... 

The answer is "no" even in the plane. Here's the intuition (which was also my search algorithm). Having an edge adjacent to every face is equivalent to an edge covering of the dual graph. A triangulation in the plane (strictly speaking, on the sphere) will have $E = 3/2 F$ and therefore $V=2+F/2$; this is the upper bound on the number of edges in $H$. So in the dual graph you're looking for an edge covering of size at most $2 + \tilde V/2$ (where $\tilde V = F$ is the number of vertices in the dual graph). Since the sum of the size of the minimum edge cover and the size of the maximum matching are equal to the number of vertices, to find a counterexample it suffices to find a 3-regular planar graph with small maximum matching size (less than $\tilde V/2 - 2$), and then take its dual. The graph in Figure 7 of Biedla, Demaine, Duncan, Fleischerd, Kobourove "Tight bounds on maximal and maximum matchings" (Discrete Mathematics 2004, vol. 285, p. 7-15, online at $URL$ is planar (uncross the crossings at the bottom), 3-regular, with 88 vertices and maximal matching of size 39. Its dual then is a triangulation that won't have a subgraph $H$ that you describe. 

Let $S_n = \sum_{i \leq n} X_i$ be the partial sums of a nice sequence of random variables $X_i$. In my application, $X_i$ is a functional of a finite-state, irreducible, aperiodic Markov chain, so the $X_i$ are as bounded and mixing as you like. (I'd prefer not to assume stationarity for the $X_i$; and of course $S_n$ aren't stationary anyway.) There are lots of inequalities for $S^*_N := \sup_{n \leq N} S_n - \mathbb{E} S_n$, since this is the sum of a nice mean-zero sequence. In my setting, these results give $S^*_N \leq C\sqrt{N}$. Unfortunately, in the application I need to work with $S_n^2$. Question: are there any non-asymptotic inequalities on $\mathbb{E}\sup_{n \leq N}\left|S_n^2 - \mathbb{E} S_n^2\right|$? I would like the answer to be $\leq C\sqrt{N}$ (for some constant $C$, which presumably depends on the moments of the $X_i$), for the following reasons: 

Let $X_i$ be a (finite-state, irreducible, aperiodic) Markov chain, not necessarily stationary. (That is, it doesn't start from the invariant distribution; I'm happy to have it be time-homogeneous if that helps.) Let $Y_i = f(X_i)$. Let $S_i = \sum_{s=1}^i Y_i$ be the partial sum sequence. Is it true that $$ \mathbb E_s [\max_{i \leq n} S_i^2] \leq C \mathbb E_s[S_n^2]? $$ Here $\mathbb E_s$ is the expectation given $X_0 = s$. The constant $C$ may potentially depend on the size of the state space, the mixing time of the chain, etc. (It's also possible that the correct functional dependence is actually more like $\leq C_1 \mathbb E_s S_n^2 + C_2$.) Basically, I'll take any sensible bound on the maximum of the partial sum sequence for a (functional of a) non-stationary Markov chain; I'm not after the optimal bound. A sub-question: there are lots of maximal inequalities for dependent sequences satisfying certain mixing conditions. (E.g.: Peligrad, Utev, and Wu, Proceedings of the AMS 2005; Rio, Journal of Theoretical Probability 2009; Merlevede and Peligrad, Annals of Probability 2013; the list is not meant to be in any way exhaustive.) Is there an easy way to convert a maximal inequality for a stationary mixing sequence to a maximal inequality for a (functional of a) non-stationary finite-state Markov chain? 

I am looking for a reference for a result of the following form: I have a sequence of discrete probability distributions, $p_N$, where the $N$th distribution has associated state space {$k/N, 1 \leq k \leq N$}. The value $Np_N(k)$ (probability of the state $k/N$, divided by the "size" of the state) is defined by a difference equation. I want to conclude that $N p_N(Nx) \to \pi(x)$ for some density $\pi$ on $[0,1]$, described by the "limiting" ODE. (Strictly speaking, I just need to show that the set of "discrete densities" has compact closure, i.e. that all subsequences have a convergent sub-subsequence; showing that the limit of a convergent subsequence is described by the ODE is easy.) Equivalently, I have a sequence of solutions $\tilde p_N$ (where $\tilde p_N$ corresponds to $Np_N$ in the previous paragraph) to (a sequence of) difference equations. I interpret the solutions as the values of the function $\tilde p_N$ at points $k/N, 1 \leq k \leq N$. The difference equations (and the initial conditions) "converge" to an ODE (with initial conditions), which has a unique solution. I would like to claim that the functions $\tilde p_N$ converge to this solution of the ODE. (Again, all I really need is that the functions $\tilde p_N$ converge somewhere, ideally somewhere absolutely continuous.) Just to clarify: I'm convinced the result is true, and has been proven a dozen times elsewhere; I'm hoping someone will tell me where. 

I would like a result along the following lines to be true, but haven't been able to locate it in the literature; pointers would be welcome. Let $X_t$ be a finite-state, irreducible, aperiodic Markov chain. Let $f$ be a bounded functional, which is mean-zero with respect to the stationary distribution of $X$. Let $$ S_t = \frac{1}{\sqrt{n}} \sum_{s \leq t} f(X_s), \quad t \leq n $$ be the partial sum process. Then there exists $\epsilon > 0$, $N_0 > 0$, and a Brownian motion $B_t$ such that, for all $n > N0$, $$ g(n) := sup_{t \leq n} |B_t - S_t| \leq n^{1/2 - \epsilon}. $$ (Interpreted either as "$t$ is an integer less than $n$" or as "interpolate $S_t$ linearly between integers".) In the case of partial sums of iid random variables, if I remember correctly, the Skorohod embedding gives $g(n) \sim n^{1/4}log(n)$, and the KMT approximation gives $g(n) \sim log(n)$. In the Markovian setting, I'm hoping for $g(n)$ to depend on the mixing time of the Markov chain. It's possible that the correct generality for this is "ergodic sequence" rather than "Markov chain". 

Is it easy to write down the large deviations rate for the maximizer of a random walk with negative drift? Let $X_i$ be the (iid, mean $-\mu$, variance $\sigma$, arbitrarily nice tails) jumps of a random walk $S_i$. I am interested in the location of the maximum of $S_i$, i.e. in $\arg\max_k S_k$. (If $X_i$ are continuous, the maximizer is almost surely unique.) There's a trivial exponential upper bound $$ \mathbb{P}(\arg\max_k(S_k) > n) \leq \mathbb{P}(S_{n+1} > 0) = \mathbb{P}(\sum_{i=1}^n X_i > 0) $$ and the trivial exponential lower bound $$ \mathbb{P}(\arg\max_k(S_k) > n) \geq \mathbb{P}(X_i > 0\ \forall i \leq n) $$ and they (of course) don't match. Is there a limit $$ \lim_{n \to \infty}n^{-1}\log\mathbb{P}(\arg\max_k(S_k) > n), $$ and is it possible to write it down in terms of the distribution of the jumps $X_i$? 

Suppose I have a family of countable state-space, discrete-time Markov chains, indexed by a parameter $r \in \mathbb{R}$. The state space is the same for all values of $r$; the transition probabilities depend on $r$ continuously. Is there some result like: If the chain obtained for $r=r_0$ is positive recurrent, then the same is true for all values of $r$ in some open neighborhood of $r_0$; and as $r \to r_0$, the associated invariant distributions converge. (I assume that if the first statement is true, then so is the second one.) In terms of simple linear algebra: I have a countable set of linear equations (defining the invariant distribution in terms of the transition probabilities of the Markov chain), which depend continuously on $r$. At some value $r=r_0$ the system has a unique positive solution. Is there a neighborhood of $r_0$ for which this will still be true? (What conditions do I need to impose on my set of equations to get this?) 

$M = \Theta(\log n / \log\log n)$ with high probability and in expectation. An alternative way of thinking about this problem is that you're tossing $n$ balls uniformly at random into $n$ bins and looking at the size of the largest bin. In this formulation, this model is used in computer science to think about load-balancing; Gonnet 1981 (Expected length of the longest probe sequence in hash code searching, Journal of the ACM 28(2): 289-304, $URL$ has a sketch of a proof. Alternatively, these lecture notes $URL$ work through the argument more carefully, justifying why the Poisson approximation can be used there. As an aside, if instead of picking your object uniformly at random you first eyeball two objects (uniformly at random), and then actually pick the less popular one of those, the maximum will drop down to $\log\log(n) / \log(2) + O(1)$. This is known as the "power of two choices" and makes for a nice term to Google. 

Consider a random walk on $\mathbb{Z}$ with triangular drift and jumps that are standard normals. That is, $$ \begin{cases} RW_{t+1} = RW_t - d + \epsilon_t, \quad t \geq 0,\\ RW_{t-1} = RW_t - d + \epsilon_{t-1}, \quad t \leq 0, \end{cases} $$ where $\epsilon_t$ are iid standard normals. Using Skorohod embedding, we can think of $W_t$ as the values of a Brownian motion with triangular drift at integer time points: $$ RW_t = B_t - d |t|, \quad \text{for $t \in \mathbb{Z}$} $$ where $B_t$ is a standard Brownian motion (and equality is in distribution). This tells me that $$ \max_{t \in \mathbb{Z}} RW_t \leq \max_{t \in \mathbb{R}} (B_t - d|t|) $$ (in the sense of stochastic domination), and the difference between them is not too large (~max of a Brownian bridge with different endpoints?). Question: is something similar true for $\arg\max$, the location of the maximum? That is, is it true that $$ \arg\max_{t \in \mathbb{Z}} RW_t \quad \text{ is not too far from } \quad \arg\max_{t \in \mathbb{R}} (B_t - d|t|) ? $$ (Stochastic domination seems unlikely.) Update: as Martin and Ofer point out, on a fixed sample path the maximizers can be arbitrarily far from each other. Nonetheless, is there a true statement of the form $$ \arg\max_{t \in \mathbb{Z}} RW_t \leq 2 \cdot \arg\max_{t \in \mathbb{R}} (B_t - d|t|) + 3, $$ for some values of 2 and 3? (In the sense of stochastic domination, of course.) 

If the $X_i$ are iid, then $S_n^2 - \mathbb{E} S_n^2$ is a martingale, and Doob's maximal inequality gives $\leq C\sqrt{N}$. Functional central limit theorem, as well as various stronger invariance principles, holds in this setting. All of these suggest that the largest deviation of $S_n^2$ from its expectation for $n \leq N$ should be $O(\sqrt{N})$.