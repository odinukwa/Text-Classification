As others have already pointed out the info you want is in . However if you want to pull from both DMVs this would work: 

In most cases where you hit this issue you will just need to follow the manual uninstall steps Microsoft provides here. That will, in cases I have had to, get the system to a point where you can successful reinstall SQL Server to a working state. 

You can utilize the trace catalog views in SQL Server to get information about a given trace. Just pass in the ID of the trace you are interested in, code below pulls trace ID of , which is the default trace. 

If it fails it will show a similar message just with a failure and should provide an error code. If not check your Windows Event log for similar events around startup time of SQL Server. To fix the issue you need to follow the KB article you provided. It is not old and is still current, check the "Applies to" section where it list all the editions of SQL Server 2012. 

I cannot provide a full example right now but I would recommend looking at two modules that can help you build out the desired Excel file much easier. 

EDIT As pointed out you can utilize the Attention event in profiler with the events for capturing the T-SQL statements. It does not necessarily specifically state what the attention event is when I tested it so I guess the fact that it follows the event sequence you can estimate those queries that have an issue. I did not get a chance to full test it out with code and all. I did however come across an exact example with Extended Events that can be used to find timeout queries, and this example is with SQL Server 2008. It is from Jonathan Kehayias: An XEvent a Day (9 of 31) – Targets Week – pair_matching 

In order to clean up files in Azure blob storage requires PowerShell or going through some app like Azure Explorer. So it is likely that Ola's backup script does not support it since there is no native T-SQL command or procedure to access blob storage in that manner. When you are deleting files you will need to account for any that may still have an active lease. BOL has code to handle dropping the active leases. 

I would start looking at your monitoring tools as the culprit. However being that your data from Paul's script shows average wait is low I would not put to much concern into it right now. Your main challenge here that I think you may be overlooking is your user's told you there was a slow down in the system/application. Most will always point to the database/server as the culprit. My general next question when someone tells me that is "what portion of the system is slow". If they tell me things like pages loading slowly, and they are not actually executing any ad-hoc reports or queries, I ask them to check with the server folks or application admins to ensure nothing else is messed up. Then while they are doing that I will go check the database server performance. However as you have shown if the XE wait type is the only one above 10% or so I don't think the issue is with the database server. If you want to find the culprit of that wait type you can begin querying , this will show you the and wait that is occurring. You can also use sp_WhoIsActive to get similar information a bit more easily. I would bet you are going to find it to be sessions associated with your monitoring tool. 

What do I have wrong in the expression for the ? I can obviously evaluate the expression successfully in the properties window, and understand that is because the variables are not populated. So how do you configure that were it will not truncate no matter what error is passed in, just set it to some large number length? I am not looking for some custom method or code to perform this as the package is fairly simple and is only run every blue moon. However, would it be best to simply create an event for each task? As I understand it any child task that fails has the error information passed up to the parent package, so it should work right? 

Have you considered looking at the minimal logging features available in SQL Server 2008? New update on minimal logging for SQL Server 2008. The author also has a few blog post on examples using this feature here, here, and here. it mentions use of a trace flag that is available to help limit the log use. 

Similar Grant Fritchey had the issue where he had closed SSMS and lost the query he had been working on...blogged about here: Oh **********! EDIT To make this a bit more detail of an answer, the referenced linked above Grant provides a query to simply go to the cache on the instance to pull out the query you had just executed (or atleast attempt to): 

I moved the script from that blog post to my GitHub repository and you can download it from here, have blog post scheduled for this morning with the link as well. 

However also noting that the error message , the 5 normally indicates permissions issue with the service account running the database engine. If the full message was provided you would see something like 

This lock type is commonly seen with deadlock queries that SQL Server has executed as parallel, sometimes referred to as "intra-query parallel deadlocks". I have seen a few statements that this also points out system resources are low, which I guess could be involved to a small degree. A general guideline that I have noticed to determine if it is parallel deadlock is when you pull the XML deadlock graph (which can be done with the system_health session in 2008 and higher) you will notice different process IDs showing the same bit of code within the execution stack. As well, looking at the resource list of the deadlock graph and noting the type of waiter event. They will most commonly show "e_xxxxxx", or something like this maybe: 

Now, with using you can actually cause scripts to receive a prompt when executing the PowerShell script. This may be what is actually generating the error because SQL Agent cannot respond to the prompt or does not know how to handle it. There is really no reason to use that policy setting as is a sufficient policy to allow scripts you wrote and setup on the server to execute without being prompted. I would except if you dug into the full error being returned it might contain text similar to the below message. This is the prompt you can receive when setting the execution policy to : 

The account will show up as just when you are looking at the local accounts via the UI (I recall). I'm not sure why you see one with the hostname but that would not be a system account on the local machine. You can always just use T-SQL to add the login: 

You run a server-side trace or Extended Event session to capture login activity. I also would not have just renamed it in production. If you are referring to the account itself, disable it first to verify if anything breaks; and leave that disabled for a day or up to a week. How long you leave it is based on knowledge by the application owner. If they are not confident leave it disabled for at least a month. If you are renaming just any old login to something else for a purpose, you obviously cannot go with the disable route. In that case you just have to let your trace run for a period of time to capture the activity. One other option you might try is to just put in a server trigger that just writes an audit of login activity. There are a few examples of this on the web. 

I ended up changing around how I am going to use SSDT with this database project. In the end I will be fixing all the issues with case sensitivity on the stored procedures. The caveat to my project is I do not know all the objects that are actually being used in the application, so I only care about those objects actually being used. I am simply starting with an empty database project and adding in the objects the developers are going to be using in the application as they redesign it. I should then have a database project with "clean" objects in order to do a schema compare.