Maintenance plans will do just fine most of the time. Ola Hallengren's scripts will do just fine most of the time. In very rare cases, you might have to grow your own. As Jyao said, it comes down to which you are most comfortable working with. If your co-worker is most comfortable with maintenance plans, why get your knickers in a twist? If he's been databasing for 20+ years, he's already written his own maintenance scripts. Right about the time you were learning to drive, there was probably some young punk in cargo shorts and fliplops that came along and was all like "hey, you old codger, maintenance plans are better - and pull your pants down, you look ridiculous!". Then there was probably a 4 year battle where the uppity youngster slipped in a maintenance plan whenever he could. Now this other young punk with skinny jeans and a freakin bow tie is telling him to go back to scripts. It's enough to turn your hair gray. Three things to consider: Are these examples of Hallengrenite superiority actually applicable to your environment? Is it going to cause you an actual problem if he uses maintenance plans? If you convince him to use Hallengren's scripts and there's an issue, will he be able to resolve it himself or will he have to call you? 

We are building a new server setup following this guide: $URL$ And it reccomends 7x 960GB SSD's in Raid 5. Question: If the databases are on the SAN, what is all this extra space for? 

How often do I need to run it? It needs to run at the same interval that I keep my backups, right? If I keep the backups for 1 day, I need to run it every day? Can I restore the databases to my test environment and checkDB there? If I've got lots of sql servers, but they all use the same SAN, am I going to want to stagger my CheckDB's? 

Here's how you can join a table to itself randomly and update a column. This method will also maintain the distribution of your data: 

3. Set up a .dtsx package: You can do it directly in SSIS, but I find it easier to use the SQL Server Import and Export Wizard by right click on the database, hitting Tasks and Export Data. Set the Data source to by SQL Server Native Client, set up your server and database, then hit Next. Set the Destination to be Microsoft Excel, and the excel file path to your shared folder (not your blank template): 

I've got a cursor sending out pager messages, and occasionally it sends out duplicates. The syntax looks like this: 

You can also do it using a date table and ordering it by newid(). I've used this technique to scramble lots and lots of data in the past. One advantage is that you can scramble any field by joining the table to itself on Note: if your person table is bigger than your date table, in this example, loop the date table insert a few times until it is bigger. 

Here's the James May method. Looking for feedback & ways to do it smarter. 1. Write your query in SSMS. To get dynamic date ranges that always fall to the first of the month, I do it this way (there is almost certainly an easier way): 

It turns a list like this: Albuterol Sulfate Amlodipine Besylate Aspirin Benztropine Mesylate Bisacodyl Ciprofloxacin Collagenase Divalproex Sodium ... into a string like this: Divalproex Sodium, Collagenase, Ciprofloxacin, Bisacodyl, Benztropine Mesylate, Aspirin, Amlodipine Besylate, Albuterol Sulfate I can write it as a dynamic stored procedure, but you can't call sp_executesql from a function (or at least, I don't know how). Question How would you write this function in a way that it could be used on any table? 

I do it more or less the same way as you, and I don't get duplicate messages. How are you actually sending the messages out? If you're doing them in batches and getting duplicates, then my guess would be that one batch is starting before the previous batch has been written to the SentMessages table. To get around this, I'd start with smaller batches - or slow down the frequency of the job, so that the last batch has been written as sent before the next batch starts. If that won't work, I'd put it in a cursor that looks something like: 

I've got a query that checks for new patients in the ER, and if they have been there before in the last 72 hours, it notifies the killbo... uhhh... care managers. I could set this up as a trigger, but I don't think this is a good idea. What I want to do instead is set it up as a job that runs every 5 minutes, with this in the where clause: 

One thing to experiment with is changing your clustered indexes - in a lot of ways they aren't really indexes at all, they are the order in which the data is stored. make copies of your current clustered indexes first, then try adding the columns in the joins & where clause to the clustered index. Clustered indexes are often the same as primary keys, but they don't have to be. your primary key and clustered index can be completely different. Another thing - the - do you need every column from that table? If you only take the columns you need, it might help. Then, there must be a way to stop a query if a website user bails... Other things you can do - archive some data. I'll bet some of your tables are pretty big. Do you need all the data there? With Enterprise edition, you can also partition tables. Check for locking - if you have two users putting this query to your DB at the same time, are they blocking each other? I'm going to catch some shit for suggesting this, but you could join with (NOLOCK) and see if that helps. Oh, and when did you last update your statistics? The query tuning videos on Brent's site are really good, if you need more detail. 

No, the schema is the user in oracle. In other databases, where the schema and user are actually different things (and appropriately so), then you could have a schema name that matches the user name but where the user does not have rights to the objects under that schema. 

In psql you can to make psql show you the queries used to generate the output of the commands. I've found these queries to be very useful as a starting point when digging metadata out of databases. 

For Postgresql on MS Windows, the Postgresql wiki has an "Automated Backup on Windows" article that may be of use. There is also "Instant PostgreSQL Backup and Restore How-to" available from Packt that provides a good overview of the various options available using the tools that come with Postgresql. There is nothing in the booklet that can't be found on the web but they do a nice job of pulling that all together in one document and the price is very reasonable. 

We have a set of spatial queries that are failing and I'm struggling with troubleshooting them. I suspect that we're running into some bug, but I'd like to nail things down a bit better so as to be sure and also so that the resulting bug report is both useful and directed to the correct party (postgresql, postgis, or other). Does anyone have any recommendations for next steps? Given: PostgreSQL v9.1 installed from $URL$ on fully a patched 32-bit install of CentOS 5 

It depends. They all speak different dialects of SQL. Not having any details to work with I would recommend Postgresql though. 

Which is what we would expect-- the $PGDATA/base/83637 directory is gone, there should be nothing to vacuum. Are you sure there isn't something else eating up your disk space? One of your other databases? log files? Something that you could try would be to: 

returns the hoped for count of 0. The thing I like about this method is that it only needs to read each table once vs. reading each table twice when using EXISTS. Additionally, this should work for any database that supports full outer joins (not just Postgresql). I generally discourage use of the USING clause but here is one situation where I believe it to be the better approach. 

If your problem is file fragmentation then no, there isn't. In Postgres each table gets it's own file, or set of files if it uses TOAST, in the file system. This differs from, say, Oracle (or apparently MS-SQL) where you create pre-sized tablespace files to drop your tables into-- although even there you could have file system fragmentation issues if the tablespace files get extended or the file system is badly fragmented to start with. As to your second question... I have no idea how to would cleanly deal with the file system fragmentation as MS-Windows is the only OS where I've experienced fragmentation issues and I don't run MS-Windows any more than absolutely need be these days. Perhaps placing the database files on their own disk(s) could mitigate that to some extent. 

Have you looked at SQLite? It is a widely used, light-weight database which has C/C++ bindings and supports in memory databases. 

Oracle has a free tool SQL Developer that has can presumably reverse engineer an Oracle database. If the database isn't too large then you could possibly use a tool like DbVisualizer (which is either free or low cost depending on the version). You can only diagram one schema at a time and results aren't directly editible (it's really more of an exploration tool) but you can save the diagram as GML and edit it with a tool such as yEd (which is a free download) (Note that you will need to edit the gml file slightly before bringing it into yEd by replacing all instances of 'customconfiguration "SimpleRectangle"' with 'type "rectangle"'). Note that SQL Developer, DbVisualizer, and yEd are all cross platform tools so you can use them on any system that has Java installed. Update -- I just tried reverse engineering (154 tables) using SQL Developer. It appears to work reasonably well but it isn't going to win any beauty contests... 

In addition to the other comments... If/when you have a database where any given table can be updated by one or more applications or code paths then placing the appropriate constraints in the database means that your applications won't be duplicating the "same" constraint code. This benefits you by simplifying maintenance (reducing the number of places to change if/when there is a data model change) and ensures that the constraints are consistently applied regardless of the application updating the data. 

Which is what we see ($PGDATA/base/83637 being the subdirectory for the new database). Dropping that database should also delete the data files: 

No, that simply enables the use of SSL. You need to also make the appropriate changes to your pg_hga.conf file. 

Once you've learned Postgresql then pick another RDBMS and learn their SQL dialect-- compare and contrast. Rinse and repeat. 

My understanding is that when you drop a database then it, and it's files, are gone. Unless you are using tablespaces then each database should have it's data in it's own subdirectory under $PGDATA/base. Using one of my servers for an example (as the postgres user):