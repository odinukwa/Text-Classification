I'm running the official GlusterFS 3.5 packages on an Ubuntu 12.04 box that is acting as both, client and server, and everything seems to be working fine, except mounting the GlusterFS volumes at boot time. This is what I see in the log files: 

I know this used to be a bug in GlusterFS 3.2 for Ubuntu, but I understand it was solved in the PPA packages for GlusterFS 3.4 as shown here: $URL$ I also remember this working in an experiment I run with some virtual machines (but since it just working, I didn't look too deep into it). I see that the gluster-client packages provides an upstart job called mounting-glusterfs.conf which contains: 

But I'm not so sure how it should work. It doesn't seem to be working out of the box. Even though mounting of glusterfs volumes happens after the network starts, it happens before GlusterFS starts: 

How can it be 11G? What's going on? Just rounding up? but then why does it fail to fit on the other machine? 

I'm really confused by this. To do some experiments with GlusterFS I created two virtual machines with VirtualBox, running Ubuntu 12.04, each with 10GB of storage. I wrote a script that created lots of little files in lots of folders. Each file was 100k of random data generated by: 

and it did create /var/lib/bacula/bacula.sql, but when I run the job it gets deleted. Any ideas what's going on? The whole output looks like this: 

When I rebooted the server, the volume wasn't mounted. /var/log/upstart/mount_public_uploads.log contains: 

I've just got a Windows Server 2008 which will work as a web server. It has two roles in the Server Manager, one is Application Server (which includes Web Server (IIS) in its Role Services) and the other role is Web Server (IIS). Do I need the Application Server role? Can I remove it? 

Is there any book you'd recommend to learn how to administrate a public-facing (IIS7, SQL Server 2008) Windows Server (2008 in this case)? I've searched for books in this matter and I've found "Windows Server 2008 Undercover", "Windows Server 2008 Unleashed", etc, which from the table of contents seems to be centered around the Windows server in a local network that is working as the PDC, or Active Directory server, or serving files, or some other tasks for local clients. The last of which I don't have any. I haven't used Windows in ages (not even in the client), so even basic things are puzzling for me at the moment. 

you can create more that 32k directories in a directory. The part I do not know (and I cannot find right now) is whether mounting it as ext4 but with less features enabled let's you create more than 32k directories. 

Given the fact the newest and biggest hard drives use SATA, you might want to get a SATA card. But if you don't need to use those drives, IDE or SCSI should be good enough, SCSI being better because it's more efficient and SCSI drives tend to be more reliable. P.S. SCSI drives aren't more reliable because the SCSI bus makes them so, but because manufactures tend to make them more reliable since they will be used in servers and cost more. 

You could try making it harder to reset the permissions of the root directory by making it append only with chattr: 

Short explanation of the AKW one-liner: is the PID, is the timestamp, is the remote port and is the remote address. The advantage of this solution is that root is not required if the server runs under the same user. The disadvantage is that all connections are counted, there's no filtering, so it won't work if the application listens on multiple ports. 

Remove from the boot command line and start in text mode (runlevel 3) to have a better idea of what's going on. Then consider reporting a bug. P.S. It seems that someone else has already reported this bug. 

I want to manage the mounted partitions from puppet which includes both modifying and creating the directories used as mount points. The resource type updates just fine, but using for creating the mount points is a bit tricky. For example, by default the owner of the directory is root and if the root (/) of the mounted partition has another owner, puppet will try to change it and I don't want this. I know that I can set the owner of that directory, but why should I care what's on the mounted partition? All I want to do is mount it. Is there a way to make puppet not to care about the permissions of the directory used as the mount point? This is what I'm using right now: 

You didn't specify the transport and the default transport is tls. The documentation regarding transports also says this about tls: 

That's the whole idea, to use systemd (or upstart etc) inside a container. Docker is used most of the time to run a single service per container, so for a complex site you would need a container for the web server and another one for the database server. With a process manager you could run both in the same container. Which approach is better is opinion-based. Though from what I've seen running systemd inside Docker isn't easy as of June 2014, for example there's bug #3629 - "running systemd inside docker arch container hangs or segfaults". For more details also read "Running systemd within a Docker Container". 

I'm trying out mod_spdy and I've run into a problem - it seems to be incompatible with AJAX requests and mod_php as in this: $URL$ The solution seems to be to run php scripts through fastCGI. Now my first question is, why is that? Is there maybe some sort of a workaround? Is this incompatibility just temporary? I would not want to switch the whole production server to fastCGI just because of this. What would be the advantages/disadvantages of that? Also, I do not understand why it needs https. Why can't a simple, let's say static, website gain speed from mod_spdy? I'm looking for plain speculation here - do you think mod_spdy will be sometime available without the mod_ssl requirement, or is the architecture so different that I should not expect it at any time? I hope I'm expressing my thoughts clearly. Thank you for your thoughts. 

both of the sites show Kloxo Default Page as an error. Both of the subdomains, when pinged ping arturas.duomenucentras.lt 

I'm quite a noob at configuring servers. Anyway, I'm running CentOS 5 and I've been meaning to install the php-xml package for my PHP 5.2.17. This server runs proprietary software which requires PHP 5.2 to be installed and not 5.3. Currently, I was using the atomic repo. But when i do a yum install php-xml it want to update all php packages to bleeding edge version of 5.3.6. I don't want that - I want to leave it as it is and install php-xml 5.2.17. So I've downloaded the php-xml package by itself and tried to install it when it threw an error saying: 

I'm trying to set up a server with Kloxo and two virtual hosts. They are pointed to the same IP, but a different directory. Here's the config of virtual host files. They're in different files, and included in the main httpd.conf file. NameVirtualHost *:80 is uncommented. arturas.conf: 

Any ideas? Edit: Ok, now it stopped saying ssl_error_rx_record_too_long and just says "Firefox can't establish a connection to the server at doo.". It loads up and works in Chrome, IE, Safari, Opera. Just not Firefox. What is up with that? 

Even though I checked and I do have php-common 5.2.17-1 installed like the rest of PHP packages. So I figure that I need a repository which delivers 5.2.17 php packages, is that right? Couldn't find it by myself. Thank you for any info on what should I do. 

I've got my regular wamp installation, and everything has been working fine with other vhosts, until I tried to make a vhost for DooPHP framework and I got an ssl_error_rx_record_too_long error on Firefox. It loads up fine in Chrome. I'm not using any SSL certs. My vhost configuration is just like all the rest of them: 

dislocate is a command that comes with Expect (at least on Fedora). Answer with when asked . You could also try using pexpect, a pure Python Expect-like module. Check out its hive.py and sshls.py examples. 

In case it matters, I'm using puppet-0.25.4-1.fc13.noarch.rpm and puppet-server-0.25.4-1.fc13.noarch.rpm. 

Use the EPEL (Extra Packages for Enterprise Linux) repository. The easiest way to enable it is by installing the package. Here's how if you have RHEL 5 x86_64: 

Adjust the and parameters as you like. The is needed so that every occurrence in a line gets replaced, not just the first one ( stands for global if I remember correctly). You can also pass a value to to make a backup. 

Enable the modules. I suggest starting with the corresponding file underneath the directory (I used ). Prepare the kernel for modules: 

Results for Scientific Linux 6.1 i386 I tested this on a KVM/qemu virtual machine running Scientific Linux 6.1 i386 (which is similar to RHEL). The following services were enabled: acpid, auditd, crond, network, postfix, rsyslog, sshd and udev-post. The swap is on a separate disk, so it's not taken into account. The stats for 85 boots, taken remotely with SSH a couple of seconds after the login prompt appeared, were: 

This could be caused by hard links which means that the files you deleted still exist under other names. To find them, run . 

This sound to me like a DoS attack, which means that you can't do anything except ignoring the attacker which you've already done. You might also want to ask your ISP to block him. As for tcpdump still seeing those packets, this is normal. They still exist on the network, but the kernel makes sure that a regular application doesn't see them. 

The manual for ST32000641AS (alternative link) says that the drive has 3,907,029,168 guaranteed sectors, while the specifications for WD2003FYYS (alternative link) say it has 3,907,029,168 sectors. Therefore the drives have the same capacity. 

P.S. works fine for owner, group and permissions, but not for SELinux. If the partitions are already mounted, puppet complains: 

Install one system, boot it and check out the block layer statistics from e.g. . Quoting from the documentation: 

How about a plain ? After all most devices are just a file located under . LE: You need to do the renaming every time you boot. 

Most Unix programs don't use locking or when they use it, it's not mandatory, so I doubt locking is stopping your log from growing. More likely the SCP transfer is slowing down the log writing. 

I have just tested an automated kickstart install (driven by cobbler) and it works fine for me. All I had to do is press Ctrl+Alt+F2 (virt-manager has a menu for this). The only problem is that the shell is not avaialble right away, you have to wait for the installer to reach a certain stage. Regarding debugging, you might find the Anaconda logging page helpful.