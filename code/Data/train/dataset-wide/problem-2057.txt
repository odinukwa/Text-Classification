Has anyone experienced behavior such as this, or know what could be causing index statistics to go awry so quickly across so many tables? Any recommendations for diagnosing next time this occurs? Manually running the stored procedures and diagnosing the execution plans have proved rather unhelpful as the statistics its reading (Expected/Actual Rows) are the same. 

The execution plan (PasteThePlan) is vastly different with this small change. It is tossing a warning that there is within the predicate. It was my assumption that would implicitly satisfy this argument as it does with . However, given that the operator returns each row from the initial input when there is a matching row in the second input, and because the warning exists, query optimizer assumes that each row is a matching row. Therefore all subsequent batch operations are ran against all 2048 rows in the table. What can I look into to better interpret what the execution plan is describing so that I can understand how to properly resolve the issue? Or, alternatively, am I simply missing the purpose of the operator when filtering result sets based on a static input? 

This returns: My clause clearly brings back only the rows that are explicitly , so it is not an issue of being an empty string and evaluating to . 

We have been experiencing an odd behavior in our application where various modules will begin to timeout in SQL Server 2012. Each time we stumble across this issue, we find that the statistics require update and that running fixes the issue. After updating index statistics, the timeouts go away. However, the frightening issue is the frequency in which we have been experiencing the need to update the statistics, and the fact that nearly all of the statistics are showing a need to be updated across all of our tables related to order processing. Due to the frequency, we have setup a job to run prior to business hours at 7:30 AM. This seemed to have calmed the issue while we continued to investigate until it occurred again today. Not 10 minutes after business opening, they were receiving timeouts. I immediately ran and the timeouts disappeared and application function returned to normal. The order volume since business opening was low (less than 20 rows added to the primary order table), yet the statistics became so bad between 7:30 AM and 8:10 AM that we began experiencing time outs. A couple of notes: 

You can consider the web and mobile as just delivery interfaces to the functionality/services for your application. With this regard the backend functionality (server side - processing etc), should be the same for both interfaces, so yes they should have the same database. You may want to add data to identify the sources of data addition or changes or traffic, but essentially its the same app. UPDATE: Think of a mobile app just like a separate UI to be supported, I am not going to exhaust the possibilities but look at potential list of possible interfaces below: 

Add a movie genres table - as a movie can have one or more genres Split up the movie_showing table as follows: 

MySQL Workbench does not have the capability to "infer" the relationships from table column names, or MyISAM tables. It requires that the constraints be defined in the database or SQL script being reverse engineered. 

I would also recommend MySQL dump with an additional twist, place the dump file under version control, at regular intervals, I would recommend when a change is complete. That way you can also track the changes to the database and make reconstruction easier 

How about making each of the elements of your XML a table or child-table and using in-built stored procedures to generate the XML from an SQL query result. This provides you with the data consistency and validation of the relational model plus additional database functionality with the ability to generate your XML document on the fly. If you need the XML documents handy you can create the equivalent of a "reporting" table into which the XML is stored ... 

I have a question about whether or not it is necessary to back the tail log up when performing a restore of a database in Full Recovery mode. I've sprawled for a good while now, and have gotten conflicting reports. Consider this scenario in SQL Server 2012: 

I have an issue where within my SQL Server 2016 Standard instance, is set to but within the file, it is capturing . Once I observed the behavior, I thought that the reason for this was that I had an trigger enabled. This trigger was created to prevent usage of a SQL Server credential when connecting with SQL Server Management Studio that originates from a that has not been delegated access. This was added to allow access from specific terminals outside of the domain (but within the same network) that the SQL Server is hosted on. This is the trigger: 

Nothing with the server setup has changed since the migration. We are running two servers, a PRIMARY and REPLICA within an AlwaysOn Availability Group. The VMs are housed on Azure and have 8 logical processors, 28GB of memory, and all SSD drives. The data file layout looks like: 

I am attempting to update a query that utilizes the operator within a clause predicate with to compare potential performance improvements and better understand what is happening behind the scenes when the two are interchanged. It is my understanding that in practice, the query optimizer treats and the same way whenever it can. I'm noticing that when the query is ran with the operator, it returns the desired result set. However, when I replace it with the equivalent, it pulls in all values from the primary table I want to filter. It is ignoring the provided input values passed to and returning all possible distinct values. The use case is relatively straightforward: the query accepts a pipe delimited string that can contain up to 4 values, e.g. or . From here I the input into a table variable to determine the corresponding internal for the . The result set returned is then filtered to exclude the that were not passed. To illustrate, here is a similar table definition: 

First of all since you are using MySQL, go for InnoDB which allows you to create relationships between the different tables (not dbs) You need three tables: 

So in maintenance mode all we need to do is apply the changerequest.sql script during deployment to production 

a) categories (db3) - I think you will use this for the tag cloud b) products (db1) - name and details of each product c) product_categories (db2) - a single row for each product and each category 

We maintain database scripts as part of our application codebase which is maintained under version control. However we use different "processes" for development and production code Development we maintain the following scripts: 

You can try Adminer ($URL$ which supports MySQL, PostgreSQL, SQLite, MS SQL, Oracle An additional beauty with this is that its a single PHP file that is deployed to your server 

Do you have an index on account_id? The second problem may be with the nested sub-queries which have terrible performance in 5.0. GROUP BY with a having clause is faster than DISTINCT. What are you trying to do which may be better done through joins in addition to Item #3? 

Go for the latest version of the MySQL Server that you can get, there have been some pretty impressive work done on 5.6 although the production ready version is 5.5 Go for InnoDB - comes as the default for the higher MySQL versions Configure InnoDB as follows: