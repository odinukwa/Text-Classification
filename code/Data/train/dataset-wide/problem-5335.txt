After a long discussion with the author of the question I am going to section off this answer into two parts. The first part deals with the answers to the three questions posed in the body of the question. The second part deals with the answer to the title of the question, which is "Does mathematical logic totally ignore context?" 1 In regards to the first question which asks (paraphrasing based off of our conversation) “In order to answer 'does mathematical logic totally ignore context?', do we need to have a definition of 'context in natural deductive systems?'” The answer to this question is yes, we need to have a definition of context before we can ask if mathematical logic ignores context. In regards to the second question which asks (paraphrasing based off of our conversation) "If the answer to (1) is yes, then what we mean by "context of natural deductive systems" corresponds to "context of statements in the English language? Otherwise logic doesn't capture what human reasoning is." The answer to this question is yes but it is a subtle yes and it only applies to when we are expressing the formulas from our logical system in English. When we make a declarative statement in English we are making some sort of factual claim. However, this may be in reference to many different contexts. This could be in context to the real world, it could be in context to some fictional world from a movie or a book, or it could be in reference to some hypothetical alternate history. So, this clearly shows that sometimes statements in English are true and sometimes they are not, depending on the context we put them in. If we say "he shot the butler" and we are referring to some film wherein a character shoots a butler then that statement is correct. However, if we are referring to the real world and "he" denotes someone who did not shoot a butler, then that statement is false. This is the exact idea of a domain of discourse which I define below. This is where mathematical logic gets its concept of "context" or "meaning." The "context" of a natural deductive system, just like English semantics, changes depending on how we use it. In English it refers to what we are talking about (our planet, some other alternate history, the world in a book, etc.) and in mathematical logic it refers to the model (which part 2. of my answer defines extensively). In regards to the third question which asks (paraphrasing based off of our conversation) "If we assume that the intended definition of context as has been stated in the beginning of the question isn't acceptable (i.e., the meaning of a statement is "different" from its context) then how is it true that natural deduction systems don't ignore context and in what sense?" First, we'll define what meaning and what context have to do with this situation. The "meaning" of a statement in this context refers to its semantic content. In terms of mathematical logic, this means the model theoretic properties of the particular statement. "Context" refers to what specific domain of discourse we are discussing. The context is the actual domain (the real world, or a book, etc..) and the meaning is whatever factual propositions are being asserted about this world. This leads directly into part 2 of my answer which shows that deductive systems on their own do not deal with meaning. In just the same way, English sentences alone do not deal with meaning. If no world exists, if we have no domain of discourse, then the sentence "The cow is happy" has no truth value, because it doesn't refer to anything. Model theory, in mathematical logic, is what gives natural deduction something to refer to. To reiterate a very clear and concise answer to (3): By itself natural deduction does not refer to specific context or meaning, because it is only syntactic in nature and refers to just the deductive processes. The "natural" part of "natural deduction" does not mean "referring to the natural world" (nor does it refer to "natural languages" which may be where some of your confusion regarding English semantics is coming from). "Natural" was used because the inventors of the theory wanted the deductive process to feel more like what the human mind experiences when it does deductions. This says nothing of the context the deductions take place in. To understand the difference between purely syntactical (proof theoretic) ideas and semantic (meaning and context, model theory) ideas, keep reading on with section 2. Only looking at proof theory gives you only half the story. 2 Now we can move on to answering the titular question: The definition of "context" in mathematical logic is given by the domain of discourse that the formulas range over. The definition given on wikipedia is as follows: 

The Problem of Induction does represent a problem for the philosophy that is often referred to as science. That empirical reasoning is not "proven" may or may not be a problem for you personally. Empiricism has lead to knowledge that is closer to true then what we had before. So empiricism is useful as the computer you are using to view this demonstrates. Is our current understanding of empiricism "true"? Almost certainly not. That is part of the philosophy of the sciences. The assumption that our current understanding is flawed in some way. This encourages people to suggest improvements. As a result our understanding improves over time. So no I do not have an issue with the problem of induction, it only represent another area where our understanding could improve. That science currently has flaws in its understanding of things is a strength, rather than a weakness. 

The many-worlds interpretation says that there is no randomness, per se, of the outcome of wave function collapse. It says that there are many parallel universes that exist and each time a measurement is made the outcomes are all experienced by a different world. The reason we do not experience all of the outcomes is because our conscious experience is limited to one world, the world in which we exist. The answer as to "how should we interpret the probability amplitude" is not yet agreed upon. There exist many different interpretations and all of them provide different answers. The issue is that, for the most part it is believed, there is absolutely no experiment we can do that will prove one or the other. We are not able to access the parallel worlds from the many-world interpretation so we will never be able to verify that the collapses are not actually random. This means that there will be no way for us to assure that the ontological probabilities of the Copenhagen interpretation are incorrect. Much of the literature of the philosophy of physics is devoted to these questions. A Snapshot of Foundational Attitudes Toward Quantum Mechanics is a very interesting paper describing the current consensus and divides on this question in the current physics community. The results of their polling shows that the majority of modern physicists believe that randomness is inherent in physics and that the Copenhagen is the most believed interpretation. It is conjectured that this is due to that interpretation being the "traditional" or "canonical" interpretation and thus most physicists learned it as the default idea from their professors. That being said, most of the most prominent physicists push for the many-worlds interpretations (Lenny Susskind has a somewhat technical paper about why they might both be correct). Now, on to complexity. Scott Aaronson is generally regarded as the most authoritative voice on quantum computational complexity, at least as far as theory goes. He has also written extensively on the philosophical side of quantum computation and what philosophers, physicists, and computer scientists should all focus on in regards to these questions. His work, along with many, many others, has shown that quantum computation is theoretically a well founded idea. However, we have not yet built fully functional quantum computers. So far, there have been no physical results that prove that quantum computation, and therefore everything predicted by quantum complexity theory, cannot be physically realized. The largest problem we have faced is the problem of decoherence, which is the tendency for quantum objects to interact destructively with their environment. Decoherence is one of the main research topics in quantum complexity theory and it is the main source of trouble for our current design of actual quantum computers. The way that computer scientists and physicists deal docoherence is by what are called quantum error correction. This is a process whereby quantum objects are allowed to remain coherent by introducing new objects into the environment that take the decoherence onto themselves and then are removed. It is an open question as to whether or not nature experiences quantum error correct by itself, however many specialists believe that it does. So, from a philosophical perspective, some people remain skeptical that we will ever be able to have fully functional quantum computers. Others, however, are fully committed to the idea and believe that they are just around the corner. Further study into the true ontological nature of probability amplitudes will surely shed light on some of the questions the field faces, such as the true nature of a measurement. Where we stand, however, is that quantum complexity surely exists epistemologically. We have (almost) all of the math formulated. What remains are the actual machines that prove the mathematics is correct. As for the Doctor Who question, it seems unlikely that a being such as that would exist, because if it was a being that had consciousness it would be able to experience itself and therefore would always be able to exist. The thought process behind the show is probably more sympathetic to interpretations that rely on the human mind playing a conscious role in quantum mechanics. This interpretation is generally disfavored and discredited by most working physicists and a lot of philosophers, however it does have its proponents. 

Epistemology is the area a philosophy that considers how we know what we know. Although, it is true that Empiricism is a tempting theory for understanding truth, it is limited to the objective and measurable. I believe in justice, though I am not familiar with any units of measure that would apply. I believe there is beauty, though I do not believe that it is objective. At some point what you believe is an element of your personal philosophy. You can believe what you want, but if you want others to agree, you need to either stick to the objective, or work on your ability to discuss and persuade. 

That depends on how one defines "the universe". Assuming you include yourself as part of the Universe and the Universe also include everything that exists, then any free will you have is "completely governed by the universe". 

Let's start from the bottom and use the example of changes to a property. Chemistry is said to supervene on physics because any change to physics will alter chemistry. By "a change to physics" we mean some rule such as "quarks no longer stick together by gluons to form protons." If this were to happen protons could no longer form. Chemistry is the study of the interaction between atoms and atoms are made of protons and electrons (and neutrons). If protons could not exist atoms would not form and the laws of chemistry would change. Similarly, think of how that would effect the other fields up the chain. If chemical reactions ceased to happen, biology would not exist either. If there were no biological organisms we would not have the means for psychology to exist. One puzzling thing that mereology considers is the problem with vagueness. It is well documented that natural language runs into conceptual problems with vagueness. Consider the paradox of the heap: 

I will assume the current psychological definition of a self. There are many other definitions, if you meant one of those please let us know. An additional assumption I am considering a healthy individual, so we will not be talking about whether multiple personalities constitute different "self" or not. Further, there is an assumption that the self is a single entity in your question so I will not be talking about conversations between say the ego and the id. Given those constraints, I agree with @Alex there is only one person present. As for the follow on question, why do it then? I expect that we do this because the act of speaking changes how we think about a topic. It requires more precision and effort. Helps us to clarify our thoughts. 

There is no universal answer to the question "Is it ethically wrong to kill an innocent person (or people) for the purpose of self-preservation?" The answer depends on which ethical framework you are using. As you point out Utilitarianism would ask which provides the greater utility. Whereas a Common Good ethical framework would ask which provides for the greatest common good? I can easily imagine cases where the preferable answer would go either way. 

Scientific Definition of Time Travel The scientific description of time travel is a phenomenon known as closed timelike curves. Closed timelike curves are paths that a particle can (more like might be able to but probably will not be able to in practice) take throughout spacetime that allow it to arrive back at its previous position in both time and space. The main reason that, under non extremal gravitational conditions, time travel is believed to be impossible is because in order for a particle to travel on a closed timelike curve it needs to travel faster than the speed of light in some reference frame. This goes against the theory of relativity (for more understanding of why the speed of light cannot be exceeded an understanding of Lorentz transformations is required). Now, there are certain solutions of Einstein's field equations that permit closed timelike curves. The most famous example is the one given by Kurt Gödel. However, it is generally agreed that the universe Gödel's solution describes is not the correct formulation of our actual universe. This paper by Al Momin explains why we should believe that and includes arguments on the philosophical reasons we should reject the model, not just the scientific (which might be closer to what you're asking for). Kip Thorne gives us a good explanation as to why this problem is so hard to solve for science: 

Assuming the Judeo-Christian God. 1) The modern Christian belief system asserts a "fallen" existence. Meaning that at one point everything was perfect, but things have become corrupt by the introduction of sin into the world. This is typically referred to as the fall. The high may be a result of this and therefore not part of the perfection that God intended. 2) The "high" itself is not the evil for most peoples perspective, rather the obsession with that high is the problem. This also may be a result of the fall as described above. 3) It is possible, even likely, that a benevolent God created materials which would result in pleasant feelings. Just as sexual intercourse results in pleasant feelings. Presumedly a benevolent God would create the means to have pleasant experiences as part of his creation. 

Before we discuss whether or not Socrates believes in a monotheistic or polytheistic god, we should state that it is clear is that Socrates and Plato rejected the Homeric image of the Greek pantheon. Of course, one of the main charges against Socrates was that he did not believe in the Gods of Athens and he preached heresy to the youth of the city. Book II of the Republic is devoted to Socrate's discussion that the real Gods must be just and honest and myths that portray them as being petty and dishonest are false and should not be taught to children. From the SEP: 

Formalism, as studied today, is slightly different in content than Hilbert's, as is outlined in the SEP article. However, at its core formalism is the view that mathematical and logical content is nothing more than the effects of specific syntactical rules, i.e. string manipulation. As such, formalism is considered to be a form of mathematical anti-realism. Logicism Formalism can be, for conceptual and historical reasons, compared and contrasted with logicism. Logicism is the philosophical view that mathematics can be reduced to logic, and therefore mathematics is merely just a larger part of logic as a whole. Historically, logicism was defending and elaborated on by Frege, Dedekind, Peano, and Russell, among others. Frege's goal for logicism was to create a purely logical system that could provide all of the truths about arithmetic. He failed to do so, as his system was subject to Russell's paradox. Russell attempted himself to create a system that avoided this and a few other paradoxes, his system of Principia Mathematica which laid the foundation for type theory. Kurt Gödel's metamathematical results about PM (and all other logical systems capable of Robinson arithmetic), his incompleteness theorems, seemed to many to be a final blow to logicism, in that no single logical system will ever be able to encompass all truths of mathematics and at the same time prove that itself is consistent. Some modern interest has been paid to a revival of logicism, called neo-logism, due to the work of Crispin Wright showing that Frege's original plan might be salvageable by replacing his Basic Law V (the cause of the failure of his system, besides the failure Gödel's results would imply) with Hume's Principle. Current criticism of neo-logicism is that it still fails to address some philosophical issues raised by logicism such as the Julius Caesar problem. Logicism may be neutral on the ontological status of mathematical objects. What is important to note is that logicists agree that logical truths are just that, truths. More specifically, they agree that logical truths are analytic truths, truths that are guaranteed to be true by the very nature of their definitions, and are knowable a priori, knowable without direct experiences. An important note is the disagreements Frege and Hilbert had about the role logic and axioms play in mathematics. This debate was centered around the proper role that axioms play in logical systems and how we should interpret them. As such, this debate exemplifies many of the tenants that contrast formalism with logicism. From the SEP: 

The definition of alive is often given as "not dead". Ironically, the definition of dead is often given as "not alive". I got those from a online definition search. Biologist struggle to define those things that are alive. Though most would agree that a squirrel is alive, there is some debate about a virus and a prion really pushes the envelope. The best definition I have found for what is alive is an entity that can acquire "food", metabolize and act. I put food in quotes on food as it may not be what you expect. Plants "eats" sunlight for instance. Given that definition a Human is dead when they can no longer do those things. 

I notice two things that run through your steps. The first it is brought up again and again that the method (tool) used to implement consciousness does not inherently matter, but I believe it does. The book is not conscience because it can take no action (it is inert). The computer may be conscience because it can take action. I believe the ability to take action is at least one requirement for being conscience. The second is that the steps accept input from an external source. Is a remotely controlled robot conscience? Is there a difference between the "lookup table" and a remote pilot to a robot? I doubt people would see a remotely controlled robot as conscience, rather they would look to the controller. The flaws come in part from a poorly defined term "conscience". First, let me offer that I believe you mean consciousness. Consciousness is the state or quality of awareness, or, of being aware of an external object or something within oneself. We sometimes say being self-aware. Implied in the definition is that this awareness is a self-contained action. It does not come by virtue of a look-up table or any external other input. I am not saying that it is not learned, just that if learned, it does not require input to be maintained. Assuming that you accept that definition then step three is not valid because the system is not self-contained. Finally, all of the above steps assume a computational model for consciousness. That is to say that consciousness can be mapped into specific steps and choices based on inputs. You have not demonstrated that this is true. The requirements for the tool to implement consciousness depend on the answer to this question. So steps 1,2,4 and 5 are ambiguous. 

Supervenience relates two things together in a way such that one of them is dependent on the other. A common example is 

The biggest trouble that you will face when thinking about these ideas is that you are mixing terminology from two distinct fields into one. Some of those words are used very differently in science than they are in mathematics. It can become even more confusing when you consider how much math is used in science, but an explanation of how the words are used in their specific contexts will help illuminate the delineating line. In mathematics, "thesis", "hypothesis", and "conjecture" are all used synonymously. From Wolfram's Mathworld: 

Nominalism is a school of thought that rejects platonism and as such it supplies arguments against Plato's Forms. Nominalists believe that there are no abstract objects (in this case the abstract objects being Plato's Forms). One of the most ubiquitous nominalist arguments against Plato's Forms is what is called the epistemological argument. In essence the epistemological argument against metaphysical platonism starts by raising this question: