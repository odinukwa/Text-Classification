This was related to two things: The web servers of the SharePoint farm had no external web access, so the root certificate revocation was failing (adding a delay of 15-20 seconds to paging and activity) - This was resolved by giving them access to the external web via a proxy. The issue with extra overheads when using SSRS integrated into Sharepoint is pretty well documented and still seems to be a problem - I guess that's part of why Microsoft are disabling the Add-In in the future. 

I've got a database which has an 850GB MDF file. Over the course of 20 months a logging mechanism in the application created a variety of huge tables. The maintenance script which was only supposed to retain a weeks data at any one time wasn't executing properly (it is now) so we had to manually clean things up. We actually cleared about 600GB worth of data. I want to free up some of this space as the drive it is on is near limit. The data file will never grow this large again so it's just wasted space. I was planning to use SHRINK-FILE to deal with this. I have tested on a clone of the database it takes around 3 hours. There doesn't seem to be any performance degradation on the database. Yeah, the indexes are heavily fragmented afterwards but I can sort those. My question is this: The application needs to remain 'up' throughout this process as it's critical. I understand that the SHRINKFILE operation is a fully online one, or am I wrong? The database has a large amount of write activity occurring pretty much every second. Will the increased I/O caused by the SHRINKFILE operation affect these writes? Turning this on its head, will the continuous writes affect the SHRINKFILE operation, i.e. make it slower to complete? Finally, is there a better way to do this? 

I'm familiar with database encryption options for SQL Server such as TDE and ensuring backups / backup locations are encrypted and secured properly. I'm doing some work for a new client who has asked me to detail options for end to end encryption, in other words, to ensure data sent from database server to application server remains encrypted en route and cannot be intercepted (or rather, interpreted) by any sort of packet monitoring. I know about using SSL certificates to encrypt connections to the database, though it is something I have seldom configured. I really don't know if that is an adequate option for end to end encryption as surely this cannot be configured using SQL Server alone? In my mind there has to be something at the application end, or on the network route that enables end to end encryption like this? My knowledge of network security is basically limited to firewall rules and general routing, so I'm looking for options available to me to either research further or put into practice. I wary that this question will be closed because it is opinion based, so I'm really asking for solid options within SQL Server itself (or within the remit of a DBA) to configure end to end encryption of data. 

I've tackled this in the past by doing the following: First, before doing anything, get a quick configuration report. You'll want to put everything back 'as is' when you configure your new Standard installation. Here's a useful script I use to get some key configuration settings - feel free to amend: 

To answer your question, in FULL recovery mode, growth of the log file is absolutely normal, and of course it will grow to either the limited size or the size of the disk unless it is maintained. The rate of growth will be determined by the type of / frequency of / load of transactions hitting your database. The rate of growth, will also determined by your auto-growth settings, i.e. grow by fixed value or percentage etc. Logs are typically maintained by performing a log backup at suitable intervals. This truncates the data within the log file itself allowing the space to be re-used for new transactions. What is important to note is that the problems you might be facing due to a large log file (i.e. where the size has gotten out of hand due to lack of suitable maintenance) is not necessarily remedied by a transaction log backup alone. You may need to shrink the file to free up disk space, for example. 

I don't use WSUS for SQL Server patching because we have so many different compatibility issues, requirements and downtime OLA's that it makes it impossible to deploy anything in this way - not sure of many who can to be honest. However, we have a custom DBA 'app' that I would like to develop a snap-in for that alerts the team to any new updates released by Microsoft for SQL Server (SP's, CU's, Hotfixes etc.) I thought, hey, let's just hook into the WSUS database, poll for new updates and display them on a web page if they exist... It works to a fashion, but I noticed that there is a lot of missing updates in WSUS from SQL Server. For example I'm not seeing SQL Server 2014 SP2 CU1 in there which was released recently, 2014 SP1 CU7 is not there, but CU8 is. 2012 SP2 and SP3 are not their either, as another example. We sync WSUS half-hourly, and all SQL Server products are selected. Any idea why some SQL Server updates aren't coming through to us via WSUS? IS this normal? Do Microsoft not publish everything through WSUS? 

Maybe it's just me but it sounds like one of those things where you need to just grab the data from each and look at it. It doesn't sound like you have specific enough definitions of the term 'similar' to write a sure-fire script. 

Next, backup all user databases. Stop the services for the Enterprise edition instance. Copy the master, model and msdb data and log files to a secure location. You'll need these later so you don't have to recreate all the jobs and logins. Uninstall Enterprise Edition. Install standard edition and apply the appropriate SP's and CU's. Restore the user databases. Stop the services. Copy and replace the system database files you copied earlier. Restart the services. 

We have a database running SQL Server 2008R2 which requires an upgrade (to SQL Server 2014) because it is a pre-requisite for the latest edition of the software which the database is the back end of. Integration Services (2008R2) is also running on this server, with many .dtsx packages stored in MSDB. They are mainly used for ETL purposes and transforming data for reporting reasons. Due to the nature of the environment, it will need to be an in-place upgrade. The database engine upgrade will be a breeze, but I was wondering specifically about the .dtsx packages in MSDB. Will they automatically upgrade when the database engine and integration services is upgraded, or will there be a need to manually upgrade them after this. Are there any known issues with upgrading Integration Services between 2008R2 and 2014 SP2? Fortunately we have a mirrored environment running in a VM which we can mess around with in terms of testing the application (and packages) before we move to Live. Thanks 

Honestly, the biggest headache I've had with this is with application compatibility and agreeing downtime. The process was simple for me and my nodes... 

I know how to get the name of the current node, but is there a simple T-SQL query to use to get the Windows Server Failover Cluster name that the clustered instance is part of? The SQL Server version is 2014 and the OS is Windows Server 2012 

This may be a very silly question, but when I'm looking at the current value of a 'per second' counter in PerfMon, for example - how is that value actually measured? Let's say I use PowerShell's Get-Counter method to return a result for a specific server, as follows: 

Since we don't know what happened with your original installation, i.e. whether it was successful or not) it is difficult to pin down the exact issue. I am sure SQL Server Database Engine has never been successfully installed, so is not eligible to repair. Also, it could be an SA or Service Account corruption Issue. You really need to be re-installing SQL Server fresh and not repairing. So you could try to do a fresh install, but first I would uninstall anything related to SQL Server by going to Add/Remove Programs. Have a look at this link Uninstall SQL Server for instructions on how to do a full uninstallation. Check Services, to ensure there are no existing services related to SQL Server. Then, I would do a complete reinstall, not a repair, and I would do it as a new Named Instance. Follow the installation through and see how you get on. Make sure you are an administrator on the machine, and pay very special attention to the service account page of the installation. For this, make sure the SQL Service is running as the default NETWORK account. If it has picked up an account that is not eligible to start the service as part of the installation you'll get issues. In short, you appear to be repairing something that either doesn't exist or is not stable to repair. You need to ensure any remnants of a previous install is removed fully and then start again. 

When I create a database, I have always changed the database owner from me (usually my domain account) to another, either a standard service account used for ownership, or to the usual sa However, I've just inherited a series of instances where a lot of the databases have been created by another member of staff, who's now left, so naturally his domain account has been deleted. Didn't run into any issues with this until yesterday when I tried to create a stored procedure on a database that would execute as owner (blindly believing it to be 'sa'). The create failed because the owner didn't exist, so I had no choice but to make an ad-hoc change as the client needed the stored procedure creating. There are tons of production databases under his name so I want to be both pro-active and change-control savvy. First question is, what are the potential issues this could cause now or in the future? Second question, can I safely just change ownership to that of a standard account or 'sa'? Could it, will it, have any weird impact? 

We have a huge suite of reports currently running on Oracle BI Server, on a Windows Server 2008R2 platform. There is a requirement to upgrade the Server and the OBI version, so my thoughts are to do a side-by-side upgrade. What I'm missing is some guidance on how to migrate the reports, and also whether it's possible to go from version 10 to 12, or would I have to do an in place upgrade on version 10 (to 11) first? I have no experience with Oracle BI server, but have been presented with this issue, so any thoughts or advice is more than welcome. 

One of our applications is causing a problem in that there is a stored procedure that returns fine in SSMS, in under 1 sec, but in the application its taking anything up to 10 minutes, depending on the parameters used. Any combination of parameters work fine in SSMS and the execution plan looks good to me. However, when profiled the Application is clearly using a different, less efficient plan. The SqlClient used to connect has the ARITHABORT setting, set to OFF, which when replicated in SSMS I run into the same performance issues. I'm guessing ARITHABORT OFF will not allow the optimizer to use the cached plan? Or does it run a separate plan? According to the Apps guys, there is no way to change the SQLClient connection to use ARITHABORT ON I'm guessing the ARITHABORT setting is a bit of a misnomer here, and it's actually a parameter sniffing thing caused by the optimizer not using the good plan? I need to force it to use a good plan regardless, so how best to address the issue? Do I create a plan guide or optimize the stored proc in some way? This is SQL Server 2008R2 SP2. 

I've been approached with a requirement. One of our devs wants to be able to store information in the database, but once there it needs to be encrypted, so that if anyone ever gets a copy of the database or access to the raw data within it they will not be able to read it. You should only be able to read the data once you have authenticated through the app via SSO. He seems to think there is a simple function for this, that he can call from the application and decrypt the encryption... I'm telling him that something like TDE is going to be required here which will take a little while to set up. I have no experience of this, so it will be a learning curve.... In short, the requirement is to protect sensitive information if the database is ever hacked, stolen or mislaid. Any ideas? Or is TDE the way to go? 

Just for anyone facing this mysterious issue in the future... This particular issue was resolved by removing packet inspection rules on TCP Port 1521 (Oracle default) within the VMWare NSX. This was causing a bottleneck of data coming out of our Oracle infrastructure (IBM AIX) and back into our NSX infrastructure. 

If I run a rebuild of sys databases via command prompt using the install media, am I able to specify an alternative drive for the files? My understanding is on a rebuild, the pre-existing default instance location is used, which in this scenario wouldn't be accessible. Assuming I can get a copy of the mdf and ldf files (by restoring a backup to another instance, for example) is there a way to start SQL Server by pointing it to a different location for the MASTER database? Like a service parameter?