A couple of things that have caught you out here. In the SQL Server version using the Point, the order of the coordinate is Lat Lon, eg Y X. The OGC Point construct is X Y or Lon Lat. The next is that the PostGIS query you've posted actually returns which while appearing similar to the SQL Server result, is quite a lot smaller. The last issue is that the PostGIS query is treating the objects as Geometries and doing a cartesian distance on them. You need to explicitly make them Geographies. Here's a few ways to do the PostGIS query. I've swapped the coordinate order 

Then to incorporate the scaling factor into the sum you can use a case statement in a subquery, similar to the following 

mysqldump does have an option of outputting to tab delimited files which you can LOAD DATA INFILE later: $URL$ 

I would seriously advise against this design. Its a database anti-pattern called the Entity Attribute Value pattern. Here are some posts as to why: $URL$ $URL$ A better approach would be to start with the columns/attributes you have and add them at a later stage. You can run statements to make the changes later and there are tools to do this in an "online" manner as well. $URL$ Other things you can do is use a document-based database or XML. You can also store XML documents in MySQL in blob/text files, but you lose a lot of benefits this way. 

The spatial index, as with other objects, have their own menus and need to be done separately. This can be done to the clipboard and pasted to the query window generated by the command. I can't confirm at the moment, but I suspect that the primary key and other constraints come out in the options since the are an integral part of the table definition, whereas indexes are not. The wizard can also be used, but it is a lot of steps if you are just doing a few items. Technet, How to: Generate a Script 

This is my interpretation of your requirement.As a simple query, this will return what you want, however you will probably want to try other options to make it perform. I don't use MySQL, but in SQL Server I would look at trying CROSS APPLIES or some sort of grouping option. This query will return all the dates on which the minimum or maximum occurred. You can of course then filter that to suit what you require. 

You would not be able to partition that table as you need to remove the unique key and make it a regular index instead. To answer your question, if you query on column that isn't partitioned then MySQL would query every partition one-by-one until it answers the query. So if you query a partitioned table on a column that it is not partitioned on, you may get slower results then if the table was not partitioned. That is why you need to choose the column you want to partition very carefully. I would select a column that fits 80% of the most important use cases. 

I know what you mean. Alter table sometimes has to rebuild the whole table when you use it. Assuming that the Alter table statement is annoying you because it takes too long AND it locks your table, then you can use 2 tools to make it an "online" alter table. One if from facebook $URL$ And one is from openark $URL$ (MySQL 5.1+ only) Both these tools basically create a new table, copy the data over and keep it updated by updating both the old and the new table with new queries until the new table is completed. This means that you need to have the space available to do this operation. Hope it helps. 

Based on the previous question and this, the following should do what you want. As with Pieter's update it doesn't assume that the ID's are in sequential order, rather it uses the ReqTime to determine order. 

Here's a variation on how I have tackled this in the past. I have added a grp and seq columns to the @Geo to demonstrate how to build multiple lines and get the order of the points in the line correct. The seq could alternatively be datetime. 

Also note SQL Server let me build a spatial index over the geometry column that contains mixed SRIDs, but it is of limited use. Even if there was a lot more data, the index would be of limited use. I would suggest to make life easier that you pick a single projection to use across all you data and transform geometries to that. This can be done alongside the current geometries or replace them. Also if you choose a Lat/Lon projection, use the Geography data type. There is an open source project, SQL Server Spatial Tools, that has some tools that will allow you to do projections. You will however have to know the parameters for the projections you are using as it does not appear to have a list of projections to work with. I can't advise how good these are, as I haven't used them. 

You need to find out first what makes the load get too high so that you are able to plan ahead. If its IO bound then you need faster disks. If its memory bound then you need to add more memory or find ways to reduce its usage. The best way to find out what the problem is, is to track your system during a peak time. Get a graphing application and look for: 

Also keep track of your info and see what are the differences. Try to notice patterns, find the bottlenecks and predict scalability issues. For example, if you system is currently doing 100,000 transactions per minute and your bottlenecks are at 75% capacity, then if your TPM increase it to 140,000 your system might crash/stall/lagg/die-a-horrible-death. 

I have struck similar issues with Geometry and Geography data types before. I think the issue is to do with where (and how) the optimizer decides to build the geometry in the plan. I was having an issue with insert statement that was basically 

This is a pretty convoluted way to do it and I am sure there is likely to better ways, but it can be adapted to handle more columns. 

I get 93ms, 4ms and 93ms respectively, removing the TOP 2000 from the queries causes the price filtered query to blow out to a minute or so. Are you able to give more details (execution plans, DDL, etc) to help us replicate your issue. My initial thought was that the was hiding the underlying problem. I think that may still be the case. 

You can use symbolic links with MySQL. All you would need to do is make a symbolic link for the .frm, .MYI and .MYD files of your MyISAM tables into the MySQL database schema directory that you want. After that, run and they should show up. It is also important that the MySQL user is able to access those files, so you might have a bit of an issue with user rights. Also, depending on your MySQL version, there maybe a setting in the my.cnf which enables/disables symbolic links, so you would need to keep an eye on that too. 

This will set all NEW sessions to be mixed binlog format. All existing sessions will be whatever was set previously until they end. You can also do manually to solve any problems with session specifically. 

The first query determines distances between points. It will only return a distance between the 3rd and 4th point as they are the only ones with the same SRID. The other distances will be NULL. The second query is for selecting geometries inside a buffer. Only a single row is returned since the query buffer was created with SRID 2. 

There is a couple of approaches that can be used here depending on the exact requirements. First of all you will need to create a spatial index on the Landmark table 

This gives a table of 1,000,000 random points with a 2 x 2 degree spread. After trying a few different options on it, the best performance I could get was forcing it to use the spatial index. There was a couple of ways to achieve this. Dropping the index on LegendTypeID or using a hint.You will need to decide which is best for your situation. Personally I don't like using index hints and would drop the other index if it is not required for other queries. The queries stacked up against each other 

RBR basically updates the slave with the entire row as it is after it has been inserted/updated and updates using the primary key of the table. The advantages to this over statement is if the master server runs a statement which requires some sort of calculation or if it has a non-deterministic function or sub query in it. With the former, you do not need to perform any calculation on the slave as you are only updating the row after the calculation has been applied to it. With the latter, it may resolve race-conditions if the non-deterministic function or sub-query ran at a time when the result would be different then the time when it is run on the slave. So you can use this to keep your data consistent with these types of queries. Also, it is important to note that you can use RBR in a session (if you are using MIXED binlog_format on your server) for part of your application that you feel has these types of statements (add to that if you create your own temp table and need the slave to do the same). You can do in your application code for that specific (and potentially problematic) part of your code. The disadvantages are if on the master, you run one statement that updates 100,000 rows. To update the slave, you just pass that one statement very quickly. In my opinion, in transactions-based applications (where everything is just 1-2 inserted rows), then RBR can be advantageous. But in any case, you should test it out for yourself.