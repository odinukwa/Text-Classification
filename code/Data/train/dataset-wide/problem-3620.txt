I want to to retrieve a list of the virtual hosts which are currently loaded and listening for requests i.e not just grepping the config files. It looks like does this but I am not 100% sure if that is just returning what is contained in the config files. 

Can anyone suggest what would be causing this and how best to resolve the issue? Where will Redis try and save temp files, which user would it use and which permissions would safely resolve this? Also of interest would be a free method of monitoring the Redis instance so I know if / when it falls over. 6359.conf: 

IP addresses and domain names are fictional. How can I add a DNS record to forward only website traffic? If I alter the "@" A record to the webserver's new IP, will email be sent to the webserver? If so, what would happen to the mail if there were no mailboxes / mail server on the new IP? Whats the best way to ensure that $URL$ and $URL$ both point to a new IP? 

I have tried numerous methods to generate a password (e.g apache's ) but I am not able to log in at /Docs with the username and password specified in .htpasswd. I see the authentication dialog with 'Restricted Area' but the password is not accepted. I've tried the methods here: 

NB: is a separate active NIC (Ethernet cross cable) but that should not matter IMHO. The problem The setup looks good to me, however, PING does not work (this was run on ): 

This is on . On the other host the same interface is set up in the other direction. Bridge A bridge is set up that currently uses only the GRE-TAP device. I need the bridge because later on I want to add more machines (my plan is to bridge all GRE tunnels together). The end result should become a VPN mesh network (with a dedicated GRE-TAP interface for each host-host combination). But since for now I'm just doing a first test with two machines, the bridge is of course somewhat useless, but nonetheless important for the test itself. 

One of the decisions left to take is whether to use software RAID1 or hardware RAID1 + BBU. Software RAID is the solution I'm very familiar with (I'm managing a number of servers since 15 years and I know how the tools work). I never had a serious problem with it (mainly only the HDD fail). These are the reasons why I prefer software RAID. What I dislike about hardware RAID is the incompatibility between controller vendors and the lack of experience I'm having with them: different configuration options, different monitoring method, different utility programs - not a good feeling for creating a cluster system. I know that, when using a BBU, hardware RAID can both be fast and reliable (write through cache). However, since all data will be stored in a highly redundant manner in the cluster, my idea is to use software RAID1 and disable barriers in the file system to increase write performance. I expect that this will lead to similar performance like hardware RAID1. Of course, I risk data loss due to the volatile write cache, however IMHO that should be handled by the clustering mechanisms anyway (the whole machine should be able to restore data from the other nodes after failure). I'm not having concerns about the CPU resources needed by a software RAID implementation. Is my assumption correct or am I missing some important detail that would help me making the right choice? 

Redis was installed as root on Centos 6.7 using the following instructions (the default yum package is too old) using the following method: 

Turns out this was caused by a Squid caching proxy which was not passing authentication headers correctly. This was proved by running something similar to the following: 

Is this because all requests containing in the name are passed to fcgi and so all rewrite rules are ignored? 

I created a ZFS pool on Ubuntu 14.04 without specifiying RAID or redundancy options, wrote some data to it, rebooted the machine and the pool is no longer available (UNAVAIL). I don't have the exact error to hand but it mentioned that there was not sufficient replication available. I created two datastores in the pool which consists of 2 3TB disks. ZFS was recommended to me for its deduplication abilities and I'm not concerned with redundancy at this point. I actually only want RAID0 so no mirroring or redundancy in the short term. Is there a way to do this with ZFS or would I be better off with LVM? 

I have tried granting privileges on (this is a vagrant box) but the error persists. Can anyone suggest what might be causing this? I can connect to MySQL from a simple nodejs script on the same box without error using the same credentials. SELinux is disabled: 

The bridge works because when I activate the interface and set up some IP addresses (just for testing the bridge), ICMP PINGs work perfectly. 

I have Linux devices with a single ethernet interface and two IP adresses. The first () is statically configured to . The second () is configured via DHCP and it may happen that it gets a similar IP like , meaning that the subnets overlap. The routing table looks like this: 

I have a relatively simple web application that consists of two Docker containers working together. One container hosts the web server and the other one hosts a image rendering software. On my development machine it works flawlessly using . For production this application will make use of Amazon SES and some database like Amazon DynamoDB. In that combination the containers itself don't need any persistent storage. Because of those services, my first thought is obviously to use Amazon EC2 to host the application. My primary goals now are: 

Update I get a working setup when I set the bridge interface into promiscous mode (). I'm not quite sure if that is normally needed. OTOH I don't think it has any downsides... BTW, there a similar RedHat bug report exists, but setting down/up doesn't help in my case.. 

How would I add a conditional here which only installs the above packages if they are not already installed? Is there another state I can use or do I need to manually run something in the shell which checks if they are installed first? e.g. something using or . I would rather not have to create a task for each package as this will become unwieldy when there are lots of packages to install 

This has no effect when making a request to . The timeout occurs after approximately 60 seconds. PHP is configured to allow the script to run until completion If I set the in the main block this resolves the issue. I don't want to set a global timeout for all scripts. 

Is it possible to set timeout directives within a location block to prevent nginx returning a 504 from a long running PHP script (PHP-FPM)? 

The stdout contains the phrase 'not installed'. Ansible version 1.9.4, running on OSX El Capitan with Vagrant & Virtualbox (latest versions as at Jan 2016) 

I want to redirect requests for any images with filenames starting with a string like 'myimage_' to a subdirectory on another server. I'd like to understand why the following rule does not work for my use case. 

I'm trying to get Linux bonding working over a VPN (GRE-TAP). The funny thing is, that it only works when I have running on both hosts, but more on that later... There are two machines, called and . They are connected together using a simple switch via eth1. 

I guess creating a queue for parent won't work as packets won't be marked on that level (right?), since all packets are VLAN-tagged. I'd like to have the same QoS as I had with FireQOS but what's really important is the VoIP part. So, how should I configure the Queue Tree? 

The other host is set up as 172.16.1.1/24 with HWaddr 02:00:0a:01:01:c7. This results in a theoretically working bonding interface: 

When two of those devices are in the same network it is obvious that the address causes a collision and should not be used. The DHCP address should be used. The 10.1.1.146 is intended for 1-to-1 connections. Unfortunately, when doing simple things like or when trying to reach the Internet, then the Kernel chooses to use the ...146 IP address as source in such a situation. AFAIK that is because it prefers the /24 network since it's smaller. Question: Can I somehow give the DHCP subnet precedence (perhaps via some command usage), even if there is another subnet that qualifies? 

In addition to having forgotten that logging cannot be configured in files, this also turned out to be an Apache version issue. The RewriteLog directive has been replaced in newer versions and adding the following to the Virtualhost configuration enabled the rewrite log for me: 

where dump.php contains In the case of the server behind squid, the output of that command was empty and I would expect to see mention of 

While creating a playbook for a vagrant box, I want to skip install steps if the package is already installed. This seems like such a common scenario but is not expliclity mentioned in the conditional docs as far as I can tell. I am running a playbook multiple times during testing and want to install packages conditionally so I don't have to keep running tasks which I know are working e.g. 

The Centos 6.5 server is a Virtualized server running Virtuozzo (A Media Temple DV). It seems there would need to be some changes in the Virtuozzo provisioning of the server which the hosting company is not prepared to do. Apparently Virtuozzo does support running docker in Centos 7 but that's not something I am going to pursue. More info here. Bottom line is the Kernel the server is running is missing some modules / config which docker needs in order to run. 

I think I need to set up Queue Trees on the Mikrotik and create a main queue for the available up/down speeds and divide by some priorities. Currently I don't know where to start as I can't see a way to create two distinct queues for incoming and for outgoing traffic (as I have an asymmetrical bandwidth). I see there are predefined (parent) queues for each interface but the problem is that I am using VLAN to provide WAN access to three completely independent subnets. The single physical port I'm using is : 

This only works as long as both machines have running. I can start/stop and consistenly only see replies while the program is running on both machines at the same time. It doesn't matter on which machine I try. Is this a kernel bug or (more probable) has my configuration some problem? Is it normal, that the bridge and bonding interface both show the same MAC address? I only configure it manually for the bonding interface, which apparently changes also the bridge.. FYI, config overview: 

Bonding A Linux Bonding interface is now set up. Again, since this is just a first proof of concept test, I'll only add a single slave to the bond. Later on there will also be a real separate Gbit NIC with a dedicated switch that will act as the primary slave (with the VPN being just a backup), but for now the bonding interface will use the VPN only.