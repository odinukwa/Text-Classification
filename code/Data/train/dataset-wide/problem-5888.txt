It is helpful to distinguish two questions here: one logical and the other epistemological. If you ask, what is the logical status of a proffered deductive proof of some proposition, then it is either valid or not. It doesn't matter whether the proof was generated by a human mind, a computer, or an explosion in a print factory. But often, we are interested in the epistemological question: how can we be sure this proof is correct? There are plenty of valid proofs that are very difficult to comprehend: for example, few people know enough math to understand the proof of Fermat's last theorem. Also, there are proofs of theorems that are so long and complex that it is likely that no single person has ever read them from beginning to end: the classification theorem in group theory is one such - it runs to tens of thousands of pages of work across hundreds of published papers by about a hundred different mathematicians. When a proof is as difficult as this, it makes sense to enlist the help of automated provers. But it does not completely solve the problem of how to be sure, because we still need assurance that the prover itself is correct. Since the prover is a physical embodiment of a formal proof system, Godel's second incompleteness theorem applies to it: it cannot prove its own consistency. Also, the prover is implemented in hardware and software and we cannot prove there are no errors in the implementation (although formal systems such as Z notation can help). So to return to your question about Hume, I would say any proof logically conforms to what Hume calls relations of ideas and so is a priori, at least in principle. But there is a hard-headed pragmatic element to Hume's thinking that makes one suspect that he might regard output from a computer as just a kind of empirical evidence. So maybe he would regard a generated proof as a posteriori until it has been internalised and comprehended. 

efficiency(QC) > efficiency(non-QC) > efficiency(Human). therefore efficiency(QC) > efficiency(Human). therefore. QC ≠ Human. 

In the freewill debate, a difference is made between metaphysical/libertarian free will, i.e. where an agent is free to choose among many possible outcomes, and the eventual outcome is caused by the agent, not by prior circumstances, and compatibilist freewill, where an agent is free as long as it is acting according to its own motivations, even if it is impossible for it to choose among multiple future possibilities. Does the concept of "decision making" have any meaning independently of libertarian freewill? If someone subscribes to compatibilism or to hard determinism, then saying that "Jane decided to have coffee instead of tea" doesn't really carry any meaning. The event of Jane drinking coffee instead of tea has the same status as the event of a rock rolling down one side of the mountain instead of the other. That she "decided" to have coffee is either epiphenomenal or completely meaningless. Similarly, consider some software used by a bank that automatically approves or rejects applications for a loan: The software reviews a set of inputs (income, assets, credit score, employment history, etc...) and then either approves or rejects the loan application. If the outcome of the process is deterministic, then to say that the software "decided" to approve the application doesn't really mean anything other than that it performed a mechanical calculation based on the input variables. For there to have been a "real decision", there has to have been the possibility of there being a different outcome given the exact same input variables. In the above scenario, even if there was a human agent involved, the reasoning is the same. If the agent has to abide by the results of the calculation, then they can say legitimately say that the decision wasn't theirs to make. The decision ultimately lay with whoever set the guidelines that the software's calculations are based on. My questions: 

Start by proving ◇(p ∧ q) → ◇p and also ◇(p ∧ q) → ◇q. It is easy to prove the ◻ counterparts of these: (p ∧ q) → p is a tautology of PC, so you can derive ◻((p ∧ q) → p) and hence by the K-axiom ◻(p ∧ q) → ◻p. The proof of the ◇ counterparts involves extra steps, because you need to use the equivalence of ◇φ and ¬◻¬φ and prove the contrapositive. In general, it is always the case that if φ → ψ then ◇φ → ◇ψ, though this isn't needed as a separate axiom within K, because it can be derived from the others. Now that we have ◇p and ◇q we need to get to ◇p ∧ ◇q. If we were allowed to help ourselves to the rules of PC, this would be easy, because this is simply the introduction rule for ∧. But your problem specifies that we may only use the tautologies of PC, not its rules. A useful tip is that a rule of PC corresponds to a conditional tautology. So in general, if in PC we can prove ψ from φ then the material conditional φ → ψ is a tautology. So the introduction rule for ∧ implies the following is a tautology: φ → (ψ → (φ ∧ ψ)) and hence ◇p → (◇q → (◇p ∧ ◇q)). You should now be able to show that ◇(p ∧ q) implies each of ◇p and ◇q and hence by transitivity that it implies ◇p ∧ ◇q. 

Both sentient and none sentient beings can see a red apple - i.e. they can have a first order mental state corresponding to apple. For a Sentient being to be conscious of the red apple, it is not enough to have a mental states corresponding to a red apple, but also to have a higher order mental state which corresponds to 'being aware of there being a red apple'. In information processing terms, the system has to store a representation of the red apple, and a second representation of the act of being aware of the red apple. Closely related to Higher Order Theories of consciousness are Self-Representational Theories of Consciousness, of which the most well known account is given by Douglas Hofstadter in his books "Godel, Escher and Bach", and "I am a Stange Loop". Here self-awareness, or sentience, is characterize by information processing systems capable of representing themselves, i.e. or storing and manipulating symbols that represent themselves. Hofstadter call this a strange loop. A conscious "I" emerges in systems that are rich enough to process a representation of themselves, the key is the ability for self-reference, which instead of leading to paradox is what leads to the emergence of consciousness. AS I mentioned earlier, the hard problem of consciousness is still considered an open question by many, and not everyone agrees that Higher Order and Self-Representational theories solve the dilemma. But they do provide one avenue to your question. 

Your question highlights an important issue with respect to different interpretations of probability. The frequentist has great difficulty attaching a probability to a single event. Strictly speaking, there cannot be a frequency of a single event, so frequentists tend to wave their arms a lot and claim that we can consider something to be an instance of a long run of trials in principle. Such a claim is not plausible in general. What is the probability that Hillary Clinton will become the next US president? (This is written in 2016 with the primaries still in progress.) Such an event, if it happens, will be unique: there cannot be a long run frequency of it. If you try to assess its probability as a frequency you cannot get any sensible value for it. What is the frequency of a female president? Zero. What is the frequency of a democrat president? About a half? What is the frequency of a democrat president given that the previous president was a democrat? Less than a half. Whatever frame of reference you choose will give you a different answer. Frequentists might try to tough it out and say that such an event does not have a probability, because it lacks a frequency, but this is implausible. You can bet on Hillary to win (or not win) and betting odds imply probabilities. The virtue of the epistemic approach to understanding probability is that the derivation of the probability calculus can be made without reference to frequencies or possibility spaces, but by starting from simple assumptions about how decisions are made under uncertain information, and about what constitutes a bad decision. We all have to make decisions, and we almost always have to do so with imperfect information. Frequently we make bad decisions, and often this happens because we have failed to quantify the uncertainty properly. One approach to deriving probability is to take a bet as a paradigm case of a decision under uncertainty, and a Dutch book as a paradigm case of a bad decision. (A Dutch book is a combination of bets that results in you losing, no matter what happens.) A fairly remarkable result, first proved by Bruno de Finetti, is that from this consideration only, if you wish to avoid making bad decisions, your calculus of uncertainty must conform to the probability calculus. Of course, this is a fairly limited concept of 'bad decision' - it corresponds to nothing more than maximising expectation value, and as many theorists have pointed out, it is not always the best strategy to maximise expectation value. Nevertheless it is a powerful concept: it shows that we can derive probability theory from decision theory. This provides you with the answer to your question: probability is used to make decisions because properly understood it is exactly that quantity that allows us to make decisions under uncertainty without falling into straightforward errors of judgement. 

I hope my explanation has removed some of your confusion. Elementary Logic by Benson Mates has sections on use and mention, and sense and denotation, which might be useful. 

The differences between Fodor and Churchlands are reflected as rule-based AI and machine-learning AI in the field of cognitive science. If the machine is indeed the model of the human mind as many philosophers of mind suggest, and if the mind generates ideas, then so does the machine. 

Just to get the discussion going, I offer my response. The situation is that you think your contractor commits a fallacy when she asserts, "[Since] most security breaches are committed (intentionally or unintentionally) by inside employees, it should be concluded that trust cannot be given to employees." You offer some reasons. I follow your reasoning to determine what the name of the fallacy might be. Initially you suggest that the argument commits some causal fallacy. Post Hoc, Ergo Propter Hoc ( After This, Therefore Because of This) is the name for a causal fallacy. The fallacy is committed when a person confuses a sequential relationship (or correlation) for a causal relationship. But I do not see her argument makes any causal claim. So not is Post Hoc, Ergo Propter Hoc. Your second idea is that security breaches are irrelevant to trustworthiness, as you separate a bad action by a person from the character of the person. In this case, her argument can be viewed as committing Non Sequitur (not follow), which occurs when the conclusion is supported by irrelevant premises. So you can argue that one may not conclude untrustworthiness from security breaches. My problem with identifying the argument with Non Sequitur, however, is that security breaches do seem to be relevant to trustworthiness. The contractor should have meant trust differently from yours. While you interpret trust as a value-laden concept (good or bad), the contractor should have meant trust with reliability, a value-neutral concept. There is no absurdity in saying that we cannot rely on those who are prone to security breach. One suggestion is to argue for the fallacy of division.The fallacy of division occurs when one infers mistakenly that since A is a part of B, and since B has the property X, A must have X. For instance, since the roof is the part of house, and since the house is painted white, one mistakenly infers that the roof must be painted white. The common sense however is that when we say white house, we mean only the exterior walls of the house. Applying the fallacy, you can argue that the contractor is mistaken to think that, since the majority security breaches happen by the employees, it will also happen by your employees. You can rebut that your employees do not share the property (breaching security) since they are qualitatively different (e.g., they met the govt security clearance). But if your employees are representative of the pool of all employees, then the contractor’s claim is not fallacious. Depending on the situation and interpretation, the contractor’s argument might not be committing a fallacy at all. The disagreements between you and you contract can be just the reflection of two competing goals in doing business: security and efficiency. 

The Bundle Theory of Self is one model where it is possible to have awareness without there being a subject. In the Bundle theory the self or the mind is not a unified entity, but just a collection of memories and perceptions which together give the mere illusion of there being a central subject, but without there being any real unified self. The Bundle Theory was first proposed in Buddhist philosophy, with the concept of the five Skandhas: A person is made of 5 aggregates - material form, feelings, perception, volition, and sensory consciousnessn - and there is no central "I". The modern formulation of the Bundle theory comes from David Hume and the Empiricists. Derek Parfit and Daniel Dennett are contemporary philosophers who subscribe to the bundle theory. Douglas Hofstadter, has an interesting variation on the bundle theory, which he calls "Strange Loop". Instead of awareness necessitating a subject, it is the other way around: It is when a bundle of sensations and memories becomes complex enough to start perceiving itself that the subject emerges. 

Conditionals in English are used for a lot more than just expressing simple truth functions. Here are some general cases where the truth functional material conditional doesn't fit. 

A good paper to read on this subject is an old classic: Gilbert Ryle's Systematically Misleading Expressions. (Proceedings of the Aristotelian Society, 32: 139-170 (1932). Also in his Collected Papers, vol 2.) Ryle's view is that ordinary non-philosophical use of language frequently contains "improper" usages, by which he means usages that, while having a clear meaning to the non-philosophers who are using them, are systematically misleading when we try to subject them to a more rigorous understanding. This is not to say that such usages are defective, only that philosophers, when they subject them to examination, find that they need to restate them in a different way to avoid the apparent misleading implications. Ryle gives as examples: 

Are not the same operation. From a mathematical and combinatorics point of view, we have imposed an order relation on the set which isn't necessary for classification. Underlying the concept of prioritizing things or assign values to propositions and objects is this notion of order relation. Consider the questions: (a) "What is the exact percentage of iron ore on Mars?", (b) "Is there liquid water on Mars?", (c) "Is there intelligent life on Mars?" From a neutral point of view (say the point of view of a computer) these three questions are equivalent. From a human point of view, most would agree that (a) is a boring scientific question that interests only a small number of planetologists, while (c) is an existential question whose answer would seriously impact the way humanity views itself. And so to say that one question is more important or takes priority, or has more value, than the other question, is to impose at least a partial order on the set of questions: 

If the interpretation corresponds to the ordinary usage of the words, then a model that satisfies the premises indeed does satisfy the conclusion. 

The Frege-Russell logicism project, ulteriorly, was to reduce biology to chemistry, chemistry to physics, physics to math, and finally, math to logic. Theories of hypothesis testing Your question on hypothesis testing is concerned with the first ideology of logical positivism: verifiability principle. The following are theories that aimed to materialize the principle. Phase 1: Vindicationism (1920's) Initially, positivists thought that sentences must be vindicated to be meaningful and scientific. Soon they realized that vindication was impossible even at the level of scientific observations (Many claims of Einstein could not be vindicated at the time). Hypotheses in particular cannot be vindicated since it is a universal statement. ‘All ravens are black’ can never be vindicated by 1000’s observations of black ravens. By 1935, Carnap himself gave up on vindicationism which he proposed a decade ago. Phase 2: Confirmationism (1930's) Instead of vindication, Carnap now offers confirmation as the way to understand the truth of a hypothesis. If there are 1000’s observations of black ravens, then the hypothesis ‘All ravens are black; is very much confirmed. So the hypothesis approximates the truth. Carnap thus asserts that what matters is degrees of confirmation, not absolute veridication. His Logical Foundations of Probability (1950) is designed to realize his idea that probability is concerned with the logical relationship between a hypothesis and evidence. Phase 3: Falsificationism (1950's) Karl Popper, in his The Logic of Scientific Discovery (first published in German 1934: introduced to the English speaking world in 1959), having criticized confirmation as a logically invalid form of inference (the fallacy of affirming the consequent), offered falsification as the logically correct form of hypothesis testing. (I have a post on Popper’s falsificationism somewhere in Stackexchange). Phase 4: Crises (1960’s) Logical positivists were at the brink of being squeezed out due to internal and external pressures. The external pressure is the advance of a new paradigm, ushered by Thomas Kuhn. The internal pressure began with the Duhem-Quine thesis. Confirmation theorists themselves start to find paradoxes of confirmation (e.g., Hempel’s raven paradox and Goodman’s grue paradox). Does the observation of a white shoe confirm the hypothesis ‘All ravens are black’? (Hempel's Studies in the Logic of Confirmation (1965)). Does the observation of a green emerald confirm the hypothesis ‘All emeralds are green’ or ‘All emeralds are grue’? (Goodman’s The Structure of Appearance (1951) These crises incentivised logical positivists to change their name to logical empiricists. Phase 5: Bayesian confirmation (1990's) A Bayesian method became the last hope for logical empiricists. Bayesians found that the paradoxes of confirmation are solvable within the Bayesian framework. New ideas, of course, generate new (pseudo) problems (like the problems of old evidence and of new hypothesis). John Earman examined the Bayesian method in his Bayes or Bust (1992). (I know this sh** because the Bayesian methodology was my dissertation topic in my previous academic life.) Phase 6: Present The doctrines of logical empiricism are fatally attractive. The building of an empire: i.e., the unification of science! If defection is not an option, as the wise Earman once said, one should continue the imperial march of empiricism since the alternative is to get busted. For Defectors For those thinking of defection, these are some options: revolutionism (“Scientific progress is made by revolution, not by approximation.”: Thomas Kuhn), historicism (“Science is what scientists do.”: Peter Galison), Aristotelianism (“I hunted for causality all my life, and I now realize that Aristotle captured it best.”: Nancy Cartwright), and pluralism (“Biology is no slave to physics. We sciences are all equal.”: John Dupré). 

I usually scoff at mentions of QM in philosophy discussions. Most of the time they are a sign that the discussion is going to head into Deepak Chopra territory very soon. But the question you asked might be the one case where it would be justified to invoke Quantum Strangeness: One might be able to reconcile a universe governed by the laws of science with a god having the ability to intervene at will in its course of affairs by allowing the deity to intervene via quantum indeterminacy. The randomness of quantum outcomes might be the one place where a deity could nudge the universe into the direction it wanted to move in. A similar form of interventionism might be implemented via statistical mechanics and thermodynamics, with the deity intervening, Maxwell's demon like, to cause statistically improbable but still mathematically possible outcomes and stir the universe in that fashion.