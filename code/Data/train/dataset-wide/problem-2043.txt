Community wiki answer: I think the MSDN is fairly clear. The R Installer is on the SQL Server 2016 install media; it's the last item on the 'Installation' tab called 'New R Server (Standalone) installation'. Alternately you could follow the instructions for Installing R Components without Internet Access. Practice on a VM if you want to confirm. 

This finally did execute, but took 29 minutes, 16 seconds. The operation itself should be pretty quick (metadata-only) so I imagine that almost all of that time was spent waiting to acquire the necessary (schema modification) lock. With the new field in place, I was able to quickly add the default value to it via the script: 

Not clear to me if you are looking for synonyms. If so, and you need mere synonyms in the future, check out the word-choice tag on English Language & Usage Stack Exchange. You might need to be careful with column names like these as they might confuse a user, since they can have a different meaning than row status or record type. 

Change the data sources to use a SQL login and password, then you can use the built in subscription method. Use a scheduled app to generate the report, and have that do the e-mailing. The scheduled app would need to run under an account with permissions into SQL/SSRS. 

I solved adding a new column only of timestamp, I order the SQL based on timestamp and the results are from 1 Jan to 31 Dec. 

Community wiki answer: Postgres has a lot more NoSQL features than MySQL. The JSON implementation is much more efficient (you can index JSON documents) and it has a very efficient key/value store (also indexable). Postgres is probably the most advanced "NoSQL relational database". 

Community wiki answer originally left as an edit to the question by its author: The problem was that the database property was set on. The solution was to set to false. 

...each might get a good enough plan to make it worth while. If each query applies the predicate early in the plan, you wouldn't have to join so many rows that are ultimately filtered out. 

Community wiki answer: The problem was the architecture. We had 32 bits; we have now migrated our system to another server with SQL Server 2014 Enterprise 64-bit and all our problems are over - MelgoV (question author). 

Community wiki answer: You might try splitting this into two queries and -ing them back together. Your clause is happening all at the end, but if you split it into: 

It's probably not the execution plan that's making it go faster. It's the data. After you run a query, SQL Server may keep the data in cache. Therefore, it doesn't have to read from disk to get the information, instead it can pull it from RAM, which is much faster. While your execution plan may also be stored in the plan cache, I highly doubt compiling an execution plan is increasing your query execution time seven-fold. The plan and data will automatically stay in memory, if you use them frequently enough, and if there isn't any "memory pressure" (caused by too little physical memory) on the server. You cannot really force a plan or data to stay fixed in cache - it will be tossed out if memory is too tight. 

The problem may have to do with the Microsoft driver itself. Use an IBM provided driver instead. Instructions to install this can be found here. Vendor-supplied drivers are often far superior: more stable and with higher performance, compared with those provided out of the box by Microsoft. 

Community wiki answer: Z should be the virtual server name of the cluster. I don't use Server Manager so I have no idea what "current host server" means and whether or not it is supposed to represent the active node for the SQL Server clustered instance (which is different from the Windows cluster). If you had two separate FCIs and one was currently active on each node, what would you expect "current host server" to say? Does Server Manager represent each SQL Server cluster separately or just the windows cluster? I understand these are physical machines - this is a completely different concept. "Virtual Server Name" is the external name of the SQL Server failover cluster instance - it's how SSMS and your applications connect to SQL Server regardless of which physical node the clustered instance is currently using. I suspect "current host server" is something completely different and has to do with the Windows cluster, not the SQL Server failover clustered instance sitting on top of Windows. is going to give you the physical name of the currently active node vs will give you the virtual name of the cluster. 

Community wiki answer: I would try installing the SQL Server 2005 Native Client in the Feature Pack for Microsoft SQL Server 2005 - April 2006. Please note also that post says Windows 2000 Service Pack 4 is required to be supported. 

Community wiki answer: A table variable is an instance of a table. Tables are called relations in relational algebra - hence relational variable (or relvar for short). Base table means that they are not views which are derived from base tables. 

Community wiki answer: Yes, you should always back the system databases up. This is an essential part of any disaster recovery procedure, which every production server should have documented and tested. That said, some folks find it a bit less painful to script out the objects they need from system databases and then run the scripts where they'll be going, rather than restoring over system databases. 

Community wiki answer: A generic reason that affects those new to is forgetting the semicolon at the end of the command, so that it doesn't get executed. Otherwise your command looks good and should do what you want. 

...you should still be able to use separate and statements, for the same effect. Using the same basic criteria as in the , you should be able to: 

Community wiki answer: Modify the job step to record job step output, and configure the stored proc to some text at the beginning of the proc code. You should be able to tell if the proc is being called by looking at the job history with that in place. Seems like you have some troubleshooting to do. It's also possible the proc is returning an access denied error as a result of not being able to send the mail to the SMTP server. 

There are more details, including a bash script to perform the actions listed above in the linked post. 

Community wiki answer: Please ignore the sleep and queue waits. These should be filtered out as they are always going to be high, by definition. They collect a minute of wait every minute. tells me it can't keep up. That said, I don't see that as your major problem here. For latch contention, you might consider this whitepaper. However, I'd be much more concerned about the and waits. Are you doing a ton of cross-server stuff? Can any of this be consolidated, or are these things communicating with non-SQL Server RDBMS platforms? I don't think 2008 -> 2008 R2 should cause any severe issues, but you may want to be sure that you have updated statistics everywhere (with , preferably during a maintenance window), as this is a common cause for plan/perf degradation after an upgrade or migration. But I would also strongly consider getting on the most recent branch and installing the most recent CU available. You should also keep in mind that mainstream support for 2008R2 ends in two months. And while I hesitate to advocate Thomas' line of thinking that a clustered GUID is always better than an column, given that you have Fusion IO and your latches are still occasionally exceeding 2 seconds, you may need to consider redesign, because obviously throwing fast I/O at the problem isn't solving anything. See this post and this post for some edge case enlightenment (again, please don't consider this an endorsement of "heaps > clustered index" or "GUID > identity"). 

Does seem somewhat contradictory, but I agree it sounds like this is not expected behaviour for that field to be updated if errors were reported. On the other hand, your examples are quite minor issues, easily fixed by running so maybe SQL Server regards that as a successful run. 

Answer originally left as an edit to the question by the author: Solution Used After presenting different options to System Admin in charge of the project, decision was made to go with BitLocker drive Encryption. I stopped the SQL Instance during the encryption process and had no problem restarting it after encryption was complete. 

Community wiki answer: The long and the short of it is: Most workloads for SQL Server are OLTP which benefits from higher clock speeds because it is a serial operation. Unless you're specifically designing for a massively parallel system, clock speeds will always win out. Edge cases exist but that's the 95% of the time answer. The fact that it ends up costing less is also a nice thing. 

Foreign key validation has to occur under (locking) read committed for correctness. See Snapshot isolation: A threat for integrity? by Hugo Kornelis for details. The deadlock graph shows two concurrent executions of causing the deadlock. Your triggers are missing a join condition between and . It seems likely that this is an oversight and your trigger is accidentally updating all rows in so deadlocks are extremely likely to occur if there is more than one concurrent trigger execution. 

I am in the process of now setting all of the bits in the table using a user-defined function (see below). It is in progress and taking about 1 minute per every 1 million rows in the ~68 million row table. 

A backup contains data since the last backup. and backups do not truncate the log. From The Transaction Log (SQL Server) 

Doing it in that order avoids updating rows you've just inserted. operations do the and steps separately in reality, so concurrency issues should be no worse than with . Community wiki answer 

Community wiki answer: You could use a computed column for . No need to be actually stored. Would something like work for you? 

Community wiki answer originally left as a comment by the question author: I believe I have solved this problem but I don't know why. In short, instead of using SSMS on my PC, I remoted into the server logging in with the account associated with the new credential. I removed the cred, created it again (this time SQL Server didn't error at the ) and then I attempted a test execution of one of the jobs. It worked?! Apparently this changed something and helped SQL Server resolve the account. 

Other suggestions left in comments in case they help others: It's not normal behaviour to have to restart except during configuration changes. You should disable the in process option if you care about your core database stability - otherwise errors in the linked server driver can cause your engine to crash. It isn't trivial to get working and has a slew of knock-on effects, for example in how and what it uses to present security credentials across the network. Expect to spend a day trying to disentangle that if you go down that path. 

This is a pretty good case for Distributed Partitioned Views. It would be fairly easy to implement, especially if the archives have the same schema. 

You'll verify your full and your diff backups; and You'll see how much time your differential will take to restore. 

The update to your question shows that the differential backup was indeed compressed. Backup larger than expected That 10GB of data could be spread over a lot of pages, and those pages could total a great deal more than the 10GB you inserted (especially with poor clustered index choices). Note also that differential backups operate at the extent level, so any change to any page within an 8-page extent will result in the whole extent being backed up. Check the after doing log backup or use . If there is an open transaction since yesterday, your full and differential backups will be larger due to the included active log. There may be no open transaction now, but if a long-running transaction was active at the time of differential backup, the backup file size will be more than expected. The fact that your log grew to 500GB (and you take regular log backups) means the log could not be cleared for long time due to something (probably an open transaction). All the log associated with that transaction was added to the differential backup. If you want to check if it's so, try to restore your database into another (new) database, perhaps on another server. This way you will do 2 things: 

Community wiki answer: If the query is now parameterized, you should be able to ignore all the ones that still show constant literals - those are in the plan cache now, but they'll age out over time. In addition to the outer join that's not really an outer join anymore, is required, or is it possible there is a skinnier pair of indexes that could cover the actual output requirements (usually some subset of )? You should also encourage your developers to always use the schema prefix e.g. . Simple parameterization only applies to trivial plans. Also consider optimize for ad hoc workloads. Kimberly Tripp has some good guidance on cleaning the cache. Think of forced parameterization as a last resort, because it will affect all your queries, with a high risk of performance regressions. 

Community wiki answer generated from comments left by the question author: I was able to create a script to parse the JSON objects without using JSON or APEX JSON packages. 

The uninstaller is not always able to uninstall everything (do not know why), resulting in problems like you are facing. 

Backup the databases Uninstall the SQL Server Delete everything from the folders where the SQL Server components exist Perform a new install of SQL Server and restore the databases.