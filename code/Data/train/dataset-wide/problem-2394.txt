An auto increment column will always provide the highest available id for a new user in your case. However re-using missing numbers from failed transactions is not possible in a single step, since the primary key will probably have been used in other tables which will need to be updated. There are no performance issues associated with having missing numbers, and there are a common occurrence. This can only be an issue if you have more failed transactions than those which complete, however there I would recommend you set a larger size for your primary key. 

a) An Entity-Attribute-Value model approach to tackle the attributes of the different devices to a device type. Each device type will have a list of attributes whose values you track b) For each device type, you track the inventory details by serial number which corresponds to a single device. 

a) Write the >50k writes to a text file and then transfer it to same server as the database (for performance reasons) b) This is an optional step depending on how the data is formatted in the text file - transform the data into a delimited format and move it to a loading directory c) Use the LOAD DATA INFILE command ($URL$ to load the text files into your tables 

The major reasons why you would use a one-to-one mapping to break a large table into two are for performance reasons for example: a) The table has binary/clob/blob data in a frequently accessed table hence slowing down performance since the large columns are handled differently. b) The table has many columns which are accessed by different queries, hence the performance is degraded therefore you would move related columns into a separate table to improve on access performance However having many integer columns does not justify the additional effort of breaking up the table into separate tables and having to query them. 

In the backup script the views are first created as tables which are then dropped at the end of the script as each view is being created, so it seems that an error occurs while creating the views at the end of the script. However when a view is created there is a user who is used in the DEFINER clause of the view who may not exist in your database, e.g., DEFINER=@ I usually remove that clause in the backup script and it tends to work. To verify add the -v option when importing the database 

I have placed the answer here since I was restricted by the size of the comment field. This is based on an approach initially suggested by @FrustratedWithFormsDesigner. I would recommend MySQL 5.5 from a cost/performance basis, but PostGresql would work as well (free license). The two step approach to loading the data is as follows: 

a) Attributes - define the attributes for all devices (anything goes in this table) columns: id, name, description b) Item Attributes - defines the allowed attributes for a specific device - itemid, attributeid c) Item Definition - defines an item say Black Berry Torch 4500, Iphone 4S, Iphone 3S etc - id, name, description, categoryid (if you want to add categories like mobile phones, switches etc) d) Devices - the individual devices - id, itemid, inventorydate, deactivatedate, serialnumber ... (basically all other attributes for a device) If you want to track any other information on device transcations then you can add more tables linked to the device as you need. 

I would consider BI a business project, because the outputs are metrics that business is required to use. This is despite the fact that about 80% of the input is technical. In my experience, if the focus of the project is technical, then alot of time will be spent analyzing the implementation technology, yet if the focus is business them more time will be spent deciding which metrics are to be delivered, how often, how fast etc, and the technology will be selected based on the constraints. Infact most BI project start with database based reporting before outgrowing to full blown BI. Bottom line, for success focus on business, and bring in tech to deliver "goods" within budget and time constraints. 

I do not think that the location matters rather it is a matter of personal preference, and the MySQL installer will automatically update the my.cnf file for you. However I would rather go with D:\mysql as the installation directory because its lowercase, has no special characters (spaces, capital letters) and is not in the Program Files directory which you may need to clean out for a new install or upgrade. You may also need to add the D:\mysql\bin directory to the PATH environment variable so that you can run the MySQL commands from any command prompt 

I would recommend the following approach: a) Each developer has their own database that they can work with, try out different things, destroy and restore at will independent of the other team members. b) Have a shared database instance for integration testing so that the whole team can see the shared progress at any one time c) Use windows authentication to speed up the connection process, and the devs do not have to remember new passwords, and easier to track and audit Of course all this depends on the size of the database and what resources are available on the test server. 

I think the question of duplicated data depends on the meaning of the relationship between the two users. For example if the relationship is who is following who, then User 1 can follow user 2, but that does not mean that User 2 is following user 1. However if the relationship is where there are no duplicates, for example if the two belong to a team or work together, then the above model would not necessarily work since you need to relate them through another entity, team in my example 

Instead of object_id and object_type columns, why not use a primary key for each object type to be commented on, but maintain the object_type (for partitioning later) so that you have question_id, comment_id, and answer_id. For each row of data, two of the columns will be null, but it is easy on your ORM (single table inheritance with a type qualifier). Your queries will also join to the correct FK, I am not sure of the space usage for this, but I expect it to be very performant since the joins do not need additional filters on the where clause for different types as well as complex "SQL join acrobatics" 

From the MySQL Workbench 5.2 window, select Database -> Synchronize model which will allow you to synchronize the model (EER diagram) with the database (you need to create a saved connection) for it. I recommend this method over just reverse engineering the model from the database since it maintains the layout of the EER diagram. One caveat is that you need to add new tables to the EER diagram manually.