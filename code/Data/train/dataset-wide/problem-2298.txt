Long shot perhaps, but worth checking file \Program Files\Microsoft SQL Server\100\DTS\Binn\MsDtsSrvr.ini or the equivalent on your setup. You may have to manually edit it with the instance name. Otherwise SSIS connections might be looking for an msdb of a default SQL instance that doesn't exist. 

Shrinking log files for user databases might be OK if internal fragmentation is becoming an issue (search on Virtual Log Files for detailed info). Best to avoid it by setting up appropriate autogrowth sizes, avoiding growth in percentages as well. The defaults are inappropriate for any realistic data load. Shrinking tempdb log files may not be a good idea at all, I have even seen data corruption that I believe resulted from a job that was regularly shrinking the tempdb log file. Best to pre-size it according to the workload. 

Checkdb creates a snapshot in the background. Snapshots are supported by sparse files (they look large in Windows but are typically almost empty). Could it be that you are looking at this file? 

I have been working on this query for a while now, and almost had it done, but then I couldn't find a way to pass the "start date" to the subquery that allocates dates to table B. The docs say that 

This is just simplified out from the original code, which does some logic to check each statement for failure. I do not have it using transactions yet, since I still have yet to learn that. The variable contains my (semi) complex query that creates the ordered stack (and hence the need for all those aliases in the ). The column is set by the stack building query, and is set to whatever the originating table was. 

So I reimplemented the subquery to move the variable to a clause, using s, but this still didn't seem to help, since I couldn't figure out how to return just the single row needed without an outer query to do that. So here is what I have, in the hopes someone can help me figure this out: 

Yes, but only from SQL2012 onwards, if I remember correctly from Bob Ward's 2013 PASS session (gave me a headache!) 

Just my 2cents from my own experiments on 1-2 year old hardware: Read-only operations (DW-style scans, sorts etc) on page-compressed tables (~80rows/page) I've found to break-even at compression size reduction of ~ 3x. I.e. if the tables fit into memory anyway, page compression only benefits performance if the data size has shrunk by over 3x. You scan fewer pages in memory, but it takes longer to scan each page. I guess your mileage may vary if your plans are nested-loop and seek-heavy. Among others, this would also be hardware-dependent (foreign NUMA node access penalties, memory speed etc). The above is just a rough rule-of-thumb that I follow, based on my own test runs using my own queries on my own hardware (Dell Poweredge 910 and younger). It is not gospel eh! Edit: Yesterday the excellent SQLBits XI presentation of Thomas Kejser was made available as a video. Quite relevant to this discussion, it shows the 'ugly' face of CPU cost for page compression - updates slowed down by 4x, locks held for quite a bit longer. However, Thomas is using FusionIO storage and he picked a table that is only 'just' eligible for page compression. If storage was on a typical SAN and the data used compressed 3x-4x then the picture might have been less dramatic. 

Edit: I found an error in the logic, I need to return the highest result (date) in a series of nextdiffs if all return 1, or the first higher-than-1 result. It currently only would return the latter. So that needs fixing too. 

Ok, I solved it. I could not solve the actual problem of using the user variable nested in the subquery, as I think there is simply no way to do this (please comment, or answer if there is), but this is how I solved it, in case anyone is curious: 

Which confirms what the docs say. However, I don't know how to modify my query (the first one) to get the result I want. 

is the starting (minimum) date. The subquery is the one inside the , and tested outside of this, as its own query, it works perfectly, when specifying a literal value (or setting the variable in a ). As a test, I did: 

I'd be very careful using this flag on a VM, as their memory has an extra level of abstraction. Had more than enough trouble with it on physical servers with lots of RAM dedicated to SQL. Example: with 3 2008R2 instances co-hosted when restarting one of them it took forever to come back because it could not find contiguous memory segments anymore. The performance benefits were neither here not there (lets say 'statistically insignificant overall). I treat it as a 'TPC special'. Also consider that 834 doesn't play nice with columnstores either. 

There is enough info for you to estimate roughly how many pages/extents were lost (deallocated by the REPAIR_ALLOW_DATA_LOSS option). What good is that though? Without backups there is no natively-supported way to recover the data. What logs are you referring to? Transaction logs or Errorlogs? TLog entries need to be interpreted (not for the faint-hearted) and then applied to a consistent database file (which you haven't got). Errorlogs are useless for data retrieval anyway. 

So, in case you missed it, the clue was to use with a subquery in the clause to detect gaps, which simplified the entire approach. 

I have just recently learned about user variables (ie, @myvar) and am trying to create a complex sorting query that takes 2 tables where the second is missing a column which then gets filled in by a subquery. In this subquery I need to generate a number by doing a lookup on the first table and modifying the result. More specifically, I have a table A (id, date), and table B (id, weekday) and I need to return a date for each row of B where it is the next date occurring on the same weekday where such a date does not occur in A, all starting from a specific minimum date, where entries in table A are all higher (ie, later) than that date. Table B is sorted by staggered weekday (First Monday, First Tuesday... Second Monday, Second Thursday..) and then these get "slotted in" (by assigning "date") where that date in A does not occur. So if the two tables look something like: 

If performance is important, Option1 has the clear edge. Every time a query goes across a linked server, performance takes a hit as what you expect to behave like set-based operations become serialised by OLEDB. They go row-by-row. It helps if you follow best practices for linked server queries (OPENROWSET for example) and ensure all remote processing is done on the other side, but the results will still come across serialised. (Look for Conor Cunningham's presentation on Distributed Queries). If all your remote tables are small lookup tables this may not be a serious issue, but if millions of rows have to come across linked servers then performance will suffer. There used to be an extra layer of problems with invisible statistics (for data readers) over linked servers, but as you're running SQL 2014 this will not affect you. If you can afford the Dev cost of eliminating the linked servers, Just Do It!