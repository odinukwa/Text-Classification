The following is an untested attempt at distilling the problem into its basic components then unions the results together. The last query (with the correlated subquery) can be turned into a join if the introduction of orders does not cause a Cartesian product. I can not test with what is provided to see if this is a valid assumption. 

@rathishDBA has a legitimate point, but sometimes intensive disk I/O must be performed on the database server. My recommendation is to run the job at reduced priority... To start a process at reduced priority (default nice is 0 - kernel runs at -20, +20 gives process lowest possible priority)... 

@a_vlad has your answer... create a new table with the same structure without keys and autoincrement (it is faster to manage indexes in batch than one row at a time). Then insert into the new table select * from the old. After population, any indexes (including the pk) can be applied and add autoincrementation advanced to the next value you want to generate. 

It seems the most important thing is matching time out with the time in. The query below shows one way this can be accomplished. The approach below attributes time worked to when the individual "clocked" in... there is no attempt to attribute the time to the day in which it occurred (there are other questions on this site that address that need). 

one thing you got to keep in mind is: if you materialize the count, you might have some fun on the concurrency side. however, a materialized column is of course always a lot faster. 

the crux might be your loop. you can get rid around that one easy: there is a function called unnest, which can transform an array into a table which can then be joined or whatever (via a LATERAL join maybe). you are on the wrong path with your approach. if you really want this loop and so on, you need a set returning function and use it inside COPY. 

i would not see this as a major issue. of course things are a little larger but if we talk about a single integer column things will most likely be lost in noise anyway. we got 24 bytes of tuple header ... then comes the data. if things are 4 bytes larger? what difference does it make in a 10 column table. so, no worries ... make sure that you do what is good for your app. 

yes, this is possible. however, you have to be a little careful. DDLs in a stored procedure USUALLY work. in some nasty corner cases you might end up with "cache lookup" errors. The reason is that a procedure is basically a part of a statement and modifying those system objects on the fly can in rare corner cases cause mistakes (has to be). This cannot happen with CREATE TABLE, however. So, you should be safe. 

What is the fastest way to determine if an IP is contained within a CIDR block? At the moment, whenever I store a CIDR address I also create two columns for starting and ending ip addresses. The starting and ending ip addresses are indexed. If I want to see which network contains an address then I look which seems less than desirable. It occurs to me I can store the right shifted number and could match similarly shifted IP address (660510 in the case of @cidr)... 

I highly recommend partitioning your table now. I'd create a new table partitioned by year that looks just like your current table then insert into it. Going forward, you can more easily prune your table. Here are several links to a good partitioning discussions... $URL$ $URL$ I'm not a SQL Server guy - I saw an MS SQL Server 2016 documentation suggests there are some differences in 2014 features - but I can't find 2014 documentation. Here is a link to MS SQL Server 2008... $URL$ Good luck! 

An approach using shifted_netmask (undesirable - I'm incurring a full table scan to discover the number of bits in the netmask)... 

"show create view" will provide you the DDL to recreate the view exactly as it is (complete with select statement). 

You can either recreate your table or use "optimize table" command to rebuild your table. Highly recommend making a backup just in case. $URL$ $URL$ 

what you need is the ts_headline function. it does exactly what you need it seems. here is the documentation: $URL$ 

from experience i can say that one large delete is usually better. however, there can be some corner cases with IN, which might invalidate my statement but basically this is true. Make sure your got enough work_mem around to allow PostgreSQL to nicely hash the IN. 

we also got the impression here at cybertec that more recent versions of the linux kernel (later in the 3.x series) are less likely to show those symptoms. 

in your case the second subselect can never have the same rows as the first one (due to the empty column) so UNION ALL is already definitely fine here (saving a pointless duplication filter). 

as far as i know there is no such tool around. however, you got a couple of choices: a.) use oracle_fdw and just join the stuff or b.) export data to text files and use a simple UNIX diff. it works like a charm usually. 

to me the most important thing in this context is to use transactions. PostgreSQL supports transactional DDLs and people should make heavy use of it. the rule of thumb is: COMMIT stuff to the real system as late in the process as possible. this avoids the need to rollback stuff and to produce conflicts of all kinds. so transactions are really the key to making things work in a reasonable way. deployment should always happen in a transactional way as well. one thing is also to use includes: 

This behavior could be achieved programmatically... Create a table of databases and the status you would like them to be.. then your application can test the status of the database prior to use.. the application proceeds if the status is available and returns a message if unavailable. You could also do this with applications in general too (same thing as described above but with application instead)... You can then control applications use of the instance. 

This is an ugly solution that may introduce other issues. It is offered as a means to eliminate the lock you are experiencing. Auto_increment is preferable to managing numbers in this manner. The number could be managed in its own table incremented as needed. This table could be locked at no detriment to the main table. When write locked, other sessions are prevented from using the table, the locking session can then increment the number (and store it in a variable - something meaningful like @next_position) then releasing the lock immediately after the update. The number is then used in your main query - no join, simply "LQ.initialPosition = @next_position". You may still have contention on the position number table - but these locks shouldn't be held too long - only during the incrementation to ensure only one process increments the number at a time. The locking scheme could be eliminated, but this would require knowing the current number first which would then be tested in the update's where clause to eliminate race conditions with other sessions. A loop will be necessary to try again if the update doesn't work (another session made the update first). This complicates the use of the table but reduces the potential for trouble. 

shared_buffers are statically taken at startup and are never resized. effective_cache_size is just a hint to the optimizer. it is never allocated. it merely gives a hint of what is going on. so shared_buffers is what you see as taken. 

this should work somewhat efficiently if you need it for all. if you need it for jsut one folk you should go for ypercube's answer. 

this is the best way as not all the data has to be transported over the network. i definitely recommend this replacement for pg_basebackup because it is the only RELIABLE way to get things back in order (nobody knows for sure what happens before the crash). sure, there might be other tricks out there but i would strongly vote for this method. 

it allows people to work on various files concurrently. it makes using git and so on a lot easier. also, consider looking at CREATE EXTENSION. it allows a pretty nice way to handle versions and all that. i strongly encourage to use that one as well. git is also a good thing to have in general. 

a sequence makes sure that values are ascending BUT it does not make sure that it does not contain gaps. it is also important to notice that a sequence cannot be rollbacked. you cannot have strictly ascending and gap-free at the same time as it would not work with a mix of long and short transactions. therefore a sequence should never be used for an invoice-id and so on. 

Otherwise it would be really useful to see disk activity from AWS, amount of data server currently writes (you can calculate it using delta between two consecutive runs) There are other ways too, but for that, I'd need a lot more details on the actual workload. 

It's a fairly safe bet that some of the tables have (including system privilege tables) have structure that MySQL 5.5 can't recognize. MySQL 5.5 and 5.6 is a major version difference (regardless it looks minor) and they are not backwards compatible. Try adding to to see if the problem is only with system tables (when you add , privilege tables won't be used at all and anyone will be allowed to connect from anywhere with any password). If MySQL would start, that means you have a broken privileges tables and should help you load the blank ones. If it doesn't help, you may have InnoDB structure changes that MySQL 5.5 can't recognize. In that case, you'll have to use logical backup (mysqldump or mydumper) for such downgrade. BTW, you can use any of the working slaves for it. BTW, if you're using on the master, there's no need NOT to use it on slave. In fact, it doesn't make much sense to. 

There are many other things you can do, but I try to live by the rule: don't fix what ain't broken. Good luck! 

no, pgstrom cannot do that. it only able to scan relations - it is not able to join. what you an do is pack an operator into a GPU routine but = is not a good candidate for that. 

i think you are facing spinlock contention. you can verify this using "perf" (in case s_lock shows up on top my guess is right). in general try the following: 

you can make use of the so called ctid (which is basically an internal thing but for a lousy hack it should do. here is an example: ' 

in PostgreSQL you should use a tool called pgbench. it allows you to run custom scripts with an arbitrary number of concurrent requests and all that. it is also able to create random values for your script and all that - it is really powerful. for the test: make sure you got a proper setting for -j (number of threads used by pgbench) so that pgbench is able to create the load. also make sure the system is properly tuned (especially synchronous_commit set to identical values, checkpoint_segments, shared_buffers). make sure you got proper fillfactor settings on those tables and ensure that prepared plans are used (pgbench can do that for TPC-B). if you got it right, it expect MySQL to be beaten by a wide margin in a high-concurrency read / write test (transactional of course).