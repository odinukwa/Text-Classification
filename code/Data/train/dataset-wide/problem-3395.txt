If you can physically touch the drives, you should be able to determine which one is clicking, since the click is (I believe?) caused by the motor moving the head to the end of the platter repeatedly. 

The CentOS / RHEL version of ProFTPd does not come with mod_sftp built-in. You must recompile it with support. There is a great blog entry here, covering the topic: $URL$ I should also note that, most of the time, sftp is taken care of by the OpenSSH daemon. ProFTPd's mod_sftp is a relatively new development, and probably not documented as well. 

I want to start scheduling remote mysqldump crons, and I'd prefer to use a special account for that purposes. I want to grant that user the minimum permissions for getting a full dump, but I'm not sure the best way to go about that. Is it as simple as 

If you've got a drive flashing, and you're not sure it's bad (it should mark it as failed in the bios setup screens), try swapping its place with another of the drives that bring up a green LED. If the amber light follows it, the drive is probably bad. Assuming that's the case, you are left with 2 73GB drives and 2 146GB drives. Assuming that you don't have any to replace in the blank spaces, I'd do a RAID 1 of 73GB and a RAID 1 of the 146GB. If you're going to use this for production, you're going to want at least one more 146GB drive to act as a hot spare. I believe that the 146 drives may be able to act as a hot spare to the 73GB mirror, but I'd test that before you rely on it. If you're in a drive shopping mood, you should be able to find those at a relatively low price. Just search Google Shopping for the model, and you'll also need to search for "drive carriers" which are the sleds that the drives live in and allow hot swapping. If you do end up buying more drives, forgo the 73 and just get the bigger drives, then make a RAID-5 array (with one hot spare). Although RAID-5 is generally frowned upon for "modern" drives, your 146GB drives will be fine. EDIT OK, as icky2000 linked to, essentially drive capacity has exceeded the unrecoverable read error (URE) rate, so that "modern" drives (1-2TB+) in a RAID configuration are statistically likely to suffer an unrecoverable error during the RAID rebuild. In other words, if a drive dies, and you get a failure during rebuild, your data is gone with RAID5. 

1: You should have no problems swapping existing disks for your own disks, as long as they're of the same interface 2: Unless something has drastically changed, empty drive bays have blank faceplaces installed to improve airflow, but do not come with empty drive carriers. You will have to order the drives from Dell to fill them, or get them from Ebay. I believe your R410 is newer, so you'll have more difficulty. You might be able to find resellers by searching Google. 3: No particular problems jump out with the exceptions of reliability and performance hits if you only have one spindle. If you purchase additional drive carriers and use a RAID layout, you'll get better performance (if you plan your RAID right, anyway) than is possible with only one drive. Not to mention that single-drive failure in your situation would seriously cramp your style. 

I'm configuring a laptop that will function as a "floater", or spare, for whoever needs one in the office. Because I want people to be able to work pretty quickly and not spend a few hours setting up their profiles, I was wondering if there is an equivalent to Unix's /etc/skel in Windows, where I can set initial new-user configurations? The OS is Windows XP SP3. I'm not to the point where I can use centrally managed profiles yet, but that will come along shortly. I'm hoping for a band-aid until then. Any suggestions? 

Sure, you can join the domain with the host server, but it's a really bad idea. It's definitely a "chicken and egg" problem. As the people in John Gardeniers' link found, in order for the DC to be up, the host has to be up, and in order for the host to be up and function correctly, the DC has to be up. Someone found an answer in making the server start automatically, but I'd highly recommend against that solution, because there are too many things that can go wrong. Get another machine to function as the DC, make it independent of this stack of machines, and consider bare metal virtualization if you're looking to run production machines in VMs. 

I would also recommend calling ssh with the full path to make sure you're running the correct executable. 

It might also be worth your time to investigate an MPLS network. Most large providers can provide you with this service at a nominal fee above the normal price of the bandwidth. Essentially, MPLS is point-to-cloud (diagram cloud, not magic-server-hosting-cloud), but you get your own private cloud. So suppose you have three offices, A, B, and C. Each of them have a, say, T1 to the MPLS cloud, and the provider itself routes traffic through the cloud to the destination, without ever touching the internet or anyone else's traffic. The benefit of this is that you don't need to use VPNs on your MPLS links. They're inherently secure, since it's a private network already. Lots of people do run encryption on their links because they don't trust their provider, but that's something you have to judge and weigh the price versus how important your data is. 

If an IP-KVM solution is in your budget, I'd go for that, particularly if you have the ability to remotely trigger power cycles and the like. Can I recommend getting the type that doesn't use thick KVM cables, but instead uses CAT-5 or the like? The big cables are a PITA to run and deal with, where as we're all pretty much used to CAT-5 by now. If you go that route, keep yourself sane by color-coding the cables so that your KVM lines don't resemble your network connections. Serial cables do work, but the ones I've used are susceptible to interference, plus the BIOS has to support redirect-to-serial (many do now, but there's always an outlier). 

In my server closets, I always put one of those cheapie push-on battery operated lights. It's great if(when) there's a power outage. 

If you're rebuilding your RAID, A) You should make backups first B) Your performance will suck. You'd be better off to move any critical services to another host while this is happening, in which case yes, I'd turn off the various daemons. 

I'm building VMs in my lab and I want to replicate the situation that my fileservers will be encountering in production. Here's a brief overview of what I want to do. I have multiple ESXi 4 servers. They are accessing a SAN, and each virtual machine will exist in its own LUN on the array. I also have LUNs which house data that needs accessed by the VMs. My goal is to have a VM (lets call it fs, for fileserver) started on VMhostA. I want fs to be able to access a data LUN in raw mode, meaning that if fs goes away, I want to be able to mount it as ext3 (or whatever) from a physical (non-virtual) machine. This means that the data on the LUN must not be housed in a vmdk file. In addition, I want to be able to use VMotion to move that VM to VMhostB (and obviously, maintain access to the data LUN) It is my understanding that I need to present the LUN as a Raw Disk Mapping (RDM)? From what I have read, the RDM file created can be stored with the fs virtual machine or on another datastore. Would it be correct to assume that it should be stored with fs? Also, there exists two types of RDM, it seems. There are physical and virtual modes. I have encountered conflicting documentation from various sources, so I'm not sure what to think. Does it matter which I select in this case? What are the differences, as they apply to my situation? Thanks very much for reading all the way through ;-) 

Here's a crazy idea, but why don't you go ahead and change the notification? What is sending out the error? Most monitoring systems allow you to specify the time span you want to be alerted during. I would work on it from that end, since disabling the mail daemon to stop a client from sending mail is backwards. Change that which has the least effect. 

I was having problems with mine at 512MB of RAM until I switched to FastCGI. That made the performance improve a lot. I had a 30MB free (not counting cache, of course) until then, and I ended up with over 100MB free. Your mileage may vary, of course, depending on the traffic of your site. And once the traffic starts to turn up, you can switch to nginx. To buy myself some comfort, I upped to 1GB. BTW, I'm hosted at prgmr.com, and I haven't seen anyone touch their prices yet. 

I have a sneaking suspicion that this may be a bug, but I'm definitely willing to entertain the possibility that I'm doing things wrong. I have a VMware virtual machine in $vm, and I'm trying to assign a boot CD for it. $vm is powered off. The documented method seems pretty straightforward: 

Windows has an entire suite of tools to remotely administer Windows machines. In addition to the remote administration tools in the admin toolpack, you can use something like psexec or even powershell to issue remote commands. As for automation, I'd recommend powershell. All of my Windows-admin buddies swear by it. 

I remember wondering these exact questions, too. Here's the scoop: A server sees an iscsi slice (called a LUN) just like it would a hard drive. It addresses it as /dev/sdwhatever, and you partition it and use LVM (if you want), and create a filesystem on that device. It works great. Now, it gets complex when you want multiple machines to talk to the same drive at the same time. It would be like hooking a USB drive up to a bunch of computers at once. Madness. Cats and dogs laying down together. Insanity. Now, it happens that there are some ways to get multiple machines to talk to the same device, but you have to use what's known as a "clustered" filesystem. This is a filesystem that knows that multiple computers are talking to it, and it accounts for that. It does this through a few possible ways, including multiple journals (one per machine, usually), or using a lock manager (which is one specific computer that acts like a traffic cop), but whichever way you go, you're going to have to cluster all of the machines that you want to talk to the same LUN. What most people typically do, if they want really high availability, is to cluster three (or sometimes two, but it can be harder) machines to work together as a file server cluster. Those three machines are the only ones who write to the LUN, but their clients use NFS, Samba, FTP, or whatever to access the files there. I actually did a Redhat Cluster HOWTO a while back ($URL$ but I never got it successfully working and stable. Now I've got a couple of machines that are set up as fileservers that don't have the LUN mounted, but can at the touch of a button. It's a trade off for me, since I didn't have the time to learn the clustering suite. 

Replacing the bundles would be expensive. If you've got it in your budget (or insurance will pay for it), do it. As long as there were no nicks in the PVC sheathing, you should be alright ($URL$ It's hard to tell if you had any nicks in the plastic, but I imagine that you're going to find out. Do you have any spare cables in the bundles? I would save yourself the time of panicking during an emergency and find and label them now. Create documentation on how to replace a poorly performing cable with a spare. Know that if you don't rerun the cables, this event is going to be the scape-goat for all future network performance issues forever, until more cables are run. 

Alright, I'm going to take a stab at this, because I think my idea makes sense. You are dealing with multiple caches in this case, and that's what is tripping you up, I think. The first thing that rsync does is to determine which files it needs to transfer. It usually does this by spawning an instance of rsync at the remote side, reading the metadata for each of the files in the directory on the source, while at the same time reading metadata for the local files, and then the two metadata sets are compared. Anything newer (or different, depending on the rsync options) gets transferred. You don't have a "remote side", according to rsync. You're working "locally", so it will iterate over both directories, the source and the destination. This is very disk intensive, particularly with a ton of small files - the more files, the more discrete disk operations. This causes a lot of disk thrashing, plus it fills the cache with the metadata from those files. Notice that this happens all the way down the stack. Your local machine caches metadata from the FUSE filesystem you've got mounted over ssh AND the local directory. The remote machine caches metadata from the local disk mount. And the VM host that your remote machine is running on is almost certainly overcommitted and giving you ballooned memory. I suspect that it's very likely that you're crossing thresholds when it freezes, and everything has to catch up and either decache or swap. I would be very interested to see if this happens when you do rsync over ssh without the disk mount. 

The next line is the segfault message. On my CentOS 5.3 machine (rpm version 4.4.2.3), my rpm immediately does this: 

As John mentioned, you can obtain a key for a certain number of activations. Each time you install a new ESXi machine, you activate with the same key. 

Note that I have not tried this, and I am not an apache guru, but assuming DaveRandom is right, my next attempt would be to use a .htaccess file in directory /A/, then use URL rewriting to point things to /A/child, with the logic that /A/child would never cause a directory traversal to /A/ to read the .htaccess file, since the directory interpretation occurs later in the process. Of course, if that works, it's probably a bug and will be fixed. 

I'm sure some more VM-savvy people will chime in here, but it sounds as though the delta file is doing what it is supposed to. As far as I know, snapshots work via copy-on-write, where you start out with the original image (that's your vdisk) and an empty file (that's the delta file). Every time anything is changed at all, that change is made on the delta disk. The original disk isn't touched, that way if you ever need to revert to a snapshot, the delta file is thrown away and everything is read from the original vdisk. Over time, this leads to the side effect of the delta file growing massively as things are changed, added, and removed. As I understand it, if you add a 10MB file, the delta file grows by 10MB. Remove that file, and it grows by another 10MB, because there is a 10MB difference. I could be wrong, and it might actually shrink by 10MB, but I don't think so. (Please someone correct me). If you consolidate your changes into your snapshot, you'll be back to the original 50GB disk image by itself. Of course, I could be horribly wrong and mistaken, in which case I'll be voted down, and you should listen to whoever comes in and knows more. 

Technically, unless your cable provider is blocking access to the ports that you want to serve (probably 80 and 443 for http/https respectively) then there's nothing stopping you from hosting your own . Seth said as much. As pretty much everyone else said, it's a bad idea. So what would it take to make it a good idea? Well, first, know that you're never going to get as fault tolerant as the expensive colocation facilities. That's why they're expensive. They have things like redundant power sources, redundant generators, redundant network connection, redundant everything. You can't afford this. Instead, a close approximation might be... A home with a section of the basement dedicated to the servers. Get a cheap four post rack, and buy rack mount servers. You can buy 2nd hand or from cheaper places like Supermicro, etc Get a decent quality UPS, and make sure it's enough to power the servers and network equipment for 15 minutes or so. Get a leased line for bandwidth. This is the expensive part. A cable modem that gets you 10Mb/s down and 2Mb/s up might cost $50 a month. A T1 that gives you 1.5Mb/s down and 1.5Mb/s up will probably run you $700 a month, if you can find a provider who will install it in your house. Depending on the area, you might be able to find metro ethernet for $1500 that gives you 10Mb/s both directions. Buy environmental sensors to track the temperature and humidity of the area around your servers. Homes aren't meant for this sort of thing, so you have things like waterlines above servers that can break and dryer vents which cause moisture and heat. Sensors will help you maintain a good working environment. Get insurance. The kind of insurance will depend on if it's a business. Here's a hint: create a business.