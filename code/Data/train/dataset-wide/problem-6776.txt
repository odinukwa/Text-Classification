I suppose you mean a rule-based parser since nobody would think of developing his own statistical parser (there are so many good open-source libraries). Building a parser is quite complicated. The best way is to have a context-free grammar (CF parsing is trivial) and build up the dependency structures via constraint rules. This is how LFG works, whose f-structures are just plain old dependency trees (in general they are DAGs but can be thought of as trees with coreferences). If you don't want a context-free backbone in the parser (which doesn't make much sense for most Indo-European languages), you should devise syntax rules based on feature constraints. The dependency tree of a phrase or sentence is a rooted spanning tree over a graph whose nodes are the words of the phrase with edges representing possible dependencies that conform to the constraint rules. In Latin, for example, one would say that an adjective depends on a noun if they agree in case, gender, and number, such as puella pulchra (nominative), puellam pulchram (accusative), etc. Likewise, verb phrases would be constructed via constraints. In a sentence like tu pecuniam debes, the constraining rules must state that the subject of a verb is in nominative and its direct object in accusative. But note that in most languages you'd need a ton of word order rules (that's why it's better to use a context-free backbone). Moreover a good parser needs a lexicon with valency frames to resolve ambiguous structures. A simple parser can be quickly developed in Prolog. 

In Greek the article is never attached to the noun or adjective. In this form it often coincide with the vocative case but for these nouns it's an exception and it just it nominative without article. When found in this form it implies an indefinite article but not a number (that is in English it would be but not since in Greece it's just one word). 

is incorrect. The 3rd person singular of an English verb would be "he asks", "he " and "he is". Id don't know any dictionary to use this form. So, it probable the infinitive rather than the 3rd person singular although in English it's hard to guess sometimes. See wikipedia article also: 

In this case the outermost f-structure is the IP whose functional head is the predicative phrase. Copulae are (in LFG parlance) coheads, i.e., they have no PRED though they extend the predicator of the content word they fuse with. In this respect they're somewhat similar to complex predicates such as causatives which extend the predicator (by adding the causee which is always a NP and changing the GFs of the direct object if present). At c-structure though copulae are (again in LFG parlance) categorial heads governing the predicative NP or AP (which is not really a complement thought this is merely a terminological issue). The analyses are by no means conflicting, in fact the two structures mesh pretty well together (by structural correspondence). Note that c-structures capture only configurations ("hierarchical" word order) whereas f-structures capture only dependencies (they're tree-like) so they're disjoint and there's no conflict. 

The use of different alphabet or an alphabet different than an already known well enough. To make it clearer: I am Greek native speaker, we use Greek alphabet which resembles the English one but it's obviously not the same. So, since my English are good enough I can apprehend German for example relatively easier being familiar with English alphabet than if I weren't familiar with. If in the language learning you also include pronunciation, then the closeness to your native or a well comprehensive language is also important. By this I mean only phonetic closeness. Example, Greek and Spanish are two distinct languages with different alphabet but really close phonetically: the only real differences is that Spanish do not use the 'z' sound and have a rough 'rr'. This fact make me learn easier this language in the sense that I don't need to invest time learning the pronunciation of various words and also I can understand quite easily the words being spoken. Another aspect that I can think of is the vocabulary shared between the language to learn and the already comprehensible by the learner languages. Example: English has made a really good job at inserting a huge amount of Latin-originated words through French mostly. This makes really easier for a person knowing English to find the correct French word. Thus he must just adopt to the French case system (non existing in English). You don't clarify what the previously mentioned relationship means so if it doesn't include the inflectional system of language to be learned. For example English native speaker find it harder to understand adjective cases than a person with an already comprehensive knowledge of language having cases. 

Yes, there are. Head-marking languages generally allow for free word order in case the language is caseless. Macedonian pops to mind, a language without cases on nouns but with free word order. Grammatical relations are indicated by clitics attached to the verb. Likewise, Northwest Caucasian languages have free word order and little morphology. In Circassian, only specific NPs are marked for case; Abkhaz lacks cases altogether. 

There's a purely logical definition (given in the MIT Encyclopedia of cognitive science): If sentence s is uttered in context c, then p is a presupposition in s if c entails p. This definition is more or less identical with most linguistic definitions and is equal to that of Jerry Hobbs if "entails" is taken to mean "abductively proves". Rephrased less formally, presuppositions (topics) can be inferred from context (that is, they're predictable from previous discourse and/or shared background knowledge). The remainder of the sentence is focal ("preferred content" in the MIT Encyclopedia mentioned above). Informally it's often said that the focus of a sentence is what's being said about its topic. In formal logic, focus is then taken to be an "Aristotelian" predicate. The unmarked sentence John sings (John topical, sings focal) is formalized as sing(John), whereas JOHN sings (John focal, sings topical) would be λP.P(John)(sing). Discourse-configurational languages assign topic/focus structurally. Hungarian is said to be one (Kiss, who coined the term "discourse-configuratonal"), other examples include Russian (King) or Georgian (Meurer). This approach only accounts for nonemotive sentences since intonation can mix things up. In most languages word order is more or less iconic with respect to information structure. Aside from word order and intonation, some languages have morphological discourse markers. There are topic markers in Japanese and Korean, focus markers in Eastern Armenian and both in (many dialects of) Quechua, to name just a few. As for formal representation in frameworks, FGD uses an ordering on nodes in deep syntax trees to express information structure. In LFG, there's a separate i(nformation)-structure for discourse functions. In the abductive framework of Hobbs, there's no implicit formalization but whatever can't be inferred/proven is taken to be focal. 

is the main factor which determines the difficulty to learn a language. Other factors that can affect someone trying to learn a language is: 

How is it scientifically defined? I don't think there is a large science behind it, it's just a convention that is used to refer to words and search them easily. It's like asking why the words are look up in this alphabetical order and not another order, e.g. by phonology etc? 

The second part of your question which you express your doubts about the inconsistencies of the criteria to give lemma status to a word, is exactly that. 

I think the major idea behind the linguistics perception of all languages (except the pidgin ones I guess) is that all languages are good to what they are made for: to make people using it communicate clearly and effectively. That said all languages include all words and meanings necessary to achieve the communication. So, when people compare languages, words, vocabulary size etc they often project to a different language idea of their own language. Example of the latter are when we are looking for a specific word to a different language that we cannot find. Or that the equivalent word is broader in sense than the one we are looking for. This often reminds me about the story of Eskimos having a huge amount of word to refer to the snow (I don't know how many words there are and I guess it's probably true if we consider they live an environment full of snow) while other languages does not have this diversity. For me apart from your opinion (with which I agree) that the 

Ditransitives of the English type are very rare. Most languages use either case marking on nouns to signify grammatical relations or polypersonal head-marking. By "English type" I mean that both objects can be passivized. Your example can be paraphrased as "A book was given to Mary by John" or "Mary was given a book by John". 

Any context-free parser can be used. However "pure" CF grammars aren't practical for real applications. I'd recommend to use LFG or a similar tool that generates more useful underlying representations. 

It actually came form Late Latin (e.g., probatum habeo). It's a natural process, a similar construction with "to have" has developed, for example, in Northwest Russian which is very interesting because Russian has no corresponding verb so it had to resort to its "у+NP" construction (e.g., у него корова подоено "he has milked a/the cow", lit. "at him cow milked") which clearly shows the origin of this tense is semantic. 

So, let me explain that faith is πίστις but hope is not ἐλπίζω but . Ελπίζω is the verb not the noun. The etymology of those two words is not related: ἐλπίς < ἔλπω (make someone have hope) πίστη < πείθω 

I am not refering to other factor that are assumed to be equal for all subject learning a language: Living in a country which the language is spoken or having friends speaking the language etc. One good link for language diversity (and you can make your own conclusions over complexity what ever sense you give the word yourself is this) 

All person except the 3rd person singular coincide usually with the infinitive, with possible exception the verb "to be": I am you are he is BUT to be. Anyway, to your question now the same link of Wikipedia just answers your question but I guess you validated the answer as complete enough: 

In the lexicon, ||loves||=λx.λy.love(x,y) and ||obviously||=λP.obviously(P). On this view, syntactic composition is function application (hence the name "functionism" for this approach). Some say that lambda calculus is unwieldy for implementing meaning assembly because it isn't monotonic. But it's a wrong approach to use lambda calculus at the level of surface syntax. (Linguistic) meaning is part of deep syntax, hence it should be assembled there. Deep syntax structures are unordered (or can be viewed as unordered for the purpose of semantic representation) and thus a λ-expression can refer to grammatical functions raher than the order in which syntactic structures are built up. In glue semantics (which uses linear logic) the meaning of "love" is taken to be