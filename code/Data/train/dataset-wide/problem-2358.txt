It's not necessarily the number of transactions, but the timeout. The parameter is set to 60 seconds by default. The purpose of this parameter is to avoid having distributed transactions in a long running wait status while something else is performing work on that row; the transaction will wait 60 seconds, then Oracle kills it. You can modify this parameter (requires an instance restart) to whatever you want (in seconds). 

Looks like Tom Kyte has the answer: $URL$ Gandolf, you were technically correct according to what Tom responds with, but for whatever reason (bug? user error?) I couldn't get that to work. So I tried the workaround: 

Assuming you mean that the database is "missing" logs from P->R, and that your primary database no longer has those logs either, then the answer is that you'll have a functional database (upon standby activation as a primary) but you will be missing data from P-> the Activation point. The thing to remember here is that your standby is only as good as your last archive log that was fully applied. Without intervention on your part, your standby database is essentially frozen in time, and will never advance. If you're missing archive logs, and they are truly unrecoverable (did you check storage backups? Backups to tape?), then your standby, IMO, is worthless as a production level standby. You, and your business, may have a different opinion based on your usage of the database in question. But if this was truly a production level standby database that is meant to take over for production in a disaster scenario, personally, I would rebuild the standby immediately. 

What this achieves is that I could now modify the new "matches" table and for example add a column or rows, which do not need to get presented in the "reconciliations" view (by adding a where clause to filter them out). I am on Postgres 9.5 so the view is automatically updatable. Initial tests shows that there are no immediate problems with this, so I am asking this question to know what kind of problems I should be looking for. Performance is not a big issue. 

I just spent one hour debugging a query which had previously worked but suddenly created strange results. The cause of this bug was that I have a table in one schema which I reference in the query. Someone had, however, added a view in the current schema which has the same name as the table in the other schema. This caused the new view in the current schema to "take over" the reference in the query causing it to reference the view instead of the table. Obviously this caused me to start looking in the wrong place for this bug and getting completely confused. My question is therefore, how can I prevent this from happening? For example by enforcing that all views an tables in the database needs unique names. By enforcing this, I mean that it will cause and error if one attempts to create two objects with identical names. Because I am using some external tools, I have set up postgres to search for tables in multiple schemas (I have forgot how I did this), which is probably partly to blame for this mistake, but I cannot change this. 

Take another look at the output from your first command. There is an error messag in it that points you to the fix. Look for this section: 

Here is an article which goes into a bit more depth on the page contention issue. OK, so now the philosophy part... :-) For myself, if I am on an SMP system, I only want as many files as half the total cores. If I am on a NUMA system, then I only want as many files as cores per NUMA node. However, I rarely see any improvement for having more that four files for TempDB. So I usually start with four and monitor contention as explained in the article I linked to. If I continue to see problems, then I would add two more. Check again, add more, and repeat until the contention disappears. 

TempDB is shared amongst all the databases on the instance. So there can sometimes be contention within TempDB for certain pages: SGAM, GAM, and PFS. In a nutshell, these pages keep track of what's been used in TempDB so far, and where space is available for new use. Typically, this is dealt with by adding multiple data files to TempDB. There are a few different philosophies as to the correct number, but all agree you should have more than one. Here are a few queries to run... This one will show you how many files TempDB has and where they are located. 

Don't get hung-up on "cloud" vs "local" - what you're doing is establishing a TNS network link between two databases. It is nothing more than that. In order to do this, you will need to have the appropriate tns entries configured in your tnsnames.ora (unless you are using LDAP or you pass in the entire connect string). If you don't know how to do this, the netca application will create one for you using a wizard. Once you have this tns entry, you can simply create the database link as you listed above, but replace the USING clause value you have with the tns entry alias. $URL$ 

As you can see, I can now "execute" the binary, but other permissions issues remain. So here's where I get to my first point: Running things, especially anything that stops/starts a database and/or the cluster as someone other than the Oracle user can lead to serious issues. I remember in my career-youth I accidentally started a development database as my physical user, instead of Oracle. Which meant that many (but not all) of the processes used by the database had my name on them (and by extension, my permissions) instead of Oracle. Your best bet here is to use the Oracle user, or ask your admin to complete whatever the task is for you. 

I am considering a model where I use PostgreSQL COPY to copy data from a file into a table. I was wondering what kind of performance to expect on high-end hardware. An interval in MB/s would be nice so I can start estimating. If you just know the answer on other databases, I would be interested to compare. 

We are creating SAAS where we will at most have 50.000 customers. We are considering creating a user in the Postgres database for each customer. We will map each user that logs into our service to a user in the database in order to be very sure that they only have access to their own data. We also want to implement an audit trail directly in the database by this solutions, which utilizes triggers. If each customer has its own database user, then it would be very easy to see who did what, even if two customers would share the same data. Will we be running into some unexpected problems because we have 50.000 users in our database? Performance-wise or administration-wise. Maybe connection pooling would be more difficult, but I do not really know whether we would need it. 

I have about 26 applications interacting with our database, some of these will store email addresses in mixed case. Since I made the mistake of using varchar as the column type and not a case insensitive one, I would like to make some kind of trigger or rule, which will convert all attempts to save email addresses as mixed case, into lower case. I could achieve the same using the following query: 

Without specific setup, Oracle Database will not know anything about any domain type users you set up. In fact, that's kind of the point of having OS Authentication in the first place; the OS does the authentication, and the Database assumes that this Authentication is legitimate. If you're using OS authentication for users, you (or someone else in your IT department) will need to set up the necessary precautions to protect that authentication at the OS level. OS Authentication can be useful in certain cases (automated backups, cron jobs, etc. Anything where you want a background user to log in but you don't want to have a grep'able password), but IMO as a security practice it isn't really too much to ask for a user to have to type a password. Security is best in layers. 

This might be over simplifying it, but could you use an block around the call to the extra procedure? As in, "check to see if the variable I just wrote to is null, if not, do more work, if it is, specifically call my custom error package? If you've built your own exception handler (and kudos to you for not falling into the "catch all the errors generically" trap), you don't HAVE to call that in an exception block. You can call it whenever you want, and trap the errors whenever they occur. 

Do SQL and SSAS ever need to run at the same time? If no, are you 100% sure? I would look at setting the memory setting for SQL and SSAS to see if they can cooperate with one another. We'll need to leave some memory for the OS, other processes, and multi-page allocations. Perhaps 4G? Try something like this... On the SQL side, set Max Server Memory to 12G and Min Server Memory to 6G. On the SSAS side, set TotalMemoryLimit to 12G and the LowMemoryLimit to 6G. Also, do not enable Lock Pages In Memory. Try a few iterations, adjusting these numbers up or down. If this just doesn't work, and you know for a fact SQL and SSAS do not need to run at the same time, then I would try Aaron's suggestion of scheduling jobs to turn services off and on. 

Basically, the file path on your machine doesn't match the original machine. The MOVE option will let you fix that. Here is an example of the RESTORE command using the MOVE option: 

First, run your query and make a note of your SESSION_ID. Then, run these two queries to see how much TempDB space your original query is using. Be sure to update them with your SESSION_ID. 

Now I would like to utilize the test view or any other view in a GUI which simply presents an editable table. This is possible with the recently introduced automatically updatable views. I want the table to contain drop-downs containing all possible values every time there is a column with constrains on it, like the column with a foreign key in this case. How can this be achieved? I know that I could utilize enums and I do that today, but on Amazon RDS they are unreasonably hard to modify. I had an idea of (ab)using the "EXPLAIN SELECT" query for each column and check which table and column is being queried. 

This fails with the error: ERROR: syntax error at or near "RETURNING" What I am trying to achieve is to be able to run an update statement on table t with a returning clause. I know that I can achieve this if the statement run in the rule was an INSERT or an UPDATE (by adding a RETURNING clause), but I do not know how to achieve this when it is a SELECT statement. If I remove the RETURNING part of the query above, then I get the following message: cannot perform UPDATE RETURNING on relation "t" query: UPDATE t SET c = true WHERE id IN (1000460) RETURNING id hint: You need an unconditional ON UPDATE DO INSTEAD rule with a RETURNING clause. It was this error message that lead me to attempt the syntax in the beginning of this post.