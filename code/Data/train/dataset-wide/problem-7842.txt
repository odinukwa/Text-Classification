So, we continue with the general argument. We now reduce to the case where there are no fixed points. Let the fixed points be $z_1$, ..., $z_N$ and let $U = X \setminus \{ z_1, \ldots, z_N \}$. In the case of Frobenius, $F$ maps $U$ to $U$. There is a Gysin sequence relating $H^{\ast}(X)$, $H^{\ast}(U)$ and a simple contribution for each fixed point, and we can see that the alternating sums of traces of $F$ on $H^{\ast}(U)$ and $H^{\ast}(X)$ differ by $N$. I think (this part isn't in van der Put) that one can still make this argument even if $F(U)$ isn't $U$. Let $U' = F^{-1}(U)$. We can map $\Omega^j(U) \to \Omega^j(U') \overset{\psi}{\longrightarrow} \Omega^j(U)$ where the first map is restriction and the second makes sense because $U' \to U$ is finite. I think we can make the same argument with this map. This seems to be another advantage of the pushforward $\psi$ over the pull back $F^{\ast}$. I was also glossing over the difference between the characteristic $p$ variety and its $p$-adic lift two paragraphs back. (Because I decided to shift to the case of a manifold, where we don't have this issue.) In the $p$-adic lift, we are removing $F$-invariant balls around each fixed point, not just points. (This is the proof of 4.10) Assume that $F$ has no fixed points. So, as $a$ ranges over $\mathcal{O}(X)$, there is no place where all the functions $F^{\ast}(a) - a$ vanish. So the $F^{\ast}(a)-a$ generate the unit ideal; say $\sum (F^{\ast} a_i -a_i) b_i =1$. So, for any $\omega \in \Omega^j$, we have $$\sum \psi((F^{\ast} a_i) b_i \omega - b_i a_i \omega) = \psi(\omega).$$ (We have used that multiplication is commutative.) Since $\psi(F^{\ast}(a) \eta) = a \psi(\eta)$, we deduce $$\sum a_i \psi(b_i \omega) - \psi(b_i a_i \omega) = \psi(\omega).$$ In other words, the summand is the commutator of the operations $\omega \mapsto a_i \omega$ and $\omega \to \psi(b_i \omega)$. Since commutators always have trace zero, we "deduce" that $\psi: \Omega^j \to \Omega^j$ has trace zero, and hence the alternating sum of traces is zero. 

This is not always possible, even just with condition (1). Consider the complex $\mathbb{F}_2^7 \to \mathbb{F}_2^7 \to \mathbb{F}_2^3$ where the basis of the first vector space is indexed by lines of the Fano plane, the basis of the second vector space is indexed by points of the Fano plane and the third vectors space is just thought of as $\mathbb{F}_2^3$. The first map sends a line to the formal sum of the three points on it. The second map embeds the Fano plane as the $7$ nonzero elements of $\mathbb{F}_2^3$. So the composisition is $0$ because, for any line in the Fano plane, the three points on it add up to $0$ considered as vectors in $\mathbb{F}_2^3$. If you had a lift of the sort you describe, you would have $7$ vectors in $\mathbb{Z}^3$ (the image of the basis vectors of the second $\mathbb{Z}^7$) such that each line of the Fano plane gave three vectors with a linear relation between them. In other words, you would have realized the Fano plane in $\mathbb{P}^2(\mathbb{Q})$, which is impossible. In general, questions about linear algebra while imposing that various matrix entries are zero resemble Mnev's universality theorem, so you should expect everything conceivable to go wrong. 

$\def\CC{\mathbb{C}}$The specturm is integral. The following trick is very useful in computing spectra of highly symmetric graphs. Let $G$ be a finite graph, let $\Gamma$ be a group of symmetries of $G$, and let $V$ be the set of vertices of $G$. Let $A$ be the adjacency matrix, so $A : \CC^V \to \CC^V$. Then $A$ commutes with the action of $\Gamma$ on $\CC^V$. Therefore, if $\CC^V = \bigoplus W_i$ is the decomposition of $\CC^V$ into isotypic summands, then the spectrum of $A$ is the disjoint union of the spectra of $A$ restricted to the $W_i$. In our case, we'll take $\Gamma = (\mathbb{Z}/k)^n$. For $(g_1, \ldots, g_n) \in \Gamma$ and $(x_1, \ldots, x_n)$ in $G_{n,k}$, we have $\left( (g_1, \ldots, g_n) \ast (x_1, \ldots, x_n) \right)_j = 0$ if $x_j=0$ and $\left( (g_1, \ldots, g_n) \ast (x_1, \ldots, x_n) \right)_j = g_j+x_j \bmod k$ if $x_j \neq 0$. Let $\chi: (g_1, \ldots, g_n) \mapsto \exp(\frac{2 \pi i}{k} \sum g_i c_i)$ be a character of $\Gamma$, where the $c_i$ in $\mathbb{Z}/k$. We will compute the action of $A$ on the corresponding eigenspace of $\CC^V$. Reorder the coordinates so that $c_1=c_2=\cdots=c_j=0$ and the other $c_i$ are not $0$. The corresponding eigenspace has dimension $j$: For $1 \leq r \leq j$, there is a one dimensional space of functions in $\CC^V$ which transform by this character and have the $0$ entry in position $r$. I get that $A$ acts on this $j$ dimensional eigenspace by the matrix whose diagonal entries are $(j-1) (k-1) + (n-j) (-1)= jk -k-n+1$ and whose off diagonal entries are $k$. This matrix has one eigenvalue of $2jk - 2k+n+1$ and all the others are $jk-2k+n+1$. In particular, they are integers. 

In your original question you require $H$ to have finite index in $G$. Most of the other answers are treating this as unintended. If you actually did want to require this, then the result is true for infinite groups as well. Proof: Let $[G:H]=n$. The action of $G$ on $G/H$ gives a map $G \to S_n$; let $K$ be the kernel of this map. Then $H/K$ and $G/K$ are finite groups, so we know $G/K \neq \bigcup g (H/K) g^{-1}$. In particular, there is some coset $fK$ which is not in any $g H g^{-1}$. So $f$ is not in any $g H g^{-1}$. 

Multiplication of differential forms is inherently anti-commutative. Thus, if $x$ and $y$ are coordinates on a surface, then $dx \wedge dy$ makes sense but $(dx)^2+(dy)^2$ is either nonsense or, if it means anything, is $0$. I'm not sure why I believed this, but I did for several years. I tried my best to avoid creating this impression in my students, but I think it still happened in some of them, simply because the curriculum spends a lot of time on integration and Stokes theorem and very little time on metrics, curvature, etc. 

No. $\mathbb{C}$ is algebraically closed, so any finite extension of $\mathbb{R}$ must be isomorphic to a subfield of $\mathbb{C}$, and therefore has dimension no more than $2$. 

Have you tried reading the Billera, Filliman and Sturmfels paper, Constructions and complexity of secondary polytopes? I seem to remember it is very clear, although it also has weakness (3). 

I think the easiest place to see the $22$ is in a Kummer surface. Let $A$ be an abelian surface, so topologically $(S^1)^4$. This clearly has $h_2 = \binom{4}{2} = 6$, and there are obvious topological repreentatives for the $2$-cycles, given by $(S^1)^2$ in $6$ different ways. Let $X$ be the quotient of $A$ by negation. This has $16$ singular points; the images of the $16$ $2$-torsion points of $A$. Let $Y$ be $X$ blown up at these $16$ points. Then $H_2(Y)$ is (ADDED rationally, see below) generated by the pushforwards of the $6$ $2$-cycles from $A$, and the $16$ $\mathbb{P}^1$'s introduced by resolving the singularities. $6+16=22$. 

I've been thinking about this, and I think that I can state Stevenhagen's conjecture in a fully elementary way. (No idea about a proof, of course.) Fix a $t \times t$ symmetric matrix $A$ over $\mathbb{F}_2$ whose rows sum to $0$. Consider $t$-tuples of primes $(p_1,p_2,\ldots,p_t)$ such that all $p_i \equiv 1 \bmod 4$ and $\left( \frac{p_i}{p_j} \right) = (-1)^{A_{ij}}$ for $i \neq j$. Let $D =p_1 \cdots p_t$ and let $(x,y)$ be the primitive solution to Pell's equation $x^2-D y^2=1$. Then $x \equiv (-1)^{b_i} \bmod p_i$ for some vector $b \in \mathbb{F}_2^t$. I will prove below that $b$ is a nonzero element in the kernel of $A$. Stevenhagen's conjecture is equivalent to saying that all nonzero elements of $\mathrm{Ker}(A)$ occur with equal probability, as $(p_1, \ldots, p_t)$ runs over primes obeying $\left( \frac{p_i}{p_j} \right) = (-1)^{A_{ij}}$. The negative Pell equation is solvable if and only if $b=(1,1,\ldots,1)$. (In other words, $x \equiv -1 \bmod D$.) So if $t=2$ and $\left( \frac{p_1}{p_2} \right)=-1$, then $b$ must be the unique element of $\mathrm{Ker} \left( \begin{smallmatrix} 1 & 1 \\ 1 & 1 \end{smallmatrix} \right)$ and the negative Pell equation is solvable but, if $\left( \frac{p_1}{p_2} \right)=1$, then $b$ could be any of the $3$ nonzero elements of $\mathrm{Ker} \left( \begin{smallmatrix} 0 & 0 \\ 0 & 0 \end{smallmatrix} \right)$ and the negative Pell equation is solvable with probability $1/3$. It looks like dealing with primes that are $2$ or $3 \bmod 4$ is just more bookkeeping, but you didn't ask, so I won't do it. 

I will show that $(2) \implies (1)$ implies your statement (and is basically equivalent to it). I'll call your Hermitian matrices $X$ and $Y$, to leave the letters $(A,B,C)$ clear. Let $A=e^{X/2}$, let $B=e^{Y/2}$ and let $AB=C$. Let $e^{\alpha}$, $e^{\beta}$ and $e^{\gamma}$ be the singular values of $A$, $B$ and $C$. So the eigenvalues of $C C^{\ast}$ are $e^{2 \gamma}$, and we note that $C C^{\ast} = e^{X/2} e^Y e^{X/2}$. Using $(2) \implies (1)$, let's find Hermitian $\mathfrak{a}$, $\mathfrak{b}$ and $\mathfrak{c}$ with eigenvalues $2 \alpha$, $2 \beta$ and $2 \gamma$ and $\mathfrak{a}+\mathfrak{b} = \mathfrak{c}$. Then $C C^{\ast}$ and $e^{\mathfrak{c}}$ are Hermitian with the same eigenvalues and, conjugating by a unitary matrix, we can arrange that $C C^{\ast} = e^{\mathfrak{c}}$. Now, $X$ and $\mathfrak{a}$ are both Hermitian with eigenvalues $2 \gamma$, so we can find unitary $U$ with $\mathfrak{a} = U X U^{\ast}$. Similarly, we can find unitary $V$ with $\mathfrak{b} = V Y V^{\ast}$. So $\mathfrak{c} = U X U^{\ast} + V Y V^{\ast}$ and $e^{X/2} e^Y e^{X/2} = e^{U X U^{\ast} + V Y V^{\ast}}$ as desired. 

Back on Scott Aaronson's blog, I gave an argument that $e^z+z-1$ should have an analytic compositional square root. The important difference between this function and $e^z-1$ was that the fixed point at $0$ has derivative $>1$, not $=1$. This should warn us that arguments based on the growth rate near infinity are inadequate. (Or else it should point out that my argument was broken!) See comments below, my argument may have been broken. But, if so, I want to figure out why! UPDATE: OK, I'm looking for some empirical data myself now. Let $e(z)=e^z+z-1$. My argument claimed that there should be an analytic and invertible $u$ (near 0) such that $u(e(z)) = 2 u(z)$. If such a $u$ exists, then $u^{-1}(2^{1/2} u(z))$ should have the desired property. The nice thing about the equation $u(e(z)) = 2 u(z)$ is that it is linear in the coefficients of $u$. Here are the first 10 coefficients, computed with exact arithmetic. {1, -(1/4), 1/18, -(1/96), 17/10800, -(47/267840), 4069/354352320, -(24907/102863416320), 475411/2893033584000, -(108314387/ 1314080143488000)} And the numerical versions of the above {1., -0.25, 0.0555556, -0.0104167, 0.00157407, -0.000175478, 0.0000114829, -2.42137*10^-7, 1.6433*10^-7, -8.2426*10^-8} They seem to be converging rapidly. Going a little further up, something odd happens. I computed the first 20 terms, of $u$, still using exact arithmetic, and I computed the ratios of successive terms. I'll just give you numeric data, because the fractions are huge. {-0.25, -0.222222, -0.1875, -0.151111, -0.11148, -0.065438, -0.0210867, -0.678665, -0.50159, -0.155914, 0.12897, -0.691029, -0.153086, 0.158892, -0.657229, -0.165837, 0.119535, -0.806045, -0.191576} So the ratios are usually small, but occasionally they jump up to larger than 0.5. That's still not evidence against convergence, but it suggests a need for caution (or the possibility of a bug!) On the other hand, I also tried computing the $k$-th roots of successive terms, and the behavior was much smoother: {1., 0.5, 0.381571, 0.319472, 0.275046, 0.236612, 0.196922, 0.148939, 0.176275, 0.195707, 0.191704, 0.185475, 0.205223, 0.200971, 0.197848, 0.213264, 0.210132, 0.203648, 0.218941, 0.217484} Again, this is exact up until the point of taking a $k$-th root of a rational number. UPDATE: Ok, I went and tried to repeat Ekhad's computation, and I think that Michael Lugo has found the error. Let $f(f(z)) = e^z+z-1$. I just did the first 5 terms, and I get: Coefficients of $f$: {Sqrt[2], 1/4 (2 - Sqrt[2]), 1/36 (-9 + 7 Sqrt[2]), 1/288 (47 - 33 Sqrt[2]), (-4350 + 3071 Sqrt[2])/43200} Table of $|f_i|^{1/i}$: {1.41421, 0.382683, 0.292347, 0.184117, 0.174302} Table of $(i! |f_i|)^{1/i}$: {1.41421, 0.541196, 0.53123, 0.407517, 0.454086} Notice that my second table, not my first, matches Ekhad. But my first table is the right thing to compute. 

We begin with a bunch of formal nonsense about spectral sequences. Let $V_1$, $V_2$, $W_1$ and $W_2$ be $k$-vector spaces equipped with bilinear pairings $V_1 \times V_2 \to k$ and $W_1 \times W_2 \to k$. Let $\phi_1 : V_1 \to W_1$ and $\phi_2 : W_2 \to V_2$ be linear maps. We say that $\phi_1$ and $\phi_2$ are adjoint if $\langle \phi_1(v_1), w_2 \rangle = \langle v_1, \phi_2(w_2) \rangle$ for all $v_1 \in V_1$ and $w_2 \in W_2$. (Note that, as yet, we don't require the pairings to be perfect or anything to be finite dimensional.) Let $A^{pq}$ be an $n \times n$ double complex of $k$-vector spaces, with $\dr: A^{pq} \to A^{(p+1)q}$ and $\du: A^{pq} \to A^{p(q+1)}$ the rightward and upward maps. Set $d = \dr+\du$. (I'll be sloppy about signs throughout, but I think my convention is that squares anti-commute.) Suppose that, for all $(p,q)$, we are given a bilinear pairing $A^{pq} \times A^{(n-p)(n-q)} \to k$; suppose that $\dr: A^{pq} \to A^{(p+1)q}$ and $\dr: A^{(n-p-1)(n-q)} \to A^{(n-p)(n-q)}$ are adjoint, as are $\du: A^{pq} \to A^{p(q+1)}$ and $\du: A^{(n-p)(n-q-1)} \to A^{(n-p)(n-q)}$. Let $E^{pq}_r$ be the corresponding spectral sequence, where we first take differentials in the $\du$ direction. Our main results are Theorem 1: The bilinear pairing between $A^{pq}$ and $A^{(n-p)(n-q)}$ descends to a bilinear pairing between the subquotients $E^{pq}_r$ and $E^{(n-p)(n-q)}_r$. With respect to this pairing, the differentials $E^{pq}_r \to E^{(p+r)(q+1-r)}_r$ and $E^{(n-p-r)(n-q-1+r)}_r \to E^{(n-p)(n-q)}_r$ are adjoint. Theorem 2: In the above setting, if the vector spaces on the $r$-th page are finite dimensional and the pairings between them are perfect, then so are the pairings on every succeeding page. To prove this, we need to recall how $E^{pq}_r$ is defined. We follow Vakil, section 1.7.7, except for the unfortunate point that he does the map in the $p$ direction first and we do the map in the $q$ direction first. This is forced on us because the notation $H^q(X, \Omega^p)$ is standard. This means that many of our coordinates are reversed from Vakil. Set $S^{pq} = \bigoplus_{k \geq 0} A^{(p+k)(q-k)}$. Let $S^{pq}_r$ be $S^{pq} \cap d^{-1}(S^{(p+r)(q+1-r)})$. (So $S^{pq}_0=S^{pq}$, and increasing $r$ makes the condition more restrictive.) Then $$E^{pq}_r = \frac{S^{pq}_r}{S^{(p+1)(q-1)}_{r-1} + d S^{(p-r+1)(q+r-2)}_{r-1}}.$$ The term $S^{(p+1)(q-1)}_{r-1}$ is simply those elements in $S^{pq}_r$ which have no contribution from the $A^{pq}$ summand, so $S^{pq}_r/S^{(p+1)(q-1)}_{r-1}$ injects into $A^{pq}$ and $E^{pq}_r$ is a subquotient of $A^{pq}$. Lemma: Let $p+p' = n-r$ and $q+q'=n+r-1$. Let $v \in S^{pq}_r$ and $w \in S^{p'q'}_r$. Then $$\langle v^{pq}, (dw)^{(p'+r)(q'+1-r)} \rangle = \langle (d v)^{(p+r)(q+1-r)}, w^{p' q'} \rangle.$$ Here $u^{pq}$ denotes the projection of $u$ onto the $A^{pq}$ summand. Proof Sketch: We have $(dw)^{(p'+r)(q'+1-r)} = \dr (w^{(p'-1+r)(q'+1-r)} ) + \du (w^{(p'+r)(q'-r)})$. By our adjointness hypotheses, $\langle v^{pq}, (dw)^{(p'+r)(q'+1-r)} \rangle = \langle \dr(v^{pq}), w^{(p'-1+r)(q'+1-r)} \rangle + \langle \du(v^{pq}), w^{(p'+r)(q'-r)} \rangle$. But $\du(v^{pq})=0$ since $v \in S^{pq}_r$, so we only need to think about the first term. Now, since $v \in S^{pq}_r$, we have $\dr(v^{pq}) = - \du(v^{(p+1)(q-1)})$ and, using adjointness again, $\langle \du(v^{(p+1)(q-1)}), w^{(p'-1+r)(q'+1-r)} \rangle = \langle v^{(p+1)(q-1)}, \du (w^{(p'-1+r)(q'-r)}) \rangle$. Since $w \in S^{p'q'}_r$, we have $ \du (w^{(p'-1+r)(q'-r)}) = - \dr(w^{(p'-2+r, q'-r-1})$. Continuing in this manner, we eventually establish the result. $\square$ Proof Sketch of Theorem 1: We first check that the bilinear form descends to the quotient. In other words, if $v \in S^{(p+1)(q-1)}_{r-1} + d S^{(p-r+1)(q+r-2)}_{r-1}$ and $w \in S^{(n-p)(n-q)}_r$, then $\langle v^{pq} ,w^{(n-p)(n-q)} \rangle = 0$. If $v \in S^{(p+1)(q-1)}_{r-1}$ then $v^{pq}=0$, so this is immediate. Now, suppose that $v=du$ for $u \in S^{(p-r+1)(q+r-2)}_{r-1}$. Note that $S^{(p-r+1)(q+r-2)}_{r-1} \subseteq S^{(p-r)(q+r-1)}_r$. So the lemma shows that $\langle (du)^{pq}, w^{(n-p)(n-q)} \rangle = \langle u^{(p-r)(q+r-1)}, (dw)^{(n-p+r)(n-q-r+1)} \rangle$. But $u^{(p-r)(q+r-1)}=0$, since $u \in S^{(p-r+1)(q+r-2)}$. We have shown that the bilinear form descends. We now recall the definition of the differential $d_r: E^{pq}_r \to E^{(p+r)(q+1-r)}_r$. Take $v \in E^{pq}_r$ and lift $v$ to $ \tilde{v} \in S^{pq}_r$. Then $d_r(v)$ is the class of $(d \tilde{v})^{(p+r)(q+1-r)}$ in the quotient $E^{(p+r)(q+1-r)}_{r}$. Take $v \in E^{pq}_r$ and $w \in E^{(n-p-r)(n-q-1+r)}_r$ and lift them to $\tilde{v}$ and $\tilde{w}$. We want to show that $\langle (d \tilde{v})^{(p+r)(q+1-r)}, \tilde{w}^{(n-p-r)(n-q-1-r)} \rangle = \langle \tilde{v}^{pq}, (d \tilde{w})^{(n-p)(n-q)} \rangle$. Again, this is the Lemma. $\square$ Proof Sketch of Theorem 2: Suppose that all the vector spaces on the $r$-th page are finite dimensional and the pairings between them are perfect. Write $V^{\vee}$ for the dual to a finite dimensional vector space $V$. We have complexes $\cdots \to E^{(p-r)(q+r-1)}_r \to E^{pq}_r \to E^{(p+r)(q-r+1)}_r \to \cdots$, and $E^{pq}_{r+1}$ is the cohomology of this complex. If two complexes of finite dimensional vector spaces are dual, then their cohomologies are also dual. "$\square$"