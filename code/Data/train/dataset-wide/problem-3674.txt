I'm adding a Synology DS 1513+ NAS to a small business network. The NAS has four ethernet ports for data transfer. The current network configuration has a modem in bridge mode, hooked up to a consumer grade wireless router, hooked up in turn to a 16-port switch. Some additional machines are hooked up directly to the router and bypass the switch. Is there any reason to split those ethernet cables across the devices? That is, will I get better performance with all of the cables going from the NAS to the switch, or would it be better to have two go to the switch and two go to the router? 

Well, things are a bit more complex. Modern hard drives don't just detect errors, they have some spare sectors and smart controllers that try to relocate bad sectors. That is, when you try to read some logical sector and it doesn't read at first time, the controller tries to read it several times, and sometimes it can read it after some retries; then it writes the data back to the spare sector, remaps logical sector to the new one and marks old sector as bad, and finally gives you your data. All those processes are completely transparent to the reader, you wouldn't notice any error. However this will normally be reflected in S.M.A.R.T statistics, and if this happens more and more often, you can see that the drive is going to fail before it actually fails. That's why it's really important to use SMART monitoring tools on your system. When a sector doesn't read at all, or the controler runs out of spare sectors, read error will be returned by the drive. Error detection is now pretty bulletproof, it uses some kind of CRC for sector data. When read error is returned, mdadm will see it, mark the drive as unusable and switch an array into degraded mode. 

I currently have nginx proxying for Thin, but set up to serve static files for the app that Thin is serving instead of proxying the request. What I'd like to know is how I can check that the rules are set up correctly. Since Thin doesn't log requests, I would need to set up nginx logs in such a way that it shows which requests were served as files and which were passed to Thin. Is this even possible? If so, how? 

This looks like it might be related to ProgramData having been moved onto a non-ssd drive during windows install, but I have no idea whether it's related to the failure of the Hyper-V feature to uninstall. Can someone point me in the right direction here? Either to get the real error I'm receiving, or force Hyper-V to uninstall or... anything? 

I can't think of any tool that would do that out of the box. This is quite rare scenario, as you can't create correct two-way NAT mapping if you only change port. Do you really need just one-way traffic ? However you can always write your own netfilter module (it's not that difficult) and alter packet headers in any way you want. 

I've found the answer and it was quite funny - ARP table overflow. The traffic in the test environment was gerenated from many IPs that resided in directly-connected networks, so the system had to use ARP first to figure out MACs, and the default hard limit of ARP table in Linux is just 1024 entries, which gives number of connections between networks connected to 2 different interfaces close to 512. When I increased net.ipv4.neigh.gc_thresh1 and also .gc_thresh2 and .gc_thresh3, the problem was solved. 

wiki's tend to have revision control built-in, and many are file-based (vs stored in a database), so rsync should work perfectly fine. I know folks who do this for 'TWiki' for replicating their installations to mulitple servers. Perhaps you only have 'ftp' access to your wiki files? you might consider 'wget' to pull from ftp (rather than the http interface) with the recursive (-r) and timestamping (-N) flags set so that it only transfers file that are 'newer' (which isn't exactly a diff). Once you have a 'copy' of what is out on the ftp server, you'd mark the update time somehow (often with just a 'touch' of a specific marker file). You would then edit normally via your local installation of the same wiki, then use 'find $dir --newer touchmarkerfile' to identify the updates for ftp and transfer them over via a script around an ftp delivery tool. I have used such a solution before (though I had the advantage of sucking the changes back to the main server via 'wget', so just used the recursive timestamping approach again. In hindsight, if I had 'ssh' access (I didn't), I would have simply used 'rsync -globtru[n]cv' to simply pull (or push) the files in each direction. 

I have a windows 8 Pro machine that has Hyper-V installed on it. Now, the developer wants to virtualize with Vagrant instead, but is having problems with 64-bit machines throwing an error. This appears to be because Hyper-V is installed. When I go to uninstall Hyper-V via the windows features dialog, I'm prompted to reboot. During the reboot, I see a message briefly that look something like "fatal error C00000d4" but the machine boots too quickly for me to read anything else. Unfortunately I'm primarily a linux sysadmin here so I don't know how to proceed. I've searched the event viewer but the only thing that I can see that looks like it might be related is an error: 

First Things First by Covey was good for me.... isolating tasks into the 2D grid of 'important (or not)' and 'urgent (or not)' was insightful. Some of the urgent 'feeling' stuff is just not important. 

Caveat! Every single time I think I understand the rules for how authentication works, I have to futz with the config repeatedly until I get some nuance correct. Use this only as a starting point. Re-read the apache documentation on mod_auth and mod_access in particular, paying special attention to the Order directive. Therein lies your answer. Hope this helps, and please post your working example if it doesn't match this one, as this is a pretty good recipe to have in an apache cookbook. --edit-- Testing the above shows that restricted area is forbidden to all except for those from the IP address, who must provide authentication. It is not clear from your question if users from other IPs need unfettered access to this 'restricted area' or if they are simply forbidden? 

You can setup transparent proxy on your client machine that will have your company's proxy as a parent and add authentication information when forwarding requests to the parent. You will need to install a proxy server that supports transparent proxying, I'd recommend squid; and you will need a firewall that will redirect your traffic to a proxy server, many windows firewalls can do that. Google for "squid transparent proxy", there are a lot of manuals. 

If you create backup using tar, it will be enough to copy all your files, however the problem will arise when you try to extract that archive into a live system. Some binaries or libraries may not update because they will be in use by the system, and you will end up in a mess. You should get some support from your hosting provider (for the new VPS). Usually they have some features for that case, for instance my hosting provider allows me to switch my VPS to "repair mode", in that mode new VPS is created with basic Linux software, and my VPS's disk is mounted there, allowing me to change it however I want and then leave repair mode. Or maybe you can just send your tar archive to the support team, asking them to extract it to your VPS when it's offline. 

We run into this when editing CGIs... the #! interpreter line gets Ctrl-M on it somehow, rendering the executable not found. It looks like a perl error but is really the 'she-bang' interpreter line having 'nearly' invisible characters at the end. In our case, we found this after the file was written. try using dos2unix command to copy to another name and try hitting that. If it works, you've found your root cause. Sorry to say that I have no real workaround except to recognise the problem when I see it. --edit-- Our error message is usually: scriptname: file not found NOT the 'file busy' mentioned in the question. 

As you all probably know, ipv4 route cache has been removed in 3.6 Linux kernel series, which had serious impact on multipath routing. IPv4 routing code (unlike IPv6 one) selects next hop in a round-robin fashion, so packets from given source IP to given destination IP don't always go via the same next hop. Before 3.6 the routing cache was correcting that situation, as next hop, once selected, was staying in the cache, and all further packets from the same source to the same destination were going through that next hop. Now next hop is re-selected for each packet, which leads to strange things: with 2 equal cost default routes in the routing table, each pointing to one internet provider, I can't even establish TCP connection, because initial SYN and final ACK go via different routes, and because of NAT on each path they arrive to destination as packets from different sources. Is there any relatively easy way to restore normal behaviour of multipath routing, so that next hop is selected per-flow rather than per-packet? Are there patches around to make IPv4 next hop selection hash-based, like it is for IPv6? Or how do you all deal with it? 

I always run the above to make sure it works, then remove the 'n' flag that once I'm happy with the results. The key features of the above combinations: 

But when I try to map port 80 to a pid I get nothing: When I try seeing what sockets that specific pid is using I get: 

GomezNetworks or Keynote are good 3rd party payware services that can provide performance data and handle the javascript nature of the website. dotcom-monitor.com seems to be another service that might help, but the key point is that you likely want 'transaction' monitoring, not just 'hit this URL' (though dotcom-monitor can do a form POST directly), and you want them to have full browser emulation (ie: javascript), not just simple HTML POST/GET etc try googling 'web transaction monitoring service'