I don't think it is supported in the way you are talking about. (With a full SharePoint and SQL Server at each office that are all mirrors of each other.) 2010 does have some increased functionality for branch office scenarios over 2007 though. Check out this blog post: $URL$ Which goes into some details - I think especially check out the BranchCache scenario. 

I don't believe Performance Monitor counters are available through SNMP out of the box. There are some add-ons available that will add that functionality though. A paid version: SNMP Informant and a free one: SNMP Tools 

So this is kind of silly, but if you add to the end of the URL, it will go to the old page. $URL$ You might report this to Microsoft's web people using the "Contact Us" button at the bottom of the page - it seems like a bug due to them rearranging their SharePoint sites after the 2010 launch. 

Is it possible somebody created a custom edit form, and then made it the default edit form for the list? I believe that if you create a new edit form in SP Designer, it doesn't include the name field by default. I would open the list with Designer, and see what the the default edit document is set to, and if there is another possible edit document that contains the Name field. Also - I don't think it is actually possible to remove the Name field from a DocLib - but you could try using something like SharePoint Manager 2007 to analyze the list and see if the Name field is still actually attached to it. 

Perhaps you need to change the preferred order of the connections? If you go to: Network and Sharing Center -> Change Adapter Settings -> Advanced Menu -> Advanced Settings you will have an option to re-order the connections, make sure that Adapter 1 is first in the list. 

The command will give you a list of data for all backup sets in the file. From there you could select the max and pass that into the param. $URL$ 

I don't think there is a direct command line equivalent, but there are a couple of things that could get you close. First, has an argument that will tell you the signed status of the drivers. It also has an argument that will dump the output to CSV. The weird part of using this command is that if you use the argument, you can't get the full path to the driver file (and if you use the option to get the full path, you can't get the signed status. Second, if you want to go down the PowerShell path, you could use the cmdlet. This one is weird because you have to pass a driver path into the cmdlet, so you need to build the driver list yourself. You can get that from WMI though, so something like this may suit your needs: 

IIS7 isn't out of the box 100% compatible with the Metabase structure of previous versions, you need to enable it. In the "role services" for IIS, make sure you have "IIS6 Metabase Compatibility" enabled. See the following 2 sites for more info: $URL$ $URL$ With that being said, I've found that ADSI and Metabase compatibility don't seem to be 100% reliable. A much better solution would be to retool your script to use appcmd.exe. The command would be something like: 

Visual Studio 2008 isn't 100% compatible with TFS 2010. To get some level of compatibility, you need to install an update to VS2008. Also see this blog post about the subject. 

In SQL Server 2008, Windows admins are no longer SQL Server admins by default. You will have to ask somebody else to change your permissions. $URL$ 

I think I would argue that trying to force IIS to do something it is not intended to do is more overkill (in terms of work required) than using a simple, known solution. If you want to keep it all on one VM, then how about using a product other than IIS in front of IIS? Something like nginx can load balance between the 2 IIS sites (or vdirs), and it should all run on the same VM. 

You can't just copy the files to the USB drive, you need to make it bootable as well. Plug in your USB drive on a working Windows machine that contains the files from the Windows 2008 ISO, then do the following: 

Here is a list of all the columns you can collect with SQL Profiler in server 2000. The big ones you would probably want to watch are CPU, Duration, Error, Reads, Writes and Success. Here is a list with data for SQL 2005. To get the "Error" column, you need to go to the "Events Selection" tab when you are setting up your trace, choose "Show All Columns" and then put a checkmark in the Errors column where it exists. There is also a whole "Errors and Warnings" event category that can be used to track such things. See the linked documentation for much more detail about that. 

I think the Microsoft supported solution would be to backup your databases, reinstall SQL Server and then restore your databases to the new installation. If you want to try something that is not supported by MS, and may be downright dangerous, you could try to update the master database through a SQL Query. I don't know that I recommend this, but it might work for you and save some reinstallation headaches. Also, I've never done this before, so can't guarantee it will even work. The steps are: 

The files are just what they sound like - logs of errors that happen within your SQL server. When you are in SQL Management Studio, if you look at the "SQL Server Logs" it is viewing the . The files with numbers at the end are archived log files when it rotates. The files are for errors that occur in the SQL Server Full Text Filter Daemon launcher service. This is a separate service used to load filters for full text search. Again, the files with numbers are archived files after log rotation. The files are SQL Profiler traces. SQL 2008 does some default tracing to help troubleshoot issues. 

This option was removed in Server 2008 as a security enhancement. This is because the application runs as the cluster service account which has elevated privileges. Even in 2003 clusters, the security best practice was to not have this checked. 

If you can't connect to it to even run this though, you will most likely need to start your server in single-user mode (or possibly minimal configuration mode) and go from there. To do this, you stop your SQL Server, then run it from the command line with either the option (single-user mode) or the option (minimal configuration mode, which also enables single-user mode). You should be able to drop the triggers from there, or enable DAC from there, whichever you are more comfortable with. sqlserver command line reference single user mode reference 

Do you have access to the log files for your site? If so, you can enable the TIME-TAKEN field in the IIS logs. That counts (in milliseconds) how long it took IIS to process the request. That, added to the data you get from Google Analytics, should give you a good idea of the total time to process a request from initial click to full page load. The only thing missing is the time it takes from somebody clicking on the link to the request hitting the webserver. (Which should be negligible unless you have major networking/dns issues) 

There are some (non free) programs that claim to do this for you. The 2 I can think of off the top of my head are: Driver Robot Driver Detective I haven't used either of them so can't vouch for how good they are. 

You'll need to extract just the hour from your StartDate field, and then compare against that. The function pulls the hour as number from 0-23. Then you nest multiple statements and you have your answer. Something like this should work: 

Depending on how your web application works, you may also need to turn on the "Reverse rewrite host in response headers" setting in the Proxy configuration for your farm. 

According to this page, you can disable the timer job to disable the news feed. To clean out the data, you run the timer job. 

Unfortunately, it isn't possible to load the IIS provider as the same thing on both 2008 and 2008R2. On 2008 the IIS provider is provided as only a snapin, and on 2008R2 it is provided as only a module. With a little bit of coding, you can actually determine which to use, and dynamically load the module or snapin in your script, depending on which is necessary. I took this code from $URL$ when I was having a similar problem. 

Officially I don't think this configuration is supported yet. See the ASP.Net 4 breaking changes whitepaper. Unofficially, you should be able to fix the problem by changing the trust level in your web.config file from to . This is NOT recommended though. 

The purpose of application pools is to limit the risk of a single application taking down every sight on your box, so this is definitely recommended. The "add one per month" part of your question doesn't make a whole lot of sense, once a site is in an application pool it will stay there forever, so this isn't necessary for your existing sites. Are you talking about future growth? If you update your question we can comment on that part further. The downside is that each new application pool adds a bit of overhead (memory/cpu) to the server, so adding 360 of them probably isn't a good idea. Chunking the sites into (somehow related) groups is recommended. A good place to start would be putting the "offending" applications into their own pool, and leaving the working applications in another - this will help stabilize things for most of your sites. 

You will want to do what is called a "database attach upgrade" of your 2007 data. The basic outline of what happens is that you: 

IIS 7 has a new "request filtering" feature. You probably want to use the hidden segments configuration: 

You are correct - WSS does not need to be installed separately, the MOSS installation will do it for you. 

This is a very broad question that I don't think can be answered easily here. Check out the 2010 upgrade/migration site to get your high level questions answered and upgrade approach started, then come back here with specific questions. $URL$ 

Look at the ASP properties for your site. Under "Debugging Properties" there are 2 settings that are relevant to your situation: 

The Trace Log section "Path" box will tell you where your logs are located. This screen will also allow you to increase the logging level if the default levels don't show you any useful messages. 

Report Library is a MOSS feature only. The Reporting Services add-in should have given you a new web part called "Report Viewer" that you can add to any web part enabled page. This gives you a view into SSRS reports. The Report Viewer 2.0 web part (and Report Explorer web part) gives you the ability to view reports on your SSRS server. The Reporting Services add-in I mentioned above only works when you are running SSRS in SharePoint integrated mode - it includes a Report Viewer 3.0 web part that pulls reports out of a SharePoint library. The Report Viewer 2.0 web part doesn't use SharePoint libraries - it acts as a view into the reports hosted on your SSRS server (and Report Explorer lets you view a listing of available reports). See this documentation for more details. Although it is for SQL 2008, it still applies to 2000 and 2005 versions as well, though the path to RSWebParts.cab changes depending on your version. Sorry for the confusion. 

An easy way to see all your databases (and what web application they belong to) is to go to Central Admin -> Operations -> Perform a Backup. This will give you a tree view of your farm with all of the databases listed. 

FastCGI and PHP both have their own timeout settings that also affect this. See this blog post on IIS.Net for details: $URL$ The problem you are going to have is that some of these things (like the FastCGI configuration section) are done (exclusively if I remember correctly) at the file level, not in the file. You would need to talk to your hosting provider to increase the timeouts. 

should work. After that though, you will need to wait for SQL Server to actually start up before issuing any commands. On Windows 2003 you can get from the Windows 2003 Resource Kit to do this. On Windows 2008 you can use the command. You will need to time a few startups to get an idea for exactly how long to wait before attempting the restores (and add a little time to it, just in case...) As far as restoring msdb and model though, it should be 2 more simple sqlcmd restores just like the one you have for master. You also need to make sure you stop the SQL Server instance before restarting the service. Final script would look something like: 

These seem pretty good, and go beyond Microsoft's governance recommendation for site deletion. The only thing I might add would be checking any web parts that might be accessing or putting data on the site to be deleted. For example, there is a custom workflow on codeplex that allows you to copy items into a list on another site. Some of the web parts that do cross-site data rollups/etc. may also give you issues. One other thing to check out would be the MS IT Site Delete Capture tool available from CodePlex. This basically acts as an administrative "site recycle bin" that allows you to quickly restore an accidentally deleted site. 

With the disclaimer that I have never used it before (and have no affiliation with it...) This product seems to fit your needs: $URL$ It generates reports off of the audit log data that are administered and viewed from within your SharePoint site.