In some cases, you could face issues copying .MYD and .MYIs because of a major weakness MyISAM has: Data changes (changes to files) are cached in the OS. (Of course, this would be 100 times worse with MySQL in Windows, so I'll leave Windows out of this answer). If you are using MySQL 5.6, I have a little good news. The command would look all MyISAM tables under a single lock. MySQL 5.6 now has the syntax . The MySQL 5.6 Documentation says 

For the table with UniqueKey and timeStamp, to see if the exists within the last 5 minutes, simply run this 

The CHANGE MASTER TO command erases the relay logs it currently has and starts fresh. Give it a Try, and see if this clears up the replication lag !!! 

The delay comes when building a brand new Clustered Index (home of PRIMARY KEY entries) If you want to totally automate the conversion of just one table, you need this: 

These will indicate whether or not InnoDB is running properly. If InnoDB is disabled, it is more than likely that the log files need to be recreated. As long as you keep ibdata, delete ib_logfile0, delete ib_logfile1, and set innodb_log_file_size to 170M in my.ini, you should be able to start mysql and have mysql rebuild new ib_logfile0 and ib_logfile0 to 170M each as suggested by the article you mentioned in the chat room. 

GIVE IT A TRY !!! As for the one off-topic: Server with 16GB RAM and LA of 8.00 ??? Of course, not !!! Try these suggestions and see if it really helps. 

You also want to make sure of the datetime stamps of every table. Search for any metadata in the system for every table, order such a list by datetime last updated, and display the output in desc order by datetime. You could also check the table size for even the slight change in size. For example, in MySQL 5.x, you have information_schema.tables which looks like this: 

In both cases, you must still form a temp table of values What you need to do is reorganize the query's execution, a.k.a. refactoring. Here is your first query totally refactored: 

You have to control the subselects by giving them as little data as possible. That's why I suggested running before joining to the correlated subqueries. UPDATE 2012-06-15 12:18 EDT Since this is a full join, remove the from your original query: 

Have I got a stunning revelation for you on this problem. I saw this type problem before. I have a MySQL client at my employer's web hosting company with this situation : The client, running MySQL 5.1.37, has two DB servers in Circular Replication (call it M1 and M2) He performed a with a 50GB CSV file on M1 Running several times on M2, Log Files and Positions were not moving. Then, I noticed something very disturbing. When I ran several more times on M2, I noticed this pattern in the output: 

ANSWER TO QUESTION #4 You may have to shift character sets with the DB Session. Here are the settings that can be changed at the session level: 

You are going to have to bite the bullet somewhere in this process. You should determine which databases need to be dumped first (smallest to largest): 

The IO Thread is what communicates with the Master. Killing the IO Thread on the Master Side using the command would abort the IO Thread on each Slave. That could corrupt the Clean Recording of Replication Coordinates. 

The Process ID causing this log jam of locks Everything coming up NULL (No transactions are blocked). This indicates that the lock is just stuck in LA-LA-LAND (Aurora is at fault). In that event, reboot RDS. 

It may be possible that both instances of MySQL are trying to record log entries to the same location (same file). I do not think mysql allows more that one error log per MySQL instance. Here is something further on using error log with mysqld and mysqld_safe: 

As shown, stopping mysqld is mandatory. UPDATE 2013-03-16 21:46 EDT There is something incredibly tough you can try 

While there is a tool that can bypass these limitations and allow INSERTs, UPDATEs, or DELETEs (called pt-online-schema-change), the simplest approach for a creating an index is to just bite the bullet and let mysqld lock the table. 

This is as granular as you can get as far as the repair threads go. Each unique index and the PRIMARY KEY is loaded during the phase. All the remaining nonunique indexes are built during the phase. Since Oracle is concentrating on the development of InnoDB, I double if they will get around to implementing as a number beyond 1. Why? Think about. Index pages are written sequentially with set to 1. If was greater 1, each set of index pages per index would be written in an interleaved fashion. This would make the index fragmented. Of course, the solution would be to write each index's set of sequential pages in eight(8) different places. They would eventually have to be written contiguously into one common index space (in the case of the MyISAM table, that would be the file). Until Oracle comes up with a way to create nonunique indexes in parallel, you will have to settle for one thread. 

aggregate functions subquery usage clauses clauses sort order of results with no explicit clause query results using older GA releases of MySQL query results using newer beta releases of MySQL the current SQL_MODE setting in the operating system the code was compiled for possibly the size of join_buffer_size with respect to its effect on the Query Optimizer possibly the size of sort_buffer_size with respect to its effect on the Query Optimizer possibly the storage engine being used (MyISAM vs InnoDB) 

Linux and mysqld has no problem with it. Windows, on the other hand, does. I tried to do this in MySQL 5.5.12 on my desktop, that is, zapping the error file and I got this message: 

Thus, the PURGE BINARY LOGS command will delete all binlogs whose datetime stamp predates . Finally, run this command 

Each INSERT command you read has a certain number of rows. This number you cannot control unless you script to put them together in the number of rows you choose. I would advise against that be mysqld decided the best number of rows for that particular batch given the current configuration settings (perhaps the max_allowed_packet) SUGGESTION #2 : Use and batch them your way When you deactivate --extended-insert, each INSERT command will have one row. You can collect the rows using a script and collect as many rows as you like. CAVEATS 

In the event of any replication lag, is always the oldest. If they are the same, fine. You choose always. In this case, you purge to on the Master. In the event the SQL thread dies, you don't want to erase binlogs from the Master the Slave has not processed yet. Getting back to multiple slaves, you choose the oldest of all Slaves. To clarify, the oldest is the binary log name with the lowest number. 

The error message simply reared its ugly head because binary logging is enabled and there are stored procedures present. Either disable binary logging, mark the stored procedures as deterministic. Here is something quick-and-dirty you can do to all the stored procedures without having to edit the scripts: Update mysql.proc and do this: 

UPDATE 2012-01-12 14:03 EDT I refactored it again to make sure the keys and keys are combined correctly before retrieving the data from the table: 

gtid_purged can only be set when it is blank. Usually, you will set this variable set during the reload of a mysqldump whose source had GTID enabled. 

OBSERVATION #1 You mentioned Ask developers put all the temporary tables into a separated database If your developers are using CREATE TEMPORARY TABLE commands to create temporary tables, they need to use CREATE TABLE instead. Here is why: With MySQL Replication processing a temporary table, this is what occurs 

If you want, you can change the format thereafter. UPDATE 2014-02-26 16:06 EST Read your last comment Just subtract the difference from 10 min (600 sec) 

It would conveniently keep the index pages of that table from ever leaving the cache. The only table index pages would leave is if INSERTs into increases the contents beyond 2.5MB. You must choose enough headroom to accommodate many INSERTs into . OBSERVATION #2 I also noticed the index you laid out for : 

Get the output of date +"%s.%N" from the OS on the master UPDATE replagdb.replagtb SET tmstmp = numberFromStep1; 

This also means you may only have database level grants for all users. How can you tell ? Login as root@localhost and run this 

page 82 paragraph 1 and pages 88-92 refers to as a conditional control statement. Personally, I would call it a conditional control column or, simply, a conditional column. 

DISCLAIMER : Not a Full Expert on MySQL 5.6 Looks like the InnoDB Plugin is complaining about the present ibdata1 file. If data and index pages exist inside ibdata1, it's probably using Antelope as the innodb_file_format. There are four(4) things you can try: SUGGESTION #1 : UNINSTALL AND REINSTALL MySQL 5.6 

HOORAY !!! This is how you can copy a function from one database to another. FINAL QUESTION : Is this all worth it ??? SUMMARY Here is the original SQL to generate function creation 

These things are meant to measure SQL command counts and the amount of data passing here and there. There are other metrics within the InnoDB storage engine that should be monitored and tuned so that any write heavy behavior is improved. Rather than name those features of InnoDB, please go to the Server Status Variables page and look for any status variable mentioning the Redo Log Writes and Buffer Pool read/writes. 

Note : prevents recording the GRANT into the binary logs. Give it a Try !!! In your question, you said you ran this 

Outside of MySQL Replication, I can't really tell. One thing is for sure: Whatever mysql had as the setting for max_allowed_packet on a server that performed , the server into which you import data via LOAD DATA INFILE better have the same setting for max_allowed_packet or greater. The max value for max_allowed_packet is 1G. It couldn't hurt to just set that in my.cnf. 

I was looking over the comments from @Federico's answer. When you ran , you got the client program's version. Thus, the mysql client is definitely 32-bit. You need to run . This will give you the server's version (i.e., the version of mysqld) EXAMPLE 

If none of the passwords are of length 16, this may explain PHP's reluctance to login. Sad to say, but the alternative would be to setup 16-character passwords, but you cannot reverse-engineer 41-character passwords. You would have to manually setup the 16-character passwords using the original plain-text values. For example, if root@localhost had 'helloworld' as the password, it would have convert it using the OLD_PASSWORD function. Here is a comparison: 

Many times, some forget to have postgres default the timezone to that of the OS. If the WAL files also contain the timezone in its internal timestamps, you must align the timezone set in postgres with that of the WAL files. Something else to consider is the following: 

ANSWER TO QUESTION #2 SCENARIO You are driving at 3:00 AM. You are the only driver on the road. You come to an intersection. You have the red light. Question : Do you stop or go through the red light? Answer : Depends on the neighborhood 

It is SITUATION #3 that's the problem. When the SQL thread dies due to a SQL error, there is no built-in mechanism in MySQL Replication that triggers the IO thread to disconnect. RECOMMENDATION The only decent way to control the growth of relay logs is to set the limit on it 

Stop the server, if it's running. If you have the space to do so, copy the whole cluster data directory and any tablespaces to a temporary location in case you need them later. Note that this precaution will require that you have enough free space on your system to hold two copies of your existing database. If you do not have enough space, you need at the least to copy the contents of the pg_xlog subdirectory of the cluster data directory, as it might contain logs which were not archived before the system went down. Clean out all existing files and subdirectories under the cluster data directory and under the root directories of any tablespaces you are using. Restore the database files from your base backup. Be careful that they are restored with the right ownership (the database system user, not root!) and with the right permissions. If you are using tablespaces, you should verify that the symbolic links in pg_tblspc/ were correctly restored. Remove any files present in pg_xlog/; these came from the backup dump and are therefore probably obsolete rather than current. If you didn't archive pg_xlog/ at all, then recreate it, being careful to ensure that you re-establish it as a symbolic link if you had it set up that way before. If you had unarchived WAL segment files that you saved in step 2, copy them into pg_xlog/. (It is best to copy them, not move them, so that you still have the unmodified files if a problem occurs and you have to start over.) Create a recovery command file recovery.conf in the cluster data directory (see Recovery Settings). You might also want to temporarily modify pg_hba.conf to prevent ordinary users from connecting until you are sure the recovery has worked. Start the server. The server will go into recovery mode and proceed to read through the archived WAL files it needs. Should the recovery be terminated because of an external error, the server can simply be restarted and it will continue recovery. Upon completion of the recovery process, the server will rename recovery.conf to recovery.done (to prevent accidentally re-entering recovery mode in case of a crash later) and then commence normal database operations. Inspect the contents of the database to ensure you have recovered to where you want to be. If not, return to step 1. If all is well, let in your users by restoring pg_hba.conf to normal. 

This will load stage2 with the first occurrence of each name from item, zap the item table, and load the unique occurrences back. If you look back in @yercube's answer and compare it to my answer, his is much more simplistic because 

I also recommend reading the file when you download and install it. Perhaps the file will give you some guidance on getting proper documentation. 

That would certainly be one gotcha. Therefore, discontinue incoming feeds. Since you want to do to M1 and M2, I have one recommendation. Please set this one hour before syncing everything: 

This is the number of pages in the Buffer Pool that have to be written back to the database. They are also referred to as dirty pages. To see the Space Taken Up by Dirty Pages, run this: 

If the dirty pages rapidly start decreasing after hitting Ctrl-C, you will have to reimport without using This is as far as I can tell you without knowing the contents of the script. 

I see twice and a trailing right parenthesis that you did not type in the query. If your version of MySQL consistently produces that error message, then you must have a bug within in the Query Parser. You may need to upgrade MySQL as a last resort. SUGGESTION Your will not have any indexes. Try creating the table with indexes and loading afterwards 

This can be a double edged sword You can run multiple instances of mysql as read-only slaves on the same server as the master, provided each instance of MySQL resides on a different disk. This is only desirable if you are running older versions of MySQL that do not take advantage of multiple cores (CPUs). The latest versions of MySQL can be tuned to actually use the Multiple CPUs making it unnecessary for accessing multiple cores by running multiple instances of MySQL. At the same time, it is also a very bad idea. Many clients of mine have done this to save money on purchasing bare metal or VM servers. Any spikes in server load can affect all running MySQL instances because of bad queries, slow queries, too many connections, glutted memory usage, insufficient server memory, cache thrashing, and so forth, in any one MySQL instance. This also adds application complexity by having to access the different MySQL instances via their port number and also you would be at the mercy of TCP/IP. 

This is the number of pages with data inside the Buffer Pool To see the amount of data in the Buffer Pool size in GB run this: