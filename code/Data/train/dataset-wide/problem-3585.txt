You installed a libdwarf package for a completely different Linux distribution than the one you are running. This is not guaranteed to work. RPM is only a packaging format and cannot guarantee that the package contents are appropriate for your system. The dependency information within the packages does that, and that is why you can't perform this installation. In theory you could have multiple versions of a library installed, but in practice this only causes problems unless it's done in a very carefully controlled way (such as Software Collections). To fix the problem, go back to the correct libdwarf package, which is the one that came with CentOS. 

You're trying to install a package for Fedora 17 on Fedora 16. This isn't guaranteed to work, since the package versions for the package's dependencies may change from one version to the next. These dependencies do exist on my Fedora 17 box, though at least one of them appears to be different or missing on Fedora 16. You have three options: 

Several other less important virtio drivers also exist on your system and are compiled directly into the kernel (such as VIRTIO_PCI, the paravirtualized PCI bus). 

Things just go downhill from there. This means Apache hit the limit of the maximum number of processes allowed to run. You can view the limit by running . To increase the limit, run or edit (probably Red Hat specific) and add a configuration directive for Apache's user to increase the value for to something more reasonable. You can also try reducing Apache's and directives. 

The secret here is that simply creates as a copy, hardlink or symlink (a symlink is preferred) to a file in . So it is possible to do this entirely from your Dockerfile. Consider: 

The RFC it obsoleted, RFC 4408, said much the same thing; the newer version of the RFC simply clarifies the intention. 

There's no way for MySQL to know what PHP script was running. All it knows is it got a connection from username, password, host, and was asked for some query. So it logs what it has available. If you're the developer, you should hopefully be familiar enough with your application to locate the code making the query within a few seconds. If you aren't the developer, or aren't familiar with the application, try . Or contact the developer who is familiar with it. 

The mail server attempted to look up the domain name in the email address, , and was unable to find it. This means your recipient's mail server is having trouble resolving domain names correctly, as I was able to look this up successfully. The problem is with the recipient's mail system, not with your Google Apps setup. If the recipient fixes it before the expiration given in that warning message, then the mail will be delivered. Otherwise you will get a final bounce message after that time. 

You also have the IUS community repo installed and enabled. Disable this repo, as it conflicts with remi (and many other repositories). 

Because the partition on the new server will be slightly smaller than the one on the old server, you'll receive a spurious message at the end of this. However, since you shrank the filesystem at step 1, this doesn't matter. Resize the filesystem on the new server to the size of the partition. 

You cannot add a route whose gateway is an invalid IP address. If you try, you receive the error. (But you can add routes to invalid IP address ranges. This is to support bogon handling, blackholing, and various other oddball scenarios that you should not normally have to deal with.) To resolve this problem, renumber your network so that all hosts have valid IPv6 addresses. 

Use kexec to reboot your server. This skips the pre-boot procedures entirely, and reboots into a Linux kernel at the end of the Linux shutdown process, rather than resetting the hardware and going to POST. Unfortunately kexec is a bit cumbersome to use, so I wrote a script to make it easier to work with: kexec-reboot will allow you to choose a kernel from your grub boot list, or simply kexec the latest available kernel. 

RFC 2822, which obsoleted it, continued to explicitly allow this particular construction (Section 3.6.2). 

Other issues Users of CentOS and other RHEL clones on certain VPS or cloud providers may find that they are unable to update to 6.5. The command will state that no packages are marked for update. So far I have seen this on Windows Azure and some low-end VPS providers. In these cases, the provider of the CentOS image in use has modified in the image to point to repositories other than the official CentOS mirrors. This can be resolved by either manually editing the repo file to restore the official CentOS mirrors, or by locating the official RPM on a CentOS mirror and reinstalling it. For example (this URL is only good today and may go out of date later; check your mirror first): 

Congratulations, you've managed to use nearly all of your swap space. The first obvious problem here is that you went very deep into swap. This is probably what's causing the system to thrash so hard (lots of time spent in system, I/O wait and software interrupts). It looks like you killed some processes before this output, though, so some memory was freed up. First thing to do is to reduce the number of Apache processes that are running. You don't need that many for a small site, and it's just going to throw you deep into swap and kill your performance...which is what already happened. I would recommend you start very small and increase when it becomes necessary. An example: 

You're missing a rule to accept traffic based on existing traffic (the rule that makes iptables stateful). This should be your very first rule: 

Your configuration states that the error document is . Nothing in your configuration attempts to load at any time, ever. Rename the file. 

You have SELinux enabled, and in its default configuration it does not allow web servers to access content in user home directories. To allow the web server to access user files, you need to set the appropriate boolean, : 

This obviously applies to mail servers attempting to deliver mail to its destination. It's utterly irrelevant to situations where you are delivering all mail to a smarthost; the smarthost that you deliver to is responsible for following this, but you are not. 

If the UEFI option is disabled, you need to install the OVMF firmware package on your system, which provides UEFI firmware to virtual machines. The package name varies depending on Linux distribution, and you didn't say what you were running, so you will need to find this yourself. 

This looks like a single file, named . My guess is that you will find the content of this file is either or empty. 

You're missing the block which passes requests upstream to php-fpm. Copy and paste it from your block which serves the HTTP web site. Except for the SSL-specific settings, the two blocks should probably be identical. 

Run to get a dump of the current running configuration. This requires nginx 1.9.2 or later. And for the love of gawd, set up backups. 

I deploy a Drupal web site using git (well, OK, the developers deploy it; I try to keep them out of trouble), and for that reason the site has a directory and a file in the document root. Currently the permissions on the files are sufficient to cause nginx to return a 403 Forbidden error if these files are accessed in the web browser. However, I would like nginx to completely deny the existence of this file and directory, and return 404 if someone tries to access them in the browser. But wait, there's more! What I really want to happen is for the browser to receive the 404 error page generated by Drupal. In Drupal's page, I see that 404 errors are being sent to $URL$ a Drupal node with one of the Internet's more interesting 404 pages. My nginx configuration already contains a block which passes a request to Drupal. So I want to pass the request to Drupal, but I don't want Drupal to try to serve the existing static file either, but to simply serve the 404 page. 

I looked at a connection to your site in Wireshark, and the server immediately disconnected after my client began the SSL negotiation. This leads me to believe you don't actually have an SSL virtual host enabled. Try enabling it: 

Bugzilla is in the EPEL repository. Install that first (every CentOS system eventually seems to need something from it) and then install bugzilla using yum. 

Your machine does not have an active Red Hat subscription. Assign an active subscription to the machine and then try again. 

You should be able to do this within the confines of Red Hat's network scripts. However, you can't due to a bug in those scripts. 

Maybe. This is called backscatter, and some services do take it into account in computing IP reputation, and some blacklists operate exclusively based on backscatter. 

Your headers show that this mail originated from 172.18.248.18. So that's the machine you need to be looking at. 

You installed the wrong packages. You've added the upstream nginx repository to your server, but in this repo the nginx package is simply named . There are no other packages to install; those are Debian-specific (and often confusing). To resolve the problem, remove the packages you installed, then install the nginx package. 

That output is from . It's added when standard output is not a terminal, and in some other circumstances. Use if you don't want to see it. 

Change the Elastic Load Balancer's SSL Security Policy to ELBSecurityPolicy-2014-10 or later, or remove SSLv3 from your custom security policy. 

Once that completes, run to bring your system back in sync with the repositories. Now, you have duplicate entries in the db for several packages now, because of the transaction you interrupted before. You're going to have to deal with each of these manually, and it's going to be a real mess: What I would do is, for each of them, remove the older version of the package from the database only and then reinstall the package. For the first one, it would look like: 

The application is disregarding the system time zone and displaying the time in UTC. In your timezone, the summer time had just ended, so your time zone was UTC-0300. This corresponds to the time displayed by your application. 

In BIND 9 the "minimum" value in the SOA record expresses how long an NXDOMAIN response may be cached. This is the last value in the following example: 

Actually, in general it's a very good idea. However, it won't solve all your performance problems. There's a nice big data center in Seattle (more than one, actually) that's very well connected, and if you are located there you'll have very short latency to most places in the Pacific Northwest. Though not all of them. The network doesn't always correspond exactly to geography; the shortest path between two points sometimes goes to unexpected places. But for the most part it does. For traffic in the US, though, this probably won't buy you more than 100ms (I'm assuming you're currently colocated in Florida, about the longest path available that's still in the US) for your Pacific Northwest audience, and will make latency longer for a small percentage of your users. In short, it'll help for most of your users, and make things worse for a very small fraction. 

It would not be cryptographically secure to reuse the same challenge. If it were to be reused, anyone could receive a certificate for your domain name, because the "proper" data was already there! This is why a new challenge is issued each time. 

You installed the EPEL repository for CentOS 7, but you are running CentOS 6. Remove the package and/or the EPEL repo files from , and try again with the correct package. 

Modern systems are generally installed with a much larger /boot partition than in the past. The number has just been growing over time. Consider: RHEL 5 created a 101 MiB /boot partition. 

nginx already can cache 301 redirects. You can change the amount of time they are cached with the proxy_cache_valid directive: 

I would also suggest that you stop blindly copying rules from different web sites, and put a little time into understanding how the firewall works. 

The typical way this is done is to have the reverse proxy/load balancer set an HTTP header that specifies the original protocol that was used by the connection from the client. That header is known as . 

How did you manage to do that? I can't reproduce this on CentOS 6; the following error occurs (after several seconds of listing all the packages it's going to remove, which should have been a clue): 

Only if your backend server identifies itself in the response. For instance, you might configure it to insert an HTTP header in the response. 

You're using an OpenVZ-based VPS, and on this VPS your root filesystem isn't really a filesystem, but a directory on the host. That directory on the host has run out of disk space. This is a strong sign that your provider has significantly overcommitted resources and isn't monitoring them well, or at all. The replacement for simfs, known as ploop, is more difficult to overcommit, has a stronger guarantee of disk space, and performs better. I would strongly advise you to find another provider as soon as possible. This is very unlikely to be the last problem you will have with your current one. (And, of course, one not based on OpenVZ.) 

Your host 203.143.83.245 does not respond to DNS queries or even to pings. Since this is the only listed nameserver for your domain, it is therefore impossible for anyone to resolve any records in your domain. I see, however, that it does respond on port 22, which indicates that you have misconfigured your firewall. Fix the firewall configuration: You need to allow inbound port 53 on UDP and TCP; and it's also a good idea not to block ICMP (as you have). You only have one nameserver. You cheated by assigning two hostnames the same IP address. This is a bad idea because if the single nameserver has a problem, there is no backup. (RFC 2182) 

Make sure that you have an up to date package. If you rely on correct time display, then this is a critical package you should keep up to date, as it holds all of the worldwide rules for daylight saving time. Make sure you set the desired timezone in in addition to setting up . Make sure you aren't accidentally using the zone, which does not observe daylight saving time. 

If a direct kernel boot is present, it will always be used, and the regular boot order will be ignored. The best way to handle this is to not use direct kernel boot for installing the system, but rather pass a where the Linux distribution you are installing lives (which can be on a local disk or a remote web server, FTP or NFS server). This also allows you to inject a local kickstart or preseed file for a completely automated installation. For example: 

is fine, but you will want to add the package so that the hypervisor can cleanly shutdown and reboot the virtual machine. That's about all I can think of.