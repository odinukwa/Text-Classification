i would like your help in the best way to tune and re-write a stored procedure. it is consuming a lot of time and don't know what should i do to make it faster 

note : the counts for IX_CoursePrerequisiteAssignment is not accurate because i just rebuild it These columns are the predicts for the top executed queries 

On a QA server I need to give access to specific login from just one IP to log to this server. I wrote This login trigger 

we are going to upgrade from SQL server 2012 Standard edition to SQL Server 2016 Enterprise edition. we can't have a downtime more than 30 mins .The new SQL server will use always on availably group 1 primary and 2 read-only secondaries . I am looking for the fastest way to do this upgrade. I am thinking of creating mirroring between the old server and new primary server , then shut down the application update the connection string , fail over the DB to the new server, stop mirroring, start the application. After the application is up, start in configuring the Availability group. IS this a correct way to use 

If I want another report application to connect to the secondary DB. The connection string should be point to secondary or the listener Also, i when i try to connect to the databases secondary server with a SQL authentication user i failed. 

If I am going to set an Alwayson Availability group with 3 nodes, with read-only routing. Do I need to configure a Quorum ? 

Update I find this Delete in the Query Store , there were 3 plans and I forced SQL to use different plan. I am wondering Why SQL choose to use the most expensive execution plan? 

we have a SQL Server DB running on Amazon RDS, i need to refresh it. But in RDS there is no option to restore a backup. What other ways i can use to refresh this DB? It is 60 GB, and has 200 tables. Thanks 

However the trigger is preventing the user from log although the IP exists in the IPAddress table. Any idea where in the problem in the trigger 

SQL Server 2016 Enterprise Edition SP1 Database Mail was working fine with no problem until a server patching was performed. I tried to do all the troubleshooting available but still all mails got queued and no records in the log. Finally when I checked the database mail program I didn't find the executable file in the Binn folder or any other folders! My questions are how this happened and how to solve it ? 

If I have database db1 with schema sch1. I want to create another schema sch2 which will be used by different application and the tables in sch2 will be populated from tables in sch1. So my approach is to create triggers on insert, update, delete perform this task So my questions are 1- Is this a good way to do this or there is a better way? 2- I don’t want to affect the performance on db1 so I am want to replicate it to different server and create the triggers on the subscriber DB, so replicated server will have Sch1 and Sch2 on it, Can I do that? This is on SQL Server 2012 Standard Edition Note: tables in sch2 have different names and different columns name. also more than one table in sch2 may get populated from data from sch1 

resizing files (data and log both) large number of inserts happening while 1. was going on one particularly heavy query running while 1. was going on a hard drive with some pretty high i/o times in general lack of memory on the server, so hitting the page file on top of everything else 

So, anyway, it was a combination of a whole bunch of different things, mostly related to SQL, but not exclusively (so Will was correct there). I'd love to split the answer between everyone, as they had portions of it right, but what can you do... 

I have a fairly high-throughput application that occasionally decides to collapse on me. It's not very often - about once every ~3 weeks or so. When it does, if I check out perfmon, I see 100% "Avg. Disk Queue Length" pegging the server. During these times, I also see lots of nice connection failed messages from SQL Server. I'm no SQL Server expert, I can do the basics for indexing, taking backups, etc., but that's it. What would cause something like this? I was thinking perhaps it was a resize of the database (it was down to ~300MB available [and it's a 30 gig database]), or maybe some reindexing gone nuts? I do have one table in particular that has tons of inserts. Very few reads, but many inserts per second isn't unusual at all. The server has only ~4 gig of RAM as well, but we do have a dedicated warehouse box that rolls up data every night where most of the heavy querying is directed. Anyone got any thoughts on what might cause that huge queue length? 

change the autogrowth settings so I'd have consistent growth when it happens, manually grew both the temp and primary database files to have more space, added additional vlogs to the temp database, and set up a notification so I can manually grow when the database gets below a certain space level. [nothing to be done about it] made the heavy query run less often; it was loading up data the user didn't always need, so changed it to run on-demand [working on getting a new server, this app is growing fast] [working on getting a new server, this app is growing fast] 

I'm transitioning from an old web/sql box combined to a much more robust solution on a completely different host (800hosting to RackSpace). Our application has very high uptime requirements. It runs 24x7x365 and impacts literally thousands of sites, and I want to ensure the minimum possible downtime during the transition. (Don't ask how we're pulling this off with a single box right now.) My plan for the web server is a little bit of DNS work to point a new subdomain to the new server setup, and forward requests from the existing server. That'll take care of flipping everyone to the new web box fast enough. The problem is the database - how do I keep it in sync until I'm ready to flip the switch on the website (or is it worth the effort vs. just accepting the downtime to stop the app, backup, compress, transfer, and restore?). A few details. We're running Sql2k5 on the old system, 2k8r2 on the new. The database itself is ~30 gigs for the primary, ~60 gigs for the warehouse. I can live with downtime on the warehouse, but really want to minimize the impact on the main database. Any suggestions for the best way to migrate the database across from the old setup to the new? 

I think I’ve figured it out (at least partially). It seems that in order to avoid creation of duplicate log file for the child package the value of “Log_Path” variable in the “child” package have to be made blank. If there is no value, the validation process will not create an extra file and will properly inherit value specified in the “parent”. This still doesn’t fully resolve the issue with the “parent” package, because I can’t run it from the development environment without any value specified for the “Log_Path” variable. The only way I found around that is to make it blank, save it and then execute it from the command line (DTExec) while passing the desired variable value via SET option. This finally results in just two files instead of four. I still don’t understand why validation process (at least I think it’s validation process) creates those “extra” files using design-time values. This just seems like a wrong behavior. 

Initially everything seems to be ok and the results indicate that all of the tables match. Next I go over to the subscriber and manually delete a few rows from some of the tables. I manually verify that the row counts in my tables are now different between the publisher and the subscriber. Finally I run the sp_publication_validation procedure again and .... it says that everything is still OK. This is wrong! I also tried to return both rowcnt and checksum and it still doesn't detect the fact that there are differences between the publisher and subscriber. I appreciate any ideas. Thank you! 

You can check the execution plan of the queries and verify that this index is actually being leveraged. The index helps, but since I have to make changes to dozens of publications at once, it still was not enough. So the second part of the fix was to simply introduce a random delay. I added up to 3 min delay to my deployment script and this scattered the queries enough to significantly lower probability of deadlocks (but not eliminate it completely :( Something like this: 

I would like to ask for help coming up with a query, which can identify groups on non-overlapping records. Here is a sample scenario (admittedly contrived). Let's say I have employees who are assigned to work on various projects. While an employee can be assigned to multiple projects, he/she can only work on one project at any given time (don't you wish we all had this luxury :). I need to find out which projects can be scheduled to be worked on in parallel because they do not share any employees. Here is some code to setup sample tables and data. 

Obviously it's still looking for the path specified earlier, but why? Again, I set "Delay Validation" = True on all my connection managers and the package itself. I appreciate any help on this. Thank you! P.S. Here is a complete expression for log path: @[User::Log_Path] + "\\" + @[System::PackageName] + "_" + (DT_STR, 4, 1252)DATEPART("yyyy", @[System::ContainerStartTime]) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("mm", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("dd", @[System::ContainerStartTime]), 2) + "_" + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("hh", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("mi", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("ss", @[System::ContainerStartTime]), 2) + ".txt"