EDIT After talking with the programmer he seems to think that passing parameters to one variable and assigning them to a declared variable helps with security AND it will cause queries to run faster. 

We use stored procedures and so far we haven't detected any injections that have been successful, but we see attempts all of the time. I started logging some of the data hoping to gain insight on what people were attempting and possibly use it to block any future attempts by new methods. Here is the code of something someone tried to run on our site today, they tried it with syntax 8 different ways. I can understand some of it, but not all of it. So I was hoping someone else who knows more could tell me how exactly this works and what it was trying to do (I believe it was trying to insert links to a page using hidden divs in MAX columns in all of our databases). 

Despite this explicit warning from the developers, some people continue to rely on undocumented behaviour to determine the value that will be selected from a hidden column. In particular, MySQL often appears to select the "first" record from each group (where the notion of "first" is itself undocumented, such as the oldest record on some storage engines or according to some sort order applied to a materialised table from a subquery). I've seen this exploited to retrieve, for example, a groupwise maximum: 

You've pretty much hit the nail on the head. If the command results in a change of default database, then the MySQL client branches into . Provided that the client was not started with the option, that function: 

After setting it to 0, it will prevent everything in your current session being written to the Binary logs. Setting it back to 1 will resume writing to be binary logs. Everything else continues as per normal. 

I'm currently investigating moving our database to an (vs buying bigger servers (again) and moving the master / slaves to these). So far my testing on a small cluster has been fruitful, and suggests that we would benefit from this (especially in data resiliency / write access). However I have one small niggling concern. We have one large table (apx 30% of the whole database size - or 60GB apx at present) which is comprised entirely of BLOB fields (80+), and a single field at the start to identify the row. Few of these contain very much data (mainly some text/ numbers/ dates etc), many are actually empty (NULL). None of these are indexed. We add new rows constantly, and read the old rows frequently too. Updates are few and far between. Is this likely to become an issue when running this in a cluster? From my reading I understand that BLOB fields are held in separate tables/pages in the background, so need to be read separately to the main data. But can't see if this is like INNODB, where the first x data is stored in the main table, the rest held separately, or if the whole thing is separate. I ran some simple tests, inserting records with similar data, (both disk based and normal), with various joins etc. which showed no differences. But I am concerned that my small test system simply isn't stressing it enough, and don't have the resources (or finances) to run a full size test yet. Can anyone with some experience of this tell me if I am going the right way with this, or am I just walking into a nightmare? 

I believe that one should never rely on undocumented behaviour because there may be unforeseen corner cases that cause that behaviour to break. For example, in satisfying a operation with an index, MySQL sorts the results and may thereby choose an unexpected value. What other corner cases can break this behaviour? Or is it sufficiently reliable for production systems? 

hashes all SQL commands; executes and hashes the results; executes and hashes the results; and for each table, calls and hashes the results. 

However, you can also manage account permissions by manipulating the MySQL grant tables directly (and then executing ). 

So I'm thinking that this means I am creating a lock called that will remain in place until either I release it, or 10 seconds has passed. If I then open another session (simulating a second application), and try to run it just returns a . No matter how long I leave it, the lock remains in place. I would presume the full process would be something like: 

In MySQL Cluster (7.3), is there any way to lock down the section in to a small subnet, rather than setting it to a single IP Address, or open for any IP Address? I've searched the documentation and the web, and I can't see anything that says Yes or No specifically. I tried it on my test cluster (all running on my Win 7 PC), and trying didn't work, nor did . It just seems to be a security risk leaving it open like that, but defeats the point of adding 'spare' slots if they have to be pre-allocated to a fixed IP address. 

That's exactly how it was injected. They declare some variables to move data around, then get the name of each of the non-system databases. From there I'm not sure what it's doing but I'm guessing it goes through all the tables looking for max columns and if it finds one, it finds some rows and writes links to them. 

Our senior programmer has been having me write stored procedures in the following format to protect against injection attacks. He says that the best practice is to take the parameter in, then declare a new variable in the body and assign that var the parameter, that this step is extra protection against injection attacks as it forces any injection attempts to be considered data and not taken literally. Is that true? I would think this would slow down the query and use extra memory and not add any extra protection, but I could be wrong. Example below. 

Looking at you config, it seems you are missing in your my.cnf files. You will need to add this in to both masters files and restart them. This tells the slave database to take anything it receives from it's master, and copy from the relay log to the binary log so it can be replicated onwards. e.g. Master 1 Updates a table. It gets written to the binary log on Master1 This is replicated to Master 1's slaves (which are slave 1 and Master2) In order for it to proceed to slave 2, master 2 has to be told to add it to it's binary log. This is what does. 

For completeness, the same can be accomplished in a standard and documented fashion with a simple join: 

This suggests that is a limit on the total size of the columns that one is selecting, above which a will be used instead of an index-based sort. In your case, selecting (5002 bytes) takes the total size over this variable's 1KiB default value and therefore is used. To raise the limit to 8KiB, you could do: 

So, in your case, it is making 582 separate database requests (one for each of steps 2 and 3; and 580 for step 4), looping over the results of each one. Ouch. Of course, it'd have been imminently more sensible to do (or at least first attempt to do) a single but ours is not to ponder whyâ€¦ 

I have the below Data / Query. Is this really the most efficient way of doing this? As it seems like a lot of work to get the result. I've tried reducing it / combining the parts but this is the only way I can get it to work. (n.b. The actual table has a lot more entries over multiple days - hence the WHERE filters at the end) 

for testing this, but you would probably want a , , datatype depending on the data you return and how accurate you need it. Also this doesn't include the or fields. You could use which would give you the timestamp field. The database name is tricky, as you would need to carry two variables across, and my script (above) only allows for one (if you are only looking at one database you could just type it directly into the query / set a default value). 

In this case, since the forced collation is defined over the same character set as the column's encoding, there won't be any performance impact (versus defining that collation as the column's default; whereas will almost certainly perform slower in comparisons than due the extra lookups/computation required). However, if one forced a collation that is defined over a different character set, MySQL would have to transcode the column's values (which would have a performance impact); I think it does this automatically if the forced collation is over the Unicode character set, or raises an "illegal mix of collations" error in any other circumstance. Note that the collation recorded against a column's definition is merely a hint to MySQL over which collation is preferred; it may or may not be used in a given expression, depending on the rules detailed under Collation of Expressions. 

Not exactly what I had in mind. Am I missing something, or have I completely misunderstood what does? I'm not sure why I didn't get the latest version either. I ran followed by . (as you may have gathered, I'm not really a Linux person) 

I'm guessing I'm doing something that's prohibited by MySQL, but can't see what. Basically, I have a set of tables that hold lists of things (, , etc.), and I need to create a joining table to hold each of the ID's (e.g. , , ). At present there is no real relationship between the various lists. This table needs to be automatically populated each time I add a new . To get the matching entry, I need to use a statement to match the first letters of the , and get the relevant entry from the other table(s). My plan is to have a set of Procedures (one for each table I need to join):