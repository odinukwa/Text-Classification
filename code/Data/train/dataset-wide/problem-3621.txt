It turns out this is a bug with Kerberos in Windows 2008. Microsoft has just released a hotfix for this (see link below). The hotfix completely resolved all my issues. $URL$ 

I have a pool of virtual machines running in ESX that have their virtual hard drives set to Indenpendant/Non-Persistent mode. I won't go into the reasons for this other than to say it is working really well for us. Periodically I need to reset these settings back to the standard persistent mode and would like to automate that via a scheduled task for script. Does anyone know how to do this? 

I'm setting up a new Search Center for our Intranet using MOSS and want to make heavy use of of Keywords and Best Bets. However two questions about this have me perplexed and I would appreciate any help/guidance? 1) If you assign a contact to a keyword/best-bet and set a review date, my understanding is that SharePoint will automatically send that contact an email alert when that time comes. However in my testing just using my account as the contact I have never received one of these. Am I doing something wrong? 2) What permissions would a user need to edit/update a keyword they are the assigned contact for? I assume too that if they have the rights to update one of them then they can probably unfortuantely update them all, correct? 

I have a SharePoint 2007 farm (w/ SP2, June cumulative update) hosting an Internet site and am seeing the nightly content deployment incremental job that moves content from our staging environment to production fail with the error below. When this happens the only way we know to resolve it is to manually run a full deployment. However the issue seems to come back again in a few days. Any advice on how to isolate the root cause of this problem and correct it? A duplicate name "9f2cdd1e-e4a5-433c-b4eb-f2baf9a46f0f" was found. at Microsoft.SharePoint.SPFieldCollection.AddFieldAsXmlInternal(String schemaXml, Boolean addToDefaultView, SPAddFieldOptions op) at Microsoft.SharePoint.Deployment.ListSerializer.CreateOrUpdateField(SPList list, String fieldName, XmlNode fieldNode) at Microsoft.SharePoint.Deployment.ListSerializer.UpdateListFields(SPList list, Dictionary`2 listMetaData) at Microsoft.SharePoint.Deployment.ListSerializer.SetObjectData(Object obj, SerializationInfo info, StreamingContext context, ISurrogateSelector selector) at Microsoft.SharePoint.Deployment.XmlFormatter.ParseObject(Type objectType, Boolean isChildObject) at Microsoft.SharePoint.Deployment.XmlFormatter.DeserializeObject(Type objectType, Boolean isChildObject, DeploymentObject envelope) at Microsoft.SharePoint.Deployment.XmlFormatter.Deserialize(Stream serializationStream) at Microsoft.SharePoint.Deployment.ObjectSerializer.Deserialize(Stream serializationStream) at Microsoft.SharePoint.Deployment.ImportObjectManager.ProcessObject(XmlReader xmlReader) at Microsoft.SharePoint.Deployment.SPImport.DeserializeObjects() at Microsoft.SharePoint.Deployment.SPImport.Run() 

This says the mail was successfully received. On my server the next step is spam cleaning. Then comes the delivery: 

I tried to add 0:01:0 as a spare to the set using but this seems to have forced the volume into readonly mode and blocked any attempts to use afacli on the controller. After waiting 10 mins or so, I rebooted. State remained as above. So no my question is: how do I convince the missing drive to re-join the mirror set? If it helps: 

It is hard to tell without seeing the traceroute between the systems. Also, of course, if there are dialup links involved, it may also be reasonable. Having said that, 566ms is sort of large. I get 510 ms over my Swedish ADSL link to a typical Chinese site, so if your site is a well-connected site in a data-centre in the West, you should probably expect less. For example, from a Swedish data-centre to a US East coast Amazon E2 host, I get less than 100 ms. 

Quality In my experience, your company suffers in reputation from bad phone lines, so make sure you get get decent quality from your installation. Quality is among other things: 

I assume that you are referring to $URL$ . For what it is worth, I use it to warn for numerical intervals to good effect. Don't know about graphing. 

The second row is standard in most modern Linux distros. In the first row, I'm pretending that my localhost is our Subversion repository, so that I can then tunnel that traffic to my workplace network. The URL $URL$ will actually resolve to $URL$ but still contain the HTTP host header Host: repo.company.com. Does that sound like it addresses your need? 

I think you should be able to solve that by using svn switch to tell Subversion that you are talking to "another" repo. --relocate may be needed in this scenario, I think. 

So yes, it is allowed and properly written software will handle it just OK. CNAME chains aren't however considered good practice and impose an overhead on the infrastructure. 

All layers of the storage stack have to support trim. After all only file system layer knows, which blocks are safe to be wiped. This post on the ext4 developers list with patch for elements of trim functionality in ext3 strongly suggest that such work is being done already. 

It depends on your load. If your app is single-threaded, CPU-bound one, the GHz trumps almost anything. If you have a multiple multi-threaded apps (e.g. a web server), then core count is more important than single-threaded performance, and if you are I/O bound, then CPU doesn't really matter much. 

Generally you want to give your users minimum privileges they need to do their job. Services do not need shell, so you generally wouldn't give shell access to accounts dedicated to running a daemon. Especially, if a service accessible from network runs as a particular user, it's good idea not to give that user shell access. The reasoning is, that if your service gets compromised, then the attacker won't get shell access to the system. If this user has a shell access, the attacker potentially has one obstacle less to overcome to take over your system. If this user has a equivalent privileges (via ), then if an attacker manages to trick the system to run some command, he can do it with authority. While having a web server run under a uid with shell access is something you can reason about, your setup is very close to running the service as . Bad idea IMO. How do you administer? Either "by hand" or find a tool that does not require you to compromise your systems security. 

If it has a port open to the net, it's a target. As simple as that. There is very hostile environment out there, and botnets provide plenty of resources to probe whatever lives on the net. If you are in a hurry and the current solution is painful, but works, I wouldn't touch it. Give it more thought, research your options and don't act until you know what you are going to implement and why. 

I have a Linux guest that uses an LVM volume directly as root file system (that is, there is no partition table). libvirt config looks thus: 

But to no effect. How do I instruct libsasl to use saslauthd authentication? (I can of course create /var/spool/postfix/etc/sasldb2, but this will still not result in connections to saslauthd.) 

I think pvdisplay tells you free PEs on that physical volume. I assume that means PEs not allocated to logical volumes. 

I am trying to build a new CentOS 6 VM from my CentOS 5 host, but it appears that something fundamental has changed between these versions: 

install radvd on the LAN side of the router. setup a 6to4 tunnel to pass your IPv6 traffic over to IPv6 Internet setup firewall rules for IPv6 

If "some considerable amounts of money" translates to expensive downtime, I think that you probably want to focus on uptime rather than performance. Can your application be redundant and/or clustered? How quickly can you restore from backup? 

However, this time it failed to do the trick. I rebooted the box in the hope that the BIOS interface would have something, but nothing obvious. The current state of the RAID1 set is as follows: 

After this edit, yum works as expected and I have a working CentOS 6 built from CentOS 5. Any chroot environment/VM guest setup instruction for CentOS 6 should be able to get you to a full-blown system. 

Typically, filesystem performance differs when you have 1) very small files, 2) very many files, or 3) very deep directory trees. Since neither of these criteria apply here, I expect you are not going to see a significant performance difference from filesystem. I would choose ext3 in order to be as mainstream as possible. 

Doing "consumer &" will simply background the task and go on. If the owning shell terminates, it will terminate any background tasks. You say the script works on command line, but your daemon won't survive if you log out? You want to start your daemon with something like start-stop-daemon. EDIT: actually, on reading your text again, I'm not sure if consumer is a daemon at all? If you just want to run some code on startup (e.g. housecleaning), you write a line in /etc/rc.local. If the script takes a long time to run, you may want to disown it. I.e: 

Depending on the configuration of sshd on the receiving end this may or may not fulfil the requirement of "no remote file modification". 

You've been lucky ;). Drivers make assumptions about functionalities present and bugs absent in firmware newer than version X. Only certain combinations of drivers and firmware are tested, because it would be prohibitively resource-consuming to test all the pairs. Also it doesn't make sense to verify new versions of code against an ancient counterpart with known bugs fixed in later releases. If the server was running for a long time it may be that you've found another working combination or simply lucked out and didn't hit any bugs possible because of the mismatch. Worst case (based on experience with non-Dell hardware): server corrupts data on the disks, crashes and refuses to boot. HW specialist called to repair it will wonder why did it ever run with this FW/driver combination. Best case: server runs without any problems until it is replaced by a newer one 10 years since. My recommendation would be to upgrade to the next to newest version of firmware and the corresponding recommended driver. This should give you something that is reasonably new and supported, but not cutting edge code which may have yet undiscovered bugs. Note: Always have tested backups when modifying storage system configuration. 

The easiest way would be to download an rpm packet with the newest version of RPM and then upgrade (. You may need to download several more packages if the new version will require some packets that you do not have installed. 

If your servers all the same hardware, then the reason must lie within software. Older version of CentOS means older kernel and older LAMP stack (unless you compile your own). CentOS 4.8 runs kernel 2.6.9. CentOS 5.5 is 2.6.18. 2nd factor would be tuning. If you fine-tuned 5.5s, but run an out-of-the-box 4.8, this will factor in as well. Developers strive to improve speed both of kernel and applications, so yes, OS version may play important role in the speed delta. 30% improvement on identical hardware? Not impossible. OTOH if your servers don't run on identical or very similar hardware, or if the 4.8 one is the one connected to the rest of the net by that ancient over-heating half-duplex 10 Mb hub locked in the broom shed, then hardware may be the dominating factor explaining the speed difference. CentOS 5.5 on quad Nehalem with 32 GB RAM should be significantly faster than CentOS 4.8 on a P IV.