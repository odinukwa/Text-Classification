It sounds like your agents already have their own e-Mail addresses. If you run your own e-Mail server, you can specify the , , and headers to anything you like. Anything more can be handled by the script you use to send the e-Mail. The main issue will be if any of your agents have SPF records configured for their mail servers. If this is the case, some recipients will likely deny the e-Mail if you change the envelope sender. (Return-Path header) In this case, you could have the Return-Path be an e-Mail address that does not have SPF records or that has SPF records configured for the sending SMTP server. Another potential solution would be to setup e-Mail aliases for all your agents and forward them to their existing addresses. 

If you can avoid cascading switches, I'd strongly encourage it. The downsides are easily exampled with two people on the same cascaded switch copying a large file to a fileserver. Having a single user under normal usage being able to cause usage issues for multiple other users is obviously bad. Implications are even worse with servers. 

What you are describing sounds more akin to a shared hosting environment. With a shared server, I would typically configure each user's Web sites within their home directories within public_html. If they had multiple domains, I would follow a similar convention as above but within this directory. I would also separate the and within their home directory but outside of the Web tree. SuExec and and mod_suphp would also be advised to prevent users' data being exposed to other users on the system and to limit scope of any users' Web sites being vulnerable. If the users did not have domains, you could use subdomains within a primary domain or mod_userdir. If you would rather not take the more organized approach, I would suggest using sub-directories, groups, and the SGID bit within /var/www. Nevertheless, that would not be the ideal approach in almost all situations. 

I've split my answer below addressing those two things separately but they are very closely related. I am addressing the technology solutions here and not any of the best practices that are related, such as change control. If this does not cover the scope of your question, please clarify and I will be happy to elaborate. This is necessary foundation, which is critical for a well-run technology infrastructure. 

There's a lot of initial diagnostics left to perform here. Errors in ? Errors in ? Packet loss evidenced in ? Issues with link negotiation? Are there any differences, physical or not, between the Windows box and Linux box? Edit 1 Can you reproduce the performance using different protocols and sites? 

One potential solution for monitoring daemons with heartbeat 1.x that I've used successfully in production is to have a perl daemon that polls the process list at an interval and if it doesn't see the process name, it initiates the heartbeat standby. You could configure mon to duplicate this functionality. Heartbeat 2.x has native support for this functionality and the ocf_heartbeat_apache resource agent will allow you to monitor the process. It is also notable that heartbeat is in maintenance only mode and that Corosync is supposed to succeed it. Where possible, I avoid the active/passive approach to high availability with Web or application servers. It wastes resources and it's often more difficult to scale horizontally. There are some situations where this is unavoidable but with Apache, this is rarely the case. Your Web application would need to share session state, which is often accomplished by storing the sessions either in a database or in memory. At this point, you can load balance all incoming connections using a solution such as LVS or Nginx. Of course, your proxy and load balancing solution would need to be redundant to prevent a single point of failure. You could use heartbeat for this as well. 

You can use dot-qmail files. Literally, you would just prefix the first line with a and run it through a command. The second line would specify the user to deliver the mail to. For example: 

The brief interruption to someone's work is more likely going to cost less than buying and maintaining a desktop UPS. That's one of many reasons why many professional solutions are server based and centralized, as the service level can be focused there. If there is a business justification with Return On Investment (ROI) in your environment, I would suggest a cost analysis between providing generated or battery backed power to the entire building versus buying individual UPSes for every workstation. A server does not necessarily need to "support" a UPS, it's just a battery that's inline with the power source. Any "support" is going to be automatic shutdown or taking a preferred action in case of power loss. The idea is to protect the equipment by conditioning the power as well as allowing the equipment to be properly shutdown in case of an extended outage. Depending on your budget, it is also often to maintain availability in case of power loss but with that service level you often have power generation in addition to a UPS. A UPS is in addition to backups and has no direct correlation to backups beyond being a best practice within properly run IT infrastructure. Without a UPS you could lose data, have hardware failure, and have system availability interrupted. It is often easier to find the ROI here than with workstations. 

You can see it by running when before executing another query when a warning is identified. The schema does not support the time of data you are attempting to insert. For example: 

Good question. In the context of MySQL, you might be better off using tools designed for that purpose. You have Sysbench, Supersmack, and others. See the links below. 

NOW() returns the current time and timezone as currently understood by the server. If it is not returning the current time in PST, it will be the server that needs changed not MySQL. MySQL Documentation 

CentOS is based on Red Hat Enterprise Linux. I've linked some of their documentation. If you have a more detailed question, we will likely be able to assist further. Interface Configuration Files 

There's also utilities like and that are handy. Edit 1 Isolating is going to depend on your environment. Where are the connections from? If localhost can it only be a system script or do you have an application being served from your DB? If remote host, what function does that remote host serve? Is it different from others? Does it have scripts or is it only an application? It should be fairly quick and simple to isolate the source but not necessarily the cause. 

is the classic method to do this in . Also, you can specify using flags as . Earlier than MySQL 4.0.2, most configuration options were set using this method. This is depreciated and even removed in MySQL 5.5. Most options can be set using their actual names in the my.cnf. If you need further help, please be more specific. 

Links to official and recommended documentation exist on the Netfilter Web site. This is not a new subject, resources are limitless. Most basic commands are fairly intuitive and can easily be reference to the manpage. netfilter, which is the kernel level technology that enables the packet filtering, is quite advanced. There are additional tables that can mangle packets, translate packets, and otherwise affect routing. The utility is the userland tool for interacting with netfilter. If you wish to learn about advanced functionality, I suggest you reference the aforementioned documentation. For an introduction to the basic functionality, please read further. To list all existing rules: 

I am not injecting opinion here. This does not reflect my overall opinion or preference but is specifically answering the question. I have no desire to debate preferences of distributions. 

I use it currently in a highly available production environment serving numerous public users. I swear by it and would encourage you to give it a try. With that said, there are alternatives depending on your application and preference. These include: 

Use a host based IDS, which is sometimes referred to as a filesystem IDS. I've used AIDE successfully against Windows before. It can be compiled with cygwin. An alternative to AIDE is OSSEC, which appears to have a Windows port. 

When you exit an interactive bash login shell, it sends a SIGHUP to all children unless the shell option is set to off. When most userland processes receive a SIGHUP, they will exit. You can also prefix the command with to make it ignore the SIGHUP. Moreover, you can it using an internal bash function. 

In a nutshell.. System wide with RHEL/CentOS, the initialization scripts are contained within , which is System V style initialization. Others, such as Slackware, use BSD style initialization. The scripts are started by init, which is configured in . On the user level, it is going to be unique to your shell. With , the default shell in most Linuxes, an interactive shell is going to use both and . Non-interactive will use . There are a lot of details here and if you want a particular answer, you will need to clarify. 

Look at your log, as it's most likely some type of table space corruption. The location of the log file is specified via two methods. The first is at run time with , which would often be specified in your init file. More commonly, it would be specified in your my.cnf with . Ultimately if left unset, it defaults to the data directory as your . Search the filesystem for a file with the extension of . Chances are, you'll find the log. 

First, you should scrub real data out of your development environment. Also be aware that there are a lot of variables and options here. On the Linux development server, I would first prevent the mail from sending out by either filtering the traffic or stopping the mail daemon. To stop the traffic, this would work in most modern environments: 

I've had an end-user authenticated proxy in place for years and yet to encounter an important situation that did not have a reasonable work-around. Point being, they are rare. In Windows, the utility is often a big help. Work-arounds are often specific to software and depending on the type you may find more help on Superuser. You could use ACLs but authentication usually makes more sense. 

100Base-T connections are capable of 12.5MB a second. I have no idea where you are getting 12KB from. You are looking for link aggregation, which is otherwise known as NIC bonding or teaming. This is supported in most operating systems. 

isn't a dynamic system variable. If you need to change it, you're going to have to restart. I find it difficult to believe that it isn't logging. This setting is specified via two methods. The first is at run time with , which would often be specified in your init file. More commonly, it would be specified in your with . Ultimately if left unset, it defaults to the data directory as your . Search the filesystem for a file with the extension of . Chances are, you'll find the log. 

Yes. Keep in mind, the queries will replicate and execute identically on the slave as with the master. If the column is on the end of the table, it could simply use the default value. If it is in the middle, you could introduce data type conflicts or potentially even replication failure if the insert failed. 

If it is convenient for you as a temporary solution, it should be perfectly acceptable. I cannot think of many scenarios where having multiple PTR records with the same hostname will introduce any technical issues. One potential scenario would be mail delivery on the new server. At least, if the forward lookup resolves to the old server. Fickle mail servers will bounce mail without hostnames/IPs being able to resolve both ways and match. Outside of that, and I'm really trying, I can't think of any. If there's more, it's likely to be of limited scope like above.