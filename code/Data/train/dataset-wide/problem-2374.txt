No, procedures that read data from the database are also generally non-deterministic (because that data might change between calls) 

Ok, that changes everything - Flashback Data Archive is not for you. Instead consider moving to a 2-table solution: one for current data and one for history. 11.2 allows many more DDL operations such as than 11.1 but AFAIK there is no way of splitting the table into two copies retaining the history. 

Apex users are stored in Apex tables and you can create groups to restrict access to various parts of an application. 

No, this type of delete generates roughly the same quantity of undo regardless of whether it is an IOT or heap. In contrast, inserting into the IOT generates far more undo. heap: 

build up a list of all possible values for the 'missing' leading columns (this can be done fairly efficiently from the index structure itself) iteratively perform range scans for each combination of missing columns and the column provided union the whole lot together in one result set 

Some explanation in response to comments: In each case (the testbed and the min/max query), the subquery factoring clause just generated a list of (year, month) tuples: 

-- edit As @Leigh has commented, an alternative, neater, approach is to have a single function concatenating the (modified) regexs: including the on the end in either case makes the ordering deterministic (repeatable) which might be a good thing even though the question doesn't specifically ask for it. 

or just : waits for all sessions to disconnect. This mode is rarely used in practice because it relies on well-behaved clients not leaving connections open. This used to be the only mode that did not cancel running transactions. : disconnects sessions once currently running transactions complete, preventing new transactions from beginning. : disconnects all sessions immedately and rolls back interrupted transactions before shutting down. Note that the disconnections are immediate, but the shutdown may not be as any interrupted transactions may take time to roll back. 

From what I can tell, "single-threaded" is a bit of a misnomer for H2. The point is that it serializes all transactions (ie does them one at a time). The crucial question regarding whether that is "ok" or not for your application is not "How many users?" or even "How many processes?", but "How long are my transactions going to take?" If all your transactions are sub-second that may be fine, if some take several hours to complete, that may not be fine as all other pending transactions will be waiting for them to finish. The decision about whether that is "fine" or not will depend on your own performance requirements - ie how long is an acceptable wait for my users hitting the database with transactions. --EDIT It seems that H2 doesn't really serialize transactions - just DML. In other words lots of short updates within a single long transaction will not block other updates. However unless you are using the experimental MVCC feature, table locking means this has a similar effect in practice. There is also an experimental "multi_threaded" feature but it cannot be used at the same time as MVCC 

You may be able to achieve better performance by searching first in rows with higher frequencies. This can be achieved by 'granulating' the frequencies and then stepping through them procedurally, for example as follows: --testbed and dummy data: 

On it's own I wouldn't let this be the reason for choosing a quirky and fragile design. If you go the route there will be no way to take advantage of the database features for ensuring referential integrity, for example. A traditional normalised way of achieving the same thing would have benefits beyond RI: 

One way is to nest analytics by using to set a flag on each change of state and then sum them up in a second step (you don't need to specify a windowing clause because "If you omit the windowing_clause entirely, then the default is RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW.") 

Note that foreign key constraints may cause this to fail. If you have circular foreign keys (ie from table A to table B and from table B to table A) then you may need to make deletes cascade or drop (or disable) the constraints before deleting. 

To my mind, the question is ambiguous: does "the minimum level; i.e. nearest the parent" apply globally (so all results have the same level) or to each sub-tree? If the minimum level is per sub-tree, you can take advantage of how hierarchical queries work and simply stop traversing descendants when your condition is matched: 

Based on this statement, I am assuming that an Employee and a Project are permanently tied to a Division. In which case you should consider using a composite key for the PK: 

If the grading rules are set in stone, you may wish to use virtual columns instead of updating them at all: 

Conclusion My preference would be option 1 if the number of properties per is likely to be stable, and if you can't imagine ever adding more than a handful extra. It is also the only option that enforces the 'number of properties = 3' constraint — enforcing a similar constraint on option 4 would likely involve adding … columns to the xy table anyway*. If you really don't care much about that condition, and you also have reason to doubt that the number of properties condition is going to be stable then choose option 4. If you aren't sure which, choose option 1 — it is simpler and that is definitely better if you have the option, as others have said. If you are put off option 1 "…because it complicates the SQL to select all properties applied to a feature…" I suggest the creating a view to provide the same data as the extra table in option 4: option 1 tables: 

In other words "transactions that were in the past appear to be in the future" and "data loss" are entirely theoretical and will not be caused by transaction ID wraparound in practice. 

Simply speaking, there is no direct analogy for MySQL 'databases' or a 'cluster' on Oracle: the closest match is a 'schema' but that is still very different. This is apparently going to change in 12c with the introduction of pluggable databases: 

Oracle has Java server-side, but I'd caution that it is not a replacement for PL/SQL. There is no better language than PL/SQL for manipulating data stored in the database - Java may be appropriate for computationally intensive business logic. For postgres, 

At a guess I'd say Marian is right and this is caused by a unique index and constraint having the same name, eg: 

(SQL Fiddle) I'm curious whether there is another way of bumping the without doing an update, analogous to the unix command? 

Error code 42601 is "syntax_error", that's the first clue. You are inserting into 6 columns: , but your SELECT clause has only 5 columns: . I suggest you try your SQL in or some other tool that give more helpful error messages: 

You need to make a judgement call about whether this will remain true. Some principles to bear in mind: 

In the event of a checksum failure on a replication master, will that corrupt data replicate to slaves or will replication halt. Does it depend on the setting of ? This README has some useful related information but doesn't directly answer the question. 

The range of a is -9223372036854775808 to +9223372036854775807, which is -2^63 to 2^63-1 — or 2^64 distinct integers. The range of your identifiers is 2^63 distinct integers so they'll fit nicely in a as long as you don't mind an offset: