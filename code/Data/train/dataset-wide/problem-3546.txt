I prefer dual master with in an active passive setup using LinuxHA (heartbeat). Passive can be used for a different database than the active, so as that the box is not idle. Infrastructure depends on your resources and budget. You can replicate readonly slaves off of either or both of the servers. But to be quite frank, it sounds like you need to do more research before someone can provide any suitable recommendations. I'd suggest reading "High Performance MySQL," which is one of the best MySQL books out there for advanced architecture. 

sets the default permissions for the user creating files. This is set in the shell environment. For bash system wide, that's . Or, you can set it for the user in or depending on how the shell is executed. 

What you are describing is the Sender Policy Framework (SPF). You can use this tool to walk you through configuring SPF records. You will want to identify all servers that are authorized to send mail on behalf of your domain. 

Sounds like it's your firewall rules, if you're bound to the routable interface as you say. Provide the output of iptables -L -n. 

I'd say the solution proposed is over engineered. There's a variety of technologies that would be suitable for your situation. I use LVS for load balancing and SQUID for caching, particularly in regards to Jboss. For static content, it's generally better to serve from Apache. You can still use heartbeat or pacemaker for redundancy with these technologies. The main reason I use SQUID is for rewrites but a lot of the content I deal with is dynamic. Caching is a bonus. Most of my Java applications have virtually no static content, so I often skip the mod_jk part. Point being, your requirements can drastically simplify even my proposed solution. One possible example: NAT to SQUID (ha cluster -> SQUID transparently proxies to LVS VIP -> LVS VIP to Apache cluster -> mod_jk to Jboss 

mod_env is used to set variables that can be passed to CGI and SSI pages. What you want to do does not necessarily have any native support that I am aware of. One solution would be to use the third party module called mod_macro, which would allow a more dynamically generated httpd.conf but is not exactly what you are seeking. If you have mod_perl installed, you can embed perl code within the httpd.conf using and , which would allow a more dynamic httpd.conf. mod_perl executes it on startup. If you are not already running mod_perl, the overhead introduced would not be worthwhile. A third option would be to dynamically generate your configuration using a set of scripts, which would be outside of Apache all together. Perl would be a very useful language for this particular application. 

You're going to have a difficult time finding an unbiased opinion regarding Linux distributions, as this is a contentious topic. If you are looking for statistics on the most used Linux distribution as a server, I am not aware of any. You will have a difficult time locating recent, relevant, and unbiased analysis of distribution usage. It would be difficult to find a definitive source for this data. Nevertheless, Distrowatch has a Web site that counts distributions based on page views. I would say this better examples the distributions more commonly used for workstations as opposed to servers. Also, This site has user entered data that provides statistics on distribution usage. 

However, being as that the files status contains a bunch of question marks, this leads me to believe you have a corrupt filesystem. I recommend a , which may correct the data. 

The filesystems at boot in most distributions use . It mounts all filesystems in fstab with specified, which is part of the specification. For ext2/ext3, can modify and display the settings that cause the filesystem to be fscked. For example, allows you to specify how many mounts until the filesystem is fscked. Ultimately, your assumption regarding the boot process is not accurate. is the solution to see if it's mounted. With ext3, I do not believe the "mount status" is stored with the partition. If you describe why you're incertain be it shared storage or NFS, we might be able to provide a recommendation applicable to your specific situation. 

You're specifying the name of the server, literally. Pick the one that you want your server to be, it's up to you. It will be the default name for any services you run on the server unless you specify otherwise. For example, this is often going to be the default host to remotely connect to the server to such as with SSH. Nevertheless, many other protocols will be specified to utilize hostnames outside of your server's hostname. For example, rarely will Apache's primary role be to serve files on the server's default hostname. With Debian, is read by the init script and will reflect any changes upon reboot. To change dynamically without reboot, you can also run the command . 

Really, pushing off topic and subjective but I really like Logitech products. Check out the Logitech MX5500. It's bluetooth 2.0, which should provide an approximate 10 meter range without line of site. The mouse comes with a replaceable and rechargeable AA battery, which can be placed in a docking station provided with it. You could also purchase low discharge NiHM rechargeables, which would allow the keyboard similar functionality. As an added bonus, the product is aesthetically pleasing. Ultimately, most bluetooth wireless keyboard and mice should satisfy your requirements as long as they are not on the low end. RF tends to be less effective in longer range applications. Be wary about security too, as you could potentially risk restricted data input via the devices. Historically, the protocols used are notoriously insecure. 

No. PCI scope data is credit card numbers, which is typically referred to as the Primary Account Number. (PAN) The definition from the glossary is as follows: 

Using this method, the ssh session will wait for foreground processes to exit. A3) To execute a background process on the remote server run: 

is the configuration file for phpMyAdmin. You are likely using based authentication. You can change this to based authentication, which will allow you to authentication against the usernames and passwords stored in your MySQL database. Specify this under the setting. You also must specify , which is used by cookie based authentication to encrypt the password in the cookie. Put it near the top of the configuration before the servers start getting specified. 

When compiling from source, is the default prefix. Some people like to use the standard system-wide locations, such as , instead even though it could be argued that it breaks the hierarchy standards. MySQL's default directory to store all variable files such as configuration and databases is . The default system-wide configuration file is . Ultimately, short of searching the filesystem, would be to look at the process list () and see where MySQL is running from, as the parameters MySQL is started with would identify its location. 

Write a script. You might find Expect suitable, which has less of a learning curve for some people. I typically do something like this to redirect input in bash: 

If you are using residential routers such as Linksys, I've encountered issues due to TCP connection limits being low. If you cannot increase the connections with the default firmware, DD-WRT allows the connections to be modified. If your troubleshooting eliminates all equipment within your office, it may very well be your Internet connectivity. You could work with your ISP to troubleshoot that connection when the issue is occurring. 

Service level, cost, and technical application are key points. If you simply want Internet connectivity for internal office use, I would focus on cost. It might be most cost effective to get cable and DSL and implement redundancy using a router. You can quantify the cost to the business from loss of connectivity, which will allow you to better weigh the ISP's SLA versus cost. If having Internet connectivity is critical for all employees' workflow, you could consider the cost of payroll for an hour versus the SLA for the connection. If most of business is conducted independent of the Internet connectivity, a cheaper connection will have better return on investment. If you are going to be using the upstream but serving the Internet or uploading a great deal of files, there is better technical argument for the bandwidth of a professional connection. In my local market the cable providers are more than capable of providing Internet connectivity for an office. 

You can generate the CSR anywhere. The certificate generated will need to be in a format that the device using it can utilize. Typically, that will be PEM. 

Do you really need a Web interface? Externally: a public Web server serving a directory accessible by a CIFS share, which users can e-Mail out. BasicAuth configuration for password protection, if necessary. Client side tools for encryption. Tons of options here in general. Internally: wiki and general file sharing solutions such as NFS and SAMBA. Again, tons of options. Have you looked through Freshmeat and Sourceforge? 

You could bind Apache to the tunnel interface. Having any part of the server on the Internet substantially increases the risk and you would not be able to consider the tunnel fully secured. 

I believe is necessary for both of these commands. I suspect the only way to be absolutely certain is to test this performance. Arthur Ogawa's comment reinforces my suspicion. 

The is adding it to the chain named . Unless that chain exists, I would not use it. You can see your existing rules with: 

When code is deployed to production, it should be deployed to all the servers at once. If this action is properly controlled, it should be mirrored as part of your controls and a technology solution will be unnecessary. Not all administrative solutions are based in technology. OpenEFS is a tool that was designed to enable change control as well as deployments, which you might find helpful. I implemented a lot of what they do on my own but for someone who has no foundation, it would be a good start. For static servers that are not in scope for change control, I have found rsync to be an appropriate solution in the past. Typically for servers that fall under this category, scaling is unlikely to be an issue but if it is that's where NFS or AFS might come into play. 

Consider specifying in the cnf instead. Typically, flags are specified in the init script, which would be located in . 

To stop the output from being e-Mailed for a specific cron entry, redirect the output for those cron to /dev/null. Suffix your cron entry with this to redirect both STDOUT and STDERR: 

If isn't located in a directory listed in your variable, will be unable to locate it. The is affected by the following circumstances: 

It's rather normal to change permissions after a deployment. I write scripts for code deployment. What's your deployment method now? 

One of the primary advantages I have found with heartbeat has been the ability to customize it to have multiple monitoring points. As per the default recommended configuration, it has multiple monitoring points between the serial uplink and the network monitoring. For example, a heartbeat resource script could be created to monitor a daemon and in case of the daemon failing, initiate a failover. CARP is based on HSRP, which as you identified monitors the interface. This certainly has a place and I like the technology but depending upon the server role you might find heartbeat to be advantageous. I suppose it could be argued that even those protocols that do not support this could have a script written to imitate some of the behavior, which is essentially what I described with heartbeat. While I have never used keepalived, it seems to be similar to ldirectord in that it monitors LVS hosts and removes them from the VIP in case of failure. I would not consider this to be in the exact same category as heartbeat or CARP. 

If temporary tables are over utilized, my first thought is that the application using the database is in need of query optimization. 

I'd guess that it's not completing for whatever reason and eventually spawns multiple processes. What's the script in full? What's the log output? Provide complete and intimate details regarding why you believe what you do. If you're running a common cron daemon, I'd think it more likely that you're misinterpreting the situation. It sounds like you may be basing this theory off the access logs. If so, you could wrap wget in a script and enable additional system level logging, which could more intimately detail the behavior. But I'm guessing, as you don't provide full details. 

The displays the actual memory available on the column. Linux uses unused memory for caching disk I/O. 

In more detail, the performance you describe is typically considered the directory in FTP land. For any special handling of the uploads, that can be configured in the daemon. As far as having it world writable, that's not entirely necessary. If you want literally everyone (such as anonymous) to be able to write to it, it is. However, if it's a limited subset of known users, you can create a group, add all the users to the group, set permissions 775 or 770 with the SGID bit set.