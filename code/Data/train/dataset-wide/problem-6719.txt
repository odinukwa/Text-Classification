Is there relationship (and translation) between Link Grammars and Combinatory Categorial Grammars? Link Grammars have very unusual parse structure - links between words, but CCGs have parse trees. Some connections are mentioned in $URL$ In this informal discussion $URL$ is mentioned that Link Grammars can be mapped to CCGs? Is it really possible. It would be nice to have such mapping. There has already been done great work with Link Grammars and translation to CCGs would allow to attach automated inference systems for the natural language understanding and automatic deduction on natural language texts. 

Natural view: this sentence describes the plot in relation to which the special action has happened that is described by the full sentence , so - the noun has relation to the full sentence and acts as some kind of conjunction that relates noun with the sentence. I understand that this it somehow weird interpretation but it conforms to the axiom, that is taught in the secondary school, that subordinate clauses explains (ore replaces) some words in the main clause. My understanding is that such axiom is complete failure - see the next point. Grammatically correct view: one should perceive as anaphoric pronoun and as the adverb (and there is pied-piping) and the full subordinate clause then can be rewritten as . So - actually - subordinate clause never explains (never can be attributable) to some word in the main clause. In reality the subordinate clause is just another full sentence which can refer to some word in the main clause only by (possibly implied) anaphoric pronouns. And anaphora is the only mechanism through which the subordinate clause describes some word in the main clause. So - this should be the ultimate axiom of the compound sentences - compound sentences always can be separated as standalone clauses that are joined together by logical connectives (conjunction, disjounction, implication - and only these) and there are no other kind of logical connectives that can serve for modelling subordination: i.e. subordinate clause is just free standing clause that adds or negates some more information (as a separate sentence/proposition (in logical terms)) about some word in the main clause (via anaphora). 

and my question is - what type of subordinated clause is introduced by the phrase/conjunction ? And is this conjunction somehow different from other subordinating conjunctions? I would like to read more about this specific phrase/conjunction, but I can not found any book or article about it, of course, there are a lot of articles and books about traditional subordinating conjunctions. I am interested both in semantics (better yet - categorial grammar, type logical, abstract categorial grammar) and syntax of this phrase. 

Clearly - pharases "in relation to which" (subordinating conjunction) function as one word. How such process is named in linguistics. It would also be interesting to know how such formation is handled in Grammatical Framework and Abstract Categorial Grammars generally? Is it allowed to handle such phrases as one lexical unit? 

I am trying to apply combinatory categorial grammars and type logical grammars (Montague semantics etc.) to the compound sentences and the subordinate clauses. Are there efforts to develop those grammars for such analysis? E.g. there is syntactic category s (sentence), are there syntactic categories for subordinate clauses? Does the type logical semantics of subordinate clause is just elaboration (additional constraint) on some word in the main sentence to which the subordinate clause refers? Are there examples in the literature of such analysis? Google gives no good references, good reference can be the answer, I can read it myself further. E.g. how to analyse sentence: 

Are there efforts to formalize and formally represent (e.g. as semantic network, as some kind of logic) of semantic of pragmatic knowledge. It is known, that every speaker/listener has two types of knowledge - linguistic/semantic knowledge (and pragmatic knowledge - sense about context) and everyday knowledge (commonsense knowledge). There are many efforts in computer science to formalize and represent commonsense knowledge. But what about efforts for formalize and represent linguistic/semantic/pragmatic knowledge? If there are such efforts then combination of linguistic and commonsense knowledge processing can lead to really, really strong natural language processing and generation system. Some examples of semantic knowledge (I know, that better list can be generated but I am only starting to learn semantics): 

So - see point 2 for how I understand subordination. Is my understanding of subordination correct (as stated in point 2)? And what about point 1 analysis? Maybe that is quite sound analysis? Maybe there are sentences that should be analysed in that way and maybe there are sentences in whom the full subordinate clause explains some word in the main clause (without anaphora)? Actually I am figthing this sentence for half a year (see my other questions - "in relation to which" - what type of subordinated clause and is this conjunction somehow distinct? What is the term for the formation of word groups with single meaning/function (e.g. "in relation to which") in lingustics Formal semantics of subordinate clauses (compound sentences) - in categorial and type logical grammars?) and just wanted to know - is this sentence really so weird and hard? 

I am reading the article $URL$ "The division of labor in explanations of verb phrase ellipsis". Technically it considers the sentences with ellipsis and tries theoretically and empirically to deduce which constructions of ellipsis are accaptable and which are not. This article mentions various theoretical notions which handle acceptability: 

Natural sciences observe facts and make models. Mathematics more or less arbitrarly generates axioms and investigates all the possible consquences of them. What is the source of semantic knowledge (e.g. one that is described in Saeed Semantics $URL$ How researcher generates the semantic knowledge from the text? Do the researcher is required to use only his/her inner sense of language to classify the possibilities of language use, roles of the words and so on? The question is somewhat obscure but its aim is to automate the semantics as a science. It is quite is to imagine experimental machine that makse observations and automatically generates and adjust models. It is quite easy to imagine system that generates and proofs/discards mathematical lemmas. In fact - such systems already exist for decades. But what about semantic science - how we can automate it, how can we automate the discovery of semantic knowledge? There is great necessity for such automation to boost symbolic processing of natural language and symbolic computational lingustics? I am aware of type logical grammars, categorial grammars, research in line with "Linguistics and Philosophy" $URL$ but work as expemlified by Saeed Semantis if far, far more richer than these efforts/ One can imagine the use of methods of classification. E.g. researcher extract all the possible uses of nouns from the texts and then researched can make automated clustering of the type of uses of those nouns and in such manner she can discover specified clusters of uses and name thoses clasters as thematic roles of the nouns? 

Are there connections/translation or common tools usage/adaptation between combinatory (concrete) categorial grammars (incl. Lambek calculus) and abstract categorial grammars? Can tools for one of them be used as tools for the other? Is the analysis for the same sentence available in both of these grammars for comparison? The answer to this question can lead to understanding how (and if) Grammatical Framwork and CCG parsers be used together or in place of one another, see question $URL$ and answer $URL$ 

I am learning about combinatory categorial grammars and the formal semantics of natural language, course by Partee $URL$ (especially lecture $URL$ is relevant for this question) is great indtroduction into these themes. So - there are syntax rules and semantic rules from natural language (e.g. English) and general principles that allows us to make semantic rules from the syntactic rules. There is some literature about supervised or even unsupervised learning of combinatory categorial grammars and this literature specifically deals with the induction of syntactic rules. My question is - are there literature, efforts, developments about supervised or unsupervised induction of semantics/semantic rules for natural language? 

My question is - is it possible to represent adverbial clauses in such manner? AS far as I know, then $URL$ is the most up-to-date analysis of adverbs but it does not include analysis of adverbial clauses which seems to be quite intricate thing: it seems to me that entire clause modifies the verb. E.g. - the entire clause modifies the verb . And it is big difference from relative clause - relative clause can be separated as standalone clause which has anaphoric pronoun and this separate clause can be added to the knowledge base. But it is quite different - it seems to me - with adverbial clause which is not understandable if it is taken as separate sentence. I.e. - what to do with the word ? 

My question is - is there a general theory which tries to separate the acceptable sentences from the unacceptable sentences? Type logical/categorial grammars define that acceptable sentences are those sentences whose derivation trees lead to the sentence type (category S). One sentence can have multiple derivations and there is need for semantical decision which derivation is intended by the speaker and which derivation represents the semantics of the sentence. That is fine. As far as I understand, then mentioned criteria (structural constraints, coherence relations) are some kind of generalization of the acceptability criteria which in the case of type logical/categorial grammars is reduced to the existence of derivation (grammaticality). So - is there good theory of acceptability criteria beyond grammaticality (in the case of formal grammars of sentences)? I guess that this theory is so far expressive that is not encoded in the formal grammar yet but it can be encoded and during encoding it can offer great possibilities to develop Natural Language Processing and to expand the scope and applicability of type logical/categorial grammars. The essence and purpopse of my question is this: if we have acceptability criteria, then we can decide which grammar rules or lexicon entries of the type logical or categorial grammars are valid. And if we can decide the validity of grammar (formal grammar of natural language) certain rules and certain entries in the lexicon, then we can design automatic procedure which can induce or machine-learn the grammar rules or lexicon entries. At present the type logical and categorial grammars should be devised by hand, but if formal acceptability theory exists, the type logical and categorial grammars (and their lexicons) can be automatically deduced and automatically extende during the usual process of the development of natural language. This can be perceived as reference request question - I would be happy to learn about key-words, research trend, important articles and researches - if only I could know this kind of information I can dig further myself. There is no need to repeat published information - if such publications exist at all? 

Partee has nice summary about the formal semantics of relative clause $URL$ (subordinate adjectival clause). E.g. has semantics 

Is there similar formal semantics of noun clauses, e.g. formal semantics of the sentence ? Noun clauses are the most mysterious clauses (from 3 types of clauses - noun, relative and adverbial), because there is no book about them (there are lot of books about adverbial clauses, although - none of them is about formal semantics) and no chapter in Sementics books (e.g. $URL$ has chapter about adverbial clauses but no chapter about noun clauses, although Chapter 68 contain some examples that seems to be noun clauses). Maybe noun clauses is more widely known under different name that is used in linguistics? E.g. can be represented by , but how can we represent ? ? Is it possible the replace object/Noun in function with the full sentence? What kind of logical expression we get in this case? 

Google is no help for me, because it return everything that is connected with the Web Semantics and description logics but it is completely unrelated to the semantics of natural language. Computer science has managed to formalized 20 types of emotions and put human personality in 5 dimensional models, there are efforts of consciousness modeling, so - why not to formalize linguistic knowledge and use it for automation of natural language? I am aware about formal semantics and categorial grammars but they are about technical parsing of the sentences and the knowledge represented by formal semantics of NL/combinatory categorial grammars (CCG) do not come near the completeness of the knowledge that is described in $URL$ and $URL$ So - my question is - are there efforts to formalize knowledge included in usual textbooks of NL semantics and pragmatics? I am interested in logical methods of semantics and pragmatics, I have no belief in statistical methods.