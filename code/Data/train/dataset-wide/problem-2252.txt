The database does not care about the IP addresses. Grid Infrastructure does. The method to change the IP addresses can be found in detail in the below MOS notes: How to Modify Public Network Information including VIP in Oracle Clusterware (Doc ID 276434.1) How to Modify Private Network Information in Oracle Clusterware (Doc ID 283684.1) 

Restart SQL Developer. Since you have access to MOS, steps are shown here: How to Use and Trace Oracle Net with SQLDeveloper (Doc ID 1991711.1) Alternatively, you can use a different tracing method for SQLDeveloper: How to generate a SQLNET trace with SQL Developer (Doc ID 1390063.1) 

You can not store Chinese characters in a Western European characterset (). Define the column as NVARCHAR2 to use and insert the string as below (notice the modifier): 

a) Use an OS that is certified for installing and running Oracle Database (Ubuntu is not amongst them). Native or virtual machine, your choice. or b) here is an unofficial guide for installing Oracle 11.2 on Ubuntu Linux $URL$ I have never tried myself, but several people succeeded installing Oracle 11.2 on Ubuntu with the help of this. 

You will get the results in CSV format the first time already. You can even spool the output just as in SQL*Plus. So you could just run the below block of code as a script (select lines and F5) and get a CSV directly in one pass: 

The message indicates that no archivelog exists with this sequence in the current incarnation. Failover starts a new incarnation, and log sequences start from 1 again. Let's say your database is at sequence 10 now, and the log sequence was 159 before the failover. V$ARCHIVED_LOG at this point still contains entries about previous incarnation, so the above query will return 159 because it doesn't consider the start of a new incarnation (resetlogs). For these kind of queries always check resetlogs_change# or resetlogs_time, e.g: 

A materialized view requires extra storage because it is a table. You want to make your materialized view to be refreshed on commit (for an immediate check) and to be fast refreshable, as a complete refresh may take too long to wait for. General Restrictions on Fast Refresh Assuming multiple items can be placed in a single box, the sum of item sizes needs to be accounted for (aggregation), so : Restrictions on Fast Refresh on Materialized Views with Aggregates A materialized view requires materialized view logs (which are also tables, more extra storage + administration) on the source tables for fast refreshes. Maintaining extra tables means generating more redo and undo as well. A materialized view provides more functionality than you need, which you will never use. What a trigger would perform on your DML operations (aggregate sizes of inserted/updated/deleted items), a materialized view refresh would perform as well. When you remove an item, the materialized view needs to be refreshed, but you do not even need to execute the trigger on deletions. When you update an item, the materialized view needs to be refreshed, but you can skip the aggregation in the trigger if . Or even better, you can cache the size of boxes in memory (result cache) and access them in the trigger without performing any actual SQL aggregation. Yes, some of the above seem insignificant, and they usually are, the difference may be unnoticeable. But with such a use case, a trigger can have the same effect as the materialized view combined with a check constraint, but with lower storage/administration overhead and requirements, so I would choose the trigger. 

The size needed in a temporary tablespace depends on the data volume manipulated, but including potentially other actions also requiring temporary space. What you should be doing is this : monitor temporary tablespace usage, (temporarily) expand the temporary tablespace. Maybe, you need to recreate your temp tablespace. That is, if there is corruption of some kind. One way - but not the only one - is stopping the database, rename the file(s) of the temporary tablespace, tail the alert.log, and startup the database. Obviously, don't do without backup, and don't test this on production. 

It COULD be simple, but then it's down to luck, more than to anything else. The way of working is very simple : get all processes listed of the main database, ignore the ones from the standby. Then, do the same with the standby database : collect all process info. What is important here, is that you capture this while all is normal. Then you compare both lists, in detail. Chances you find some process description active on one, but not on the other, is pretty high. There you go, differing both databases on OS level. It must be said, although you technically may be able to differ (as explained above), it's never a good sign if questions like that are asked. Either no permissions are granted, or knowledge is limited. Both are not good. The good way of differing both databases, is querying some V$ view. What I'm describing above, is an alternative method, but you should not rely too much on it. Why not ? Because when alternative situations emerge, the procedure may not work anymore. I'm thinking of; databases being started up and such, but you described already that things may not work if different database modes are used. 

I would move the old records (closed incidents) to another table, not to another partition in the same table. Why ? Because if there is a LOGICAL reason, in my mind it makes more sense to put them as much seperate as possible, on a physical level. Consider this : you can store incidents, but your data schema can also store "open incidents" and "closed incidents", as a separate component. Partitions are nice, but behold the fact that your closed incidents counts, will continue to grow. So, you can't say that when you create partitions, all partitions are behaving similarly. That on itself, looks wrong to me. But technically, it's possible. 

That is not how you load a field. returns a locator, and data is not stored in the database, but in files outside of but accessible to the database. The above command would not even succeed if BLOBCOL is really of type, you would get the below error: 

It sounds to me that you want to restore a newer backup of the previous incarnation, but the start of the current incarnation is before the time this backup was taken. Check the output of . If the reset time of the current incarnation is before the time this backup was taken, you will need to , where X is the incarnation key. 

Yes, you need to bother about datatype, always know your datatypes and use the proper ones. The database can implicitly convert compatible datatypes, so it is not always mandatory to specify and convert to the correct datatype, but it is a really bad practice to rely on implicit conversion. The below is a typical scenario I saw countless times: 

Output is from a 11.2.0.4.6 Enterprise Edition database on Oracle Linux 7.1 x86-64 platform. Lets start with question 2 and an easy example. DISTINCT and GROUP BY are handled differently: the optimizer is able to completely eliminate a DISTINCT under certain circumstances, but it can not do the same with GROUP BY. Here is an example: 

This was set to prevent Complex View Merging Transformations like DISTINCT Placement, GROUP BY Placement, Subquery Unnesting appearing and making things complicated. 

This is a restriction of direct path load, not staging tables. Direct Path Loads, Integrity Constraints, and Triggers Triggers fire during conventional path loads. 

By the way, this is a really bad idea (autonomous transaction trigger). Check what happens if you do something like this: 

7 is the id of your file on your screenshot. When all the extents are gone (the above query does not return anything), drop the datafile: 

Something like this. The below lists everything: which student took which course taught by which professor.