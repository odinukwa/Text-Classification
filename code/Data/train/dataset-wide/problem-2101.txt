The GUI is nice for some simple quick tasks but not for partitioning a table with billions of rows. Use T-SQL so that you know exactly what is going on. Large tables are unforgiving. 

Community wiki answer: Check for triggers, and DDL triggers on the database. The error message gives the name of what to look for: procedure line 47. 

Community wiki answer: You need to switch to mode in Visual Studio via the menu SQL > Execution Settings. You can also have all SQL files open in by default if you dig into Options > SQL Server Tools > Query Execution. 

Answer originally added to the question by its author: As per Jason's answer, I issued the following update instead: 

Community wiki answer: For the first case, see the Q & A: How SQL server generates a Query plan with Auto Create Statistics set to OFF. You use a variable so it's considered unknown input and is estimated as 30% of table cardinality (30% of 1000 rows). In the second case it uses statistics histograms. You can learn about them in SQL Server: Part 2 : All About SQL Server Statistics :Histogram by Nelson John. You'll also get the 700 row estimate for your parameterized query if you force a new plan to be compiled based on the current value of . For example: 

Community wiki answer: Yes of course the other transaction will wait. (Nearly) Every DDL statement takes an exclusive lock on the object being modified (dropped, created). See Explicit Locking in the documentation. 

Community wiki answer: From CHECKDB From Every Angle: When did DBCC CHECKDB last run successfully? by Paul Randal: 

This question was also asked in a comment on Paul Randal's blog Inside the Storage Engine: Ghost cleanup in depth Paul replied: 

Community wiki answer: A given athlete can play in multiple sports, so you should probably separate that. Also I do not understand the ranking part attached to countries. This should be at best a view, not a table. As a general advice, do not store ages for individuals, but date of birth, because this does not change while the age changes each year. You should just store the result (position) number for each athlete and competition, not medals that will be derived from it. You also need to take into account ex-aequos. Also, even for a given competition, a given athlete may participate in multiple steps (semi-finals, final, etc.) so you may need to cater for that. 

The inner join to drops rows (in particular, , but also the "outside" boundary, so you end up with only returned in my example). Change this to a . This is SQL Server 2008 R2 (see question tags) so doesn't work. 

Most of the Stack Overflow solutions would work as they are, or with minor modifications for SQL Server. Note how the more complicated queries, with multiple joins or subqueries (that would need dynamically produced code for the arbitrary cases) are more efficient than the query. 

If you want to use to sync the table on the remote server, but you can't because it's on the remote server... 

Answer originally provided by the question author in a comment: So, somehow, I neglected to mention that I was pulling the insert statements from a file...and including the part where it turns the index off before the inserts, and on again after (the code looked like it was commented out - I just didn't understand the syntax). Apparently, turning the index off and on like that ALWAYS causes the table to reindex from scratch...removing the or switch on the fixed the issue, and my inserts take a couple seconds now. 

The transaction log is not human readable. Depending on the reason behind wanting "to do this for specific table" (not specified in the question), you could look into buying a third party tool, rather than attempting it on your own. 

Community wiki answer When I got back from lunch the backup completed successfully when was not specified. After seeing it fail all morning, and now having one successful, uncompressed backup, it seems as if the failure is happening when it gets to the log file. 

Community wiki answer: James answer is a pretty handy script ( looks like the key table I missed). Unfortunately it still seems to have 2 problems: 

Sargability wouldn't be a problem if the query is written to use , since SQL Server will include a helper covering seek predicate. See Understanding SARGability (to make your queries run faster) by Rob Farley (recorded at SQLBits VII in 2010) which demoed this. Note however that the conversion might still affect the accuracy of cardinality estimates. 

Community wiki answer: The estimates vs actuals are pretty far off on some of the tables, like SQL Server expected 1 row would come back, but over a quarter million came back. What you'd probably want to start with is taking apart that view, and only joining the specific tables you need - especially since you only want one row out of the hundreds of thousands of rows it's reading. Also, side note - running Dynamics AX on SQL Server Express Edition (with no parallelism, and 1GB RAM) is going to be bad if it's the main database (the store database shouldn't be a problem). 

Community wiki answer: You're basically right. It will also help if you define your separate tables with a certain amount of free space (see ) so that PostgreSQL can perform Heap-only tuples updates. See Increase the speed of UPDATE query using HOT UPDATE (Heap only tuple) by Anvesh Patel. 

Or you could try . One problem is that table variables don't have statistics and that causes the optimizer grief - using can frequently solve this. Another typical work-around is to materialize the table variable into a local temporary table (which does support statistics) and use that instead. An example with query plan (nested loop join changing to a hash join) can be found in Improving SQL Server performance when using table variables by Matteo Lorini. 

Community wiki answer: Best guess: The plan chosen to update stats is either parallel, or more parallel, on the 2014 box than on the 2008 R2 box. Parallel update stats for has been around since 2005, and for sampled statistics starting with 2016, see Query Optimizer Additions in SQL Server 2016 by Gjorgji Gjeorgjievski on the SQL Server Database Engine Blog. If you have Enterprise Edition, you could use Resource Governor to limit the CPU being used by your maintenance job. Consider also voting for the Connect suggestion Add parameter to Update Stats by Javier Villegas. Related Q & A: Parallel Statistics Update 

Community wiki answer: You have a high number of databases in availability groups, that's going to be where your threads are going. There is a lot involved in the compression, encryption, and transport cost. Try turning off compression, it will reduce your thread usage by a about a third (depending on the replica count). The question is tagged SQL Server 2014, which will by default use compression. SQL Server 2016, by default, will not use compression for sync. You may need to increase the worker threads on the instance, or better: balance out the most active ones and inactive ones on multiple servers. See the related Q & A AlwaysON Availability group query very slow. You may also find it is an application that is unable to close requests properly. This can result in lot of sleeping sessions lying around (which consume workers). The number of threads actually used depends on how active the databases are. You could have 1,000 databases and, if most are idle 95% of the time, you won't have any issues. It seems that your databases have become active more often and have eaten more of your threads. That's the long and short of it. 

You mentioned you added to the PK, but it was not partitioned. You also need to specify the partition scheme when you recreate the PK index so that it is aligned. Also, note there are restrictions on tables with foreign key relationships. See Transferring Data Efficiently by Using Partition Switching. 

Community wiki answer: Postgres will perform a step called view-resolution before executing the queries that contain a view name instead of a table name. This step will copy the select statement definition of the view into your executed select statement. If you have an index on your ID column in the source table, this index will be used. Otherwise if ID is a stand-in for multiple columns from multiple source tables, you can create a materialized view with an appropriate index on it. If ID is a single column in the source table it is unlikely that a materialized view will yield a speed-up of your query, because the first and second queries are structurally different. In the first case Postgres makes 1000 (if your example actually had that many different ID values) single queries and concatenates all results in a single result cursor. In the second query Postgres is able to see the big picture and can better choose a strategy for selecting from an index (e.g. a range scan). 

Community wiki answer please edit to improve if you can From an indexing and execution standpoint, you're introducing the likelihood for execution plans that cannot use the seeking capability of indexes. That means poor performance on updates and selects. Seems like a lot of extra work for no payoff. 

Community Wiki answer containing answers originally added to the question or left in comments by the question author 

Research Tally Table and Numbers Table for better, more efficient, ways to do this. Here's a link about tally/numbers tables, and here's a link about creating a calendar table. 

You could use SSMS for this. Right-click on the database -> tasks -> generate scripts. Generate Scripts can script out permissions, but the method is pretty hidden. When you get to the screen where you choose the location to script things out to, click the Advanced button. Scroll through there until you see the "Script Object-level Permissions" option, and set it to "True". The script will then include permissions for whatever you scripted out. 

I would only recommend #2 if you don't really need/want the users managing their own subscriptions. If the users are going to need the ability to generate their own subs then #1 is your only real option. 

Those pages in the documentation are an overview. The left-side navigation links to further information, and there are links in the page as well to more details. 

Community wiki answer: Start by fixing the Key Lookups. They're mostly for predicate evaluation, and the estimated number of executions is bananas. Unless old data is being archived, you will always run into trouble with this query, since you are selecting everything stored before today. No matter how much you tune, one day it will be too much to handle. A quick hint: notice the "select distinct", quite an important part of the view (120 lines). A "select distinct" shows you're selecting too much an have to throw the doubles away. Run that part without the distinct, find out where the doubles come from and find a way to avoid them. 

Community wiki answer: Use backup and restore instead. You can write dynamic SQL to do what you said, or just use Copy-DbaDatabase in the PowerShell dbatools to fully automate the task. 

This a typical use case for a queue. The problem is that you're mixing state and events. It will never work. Separate state (your table) from the event ('needs processing'), store the events into a proper queue table, and dequeue by deleting. 'Consuming' is a dequeue operation. Adding a new record (with nulls) is an enqueue. Dequeue destructively from events, then visit state as many times as you fancy. See Using tables as Queues by Remus Rusanu.