I had created table with engine BLACKHOLE basically the BLACKHOLE storage engine acts as a “black hole” that accepts data but throws it away and does not store it. Retrievals always return an empty result. I heard that we can retrieve the data by creating a new table same as the old table with storage engine as innodb or myisam. but i had tried that also but unable to get the result. Can any one pl help me on this issue to fix it. 

In create table you have set teams as primary key, and also you are aware that primary key does not allow duplicate values. 

I am getting the below error in mysql while trying to ping. i am using mysql version 5.5.37-0ubuntu0.12.04.1. when ever i use mysqladmin i am getting the below error. Please suggest how to clear this issue 

It will ask for password don't enter anything first time because it will use blank, n just press enter you are done. N later you can set password too...:) 

Step 2 will be the .dtsx package. Change the step type to be SQL Server Integration Services Package, and at the bottom, specify the path for the package, which we saved at 

but I think I'm barking up entirely the wrong tree. Also, the performance is horrendous (not a deal breaker, since it will be automated, but still something to be mindful of). Question: How would you break this up by a foreign key? Some sort of pivot? The number of diagnoses is quite variable... Ideally, it would be something I can easily smush into a larger query, but there's obviously ways to work around that if needed. Here's my current solution, but I'm pretty sure it's not a good one: 

Here's how you can join a table to itself randomly and update a column. This method will also maintain the distribution of your data: 

I've got a query that checks for new patients in the ER, and if they have been there before in the last 72 hours, it notifies the killbo... uhhh... care managers. I could set this up as a trigger, but I don't think this is a good idea. What I want to do instead is set it up as a job that runs every 5 minutes, with this in the where clause: 

Your BHCR is 98.92% its perfectly Good and Great. If the value of the Buffer Cache Hit Ratio (BCHR) counter is "high", then SQL Server is efficiently caching the data pages in memory, reads from disk are relatively low, and so there is no memory bottleneck. Conversely, if the BCHR value is "low", then this is a sure sign sign that SQL Server is under memory pressure and doing lots of physical disk reads to retrieve the required data. Prevailing wisdom suggests that "low" is less than 95% for OLTP systems, or less than 90% for OLAP or data warehouse systems. You can also test the true nature of the BCHR 

Check the insert query for teams field you have mentioned values as 1 and if you insert the same value for second record it will behave as below. 

If the tables were dropped, ApexSQL Recover can recover them even from databases in the simple recovery model. ApexSQL Recover can recover both table structure and table records On the other hand, ApexSQL Log cannot recover records lost when a table was dropped, it can only recover the table structure In case the records that were lost using DELETE (not DROP TABLE), both ApexSQL Log and ApexSQL Recover can help The advantages of ApexSQL tools over recovery to a point in time is that ApexSQL will recover just the tables you specify (creates CREATE TABLE and INSERT INTO scripts) , while a point-in-time recovery will roll back all the transactions that happened in the meantime A DROP TABLE statement marks the MDF file pages used by the dropped table for deallocation. These pages are actually still in the MDF file until overwritten by new operations. To prevent new operations overwrite the data necessary for successful recovery, we recommend creating a copy of the original MDF and LDF files immediately 

How often do I need to run it? It needs to run at the same interval that I keep my backups, right? If I keep the backups for 1 day, I need to run it every day? Can I restore the databases to my test environment and checkDB there? If I've got lots of sql servers, but they all use the same SAN, am I going to want to stagger my CheckDB's? 

3. Set up a .dtsx package: You can do it directly in SSIS, but I find it easier to use the SQL Server Import and Export Wizard by right click on the database, hitting Tasks and Export Data. Set the Data source to by SQL Server Native Client, set up your server and database, then hit Next. Set the Destination to be Microsoft Excel, and the excel file path to your shared folder (not your blank template): 

You're nearly there. the trick is writing your group by. I don't want to bitch about your question being too long to read, but I definitely didn't read the whole thing. one other suggestion, if you're only putting dates into a dateTime column, consider altering it to a date column. 

Error Cause: The control file change sequence number in the log file is greater than the number in the control file. This implies that the wrong control file is being used. Note that repeatedly causing this error can make it stop happening without correcting the real problem. Every attempt to open the database will advance the control file change sequence number until it is great enough. 

Hi I need to set auto increment for a table as S1. I tried by altering the table and also by creating by new table I am aware how to set auto_increment but when i try to set auto increment as S1 i am unable to do. since i need the primary key values should be S1,S2,S3 as following records. can any one please suggest the data type for that, 

If you anticipate that your database file(s) will exceed more than 1TB in size you should configure the server to put the data files across multiple VHDs. How can spread my databases across multiple VHDs? Create one or more dynamic volumes consisting of multiple VHDs at the operating system level and put your database files on these volumes. Split your tables and indexes into separate database filegroups and then place those filegroups in different files (MDF + NDF) which are spread across separately attached VHDs. Note that this will not offer any benefit for transaction logs because SQL Server does not stripe across transaction log files but uses them sequentially. If you are migrated to Azure and want to compare the performance of your databases before and after the migration then take a SQL Server baseline(enter link description here) 

I have a form in a C# application that I can't change with a data table bound to a query. The user enters a mark for a student's task or 'A' for absent. However, the person who designed this had the underlying table as all varchars, when really it would be nice to have decimals for marks and char for 'A'. To this end, I thought I would make an instead of update trigger that moves 'A' values to an absences table and puts marks in the existing marks table. I can then change the data type in my marks table to decimal. This is my trigger code: 

The trigger works for decimal values, however I get "Error converting data type varchar to numeric" when I attempt to update the table with 'A'. The absences table has char as the type and the value should just be inserted into that, with nothing in the actual marks table, thus not violating the decimal type constraint. Why is this happening? In addition, is there a better way to handle this scenario? 

One thing to experiment with is changing your clustered indexes - in a lot of ways they aren't really indexes at all, they are the order in which the data is stored. make copies of your current clustered indexes first, then try adding the columns in the joins & where clause to the clustered index. Clustered indexes are often the same as primary keys, but they don't have to be. your primary key and clustered index can be completely different. Another thing - the - do you need every column from that table? If you only take the columns you need, it might help. Then, there must be a way to stop a query if a website user bails... Other things you can do - archive some data. I'll bet some of your tables are pretty big. Do you need all the data there? With Enterprise edition, you can also partition tables. Check for locking - if you have two users putting this query to your DB at the same time, are they blocking each other? I'm going to catch some shit for suggesting this, but you could join with (NOLOCK) and see if that helps. Oh, and when did you last update your statistics? The query tuning videos on Brent's site are really good, if you need more detail. 

However, when someone enters, say, 23.5, SQL Server rejects the input. How can I fix this? I know it is because it is testing things as , but how do I get it to test the entry as a decimal? The column has to be NVARCHAR since, as you can see, I also need my users to be able to enter 'A'. 

I have two dimension tables in my data warehouse and I want to know whether I should converge them, considering that they have limited attributes in common (keeping in mind Kimball DW design principles). Table 1 is called and virtually all of its data comes from an ETL from another small departmental database. Table 2 is called and all of its data comes from an ETL from a large enterprise wide system (I work in a large university). There are a very large number of students in Table 2 that don't exist in Table 1. There are a small number of students in Table 1 that don't exist in Table 2. There is no current attribute cross-over between the tables, except for a common identifier for many students in Table 1 that also exists in Table 2. If I converge them as they stand presently, there would be a large number of or text values in the table. Most of my fact tables do not require access to the two different data sets. One of my fact tables does. Presently, I would have to have two different FK relationships for this fact table - this feels wrong. Equally, it feels wrong to have a dimension table where most of the attributes have or values. It would be much appreciated if someone could advise me based on their experience of DW projects whether converging fairly disparate dimensions (in terms of attributes), albeit conceptually similar ones (things about students), makes sense. Should conceptual purity be promoted in spite of a somewhat unintuitive result? 

Maintenance plans will do just fine most of the time. Ola Hallengren's scripts will do just fine most of the time. In very rare cases, you might have to grow your own. As Jyao said, it comes down to which you are most comfortable working with. If your co-worker is most comfortable with maintenance plans, why get your knickers in a twist? If he's been databasing for 20+ years, he's already written his own maintenance scripts. Right about the time you were learning to drive, there was probably some young punk in cargo shorts and fliplops that came along and was all like "hey, you old codger, maintenance plans are better - and pull your pants down, you look ridiculous!". Then there was probably a 4 year battle where the uppity youngster slipped in a maintenance plan whenever he could. Now this other young punk with skinny jeans and a freakin bow tie is telling him to go back to scripts. It's enough to turn your hair gray. Three things to consider: Are these examples of Hallengrenite superiority actually applicable to your environment? Is it going to cause you an actual problem if he uses maintenance plans? If you convince him to use Hallengren's scripts and there's an issue, will he be able to resolve it himself or will he have to call you?