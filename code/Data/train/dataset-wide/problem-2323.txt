You already have them, you just need to do this for every log backup ever taken across all of the time the database has been in existence. Also, this doesn't take into account actually reading the log structure which isn't publically documented. This doesn't even include the newer log structures for things like Hekaton which have their own structure inside of the log structures or any of the trace flags you'd need to set certain behaviors. All-in-all you're better off doing something else with your time. If this is needed as part of database recovery then I'd employ some professionals. If this is purely for academic then I would suggest creating a very small database, putting it in the full recovery model, taking a full backup, and not to take a log backup until you're done looking at that part of the log. 

Initially I believed there to be an issue with the actual in memory table and indexes not being optimal. While there are some issues with the memory optimized hash index definition I believe the real issue to be with the queries used. 

There is one proper way to pre-stage the listener and one way to allow the cluster to create the listener itself. Please note that YOUR account is not what is used to authorize to AD to create the listener when creating it through FCM/Powershell or SQL Server, the CNO is used as security context. The official pre-stage way 

There is no such thing as a "trusted user or login". Local windows accounts are only known to the local server, thus you could not use webserver\localuser unless the database server was also on the webserver. Since it is not, this is not possible. Since the local user is not in active directory, this will also not work. Thus, no, you cannot use that method. Your best possible scenario for this is to use encryption of the connection string at rest and potentially even SSL connections (though that might be overboard and doesn't make the password kept in the registry and more or less secure, it just encrypts the traffic on the line). 

A log backups allows VLFs of the log to me marked for reuse if nothing is actively depending upon them. Nothing is physically cleared, just marked to allowed to be reused. 

Yes! It's the same error as if you were disconnected for any other reason. Depending on the driver used, the error checking may be different. For example if you're using .Net and the ADO.Net sqldata client you can use the StateChanged event to know when you connect or become disconnected. If your delegate is fired, check the connection. If you are disconnected, attempt to connect again, etc. 

It does not do this by default, you have to CHOOSE to enable the share. This is done via SQL Server Configuration Manager. If you deselect the the share will be removed. 

There are a number of different factors involved but it is entirely possible to have a replica that is synchronous be faster or slower than an asynchronous one. To list a few: 

So you know the backup cert/DMK files are now useless. I would IMMEDIATELY take new backups and save the passwords. This way you can restore older backup files should anything happen. I would also backup the SMK as that's the only means you have to decrypting the DMK/Cert at this moment. 

Thank you for the code - it helps! The encrypted connection information shouldn't have any bearing on this, though. 

Mirroring. If you can setup mirroring you can setup clustering and use AGs - I'm not sure why the big push to not have to use Windows Clustering. 

Bjorn, There is nothing out of the box that will force the developers to use non-deprecated features (as they are deprecated, not removed). The most tailored solution that could be created is with DDL trigger(s), but note that they can be tricky... especially if not extremely familiar with them. I would suggest PBM, but it has limitations that won't necessarily capture everything you're looking for. As already discussed, you can use extended events (in the previous link I had) to check for the use of the deprecated items. Here is an example that will stop the use of fn_get_sql() and give a helpful message while stopping the use. Note that multiple checks can be made in the same trigger, this is just a trivial example. 

The Service Master Key (SMK) protects the database master key (DMK). When we say "protects" we mean encrypts. This could be said as, "The SMK encrypts the DMK." The DMK in the master database "protects" (Encrypts) the Server Certificate. The Server Certificate "protects" (Encrypts) the Database Encryption Key (DEK). 

Strict RPO/RTO Does your SAN support write ordering across multiple LUNs? OS/SQL Patching issues Controller level IO corruption Downtime for Maintenance 

Let me preface this answer with the statement that there could be entire books written on this subject and this is without a doubt some of the most interesting and complicated parts of SQL Server. This answer will not attempt to go into great lengths of detail, but a simplified overview of certain items. 

It works at the Availability Group level, that's up to you on how and what you put into your AGs. You might have a single app in an AG or you might have multiple. Personally, I would only put the databases that need to be together in the same AG and not co-mingle any other application databases. 

What should I monitor to trend my workload appropriately? Whatever the limits of the hosted environment are, you'll definitely want to be gathering those metrics and statistics. There may be additional items that are Application defined which should also be included. For example, if your application is a website that takes orders from users and the fulfills them there may some metrics you could add at the application layer: 

Depending on the settings of the listener in the windows cluster, your clients may need to flush their dns cache (to pickup the new ip if they are legacy clients). Once the first DC comes back online we'll want to make sure to reconfigure and re-evaluate the situation. Resources that will help you: 

The share allows for clients (local and remote) to have a singular shared location to use the streaming windows api for access to filestream data. This works in conjunction with the SQL Server Instance level settings for filestream access of , any other access setting should not work with the streaming API. 

SMSS isn't saying it can't use the database, it's asking for the password to open the master key. What's the fix? You have a few options, but here are the best two: 

The error message is pretty self explanatory. Mirroring doesn't support memory optimized objects (In Memory OLTP you may see it called [IMOLTP] or Hekaton). Mirroring as noted in the comments is deprecated - it's only AGs going forward. Since 2016 supports a multitude of new options in terms of clustering and availability groups, you should be able to do the exact same thing with AGs as you can with mirroring - albeit a few more steps in setup and configuration. If you want to use mirroring, then I would suggest using a blank, new, database and configuring it there. Also, don't rely on the GUI. 

Could you let me know what those limitations are? AFAIK there aren't any... is the defacto way to deal with your issue. Period. Hence why I would be interested to know what the limitations of the parameter are. 

Each SAN implements this differently. For example, some can define exact RAID groups, type, etc. Some you just put disks in and they create their own internal sets and configuration - you just tell it how big of a LUN you want and where to zone it. Some SANs will automatically tier your data and you may be on many different sets of drives. Others will yet again operate completely in cache and only flush to disk like a checkpoint operation would in SQL Server. This also doesn't take into account anything in the fabric that may be acting as yet another intermediate tier such as SVCs. Really, the question you should be asking is, "Is one LUN allowing me the throughput I require in order to properly run this instance of SQL Server and all of the associated databases." If there is only one LUN, most drivers for the HBAs won't multipath. This may or may not matter depending on your fabric - but most of the time it helps. Additionally we don't know the queue depth of the HBAs (though not all implementations have such a configurable item) and don't know the typical profile of IO for that server. Just some extra items to think about :)