The halting problem is algorithmically unsolvable: Suppose there is an algorithm that solves it. Construct a Turing machine that behaves differently from every Turing machine. Then it behaves differently from itself, contradiction. Gödel's incompleteness theorem: Suppose a consistent and complete formal system is strong enough to express elementary mathematics. Then it can express the statement that a given Turing machine halts on a given input. By enumerating all proofs, we can solve the halting problem, contradiction. The synopsis for the incompleteness theorem is is a little bit of cheating. The technically hard part of Gödel's proof is to show that a particular version of Peano arithmetic is strong enough to make the argument work (he had to do this since Turing machines weren't yet invented). But if he hadn't been able to pull this off, he would just have invented a slightly stronger system. The philosophically relevant part of the theorem is contained in this synopsis. 

The game of Bidding Chess (see Discrete Bidding Games by Mike Develin and Sam Payne) offers some rather tricky rational values. Let's put the two kings on their original squares e1 and e8, and a white knight on g1 (admittedly not a natural position for any mathematical reason). As is explained in the paper, bidding games are related to random turn games, where before each move, a coin toss decides who gets to move. The objective of the game is to capture the opponent's king, so there are no concepts of check, checkmate or stalemate. We can define two numbers $\alpha \leq \beta$ (representing the value of the position from White's perspective) such that $\alpha$ is White's maximum probability of winning, and $\beta$ is White's maximum probability of not losing, taken over all White's strategies (and minimizing over all Black's strategies). For Black, the maximum probability of winning is $1-\beta$ and the maximum probability of not losing is $1-\alpha$. It is not true in general that $\alpha=\beta$, but in this particular position, numerical evidence (!) suggests that $\alpha = \beta$, and that they are equal to $$ {\tiny 140297187507809718787571560535429924880742491099805023919271172220178831973930689386662219848285322420407060840990534312850343}\\ \big /\\ {\tiny 194465010677647568859234587998029627946213150285790861705846175978932032134223961344086399663594245257797038922571722208051200}$$ 

I am also interested in learning Chinese and Japanese enough to read mathematical articles. While also learning basic language notions with traditional courses, I am practicing by reading small and elementary mathematical wikipedia articles in english, chinese and japanese, that I sometimes translate back with google translate to match with the english version. I making slowly my own quadri-lingual dictionary (with english and french) with an electronic card system (Anki), so that key hanzi/kanji for mathematics allow me to progressively guess the subject of an article for instance. I devote some time to writing correctly by hand each new characters many times to reinforce memory by gesture and concentration. 

What can be said of the various adaptations or specializations of this constant to various class of functions, and extensions of these constants to several complex variables or other functional spaces ? 

There is a french talk by Alexander Zvonkin which can be a good introduction to this subject as well. If readers are interested I can translate parts of it in english. 

The Bloch constant B (based on a theorem introduced by André Bloch in 1925 on the maximum radius of a one-to-one disk in the image of a normalized analytic function of the unit disk, see for instance Remmert Funktionentheorie II or Steven Finch marvelous "Mathematical Constants") was conjectured by Ahlfors to be $$ \frac{1}{\sqrt{1+\sqrt{3}}}\frac{\Gamma(\frac{1}{3})\Gamma(\frac{11}{12})}{\Gamma(\frac{1}{4})}$$ (This value, if I remember well Ahlfors' article corresponds to a particular function that he constructed for this purpose). The Bloch Constant $B$ is currently known to be at least slightly greater than $\frac{\sqrt{3}}{4}$ (several articles improving upon each other by Mario Bonk, Chen and Gauthier, Xiong). Has there been some progress since 1998 on the lower bound ? Same question for the closely related (univalent) Landau constant (quite often called Bloch-Landau constant, sometimes seen as $B_\infty$) ? The conjectured upper bound is $$\frac{\Gamma(\frac{1}{3})\Gamma(\frac{5}{6})}{\Gamma(\frac{1}{6})}$$ 

Oops, too long for a comment: The expectation of the time until you reach a given point other than the origin is infinite already in one dimension. To see this, let $T$ be the expected time until you get from 0 to 1 (in the obvious 1-dimensional setting). The first step is either to the left or to the right, and if you go to $-1$, the expected remaining time is going to be $2T$ since you have to get back to the origin and then to 1. Consequently $T = 1 + 1/2\cdot (2T)$, from which we see that finite $T$ leads to contradiction. In higher dimensions it becomes even worse. Presumably you want to modify your question and ask about something like the distribution of the time (in dimensions 1 and 2 you actually get there!) or about the expected time to get from one point to another in a finite box. 

If I haven't missed something, this is the "ordinary" two-person zero-sum game, which is a linear programming problem, and solvable in polynomial time. Game theory is full of slight variations, and you might have read about one of those being NP-hard (for instance, there is a paper by Fortnow and Impagliazzo, $URL$ ) An excellent and concrete description of an algorithm for solving this sort of game is given in Section II 4 of Thomas S. Ferguson's electronic text on "Game Theory", $URL$ I don't know about the worst case complexity of that particular algorithm, but it probably works fine unless you cook up specifically hard games. And in the intermediate stages, the (sub-optimal) strategies of each player will give bounds on V(P) that successively improve. 

Motif in french has both the meaning of english "motive" and of "pattern". It is still actively used in decorative arts and art history "Cette tasse est ornée d'un très joli motif", "This cup is decorated with a very pretty pattern", and so on for tapestry, greek freeze, wallpaper, etc. And still used when describing a police case : "Il a un alibi et n'a aucun motif". So I believe that Grothendieck was well aware of this ambiguity. 

About learning about logarithmic integral and polylogarithm identities, there are large compendium of formulae on everything (Gradstein Ryzhik Jeffreys for instance), a few classic monographies such as Nielsen, Lewin, some new books (some of them with a physicist point of view since polylogarithm appear when dealing with Feynman path integrals among others) and also this online resource. You can print the pdf version of this list of dilogarithm identities for instance. 

Another source are books about the camera obscura and pinhole photography. More contemporary: the techniques used to enhance digital images and their perspective with mathematical models of camera lens deformation have given birth to relatively sophisticated applied mathematics. There are a few private companies such as DxO selling software to correct (among other things) perspective in image files by reversing the nonlinear effects of multiple lens systems found in camera. 

The lifetimes of the coins can be modeled by independent exponential variables of rate $1-p$ (that is, mean $1/(1-p)$). It might be easier to think of the coin as being repeatedly eaten at the events of a rate $1-p$ Poisson point process. Such a process can be generated from a rate 1 process by discarding events with probability $p$. Discarding will correspond to returning the coin. By memorylessness of the Poisson process, at any time, each uneaten coin is equally likely to have an event in its rate 1 process, but the larger coins will more likely survive by having the event discarded. Therefore the order in which the coins disappear will be distributed as in the original model. The probability that a nickel survives 100 dimes and 99 other nickels is equal to $$\int_0^\infty 0.95\cdot e^{-0.95t}\cdot (1-e^{-0.9t})^{100}\cdot(1-e^{-0.95t})^{99}\, dt.$$ Here the factor $0.95\cdot e^{-0.95t}$ is the density of the lifetime of the particular nickel, and the rest is the probability that all other coins have been eaten by time $t$. Multiply by 100 to get the probability that the last coin is a nickel. Remove the factor $(1-e^{-0.95t})^{99}$ in the integral to get the expected number of nickels surviving all dimes, etc. 

I think the main idea of Brendan McKay's answer to Terence Tao's question still works. It's a bit more complicated (and less non-enumerative), but still "robust" in the sense of Tao's question. First one should somehow "estimate away" those permutations that have an interval of length 3 or more. The proportion of such permutations is only $O(1/n)$. Then, since the average number of intervals of length 2 is 2, it suffices to show that (i) there can't be many more permutations with 1 interval than with none, and (ii) there can't be many more permutations with 2 intervals than with 1. There is a variety of ways to define a "switching operation" that provides (up to a $O(1/n)$ error) the necessary comparisons. For instance, consider the operation of taking a symbol that belongs to an interval of length 2 and switching it with the first symbol. The inverse operation is to look at the first symbol (say $i$), and switch it with a neighbor of $i-1$ or $i+1$. For a permutation with $k$ intervals (of length 2 and disjoint), there are (in general) $2k$ ways of performing the "interval destroying" operation, and, regardless of $k$, 4 ways of performing the "interval creating" inverse. Taking $k=1$ this shows that asymptotically there are only twice as many permutations with one interval as with none, and with $k=2$ it shows that there are roughly as many permutations with two intervals as with one. 

(too long or too complicated as a comment). There are at least two relevant properties in your problem. The part of the expression inside your initial sum that does not depend on $c$ is already equal to the result. \begin{align*}\sum _{d=0} ^{n} \frac{1}{d!(n-d)!} \frac{\Gamma (b+d) \Gamma (b+n-d)}{\Gamma (b) \Gamma (b)} = \frac{1}{n!} \frac{\Gamma(2b+n)}{\Gamma(2b)} = \frac{1}{n!}\prod_{i=0}^{n-1}(2b+i), \end{align*} a binomial sum (or a finite product if you like) you can manipulate in many ways. You succeeded in finding a one-arbitrary-complex-parameter weight expression that leaves that sum invariant. It is a very interesting property that can be exploited if one sees this sum as a discrete probability distribution. The sum without the $c$ fractions is probably a limit when $c$ approaches an integer. Another related point is that the original summand is invariant by $d\rightarrow n-d$ so you would always obtain a symmetric distribution. 

Just to stress a few points already addressed in comments and answers: Euler in his time discovered many important facts and solutions to classical questions, advanced rigor and gave examples of the power of the recently created methods (infinitesimal calculus), popularized the science of his day (notably books dedicated to a German Princess), wrote some of the first textbooks in analysis (still pleasant reading today), gave strength to the prussian and russian academy of science, courtized by two of the most powerful powers of the day (the King of Prussia and the Czar of Russia), filled international academic journals, some of them he edited himself, with quality articles (in fact up to several decades after his death because of the sheer size of his output), fostered international cooperation, wrote in the most important languages of his day (latin, french, german, I think he also learned russian), published in applied science, was part of state scientific advisory commission, etc. In fact Euler's work has been instrumental in progressively establishing the "rigor" some of us are so proud of. So a better equivalent of his investigation of what we call now Zeta(2 n) and the Gamma function would be the solution of outstanding problems by one of the most recognized mathematician of his day building on recent work by one of his even more famous and established mathematician, Bernoulli, who was his PhD advisor and whose several family members have established positions in the scientific community. I think he would have no difficulty publishing it. And his work would be quickly read and commented upon by many other mathematicians. Even if we imagine a Leonard Euler finding himself straight-jacketed by the mathematical discourse and style of the XXIst century, he would pair up with another good mathematician to write scholarly articles, as Ramanujan and Hardy used to do at the beginning of the XXth in a mutually benefical couple. 

You should submit only one version of the paper. Think of the referee as a typical reader, not a judge providing a certificate of correctness. If the referee needs additional information, then other readers will need it too. Regarding putting a proof in an appendix: There are exceptional cases when this is necessary, but most often it is not, and it can even be annoying to the reader (or seen as a warning sign of a bogus paper!) when important arguments are removed from their context. Reading a math paper will always be time consuming, but I don't think there is a general convention that proofs should be sketchy. As an inexperienced writer, it is sometimes hard to know when something should be left out, and when leaving it out will be regarded as a gap. Feedback from teachers and supervisors on this matter can be contradictory and confusing. If you feel you still haven't got the knack for it, bear in mind that it is much easier for a referee to ask you to remove details than to figure out when they are missing. I would not endorse any general advice to write sketchy proofs in order to keep the paper short. 

If I understand correctly, the question can be phrased as follows (let's take the prime numbers as suggested in the update): If we put the first $n$ prime numbers in a hat, and pull out three of them, $x_1$, $x_2$ and $x_3$, then is there a decent probability $\beta>0$ that $x_1-x_2+x_3$ is also prime? The answer clearly has to be no, and I guess a proof can be obtained by noting that for every prime $p$, $x_1-x_2+x_3$ will be divisible by $p$ with probability roughly $1/p$ when $n$ is large (because the primes are asymptotically uniformly distributed among the nonzero congruence classes mod $p$). Since $\sum 1/p$ diverges, the probability of $x_1-x_2+x_3$ being prime ought to tend to zero. Update: After thinking about it a little more, I realize this doesn't prove anything, so it should perhaps have been a comment rather than an answer. 

For permutation polynomials, you can also look into the relevant part of "Finite fields", by Rudolf Lidl and Harald Niederreiter, CUP. 

Its theme is the interaction of mathematical and artistic inquiries as characteristic of Western art in the Renaissance, with perspective and precise description of geometrical forms (such as polyhedra) as turning point, and embodied in several key artists such as Piero della Francesca, Leonardo da Vinci, Albrecht Dürer. The same author has written a book dedicated to Piero della Francesca: 

In Group Theory there is a notion stronger than isomorphism, i.e. similarity. For two permutation groups to be "similar" there must exist both an isomorphism between the group and a compatible (commuting in the categorical sense) isomorphism between the sets being permuted. The couple of these two isomorphisms is called a similarity. Two permutation groups can be isomorphic but not similar. Similarity is useful to compare group constructions such as the wreath product, group presentations and classifications of subgroups of the symmetric groups. In these situations, using only isomorphism would be a mistake and would lose information. 

I give as background the original article from Bloch, Ahlfors and Grunsky. (1) A. Bloch, Les théorèmes de M. Valiron sur les fonctions entières et la théorie de l'uniformisation, Ann. Fac. Sci. Univ. Toulouse, vol. 17, (1925), pp1-22. (2) L. V. Ahlfors and H. Grunsky, Über die Blochsche Konstante, Math. Zeitschrift 42 (1937), pp671–673. (3) L. V. Ahlfors, An extension of Schwarz's lemma, Trans. Amer. Math. Soc. 43 (1938), pp359–364. (these two are reprinted in Ahlfors Works vol 1) 

My guess is that such sets exist in all dimensions. Here's a partial answer that explains why. Let's consider tilings of a rectangular box of area $\zeta(\alpha)$ by axis-parallel rectangular tiles of areas $1/n^\alpha$ for some $\alpha>1$. We allow the tiles to be squeezed and stretched by axis-parallel linear transformations as long as the area is preserved. Suppose that we have carelessly placed the first $N$ tiles. Then the remaining space can be divided into $3N+1$ rectangular sub-boxes. Since the next tile has area roughly $(\alpha-1)/N$ times the remaining space, we can fit the next tile into the largest sub-box provided $\alpha<4/3$. If we don't permit squeezing and stretching, we might get into trouble because all sub-boxes that are large enough are too oblong. But it seems that if $\alpha$ is small enough and we subdivide in some reasonable way (say to minimize the total perimeter of the sub-boxes), then this should not happen. 

Not a huge denominator, but I still think it's amazing that $$\prod_{\text{$p$ prime}} \frac{p^2-1}{p^2+1} = \frac25.$$ 

Here is a more careful (EDIT: even more careful!) argument that gives an affirmative answer to the weaker version of the question (as stated in the edit to my previous post, I doubt that the stronger version is true). The argument uses the following lemma, which ought to be known. If someone has a reference, please leave a comment. Lemma: Let $a_1,\dots, a_n$ be real numbers with each $a_i\geq 1$, and let $X_1,\dots,X_n$ be independent random variables, each uniform on $\pm 1$. Let $I$ be an interval of length $2r$. Then $$Pr(a_1X_1+\cdots+a_nX_n\in I) \leq \frac{1+r}{\sqrt{\pi n/2}}.$$ Proof: Let $f(X)$ denote $a_1X_1+\cdots+a_nX_n$. In the Boolean lattice of all assignments of $\pm 1$ to the variables $X_1,\dots,X_n$, consider a random walk starting from the point where all $X_i$'s are $-1$, and moving in $n$ steps to the point where they are all $+1$, in each step choosing uniformly and independently of the history a variable which is $-1$ and changing it to $+1$. What is the expectation of the number $N(I)$ of steps of this walk at which $f(X) \in I$? On one hand, $N(I)\leq 1+r$, since $f(X)$ increases by at least 2 in each step. On the other hand, the probability that the walk passes through any given point in the Boolean lattice is at least $2^{-n}\sqrt{\pi n/2}$ (this probability is minimized at the middle level(s) of the lattice, and the claim follows by well-known estimates of the central binomial coefficient). Therefore $$EN(I) \geq \frac{\#\{X:f(X)\in I\}}{2^n}\cdot \sqrt{\pi n/2} = Pr(f(X)\in I) \cdot \sqrt{\pi n/2}.$$ It follows that $$Pr(f(X)\in I) \leq \frac{1+r}{\sqrt{\pi n/2}}. \qquad \square$$ As was explained in the earlier post, we can randomly choose $n$ pairs of opposite points $\{z_i, -z_i\}$, then find $z$ with $\left|P(z)P(-z)\right|=1$ given only this information, and finally fix the $z_i$'s by $n$ independent coin flips. In order to apply the lemma, we want to have, before the coin flipping, $n/2$ pairs $z_i, -z_i$ making an angle of say at most $60$ degrees with $z, -z$, so that each of the $n/2$ corresponding coin flips determine the sign of a term of at least $\log 3$ in $\log\left|P(z)\right| - \log\left|P(-z)\right|$. Actually, after choosing the $n$ pairs $z_i, -z_i$, this a. a. s. holds for every $z$. The idea is to divide the circle into, say, 100 equally large sectors. With high probability, every pair of opposite sectors will contain at least $n/51$ pairs (as opposed to the expected number, $n/50$). We now condition on the outcomes of the coin flips for the smaller terms (pairs $z_i, -z_i$ more or less orthogonal to $z$). The lemma above tells us that for any interval $I$ of length $4\alpha\sqrt{n}$, the probability that $\log\left|P(z)\right| - \log\left|P(-z)\right| \in I$ is at most $4\alpha/\sqrt{\pi} + O(1/\sqrt{n})$. In particular, with probability at least $1-O(\alpha)$, the absolute value of $\log\left|P(z)\right| - \log\left|P(-z)\right|$ is at least $2\alpha\sqrt{n}$, and since $\log\left|P(z)\right|=-\log\left|P(-z)\right|$, $\max\left(\log\left|P(z)\right|, \log\left|P(-z)\right|\right)\geq \alpha\sqrt{n}$ as required.