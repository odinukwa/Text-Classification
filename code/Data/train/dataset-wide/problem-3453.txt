Authentication failed because credentials were sent but the credentials were invalid. Authentication succeeded using the credentials presented. Authentication is ignored because no credentials were supplied. 

Since the strongest one does not fail (hmac sha256) its unlikely you are using the failed ones even if they could be selected. To summarize - 

I assume you are attempting to request a username with a '' in it (probably an indication of someone thinking file globbing works on usernames) or responding messages coming out of ldap contain a user with a '' in it. You could either attempt to resolve the offending username or alter the regex such that it accepts '*' as a valid user name. Note that doing this may lead to unexpected problems, especially in shell scripts where '*' is used as an expression if its not properly escaped or quoted. 

I've done a bit of research into your problem. Not easy but looks feasible. The area of code breaking you is this (well, in newer kernels): 

First byte time typically represents the time it takes for the request to arrive, to be processed by whatever application is handling it, a response to be formulated and then sent back. Basically, the lions share of the amount of time it takes. The lastbyte time - first byte time in general is giving you a measure of throughput, the send request time - the first byte time gives you a narrow enough measure of how efficiently the request is processed. You need to focus your efforts on the period of time taken to process the request on the server once it arrives. 

You can check for the existence of /proc/[pid]/exe and if it exists you know the process is running the correct version. If the source file exe points to has been overwritten /proc/[pid]/exe becomes a dead link. Providing you know what your looking for this is probably the most reliable means you can use to get the data. If you dont know what your looking for (just looking say for all pids that dont have a media backed executable), you'd have to employ some heusteric to try to figure out the original execution path of the process based off of its $0 given name (which can be altered by the process at execution time). I assume this is what @Zoredache's suggestion of checkrestart does. As far as I know (and as I've tested) this behaviour of /proc/[pid]/exe is always true - even if a new file name in the same path as the old file name is written there. /proc/[pid]/exe always becomes a dead link when the original copy is gone. Whats nice about this is that it should be distro ambiguous since it does not rely on the package manager but the manner of which the kernel behaves.. 

You can also see the selection priority with the 'priority' field. The higher the value the stronger the PRNG according to the module author. So, happy to be wrong here as I dont consider myself a kernel programmer but, in conclusion - When loads it appears to select other algorithms from the list of acceptable algos which are considered stronger than the failed ones, plus the failed ones aren't likely selected anyway. As such, I believe that this no additional risk to you when using luks. 

This program creates a 100M area of memory, and fills it with 'A's. It then spawns 100 children (101 total processes) then waits for a ctrl-c. This is the scenario before. 

Corruption can also occur on most modern disks due to in-disk re-ordering. Modern disks typically do re-ordering of requests that are used to speed up performance (by re-ordering writes to make the entire list of requests less seeky), this is called Tagged Command Queueing. It is possible the write to the journal on the disk is delayed because its more efficient from the head position currently to write in a different order to the one the operating system requested as the actual order, meaning blocks can be committed before the journal is. The way to resolve this is to make the operating system explicitly wait for the journal to have been committed before committing any more writes. This is known as a barrier. Most filesystems do not use this by default and would explicitly need enabling with a mount option. 

Note that a fifo is typically necessary in programming where the amount written in can surpass the amount read out. As such a fifo wont work entirely smoothly as you anticipate but would solve your main problem whilst introducing another. There are three possible caveats. 

Confining what is in effect a programming language can be pretty tricky because of how robust the policy needs to be to work with many different web applications. 

Theres a protocol that recently made it into the linux kernel to do this called BATMAN. I've never tried it but theoretically should be available in some of the newer distros already. 

There are examples on the netfilter site which help explain the functionality. Here is a function I wrote in my own code that sets up the netfilter NFLOG. Here are the examples they provide: $URL$ 

Other zones can in effect be free of memory but still you get an OOM because the zone you want is not free. OOM killer is a possible means to free memory when the value is greater than the value. If you check your zone you can see that this is the case here. Your biggest memory consumers are and . Either retune them to alter their memory usage, or get more memory for them. 

This behaviour only applies to POSIX ACL entries. The reason this is here is if you have a folder and inside that folder exists a file, you can acl as rwx (for example) the folder and the file. If the group permissions of the file are rw- (which they might be as a typical scenario) the mask thus gives the acl the effective permissions of rw- even though the ACL explicitly denotes rwx. On the other hand, the directory which nearly always is +x has effective ACL mask permissions also permitting +x. In summary, this mask is basically used to differentiate permission between files and folders for the POSIX ACL set so that a file does not become executable when it should not normally be. 

Total guess: You are denied because does not set traverse permissions for or does not set traverse permissions for . Not because is not readable for . As such, you never actually reach the file but fail to reach from one of its parent directories in the path. Subsequently there is nothing to report on audit for the file as the file was never reached. 

Probably selinux. SELinux restricts the domain that httpd runs in so it can only view its own processes. You can either:- 

As a matter of fact, that is not all which is evaluated. Even more questions are asked before we allow a transition. 

This sounds as if the two events are unconnected to me. You've probably run the query, then it got stored in the query cache, so when you re-run the query the results are fetched from cache rather than scanning the tables to retrieve the data. 

This represents the sector reported as producing an I/O error. The block size being 2k, sectors being 512 bytes. The additional one extra accounts for the starting sector offset for the partition. To correlate with SMART, we'll need to convert the value we now have into hexadecimal. 

The in your bash script is executed as a forked child process of the original bash. This is normal behaviour. If you do you'll see the parent/child relationship, although it wont say as the command name as is a bash internal command. 

The memory on your router is split up into zones, one zone -- HighMem contains 128MiB of memory, the other zone, Normal contains 128MiB of memory. The zone are functionally independent of one another. The memory manager works in each zone in an isolated way. oom-killer is invoked in the normal zone. You can practically eliminate 128Mib of memory when you are looking at your request. Now to the actual anomaly: 

So it turns out, in you're case, there is actually a whole bunch of different subjects that can do this, they all end up in the httpd_t domain. Thats now always the case though:- 

This particular setting falls under the influence of the network namespace that docker runs in. As a general rule does alter settings that are relevent systemwide, technically speaking however you are altering settings in which returns results on a per network namespace basis. Note that is actually a symlink to as it really does reflect the settings of the namespace that you are doing the work in. 

Yes, theres a very plausible way to do this with device mapper. The device mapper can recombine block devices into a new mapping/order of your choosing. LVM does this. It also supports other targets, (some which are quite novel) like 'flakey' to simiulate a failing disk and 'error' to simulate failed regions of disk. One can construct a device which deliberate has IO blackholes on it which will report IO errors when crossed. First, create some virtual volume to use as a target and make it addressable as a block device. 

This patch should work, but i've remade it from my original patch and have not tested it. Not even tested it compiles! So no refunds! 

Assuming the order of the data unimportant, one way to do this -not so much of a faster way- but at least somewhat parallel would be to write a script that does the following. 

I should point out that IOPS are not a great measurement of speed on sequential writes, but lets just go with it. I suspect the seek and write times of disk heads is pretty consistent despite the size of the disks. 20 years ago we we're all using 60GB disks with (roughly - certainly not linearly) the same read/write speeds. I am making an educated guess but I dont think that the density of the disk relates linearly with the performance of the disk. 

But the best way to make this work is to run rpm -aV, check the output and ensure / account for any anomalous results each package/file produces. Then on the files/packages that produce errors you can then either reinstall the package (if files differ) or reset the uids/gids/mode on the package provided in the verify output. 

Another problem can be that bacula has ran out of maximum connections, but you can just restart the agent in this case to fix that. 

Another implementation, this one avoids in groups of 8 (which would skew the test). Here the parent always attempts to keep the number of children at the number of active CPUs such it will be much busier than the first method and hopefully more accurate. 

The target types are file_t. That normally is the case where no type was ever set on the file and that is the default. You'll need to relabel the filesystem in order to get things going again. Normally the command fixfiles is used for this on redhat but I'm pretty sure that relates to RPM databases for some work so isn't likely as relevent in gentoo. You should be able to use restorecon though. You'll need to boot into a permissive mode to try and relabel the filesystem. 

The answer is you run , check if the setuid bit is set. If it is you'll run the program as the files owner, if not you'll run the program as you're own user. 

Some repositories are simply badly maintained. They dont update with security fixes for packages. People tend to write bad RPMs, they dont mark config files as config files, which overwrites your config each time you update, which could cause problems. I've seen this problem before. They do not sufficiently declare their dependencies properly. I've seen this before too, where a package was put on the system but didn't update the package which introduced issues. Installing multiple repositories all offering the same package names can lead to unforeseen dependency problems on your system. Some packages overwrite or rewrite system configuration files that other packages depend or expect to exist. This leads to problems with other packages you might not expect.