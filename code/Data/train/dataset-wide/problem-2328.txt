The alternate query plan This query is semantically equivalent to the original query but has an estimated cost of , about half the cost. It is formed by promoting the second UNION ALL up to the top-level of the query. This requires us to reference (the set of customers) twice, but even so yields a plan with half the cost by allowing seeks into the and tables. 

SQL Server statistics only contain a histogram for the leading column of the statistics object. Therefore, you could create filtered stats that provide a histogram of values for , but only among rows with . Creating these filtered statistics on each table fixes the estimates and leads to the behavior you expect for the test query: each new join does not impact the final cardinality estimate (confirmed in both SQL 2016 SP1 and SQL 2017). 

* The primary key is the clustered index here and, as Max Vernon points out, the clustered index is the table. So another way of putting it is that your non-clustered index is duplicating the whole table. 

Performance comparison In the following two test cases, I try two very different use cases. The first is the singleton seek mentioned in your question. As commenters point out, this is not at all the use case for columnstore. Because an entire segment has to be read for each column, we see a much greater number of reads and slower performance from a cold cache ( rowstore vs. columnstore). However, columnstore is down to with a warm cache; that's actually quite an impressive result given that there is no b-tree to seek into! In the second test, we compute an aggregate for two columns across all rows. This is more along the lines of what columnstore is designed for, and we can see that columnstore has fewer reads (due to compression and not needing to access all columns) and dramatically faster performance (primarily due to batch mode execution). From a cold cache, columnstore executes in vs for rowstore. With a warm cache, the difference is a full order of magnitude at vs . 

I agree with oNare that adding a sqlFiddle that builds a simplified version of your data model, shows what you have tried so far, and shows the desired results would be most helpful. That said, you might want something like the following in order to avoid double-counting the sales for any customers that were given multiple offers. I've put inline comments quoting the relevant parts of your question in this example query: 

You may want to re-write your query as follows (I'm using rather than so that I do find some synonyms on my testing database). This is similar to the re-write you found to be more efficient, but avoids the need to declare a variable and use two queries. 

Caveats The CLR approach provides a lot more flexibility to optimize the algorithm, and it could probably be tuned even further by an expert in C#. However, there are also downsides to the CLR strategy. A few things to keep in mind: 

One very interesting thing about the operator in this plan is that it has a predicate that performs an internal check. This filter removes any objects that the current account does not have sufficient permissions to see. However, this check is short-circuited (i.e., completes much more quickly) if you are a member of the role, which may explain the performance differences you are seeing. 

If I replace the statement with a of the statement in order to avoid having to stream all those rows to the console and then run the query on a real 100MM row columnstore table I have lying around, I see fairly good performance to generate the requisite 300MM rows: 

I didn't see official documentation answering your questions, but some of these things you can yourself (on a testing/dev server first!). Here's what I found: Unless you are hoping to use a very large number of concurrent locks, you likely will not have any problems. On SQL Server 2017 CTP2.1, I was able to acquire in about . There were no collisions, and this appeared to use . The SQL Server documentation specifies a maximum of 2,147,483,647 locks for 32 bit and "Limited only by memory" for 64 bit (which I assume you're using). Test script Below is a script that acquires 1MM locks and does basic monitoring of the elapsed time, memory consumed, collisions encountered. 

When run with an account that is not a , this same query takes about eight minutes! Running with actual plan on and using a p_queryProgress procedure that I wrote to help parse output more easily, we can see that almost all of the processing time is spent on the operator that is performing the check: 

Expanding upon your list, here are a couple potential downsides that we have come across in real production workloads: Seeking into multiple partitions Expanding on the queries that do not use partition elimination could take longer to execute point, there is a specific pattern that is particularly affected: singleton seeks. This operation will become much slower if all (or even a modest subset of) partitions need to be accessed. The skip scan operation essentially performs a seek into every partition that cannot be eliminated. 

I think something like the following will work. You already have the database name () in your first query, so you can use this to join to . Beyond that, you are using an aggregate function in your second query. So you simply need to group by the columns you are selecting from the first query. 

Here is another option that only takes a single pass over the data by performing the left join and then, for each resulting row, tacking on the NULL row if necessary. 

sp_whosiasctive While running this script, you can use sp_whoisactive to view the current server activity. You can often view the query plan for the specific statement that is currently executing. In my case, I see the following because the statement is most likely to be running at any given moment in time: 

You can apply your final filter on after computing the running averages. In order to reduce the number of rows that need to be processed for the running averages, you can apply an earlier filter on to ensure that you are processing the minimal amount of rows but still including all the rows that are necessary to including the following 11 rows in the running average. (The only reason for the is to handle values at that fall at the end of the year.) This will still result in a table scan given your current table structure, but at least the rows can be filtered out earlier on in the query plan. 

The sys.partition_range_values documentation states that , which appears to be used to define the partition number, is an 

I think that the approach you came up with is a pretty good one. Here is a proposed simplification that avoids the self-join on by using the new SQL Server 2012 LAG function as well as CTEs to distill the logic into a single query (also in SQL Fiddle form). 

For SQL Server, a quick and dirty approach that we have used for a number of years without any problems is to use the existence of a temporary table that has a specific name (typically based on the trigger name) in order to provide an easy manual override that bypasses the trigger. The #temp table will be specific to this connection, but remains in scope in the trigger and can therefore be used as a way to pass information. If you are dealing with a very high concurrency OLTP type of environment you'll want to test this approach and measure any overhead before proceeding in production. Let's say you have a trigger called . Here is what the code might look like when you want to bypass the trigger: 

Tying it all together in SQL Everything up to this point has been in C#, so let's see the actual SQL involved. (Alternatively, you can use this deployment script to create the assembly directly from the bits of my assembly rather than compiling yourself.) 

Let's say that you have a billion row table () with rows divided equally into 1,000 partitions (). A single seek is roughly in a non-partitioned table. However, this same seek operation becomes roughly in this hypothetical partitioned table. So the seek now performs over 500x more work if data from all partitions is needed (or sometimes even if it isn't needed, but SQL can't prove that based on your query). Note that this can come up both when you explicitly query the table for one row (or a small range of rows) and in more complex queries when the partitioned table appears in the innder side of a loop join. The good news is that SQL Server is reasonably good about taking this into account in cost-based optimization, but that still typically means that you get a hash join when a loop-seek into a non-partitioned table would have been far more optimal. Thread skew in parallel query execution In parallel query plans, threads are allocated to partitions. If there is one partition that is much larger than the others, queries against the table may be particularly susceptible to thread skew. It's possible that one thread gets too high a proportion of rows and is processing long after the other threads have done their work. This situation can happen with non-partitioned tables as well, but any partition functions that do not equally distribute rows are particularly vulnerable. See Parallel Query Execution Strategy for Partitioned Objects for a more detailed description of the allocation of threads to partitions. For example: 

But it sounds like you might actually want to grab all transactions that occur yesterday or earlier. In that case, you could simply check for all transactions that are less than today. 

In this way, SQL Server will be able to seek directly to the beginning of the range of rows that are needed, read those rows, and stop at the end of the range of rows. Note that you can see how this seek works by looking at the property of the operator in your query plan. More generally, a multi-column index across columns can support a seek on any leading subset of columns, such as or , when you are matching for equality (using ). If you are looking for a range (e.g., ), SQL Server can no longer seek on any columns that fall later in the index. In such a case, it would have to perform a separate seek within each distinct value of , which is not supported (except in a more specific case you probably don't need to worry about: across partitions of a partitioned table). 

Helpers The following logic could be written inline, but it's a little easier to read when they are split out into their own methods. 

If we use the following test script, we can see that we get the correct results and and that the execution plan takes a single pass over both #Table1 and #Table2. At least on this 10,000 row data set, performance in terms of run time and IO is improved when compared to the previous solutions that were presented. 

Maintenance cadence We already have a nightly maintenance window, and the fragmentation calculation is very cheap to compute. So we will be running this check each night and then only performing the more expensive operation of actually rebuilding a full-text index when necessary based on the 10% fragmentation threshold. 

This generally makes sense given that SQL Server will place the smaller (estimated) rowset on the build side of the join. The probe side is expected to be larger than the build side (and therefore to contain at least some rows). In addition, SQL Server has a potential bitmap optimization that it can apply to the probe side of a hash join in some cases, but this optimization requires that the build side be processed first. An example For a simple query where we force a large build side of the hash join and an empty probe side, the actual execution plan shows that all rows are processed on the build side of the join. 

Full script See this Pastebin for a full set of scripts to generate the test data and execute either of these scenarios. Note that you must use a database that is in the recovery model. 

There are at least a few possible reasons why a plan might not show up, but I haven't observed this with the same frequency that you report, and it will be interesting to see any other answers that fill in the gaps. Reason #1: sys.dm_exec_requests reports a statement_start_offset of -1 First, let's take a look at a simplified version of the section of that looks up query plans: 

We can learn a lot by creating a test script! You should be able to run the SQL below on any SQL 2014 instance (probably SQL 2012 as well, but I didn't test it there). From this script, we can see that the non-clustered columnstore index on (b, c) does store the data for the clustered index column (a) and it does so in the same manner (via segments) that it stores b and c. This allows the columstore to efficiently process queries that access a, b, and c. Technically, it even allows the columnstore to be used, in combination with a key lookup, in order to process a query that needs all of the columns in the table (although this is not likely to be favored by the optimizer given the expense of the key lookup). In terms of your questions about the "structure" of the columnstore index, I think that's too deep a question to answer here. But if you are interested in learning more, I think that Niko Neugebauer's excellent 55-part (and growing) series on columnstore has a ton of valuable information on the structure and internals: $URL$ 

Query #1: Inserting 5MM rows using INSERT...WITH (TABLOCK) Consider the following query, which inserts 5MM rows into a heap. This query executes in and generates of transaction log data as reported by . 

Further Testing With a bit of testing, we can see that there are multiple cases in which SQL Server makes query plan decisions that appear to rely on the fact that partition numbers are in order by value. Here's the full test script, and here are a few of the most relevant queries: 

FULL OUTER JOIN The chooses the small of the two tables as the build side of a hash join, meaning that the memory usage is proportional to the size of the smaller table (500K rows). 

Business context We are semi-frequently moving around millions of rows of data, and it's important to have these operations be as efficient as possible, both in terms of the execution time and the disk I/O load. We had initially been under the impression that creating a heap table and using was a good way to do this, but have now become less confident given that we observed the situation demonstrated above in an actual production scenario (albeit with more complex queries, not the simplified version here).