I am developing a number of tabular models, which currently have a limited number of users assigned to roles within each. At the moment, with the exception of 1 or 2 cases, the majority of the users are "full read" users in all the reports. I am in the process of implementing row-level security in to a number of these roles and I am getting to the point now where, before these models get deployed in to live I think I should be developing a security model to be able to easily add/remove members to/from these roles. My question is, how can I deploy a model that allows for dynamically adding and removing users, without having to deploy the solution each time. In my mind my options are as follows: 

I have a MySQL master server and slave became corrupt because of space issues on the master. Binary logs are potentially corrupt and I don't trust them. The databases are using MyISAM tables. I want to create a new slave, but I can't afford to take the master down or lock the tables for a mysqldump. Is there a way I can get a known place on the master to seed the slave without creating an outage? If not, what is the way to do this with the briefest outage? 

Index tuning is a science, there isn't a "one size fits all" solution, so when you ask "should I aim for a fill factor of 90%", this is a genuine time when "it depends". I already posted this link above, but I really recommend you read it, to understand the impact fill factor has. There are tools out there to help you on your quest (and I don't mean the Database Tuning Advisor!!!). sp_BlitzIndex is the one that I have stuck to over the years... hence my numerous links to their websites!!! :-) 

I have a MySQL 5.6 server that feeds to 6 web servers and a slave read-only mysql 5.6 server. These are the only clients with the exception of a monitoring software. If any were failing to work, I'd expect my users to notify me immediately or get a report from my monitoring software, instead I am seeing performance issues under heavy load that I'd like to improve upon. All the web servers and the db servers are on the same subnet in a VMWare cluster with SAN storage behind. I'm not seeing any performance/latency/service-affecting issues on the hardware or storage networks on other VMs hosted on this infrastructure so I'm not seeing a reason to blame hardware. In looking into this, I've noticed high connection problems on both the mysql master and slave servers. Details below are from the master server, but look similar on both. 

I realise this question is already answered, but I feel there are a few points needed for clarification. 

Yes that looks like the correct syntax (except you have two '--' for your last index name? :-) ) Also, when you are writing your script for these indexes only, you do not need the '-' before the index name. more info here: $URL$ 

I want to maintain a clean environment, so when somebody looks at the model in future they don't see users who left years back. Ultimately, when somebody leaves their AD account will be deactivated so this is less of an issue and more of an OCD. However, new members will need to be added. I'm leaning towards option 3, but are the above options my only choices? Has anybody deployed a different model? Am I missing something obvious!?!?!? I hope not. Thanks in advance for any help/tips/advice. EDIT: I have also just thought of using PowerShell to add/remove users. Based on what David said, I could get one of my guys to maintain this instead. 

I'm planning an upgrade to MySQL where DATETIME, TIMESTAMP, TIME and YEAR are potentially a problem. I'd like to query the database to find all uses of these datatypes. How can I accomplish this? 

I'd like to set the default prompt on MariaDB across all our database servers for everyone. I assume this would be best done in my.cnf. I can set the prompt in a session, but when I try to add something like in my.cnf, MariaDB fails to start an complains about it. What is the proper way to do this? 

Do this manually... not dynamic! Create user groups in AD and add the user groups to the roles. This will be ok for "full read" users (e.g. Directors), or "manager" roles (e.g. sales manager for sales reports), but would not work for users that will be using row-level security (e.g. salesperson). Create a "global security" table to add to the model, which contains the 'variable' for read access permissions for "full read" access (e.g. Directors and Managers). I will also have another security table that will hold the data for row-level security for other users (e.g. salesperson filters). Then each role in the model will essentially use row-level security based on these security tables. Use TMSL to process each deployment in the SSAS instance and "createOrReplace" each role deployment where this user exists/needs to be added... unfortunately I don't know any TMSL so I don't know how hard this is to learn? 

During normal day use, I'm seeing the rising by around 5/sec. I looked into turning on the general log, but that didn't seem to provide any insight. I'm open to possibly being blinded by the volume of information coming through that log file and not knowing where it goes. I have it off right now because of how quickly it grew in size. How can I get more details into the cause of these aborted connections so I can clean them up? 

In my mind there are a few ways you can achieve this, none of which are simple solutions. Which ever way you approach this, I think you will need to make two passes in to the database: 1) to get a list of current company names 2) to get all the contacts associated with chosen company. First up you need to decide where/how you are going to store your company names. You could either store them in a local table, and provide a lookup via the CompanyName column using a Data Validation list option. This will mean that only the names in the list can be selected. Good for data integrity. Or, if you don't want to store a local table then you will need to execute some vba code to connect to the database and download the company names. Iterate through the list of names and deliver the options to the user in whatever format you like. Alternatively, you could manually write a list of names, but this will require additional maintenance when new company names are added (not ideal). Or, if you are really brave/mad, let the user write the company name in (I don't recommend this!!!!!). Back to your original question, if you are using MS Query you should be able to parameterise your query in the query editor (if I remember correctly?!). Parameterise it such that the parameter value is taken from the cell where the user selects the company name. MS Query should do the rest for you... provided the query is parametrised client-side (i.e. in excel). Alternatively, you could once again go back to vba and dynamically create the query string in the code, passing in the CompanyName to replace @CompanyName and executing a full un-parametrised query. If your query is stored server-side then perhaps create a stored procedure to accept @CompanyName as a parameter (you will need to write your DECLARE in your usp, as previously mentioned). Then, again using vba code, you can dynamically create the EXEC statement to execute against the connection. These are just a couple of options, I'm sure there are others. But if you are looking for code as an answer, then I think you may be out of luck as there is more going on here than just "declare the scalar variable @CompanyName". 

To answer @a_vlad, the environment hosts customer dbs for a SaaS. Here is how we dealt with it: Load occurs on some customer DBs around the clock. We ended up writing a script to detect usage, if no usage occurred during our lighter times, we did a mysql dump and import for that particular organization on the master, forcing a full sync to the slave for that database. We then ran this overnight for a month and were able to catch most of the organizations on that particular database master in a usage lull. 

I believe it depends on what you set max memory to, what is available on the host, and what edition you have. Aaron Bertrand wrote a good blog post on exactly this: $URL$ To try and answer your question, "If the server has less than 128GB, you will see these technologies compete with buffer pool memory, and in fact be limited to a % of max server memory." 

ask yourself if fragmentation is really causing you an issue? Fragmentation is only really a problem when reading from disk. Cache all your data in memory, and concentrate on statistics maintenance (link). Depending on the size of your database, if you can't cache it all in memory do you have a resource problem (e.g. RAM), as opposed to a fragmentation problem? Could your server be getting a high volume of hard page faults, and you don't even realise it?! How big are your indexes? If your indexes are less than 8 pages then they will be stored in mixed extents, and no amount of index rebuilding is going to solve fragmentation... move on! What keys are in your index? If any of the keys use something like 'uniqueidentifier', especially as the first key column, then I'm afraid you are unlikely going to solve fragmentation. Due to the nature of this data type, no sooner than you have rebuilt your index, it will be fragmented again after the first few inserts... that's the nature of the beast unfortunately. If you decide that fill factor is a route you need to go down, DO NOT set it globally for all indexes. Doing so could actually make performance worse. Fill factor increases the free space on index pages, ergo making the indexes larger in size. For an index that has an incremental key, reducing fill factor from 100 (or 0) will likely hinder performance, depending on the size of the index, because you will be causing SQL Server to read more pages to acquire the same amount of data.