there are similar questions without a conclusion: DatabaseMail process is shutting down What is missing? 

In the meantime is browsing the data without locks, but if he decides that he needs to change something on the id=4 , when he presses the button, something like this should happen: 

This has been working fine for me, however, I am wondering of any possible security breach that I might open when I enable the data access. Is it a good practice enabling data access this way? Any reason why I should not do it? 

My question is: How can I use the above script that uses is_srvrolemember to find out the server roles of ? 

I have a few databases involved in replication as subscribers and they have triggers and are used by a different set of applications. Every time I need to reinitialise those database I need to script the triggers, permissions and indexes, apply the snapshot and then re-apply those triggers, indexes and permissions. Is there a script that I can use to script all the triggers of a current database? 

this script looks for all databases to find every place where a specific stored procedure lives, but if you leave the parameter out, you would get a complete list of all stored procedures. 

this was really nonsense. I right click on ssms and run as administrator, and added the permission there without a problem. 

The only extra is that this indexed view is partitioned according to the PARTITION schema and PARTITION functions below: 

I have a table that is highly hit on my database. this table has 370,370 records. when I run the following select: 

while tackling and unexpected CPU spike that was happening every 10 min one of my colleagues prepared a server side trace as follows: 

On my above code, I don't list the permissions on my User-Defined Table Types. How can I achieve that? 

As this table is located in the DBA database on my DBA_SERVER server, from my local machine I created a linked server to the DBA_SERVER so that I can insert scripts into my table. this is how I insert a script into my table, the script is located on my local machine: 

A few days ago I had a question about warnings in the execution plan Type conversion in expression may affect “CardinalityEstimate” - on a computed column? and that was solved, thanks Paul. Now I have another question regarding the , this time it is caused by the data type. I am using sys.data_spaces and sys.indexes to run a simple query to get some information about my indexes: 

I can see that the index on row 1 is contained in the index on row 8. index on row 2 is contained in index on row 4 based on the included columns and so on... questions: 1) performance in retrieving data from this particular table is a must, this database has much more reading than writing, is it ok to leave the overlapped indexes if I can see that they have been used? 2) How can I improve this script so that I will have a column called [contained] that would show the list of indexes where the current index is contained? for example on the first line it would have on the [contained] column "IDX_ItemNoLanguageID", which is the name of the index on the 8th row. 

if you want to know who wrote what and when, I would create 2 users, one for each of the procedures involved, and then create procedures with execute as user add an extra column on your testtable to keep the session_user or current_user so that you know who inserted/updated each record since insert_procedure will be executed as insert_user and update_procedure will be executed as update_user. this script can help you to find out the original login, and compares different ways of addressing the users. 

I have been reluctant to learn powershell, but I plan to accept it soon... meanwhile, the question is: Is there any way in sql server to find out the speed of the processor (in this case 2.60 Ghz)? 

then I can see the job runs several packages. how can I find using T-SQL, all the users that have permissions to run this job? 

How can I have access to the backup files? Where are these azure permissions managed? EDIT: The answer by David Browne got me there, thank you David. I am just adding here a set of steps that took me to my backups (after successfully connecting to azure). 01 - Click on 

I have just recently started to have a look at one particular server, after re-starting sql server I get the following message in the event log, as you can see on the picture below. 

Just for illustration, I will post below the process that was deadlocked against the procedure above: (it is a monster - and it should be at least a procedure but it is an AD HOC query at the moment) I could not post the process here, because it is over 30,000 lines. Anyway, I have been thinking about PARTITION these tables, one by one, starting with the ORDERS table, which is the one that causes most of the DEADLOCKS. Other than create missing indexes, I just cannot tune all the processes because there are too many. I believe that my partitioning the orders table (and all of its indexes) I can get a better management of LOCKS, and quicker updates from the replication agents. I would partition the order table by DAY OF THE WEEK. I would have to create another 6 different filegroups. I have one single drive where I can put the data. What can I monitor in order to guarantee that this project would work? By work I mean - eliminate or at least reduce a lot of the deadlocks. How would I know that partition by day of the week is the best way to partition the order table? I could partition by month, but I believe the chunks would still be too big. I could partition the previous years in an Archive partition, and the current year partition by day of the week. How would I know that this is the best choice over enabling SNAPSHOT ISOLATION level for this database? 

Basically I generate a table with random date and random values, and from that table #foo, I create the report with the help of table #bar. I have only done partition by date, not year, month or hour or minute, but I guess it could follow the same logic. everything else should be fine. 

Just complementing MartinC's answer: Instances yes, but regarding you can definitely use the following command to affect only your specific database: 

This article is also very interesting: First look: back up files and folders in Resource Manager deployment 

that gave me one operation but not the second one, I would like rather see everything that happened after I opened the . and 

this script gives the permissions assigned to a table or stored procedure in a script format. have a look what it returns for your that could be a start. 

you can wrap the code inside a stored procedure with as a parameter, that is how I did it here, including the error handling stuff, which is important too. Another thing is to install the active directory module for windows powershell 

I have recently added a SQL Server JOB on SERVER-A that goes to SERVER-B and backups all the SSAS databases. This seems to be fine. However when I want to delete backups older than 3 days, using the function below, I get the error message on the picture below. It says that the path is wrong, but that is exactly the path where my .abf files are. How should I pass the path to powershell? this is my function in powershell: 

that has an identity as a clustered index. however, as it is queried mostly according to these 2 columns: 

but when I select from that table, there is always 7 rows less. my count always tell me the table has 7 rows more that it actually has. one thing I thought might be the case, is this is actually not a table, it is a heap, because it does not have any indexes as you can see on the script below. 

I have a stored procedure called sp_radhe that I put on my servers and it has been helping me to "see" what is happening internally. here is the code of this stored procedure: 

the session 92 you can see on the above picture is a select and the session 75 is an update that I left the transaction open. session 92 

However, when a process is being blocked by some other process that is not active, I am struggling to figure it out the T-SQL of the blocking code. For example: 

So far whenever I did the replace from the view to this table, I got performance gains, because I am not doing and all the time anymore. However, I got the overhead of putting the table together at the first place, which can be an expensive operation. Questions: 1) at the moment, I am creating the table from the view without the clustered index, then adding the clustered index. Are there other things I could consider that might make a difference to reduce the overhead of inserting about 10 to 20 million rows into this table? I thought about putting this table on a different filegroup, but it will have to be on the same drive anyway so not sure if that would help. are there any trace flags to improve the insert? the database is already in simple recovery mode, with enough free space inside it, so to avoid an autogrowth. 2) as I find this type of situation not so uncommon, without changing the database design are there any other alternative that I could test against my current solution, so that I could compare both?