Fantastic job adding those details to your question, that was really helpful. It sounds like what you were running into is what Shawn Melton mentions in his answer here. In short, events (queries, in this case) are only written to that file when the (defaults to 30 seconds) or (defaults to 4 MB) is reached. For what it's worth, I have experienced that is not always enforced (i.e., if the buffer limit is not reached, the file is not updated - even if it's been longer than 30 seconds). However, as Shawn mentioned, stopping the event session flushes the entire buffer of events into the file. 

All that to say: seeing peaks and valleys in the number of pages being flushed to disk by checkpoint operations is completely normal. If the purpose of the performance testing you mentioned is to normalize (as much as possible) the number of pages flushed to disk per second, consider reading the documentation page I linked to above and adjusting those settings. 

Save the results somewhere (like Excel or something - probably not in tempdb :P) Wait 7 hours (until the job finishes) Run the same query and save the results Edit your question to include the results 

This shows that there is some branching in the method based on the isolation level / nolock hint. Why does the branch take longer? The nolock branch actually spends less time calling into (25ms less). But the code directly in (not including methods it calls AKA the "Exc" column) runs for 63ms - which is the majority of the difference (63 - 25 = 38ms net increase in CPU time). So what's the other 13ms (51ms total - 38ms accounted for so far) of overhead in ? Interface dispatch I thought this was more of a curiosity than anything, but the nolock version appears to incur some interface dispatch overhead by calling into the Windows API method via - a total of 17ms. The plain select appears to not go through the interface, so it never hits the stub, and only spends 6ms on (a difference of 11ms). You can see this above in the first screenshot. I should probably run this trace again with more iterations of the query, I think there are some small things, like hardware interrupts, that were not picked up by the 1ms sample rate of PerfView 

The query is currently using the clustered index to do the key lookup. This comment points out that you could add as a key column in your existing nonclustered index (the one being used in the scan: ) to support the predicate (it looks like it already supports the output list that you want). 

You absolutely do need to specify intent when making read-only queries from your application in order for the listener to know to route queries to the readable secondary. From the docs page you linked to: 

So the difference is 51ms. This is pretty dead on with your difference (~50ms). My numbers are slightly higher overall because of the profiler sampling overhead. Finding the difference Here's a side-by-side comparison showing that the 51ms difference is in the method in sqlmin.dll: 

As a couple users mentioned in the comments, if you are not actually having problems with the AG, this is likely a bug with SSMS. You mentioned you're on 17.4, and I see in the release notes for 17.6 that there are several fixes for AG issues, including: 

We can then take the difference of the two snapshots and determine how many bytes were read / written during the job. You can also use those numbers to calculate overall latency during that period. Note: a more granular approach would be to log the results of that query to a table every 5 minutes (or less if you want) 

And then: Check to see if there is an explicit maximum preventing your log file from growing If this is the case, set to a larger number to accommodate your actual transactional load. The number for this will depend on your usual number of transactions, and what recovery model you're using. Check to see if autogrowth has been disabled (growth = 0) If it is disabled, you could enable it. If you can't do that, and you're in FULL recovery model, you could schedule more frequent log backups. If you can't do that, and you're in SIMPLE recovery model, you'll need to increase the size of the log file manually until it's large enough to handle the transactions you have between automatic or indirect checkpoints. It's possible, maybe, that the disk of your Azure storage is full and that's what is preventing the file from growing (although I'd expect different error messages). 

Any query that you know is completely read-only would then use the read-only connection string. A caution about ORMs You should be especially careful about leveraging read-only connections if your application uses an ORM for data access. They make it very easy to change data in application code (even accidentally!) and "auto-flush" updates back to the server. If you used the read-only connection for that, AlwaysOn will route the query to the secondary and it will fail with this message: 

The solution you linked to overcomes the problem with connecting to multi-subnet failover groups by using the connection string setting. The way that setting deals with the problem is attempting many connections in parallel, rather than one after the other (the default behavior). This increases the chances that a connection will be made successfully before reaching the connection timeout limit. If you don't want to use that option (you mentioned not wanting the overhead of configuring ODBC connections on all your cluster servers), another solution would be to increase the (linked) server-level connection timeout setting to a value that's high enough to deal with the serial connection attempts. The default connection timeout on linked servers is 10 seconds (represented by "0" on the settings screen). You can increase the timeout to 20 seconds by doing the following: 

If the SSD is slower than those other two devices, and nothing else has changed* in your setup, it's likely there is a problem with the disk itself, or with the driver being used, or the controller for the array this disk sits in, etc. *things that might have changed since you moved tempdb: 

This is far and away your best, and EASIEST, option. This is the home run. Do this. Option 2: Rebuild online Index rebuilds require a SCH-M lock. If you add to your command, that lock will be deferred until the very end of the rebuild operation, which might increase the potential that whatever is preventing your maintenance task from completing has released its locks. Option 3: Identify the blocking query This is probably the most work of the options. You can run sp_WhoIsActive while the command is running, and it should show you what else is running, and specifically it will show you what other session is blocking the command from acquiring its lock. At that point, you have a bunch of options to deal with the problem: 

But according to the failover docs, a sync mode AG guarantees zero data loss during a manual failover 

If you used that template, it should definitely be logging the queries you ran from SSMS. Extended Event sessions don't start by default when you create them - you need to tell them to start. Perhaps you did that on the other server, but not in this second case? You can check to see if the session is running by look at the results of: