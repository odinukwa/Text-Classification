I might be reading you wrong but you're just trying to capture the presence of a child record that doesn't have a parent. In your statement, you could potentially have a parent record (r) that has a null value in the SerialNum field - which would return a row. Unfortunately, the rest of the logic isn't present. As such, I don't think it's a problem with EF. 

Probably depends on how quickly you want to be alerted. If you just want to able to report on it then I'd probably suggest setting up a policy in Policy Based Management. You should be able to evaluate your instance on demand or on a schedule. You can generate an alert based off the SQL job it creates as well so you can be notified. You could also look to just create a job that looked much like your query but either raise an error if there are databases that breach your policy (and append it to the alert) The two options that would almost immediately let you know if someone had changed an autogrowth setting would be either via 

I wrote a report that is executed by a few hundred users each day and the underlying stored procedure for the data set only takes milliseconds to execute. According to the ReportServer.dbo.ExecutionLog3 table I noticed the TimeDataRetrieval was regularly showing 5-10 seconds. The AdditionalInfo column shows the ExecuteReaderTime is a few milliseconds as expected and the TotalTimeDataRetrieval shows 5-10 seconds. What can I do to reduce the TotalTimeDataRetrieval for each report execution? A few more useful facts. The server itself has 128 GB of RAM devoted to SSRS and the OS, so it isn't a memory constraint. The number of rows retrieved is 1-20 rows and estimated memory usage is well under 1,000KB. My best guess is that it is taking about 5-10 seconds to open a connection to the database. I believe this can be substantiated by the ConnectionOpenTime attribute in the AdditionalInfo column. It shows 5-10 seconds and the sum of the ConnectionOpenTime and ExecuteReaderTime equals the TotalTimeDataRetrieval. Perhaps there is a way to force SQL Server to keep connections open longer? Maybe the worker threads are being killed after being idle for more than a few seconds? 

Pros and Cons The benefit of implementing this solution is that the hierarchy is not linked to the employees, but rather the employee is linked to a hierarchy. Likewise you can have an infinite number of sub-departments / sub-structures and don't have to think about limitations. The disadvantage is if the structure changes inside the company (e.g. IT1 (14) doesn't belong to CFO (2) but reports directly to the CEO (1)), because it requires a separate user interface to modify the hierarchy. 

In response to your comment: You might have to switch the SID and run the query for each SID in order to determine which database files names have already been used. You can't have the same database file (name) for a different SID and a different tablespace in the same directory. Possible Solutions 

If you just want to backup to a non-network path location, then there is now a [tool for backing up to Windows Azure. Also, SQL Server 2014 will allow to backup directly to Windows Azure without a separate tool. This is a viable alternative to trying to backup to a FTP site while eliminating the need to keep a local copy of the backup. Please note the backups can be encrypted which is an obvious must if you are storing the backups off premises. Otherwise, you may want to find a way to map a FTP site as either a network path or drive. Although I have never used either of these products, FTPUse and NetDrive should allow you to accomplish this goal. There's a blog post on how to map FTP sites as network drives in Windows 7 that might also work for you, but it appears to be an OS-specific solution that might not work for you. Although these solutions may work, I would strongly recommend that you backup the file locally and then copy it to a FTP site. Drive space is cheap. If you store the backup locally, then you can easily restore from backup without having to download the backups. You should also periodically test restoring your backups to validate that you have good backups. You should also periodically test restoring your backups from wherever you are sending the backups to make sure these are good as well. Backing up a database is an IO intensive process and backing up to a FTP location even if it is represented as a local drive will probably be very slow. Even if the performance is adequate for your purposes, FTP connections can be unstable and can be interrupted for a variety of reasons. You don't want to have to run another backup just to get the file over the to FTP site. It would be safer to backup to a high speed local drive and upload the file separately. I would strongly recommend finding a way to solve your storage problem rather than going down this road. 

Open SQL Server Configuration Manager Set the SQL Server (INSTANCE) service account to a user you don't want the service to run under Restart the SQL Server (INSTANCE) service for your instance (should fail) Close SQL Server Configuration Manager Add the service account that you want the SQL Server service to run under to the specified group (as detailed at the beginning of the post; steps 1-7) Open SQL Server Configuration Manager Set the SQL Server (INSTANCE) service account to the user you just added to the SQLServer2005MSSQLUser$ComputerName$InstanceName group. Restart SQL Server (INSTANCE) service for your instance 

However, the latter would probably need to use tokens and would definitely require Service Broker to be enabled on all databases to work. 

Depending on your version of SQL Server, you can do index rebuilds as an ONLINE operation. This is an Enterprise feature. Failing that (ie Standard Edition etc), look at re-organizing indexes - these will keep them online. Microsoft best practice says that you should generally not do a rebuild operation unless fragmentation is over 30%. You should always do some form of intelligent rebuilding - i.e check fragmentation and take an appropriate action. I can't recommend Ola Hallengren's maintenance solution enough for dealing with this. $URL$ If you are worried about the blocking effect of index rebuilds, I would recommend this sort of approach. 

Size column in sys.master_files There is a delay in the size reported in sys.master_files according to the MSDN article sys.master_files (Transact-SQL) 

Transaction Logs are Circular Equally important: The TLog is circular in nature (round robin), so if modifications can't be stored at the end of the TLog and there is free space at the beginning, the transaction will be stored there. 

Reference: sys.dm_db_index_physical_stats (Transact-SQL) | Scanning Modes (Microsoft Docs) Even if Ola's script is executing the in mode, depending on the amount of data, it can take a long time to scan a very large heap. And because you are using you are telling the script to update all the statistics ( and ) which will include statistics on heap columns. Possible Solution You might want to consider not updating the statistics on all objects, but instead limiting the scope to or then consider changing the following parameter: 

The behavior you are seeing happens if you aren't recognized as having permissions to manage services on the remote server. This MSDN article describes some possible solutions on how to grant users rights to manage services. Does the service account for the agent have local administrator permissions on the server? Have you stopped and started database services and the SQL Server agent service on the server since encountering this problem? 

We had this issue on a production server tonight and it turns out a query was blocking replication. We killed the process that was blocking replication and the second_behind_master caught up. We found the process by analyzing the results of SHOW FULL PROCESSLIST. 

Addition There is a slight twist to the answer in that the permissions can be changed so that the owner can longer modify his own objects, BUT the owner is then no longer the owner. This is a loss of ownership for the original owner. This can be achieved using the following technique: How to change an Oracle table owner 

You can search the command column of the sysjobsteps table for the path you have in your current jobs with the following statement: 

(Please read the full documentation) There is a small disclaimer at the top of the article which states: 

Interesting one - difficult to pin this one down. Have you thought about looking at the public role? sp_helprotect 'CREATE PROCEDURE',NULL,NULL,'s' Does that bring you back anything? 

I'm unsure if this is in the version of SSDT/Visual Studio you're using but as far as I know, in the Database Project type - you can add Pre/Post Deployment scripts by simply adding right clicking your project and adding a Script > Pre/Post-Deployment script. When you publish this, and generate the script, you'll see the pre and post deployment scripts in the SQLCMD syntax. In terms of doing it automatically, I don't think what you're asking is possible. Personally, I found generating DACPACs to be quite useful for deployment. Develop your database and publish to a DACPAC and you can almost automate your deployment. However, your specific example requires some knowledge of your implementation, so I can't think of a way it could programmatically know this. Apologies if I'm not reading you correctly. 

Could this be the issue? Is there a version of SQL Serer out there that has an issue with more than 2048 bytes in the comment? 

Having the MDF/NDF and LDF files on 64k formatted drives can help improve performance regarding read ahead speed. 

Visualising Log Truncation Log truncation can be observed when you query the TLog size using SQL statements or the Database Space report in the SSMS UI. You might observe that the used space inside the TLog file might only be 1% of the available TLog file size. To Shrink or Not To Shrink The general recommendation is not to shrink the TLog file, because it grew for a certain reason and will possibly grow again to the size it once was. But that is a story for another post. There are some good reasons, one being when you are re-creating the size of the VLFs inside your TLog file. Answering your questions Inline right under your assumptions and quesitons