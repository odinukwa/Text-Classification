Instead of storing the merge_id in the topic table store the id of the base topic, the one this topic's posts now appear under. A null would show the current topic has not been merged, or is the base topic into which others have been merged. To merge a new topic into an existing set would only require setting that one row's . 

Using the name "has" for every relationsip is a problem. For example, the link between Company and Address - is that the mailing address, delivery address, registered headquarters etc. By using such a weak word you obscure these differences which may cause ambiguity and data inconsistencies. 

If you intend to transfer money from one user's account to another user's account you want to make absolutely sure that the amounts all balance out and nothing gets lost anywhere, even on a system failure. Mongo does not offer this guarantee since it does not support multi-record transactions. You will have to include additional application programing to provide this guarantee. If money is involved then there will be audit considerations. Ask the authorities in your jurisdiction what their requirements are. Ensure you build these into the system from the start. Anti money laundering is a big thing too. Make sure the system can comply with local regulations. It will never be sufficient to store the balance alone. You should have a complete transaction history of debits and credits for whatever duration your regulator requires - typically seven years. Double entry book-keeping principles will be applicable. Never, ever use floating point for anything. Know where all the rounding errors end up. For security you will need multiple levels of prevention, detection and remediation. No software is perfect. All software has bugs, or relies on software (OS, DBMS, comms) that has bugs. These bugs will be exploited. Isolate each part of the application from others as much as possible. Grant minimum rights to run-time credentials. Make sure the lawyers agree the end-user terms and conditions. Good luck. 

My suggestion for a re-design would be to use a linked list, or better still a doubly-linked list. Remove the sequence column. The table becomes 

The DDL to make the change is just a text string. The necessary meta data is in the relational database already. This meta data can be queried by SQL. SQL can produce a string as output. Put this together and you get a meta-SQL-generator ... thing. Something like this: 

will, at worst, hold a few bytes of status values. No big deal. Most programs will do actual work within the transaction and this is another matter. The point of a transaction is so you can be sure that several facts within the database are true simultaneously, despite there being other users writing to the same database concurrently. Take the cannonical example of transferring money between bank accounts. The system must ensure that the source account exists, has sufficient funds, the destination account exists, and that both debit and credit happen or neither happens. It must guarantee this while other transactions happen, perhaps even between these two accounts. The system ensures this by taking locks on the tables concerned. What locks are taken, and how much of other peoples' work you see, is controlled by the transaction isolation level. So if you do a lot of work there is a good chance other transactions will be queued waiting for the objects on which you hold locks. This will reduce the system's overall throughput. Eventually they will hit timeout limits and fail, which is a problem for overall system behaviour. If you use an optimistic isolation level your transaction may fail when you try a commit because of other's work. Holding locks takes system resources. This is memory which the system cannot use to process other requests, reducing throughput. If a lot of work has been performed the system may choose to perform lock escalation. Instead of locking individual rows the entire table will be locked. Then more concurrent users will be affected, system throughput will drop further and the application impact will be greater. Data changes are written to the log file, as are the locks which protect them. These cannot be cleared from the log until the transaction commits. Hence very long transaction may cause log file bloat with its associated problems. If the current work uses tempdb, which is likely for large workloads, the resources there may be tied up until the end of the transaction. In extreme cases this can cause other tasks to fail because there is no longer enough room for them. I have had cases where a poorly coded UPDATE filled tempdb so there was insufficient disk left for a report's SORT and the report failed. If you choose to ROLLBACK the transaction, or the system fails and recovers, the time taken for the system to become available again will depend on how much work was performed. Simply having a transaction open will not affect the recovery time, it is how much work was performed. If the transaction was open but idle for an hour recovery will be almost instantaneous. If it was writing constantly for that hour the rule of thumb is that recovery time also will be about an hour. As you can see long transaction can be problematic. For OLTP systems best practice is to have one database transaction per business transaction. For batch work process input in blocks, with frequent commits, and restart logic coded. Typically several thousand records can be processed inside a single DB transaction, but this should be tested for concurrency and resoruce consumption. Do not be tempted to go to the other extreme and avoid transactions and locks entirely. If you need to maintain consistency within your data (and why else would you be using a database?) isolation levels and transactions serve a very important purpose. Learn about your options and decide what balance of concurrency and correctness you are prepared to live with for each part of your application. 

At what point should these non-values be handled: pre-processing (before they get near the database) or post-processing (after the fact)? 

From the implementation point of view for a database, is it preferable to handle optional form fields by setting to some default value (e.g. false, 0, "", etc.) or just ignore such fields (do nothing with the specific attribute) So supposing you have a form of 

PREAMBLE: I'm taking a bit of a risk asking a slightly abstract question in a stack exchange (cough) such as this, but I'm hoping that someone may have come across this type of situation before. This is a question which is also likely to be useful to be others in the future. I can give detailed technological aspects in relation to this question, and the answer need not be "opinion" based. Furthermore, aspects in relation to this question are unlikely to change temporarily. So if you wish to downvote me because of my inexperience, or because you think you are smarter than me go ahead, but don't pretend it is because it is an inherently bad question. /PREAMBLE I'm working with MySQL, and have developed a database to support a room allocation program. Aspects such as time, rooms, buildings are fixed and never change. However, the actual allocations entirely change every half year. As such the tables related to the allocation of users to rooms is entirely overhauled every 6 months. In order to preserve these records I duplicate the entire database, archive the old one and start off from scratch with the allocation information every time an overhaul is required. I dislike the redundancy that is inherent to that process. The tables that never change need not be duplicated (and in my opinion, should not be duplicated). Instead it would make more sense for them to be shared between database children, which inherit this information. Is this even possible with MySQL? No course that I have attended concerning rdmss have ever mentioned such a process. Looking on-line I have seen people state that the sharing of tables between databases would be a design flaw, but I do not see a more elegant solution. Are there better solutions to this design problem? 

Where firstname and lastname are required values (and suitably validated). In the form processing is it a "better idea"(TM) to automatically set values for icecream_type and flakes_are_the_best_thing_ever if no such values exit? Is it preferable for an implicitly boolean type such as the checkbox inputs, or indeed those relating to text, to have explicit false values instead of nulls? Bearing in mind that prepared statements to mitigate against SQL injection will normally consider all aspects of the form for insertion e.g. While SQL statements will normally select all attributes from a particular table