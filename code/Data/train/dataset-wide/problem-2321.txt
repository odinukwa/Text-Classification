The process at $URL$ can help with this. To prevent it happening in future you may need to look at the configuration of the Distribution clean up: distribution job. 

Having a column named "EmiAmount" that also indicates finance availability (a interrelated but separate concept from EMI if I understand your post correctly) is bound to cause confusion. Even your post seems to confuse the terms. And there's probably no compact name that can alleviate this. If you choose to use a single column, I would at least suggest a non-nullable column, and using -1 instead of null to indicate "no finance". The reason is that it's generally best not to assign an actual meaning to null, as the standard interpretation is "data optional and not available/applicable". Also, when you can avoid nulls it's best to do so due to the complications they cause. (Note I'm not suggesting to avoid nulls when "not applicable/available" is fully valid from a data modelling standpoint, which is very frequently the case.) 

It's critical that you do not enable RCSI on a database without the application vendor and your own developers certifying that they support it. The reason is that the behaviour of all your read queries will change, but not actually break in such a way that replaying your workload will detect. Read section 3 at $URL$ To answer your specific questions: 

That will remove the fragmentation and shrink the associated datafiles plus it will set the HWM in the "lowest" possible position. CAUTION: This is a n I/O intensive operation, never do it during business hours or outside a maintenance window for production environments. Another option that can be used is the creation of a "backup as copy" of the fragmented datafiles and then switch the database to point those "image copies". Then drop the original and redo using the image copy as the original and backing them up "as copy" to the original location. It's a little longer and some more complex, but the "downtime" is just a few seconds while the switch is performed. Refer to the Oracle Database 11.2 RMAN Reference Manual for more details on this option. Hope this helps you. If not, please add more detail to the question. Like version of the software, maintenance window time, accepted downtime, etc. 

So changes are made to blocks kept in the Database Buffer Cache (DBCache). Once commited, the changes are pushed to the Redo Log Buffer (RLB) which is dumped on a regular basis to the Redo Log Files (RLF) and, eventually to the database storage files (DF). Also on a regular basis and not completely unrelated to commit, the checkpoint process dumps the dirty blocks from the DBCache to permanent storage DF. During the checkpoint process the SCN associated with the latest DB block written to storage is written on the DF headers and the control file. That will be from that moment on the latest consistent estate of the database. 

Does this sound like a good idea? I could then swap the Log + TempDB if needed. Am I breaking a cardinal rules like never put TempDB on OS disk due to paging concerns, or perhaps never put log on slower disk than data? Edit: We also have a SSAS on the system and the end users access only the Cube. The 50% read above is based on the time it takes to process SSAS database. 

I can connect SQL Server Management Studio to a SQL DB by passing command line parameters like this: 

(Obviously -E is for window Authentication) This connects to the SQL Server. But I often find I want to connect to an Analysis Server instance instead. Can anyone tell me the command line option please? 

Each night the SQL Server will do about 2-3 hours processing, followed by 2-3 hours of AS processing. Then throughout the day only the AS are queried. Assuming this is a dedicated server, and no other apps are of concern, and that the two sets of processing are completely synchronous - no overlaps always one after the other - how can I best set the SQL and AS server memory limits. The reason for asking is that if I don't set a limit for SQL it will grab all the memory it can. However - my understanding is that SQL will happily relinquish this memory if: 

A final piece of advice: Set the db_create_file_dest to point to the ASM DATA disk group and remove the datafile name parameter from your command: alter tablespace CWSC add datafile size 100M AUTOEXTEND ON; As you may know, even if you give a name to the datafile, when it's created on ASM, that name becomes an alias and the real filename is set by the ASM instance itself. I hope this info helps you. 

Due to the bad naming convention on your question, I will use ColumnA as A, ColumnB as B and ColumnDate as Date. I am assuming that you require the first item as it's fetched from storage, no particular order. And you have to know that if the row gets updated it can be migrated from the current block and so, the query result will eventually change. This queries are examples only and you have to edit them to get exactly what you want. You may try the following query: 

Actually they don't do the same. TDE encrypts the data in the database, you need to configure the wallet and have it open before you can see/use the data after that. VPD is an Access Control Mechanism, it allows you to define sub-datasets that will be owned by different users, allowing them to see/use only the data that they actually own (even if the same set of tables are shared among different VPDs) To have more detailed information about each tool, please review the documentation on $URL$ 

So when I run the job it fails with the message that NT AUTHORITY\SYSTEM wasn't allowed to proxy 1 (I only have one credential) for CmdExec. 

So from a logical perspective I believe allow SQL to take as much as it needs, but I'm not so sure about AS' . I'm not sure if AS will relinquish it's memory. In fact reading more leads me to believe that it is wrong to let it take it all. Does this mean that I need actually set limits for both? I'm confused as to what the best practises should be, and what we need to be measuring considering the processes don't overlap. Hope this makes sense. 

My problem is that Iâ€™m trying to understand where to best put the TempDB and OS and the Log. My experience is limited in optimal configuration of these two This is not an online transactional system. It has heavy data write (new data + indexes rebuild/reorg) then heavy data read (I'm estimating at about 50/50) processing for about 13 hours, and then just quiet. My understanding is that the TEMPDB is heavily used during normal processing compared to the log. My idea is the following 

I've done a bit of google search for the permissions needed, and I read that I need to give operator access to the NT AUTHORITY\SYSTEM login in the MSDB. I've tried this but still no luck. 

EDIT 1 As some of the comments point out that the concepts aren't completely clear I will provide some more information here. We have 3 major structures involved in the commit and checkpoint concepts. 

Question: Is overwriting the PFILE with the database ON dangerous? Answer: No. The PFILE is read only when the database starts, and that's if no SPFILE is there or if the PFILE argument is passed when invoking the startup command. Otherwise, when you start the database, the SPFILE is read for initial memory values, control files location and such basic information. After that, everything will remain in memory. Note: If you change the PFILE at the OS level, you need to bounce the database to make the changes take effect. On the other hand, using the ALTER SYSTEM SET ... SCOPE=MEMORY may be used as @ibre5041 suggested and achieve the result you want. Nevertheless, you're "losing" those archives from the original location, which makes me think you could as well add the NOLOGGING clause and avoid the generation of the logs that are filling up your storage. So my personal suggestion would be to use the command 

The SCN (System Change Number) is kept in the control file and the headers of each datafile, uit allows the database to know which datafiles are in sync and where the database writer DBWR has to perform the next writes from the database buffer cache. Each backup also is "tagged" with a SCN (and thread-sequence) to allow the RMAN process know the exact "time" those were taken. Hope this helps.