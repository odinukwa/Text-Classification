One of those thing will be true so the statement is always true. Yet is says nothing about the actual ability. Trivial sentences say nothing. So as a different approach, let us look at the term believe. It is often used to convey an assumption. So the statement might as well be: 

Philosophy of science is the science about science, or to put it differently the theory of science. It is a meta-science which determines which endeavors are scientific in the first place and not merely pseudoscience. So basically, it asks what science is. What do we have to assume to conduct science? Can we know anything (for sure) and, if so, what can we know? What rules one should apply in science? Which standard should we adhere to, strict or pragmatic? Is there such a thing as scientific progress? Are there different fields which require a different methodology, e.g.: Can we conduct "physics" in the same way we conduct "sociology"? Can science influence our sense of morality? Can it affect ethics or is that something else entirely? It is a very broad field, and for me philosophy of science is quite frankly the king of all sciences. 

EDIT: The above is how one can see that negation as suggested by Belnap and Dunn is reasonable. But I agree with the questioner that it is unclear that the logic, so described, is a product system. A bit more detail on how to see Dunn/Belnap's system (DB henceforth) as a product system: In DB we have four truth degrees: 

I have no idea whether you are referring to this, but in medieval disputations de obligationibus, the rules of the disputation game admitted positing false propositions. In fact, the idea of the game was to make one of the players "concede" two contradictory propositions: he was obliged to admit any positio (proposition posited) as long as it was contingent (albeit false), and he then should be guided by certain principles in counterfactual reasoning in accepting other propositions. This more or less fits the description in your question. Apart from the SEP entry linked above, a very good resource on obligation games and medieval theories of modality in general is Knuutila's Modalities in Medieval Philosophy. 

No, there is no must per se. There is only a should in order to ensure an outcome, and as you stated, your intent is to retain the capacity of self-legislation. Another question that is hiding in yours is, "Must any freedom be guaranteed?". If so, where does the necessity derive from? And how are conflicts of certain freedoms be solved, e.g. when people use free speech to agitate or spread lies? In the end, it's about values and how those values are prioritized. In Germany for example human dignity takes precedence over freedom of speech and therefore among other reasons Holocaust denial is considered a crime. A constitution reflects values of a society at the time when it was written. It does not ensure that those values remain the same over time. And there may be a time when people will value other things more than freedom of speech and maybe they even want to give up the capacity of self-legislation. You cannot stop that. And then a constitution will be changed. And even if the people would still value those things, you cannot ensure that a political system will remain in place with the help of a text. A powerful minority may use loopholes or start a revolution and proclaim their own set of rules. 

Nothing paradoxical here, is there? Maybe the only enduring and unchanging Tao is one that people is prevented from trodding -- so as to avoid wear from all that trodding, and stay unchanged, say. Suppose we introduce the following notation: T: stands for the property of being a Tao EU: stands for the property of being enduring and unchanging R: stands for the property of being susceptible of being trodden then your sentence can be rendered thus: 

appropiate. In the philosophy I am familiar with, the debate is cast as being about the norm of assertion: The quest for the norm of assertion is the quest for a norm (roughly, an imperative) such that a speech act counts as an assertion if and only if it is subject to said norm. Candidate norms of assertion take the form Norm Schema: : Assert that p only if F(p) where F(p) is a function that takes the proposition to be asserted and outputs a specification of the circumstances in which such an assertion is warranted. There are various contenders for the role of F(p) . Let me discuss them briefly in turn. Truth: : F(p) takes p to itself. That is: one should assert only what is true (see Weiner 2005, 2007.) This looks like a natural demand on assertion; after all, assertions aim at the truth -- that is, it is very likely that they fulfill their function in language by mapping onto facts (see Millikan 1984, p. 108f). The standard complaint against Truth is that it is far too weak to be the central norm of assertion. For what if one truly asserts that p on the basis of extremely poor evidence, e.g., one asserts a lucky guess or a correctly believed product of wishful thinking? Surely, one may claim, the asserter in these instances would be subject to criticism despite satisfying Truth. (Lackey 2007, p. 604) The obvious fix to the this problem is to demand that the asserter be justified in believing the asserted proposition. Thus, the justification norm, Justification: : F(p) takes p to the asserter is justified in believing that p. This is sometimes put in terms of the asserter having a reasonable/rational belief in p. Justification is also plausible: it seems reasonable to assume that the asserter has to be in the right kind of evidential relation to the proposition asserted -- and, indeed, we find assertions blameless when they are done in these circumstances. On the other hand, there are cases of objectionable assertions made in the appropriate evidential state. Lotteries seem to supply such examples: if I assert 'Your ticket did not win' merely because I know that only one in a milion does, I may have extremely good evidence -- one that makes you ticket not winning overwhelmingly likely, and my belief that it won't rational and reasonable -- but I still contravene the norm of assertion. Williamson summarises the idea: 

A lot of different forms of suicides were and are, depending on the culture, socially accepted. E.g. the hero's death: Killing in order to help someone or something else is often considered, be it family member or a nation, to be an honorable act. Though as killing oneself is not the main goal, it is a special case. More to the point would be soldiers and secret agents, who carry around suicide pills in order to avoid a more painful death and not to leak essential information, or even suicide attackers. Also, in some Asian cultures, ritual suicides served as a form to restore one's or the family's honor after failing an important task or committing a terrible offense, e.g. Seppuku. Those forms have one thing in common: They are not only committed for oneself but for others. What you are describing is a person who wants to die because they want to die never minding the consequences. So one could boil your question down to: Is it OK to commit suicide for selfish reasons? Does the person really have to care what others around them think and what consequences they have to endure due to it? You may cause your parents and family sorrow, or your children may be orphaned, yet where does the obligation derive from to not cause them such harm? I for one would allow for such selfish suicide. All I would ask was that the person in question would seek help in order to determine if the person has made up their mind in a sound way. The choice to kill yourself is quite final by definition, so it should be done with extra care. Many people who undergo a suicidal phase feel that their problems are unsolvable, even though they appear quite insignificant from an outsider's perspective, and very much can be solved. So I would argue that we at least try to help those who want to commit suicide to understand themselves why they want to do it. Yet there is no necessity to stop people from committing suicide, and keeping people alive against their will seems unethical to me, simply due to the fact that I can think of situations where I would want to die yet could not kill myself. (Just watch Johnny got his gun.) People are not asked to be born, yet they can control how they die. So for me, I will handle it in this way: "As long as I want to live, I keep on living. If I do not want to live anymore, I will seek help in order to analyse my unhappiness and if it can be resolved. If I understand my reasons and my unhappiness cannot be resolved I will commit suicide", and I recommend the same procedure to others who feel suicidal as well. 

It's not a logical fallacy. It's faulty inductive reasoning: as you rightly say, they infer a trend from a finite number of cases in an unwarranted manner. For that matter, there are many cases in which three fails do not warrant the inference that the person will always fail (consider learning to ride a bicycle). 

is meaningful under the following interpretation: would be a plural constant designating a certain plurality of kids; a plural constant designating a certain plurality of plants, and designating the relation of shouting as above. Under this interpretation would be as well formed as , when used to mean that Socrates is a man. For the notions of plural constant and plurality see this. 

I find myself not agreeing with any of those possibilities by themselves, yet agreeing on some on some levels. I think there is no such thing as malevolent intent for a drone, so every damage would be an accident. I hence doubt we could treat such a drone like a human and have it put on trial. An accident then could either be caused by a force majeure, e.g. due to a natural disaster, or be boiled down to technical failure. For technical failure, it would be either the the company's fault for not testing the device enough or the developer's one for making a mistake. The severity would then be determined if it was a somewhat understandable oversight or a grossly negligent act. I think the real question hiding in there is: What kind of security standard do we want to apply as a society on such devices without hindering innovation too much? Or don't we want them at all? If a company or a developer must fear high compensation for damages, wouldn't the rational choice be not to build such drones?