By compactness, you only need to prove the Lipschitz property locally. First prove that Möbius transforms are Lipschitz (easy – they are compositions of translations, multiplications by constants, and inversions). Then, by composing with suitable Möbius transforms, you only need to show that a rational function which maps 0 to 0 is Lipshitz on a neighbourhood of 0. This is trivial, I hope you agree. 

0 is a natural number, the successor of any natural number is a natural number, and nothing is a natural number unless it must be, by 1 and 2. 

Consider an elliptic curve $y^2=x^3+ax+b$. It is well known that we can (in the generic case) create an addition on this curve turning it into an abelian group: The group law is characterized by the neutral element being the point at infinity and the fact that $w_1+w_2+w_3=0$ if and only if the three points $w_j$ are the intersections (with multiplicity) of a line and the elliptic curve. Groups can be hard to work with, but in most cases proving that the group is in fact a group is easy. The elliptic curve is an obvious exception. Commutativity is easy, but associativity is hard, at least to this non-algebraist: The proof looks like a big calculation, and the associativity seems like an algebraic accident rather than something that ought to be true. So this is my question, then: Why is the group law on an elliptic curve associative? Is there some good reason for it? Is the group perhaps a subgroup or a quotient of some other group that is easier to understand? Or can it be constructed from other groups in some fashion? I gather that historically, the group law was discovered via the addition law for the Weierstrass $\wp$-function. The addition law is itself not totally obviuos, plus this approach seems limited to the case where the base field is $\mathbb{C}$. In any case, I'll elaborate a bit on this shortly, in a (community wiki) answer. 

In both cases, Com(A,B) is a weakly closed algebra (i.e., closed in the weak operator topology). In the second case, it is a *-algebra too, so it is a von Neumann algebra. 

I don't know if this has been studied, but at least if you forget about a bound on the degree, a sledgehammer approach gives you a positive answer. For simplicity, assume $x_i\in[0,1]$ and proceed by induction on $n$, with the induction hypothesis being the existence of an increasing interpolating polynomial $p_n$. To get from $n-1$ to $n$, let $$p_n(x)=p_{n-1}(x)+(x-x_1)\cdots(x-x_{n-1})q_n(x)$$ where $q_n$ is a polynynomial to be determined. Given $p_{n-1}'(x)>\epsilon>0$ we have a little wiggle room: We merely need $|q_n(x)|$ and $|q_n'(x)|$ to be very small for $0<x<x_{n-1}$ (exactly how small left as an exercise) and $q_n'(x)>0$ for $x_{n-1}<x<1$. To achieve this, write $$q_n(x)=\int_0^x r_n(x)$$ and use the Weierstrass approximation theorem to let $r_n$ approximate a suitable continuous function. Adjust with a positive multiplicative constant to hit $p_n(x_n)=y_n$ exactly. The astute reader will notice a problem with this: If $p_{n-1}(x_n)\ge y_n$ this prescription loses. So we have to make sure that $r_{n-1}$, after shooting up to a nice big value around $x_{n-1}$, comes quickly back down to a small value in order to have this not happen. This complicates the proof quite a bit though, and I am not about to work through the details. I'd be interested to hear about pointers to the literature. 

The wave equation behaves differently in even and odd space dimensions. In odd-dimensional space, radial waves satisfy a modified version of the one-dimensional wave equation. In particular, Huygens' principle holds. This is not so in even-dimensional space. This difference is reflected in the usual existence proof for solutions of the wave equation, which is easier in odd-dimensional space. Then one handles the wave equation in even-dimensional space by adding a dimension. 

First, note that a mapping between metric spaces is continuous if and only if the inverse image of an open set is always open. There are various concepts for metric spaces that you can likewise find equivalent formulations for in terms of open (and closed) sets, for example compactness. Convergence of a sequence to a point can be rephrased in terms of neighbourhoods of the point, with no reference to any ε. Then you could, for example, notice how you can talk about pointwise convergence of functions, but there is no corresponding metric. So you need a more general framework for talking about different kinds of convergence, and soon enough, topological spaces won't seem so strange anymore. 

In addition to the other answers, all of which are quite good, I offer a rather pedestrian observation: If you perturb the diagonal in each Jordan block of your given matrix $T$ so all the diagonal terms have different values, you end up with a matrix that has $n$ distinct eigenvalues and is hence diagonalizable. Such a perturbation can of course be as small as you wish. Edit: As gowers points out, you don't even need the Jordan form to do this, just the triangular form. 

No. Such an element $a$ exists if, and only if, $f$ is affine and continuous in the weak* topology. Notice that $s(\mathcal{A})$ is a convex set, so it makes sense to require $f$ to be affine, meaning $f(\lambda\phi+(1-\lambda)\psi)=\lambda f(\phi)+(1-\lambda)f(\psi)$ for all $\phi,\psi\in s(\mathcal{A})$ and $\lambda\in[0,1]$. Addendum: A good reference, though it does not even mention C*-algebras, is Erik M. Alfsen: Compact convex sets and boundary integrals (1971). The reason is that the self-adjoint part of a (unital) C*-algebra is an order unit space, meaning a partially ordered vector space with a special element $e$ (the order unit) so that the unit ball is $\{a\colon -e\le a\le e\}$. Such a space has a state space $K$ consisting of all positive linear functionals $\phi$ with $\phi(e)=1$, and indeed the space can be recovered as the set of continuous, affine functions on $K$. For a more modern treatment of state spaces of C*-algebras, see Erik M. Alfsen and Frederic W. Schultz: State spaces of operator algebras (2002). 

Title: Mathematics: A very short introduction Author: Timothy Gowers Short description: As the title says, very short. Gives the non-mathematical reader a good idea what mathematics is all about in just about 100 small pages. 

Now that this is turned into a community wiki post, I'll throw an answer out there: No, a non-answer! And my non-answer is this: There are too many conferences, and the vast majority of them too specialized for most of us, to answer this question in a way that would be useful to most mathematicians. (Yeah, I am sticking my neck out here, speaking for “most mathematicians” when it's not altogether clear that I am competent to speak for myself.) I tend to prefer narrowly focused conferences myself, in a field that I actually know something about. That way, I can at least feel appropriately ashamed when I can't follow a talk on a subject I had thought I understood. Which provides an incentive to do something about it. So if I may add to the original question, then? Why do people go to big general conferences? What do they get out of it? Okay, this may sound too negative. Breadth is good, and I wish I had more of it, but mathematics is a vast field and only a few of us can attain deep understanding in more than a couple of subfields, at best. 

Sure. In 2 dimensions: $$\begin{pmatrix}\cos\theta&k\sin\theta\\\\ -k^{-1}\sin\theta&\cos\theta\end{pmatrix}$$ The idea: Scale the $x$ axis by $k$, rotate, then scale back. Now pick $k$ appropriately (left as an exercise). 

The book Interpolation Theory by Alessandra Lunardi seems quite readable, though perhaps not really “applied maths style”, whatever that might mean. See MR2523200 for a review. ISBN: 978-88-7642-342-0 or 88-7642-342-0. 

Here is a more elementary counterexample to statement 1. Let $V$ have (algebraic) basis $(v_n)_{n\in\mathbb{Z}}$. Let $a$ be so that $av_n^2=\operatorname{sign}(n)$ for all $n$ (note $\operatorname{sign}(0)=0$). Further, $av_iv_j=0$ if $i\ne j$ and $0\notin\{i,j\}$, and finally, $av_0v_n=av_nv_0=-\operatorname{sign}(n)$. Then the linear span $V_+$ of all $v_n$ with $n>0$ is maximal positive, the span $V_-$ of those with $n<0$ is maximal negative, and yet $v_0$ is not in the sum of the two. To see the maximal postiveness of $V_+$ it is helpful to note that $aw^2=0$ for a symmetric vector (obvious definition). To see that $a$ is non-degenerate, note that if $awv_n=0$ for $n\ne0$ then $w_0=w_n$, so all coordinates of $w$ are the same. 

One nice example is Bernstein's proof of the Weierstrass theorem. This proof analyses a simple game: Let $f$ be a continuous function on $[0,1]$, and run $n$ independent yes/no experiments in which the “yes” probability is $x$. Pay the gambler $f(m/n)$ if the answer “yes” comes up $m$ times. The gambler's expected gain from this is, of course, $$p_n(x)=\sum_{k=0}^n f(k/n)\binom{n}{k}x^k(1-x)^{n-k}$$ (known as the Bernstein polynomial). The analysis shows that $p_n(x)\to f(x)$ uniformly. S. N. Bernstein, A demonstration of the Weierstrass theorem based on the theory of probability, first published (in French) in 1912. It has been reprinted in Math. Scientist 29 (2004) 127–128 (MR2102260). 

The formally real Jordan algebras include the class of JB-algebras, a class of normed Jordan algebra which is the Jordan algebra equivalent of C*-algebras. From this you might imagine that obtaining a complete classification is a rather non-trivial task. In this class you also find JW-algebras, which are JB-algebras with a predual, thus corresponding to von Neumann algebras. Some of the concepts from C*- and von Neumann algebra theory carry over to the Jordan algebra setting, but this margin is too narrow to summarize what is known. With apologies for tooting my own horn here, in 1984 I coauthored a book “Jordan Operator Algebras” with Erling Størmer which pretty much summarized the state of the art at the time. (Since then I have left that field, so I don't know if a lot has happened since.) 

$V'$ is indeed dense in $L^2$. Taking Fourier transforms, note that any bounded measurable function with compact support is the Fourier transform of a function in $V$. And the Fourier transform of $f(x)-f(x-c)$ is $(1-e^{c\xi})\hat f(\xi)$. For the clincher, note that the space of bounded measurable functions with compact support contained in the complement of $(2\pi/n)\mathbb{Z}$ is dense in $L^2$. (Adjust signs and factors $2\pi$ accroding to your taste in Fourier transform conventions.) 

I rather dislike the notation $$\int_{\Omega}f(x)\,\mu(dx)$$ myself. I realize that just as the integral sign is a generalized summation sign, the $dx$ in $\mu(dx)$ would stand for some small measurable set of which you take the measure, but it still rubs me the wrong way. Is it only because I was brought up with the $\int\cdots\,d\mu(x)$ notation? The latter nicely generalizes the notation for the Stieltjes integral at least. 

It's way too expensive. In fact, I wouldn't have bought it if it weren't for the fact I am on sabbatical and need easy access to piles of papers without the paper. The battery life is too short, definitely shorter than advertised. A couple hours is no problem, so it's good for note taking in lectures. However, it couldn't keep up with a full day at a conference. Also the battery is not user replacable. It's too easy to ruin the charging connector if you try to use it while charging. In fact the whole device is a bit too fragile. The software is buggy and crash prone (but not horribly so). There have been software updates, but they are too rare and don't fix enough bugs. Also it takes a couple minutes to boot, which is bad if the device crashes while you're taking notes in a seminar or meeting. The digitizer is a bit off, enough so that drawing nice pictures is impossible. (Rough sketches work okay though.) It's especially bad near the edges. Screen refresh is slow (but that is inherent in e-ink technology). 

With $$H(r)=\int_{B(r)} h(x)\,dx$$ note that $H'(r)$ is the surface integral of $h$ over the sphere of radius $r$. Thus $$f(t)=t^{-1}\int_0^\infty H'(r)e^{-r^2/2t}\,dr=t^{-2}\int_0^\infty H(r)e^{-r^2/2t}r\,dr$$ after a partial integration, assuming $H$ does not have superexponential growth. The ansatz $H(r)=r^\gamma$ gives $$tf(t)=t^{-1}\int_0^\infty r^\gamma e^{-r^2/2t}\,dr=t^{-1}\int_0^\infty e^{-x}(2xt)^{\gamma/2}t\,dx=2^{\gamma/2}\Gamma(\gamma/2+1)t^{\gamma/2},$$ and so it is the case $\gamma=2$ that corresponds to constant $f$. This argument can be weakened so the ansatz takes the form $H(r)=(1+o(1))r^\gamma$ as $r\to\infty$, and the above calculation should yield $tf(t)\sim\mathrm{const}\cdot t^{\gamma/2}$ (modulo details I haven't bothered with). So the case you are interested in seems to yield $\gamma=2$, not $\gamma=1/2$ as your question indicates. (Edited to correct a mistake and provide more detailed calculations.) 

I don't have that book, but as far as I can understand, the “circularity” must mean this: in the phrase “iterating the successor operation a finite number of times”, we should mean a number of times corresponding to a natural number. But since the natural numbers are what we are defining, this is circular. So one has to define the natural numbers without reference to the concept of “finite”. Where the circularity is broken is if you rewrite your definition as follows: 

I have invested in an IREX Digital Reader DR1000. The sum total of its features and shortcomings appear to be ill defined, somewhat the sum of a conditionally convergent series. So I really have developed a love-hate relationship with the damn thing. Let me get the negatives out of the way first: 

No. Actually, the original version of the lemma does not require the convexity of A. Reference: Hans Rådström (note spelling): An embedding theorem for spaces of convex sets. Proc. Amer. Math. Soc. 3 (1952) 165–169, MR0045938. doi:10.2307/2032477. 

The proper analogue is rather based on the characterization of the state space of a unital C*-algebra found in (sorry about the self-advertisement) E. Alfsen, H. Hanche-Olsen and F.W. Shultz: State Spaces of C∗-Algebras, Acta Math. 144 (1980) 267–305. So the category to replace CompHausTop would be the category of state spaces equipped with orientations on their facial 3-balls, and whose morphisms are certain affine maps between these compact convex sets. In this context, a compact Hausdorff space X is represented by the set of probability Baire measures on X, which is in particular a Choquet simplex. 

Let $f\colon\mathbb{R}^n\to\mathbb{R}^n$ be a smooth, bounded vector field. Further, let $u\colon\mathbb{R}^n\to\mathbb{R}$ satisfy $$-\Delta u=\operatorname{div}(fu).$$ If $u\in L^1(\mathbb{R}^n)$, then $u$ has one sign, i.e., either $u>0$ everywhere, or $u<0$ everywhere, or $u=0$ everywhere. I have a direct proof of this for $n=1$. For $n>1$, I have a proof using the theory of parabolic equations (see below). My question: Is there a direct proof using only the theory of elliptic PDEs? (Edited to assume $f$ bounded and fix the case $n=1$ below.) For $n=1$, my proof goes as follows. The equation is $-u''=(fu)'$, which integrates to $u'+fu=A$ for some constant $A$. If $u$ changes sign then we may without loss of generality take $u(0)=0$. Thus $$u(x)=A\int_0^x e^{F(t)-F(x)}\,dt$$ where $F'=f$. If $|f|\le c$ then $F(t)-F(x)\ge c(t-x)$ for $t<x$, so $u(x)\ge Ac^{-1}(1-e^{-cx})$ when $x>0$, and hence $u\notin L^1$ (unless $A=0$). For $n>1$, my only proof is much more involved. Here is a brief outline. Assume the conclusion is wrong, so we can write $u=u_+-u_-$ with $u_\pm\ge0$ everywhere and neither identically zero, and $u_+u_-=0$ everywhere. Now let $v_\pm$ solve $$\frac{\partial v_\pm}{\partial t}=\Delta v_\pm+\operatorname{div}(fv_\pm)$$ for $t>0$, with initial conditions $v_\pm(0,x)=u_\pm(x)$. By uniqueness for this equation (with suitable growth conditions at infinity), $v_+(t,x)-v_-(t,x)=u(x)$ for $t>0$ and $x\in\mathbb{R}^n$. Also, for $t>0$ we find $v_\pm>0$ everywhere, and also $$\int_{\mathbb{R}^n} v_\pm(t,x)\,dx=\int_{\mathbb{R}^n} u_\pm(x)\,dx$$ since the equation is on divergence form. We conclude $$\int_{\mathbb{R}^n}|u(x)|\,dx=\int_{\mathbb{R}^n}(u_+(x)+u_-(x))\,dx=\int_{\mathbb{R}^n}(v_+(t,x)+v_-(t,x))\,dx>\int_{\mathbb{R}^n}|v_+(t,x)-v_-(t,x)|\,dx,$$ which is a contradiction. 

Far from a complete answer, but the answer is yes if $V=\ell^1$: For then $X$ must contain every sequence $x\in\ell^\infty$ with $|x_n|=1$ for all $n$ (consider $v\in\ell^1$ given by $v_n=\bar x_n/n^2$), and the space of linear combinations of such sequences is dense in $\ell^\infty$. To see the latter, merely note that any complex number $z$ with $|z|\le2$ can be written as $x+y$ with $|x|=|y|=1$. (Over the reals, you need to work a tiny bit harder.)