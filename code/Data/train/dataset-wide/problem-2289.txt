I have an ORM generated query (Django) running against a local Postgres 9.1.11 instance that looks up ~5 records from two specific match fields. Due to the nature of the EDI load program, I need to execute this query many thousands of times. The planner insists on scanning the entire table for each query and the whole process is taking an unreasonably long time (1+ seconds per query). There has to be an index I can create to allow Postgres to seek to the few records I want upon each execution. Query, generated by the ORM: 

If I constrain the plans to disable seqscans (set enable_seqscan=False), it will index range scan on the first key, "campaign_id" but not "phone_number", which really doesn't help any. Constrained plan: 

I'm at a loss as to why it won't use an index for the "phone_number" lookup. Edit: It's definitely the "IN" that is causing problems. I tried changing the "phone_number" condition to a straight "=" and it will use the 3rd index then (and maybe others), seeks both fields, no need to touch the heap. 34ms execute time from a remote hostÂ instead of 500ms+. That doesn't really work in my case as the program may need to look up multiple values through the ORM. I would think that the IN clause is equivalent to "=" for a single match, but that does not seem to be the case. Edit 2: more weirdness I tried the suggestion of putting "phone_number" first in the index, and when I asked PG to EXPLAIN the query, now it is using the plan I want (one step index range scan with both args, no sort, no heap access), but with the "dialer_callrequest_campaign_phone2" index, which is the one I had yesterday! At this point it seems to be a statistic problem. I thought "ANALYZE " was supposed to fix that? Is there a way I can ensure it won't revert back to the bad plan in the future? Current EXPLAIN: 

It creates intelligible data structures in your database that are easier to compose and re-use in other areas of an application. All of the most costly query operators have been factored out of the query plan using some basic indexing. 

I'm getting behavior in a query plan I cannot explain. The difference is between two filtered indexes I'm testing with. One uses a and the other uses . In my actual data I get a 95% favorable runtime using the index. I can't see why they would be different... my keys are auto incrementing integers starting at 1 referenced by the adjoining table on a nullable column. Below is a script that will generate structures and data analogous to my production data. Please note the following... if you execute these scripts and then compare the two select statements, you will probably get fifty fifty performance like I did. However in my production data the query that utilizes the filtered index chooses an index scan instead of seek. This scan runs much faster. Below is a screenshot of my actual query plan comparison. Secondly, I have rebuilt these indexes, fragmentation is not an issue. Questions: Why the disparity? Where does the scan vs seek come from? Wouldn't and be equivalent in a join when the datatype is int identity(1,1)? 

...All scans with a full sort. Notice performance cost of hash matches take up bulk of total cost... and we know that the table scans and sort are slow (compared to the goal: index seeks). Now, add basic indexes to help the criteria used in your join (I make no claim these are optimal indexes, but they illustrate the point): $URL$ 

I would have expected this to result in a simple clustered index scan. However, looking at the live query statistics, and the actual execution plan, the majority of the query's execution time comes from sorting the results after the index scan. Why is the ordered data being sorted again? $URL$ 

I have a table with a non-clustered index on the id, and a clustered index on some other fields. The fields of the clustered index are non-sequential, and are frequently modified. I'd like to change the table to use the auto-increment id column for the clustered index. 

My first attempt to delete the data simply took far too long - it's historical data so there are a few million rows. After that failed, I asked around and someone suggested disabling check constraints, deleting, and then re-enabling the constraints for those specific tables (which I will NEVER do again, terrible idea). The disable and delete ran fairly quickly, I had to leave the enable running all night but it did succeed. From then on, the database has been at 100% CPU, and one of the queries occasionally never completes. It's the same query every time: 

Are the partition numbers in order of when each partition was created, or are they in order of ? E.g. if I were to add another split to the function while it is in use, would ordering by partition number still work? 

If I want to query the logs, in order of date, I have been told I can use the partition number in order to avoid a sort when the results from each partition are joined back together. 

FYI, I used mssql 2008, so Postgres won't have the "include" index. However, using the basic indexing shown below will change from hash joins to merge joins in Postgres: $URL$ (no index) $URL$ (with index on join criteria) I propose breaking your query into two parts. First, a view (not intended to improve performance) that can be used in a variety of other contexts that represents the relationship of inventory dates and pricing dates. 

Then your query can become simpler and easier to manipulate for other kinds if inquiry (such as using left joins to find inventory without recent pricing dates): 

This shows improvement. The nested loop (inner join) operations no longer take up any relevant total cost for the query. The rest of the cost is now spread out among index seeks (a scan for inventory because we are pulling every inventory row). But we can do better still because the query pulls quantity and price. To get that data, after evaluating the join critera, lookups must be performed. The final iteration uses "include" on the indexes to make it easy for the plan to slide over and get the additionally requested data right out of the index itself. So the lookups are gone: $URL$ 

Update1 I copied production data into my test tables. Instead of fifty fifty, the results matched the included query plan and reproduced the disparity. The test scenario is structurally analogous. Update2 

Now we have a query plan where the total cost of the query is spread evenly among very fast index seek operations. This will be close to as-good-as-it-gets. Surely other experts can improve this further, but the solution clears out a couple of major concerns: