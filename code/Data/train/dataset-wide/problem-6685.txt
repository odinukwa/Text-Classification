I'll just drop a list of some philosophical languages dedicated to a strictly compositional system making use of such "primitive meaning blocks", there have been many such attempts (there was a hype among philosophers around the 17th century): 

Edit: This answer focusses on mainstream generative grammar approaches which are heaily based on English (or Indo-European languages') syntax, assuming that this is what you are primarily interested in. However, as WavesWashSands pointed out in their comment, things might look completely differently for other languages where the assumption of something like a category I is of not much avail and S might indeed be approriate. S as the root of a sentence is a rather old assumption that is rejected my most modern linguists and is only used for simplification when a detailled analysis of what a sentence constitutes is not relevant (e.g. when explaining how phrase structure grammars work in general, or in very simple "toy grammars" to demonstrate parsing algorithms in computer science/NLP). When not immediately related to natural language syntax, but for the description of formal languages (such as {anbn : n ∈ ℕ}, i.e. the language consisting of all possible sequences of aaaaa...bbbbb... where there are just as many a's as there are b's) with phrase structure grammars, which has its use mostly in computer science and mathematics, S is still the convention for the root label, but usually thought of as an abbreviation for "start symbol" rather than "sentence". The reason why S is not so nice is that it doesn't really give any information of what it is built from, and doesn't fit the general scheme of constituent trees. In X-bar-theory, variants of which pretty much any major syntax theory is based on, you want to build up your trees systematically starting from lexical items (words) below and making constituents larger and larger, going over to phrases until everything is combined to one maximal structure, thereby assigning every node a combination of a category (such as nominal, verbal, prepositional, inflectional, ..) and a so-called projection level (roughly, it tells us how complete our tree is up to this point: is it a single word, a phrase (= a group of words which somehow belong together) or something in between) encoded in the label name. Labelling the root of your syntax tree with an S doesn't fit into this scheme now, because you do have "S" as an abbreviation for "sentence" and thus a kind of category, but it doesn't tell us anything about the mentioned projection level and isn't build up of anything S-like either (a CP (= complementizer phrase) has a complementizer in it, an IP (= inflectional phrase) contains an inflection node, ...); for all the other constituents further below in your tree, you have a so-called head which determines the category of the phrase, dependents which complement the head and so on, but the S is just standing there as a single S-thing and branches into, e.g., some NP and VP which violates the assumption that sentence structures are built up in sysetmatic and preditable patterns. TP, IP, CP etc. are different here; they do fit the widely accepted X-bar scheme and give us the information we want, namely of what category a sentence is (something tensed/inflected/...) and the projection level (namely a complete phrase that can stand on its own). That's the reason why most modern syntax theories rely on these labels as the top nodes of sentences, because they don't break the general explanatory pattern of how constituent trees are built up. Additionally, IP and TP are labelled so because they are headed by a category "inflection"/"tense" which makes sure that the sentence is properly inflected w.r.t. tense, subject-verb agreement and so on, and it is assumed that this happens at a stage higher up in the hierarchy after all verbs have started off as uninflected. If you don't include an I or T in your tree, but simply say that an NP and a VP combine to an S, the question is where the inflection comes from, because it must be reflected in the tree somewhere. So, no, you can't simply put everything under S without violating basic assumptions shared by major syntax trends. But you can, if you want, make the generalization for yourself that "CP", "IP" etc. are more or less equivalent ot "sentence". TL;DR 

1 and let's also ignore the rather unplausible third reading where we have a yes-no question but with an exclusive or, to which a valid answer would "yes" if the adressee eats either fish or meat, but not both 

Almost, and I don't see the problem with it. While, taken precisely, an adverb phrase is something that contains a lexical adverb while an adverbial might (but not needs to) consist of something different, like a PP or an NP, their function is essentially the same: They modify the verb, the verb phrase, the sentence, sometimes adjectives, and in general anything which is not nominal. In a syntactic structure, what matters are not primarily the precise lexical classes - these can already be encoded in the tree by differentiating between node labels AdvP, PP, NP etc. Rather, what is relevant in syntactic considerations is what role a constituent plays in a sentence. The reason why it is called adverbial is because they behave the same as adverbs. They occur in all the same positions, they make all the same kind of modifications, and if it wasn't for the lexical categories they consist of, you couldn't tell any syntactic difference between an actual adverb phrase and a general adverbial phrase. This makes adverbial a useful coverterm for "everything which behaves like an adverb (= adverbial)", which is at the same time more specific than the terms modifier or adjunct, which can as well refer to, e.g., PPs or AdjPs modifying an NP by adjunction, which I would not call adverbials. To summarize, they are not exactly identical in that an adverb phrase is headed by a lexical adverb while an adverbial phrase isn't necessarily, but they are identical in their syntactic function (modifying verbs, VPs, sentences etc.) and this is what is relevant in syntactic analysis - after all, playing the same role as lexical adverbs in a sentence is the very reason why they are called so. And as long as they serve to distinguish a certain class of constituents, the mere etymology of a term shouldn't bother one too much. 

You seem to be mixing up letters with sounds - don't mind the downvote; this is something that confuses many people who haven't had much linguistic training. "u" as the written letter is just a symbol in the alphabet, which happens to be classified as a vowel in English, since "u" behaves as a vowel in words such as "turn", "tour" or "up". In this sense, "u" as a letter is a vowel. Similarly, "you" as a written string of letters starts with the letter "y", which is sometimes a vowel an sometimes a consonant in English: It behaves like a consonant ([j]) in environments like "you", "yes", "voyage", but it behaves like a vowel ([i]) in environments like "happy", "layman". So as a written letter, "u" would be a vowel. But the letter "u" des not only have a written spelling ("u"), but also a pronounciation: It is pronounced as the sound sequence [juː] - which starts with a consonant, [j]. The pronounciation for the letter "y" is even different: It is pronounced [waɪ] - starting with the semi-vowel [w], like in "water". So the spelling of letters can be different from the pronounciation of the letters. If we look at how these individual letters are realized as sequences of sounds in words, we notice that most words are not written as they are spelled: In "turn" or "up", for example, the "u" is not pronounced like in the word "tour", the "o" in "tour" is not pronounced at all, and so on. And only in words such as "unicorn" or "use" the letter "u" is realized like its letter counterpart "ju". Smililarly, "y" is often pronounced as the vowel [i] rather than the consonant [j], and almost never as [waɪ]. So, as a letter, the written string "u" is pronounced as the sound sequence [juː], and these are two different things. And as used in words rather than as a letter in the alphabet, it can be pronounced even differently - the same holds for "y". And it so happens that the word "you" is pronounced the same as the letter "u", namely [juː]. The pronounced word "unicorn" starts with the same sequence of sounds: [juːnɪkɔːn]. So in their pronounciation, both "u" and "you" and "unicorn" start with a consonant - [j]. In their spelling, two start with a vowel ("u") and one with (depending on how wants to classify "y" as a letter) a consonant or a vowel ("y"). Whether or not now "you" or "unicorn" start with a vowel depends on which level of reprentation you are talking about: In terms of spelling, "you" starts with the letter "y", which can be seen either as a consonant or as a vowel, and "unicorn" starts with the letter "u", which is avowel. In terms of pronounciation, "you" and "unicorn" start with the sound [j], which is a consonant and not a vowel, followed by the sound [u], which is a vowel. But in any case, it is false to claim that "you" starts with a vowel because it is pronounced like "u" - this would be mixing up two differnt things: It is true that they have the same pronounciation, but at the level of pronounciation (i.e. sounds), they start with the consonant [j], and this is something different than the concept of "u" as a (vowel) letter, which, as a letter, just happens to be pronounced the same way as the word "you" (despite their spelled forms starting with different letters ("u" and "y"), and despite neither of the pronounced alphabet letter "u" nor the pronounced word "you" starting with the sound [u], but both with the consonant [j]). Letters ≠ sounds. 

Note: My explanations presuppose a highly truth-functional approach to natural language semantics; I am aware that one could try to account for the two concepts in a completely different way, e.g. by cognitive semantics making use of more psychological evidence, and that there is a lot of criticism one could impose against such a bare truth-functional/set-theoretical/formal-logical account of meaning. I still think that they are not too useless especially when it comes to the actual meaning of very vague terminology; sticking to rather strictly defnied entities like functions, sets and truth values often helps to disambiguate between notions that are hard to capture concisely. Sidenote on the correct translation of Sinn and Bedeutung: It is true that the translation of the terms Sinn and Bedeutung is not as easy. Actually, I would argue that the English set of terms is a more suitable one: Bedeutung is, in ordinary language use, mostly understood as something more intensional, while denotation makes it clear that we refer to a particular instance of that meaning in an actual world. Sense is a direct translation of Sinn, and probably more staightforward to interpret, although sense is probably not something you would find a lot in the terminology of more recent semantic research. 

Men, women and children is not a word, but a phrase (a noun phrase = NP 1 ), so you can not apply morphological terminology like compounding here. I don't think there's a special name for NP formation via conjunction other than exactly that - NP formation by conjunction. 

This is what we used to do as kids when we decided it was opposite day. To a question "Do you like ice cream?", you were supposed to reply with "No, of course not, there is nothing I hate more than sweets", however it was perfectly valid to say "seY" instead. With all the weird stuff that happens in natural languages' phonology, I have never found a language that uses phonetic reversal this systematically - probably because it completely destroys the nice compoistionality. 

To view the individual XML files in an editor (because this will help you understanding their strcture), just go to the directory where it is stored (default directories are given here). If you want to use it in Python: seems to be a function provided by the corpus reader that returns a list of posts (which are again lists of words) and extracts only the inner elements, ignoring attributes in the XML elements. The documentation together with the source code for the reader can be found here. I'm too lazy now to go through it in all deatil, but it should be easy to find out whether there is an already implemented function that enables you to return the author of a post. If you need to process the author within your code (because you could otherwise just look at the raw XML) and the reader doesn't provide such a function already, you probably need to write one on your own. It shouldn't be too hard actually, there are good libraries to process some XML files - more precisely, the reader makes use of a special XML Corpus Reader - and while writing, you can cheat by looking at the already provided fuctions. Actually you'd just need to check what the function exactly does and modify it so as to return not just the inner elements of a post but also the value that is stored in the attribute (you'd need to consider then how to bring the return type together with the word list, because you can not return both the word list and the user name in one element, so you'd need e.g. an additional function returning only the user, or one combining both somehow), but that should be doable, as soon as you have managed to access an individual post with an XML parser. 

I know this question was posted four years ago, but in case anyone has the same question and comes across here: For a general introduction to the field of computational linguistics, I think Languages and Computers is a great book to start with. It gives a well-understandable overview to the problems and solution concepts of most basic areas that computational linguistics deals with, such as encoding, machine translation, searching, dialog systems, spell checking and language tutoring systems. The texts are more focussed on the general theory behind the various applications and not so much on the implementation of actual algorithms, it is really more an introduction than a programming book, but definitely a good one. If the quetsion is more specifically about NLP, parsing is also discussed, however not in that much detail; the references to further reading provided might be helpful then. 

For corpora in general, you can take a look at the Virtual Language Observatory (VLO). Just search for "English vowels" or something and see if you make a find. Alternative suggestion: There is a plugin for Praat (the quasi-standard for phonetic analysis among linguists) that provides a script which "extracts the vowels of every selected Sound object and joins them into a new Sound object". I haven't used it myself so don't know how well it works, but you could give it a try. That way, you could continue to work with your existing corpus - read out full paragraphs seem like a more naturalistic, thus more promising resource than recordings of isolated vowels (if there exists such a corpus).