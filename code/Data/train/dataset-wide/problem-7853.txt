and Octave and Scilab are free software and open-source software which can be freely downloaded and used on multiple operating system platforms. 

If the Voronoi tesselation of a set of points is a honeycomb pattern, then the points are all located at the center of each of the hexagons of the honeycomb, except for the edges. This is only true for a simple Euclidean geometry, of which "London" is a small enough region (steradian/solid angle of the Earth's spherical surface) that a patch of the Euclidean 2-d plane is a reasonable approximation. So the result you are looking for is going to be a "hexagonal grid", similar to what you would see in board games that have playing fields divided into hexagons. If you limit yourself to small patches of the Earth, say London, or Paris, rather than trying to do it for all of the Earth at once, then a regular hexagonal grid will be fine. Once you have a regular hexagonal grid, you will have to choose the scale you would like to use (the edge length of the hexagons, or the area encompassed by each of the hexagons). Notice that on the board games, they colour the countries by the hexagon, and approximate the "border" as boundaries between the hexagons, rather than allowing a hexagon to straddle two different countries. You will always have the problem of deciding between very small hexagons, with many partitions and small populations but better approximations of the boundaries of the post-codes, and larger hexagons, with fewer partitions to deal with each containing larger populations but which will resolve to a poorer approximation of the boundaries of the post-codes. The scale which you decide to use will probably depend upon the nature of the similarity of the members of the group clusters. You may be introducing an artificial group of boundaries which you may not need to. You may also want to look at k-means clustering as a way of generating a clustering of these data points. It will not give you a regular hexagonal tesselation, but will give you a reasonable approximation of where clusters are located in your data-set. One problem with k-means, fuzzy-c-means, (and similar clustering techniques) is that the centroids are defined by the location of the members of the clusters, and the final segmentation which is reached by the iterative algorithm is strongly dependent upon the choise of the initial points of the clusters, and on the number of clusters ($k$) selected. Selecting the proper number of clusters is essential. This is usually a big problem for particular data-sets, however, in your case, your desire to use the "city centers" or "post-code centers" as the centers may be an advantage for you in the use of k-means. Set the number of clusters to be the number of post-codes which you wish to use. Set the initial centers of those clusters to be at the "geographic center" of those post-codes. Then let the k-means clustering algorithm run on your data set. The algorithm may coalesce multiple nearby clusters into single clusters. The end-result of k-means will not be a regular hexagon tesselation. It will, however, give you a good pointer as to which local areas can be categorized together and considered as a single cluster. You can then decrease (or increase) $k$ and rerun the algorithm and allow it to refine the clustering. 

If you look at your +1, -1 weighting of edges, you'll see that the same results would be obtained if the weighting were applied to the vertices, as each face is an $n$-gon polygon, and every (simple non-self-intersecting planar) polygon has the same number of edges and vertices. Is there any particular reason that you are applying the weighting as an attribute to the edges, rather than to the vertices? What kinds of systems are you looking at? 

I believe that if you look at the correct parametric form of the lissajous curve, your equation will be correct. 

I find it useful to write notes during a lecture of key formulae that I don't follow or understand completely, so that I may refer to it in detail and in context later. Sometimes, I end up understanding the formula better 10 or 15 minutes later during the talk, and I can refer to the formula clearly while the lecturer has moved on to another slide. As an amateur and a beginner, I also wrote notes by hand because my brain did not always comprehend or parse the equations and symbols correctly. Now that I am more familiar with the symbols and notational conventions, I find it possible to type LaTeX during a lecture, but I tend to avoid doing it in the smaller classrooms as the typing noises can be a distraction to others. 

This is also not an answer you seek, however this is a furtherance of a comment I made above, where I agree with Daniel Litt that with enough pockets, the cue ball will ultimately scratch. In fact, as long as there is more than zero pockets, it is always very likely that the cue ball will ultimately scratch, except for precisely finicky situations. I shall construct such an example. Also, with Joseph's conditions, no momentum is lost with each ball-to-ball contact, thus the white ball will always have to keep moving. In fact, it's almost an analog (real, in $\mathbb{R}^2$) analogue to a discrete stochastic matrix: if there are any elements of the Markov chain which are absorbing states, then ultimately everything will be absorbed. Even if there is only one pocket (or just the standard six), the cue ball will ultimately end up in one of the pockets for this Q's criteria, unless you set it up just right. The pockets are analogous to the aborbing states in the stochastic matrix of the Markov chain. The exception to the absorbing states' always absorbing all possibilities occurs if there is a cycle in the Markov chain. Here is an example where you end up with an infinite loop, equivalent to a cycle in a Markov chain. 

You can find the steady-state distibution over long periods of time to be stable: it is equally likely to be in any of the $n+1$ states with probability $1/(n+1)$. 

You will find a lot of literature about this from the 1950's and 1960's about matrix representations and matrix manipulations with computers, and a lot about this if you research floating-point representation standards. 

However, the computer science courses that talk about computability theory are talking about these concepts abstractly, even when they talk about it for a particular algorithm, or even for a particular circuit (like a bit-adder with carry-over shifting) in electrical engineering circuit design classes. 

This should show you how you can return a function of a variable as the result of a function that takes another variable to help define a function like exponentiate that takes two variables (the base and the exponent). 

I would suggest searching for round-off error, floating-point representation and floating-point precision and numerical simulation of dynamical systems. This is equivalent to doing numerical simulation computations using integers for all of the variables. Integers, short and long, signed and unsigned, are represented using a fixed number of bits in computational systems. Floating point numbers are also represented with a fixed number of bits, with a certain number allocation to the mantissa, and the remainder of bits allocated to the exponent in base $2$, representing what the mantissa is multiplied by This is also equivalent to what must happen in fixed-digit or limited precision representation of floating point digits in any computerized simulation of any dynamical systems. The numerical simulation is only correct up to a certain number of digits, and the imprecision can build up rather quickly depending upon the variance of the elements of the matrix. Rounding occurs in every floating-point representation using a fixed number of bits; it just occurs at smaller magnitudes with larger number of bits used for the mantissa. There are two points to consider: 

To answer Benoît Kloeckner's comment that <<[then] the solid angle under which each face is seen from the center of gravity alone would determine the probability of the dice landing on that face. But to determine all polyhedra for which this solid angle is constant is already a nice problem.>> I don't believe that having similar solid angles is sufficient to determine the equal probabilities of the die landing on faces with similar solid angles. Here is a construction for 2-d die (which can easily be converted into prismatic die, disregard if the die lands on "top" or "bottom" face, and look at the relative probabilities of landing on the prismatic faces) Using polar coordinates $(r,\theta)$ , let's define a fair hexagonal die's profile as the closed path determined by the six vertices at $(1,\frac{\pi}{3}), (1,\frac{2\pi}{3}), (1,\pi), (1,\frac{4\pi}{3}), (1,\frac{5\pi}{3}), (1,{2\pi})$ Now let us define an unfair hexagonal die's profile as the path defined by the polar coordinates $(1,\frac{\pi}{3}), (1,\frac{2\pi}{3}), (100,\pi), (1,\frac{4\pi}{3}), (1,\frac{5\pi}{3}), (100,{2\pi})$ Now this die's center of mass (center of gravity) remains at $(0,0)$ since the material the die is composed of has uniformly homogeneous density. This unfair die also has each prismatic face subtending equal solid angles (and equal angles of $\pi/3$ for each edge in the $2$-dimensional case), however the this unfair die is highly biased towards landing on two faces to the detriment of the other four faces probabilities. Thus Benoît Kloeckner's conjecture that "the solid angle under which each face is seen from the center of gravity alone would determine the probability of the dice landing on that face" is incorrect. In fact, using this polar coordinate approach, it can be seen that using any three radii greater than $0$ in length yields a rotationally symmetric die profile with equiangular faces (edges which subtend equal angles in $2$-d, prismatic faces which subtend equal steradians of solid angle in $3$-d) and with center of mass still at $(0,0)$: $(r_1,\pi/3), (r_2,2\pi/3), (r_3, \pi), (r_1,4\pi/3), (r_2,5\pi/3), (r_3,2\pi)$ but very few of these would be fair. Particularly the non-convex profiles, which also are equiangular, but make it possible to land on pairs of vertices/edges without landing on a specific face