I suppose the shell TERMs it when it exits, which would be immediately, due to the '&'. Maybe you want to use start-stop-daemon? E.g: 

In order to make this work, I had to modify /etc/fstab on nic-standalone so that it pointed to the correct root file system and swap partition (/dev/sda, /dev/sdb). Kernel and initrd above are standard CentOS 6 stuff. (There may be various performance parameters/tweaks desired as well, I just haven't gotten that far yet.) 

You may want to consider switching the guest over to using bridged networking instead. This means the guest will request it's own address from your NAT:ing modem. If you can tell your modem to assign a specific IP, you can then make sure your server gets a persistent address that the modem can forward too. This of course still requires firewalls to be properly configured or turned off. 

Many applications can log to syslog, which means you can get the logs to a log server. apache, mysql, tomcat (log4j) can, at least. Then you need a competent syslog server to do the aggregation. I use syslog-ng, but that's because it was the only serious alternative 7 years ago. Debian Lenny switched to rsyslog, which probably has a saner codebase and even more features. In my experience, a good regex engine is the most important part of an aggregating syslog server. There is so much gorp you want to filter out so you can see the relevant parts. You can also point logwatch at the aggregated logs if you want to get started quickly. EDIT: I should be explicit. Our strategy is to log everything from a specific host to one or more files in a folder for that host and simultaneously log to heavily filtered files that logs certain activities across all hosts. For example, there may be a file with failed logins across all hosts. 

Is it possible to resize the guest's root partition without rebooting the guest? Just doing lvextend on the host and resize2fs from the guest does not seem to be enough. 

You do not need to run your own DNS server. It is customary to have a so-called MX record describing how to deliver mail to addresses in that domain, but it is not strictly necessary. This is how it works: A mailserver that wants to deliver mail to your server asks its DNS server for an MX record: 

It seems unlikely that or (and in other samples , and even ) suddenly decided that it wanted to read hundreds of MBs, having previously read very little for hours on end. This behavior goes on for about an hour and various processes show up with 100+ MB read over this period. What can explain these high numbers for normally well-behaved processes? I thought read for these numbers which should be reasonably accurate for actual EBS activity? 

Is there a way to let anonymous access to a certain S3 bucket only from my EC2 instances (all of them) within a single AWS account? 

For some reason the CPU usage is at 20% CPU while I'm doing absolutely nothing, exactly every 10 minutes spiking to 28-30%. I thought there was something wrong with the instance, so I've re-created it, same thing. What does this? Is this an RDS phenomenon in general or is this specific to the burst capable instance classes? 

Basically what it ways on the tin, how can I create individual per-instance alarms inside an auto-scaling group created with a CloudFormation template? I can reference the ASG itself in an alarm and create ASG-level alarms, but cannot seem to specify dimensions to be "any EC2 instance belonging to this ASG". Is it possible or is my only option user-data script? 

When creating an autoscaling group I can choose an ordered list of termination policies for its instances. Amazon's documentation states that 

I want to forward TCP connections on a certain port of the machine A to another port on the machine B (which is actually the same that originated the connection to machine A) and simulate random or deterministic packet drops. How can I do it with iptables? 

It's single availability zone no backups not in a security group that's reachable from the outside world 

I cannot use UserData, because anyone can read it. I cannot use private S3 buckets for the same reason (metadata and hence credentials can be accessed by anyone on the box). I'd strongly prefer not to bake my own AMI, as it's quite a hassle. 

Within plain EC2 environment, managing access to other AWS resources is fairly straightforward with IAM roles and credentials (automatically fetched from instance metadata). Even easier with CloudFormation, where you can create roles on the fly when you assign a particular application role to an instance. If I wanted to migrate to Docker and have a kind of M-to-N deployment, where I have M machines, and N applications running on it, how should I go about restricting access to per-application AWS resources? Instance metadata is accessible by anyone on the host, so I'd have every application being able to see/modify data of every other application in the same deployment environment. What are the best practices for supplying security credentials to application containers running in such environment? 

How can I tell (in ) if I'm running in interactive mode, or, say, executing a command over ssh. I want to avoid printing of ANSI escape sequences in if it's the latter. 

MySQL failed to start. The reason this occurred should be logged in /var/log/mysql.log (or is it still /var/lib/mysql/.err?). Can you paste the output from there, and maybe we can figure out what went wrong. 

Also, it is worth noting that in the case of a crash, if all your processes log to the same tty, your terminal emulator will display e.g. a kernel panic or a spontaneous reboot (if it was connected at the time of disaster). However, if that screen was not the visible one, you will not see it since you can't switch to it. OTOH, if it is a problem with just the one process, screen's scrollback buffer may come in handy. 

Doing "consumer &" will simply background the task and go on. If the owning shell terminates, it will terminate any background tasks. You say the script works on command line, but your daemon won't survive if you log out? You want to start your daemon with something like start-stop-daemon. EDIT: actually, on reading your text again, I'm not sure if consumer is a daemon at all? If you just want to run some code on startup (e.g. housecleaning), you write a line in /etc/rc.local. If the script takes a long time to run, you may want to disown it. I.e: 

I have a Linux guest that uses an LVM volume directly as root file system (that is, there is no partition table). libvirt config looks thus: 

If you have any control over the running applications, why not have them log to syslog and have syslog output to the serial terminal? Having said that, asking screen to start all the applications in separate sessions using e.g. the at or exec commands is an interesting idea. However, there are two concerns: 

If you want a bit extra security above other fine measure suggested in these answers, you may want to consider port-knocking. Port-knocking gives good protection against attackers that scan ranges of IP numbers for servers to attack, which in my experience is by far the most common attack in real life. 

The short answer to your question would be NFS, I think. Alternatively, for a more fancy setup, you could have the two servers share a DRBD volume. However, I wonder what's wrong with HTTP as a file access protocol? Can't server B have it's own web server and serve its files for itself? Split your webapp so that the part that processes the files resides on server B? Update: I didn't read OP carefully enough. If "geographically separate locations" means long latency, both drbd and nfs may be suboptimal, depending on the exact use case. If there is more than a 50 ms latency between the two locations, I would try to use different web servers that both talk directly to the end user, if possible. 

I want to get an automated process for AMI creation going, and one piece remaining is automatically cleaning up the instance after image creation. The instance is booted with a user-data script that does the necessary setup, then kicks off image creation from self using AWS CLI. Then it shuts down. I could go with option and wait there until the image is ready, then terminate, but the docs state that "file system integrity on the created image can't be guaranteed", so I want to avoid using it. What's the best way to kill the instance from itself after image creation is completed? 

I don't specify any access/secret keys in the command line. My instance role was manually created (by me) and definitely does NOT grant any permissions on resources. 

As I understand it, an instance needs to be granted access to resources in order to do anything with CloudFormation. But when I run this on a Beanstalk web server instance: 

with associated scale up/down policies, launch configuration, IAM role, etc. 2 of for EC2 launching/terminating events. (in a simplified example) where lifecycle notifications get posted. role for the autoscaling group to post notifications to the SQS queue. 

I am trying to use AWS autoscaling lifecycle hooks in a template that encapsulates the following things: 

Is it possible to buy an intermediate certificate to use it to sign subdomain certificates? It has to be recognised by browsers and I can't use a wildcard certificate. The search turned up nothing so far. Is anyone issuing such certificates? 

What would be the best way to pass sensitive data to EC2 instance (on boot or otherwise) that only root can access? 

I want to aggregate CoreOS logs to Papertrail service, which basically provides a syslog endpoint for aggregate logging. Common advice for this setup seems to be starting a service that does something like this: 

If a directory "foo" is owned by user A and contains a directory "bar", which is owned by root, user A can simply remove it with , which is logical, because "foo" is writable by user A. But if the directory "bar" contains another root-owned file, the directory can't be removed, because files in it must be removed first, so it becomes empty. But "bar" itself is not writable, so it's not possible to remove files in it. Is there a way around it? Or, convince me otherwise why it's necessary. 

I know I can use IAM roles, but I found it's just too many moving parts, and complicates any scripts that have to use the access key/secret key (e.g. rewriting /etc/apt/* lines when it changes). Not to mention there is no way to attach roles to existing instances, which makes it even more pain. It's also not possible to simply restrict access by using VPC subnet, because S3 bucket access goes via public EC2 interface. 

I need to rsync a file tree to a specific pod in a kubernetes cluster. It seems it should be possible if only one can convince rsync that kubectl acts sort of like rsh. Something like: 

install radvd on the LAN side of the router. setup a 6to4 tunnel to pass your IPv6 traffic over to IPv6 Internet setup firewall rules for IPv6 

The second row is standard in most modern Linux distros. In the first row, I'm pretending that my localhost is our Subversion repository, so that I can then tunnel that traffic to my workplace network. The URL $URL$ will actually resolve to $URL$ but still contain the HTTP host header Host: repo.company.com. Does that sound like it addresses your need? 

I can not seem to find a method to help rsync construct the rsh command. Is there a way to do this? Or some other method by which relatively efficient file transfer can be achieved over a pipe? (I am aware of , but it can only be used onto the node?) 

I suppose what you want is to make sure that there is no "positive" response on any port apart from a short whitelist. I can see how you would prefer not to have 65000 check_tcp:s on each host :) Mind you, I'm not sure nagios is really your best bet for this. Partly, it risks being a test that is always red and also, if you are serious about it, you should not limit the check to hosts that you actually know about. This sits awkwardly with Nagios which expects a host as the basic unit of configuration. Personally, I would probably have a separate tool that mailed me when something new shows up. In its most trivial form, this would be just a script that reacted to a non-zero diff of nmap output between today and yesterday and mailed me. In more complex form, such software tend to be sorted as IDSes which are not my expertise, but Google may be able to help. 

Solution WARNING: Below solution to issue 1 is DANGEROUS. If RPM upgrade breaks you are likely to end up with a broken system. Solution to issue 2 may result in installation of broken packages! Issue 1 can be circumvented by backporting RPM 4.6 from source RPM. See $URL$ Build dependencies for soure RPM: 

I think you should be able to solve that by using svn switch to tell Subversion that you are talking to "another" repo. --relocate may be needed in this scenario, I think.