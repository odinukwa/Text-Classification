C → D shouldn't be lost but isn't in F¹ nor in F². Why is it important to say that? (from F¹) and (from F²) therefore, The result which I don't understand at all is that it is without loss of dependencies... 

Good evening! I'm working on designing my very first actual database from the following UML class diagram (in French unfortunately): 

A secondary index is defined on a page of 8 bytes. What would be the maximum number of records that can be indexed with a three levels B-tree? And with a three levels B+tree ? Here are two examples of these trees : 

I don't understand why is he saying the first and where does he take the second. My teacher uses this book he co-writted. 

I have several csv files on university courses that all seem linked by an ID, that you can find here, and I wondered how to put them on Elasticsearch. I know, thanks to this video and Logstash, how to insert one sole file csv file to Elasticsearch. But do you know how to insert several such as those in the provided link ? At the moment I started with a first file for a first csv file : . But it would be painful to write them all... The file is : 

I am learning dynamic tree-structure organizations and how to design databases. Consider a DBMS with the following characteristics : 

I've been doing a lot of reading on Data Warehouse architecture as we are thinking about reforming our current setup feeding our BI. The 'maturity' of the Data Warehouse is quite low, despite being rapidly deployed. I'm still confused over a "Data Warehouse" vs a "Data Mart." What we currently do: Go to source systems. Use an ETL process to insert "cleaned + transformed" data into data tables. We named these tables thinks like surveymonkey.FactSurvey. IE, the schemas are named after the data sources to aid in comprehensibility or 'what is this, where did it come from'. Tables are labeled as fact or dimension tables. Currently we have 13-14 of these 'schema-sources' each with say 8-9 tables in one giant database. How do we integrate data? Conformed dimensions, of course. Either dimensions are "cleaned/ conformed" via logic in the ETL scripts, or they are "cleaned" post-load in views atop the tables. In my view, this giant database is a "data warehouse" - it feeds our business intelligence OLAP cubes. What exactly is a data mart, and what is the purpose? I thought we were already following the Kimball approach -- fact and dimension tables. ETL from source to database. A series of 'views' denormalize the data to feed into the cubes. But isn't it the Kimball approach that says "well a data warehouse is simply all the data marts combined together" -- that sounds kind of like what we're doing. Dump all the transformed source data into one database. Views join table within source schema and between them (integrating business data). I guess I'm fairly lost at what defines a "data mart" in the logic here. Maybe I should post a separate question here, but then does one divide "data marts" via schema, or different databases? I'm not sure the logic of dividing a Data Warehouse into separate Data Marts -- they all feed ONE BI application we are using. This BI application imports ALL data once a day into itself. If Finance uses their Finance data 10,000x more often than Marketing queries the Marketing table, that is completely irrelevant to our database structure as they aren't directly querying/ hitting our Data Warehouse whatsoever. And what is the logic of separating Data Marts via Department or Function, generally? Is it a performance issue or a UX issue? If performance, then Department shouldn't be the divisor, IO load or usage should be ... Does anyone know what the difference is here in terms of architecture and use cases? I've read a lot online, but honestly, it seems like most authors are waxing philosophical and not talking about specific tables or brass tacks. Maybe it would be easier if the literal architecture/ db/ schema structure were explained. 

I have to create a check to know if the date in Livraison (Delivery in French) is >= Date in Commande (Order in French) 

I think that the answer in relational algebra is : ΠConstructor(Product ⋈ Laptop ⋈ Printer) Yet I don't know how to write it in SQL, would it be : 

What does the data looks like there are columns that are not below a column heading : Thus, how to import a huge csv file into Cassandra ? The idea I have at the moment is to create a csv file with python in order to fill empty spaces between with . 

Therefore would it be 203.7 number of records ? B trees For them, as far as some values are stored in the node, I have to do a division by the number of nodes. And I'm stuck there. 

Update May, 3rd Without knowing how to use a file to implement csv files to Elasticsearch, I fell back to Elastic blog and tried to do a shell script for a first file before trying to generalize the approach : importCSVFiles : 

R is stored with heap organization with data unsorted with both respect to K and A, in pages of size Dpag=1024 bytes each. I have an array giving me the following costs for heap organizations (but actually for deletion it is Cs+1). 

Therefore there is a difference, something we store in the nodes in a B tree is stored in the leaves in a B+ tree. Thus, to my mind it was (m-1)h (m being the order and h being the height) as far as each nodes contains at most (m-1) keys to another node. But this is not linked with the number of bytes. Yet I found in the book mentioned above the following table : 

This shows me that the initial query actually performs two queries, one after the other. However, both of the updated columns have no index defined. I would like to make the initial query perform faster. Do you see any optimization possible? I am afraid that, since the trigger is on UPDATE, and it actually updates the column, it may trigger multiple times. Is it possible? If yes, how do I prevent it? 

I would like to do the same thing, in the most portable way possible, using only ASCII in my query, for instance: 

A server SERVER_A has some data. A server SERVER_B defines A as a linked server, and defines some views on the data. The queries are like this: 

The table has a trigger FOR DELETE, which I consider irrelevant here, and a trigger AFTER UPDATE, which reads: 

I have no idea what application is causing these queries, and therefore these annoying errors. By looking at the query, do you have any idea? What could I do to investigate? 

There is a FK on documents.person_id pointing to persons.person_id Am I missing something? This is my @@version: Microsoft SQL Server 2016 (SP1-CU1) (KB3208177) - 13.0.4411.0 (X64) Jan 6 2017 14:24:37 Copyright (c) Microsoft Corporation Standard Edition (64-bit) on Windows Server 2012 R2 Standard 6.3 (Build 9600: ) (Hypervisor) 

The remote database is outside my LAN, but firewall is configured on both sides, so I can query it by opening SSMS directly from my database server. I perform the following: 

Currently I have ETL software installed/ running on a machine separate from the destination data warehouse database (for security purposes apparently). So it must send data/ write records across a network/ web connection, unfortunately. Due to this final step (write the data to a database over the web/ network) being a bottleneck, I have used parallelization to speed things up. I have the ETL software "round-robin" the data to 15 copies of a SQL insertion step. It has worked just fine in this regard. 15 connections are opened, the data is written in 20 seconds, and this process is repeated about 1,500 times for the initial data load. Recently, we have switched our date warehouse from Machine 1 to Machine 2. Now I'm getting connection errors: 

in the example you see that bob is omitted, as max/ min will only find the two endpoints. If there was some kind of function to pull the second highest value, or 50th percentile value, on strings, that would be helpful. 

I need a way to get subjective call QA scoring into our business intelligence system. It currently lives in Excel. The problem with a direct upload from Excel to the DB is that there is no validation or response from the db necessarily (although I guess I can program these in). Is there a solution that already exists that allows a non-technical user to import data, and already have it validated against the database? There are myriad data entry software solutions out there, but as far as I know, actually receiving input from sql server (These are the valid values) is something that's not very common. MS Access is a possible solution - however, for some reason, their data entry forms do not allow for a bulk copy/ paste, which would slow down end users. You have to enter one item at a time. 

R is decomposed in & & Is it without loss of information? I would have said yes as far as with the following array 

A → B x²²= b B → C x¹³=x²³=c Therfore we should see that it is without loss of information (we should have a straight line of determinated data). However, the answer says that it clearly lacks informations and in order to show that it is with loss of information, one should find an instance r such that r ≠ r¹⋈ r²⋈ r³ I know that I have then to do some arrays and some junctions but I don't even know how to start the arrays... 

Yet I did the advised settings Apache gave in order to make the data persitent I tried to run a and it gave me back : 

I'm lost in the sea with the following example of decomposition without loss of dependencies... Let be the dependencies R is decomposed in & I want to know if such a decomposition is without loss of dependencies and without loss of information. I understand that it is without loss of information but why is it without loss of dependencies? It is said so because 

Npag(R) "because indexing is on the key" and "according to the screenshot" (which is provided above, but for me he is wrong on the cell as far as it isn't a memory operation but a equality search) Npag(R) Yeah, it depends ... 

Make the ETL software physically close enough to the destination DB so that parallelization (15 connections) is not necessary. So far I've been sending data across the US (ping 48ms) and across the Atlantic (ping 112ms). I'm not certain if getting a response time of 1ms (possible) by putting the two endpoints in the same room would make the SQL write speed much faster. I would love for it to be on the same computer, but don't have the approval. Somehow circumvent this Machine 2's response of trying to cut off my connection. I mean, sure, maybe it's some sort of spam/ DDOS defense. But honestly --- I thought it was quite common for a server to take lots and lots of connections and queries like this. 

It's only the two of us that work on the database. It's possible he wants to really segregate our 'fiefdoms'. Really, hasn't been an issue, but can't user permissions be determined at the schema level anyway? What ARE valid reasons for splitting a Data Warehouse into multiple databases? Would love to further my knowledge here about databases in general. Yes, I happen to be doing a lot of work on one with gaps in my knowledge, but well the job is what it is, what I've been thrust into. Stuff has been working great so far (knock on wood). 

As you can see, this is already a head-scratcher in terms of proposed benefits as the storage won't even be remotely evenly split, with 80% remaining on 1 database. Apparently partitioning our db by schema isn't possible (from a hardware perspective) because we do not have Enterprise level SQL Server. Reasons given for split: 

I have logged the slowest queries on my database, and one surprises me, showing many times in the list, and taking often many seconds to execute. 

I have a SQL Server 2008R2, and I want to connect remotely to a SQL Server 2012SP1, for which I am given: 

book_id is int identity PK (clustered), last_read is a datetime. The query is written with the 15 in single quotes, thus requiring a conversion, but I cannot imagine this being a big deal, because the conversion would only be done once per query. There are 6 indexes on the table, but the column last_read is not involved or included in any of them. The PK is on book_id and is nothing special. The estimated execution plan tells me: 

I have a database on Microsoft SQL Server 2016, with a table of about one million rows, let's call it books. This query takes more than one minute, which is not acceptable: select * from books where publisher_id in (857413, 857317, 857316) There is a proper FK from publisher_id to the publishers table. I display the Estimated Execution Plan, and it tells me that 100% of the cost is in the "Clustered Index Scan (clustered)" on the primary key of the books table. And what worries me, it does not recommend an index at all. By looking at the query, it seems obvious that an index will help. And in fact, when I create the index the query returns results instantly. Did something go corrupt in my database, maybe statistics? Or do you believe I should nor, in general, trust what I read in the estimated execution plan? 

I'm not sure how to accurately phrase this question. Essentially, say I have a bunch of salespeople on different sales teams. Like, the Blue Region, the Orange Region, the Red Region, whatever. Now, these sales people are each connected to one sales region, but may, on occasion, change sales regions over time. Say Bob Jones was in the Blue Region for January 2015, February 2015, March 2015, but on April 2015, he went to the Green Region. So I'm trying to create a report that would aggregate sales revenue based on Region (even though at the technical level, its logged to sales person). So we would have transaction (table: employee_sales) data as follows: 

How would I join the fact to the dimension table so I know which "company" picked the 100 apples? In this case, logically, given the data, it's 'Da_Big_Apple' ... since John began working there in March 17 until Oct 2017, in which his apple picking task took place. How do I do a join of these tables though? (assuming thousands of records). I just get stuck. I know I should do something like 

My rookie thoughts: Aren't these problems irrelevant to splitting the database? They are simply problems that need to be solved on their own either way. 

I guess I'm not really sure. I guess I can use a view to convert the type 2 to a type 4 (with an end date in the table). Then it's simpler. Make sure the fact date is greater than the start date, yet before the end date. But is that the most elegant solution?