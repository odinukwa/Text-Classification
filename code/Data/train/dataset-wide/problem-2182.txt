where and are 50% and 75% of total RAM size respectively. should lay between and for your use case.Test it and give us a feedback. There are also two other moves you should make: 

After asking on pgsql-performance list, Jeff Janes figured out that the cause was associated to the default collation used by Postgres (see this link for more informations). MacMini was using the much performing collation while Dell T420 was using the en/US collation. T420 (Postgres 9.4.1) 

I've searched through the Internet but found nothing that solves my problem (for example issuing and adding to ). returns: 

Execute this from the original database: Execute this from the destination database (the one where you have the out of memory issue): . 

It subtracts 12 months from and checks if the resulting date is greater or equal to . This query works too: 

This query will return the times a student retook an exam after more than 12 months from the first take: 

You can play with to obtain the merging concept you have in mind. Here's a link to the official documentation (PostgreSQL 9.4): UNION clause. I think that you would like to remove duplicates (if there are duplicate entries in both tables), so probably the is right for you. 

In detail ... What are some causes a table scan or an index scan/seek is used. These are also general helpers to determine index creation and use for most general situations. Follow these few steps and you will achieve big benefits right from the get-go. One reason, as it is mentioned in the comments above, is the use of SELECT * FROM WHERE . SELECT * is a sure-fire way for the engine to decide to avoid using indexes. It's faster to get all fields from the table (Clustered Index/Heap) by scanning/retrieving from the table itself, bypassing any indexes. First human choice is to minimize your select fields to those in the INDEX KEY and in the INDEX INCLUDES. The second reason is to how many records are in the table. The fewer the records, the easier it is for the engine to simply scan the table. The statistics can have a part in this. Because the engine will use the statistics to predict where the data is on the pages/disk, it can be said that there is not enough rows/distribution to use the index. And thirdly, having values in your table that are less selective (less unique), like Male and Female, the less likely the index will be used. A table that will use indexes more in queries will be for fields that are highly selective (more unique), like zip codes in an address table (so long as your list of address are NOT all of your neighbors with the same zip code). There are many techniques and strategies. But one piece of advice, become knowledgeable in reading and experience with indexes/statistics and how/where to put them on DISKS/LUNS and you'll go far as a DBA and offering huge performance gains. 

Anyone can help me to figure it out? Many thanks to all Pietro UPDATE I forgot to logout and then login after editing the bash profile. The suggestion made by Daniel Vérité was right. I just edited the env variable in in order to make it visible at a global level and not only from the interactive shell. I added the following line to : 

PART1 - INSERTING You don't have to use but and add square brackets to the JSON data structure. I sugget you using a JSON validator website like JSONlint to test the correctness of your JSON data. This codes inserts 3 new records in : 

That time is spent by pgAdmin to pack and render data and is not the time spent by Postgres to complete the query execution. Why are you fetching 250.000 rows into pgAdmin? If you need to export the table to a plain-text file (like a CSV with header) you can execute this query: 

PART2 - UPDATING In order to update you must specify values, so you have to know them a priori. This works perfectly, but probably there are other naive solutions: 

Also, the PK_ID column that you described should be the PK and clustred index for the table, otherwise its just a heap. Also, in terms of logging server logins, perhaps you'd have better luck using an Audit or extended event session? The "The definition of object ‘my_proc’ has changed since it was compiled" error that you described should be resolvable by calling sp_recompile. 

Yes, if you want to reuse an image between multiple entities and a single entity can have multiple images you'll need to make it a many-to-many relationship. And yes, you would want to create multiple associative tables for each many-to-many relationship. The only way around this would be to create one Entity table that all of the other tables pulled their IDs from, but I think what you currently have is easier to maintain going forward. 

I would stay away from LOOP joins, or specifying any specific joins. The SQL Server query optimizer is very good at picking the best join method to use, and it will pick LOOP on its own if that is the right one. Also, if there is a missing non-clustered index that could improve the performance of the query, SQL Profiler will find it and suggest it to you. One thing that I noticed missing from your query is the use of common table expressions. I haven't had a pivot yet where I haven't needed to use common table expressions to get the data ready to pivot. I would rewrite your query using common table expressions (particularly the correlated subquery) and see if that improves performance. If it doesn't another thing you could consider doing is changing the order of columns in the clustered index on the tblRespostaINT to better support that query. Finally I believe that what you are doing in the inner select can be accomplished with a windowed function, and if it can that should be faster. 

It handles models that has the same rank, displaying more than 5 rows per device type just in case. In order to test it, create a table and fill it with data: 

After setting collation on T420 "C" the A transaction went from 195 seconds to 33 seconds against 40 seconds on Mac Mini; B type transaction went from 141 seconds to 78 seconds against 101 seconds on Mac Mini. This is the best performance improvement after modifing BIOS settings. Many kernel adjustments didn't provide significant improvements. So, running the following command will initialize a new database with collation C and encoding UTF8: 

You should benefit from increasing to a resonable value. Being a test machine you can push hard on memory settings, so try the following: 

It's clear that the issue is related to . If you have control over the dump process, a possible solution is to do not execute the file from . To do so, you simply have to run the following two queries: 

You can mess around with the various parameters available in . For example, you can export a gzipped file or a CSV with headers and custom separator: $URL$ . You can also issue a to export the entire original database and run to recreate it on the new database: $URL$ . This should work.. I've just dumped an output file of several tens of GB on OS X (linux-like) without any problem. Se also for more selective database dump options: $URL$ 

I like providing direct answers to questions; however, this topic can go deeper and longer, therefore I am adding a few articles at the bottom that expand on these details for all to learn from. In summary ... A few things that can affect using an index (mind you there are a lot of reasons too as you'll see in the articles posted below). The main priority of the engine is to predict how to get the data off the disk as fast as possible in the most effective/efficient way (use of statistics). What becomes the primary choice of the engine is to perform a table scan or index seek/scan. (There can be more depth to this conversation, but I will keep it to the context of your question). 

These last two settings help to make sure that the resources for the SQL are not entirely consumed by SQL and the OS and Network and other services can respond and perform their standard requests. I've had situations where so much activity was happening (1K of simultaneous users, all indexes, stats, etc were good) that the box resources were completely consumed by SQL and the network services reported that the box could not be reached. Once I made these changes, and have been doing this ever since, many of my resource-consuming problems have gone away. You could even try these last two with Named Pipes on and it should work because the Network can respond with the high traffic. But I would recommend TCPIP instead. 

I hope to have fully understood your request. With this query you get a concatenation of tables (a UNION) with all records. 

What I get from a transaction like the one above using the previous bash command are three outputs: , and bash variable. I'll post some extract of them to make it clear what's their content. plog.log 

If I connect using pgAdminIII I get no problems. The command displays as the encoding used by pgAdminIII (the default installation gave me a encoding and that's why I run the command). The problem is that I'm not able to connect to PostgreSQL using . Whatever I pass to it, I get the following error: 

As @a_horse_with_no_name said, without a plan made on the 100.000.000 rows it's difficult to give other advices. Try the previous query and tell us if it's faster (and correct too). 

The problem is that I can't real-time access the content of variable. I can instead access the content of and , so the usage of doesn't make sense at all because it doesn't appear in both and but only in . Should I stick to parse or in order to find the output value from the function (removing the statement in that function because of it's uselessness) or there's an alternative method? Am I missing something? Thank you 

update the CPU Affinity to use all but 1 or 2 cpus (if box has 8 cpus, use 6 or 7 for SQL and leavning 1 or 2 unchecked for the OS/Network/Services to use) set the MIN and MAX RAM for the SQL Server to be "pinned" at the same RAM setting (if box has 8GB or RAM, set MIN to 6144KB and MAX to 6144KB), or something where MIN=(MAX Machine RAM * 20%) or MAX=(MAX Machine RAM * 10%) rounded to multiples of 1024KB. 

Use less fields in the SELECT that are more cohesive with the INDEX KEY and INDEX INCLUDES. More rows in the table will associate with the use of indexes The more the rows are selective (unique) the more likely the use of indexes, thus many NULL values will cause the engine to avoid the index 

What do you consider to be high-traffic? Turn off Named Pipes and turn on TCPIP It's lighter in it's communication. In my SQL installations, I never use Named Pipes except if the Application is on the same machine as the SQL service, and that's really ... never ... I always separate the application and SQL Server because monitoring and performance measures are more difficult to obtain and decipher. Separate Applications/services from SQL Server Also, if you have SQL and your application on the same server, put them on different servers from each other so that the SQL server is dedicated. Index and stats maintenance Make sure your indexes and stats are all maintained. Sometimes the activity can be high enough when stats and indexes are stale or fragmented that this in and of itself consumes most resources. Especially with standard installs and with no adjustment to CPU and RAM pinnings (see next item). Pinning CPU/RAM to settings so OS/Networks/Services are not entirely consumed by SQL