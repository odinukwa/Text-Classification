There are third party products, such as SQL Lightspeed, that provide backup compression for Standard editions of SQL Server. There is a good chance that the purchase and installation of such a product will allow you to back up locally. The size of the nacho file depend on how much data is actually stored in the database and it's compress ability. 

Assuming that your indexes are up to date, dropping from 100% cpu to something less, while having timeout problems, implies that the app is now either waiting for locks or for disk. The IO latch waits implicate the disk. I would use perfmon to look at the disks to see if there seems to be an unusually high amount of reads, look at the query plans for exact queries that are running slow and re-think what I did about the indexing. Another thought is, if I've made a change and things seem worse afterwards, the first thing I would do is undo that change. IOW, put the old indexes back. 

As a workaround, you could explicitly convert the output of the function, which should give you the correct results: 

.. with one exception: Your subquery won't work if it returns more than one record (i.e. if ID isn't unique in the tblPerson table). GROUP BY, on the other hand, is used to aggregate data. That is, to calculate sums, counts, and so on. If you want to know how many orders each buyer placed, you would use GROUP BY along with an aggregate function (SUM(), MAX(), COUNT(), etc), like this: 

Not that I know of. The clause requires an existing table. On a related note Using as opposed to or can have negative side-effects depending the situation. Some examples why I think you should create the table first: 

A solution that might work for you is using the OUTPUT clause, which spits out all the inserted rows, so you can re-insert them into a different table. However, this puts limitations on foreign key constraints on Table2, if memory serves. Anyway, the solution would look something like this: 

It seems that compatibility level can have an affect on the publishing and subscribing databases, with respect to object ownership and the existence (or not) of a particular schema. If you aren't seeing that, it's probably not a problem for you. As far as the distribution database goes, there doesn't seem to be any effect. I would leave the compatibility level of any of the SQL Server system databases (master, msdb, tempdb, the distribution database and even the model database) to whatever MS set it to unless Microsoft or a third-party vendor tells me otherwise, gives me a reason and says that I'll be unsupported if I don't change it. 

I am pretty sure that I have done this, though I didn't think twice about it at the time. You definitely can restore a bak file using a UNC path. Mapping drive letters is to be avoided because the mapping must be done in the context of the SQL service account. Most people map the drive under their own account and are frustrated when the restore command doesn't work as they expect. Naturally, there will be a little more load on the NIC on the server that holds the BAK file and there could be some contention if the database mdf and ldf files are on the same SAN, but these are probably minor problems, if you notice them at all. 

is a server-level role that allows its members to create, alter, drop, and restore databases on the instance. Intuitively - and without having seen your dacpac - I would grant the executing user in question membership in the database role, so it can view all objects, as well as , so it can create, alter and drop objects in the core schema as needed. From there on, I would monitor what types of permission errors you encounter running the dacpac. Judging from your question, I'm guessing that it wants to alter or drop/recreate the database - if that's the case, see if you can fix that when you're building the dacpac. 

Without commenting on your encryption solution per se, storing a string or password in the database can be as trivial as creating a single-row, single-column table and properly restricting the permissions on that table. Another approach would be to create an encrypted view with a hard-coded column. 

UPDATE STATISTICS does not have any sort of internal parallelism. It does not matter if you are running with either FULLSCAN or SAMPLING. Of course, you can run several UPDATE STATISTICS commands at once, each on a different connection, through multiple SQL Agent jobs or some other contrivance. Depending on your exact situation with hardware and data, you may find that simply reindexing the tables is faster than UPDATE STATISTICS with FULLSCAN and possibly a better option. 

I'm assuming that you are using a "sql server login" for the vendor's server, and you are using that when creating the linked server. When you use a linked server, a query running on your server connects to the vendor's server. If a firewall blocks your local server from connecting to the vendor's server, your connection attempt will fail. If connecting directly from your workstation to the vendor's server you go through a different firewall or no firewall, then the connection may succeed. This scenario matches the behavior you describe. My usual tests would be: 1. Can I ping from my server to the vendor's server? Both by IP and by hostname? ping usually gets through firewalls. 2. Can I connect using SSMS from my server (using an RDP session) to the vendor's server? Both by IP and by hostname? If you can ping OK but not connect with SSMS, this usually indicates that the ports for SQL Server probably need to be opened on the firewall. In short, check all of the firewalls involved. That would include any software firewall on your server or on the vendor's server, or any hardware firewall between them. In these situations, ping and traceroute are your friends. Ping and traceroute may help you locate the IP of the router that doesn't send your packets on to the vendor's server. 

This query would be functionally the same if you removed and . The point of having a nested transaction like that is that you can roll back some of the work if you want to (for instance, if you find something went wrong with your initial update). 

The way I understand your question is that you have an existing table with a column that has up until now been populated with manual values, and now you want to (1) make this column an column, and (2) make sure that the starts from the most recent value in the existing rows. First off, some test data to play with: 

I just recently solved the same issue at a client's, and I blogged about the solution. The short version of it is: I've set up a stored procedure that runs on each of the replicas, connects through a linked server to the primary replica to retrieve login SIDs and password hashes, and applies those to the secondary replicas where neccessary. The SID (the unique identifier of the account) is what connects the login (the server principal) to the user (the database principal), so if you're creating new logins on the secondary replicas, you'll need to bring your own SID with you, so the login and the user matches. The SID can be found in , the hashed passwords in . Those two tables are connected by the column. To address your security concerns: The client application itself never has permission to do this. Rather, I'm running a SQL Server Agent job that operates on a very strict set of permissions. The synchronization doesn't happen immediately, because it's scheduled, so you'll either have to set the job to run relatively frequently or provide some type of DDL trigger like @SeanGallardy suggests. Disclaimer: don't use T-SQL code from strangers on the Internet without testing it first. I'm providing it on a best-effort basis. 

I see that you found that TableDiffGui has bugs. Have you tried using TableDiff directly? You might not run into the same problems. 

The syscomments compatibility view contains a copy of the code for all stored procedures, triggers, functions and similar objects. You can easily search it using "LIKE" criteria to find a list of object IDs that need to be updated. 

In the past, I have used ErWin from Computer Associates and PowerDesigner to move schema between RDBMSes. It's been many years since then, and I would evaluate their current feature set if I had to pick something today. 

The optimizer bases much of it's decisions on the statistics for the tables. If they are not up to date then SQL can't make a good choice. Whenever you see poor behavior from queries, when you know that the number of rows involved is very small compared to the overall size of the tables, the first thing you should do is make sure that the statistics are up to date. This is quickly done. The usual method is reindexing the tables, but you can get similar results with commands that specifically update the statistics. The number of times that specifying hints has actually helped queries I've worked with has been extremely small. 

Yet, the severity of the failed test is, as mentioned, considered high. Octopus Deploy While configuring the Octopus Deploy Server, the setup fails with a FATAL error during the initialization of the OctopusServer-instance. The article related to the error-message does not explain why this is a requirement, but simply states that it will be a requirement for future deployments, from and including Octopus version 3.8. As a side-note, RedGate's CI-tool package, the DLM Automation Suite, supports deployments with varying collations without complaints. The recommendation of keeping all column collations to the database default seems more like guidelines or best practices to me. Why is it considered such a serious error by some? 

I've stared myself blind at this. It started as a more complex procedure, but I've stripped it down to the bare bones, trying to make it run. This is the current code: 

I am trying to create a log-table for storing events to a -object, and I am afraid I might be taking the wrong route. I've arrived at the conclusion that I should log different data-types in the same column, and it doesn't feel right. I'll explain the basic use-case with 2 tables; and . 

Oh, and you can the text "00:" with "" (nothing) if you want to hide the hour if it's less than 60 minutes: 

I also removed Status1 from your aggregate, because it would create a separate row for each occurrence of employee and Status1 - what you want is one row per employee only. 

.. but if you do, consider including any columns that the query may need, in order to create what's known as a covering index. 

One of the nice things about is that it allows you to access the source columns as well as the built-in and tables in the clause. My code may contain errors, as I haven't actually tested it. My blog post from a few years ago goes into a little more detail, including on query performance. 

The above VBA function will return October 1, 1971. The parameters are . Also, the following will work (thanks, @ypercube!) 

As a BI consultant, my view on datawarehousing is that it provides (primarily) non-technical users with an easily accessible set of facts and dimensions. Often, you'll see the following features in a data warehouse: 

I would also expect things to be faster on the new system, especially if you have also changed the hardware. When migrating a database from older versions of SQL Server to newer versions of SQL Server, using either the backup-and-restore, detach-and-attach or upgrade-in-place methods, Microsoft recommends reindexing all of the tables in the database. Did you do that? This minimizes fragmentation in the indexes, but it also seems to update/improve the statistics for the tables in some way. Better statistics means better query optimization and (hopefully) shortened query durations. Depending on how much data we are talking about, the "DISTINCT" and "ORDER BY" clauses may cause significant tempdb use. If your tempdb is somehow slower on the new system than it was on the old system, this could be your problem. It would be very noticeable if the other queries in the system don't use tempdb as much or at all. "About 1 minute" also sounds suspiciously similar to the default timeout for queries from ASP pages. In other words, if the query used to run in 59 seconds and now it runs in an average of 1 minute and 1 second it may "suddenly" seem like it fails all of the time even though the running time has only gotten a little bit larger. The query time out is adjustable from your ASP code (the "query timeout" setting on the server pertains to linked servers, not client calls from IIS servers), but I would only suggest lengthening the query timeout as a last resort. 

In my mind, that should give you the same result, but it's late here, so you'll have to verify the results yourself. :) Here's my query plan: 

Joining tables is a fundamental principle of relational databases. In your case, A and B are related with the column, which means that you can use a syntax similar to this one: 

Ridicule and disdain aside, the following query would return all possible permutations of your integers. 

Because we've partitioned the function on and ordered it by descending, will always return the most recent row for each , which is what we're isolating using the clause at the end of the query. If you need the time that each item has spent in a state, you could use (requires SQL Server 2012 or newer). Within the CTE, use the following expression: 

In all probability, your query takes half a second to run, but it takes a lot more time to display all the results. The latter depends on the speed of your network and that of your computer and is not really relevant from a performance tuning perspective. To answer your question, the query execution time (0.5 seconds) is the execution time.