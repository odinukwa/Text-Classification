Depending on the details, you might classify it as involving an inappropriate "appeal to authority". 

He's addressing the issues that can arise when considering infinite rewards. Instead of considering heaven to be "infinite" reward, consider a Pascal's wager where the benefit of correct belief is some large, but finite, amount of "gain", call this amount . If you can find a finite such that it is rational to believe in god on cost/benefit analysis, and that the belief remains rational for larger (but still finite) values of ; then one can argue that the belief continues to remain rational in the limit of infinite rewards. 

Liebniz himself gives an answer shortly after your quote: extension and force. This can be interpreted in more modern language (and more formally). Extension and force can be identified with the canonical coordinates (operators) of classical (quantum) mechanics. I don't understand his argument against the reality of motion, but I do know that at the practical level, all of physics deals with change (or the absence thereof), which is the study of motion (sometimes of abstract features). 

It's probably useful to note that the very appealing "definition" of Frege for 2 as "the set of all sets with 2 elements" is still a useful philosophical guideline, even though the Fregian logic itself turned out to be unsound. In particular, the theory of cardinalities starts by defining a cardinal as an equivalence class of sets, which itself may be larger than a set, and then identifying a special representative of that class, which is a particular set, usually an ordinal. The advantage of ordinals is that they are somewhat canonical, though one must fix a particular construction of ordinals to avoid similar issues. So morally 2 is the set of all sets with 2 elements, though one may choose the ordinal {0,{0}} as a canonical representative. There are consistent theories which do allow the impredicative definition, e.g. System F, where 2 is the function which takes a function and an argument and applies the function twice to that argument, or Quine's New Foundations. 

Several things might be important to note. The first is that formal logic is relatively recent, by mathematical standards: it's modern version is about a century old. On the other hand informal reasoning has been around for quite some time, and there is little doubt that we can do good mathematics with it, as others have noted. The second is that while some may reject the use of the excluded middle or even the use of the principle of explosion (also known as ex falso quodlibet), the relationship between these principles of reasoning are well explored. In particular: 

Foundations don't require truth. They do require correlation to involve communication and a foundation for belief, but this is much weaker. Systems that look to truth are very common throughout philosophy and have to face a very fundamental issue: where does foundational truth come from? There are many attempts at answers to this. Most forms of idealism dating from Plato on have had to find a way to relate ideal truth (as a metaphysical concept) with epistemic truth and what we can communicate about. Kant had his synthetic a priori as his point for the growth of truth into the epistemic world. Heidegger posited related transcendental relations of truth in being. Popper has tried to turn the relationship around by focusing on falsifiability and hypothesis, but still confronts the fundamental issue of where the meaning of concepts is meant to come from in this approach. It's my belief these are all missing a much more fundamental relationship, and by attempting to look at bivalent ideas of truth as foundational are completely led askew. When you look at all the examples that Kant tried to show were consequences of synthetic a priori truth (things like logic, geometry, even at one point stating the inverse square law) - all have been invalidated by modern science (we live in a universe who's logic is not classical, it is an orthomodular logic based on the projection of Hilbert operators and noncommutative, our universe does not obey classical geometry, and the inverse square law is only a good approximation...). Similarly, it is well known in Computer Science that you don't build a semantics from a negative logic. What you can do, though, is something that has been discussed by people like Quine, Putnam, Wittgenstein, and a number of phenomenologists: you can build meaning from correlation. This is now something that has rigorous mathematical foundations today, which actually refute a number of the initial philosophical objections to this approach. When you point to a scene with a rabbit and utter "rabbit" and another person is able to pattern-recognize a similar object, that correlation is able to begin a process of training meaning into words. If there is any correlation in shared experience, repeated utterance is sufficient to begin to build shared meaning. This is not sufficient to build absolute truth at any point, but that's not necessary when the correlation makes likelihood high enough for information transfer. Information theory codifies this in the concept of channel communications and symbol error. Effectively, repeat usage of symbols trains the relationship between syntax and semantics to a certain likelihood (never certainty). The fact that language appears to exist is a good indication that shared experience is likely happening fairly frequently. In this way, you don't need foundational truth to build a theory of epistemic values. Epistemic values come from likely semantics of our experience, interpreted possibly in various terms like "experienced" or "exists" indicating internal states or their mapping to metaphysical ontologies. 

Classical logic is consistent if and only if intuitionistic logic is as well, as shown by the Gödel-Gentzen translation The principle of explosion is consistent if and only if minimal logic (which removes this principle) is as well. See this paper for a nice survey. 

Note that Reinhardt cardinals do not provably exist in ZFC. If they were, it would contradict Gödel's theorem. It is the largest cardinal currently defined which is believed to be consistent with ZFC. As Mozibur notes, you can't have a largest such cardinal, since given a consistent extension of ZFC, you can always (in theory) find a stronger theory which proves the existence of larger cardinals. However, it may be found next week that Reinhard cardinals are actually not consistent with ZFC. That's the tragedy of the incompleteness theorem. It turns out, however, that if ZFC is consistent, there is a smallest cardinal which is not provably such in ZFC. To show this, you can simply consider the set of all uniquely defined syntactic objects which ZFC proves to be cardinals, and take the smallest cardinal not in that set. Edit: I missed the fact that Reinhardt cardinals are inconsistent with Choice. You can replace ZFC by ZF everywhere in my comment though, or Reinhardt cardinals by some smaller cardinal numbers (superhuge for instance). 

There certainly has been a large amount of development of modal logic interpretations of Quantum Mechanics. Once you have a Kripkean accessibility relationship (induced from nonorthogonality), the development is simple. Early work here is Goldblatt - "Semantic analysis of orthologic" Journal of Philosophical Logic (1974) and "The Stone space of an ortholattice" Bulletin of the London Mathematical Society (1975), and Dalla Chiara - "Quantum logic and physical modalities" Journal of Philosophical Logic (1977) and "Physical implications in a Kripkean semantical approach to physical theories" Logic in the 20th Century (1983). The basic idea to obtain the accessibility relations uses work from Foulis and Randall on lexicographic orthogonality, as it's not so simple to avoid what would become "extended probabilities" in a counterfactual view of separated events to build the modal relationship. It is important to make some distinctions with the work described on the page you linked. That research programme is linked with modality in the view of possibilities as a means of regaining realist foundations. That's an involved program that delves into operationalist interpretations and can indeed be used to build modal operators as one is familiar with in modal logic. In fact, there are S4 interpretations on the surface of most operationalist approaches. However, that is not quite the same things as the modal operators of the quantum events themselves, based on the standard orthomodular logical foundation. My first paragraph deals with the latter program. 

The combination of 1 and 2 shows that there is no fundamental disagreement between these various formulations of logic, just about what the statements in various logics express. But this is more a philosophical question, and mathematicians are not necessarily interested in these. Note that there are deeper disagreements about which principles are acceptable in mathematics, more or less related to the question of finitism. In this case, there is no hope for a clear resolution as above, since consistency of non-finitary systems can not be shown to be equivalent to that of finitary systems. This last point is essentially the failure of Hilbert's Program for building a consensus about the consistency and completeness of mathematical foundations. However, despite the prima facie failure of this program, reverse mathematics and proof theory can be seen as a rigorous way to explore the different viewpoints and find a compromise. My conclusion is that, while these are very interesting philosophical questions that are still being explored, there is no fundamental disagreement about what constitutes a sound mathematical argument.