where one and only one of course_id and sub_combination_id is not null. A Course may have one of the combinations as a prerequisite (so changing the COMBINATIONS table): 

A record would be inserted into CATEGORIES_L for each supported language; indicating this was the translation to use; it has a compound primary key of category_id and language_id: 

Your question is less about whether to use many-to-many relationships and more about the choice between a surrogate or natural key. You do need three tables - Employees, Languages and Employee_Languages - but you could choose to use the Language Name as a natural key instead of Language ID as a surrogate key. The choice of whether to use a natural or foreign key is a pragamatic one made of the basis of familiarity, irreducibility, stability and simplicity. Your question indicates that use of a natural key may be the best for your circumstances. You should analyze all the uses of Languages within your application and decide, given the criteria above, whether the use of a natural or surrogate key would be the best for you. 

I am trying to understand why is my query (update statement) causing clustered index update in the plan? Based on my clustered index (in the where clause), I am just updating columns in my table. What is the need for SQL Server to do clustered index update? I am not updating clustered key for SQL Server to reorganize/order the table according to clustered key, instead it has to update the columns or NC index on them wherein clustered key pointer to the NC rows will be the same (since I am not updating the clustered key). Can someone explain me why such behavior from SQL Server? 

I am looking for a robust open source tool for monitoring SQL server. Ours is a small organization and we don't have any monitoring tool in place and would like to know if any open source tool is available so that we can start monitoring lower environments to start with. 

Being the naive guy that I am, I let my DB get full (10GB is the max for SQL Server Express), and have now decided to drop columns to reclaim space. I'm dropping 10 columns from a ~50M row table to save space, but even after dropping them, I keep getting "Error 1011: Could not allocate a new page for database ...". I cannot shrink the database, I cannot reindex the database, I cannot rebuild indexes, I can't add a filegroup, or anything like that. I have 97% fragmentation (even after defragging my hard drive), and I feel like I'm running out of options. Even attempting to run an gives the same error message. What are my options now? Should I just migrate to a different DB (MYSQL, PostgreSQL)? 

I'm going to run a bunch of simulations using an application I wrote in the near future, and I'm having trouble designing my database around this. Here's how it works. Each simulation has 6 parameters that can vary, and each parameter has ~10 options. So we have about 1 million permutations of this to somehow store in the DB. The simplest way I can think is to have a column for each parameter, with an additional column for my primary key (ExperimentID). I could then access the parameters for a given experiment directly by doing a simple 

Issue : I was moving a non clustered index on a 3TB table from one file-group to another (userfilegroup to index) and it took 11 hours and never completed and i had to kill the process as it was blocking other processes. Rollback is taking forever. It has been 10 hours since i killed the process and no clue what is happening. Locks are still held and are impacting other processes. I see the percent_complete as 0% and kill with statusonly also shows 0% after so many hours. Can you please suggest me if there is a way that i can check if rollback is still doing anything or just got stuck . I am afraid to restart the server as it has to go through the rollback process anyways. Only positive thing that see is under sp_who2 diskIO moving for this SPID. ****Any help is much appreciated....**** 

You may also need to combine some of these combinations (so changing the COMBINATION_COURSES table): 

CATEGORIES_L should contain every value in the Cartesian Product of CATEGORIES_B and LANGUAGES. As the translations became available the records in the CATEGORIES_T and CATEGORIES_L tables would be amended as appropriate: 

LANGUAGES contains all the languages that you wish to support in your application, it has a primary key of language_id: 

It appears you are looking for a SQL database that implements the SQL Standard CREATE ASSERTION statement. AFAIK there is no mainstream SQL database that actually supports this. Therefore, the more complex data integrity rules that you wish to implement must be done so programmatically. This can be done using triggers in the DBMS or procedurally either using stored procedures in the database or within the external application. However it is implemented, careful consideration needs to be made to ensure the ACID properties are followed to ensure the reliability of the database transactions. In particular this may mean explicit locking mechanisms are employed wherever the constraints are validated. There is generally also the requirement to ensure that the constraints are enforced as efficiently as possible. For an in-depth examination of the implementation of complex integrity constraints using DBMS triggers I would suggest reading Applied Mathematics for Database Professionals by Lex de Haan and Toon Koppelaars. 

We are just entering Azure world and will be migrating SQL server soon.I am trying to test throughput on Azure VM (DS13_V2) which is supposed to give 384MB/Sec as per documentation. Here is the configuration : a) No of cores : 8 b) Attached 2 disks (1TB each formatted with 64K block size) to the VM which are P30 disks(200mb/sec). c) I have striped 2 disks into one and called it F: drive with total 2TB which in theory is capable of giving 400mb/sec. Test: I ran SQLIO (with Test file size - 20GB on the striped drive F:) with multiple combinations and looks like throughput is throttled at 256mb/sec consistently. I even ran diskspd which also gives me the same 256mb/sec throughput. Question: I am wondering where the bottleneck is or anything that i am missing as VM is capable of giving 384mb/sec and my striped disks are capable of giving 400mb/sec (200mb/sec each) wherein i should be seeing a throughput of 384mb/sec. Please let me know if any suggestions would help me in doing a better storage test. 

CATEGORIES_B contains the non-translatable attributes of each category, it has a primary key of category_id: 

Course 1 - Artificial Intelligence has a prerequisite of the Combination 12. Combination 12 is Course 2 - Data Structures AND Combination 11. Combination 11 is Course 3 - Introduction to Logic OR Course 4 - Mathematical Logic and Proofs. Hence, the prerequisite of Course 1 - Artificial Intelligence is Course 2 - Data Structures AND (Course 3 - Introduction to Logic OR Course 4 - Mathematical Logic and Proofs). This structure should allow you to store any prerequisite that is the any logical combination of any number of courses. 

A record would be inserted into CATEGORIES_T for each available translation, it has a compound primary key of category_id and translation_language_id: 

I have used this method when I did not have every translation available at the time the category information was to be inserted. It also indicated, when multiple but not all translations were available, which translation should be used for each language. 

Each parameter will be stored in the DB as an int, which will be interpreted by my application as the necessary parameters at runtime. A more complex, but I think faster, way to do it is to take advantage of the speed of binary shifting. Instead of storing the parameters directly, I'd store the information about each parameter in 4 bits of an unsigned int. I don't actually know how ints are stored (binary wise) in the DB, so that's the major hold up. Any advice on what to do in this situation? Also, I don't yet have these columns, so if I decided to go with the first option, how would I fill a table with all of the permutations of a set of parameters? 

Currently, I've only got one backup file, IMTDB.bak, and it's on the same HDD as the Database itself. I want to increase the redundancy of this DB backup by essentially "copying" it to another disk, but I get an error "Backup failed for Server, The media is formatted to support 2 media families". I think this means that when I created the backup, it's only meant to put the backup on one drive, and I can't retroactively add more. I want to migrate the backup to this other drive (really just copy it over), but I don't want to have to delete the current backup to do it. What do I do? Am I safe in just copying IMTDB.bak to a folder on the other drive? 

We have migrated our SQL server 2012 infrastructure from on-premise to Azure cloud recently where in all the production VM's are build with pre-images of SQL 2012 enterprise edition. So,here is the question and issue that we are currently facing : We want to upgrade SQL server from 2012 to 2016 and wanted to know if there is a way to perform in-place upgrade with pre-images installed without having to actually migrate ? . What we heard from our infrastructure team is that we can't do an in-place up-gradation if it comes as a pre-image and will need to spin of another VM and perform migration which would incur cost again. Any suggestions or workarounds without having to put migration efforts are much appreciated and thank you for all the help that you render to folks like me and to SQL community.