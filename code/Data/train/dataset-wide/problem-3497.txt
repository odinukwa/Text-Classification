means that a single request does run faster if it is the only one being executed . If there are multiple requests competing for resources, a single request will take longer to complete. But if you take into consideration the fact that you are performing 10 such requests at the same time, in this 7.303 ms you deal with 10 requests instead of 1. So as a measure of delay for a single request - the 7.303 ms is more useful. But as a measure of performance - 0.730 ms is more meaningful. In fact as 0.730 ms < 3.912 ms you see that you will be able to serve more requests per second on aggregate if you allow for 10 concurrent requests. 

First off, there's no such a thing as to a static/dynamic page. It's done on a host level. So unless you're using different servers for hosting static content and dynamic pages, won't be really helpful in that case. As to the number of spawned processes - this is entirely configurable and should be balanced between resources you have and the load you expect to have. The thing is you want as many processes as you can handle (to serve multiple concurrent users without waiting in queue), but at the same time don't want to deplete your memory completely. If your server starts swapping the performance will degrade severely. You can try diagnosing this situation a bit more by checking your current memory usage ( or ) and verifying if you're not hitting some limit here. That said, I don't know what your statistic is, is this request time for the main page only? It may not be brilliant, but it's not that bad either. Try comparing this with some static pages and other PHP applications to see where the problem lies. WordPress without caching may not be the fastest under the sun, but you can try optimizing things later on if you know what's causing the problem. 

is using ICMP, normally generates either UDP or ICMP traffic (Unix/Windows difference). Hence, technically you should be able to easily discern diagnostic traffic from application generated one based on protocol used (assuming it's using TCP). Therefore, it is possible that the routing policy is set to apply only to TCP traffic (as an example). So the statement in your question may be valid. But it would be a specific decision of the administrator. There should be no problem with applying the policy to all traffic, no matter the type, based on IP ranges. So if they wanted, the diagnostic traffic would use the same policy. In fact, I think it would be more reasonable to do so in the first place. That said, the rep may have some merit, so you shouldn't dismiss him too early. But I would tread carefully, as the other problems you're mentioning (policy triggered when it shouldn't be) mean that probably there are some configuration issues. So it's equally probable that he may indeed not know what he's talking about. 

Runlevel 6 is for reboot, so that's not what you're interested in. And K in K09sshd is for kill. ;-) Try using the solution mentioned by odk and see what happens. 

Apart from a unique prompt (which seems to be the most reliable solution), if you're logging in from the same workstation, you can use different profiles for your SSH sessions. For instance, I have red backgrounds for production systems, green for development, blue for infrastructure (routers etc.) and white for local workstation. If you use GNOME, there's an easy way to fire up a SSH connection with your desired profile: 

You can try adding logging (SSH traffic) to your firewall rules and restart the machine. This way you should be able to verify if your packets reach the destination (hence sshd is not working). As to this part: 

Physical drives cannot be seen when listing, but can be selected anyway and investigated further. All the actual volumes show up as they should: 

This time instead of 3.912 seconds we need only 0.730 seconds to get the job done. We've performed 1000 requests in 0.730 seconds, so one request would take on average 0.730 seconds / 1000 = 0.730 ms (last line). But the situation is a bit different, as we are now performing 10 requests concurrently. So in fact our number here is not reflecting the real time it takes for one request to complete. 0.730 ms * 10 (number of concurrent requests) = 7.303 ms. That's the time it takes on average for a single request to complete if it were executed non-concurrently (or more correctly, in an isolated manner at the current concurrency level). The last number you see (0.730 ms) is used to tell approximately how much the total time would increase if you've added 1 request () using current concurrency level (well at least theoretically it is so). The 7.303 ms gives you an overview of how long a single isolated request would run. The change you see between example and : 

If you feel confident, you can try VirtualBox with raw disk support. This should leave your system on a separate partition/drive intact. However, I'm not sure how your system would react to switching back and forth between virtualized and normal boot. Windows could probably freak out here. ;-) 

Depending on your setup it can happen that those settings are not permissive enough. You may need to grant write access (77*) to the group (HTTP server process) too, at least to selected files/directories. See Changing File Permissions on WordPress.org for more information. 

is indeed the most viable solution that I can think of at the moment if you want to keep those sites physically separated. Basically its a textbook example for 's flag: 

I'm not sure I understand your question correctly, but I would try to mimic the actual production environment you're targeting. Depending on your project, add required dependencies for your Rails app to your current stack. As to RVM, yes. I think it's worth to add it to your setup now. It may seem as an unnecessary burden at first, but in the long run you will save yourself a lot of hassle, especially if you begin to host other Rails apps later on. 

and work fine in an mdraid array. and are a part of a Windows mirrored volume. is a Windows recovery partition. When booting into Windows, I can normally use the system and access the mirrored system volume. However, the goes crazy: 

If you're fine with limiting the access to SFTP only (upload/download files), you can have a look at a chroot SFTP solution. Something like Chroot SFTP on Ubuntu should get you started in the right direction. The idea is to lock the user into a chroot consisting solely of the website files. This way he only has access to those specific files. If you need some more functionality (executing commands on the server, full console access), you can have a look at rssh (limited to rsync/cvs) or some other solutions involving jailing/chrooting. But this can become a maintenance burden. In such a case, I think you should ask yourself if a normal secure system with standard permission management features won't be sufficient for your needs. 

seems to be about your config being improperly formatted. Are you sure you didn't miss some slash/spelling mistake in the actual config? The provided snippet looks OK though, so I'm not sure why you would have something like this. By the way, what are you using for your virtual hosts? 

After reading through the comments, I think the most straightforward solution is to rewrite the root page to the actual file (, I suppose?).