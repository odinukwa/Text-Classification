If you are creating comma delimited files, you should probably use double quote and comma delimited. Excel can read the files as is, and you are less likely to have the data in the database mess up the format. I have also used the tilde ~ as a delimiter. It doesn't get used very often so it tends to be safe to use. 

Generally we use a different spfile for every database. Even though the spfile is binary, you can use the strings command to extract out the settings in the spfile. and you can also compare spfiles from one database to another to look for differences. It seems to me that the underlying issue is a change management issue. You should have a configuration that works and make every database use the same configuration. 

In addition to someone doing full table locks, there are two other things that I can think of. If you have a table where there are several columns that have a parent child relationship to the same column in a child table and if the child column is not indexed, then indexing it could help. I had a case with an online content management system that had unindexed foreign keys that caused cascading locks. I was able to resolve that by using the Tom Kyte query to find the missing indexes and add them in. 

It seems like the obvious answer is that you can't stop a shutdown that is happening, but you can do either a shutdown immediate or shutdown abort from a different session if you did a shutdown normal. As long as you have enough online redo log groups and each group has a log that is large enough, then you can recover from either a shutdown immediate or shutdown abort. If you ask someone what is the exact risk that someone faces for doing a shutdown abort, they may struggle to answer you. On shutdown immediate transactions are rolled back as sessions are killed. On shutdown abort, all transactions are rolled back when the database is started up. Hence the same thing should be happening in both cases, just in a different order. Once you start a shutdown normal, either ask everyone to log out, or just do shutdown immediate in a different session. Either way, Oracle won't allow a database to open, if it isn't in a consistent state. 

Since it is a database you can create a table with key and value. You can create a package that has a CONSTANT which points to a key. You can create a function that takes a key and returns a value from your key/value table. Then you would just look up and use that values from the key/value table and rather than changing code you change data. 

Try changing the tables parameter to this. You only need parentheses when you have more than one table. 

You can read the files with a hex editor, but you can't restore Oracle data files backed up from a Linux server to a Windows server. The endian is different. Depending on how your database is setup you can use data pump with transportable table spaces to migrate an Oracle database from Linux to Windows. Transporting Tablespaces Between Databases 

Personally I would schule it using the Windows Scheduler. You might want to get robocopy if you are copying your backup files to another server. Having an email client that you can send emails from DOS can be helpful. But otherwise you would just write .BAT files and or a Powershell file. You want a process that creates a backup log and checks that log for errors. Then you can send out emails when there are errors. There should be something already written online somewhere. 

That could be caused by unindexed foreign keys. Tom Kyte has a query that you can use to find them. It could be as simple as adding a few indexes on the child tables. Unindexed Foreign Keys 

Access is not very good as a multiuser database. You might want to move the tables into a mySQL database, for example and then link the tables to the database using ODBC. SQL Server or Oracle would work as well. It allows you to use the forms, reports, etc without having the data in Access. 

You should also point the db_create_file_dest to the default directory for creating datafiles. As long as you have restore the controlfile, then you can just do the following: 

If you have advanced partitioning and can partition by date range and sub partition by charge, then you can reduce the amount of logical IO required to get to your data, provided that you are doing date range based searches that mostly use charge to filter. If you don't have advanced partitioning, you can still partition it your self, but will have a higher level of complexity to replicate what Oracle will sell you. As with anything look at the trade offs and see what make sense. 75 million rows is a lot, but not unheard of and Oracle should certainly be able to manage that amount of data. Ideally you want to avoid full table scans of 75 million records given the high cost in terms of logical IO. You also want to partition so that you don't have global indexes if at all possible. 

I worked with an insurance company that used active directory on all of their Redhat Linux servers so that their DBA's could log in with their own credentials. The DBA's would then SUDO to Oracle which did not have a password. Hence, the upper management could track when DBA's logged into each server. Using AD can work well in Linux the manage authentication. It also makes for fewer passwords that the DBA's need to remember. 

I supported 8.0.5 databases at the beginning of my career in the late 90's. I need to point out the transportable table spaces and data pump were added in 9i. You could go through 8i to get to 12c, but it would be better to do it in one shot. Think if this problem as if you are migrating from a non-Oracle database to Oracle. You should be able to use heterogenous gateway to ODBC to connect from the new database to the old database. Database Gateway for ODBC User's Guide Now you can read data directly from 8.0.5 to 12c. Next get the DDL for creating all of your tables and create a script that will create the users and all of the tables. You can probably run the migration of data in parallel by creating several jobs that can run using the 12c scheduler. Each job would simply do insert into the target table without any constraints. If you have any tables that use long, long raw or raw you need to write an external program to migrate those using: c, pro*c, Perl, etc. Once you can migrate the data you need to generate the ddl for enabling the constraints. Finally if you have stored procedures or triggers in the database you should rewrite them. This seems like an iterative process that will probably run many times over months trying to get the conversion correct and complete, and have it run in the allotted 12 hours. You can do this, but it will take some effort. The good news is that while the conversion will take time, you should not need to lose data or take an exceptionally long down time. So people can keep using the application during that time. 

Why do you think that this table is a candidate for an IOT? IMHO, IOT's are meant to be narrow tables that don't change often. I have done testing with narrow tables and have found IOT's to do inserts at a slower rate than a traditional table with indexes. You may want to start by recreating this as a traditional table and then think about what indexes you want on the table. Also there is a note in this article on IOT's. Performance Tuning Guide 

I generally use PL/SQL collections for this. Essentially you create a table type, then create a variable with that table type, then do a select for update to get all of the data that you want to update, then do a forall update to update the data. Since you are updating the a table the record type should be that table. Such as: 

Have you tried creating a view in PostgreSQL and then query the view from Oracle? You need PostgreSQL to present the function like a view since Oracle doesn't really have that function. 

IMHO, A better index would be shown below. You want an index to start general then become specific. If you could create an index for certain values, then why would you need that column in the index because it would already be 'Y'. But if you put it as the first columns all of the rows with isdefault='Y' are clustered together, therefore you already have a smaller section of index to look at to get the rows you want. I put partner id before client id assuming that there are fewer partners than clients. Try this and see if it works. 

If you have more than one backup of your control file, try using the oldest backup. Essentially the message is telling you that the control file is newer than the rest of the backup. for example if you backup the control file, then backup the data base, then backup the control file. Doing a restore using the second backup of the control file will result in the error that you are getting. Doing a restore using the older or first backup of the control file should work. 

I use set events 10046, for the explain plan and 10053 for the waits. Once you run the queries that you want timings for, assuming that you are using 10g/11g/12c go to the diag trace folder, find your trace files and run tkprof against the trace files. 

If someone wants to learn how to manage Oracle databases, they can download and install the software and use every feature. It is only if someone wants to use the software in a commercial sense that they need to pay for the features that they use. Oracle lets you use all the features that you want, regardless of license. But if you use features that you aren't paying for, and you are audited, then you will certainly pay for the features that you use. There is a query available online that will show you what features you are using. For example, if you are running standard edition, don't query the performance views otherwise that you have used them will be tracked in your database. Also don't use advanced compression, since that is an extra feature as well. It pays to know what you are allowed to do.