That was fixed. Upgrade to the current version, v5.3 via the Github repository or our site. (Disclaimer: I'm Brent Ozar.) 

There's several parts to this question: Do I need to set the database to single user mode to change the compatibility level? No. Will changing the compat mode from 90 to 110 break log shipping? No. Can I change a log shipped database to single user mode? Yes, but don't do that unless you have to. Sometimes, you might not end up being the single user (like if someone else grabs the connection from you, like if your SSMS crashes after you set it to single user mode.) 

This is sometimes due to an MSDB.dbo.backupsethistory table that's never been purged. SQL Server inserts a row into that table, then goes back and updates it later during the course of doing backups. If your MSDB is on really slow storage, and you can't cache it in memory, and it's got a lot of history, then accessing that table can be your biggest bottleneck. I wrote a blog post called Brent's Backup Bottleneck: MSDB about a really bad occurrence of it. To check that, run sp_Blitz (disclaimer: free open source script I wrote) and one of the warnings is MSDB History Not Purged, which warns if you've got more than 60 days of history stored. To fix it, add a History Cleanup Task to your maintenance plans. However, if you're in this spot - where updating this table is taking forever - then clearing out history is probably going to take forever, too. You may have to run sp_purge_jobhistory manually yourself, nibbling off a little history at a time, to keep downtime low. 

In 30 seconds, your queries spent 1,655 seconds waiting on storage. Your storage is probably slow - if you skip down to the next section in sp_BlitzFirst's output, it will show which data and log files SQL Server was waiting on, and for how long. However, before you leap to playing Pin The Blame On The SAN Administrator, consider: 

No, there is no publicly known way to do this for the SQL Server Meltdown/Spectre patches. Microsoft might have a trace flag to do it, but if so, they haven't shared it publicly as of yet. 

The transaction log file (LDF) is made up of lots of virtual log files (VLFs) inside. Think of it like a cabinet with several pull-out drawers. You could choose a large cabinet, or a small one - but it's still going to be a fixed size with just different numbers of drawers. As SQL Server works, it puts your transactions into drawers (VLFs.) It starts at one end of your cabinet, fills up the first drawer, then when space runs out in that drawer, it moves on to the next drawer. When you back up the transaction log, what you're really doing is: 

SQL Server Express is free, but doesn't include SQL Server Agent. Agent is the job scheduling piece of SQL Server. To fire off regularly scheduled jobs, you'll either need to use Windows Task Scheduler or a fully licensed SQL Server (Standard, Enterprise, Datacenter Edition, etc). Those have Agent built in, and you can set up jobs on those servers to back up your Express servers. 

This is one of the reasons that sp_BlitzCache (disclaimer: I'm a very small author on that open source script) shows warnings for trivial plans - you don't get all of the cool stuff you get on fully optimized plans, like missing index recommendations. I'm certainly not saying that adding "WHERE 1 = (SELECT 1)" is a good idea in your queries - just saying that when building repro scenarios, you gotta watch out for trivial queries. To know if your query is trivially simple, view the execution plan, right-click on the root node (like the SELECT), click Properties, and look at the Optimization Level. If it says Trivial, you're not getting the full genius of those folks at Microsoft. 

Rather than picking random metrics and trying to tune them, what you want to do is step back and ask SQL Server, "What have you been waiting on?" My favorite way to do that is with the open source tool sp_Blitz (because, uh, I wrote it.) You can download it from FirstResponderKit.org, run the sp_Blitz.sql script to install it in the database of your choice. When you run it with just plain old: 

No triggers on system DMVs, sorry. Besides, you wouldn't really want to do this: that DMV is updated every time a query is run with a missing index request. That could be hundreds or thousands of times per second - and performance matters there. Instead, consider running an Agent job every 5-15 minutes to capture the data you're looking for. 

Because SQL Server 2012 isn't out yet, and we're dealing with beta code, this is probably best handled as a support request at Connect.Microsoft.com. Include all of your logs, and they may be able to diagnose it. If you do want to pursue it here, include the full build number that you're using, and expect to hear people answer with, "You should try again with the latest release candidate." 

Sadly, sys.dm_hadr_availability_replica states is not a reliable indicator of replica health. Here's the Connect item on one of the bugs we've run into where that DMV stops refreshing - note in the comments that log_send_queue_size in the DMV sys.dm_hadr_database_replica_states shows 0 even when there's log data to be sent. Note that the Connect item is marked as Won't Fix. Sad trombone. 

As of today, you can't. There are things inside the engine like advanced read-ahead that you simply can't turn off. There was a Connect request for it, and Microsoft turned it down. 

Microsoft's Bob Dorr wrote that trace flag 8048 is no longer required for SQL Server 2016 and newer: 

As you noted, your 2008R2 one is going single-threaded, whereas the 2014 one is going multi-threaded (thus finishing faster, but maxing out CPU while it runs.) To find the right balance for your stats jobs, think about: 

As one of the guys writes demo DMV queries that way, I'll explain why. Does it matter if you're only querying DMVs? No. But sooner or later, you're going to take one of your DMV scripts and tack on a join to sys.databases or sys.tables or some other system object in order to get more information about what you're looking at. If you don't have read uncommitted on there, you can be blocked by other queries, and block other queries. I've been burned by that repeatedly, so I just use READ UNCOMMITTED by default whenever I'm doing any diagnostic work whatsoever. 

That would be the kind of event I'd pay a little deeper attention to. If you Google for text involved in that error, you get KB articles like 2615182, suggesting that it's a Windows backup failing, not a SQL Server one. 

You basically asked, "What should I do when Page Life Expectancy changes?" My answer: nothing. I don't start by looking at Page Life Expectancy. That metric made sense in the SQL Server 7/2000 days when it was all that we had, but today, in 2018, we can do better. Start by looking at wait stats - that tells you what SQL Server is waiting on. I don't care whether PLE is 300 or 3,000 - tell me what you're WAITING on, SQL Server, and then I'll go troubleshoot that metric. My personal favorite way to check waits is to use the open source sp_BlitzFirst (disclaimer: I wrote it.) By default, it takes a 5-second sample of your server's metrics and gives you a few guesses as to why it's slow right now. Because you like writing long questions, you'll probably also like these: 

No. During a failover, clients will notice. Your connection to the SQL Server will be broken, and you'll have to retry your transaction. 

That update just hangs - he's blocked. Don't cancel him - leave him running. He's blocked by the Window #1 transaction. In RCSI, writers still block each other when they're trying to work on the same rows. Switch back over to Window #1, and run: 

That means if you have a nonclustered index on CustomerID, some CustomerID values might produce a plan with a nonclustered index seek followed by a key lookup, whereas other parameters will do a clustered index scan across the Widgets table. There are a few ways you could fix this scenario, and I'm going to list them in a generally safest-to-most-risky way: Use OPTION (RECOMPILE) on the query. This does require a code change to add the line to the query, but then every execution of this query should get the most appropriate plan. The risk is higher CPU use for plan execution (although that generally won't matter in a single-table, single-predicate query like this where the plan will be ultra simple to generate). Cache every variety of the plan. You noted passing the query in as a string will get each parameter to cache its own individual plan. While that will work today, it does bloat the plan cache (taking up more of SQL Server's memory). The risk here is that someone will turn on Forced Parameterization, a database-level option that will parameterize all of your queries whether they're sent in as strings or not, and suddenly you're back to troubleshooting this issue again. The rest of these are valid solutions, but not for your single-table, single-predicate query. I'm only listing them here for posterity and clarity: Use the OPTIMIZE FOR UNKNOWN query hint, or as we like to call it, optimize for mediocre. Requires a query change, and gives you a generally-good-enough plan. This will avoid random changes of the query plan due to parameter sniffing, but the risk is that it still won't be the most performant plan. Use the OPTIMIZE FOR query hint with a specific CustomerID. This also requires a code change, and you would optimize the query for one of your larger customers. This will get a query plan that's great for big customers, and not-so-good for small customers. Small customer performance will go down, but the big customers won't cripple the system. The risk is that your customer distribution will change, and this will no longer be the right plan for the app as a whole. Use a query plan guide. You can get exactly the query plan you want, and then pin the plan guide to memory. Here's the Books Online section on plan guides. I'm not usually a big fan of this because if your indexes change, the query plan won't take advantage of the new indexes. If your query changes, the plan guide will no longer be in effect. Suddenly the system might perform terribly, and people will have forgotten that a plan guide was helping before. Use a stored procedure with manual logic. Have branches that call different stored procedures, one for large customers and one for small customers. This is only used for much larger, more complex queries that can have variations between minutes and hours (or not completing at all). 

I'm reaching here, but I can think of at least one dangerous scenario: if you restore a database that has a filetable, those files are now on your network by default (and specifically, on your SQL Server). You could restore a virus. That by itself won't do anything, of course - the virus doesn't suddenly become sentient - but if your users then try to access the file, they could be infected. (Hey, I said I was reaching.) I'm envisioning a scenario where an outside hacker wants to get malware in the door, and he then sends an email to Bob in accounting saying, "Here's the file: \sqlserver\filetableshare\myvirus.exe" - at that point it's gone past your firewalls without detection, and we're now down to your internal antivirus and anti-malware tools. 

Express Edition doesn't include database mirroring, but you can do something similar with log shipping. Log shipping is a technique, not a feature - it's just a matter of frequently taking backups on your primary server, and then restoring them on the secondary. The normal way of doing this requires SQL Server Agent to run the backup & restore jobs - but again, that's not something Express Edition includes. You'll need some kind of job scheduler. Kathrine Villyard has a post on doing this with PowerShell. When it comes to failing over, the basic concepts are the same as failing over Standard Edition's log shipping. They'll be manual steps on Express Edition - but they're also manual on the big brother, too, so don't feel bad. 

That'll export the results in Markdown, a text format that you can copy/paste into your StackOverflow question. 

Getting your server's wait stats since startup with sp_BlitzFirst @SinceStartup = 1 Getting server health data in Markdown format so you can copy/paste it into your Stack question with sp_Blitz @OutputType = 'markdown', @CheckServerInfo = 1 

The first result set is a set of deadlock details, and the second is analysis that looks at which apps, users, and tables are most commonly involved in your deadlocks. It also gives you parameters to use for sp_BlitzCache to analyze their query plans, and sp_BlitzIndex to analyze over-indexing and under-indexing issues on the tables that may be contributing. 

User acceptance testing (UAT) usually involves clicking around in an application, which usually means making changes and testing them. This is especially true if you think you're going to "replicate those issues/bugs to our development environment" - it sounds like you're thinking about synchronizing in both directions. That requires a writable database, and unfortunately, that rules out transaction log shipping, database mirroring, and Always On Availability Groups. All of those scenarios are designed for read-only secondary databases. If both sides are going to be changing the data, then in theory, you could use SQL Server's replication techniques (merge, peer-to-peer, bidirectional transactional replication) to push data in both directions. In practice, your developers will be making table schema changes to the development database, and that's likely going to break replication. 

From a general perspective (not platform-specific), here's what I'd recommend mastering for data warehouse projects: Know how to load data fast. BI projects usually involve nightly loads of large amounts of data. The ETL guys need to shove data in fast with a minimum of concurrency issues. This means knowing when to disable indexes, when to perform tasks in temporary staging databases rather than the production environment, and how to offload processing to the ETL server instead of the database server. Know how to handle large table scans. BI environments usually have multiple terabytes of data, beyond what can fit in memory. Index tuning can only take you so far. You need to know how to get as much throughput out of the SAN as possible. Know how to segment archive data. BI environments usually have a small percentage of live, changing data and a much larger percentage of read-only (or read-biased) archive data. You have to know how to recognize those patterns and how to separate that data out into different tables or different storage targets with a minimum of work required by your ETL people. Know how to handle maintenance tasks. Defragmenting or rebuilding indexes is easy on a 100GB database, but not so easy on a 1TB or larger database. Maintenance windows have to be carefully planned. Backups are another story altogether. Know when to design reporting tables. If your users constantly access the same aggregate data (like grouping sales data by month or by salesperson, or constantly recalculate a profit percentage) then you need to recognize those trends in the end user queries, design a pre-calculated set of reporting tables, and train the users to access the data that way. 

That rule of 5 and 5 can be violated - the less write activity you have, and the faster your hardware is, and the better you tune your queries, the more you may be able to get away with more indexes. On the flip side, if you have crappy hardware and crappy queries, you might need to drop those numbers lower. The 5 and 5 rule stems from the fact that I've gotta start people somewhere, and I have 5 fingers on one hand, and 5 fingers on the other - so the rule is easy to communicate. 

Let's start with the basics: Should you store 16 trillion raw data points per year in a relational database? Probably not. Think about what you're going to do with it afterwards, and what those queries will look like. Relational databases are great for data that has relationships to other tables and needs to be inserted/updated/deleted frequently, or read by common reporting tools, but it's not quite as good of a fit here. If you don't do it in an RDBMS, where do you do it? Meet the time series database, something designed for this exact purpose. Examples include Graphite and InfluxDB. If you had to do it, what should you look out for? Any kind of maintenance will be extremely challenging - index rebuilds, stats updates, backups, CHECKDB, etc. Instead, consider a dynamic sharding design where data is split into smaller volumes. In the data warehousing realm, for example, large tables are typically split by date range (2017Q3, 2017Q2, 2017Q1, etc) so that if you need to add new columns for new incoming data, you only have to modify the current table. For reporting purposes, you can union all the tables together into a view - but just be warned that any kind of ad-hoc queries on a trillion-row table can prove challenging. (Heaven help you if someone wants an order-by and there's no supporting index - goodbye, tempdb drive space.)