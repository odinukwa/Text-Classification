Uh oh! Well, that's OK that it won't let me move it. I can just get it back into the schema by dropping it and re-creating, right?... 

will return only . How does it work? Well, I just took a straightforward approach to assigning order to the returned list by using . In this example, I've used a CTE to add the row after the fact, but in your case you could add it into your query like so: 

I immediately see a few things that confuse me here, which you should look into. Unnecessary Join In your query you have a portion 

Free... I'm not 100% sure that there's a good, free, open source alternative, if that's what you're looking for. The closest you are likely to find is Orange, a data mining and visualization tool. For a little bit of instruction, just do a little Googling, or consult helpful Youtube videos such as the one linked. or Paid? If you're willing to pay for it, I would highly recommend using Tableau. They have a free version, called Tableau Public, which only works with flat files, but will allow you to do some basic testing. The licensed version allows native connections to many databases, including Postgres. Lastly, if you are a larger enterprise client who has a business model which permits cloud storage, Chartio may also be a good alternative. Full disclosure: I have no business connections to any of the companies mentioned, and my post is meant to be purely informative and non-promotional for the companies listed. 

As a fairly newly minted DBA under the gun, I have run the gamut of free tools and done some experimentation in the paid space (DPA, SQL Sentry, and Foglight) and it really depends on what you want the tool for. In my experience the most important thing was not just communicating performance baselines (management vastly didn't care unless there was someone to yell at), but produce something in an easy to consume format that made the priorities clear and was able to track down performance issues in production. You can absolutely build up your skills by going the free route, and the tools for SQL Server are great. 

This means we could count all of Michael Jordan's games by sport(as he played multiple) with a query something like: 

Yes, SQL Server can report how long it took to do any of those actions (though you may have to run it to get some additional details such as actual row counts returned) Statistics Time 

yields s in the fields if the was only present in table . See this SQL Fiddle for confirmation of the error. However, applying my query with will yield the desired results. Use this SQL Fiddle, to confirm. 

Sort Merge Join So, the planner has decided that a sort merge join is the best plan for joining the data, so some form of sort is necessary to complete that action. However, I understand your point, that is that the and fields are already sorted due to indexing. In this case, however, the planner is using a sequential scan on both the and tables to access all the entries, and as such will treat them as unsorted since they are being read sequentially of of the table heap. I'm not 100% sure this will fix your problem, but try giving the query planner a hint at what you're trying to do by modifying your portion of the : 

Simple answer, you grouped by your sum. Solution is simply to remove that from your group by statement, eg: $URL$ 

For an example of windowing functions and subqueries in T-SQL(if you assume g1 is your table and not some subquery I just invented): 

If you find yourself saying "nullable columns" before you have written code I generally think you need to normalize more(as always, it depends.) If you want one system to maintain, the last option seems the most relevant, but "splitting them into two tables" is not really where you would want to take it. if we want to track basic things like Players, Games, and Sport type you would simply add intersection tables between the relevant things you want to store. 

This table would only store information relevant to an exam, such as the title of the exam, the exam's attributes and categories. 

Edit:Adding a little flavor text as requested. Basically what is happening is you are choosing to aggregate one of the values (in this case the counts of the salary) so that it "rolls up", and the group by generally indicates which value you want to do the rolling up by. It makes some sense to say "I want to group by the number of employees" but you are actually trying to express "I want to return the number of employees grouped by department." 

Note that the sub-select , given no additional criteria, will extract rows in ctid order. By selecting appropriate and values for a rnage of queries, you can essentially run the query in parallel. Denormalization Don't rule out denormalization as an option as well. In this case, it can help you to much more quickly get you the results you want. Of course, denormalization will only really help if your query criteria will be fairly stable over time, so that you can effectively pre-calculate results sets. I'd say there are two decent options: (1) Use partial indexing, where a given index is designed around some set of criteria, as 

The super quick fix will be to apply to your query. DISTINCT ON Postgres allows the use of the qualifier, but allows it to be applied across a subset of your returned columns from your query. In this case, you could simply rewrite your query as 

SQL Server Setup Control /INSTALLSHAREDDIR Specifies a nondefault installation directory for 64-bit shared components. Default is %Program Files%\Microsoft SQL Server Cannot be set to %Program Files(x86)%\Microsoft SQL Server /INSTALLSHAREDWOWDIR Specifies a nondefault installation directory for 32-bit shared components. Supported only on a 64-bit system. /INSTANCEDIR Specifies a nondefault installation directory for instance-specific components. 

You are grouping by the thing you are counting, not by the department name. Change your group by to: 

To explore a bit more about the question and comment, I want to note that generally when normalizing your goal (among others) is to reduce duplication. A naive example of your table might be everything you have but adding things like the CSV list you mention, this ends with a very wide table that has multiple areas of duplication and which will require you to manage and clean your data (and is generally bad for performance.) A solution to this problem is to simply project the data into another table and provide the keys to join back to the original. Eg: 

and finally all of your measured facts, with foreign key references to the dimension tables (that is references & references ) 

Installing extensions into are, as far as I'm aware, not advised. You should use the default schema, which is also in the by default. Why? As an example, I will work with the extension which I've already created within the schema. All functions are, by default, accessible to all schemas in the database if they are located in the schema. Now, I try moving it to the schema, using 

Of course, you can see how this might be extended to the other measurements. Use case: So this won't give you a tremendous speed up for a query, perhaps only a linear increase in speed when you are querying about one measurement type. This is because when you want to look up info about speed, you need only to read rows from the table, rather than all the extra, unneeded info that would be present in a row of the table. So, you need to read huge bulks of data about one measurement type only, you could get some benefit. With your proposed case of 10 hours of data at one second intervals, you'd only be reading 36,000 rows, so you'd never really find a significant benefit from doing this. However, if you were to be looking at speed measurement data for 5,000 trips that were all around 10 hours, now you're looking at reading 180 million rows. A linear increase in speed for such a query could yield some benefit, so long as you only need to access one or two of the measurement types at a time. Arrays/HStore/ & TOAST You probably don't need to worry about this part, but I know of cases where it does matter. If you need to access HUGE amounts of time series data, and you know you need to access all of it in one huge block, you can use a structure which will make use of the TOAST Tables, which essentially stores your data in larger, compressed segments. This leads to quicker access to the data, as long as your goal is to access all of the data. One example implementation could be 

I dont think so except for the fact that there is a 5NF, which describes a design where your joins are only on the candidate keys. Many "4NF" designs meet this criteria, but not all, and it is definitely something you can change a 4NF "into" to be be more normalized. 

It looks like based on your error that the user you are trying to create a login for already has a login in that database with another name, so you are effectively "double mapping" them. If you have access to SSMS you can see what their mapping is by using the Object Explorer and navigating the following path: Security -> Logins -> Double click on the username This should pop up a GUI, you can see the "User Mapping" in the top left hand corner, this should show you to whom the user is mapped. 

With these and some additional databases/tables and jobs and time you can build out a basic monitoring system (but it isn't pretty) these are tools for DBAs; unless you are good at BI stuff you will struggle to find time to produce useful business friendly stuff from it, though the Ozar sp_blitz app is pretty dang cool. After spending around a year doing the free thing and resolving plenty of issues (but not getting much buy in) I was able to make it clear, after a major issue, that perf monitoring software was a priority, and we were going to buy it come hell or high water. After demoing the previously mentioned clients, I chose DPA because management could easily consume the results, though I definitely have client licenses for SQL Sentry Plan Explorer Pro (1000% worth the money) and really liked using the server version, it just didnt grab them the same way. I also tried getting SQLNexus working at one point but I ended up working a lot than I was interested in, it may suit your needs.