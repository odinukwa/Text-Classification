Hi all i'm building a data warehouse and noticed that my tran log is massive (in simple recovery). MDF file is 2.6 GB. LDF file is 7.8 GB!!! Why is this??? i don't want a stupid tran log that's why i've put in simple recovery! (i know the system needs tran log). so i tried to shrink the DB and get this error message. 

except the bottom 6 rows attached to the right of the query... Basically the 'B' items make up the 'A' item. Which is the purpose of this report, but all the data exists in the same tables which is what i'm really struggling with... thanks! 

So this is what i came up with. Was fairly straightforward actually. So as I stated in the post that code was in a CTE. I left that code alone, later in the CTE I added 

Simple question, not so simple answer! I have a bunch of dates... how do I remove the middle numbers (which are incorrect) so I just have Week 3, 2011/2012 etc? Thanks! 

As you can see i have 4 rows and 2 sets. Where the CREATEDDATETIME is the same I want to combine the rows and add a 'Previous Location' column based on ascending status receipt order. (Status receipt identifies previous row) How can this be done? Thanks for your help. Apologies for formatting I don't know how to format data in this... 

I am trying to create a simple user with the rights permission to access to any database and can do any actions. When I trying to execute the command I got this error: 

Now, when I run the failover.sh manually, it works, so why it doesn't run with pgpool? I run pgpool with sudo user so: 

Do I need to run pgpool with other user? Do I need to create an specific user to pgpool configuration? It works manually with sudo!!!. Do I have to change my failover script command? 

The replication is always running. But I want to separate the write and read process. So my summary: The master instance detecte the different between writes/reads operations and all read operation will be manage by Read Replica. I need this solution because aurora offer a good features to improve my RDS, but the only problem is that I need to create a balance between write and read operations: The write operation be processed in the master and it send the read operation to the Read Replica. I don't want to define this process and make selection between them in my application code as Amazon propose. 

Hi all please see attached data below. I'm trying to get a row based on various conditions. scenario 1 - get highest row if no hours exist against it that has (setup + processtime > 0). scenario 2 - if there's hours (like in this example) show next operation. (which would be 60). 

Which says is the costgroup repeated and is it standard material, if so mark substitution variance as 0. I thought you couldn't use OVER within a CASE statement? Or is it only certain window functions you can't use within CASE? 

seeing as no one has helped me with this i'll answer myself to help anyone who is looking at the same thing. It looks as if to enable CDC on an existing warehouse you need to do this on a table by table basis, recommending capturing all the ETL in package per table. Also, it is recommended to enable CDC state per table to avoid conflicts. Recommended approach would be --> enable CDC on source DB. Transfer incremental loads to staging DB or schema inside DWH. Incrementally load records to Dimensions and Facts. Every example i can find on net is for one table, which is fairly useless... be much appreciated if someone can find a CDC example for multiple tables... 

Of course, I have been copy the failover to /sbin/ directory in pgpool nodes. There is the failover.sh script code: 

So If the log says that it waiting for connection on that port give me a respond but I can't see the http admin view, any idea or tips? 

The problem above only happen when I enable the auth configuration and I need it. So, How do I create an user with admin permission for any database. I want it because I configured my mongo services to uses authentication connection. If I want to execute a dump of my data I have to use this authentication parameters. Please any help? Using mongo version 3.0.5. the service is on Amazon Linux AMI 2015.03 (HVM), SSD Volume Type - ami-1ecae776 

I want to know if there is a way to define in Mongo the frequency of backups have to be executed. There is any config file for that? An example will be excellent!! UPDATE I want to create a automated mongo configuration. With this configuration I want to define the process of backup. I want the backups process be executed based on my time frequency, for example every two minutes. So I don't know if there is a possible configuration for that. I prefer a configuration file because I want to create an automated provisioning and with commands it becomes more hard to automate. 

but this gave me 6 * 6 36 rows... I want to just bolt the right query on to the end of the left query. exactly like a union 

There's a simple answer to this. If the relationship between the tables does not exist to give you the results you want, you need to CREATE your own relationship. I used ROW_NUMBER OVER (ORDER BY PALLETID) to give me unique ID's for both datasets. THen simply join (make sure you check different joins to make sure you don't exclude rows from one side or the other) ACODEID = BCODEID. Simple! 

I resolved this issue by creating a named query in the DSV which contains the product key and productorder keys, linked these back to the two fact tables, created a new dim for production orders (so i had one for each fact table), and created a new measure group off of the named query which acts as a bridge. In dim usage I then setup regular relationships to the 2 dims for each fact table. This seems to have fixed my issue! 

I'm trying to figure out how to create a From and To column based on ID.LocationID Column. I'm hoping with a suggestion this can be done (i can figure it out) without having to create sample tables and data as that would take me all day. (fairly complicated query) I'll paste basic structure to give you an idea of what i'm doing... 

What do I need: I need the Master always keep with the write operations and it redirects the read to the Read Replica instance. 

I have been installed pgpool-II 3.3.4 from source. I have two pgpool nodes and I also used the watchdog configuration to get a pgpool services with High Availability. When I stop one, the other wake up and everything is fine. Also I have two backends, one master and one slave, then I have configured the failover command. Here is my problem, when I stop the master, pgpool doesn't execute the failover. There are my backends: 

I want to migrate my database instance from AWS RDS MySQL to Aurora, but I have a doubt about replication and how Aurora management the write/read operations. I have my application and I want to separate the write operations from the reads. I want to create a Master instance only for write operations and other instance (Read Replica) only for read operations. The problem is here, I read on AWS documentation that I need to do this separation on my Application and I think or I hope to find a way to do that and be transparent for my Application. I draw a simple schema do I have to do to get with Aurora (from AWS): What AWS says do I have to do: