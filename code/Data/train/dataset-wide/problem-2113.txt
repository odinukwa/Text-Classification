You can also use that will provide much more information if you can run it during the hours noted. Returns CPU, Memory, and I/O usage of each query. You can even have it pull the execution plan of the offending query as well. Do a basic Bing/Google search for SQL Server DMV and high CPU queries or something will likely bring up some other good scripts/blog post on researching your particular situation. Just a note on your comment regarding moving the scheduled reports, I would also look at ad-hoc reports on those that users may run on their own. Especially if you are running SSRS on the same server as the database engine. I usually take a look at the ExecutionLog table in the SSRS database (default name ReportServer). There is a wealth of information in there on execution time and rendering time of the reports. 

Database Mail profiles work a bit differently when it comes to SQL Agent jobs. In configuring SQL Agent itself through the properties window you will configure the Alert System. 

Caveat: Much of the information provided below I learned strictly from going through two Pluralsight courses by Jonathan Keyhayias. Well worth the one month expense for the plus subscription to go through his two courses. First just a few points of interest that I think will help (or at most be of interest): 

Since you do not know the schema or tables names to grant them to, no to specifically doing it. You can add your AD account to the role within the database, and this should be copied into each database created on that instance. In addition, if databases happen to get created via restore or anything other than create database statement you can use Policy Based Management rules to easily verify if your AD account is a member of that DB role and if not take an appropriate action. 

Config: Window Server 2008 R2 Enterprise SP1 / SQL Server 2008 R2 Enterprise (RTM) I have searched through Google using many different words and phrases. I know this was a common issue to occur with SQL 2005 and most mention doing a command line to re-register the mof file and a few other things. I'm getting the same issue similar to what everyone else receives: I can only see SSIS service within SSCM. 

Adventureworks database is already setup with flaws that provide good practice. There is also a BI database offered as well by MS, search for "Contoso BI demo". This database is only offered as a backup file which provides you practice for one vital task that every DBA should know: restore. There are a ton of data sets on the world wide web that offer practice on building your own "small" database. I am doing this now to practice design, scripting, and working with SSRS. 

In SSMS if you are connecting to SQL Server instance set to specific port other than 1433 you simply put ",port number" after the instance name. In the same regard if I am using and want to populate a data set with , I would do the same when specifying the Data Source portion of the connection string to pass into that object. Is there not a way to do this using class? I have tried variations with no luck like: 

No, there is no option to adjust the file size of database files during the restore. You would have to shrink the files in production before taking the backup. If this is a copy of the database on the development instance you can always tell the developers if they want it to give up space (aka start dropping the other databases on that instances they don't need). Outside of that I would suggest letting the developers tells you what tables they actually need "fresh" data from in production, then just pull that out or a sampling of it out. However, depending on the method they use to develop and test their code they could always come up with their own test data. 

The previous setting is retained during upgrade as stated above. Now I would like to point out the fact that other links provided by folks state that SQL Server 7.0 is when torn page detection was available. Which as stated in those articles is true, however it is proven many times over that Microsoft documentation should not be held as truth in all circumstances. There are many where they are wrong. So with that said how can you determine which answer is acceptable? We all provided documentation by Microsoft to support our answer. As well note that torn page detection is on the depreciation list as of SQL Server 2012, so what is the concern with how it was set on your databases to begin with. If I saw it set to anything other than CHECKSUM I immediately change it and move on to other more important task. I have no concern on how a bad configuration was put in place it is more important to correct it and then ensure those who have permissions to change it are informed of why that configuration item should not be changed to anything else. Just my $0.02 

I would rethink the business need for this before going to that level. You have to understand that the objects used and permissions required for maintenance plans could change between major version, or even service packs and cumulative updates. If there is a dire business need for this, I would likely create a dedicated account strictly for this and not grant the day-to-day user account the permissions. Then hope I was supporting Enterprise Editions only and use SQL Server Audit to track the usage of that account. Though you also have to entrust the level of permissions to the individual you are providing it to, whether that means a training lesson prior to giving the account information away or even having your HR department come up with a security policy for privileged access. If I am not comfortable giving privileged access to someone you can be assured it will be voiced (by email) to management, letting them provide the finial decision (by email). I would then save off that little email as documentation on why I provided the access (for auditing purposes). 

Anyone that has permissions to the database instance can modify that value. It could have persisted through upgrades as stated on MSDN here: 

Depending on how badly you need to get the password, there is a way to read it from memory if the application has an active connection to the instance. However it is more of a parlor trick and not advised to perform on a production server. I would suggest going the route of checking the application code to see if a configuration file may have the password stored, or the developers may have the password documented. It is best to document the password somewhere secure, whether it be on a piece of paper in a safe or an encrypted master file that has limited access. 

It has to do with the hierarchy of permissions and using role membership over granting permissions directly to each login. This SQLMag article shows a good image of the hierachy in SQL Server. [In the event the article ever goes dead I have copied the image below.] 

Adding to @Surfer513 4) Policy-Based Management policy either to enforce the simple recovery model, or at most let you know when a DB is not Although I favor setting model to simple this does not prevent the T-SQL command from being used and setting it to something else. You can use a policy to evaluate if the recovery model is not Simple and opt to have the policy change it for you. This MSSQLTip.com article is on checking for Full, but you can easily just have yours check for Simple. You can also throw in a check to see if a backup has ever occured on the database too. 

Just as caveat it is best not to disturb the physical files involved with FILESTREAM for a database. Below are a few good links on the architecture: Paul Randal White Paper - FILESTREAM Storage Paul Randal Blog Post - FILESTREAM directory structure As quoted from Paul's blog post above: 

Expressions: Subject - MessageSource - (line breaks added for readability) Now what occurred after I forced an error: 

You are pretty much out of luck unless you have a full backup to the differential backup you want to restore. Your backup chain is incomplete if you do not and with that your data is simply gone. It sounds as if you received the error for the need of a full backup, and then took a full backup and restored that, then your differential. Sorry to tell you that is not going to work either. Your only hope is that the data can be recovered manually because you are not going to be able to restore that database with out a previous full backup. I would highly suggest you adjust your backup process now so this can be prevented. You need to ensure your backup chain is complete in order to provide a proper recovery process (along with ensuring integrity). 

Well, right off I would verify with the person asking the question that they are referring to corrupted and not , there is a difference. I would also determine who was interviewing me and base the answer on that (e.g. non-DBA would not care about the details, where a DBA Manager or something could require me to provide more details). For somewhere in the middle... Corrupt Data With corrupt data entering the database it would be based on how quickly it was caught and likely how wide spread it is (isolated to a table, or effects multiple tables). If a single table was involved it could be as easily as restoring a copy of the database and replace the data in the table. If the database is large this may not be so easily, or if multiple tables are involved, require recreating the data. The focus would be in designing of the application to put in controls that prevent this from happening to begin with because of the pain it can cause, and possibly be unrecoverable. Database Corruption Database corruption would mean the database as a whole or underlying structure of the database is damaged. I would start out taking a full backup of that database in its current state before starting anything. I would start out reviewing the previous output since I would have had that information readily available for systems I manage. Reviewing that output would indicate what level of severity I am dealt. I would want to determine and ensure the underlying disk the database files reside on are in good condition and may not have been the cause of the issue. I would then start re-reading post from folks like Paul Randal on corruption recovery to ensure I fully understood the liability of my actions, before doing anything. In a last ditch effort and if the tables that are found having the problem have data that can be recreated easily, I would issue the if it was in a critical timeframe. Outside of that it is time to prepare for the restore process. I would also go into small rant that alerting and ensuring backups are good, and meet the business needs, would ensure the company is ready to easily handle this type of issue. If my backups are current and good, I would just start the restore process to another database, while I was trying to recover the corruption. This would allow me to have a "backup" plan if the recovery steps above did not work. It is all really based on the criticality of the system. 

If you want to store the configuration file and/or the file you need to "pick up" via the SSIS package on a UNC path, SQL Server is going to need permissions to that direct UNC path. This permission is based on your service account being used for SQL Server Agent. If you are in a domain environment, then it is best to have your service run as a domain account and it will allow you to easily provide access to network resources. If this is a configuration that is meant to change often and I would not be the one to do it necessarily every time, then I might chose to store the configuration in SQL Server. There are a few examples out there online that show how to do this, just one example here. 

This would capture I think 20% of the active sessions for that database at any given time. However if you want all events then you would have to suffer the overhead of that action. With it being a Microsoft product I would not be to concerned with what queries it is producing. If it is experiencing performance issues you should follow Microsoft guidelines for the product or consult Microsoft support. 

To run your script on a scheduled basis you will need to build a PowerShell workflow using the Azure Automation service. There is a good walk-through of this on Azure's documentation site here. I will also note that there is a better version of this offering coming as "Elastic Jobs". It will offer much more control and features that are closer to what SQL Agent does for on-premise. They are opening up invotes for private previous now but you can read more about this from Mark Vaillancourt here. Just remember that Azure SQL databases still have a log just like any other database in on-premise SQL Servers, so make sure you your delete query is optimized for log usage. It can ensure the delete runs efficiently and does not lock up the table for an extended period. 

You have two options, both of which will require SQL Server Management Studio to be installed on the host you are running the commands from: 

As stated on MSDN side-by-side installation of SQL 2012 with earlier version is supported. However take note of the instance name, if your SQL 2000 instance is a default instance then your SQL 2012 install will have to be a named instance. I would take note also that some issues may occur with SQL 2000 tools as your SQL Server 2012 installation may attempt to upgrade any shared components it finds. Although I don't know what of SQL 2000 would still exist in SQL 2012. Now, my opinion and not the opinion of StackExchange, or my dog...I would not suggest configuring a side-by-side install with a SQL 2000 instance. Not knowing the setup, I would highly doubt the server configuration of a SQL 2000 instance is very well suited to what Microsoft suggest for a SQL Server 2012 installation. Especially considering the fact that SQL Server 2012 is only supported on Window Server 2008 SP2 and higher. This all depends on how old the server is and such but just my opinion. 

As is the command is trying to execute on your LOCAL instance and not REMOTE. You need to have it execute through the linked server itself. You can build the query as a dynamic statement and then simply execute the query against on the linked server. So something like this should: 

Now why this has changed I have not found yet. If I ever do I will try to remember to come update this post. 

You could also review the ERRORLOG for your instance and likely find an error message regarding not being able to find the specific database file. 

For the article you linked it references the specific start up options used by the Database Engine ("sqlservr.exe"). So the "-" dash is used as stated in the article. This article points to the method used in a DOS prompt to start SQL Server in single user mode. I believe using would work the same way, but I normally use the commands noted below. So from a DOS prompt, the command to start SQL Server default instance in single user mode and only allow SQLCMD to connect would be: 

Testing It To test it through SQL Server you can just try to backup a database that does not exist like this: