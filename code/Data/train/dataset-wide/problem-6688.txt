You'll notice that the list is not all that inclusive, but my purpose is not to get a language by language prediction and I felt it was best to keep it to languages I was most familiar with (the one exception being the Asian languages). Setting aside all reports of an asteroid hitting Earth in 2025 or reports that we will reach "singularity" by 2045, I am curious about the opinions of others who are more learned with the topic of linguistics, machine learning, and/or artificial intelligence with regard to the predictions I've made. In fact, I thought about posting this on the Artificial Intelligence StackExchange, but I sensed it would be seen by more people who have discussed this question by posting it here in the Linguistics StackExchange. I'd also be interested in knowing how you come up with your predictions. Mine, as I said, were based off of Mr. von Ahn's predictions, but beyond that, I factored in the difficulty of the various languages based off of previously established models (such as the FSI's categories of language difficulty), my own personal experience with the language, and how well I've noticed tools such as Google Translate perform. It would seem to me that this process of going from human translation to machine translation will be a gradual process and quite different from the transition that took place between use of the abacus and the hand-held calculator. However, the path technology has taken and the speed with which it has spread seems to indicate that it is inevitable that one day machine translation will eclipse human translation. Sadly, however, I've never heard anyone lament, But I was really good with the abacus! What am I going to do now that the calculator is here!! I say sadly, because learning a language may be the one true talent some of us have and it takes a lot of time and energy to learn a language. Despite that (or perhaps because of it) I wonder if it makes sense to continue such a pursuit. The quality of some machine translations I've seen is so good that I wonder if continuing to learn a language is akin to continuing to master your skills with the abacus when the calculator is about to hit the market. In other words, is it now a waste of time to bother learning a foreign language because a machine will soon be able to do it so much faster and so much better? Thank you in advance for taking the time to consider and answer my question. 

According to formal sementics propositions (semantic term for "sentences", "clauses") have truth value. The truth value shows whether sentence is true or false and it is denoted as 1 or 0. What about sentences that have failed in presupposition? Do they have truth value? Ex: The king of USA lives in NY. Here, the interlocutor presupposes that the USA is ruled by the king, and s/he fails in his/her presupposition. So what be the truth value of this proposition? I think that it is impossible for proposition that has failed in presupposition to have a truth value but I am not sure. Note: here I am talking about the direct meaning of the word "king", not possible metaphoric. 

unergatives have both vP projection and VP which consists of just the verb unaccusatives have just VP projection. They do not have vP projection Transitive verbs have vP and VP which consists of V and Object 

But this doesn't seem to work for word or phonword-type variables (works for phone-type variables though). There is no error message, just 0 results, so I'm guessing just doesn't return anything. I know I can get around this by accessing the orthography attribute directly ( ...), but this won't work for non-terminals. So if I want to access the entire text under a certain non-terminal (which could dominate a number of terminals), there's no straightforward way of doing this? 

What we tell undergraduates in LING 101 is that the conventions of written language are not the object of linguistic investigation. But this might be a bit of an oversimplification (and I wonder if others here would agree). After all, corpus linguistics is a very fruitful approach to language research, on the assumption that the texts collected in corpora represent the writers' language experience, even though they may not fully represent their "competence". Now, what gets collected into corpora is usually not text messages or chat histories, presumably because they don't reflect spoken language use very well. But I think it can be useful to think of the language of these mediums as a sort of register, in which case it could be in the range of sociolinguistics. A few months ago I saw a talk by sociolinguist Sali Tagliamonte on IM language. Some googling reveals this article (pdf). I haven't read it, but if it is similar to the talk I head, it might be of interest. There might also be a newer incarnation that you could look for. References Tagliamonte, Sali & Derek Denis (2008). Linguistic ruin? LOL! Instant messaging and teen language. In: American Speech, Vol. 83, No. 1, Spring 2008. doi 10.1215/00031283-2008-001 

There are two kinds of NPs existential and definite. Sometimes NP that we would expect to be existential behave as they are actually definite. One example of such NPs are those that are kind-denoting. ex: The snake is a cold blood animal. The rose is a flower. These sentences have general meaning and we would expect them to have existential meaning and take indefinite article. However, they take definite article and have definite meaning. Could any one give examples of relative clauses that have kind-denoting meaning and are definite or is there any articles on this topic? Thank you. 

If you arecomming from the non-linguistics environment, the best book is to start with Is Fromkin, Introduction to language. It provides a general introduction to syntax, morphology, phonology, semantics, pragmatics and brain and language. It is easy to read. I really advise you to start with it. For syntax, you can read Andrew Carney's Syntax: a generative introduction. Hageman's introduction to government and binding. Redford's Minimalist syntax. I have benefited from these books a lot and i advice you to read them in this order. For phonology, you should read Oddey's Introducing phonology. Good luck 

it is easy to label the word "teaching" as a noun; "boring," on the other hand, is being used as an adjective. Or what about a sentence like this: 

The word "identifying" clearly seems to be something that could be considered a noun, but the word "asking" is clearly part of a verbal phrase. My dilemma is this: 

Do these 2600 nouns include gerunds? I've actually searched the study for any mention of how they are treated, but it does not appear to address it. My initial thought was to eliminate them, but not even halfway through the collection phase I see that they are quite prevalent. I've already made some judgment calls on certain words to exclude, but to omit gerunds simply because I'm not sure how to classify them seems like a step too far. Some of you might suggest I contact Professor Davies directly, and I may just do that, but in the meantime, I wanted to know if anybody in this community is aware of standard practices with regard to this topic. Thanks in advance for any insight or references you can provide. 

I tried to add it as a comment, but I couldn't so I post it here. I think there is a misunderstanding between free word order and scrambling languages. Free word order languages are those that do not have any order. It is a property of non-configurational languages. (Of course there is a big discussion whether these languages really non- configurational and whether they really do not have any order, but at least it looks like this). Warlperi is one of these languages. On other hand Turkish is not a free order language, its neutral order is SVO. It is a scrambling language. It may scramble objects and adjuncts to a higher position for topic, focus and etc. (there are some other scramblings as well) however, there are restriction on scrambling in Turkish. Not all possible word order combinations are grammatical in Turkish. That's why it is not free word order language, it is a scrambling language. 

The passage above is referring to a study conducted by Mark Davies, a professor of linguistics and author of numerous publications including, â€œMaking Google Books n-grams useful for a wide range of research on language change" and "A Frequency Dictionary of Spanish: Core Vocabulary for Learners, among several others. Sometime after reading this article, I decided to conduct my own study with various subsets of corpora and one of the things I'd like to add to this study is a comparison of parts of speech. It may sound simple enough, but, there's actually a number of factors to consider that aren't so easy to iron out. I'm willing to live with the fact that the survey I am in the process of conducting may not be a perfect scientific study using rigid and rigorous methods of determining how and which parts of speech should be assigned to each word across my categories of corpora. However, one aspect of my study that I am grappling with how to account for relates to gerunds. Though my earliest understanding of a gerund was its use as a noun, often times they are used in a verbal phrase. In a sentence such as 

I need to find a way to splice and recombine bits of this sound file to create words that were not originally there. This is because I just don't have enough usable words that I can just extract from the sound file, and for various reasons I can't go out and record my own. I've been staring at these sentences for a while, trying to figure out what could be spliced together to form something close to actual English words, but I'm not getting anywhere fast. Can anyone think of a way of approaching this problem that doesn't require my own imagination? Is there a searchable dictionary with phonetic transcriptions and regular expressions, or something similar that I could use? Extra points for being able to specify the resulting stress pattern. 

The NXT query language includes the query function TEXT, which should return the textual content of the variable it applies to. In theory, Switchboard should support this. The file README.SWBD-QUERIES.TXT, which comes with the NXT Switchboard Annotations download, recommends using TEXT to query the orthography: