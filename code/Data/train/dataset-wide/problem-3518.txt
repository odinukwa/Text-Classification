The code opened a connection to the database, performed an sql query, but never closed the connection. I use PG bouncer as a connection pool which had a maximum limit of 20 connections. Since the middleware was spawning up new connections all the time, but never closing them, PGBouncer thought that all connections were in use and prevented more connections being opened. So the request was hung waiting for a connection to the database. I refactored the code, adding the following lines to close the connection and now everything runs smoothly. 

So the server isn't running out of memory. Is there anything further I can do to debug this issue? or any insight into what's happening? 

I figured this out with a little help from the guys over at Unicorn. The problem originated from a query that was being run as part of custom middleware. Which is why it didn't show up in any logs. Here's the offending code: 

Doing this on Apache level is not only impractical but introduces a huge overhead. If you need to rate limit per IP the best way is to do it within OS's firewall (IPFW in FreeBSD for example). Not only will it be more flexible in the long run, since it's running on system level the filtering is done at lighting fast speeds. In regards to actual API implementation of this, you should be handling this within your API application not Apache. Record requests to a fast medium like Memcache and have cron retrieve and store the data within database for processing. When user XYZ reached threshold, simply impose it within the handshake or next request. 

Procedure will not create temporary tables unless you tell it to create them. Perhaps it's best to split up that 'business logic' in a few different iterations. Also make sure you are using proper indexes. 

If you control the application, you have a couple of options. If it's web-based, you could launch the URL without a location to menubar using a JavaScript link like: 

Is it possible to require Multi-factor Authentication (MFA) be enabled for specific/all IAM accounts in Amazon Web Services? There are options for password requirements and it's clear how one can choose to add it to one's account, but it's not clear if there is an option to force users to have MFA. 

Given that you indicate you are copying to E:\Backups that it represents another server, it sounds like you are using mapped drives. Whatever agent you are using is probably being run in non-interactive logon mode, which precludes the use of mapped drives. I suggest you try to use UNC paths instead, ex: \\servername\backups 

Okay I have not used this for a while, but appliance we used pulled logs from our web servers via transfer and we had to tell it to parse them manually. This was back in the day so things might have changed. If you are still using that process I would advise checking the parsing scripts/permission/data of logs. Your next best bet is logging into the shell of the appliance and looking at /var/log/ for any clues. 

It's disabled by default because a lot of LAMP configurations are on shared environments that might or might not like their users loading random extensions without administrators permissions. You seem to know what you are doing, so there should be no security risk unless you will be using the same configuration on a server web server where you have untrustworthy users. 

I'm getting intermittent timeouts from unicorn workers for seemingly no reason, and I'd like some help to debug the actual problem. It's worse because it works about 10 - 20 requests then 1 will timeout, then another 10 - 20 requests and the same thing will happen again. I've created a dev environment to illustrate this particular problem, so there is NO traffic except mine. The stack is Ubuntu 14.04, Rails 3.2.21, PostgreSQL 9.3.4, Unicorn 4.8.3, Nginx 1.6.2. The Problem I'll describe in detail the time that it doesn't work. I request a url through the browser. 

Again. There's no load on the server at all. The only requests going through are my own and every 10 - 20 requests at random have this same problem. It doesn't look like unicorn is eating memory at all. I know this because I'm using and this is the result. 

After a bit of looking around, it appears that the answer is "kind of". In IAM, an administrator can configure a MFA for another IAM user. Although this may be a bit tricky if you are setting up a virtual MFA, it's possible. Then, if the user has not been granted permissions to update/remove their MFA, it is effectively required. While I have not yet determined the complete list of actions that should be denied (or simply not granted), this post seems to have the information, and I will update this answer once I have tested it. [Update] I was able to setup users as power-users (thereby not granting them access to an IAM functions, although I'm sure you could get more granular), and implement their MFA with them. Using this methodology, they will be unable to disable it. 

@Tom O'Connor this is not a bandwidth/pps type of DDoS. Sounds to me like a simple service denial. Keep alive will make it worse, you problem here is that Apache can't process requests as fast as it should and spawns a lot of workers that are unable to keep up with requests. As this grows the chances of recovery are pretty much at zero if attack continues. You can obviously increase MaxClients directive but from what you described it will just make you go down a minute or two later. I'm not sure what stack you are running but the goal for you is to simply improve Apaches response to a single request (are you running PHP? Is it connecting to MySQL? Are you not caching?) page that loads within 0.010 seconds will respond 100 times better to a service denial .vs page that looks up tons of stuff in MySQL and finishes in 2 seconds per request. If somebody makes 100 requests, your server has to work for 200 seconds but since it does it all at once that 2 seconds/request is now 40 seconds/request * 100. More requests, more load. Another way to address this is to identify top xyz connections and simply block them, but this will be a little bit more tricky and requires a bit more knowledge to properly attempt. 

Beyond space, there are at least a couple of reason why rack mount servers are desirable. First, it is easier to implement air circulation and cooling systems when you can control the airflow by closing the gaps between machines. This would not be easy to do with non-standardized case sizes. Second, some might find it helpful to be able to physically secure a group of machines via a locking rack. There's probably more, but space would be the primary. 

It sounds like you've installed SQL Express under the default instance (SQLEXPRESS). To connect via SSMS, you should use the machine name (or localhost, if it is the local machine), backslash, and the instance name as the server name. So in your case, try entering localhost\SQLEXPRESS. As for the authentication, unless you specifically enabled Mixed Mode Authentication during the installation, you only have Windows Authentication available. You likely added the account you used during the installation as an administrator, so you should be able to run SSMS as that user, and select Windows Authentication to access your sever. 

This correctly serves the maintenance page when the file is present. I've also created a which contains a message specific to when the site is under heavy load. What is the best way to serve when the site is receiving too much traffic, but also be able to serve the maintenance page with a 503 status code? 

I have rails app running on Nginx/Unicorn/Ubuntu. Nginx is setup with Unicorn as an upstream as follows: 

Everything works except one type of request. The page is trying to save HTML to the database. As such it's a large request. The request is passed to unicorn and the HTML is saved to the database, but then the page shows a 502 error. I checked the nginx logs and this shows up: 

I believe db_ddladmin is intended for that purpose. It won't give them the ability to CRUD data, so they will also need db_datawriter and db_datareader as well. That being said, I'm not 100% sure that it will preclude the user from dropping the database, so you may want to test that one. If that does not meet your needs, then you will need to define your own role. Under the Security node of the database, you want to create a new database role. You can then add a Securable for the database, which will contain many of the permissions you're looking for. 

You should use a UNC to the directory to which you want to backup. ex: Since mapped drives are only created on an interactive logon, if you schedule a backup to a mapped drive, the service account will not have access to it and fail. I can only presume this is why Microsoft excludes them from this dialog.