To answer this I have to make some guesses because this is not an area of research for me, but having a spouse from there and having spent time there, I think I could make a somewhat educated guess. Especially because it uses a market system rather than a rate setting system for generation. First, Massachusettes has the third highest population density of any state and it is concentrated mostly around Boston. Like rail, short distances are comparatively costly. Higher loads in small locations require greater infrastructure costs. Further, the variability will be harder to control because things that impact east Boston also impact west Boston, while things that impact eastern Montana do not at all impact western Montana. There is less independence for unplanned events in a small space. The covariance is high. A noreaster can be far more damaging, if it is damaging, in a small space. Repairs are costly. Boston also has an old system. While I could be wrong, I wouldn't be surprised if it was aging and depending on the regulation on carrying power, it might not be worth the investment without a regulatory structure that guarantees cost recovery. Further, Massachusetts is growing and this means plugging costly new infrastructure onto old infrastructure. As I am guessing, I don't know this, but it wouldn't surprise me that the influx is straining the infrastructure and I wouldn't be surprised if the infrastructure is past its planned life. Boston was running electric cable cars in the 1880s. The correct place to place this question would be with civil and electrical engineers. You will probably find there is a really long history of how the current state of affairs came to be in Massachusettes in general and Boston in particular. I would be surprised if Boston was not driving your costs. I have no knowledge about Connecticut's system and if I had to guess for Hawaii it is due to the intrinsic fragility of being on an island with lots of people and no grid to support local emergencies. 

A Bayesian prediction is a distribution such that: $$\Pr(\tilde{x}|\mathbf{X})=\int_{\theta\in\Theta}\Pr(\tilde{x}|\theta)\Pr(\theta|\mathbf{X})\mathrm{d}\theta.$$ If you note, there is no parameter in $\Pr(\tilde{x}|\mathbf{X}).$ This is because the prediction does not depend upon knowing the true value of the parameter. This is an entire distribution of predictions, however, a point prediction can be formed by minimizing a cost function over the density. Ignoring the coherence issue, the Frequentist prediction has quite a few problems. For starters, the distributions involved lack a sufficient point statistic. Any attempt to create a statistic would lose information. This is also true for someone minimizing a cost function over a Bayesian posterior density, but not the predictive density. The Bayesian posterior is sufficient, however. In addition, the limitation of liability truncates the distribution at -100% returns. This shifts the median away from the center of location and no Frequentist estimator exists for the mode that is admissible. As a result, Frequentist statistics overestimate returns by 2% per year and understate risk by 4% per years for the period 1925-2013. Using the log returns retains this bias. Furthermore, in log space, no covariance style construction exists in these distributions. The criticism of prior forecasts is not really relevant. If I forecast anything using a non-converging random number generator, which is what the method of least squares does with stock prices, then I should not be upset that I have bad forecasts. Predictions will get either/both the center of location and the scale parameter correct with measure zero. Any countable set of points in a continuum of points has zero measure and therefore has zero probability. Forecasts should be judged on a system of scoring. There is an entire field that studies the proper scoring of forecasts. In this sense, Thaler is correct in that if the distribution of forecasts was stochastically dominated by the raw data alone, then the forecasts are worse than chance alone. As the entire set of raw data is a sufficient statistic for a parameter, having forecasts worse implies that they come from a forecast that is not sufficient for the parameter involved. Do note that expectations cannot exist for distributions without a first moment, so rational expectations is a meaningless statement if it is over the raw distribution. They would exist for log utility, though. All risk-averse utility functions should have an expected utility. Hopefully, a paper will be forthcoming soon that derives a pricing model for European, Asian, look back and American style options that are both distribution-free and assumes the absence of the first moment. It will, I hope, also provide a set of new operators for stochastic calculus to deal with the absence of expectations. Ito methods do not give rise to an admissible statistic, except in a handful of special cases, such cash-for-stock mergers. I am hoping to have it complete soon and I am looking for comment on the mathematics of certain components as they appear to be novel innovations and I want mathematicians to kick the tires. 

One of the challenges to rational expectations is that it must be the case that the first moment exists for the variable of interest. This cannot be the case either for equity or equity-like investments, or economic growth. The assumption is that the marginal expectation, or possibly the average expectation is equal to the true value at some limit. For some equations, such as $x_{t+1}=Rx_t+\varepsilon_{t+1}, R>1$ then while the maximum likelihood estimator and the mvue is the least squares estimator, the test statistic for $\hat{R}-R$ is the Cauchy distribution which has no mean. See 

First, I do not see why this is a Bayesian construction. Indeed, if you do mean that the density is known, it cannot possibly be a Bayesian construction. Consider the case where $f(x)=280{x^3}(1-x)^4,x\in[0,1]$,. There is no unknown parameter here. This is both your prior and your posterior as no amount of data will alter anything. If this were a Bayesian construction then there would have to be some uncertain parameter, but there is no uncertain parameter. This is a beta distribution with $\alpha=5$ and $\beta=4$ The prior is forced to be $\Pr(\alpha=5;\beta=4)=1$. The question is "what is x?" The only uncertainty is in the valuation. This is a Frequentist problem. EDIT In the case where it is drawn from an unknown distribution, you are facing two options, even if the distribution is known with certainty to the actors. The first is to use Bayesian non-parametric methods, the second is to use Frequentist non-parametric methods. Depending on what I wanted to accomplish, I would choose one or the other. The Bayesian method will be coherent and so you could place gambles on it. It will also likely be very difficult to implement. There cannot be a Bayesian solution that is free of its prior. Such a thing does not exist. It might be that it is uninformative, but it must exist. The alternative is to use Fisher's failed method of fiducial statistics. The Frequentist method will minimize the maximum loss you could experience from making a choice based on the data by using an incorrect inference. It will also allow you to control for power. It will usually be far simpler to implement. Bayesian non-parametric methods are potentially infinite dimensional constructions and you would need to do a bit of reading on them. A simple approximation though would be to use the beta distribution because of its incredible flexibility, although you could use any high degree polynomial that stays above the axis since your bounding guarantees that a constant of integration exists. You would then perform model selection. As long as you believe it is unimodal, the bounding on both sides guarantees that a mean exists. Even though your distribution is unknown, it is guaranteed to have moments. The t-test is probably inappropriate because of the bounding is so tight, but you could use the empirical quantiles to test significance. If you felt you needed the higher moments, the method of moments is always available. Finally, in either case, you have kernel methods available to you. You cannot avoid a prior using Bayesian methods, but the greatest advantage of Frequentist statistics is to be able to solve problems when you cannot form a prior.