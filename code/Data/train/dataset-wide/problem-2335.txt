Otherwise, try to connect to the server local\s12 using SQL Server Management Studio using windows authentication from the same machine that you are running the powershell script. if that fails then there is a connectivity issue. Check firewalls (on the SQL Server machine and on any routers in between the client and server). Check that the TCP/IP network library is enabled on both the client and the server. If TLS is enabled then confirm that the certificates are all correct. 

Alternatively, you could query the management view and find the table's object id. Use this to generate a dynamic SQL statement. Copy the object id from the results and paste it into the dynamic SQL. 

To correct one common but important misconception: You don't "normalise a database", you normalise a Logical data model. Data design, the process of designing how your data will live at rest, has four distinct phases: Conceptual, Logical, Implementation, Physical. Normalisation is the process of refining the Logical model. Is it required? Yes, if you want your Implementation and Physical models to protect data integrity and to perform well. Tactics that protect integrity can reduce performance so sometimes the normalisation process applied to the Logical model is reversed in the Implementation and Physical models, a process usually called denormalistion. This is a tradeoff - denormalisation means accepting less data integrity protection in the name of better performance. (This also means that questions posted here of the form "is this table normalised?" are meaningless unless the poster includes the conceptual and logical models from which the table was derived.) I suspect the OPs situation is that they learned "a database system" rather than learning "data design". I often see this in people who learned Microsoft Access. They come to believe that the database (the physical model) is the whole data model. 

Foreign Key Constraints A Foreign Key constraint cannot reference a filtered unique index, though it can reference a non-filtered unique index (I think this was added in SQL Server 2005). Naming When creating constraint, specifying a constraint name is optional (for all five types of constraints). If you don't specify a name then MSSQL will generate one for you. 

To Quiesce means to pause/alter an application/device to allow the application/device to achieve a consistent state. In terms of DB2 the Quiesce command should force all the users off the specified database and should flush all the buffers to disk (been a while since I've used DB2). While the database is a Quiesced mode there are users who can still access the database such as SYSADMIN and SYSMAINT. There are several DB2 functions that can be used to determine the status of a table such db2tbst and MON_GET_TABLESPACE_QUIESCER 

to the offending SP to force just that SP to use the prior CE. The SP should now perform in Compatibility Level 120 as it did with Compatibility Level 110. <-- I prefer this approach as your Database/System gains all of the other advantages of SQL 2014 while the "fix" only impacts the offending SP. Irrespective of the approach I would then profile the execution plan/read/writes etc of the offending SP on a test system in SQL Server 2014 Compatibility Level 120 and rewrite the SP to cater for the new CE. Once tested and verified I would then apply to production. Microsoft have a good article on the Cardinality Estimator changes in SQL 2014 

I have recently seen this exact same problem with an upgrade from SQL Server 2008 R2 to SQL Server 2014 where a very small number of Stored Procedures (SPs) (approx 5 out of 3,000 ) performed worse in SQL 2014 Compatibility Level 120 than in SQL 2014 Compatibility Level 110 or in SQL 2008. In SQL 2014 the Cardinality Estimator (CE) has changed which may impact how a SP performs in SQL 2014 Compatibility Level 120. There are a few possible solutions 

From reading the YQL Documentation and viewing the data sources it appears that while the yahoo.finance.quote does provide data on all the stock data, running a query on a large data set make take some time and the data may be returned in multiple pages which means that your application will have to handle the pages. It also appears that the YQL data requires a where clause in order to run so you would need to know the symbols for all of the stocks. There is a YQL Console which allows you to access the YQL data and run queries, view the output etc. I would suggest reading the YQL documentation and running some sample queries in the YQL Console to see if the data you require is available. To access the YQL Console you can 

and raise an alarm if that max. age is over 1 Billion or so. And if it gets close to 2 Billion you are in imminent danger! You can use a tool like check_postgres.pl which will handle this check for you with some configurable thresholds. 

Not sure why Teradata has that limitation, but should be fine in PostgreSQL even when other tables have foreign keys depending on that index. PostgreSQL has fairly sophisticated tracking of such dependencies -- for example, if you tried to do you would see a complaint like: 

So canceling that , either through or a Ctrl-C issued from the controlling psql prompt, will have a similar effect as if you had done 

If you'd like a snapshot of your primary database refreshed nightly, you could do this with a cron job restoring RDS snapshots every night. I don't think RDS has a button to do this automatically for you, but it shouldn't be too hard to script up a nightly create-db-snapshot + restore-db-instance-from-db-snapshot using the AWS CLI, or boto, or whatever interface to AWS you like. You could even maintain a Route53 entry which would always point to the most-recent instance, and leave the old instances lingering for a day or so before being killed off, so that sessions running against existing instances overnight wouldn't be interrupted. 

Each of those directories with an integer as the directory name represents a database inside my PostgreSQL cluster. The integers (OIDs) in the directory name match the you would see from a query like: 

Supposedly it is possible to hook up Bucardo to RDS now that RDS Postgres supports the session replication role, but if you want a nightly snapshot I think you'll be much better off using RDS instance snapshots. 

I'd be interested to hear what privileges your existing superuser role had which were not automatically granted to whatever new superuser role you created by default. Edit: and if you're interested in copying over the per-role configuration parameters (i.e. those documented under configuration parameters), then you could use a function like this (demo only, you may need extra error handling, security considerations, etc. for production use): 

Metadata visibility means that if a database user queries sys.objects then they will only see the objects they have permissions to access or that they own. If you require the actual permissions they have on those objects then you need to query the permissions management views, as noted in Kin's comment above. 

I think they key phrase is "set". Codd's 1972 paper said "A relation is in first normal form if it has the property that none of its domains has elements which are themselves sets." (quoted from the Wikipedia article on 1NF). In your example, the set is "phone numbers on which we can call this person". Whether you store the phone numbers in a single column holding an array ("555 123 4567,555 987 6543") or in two columns (called phone1 and phone2), this is still a set - a repeating group - and forbidden by 1NF. On the other hand, if all the entities all had exactly two phone numbers and there was a functional difference between phone1 and phone2 then having two columns phone1 and phone2 would be in 1NF. 

Both the default instance and SQL Browser maintain a list of instances running and their current port numbers. If there is nothing listening on those two ports (maybe there is no default instance, maybe the default instance is using a different port, maybe SQL Browser is stopped) then the only way to connect is to manually specify a port number in the connection string using a comma. For example, PRODDB\Payroll,14550. SQL Browser sends broadcast network traffic so a lot of administrators prefer to not run it. TechNet: Default Client Connection Behavior. As an aside, port tcp1434 is used by the default instance for the Dedicated Administrator Connection. 

Short answer: yes. Slightly longer answer: yes as long as you have one developer license for every person accessing the development and staging servers. From the Licensing Quick Reference Guide: 

A commit is a user initiated action that tells the database (Oracle in this case) that the transaction is completed and that the changes may be committed and any locks/resources released. Normally the changes are committed to in memory data buffers and to the redo log buffer. A checkpoint though is a database initiated action that writes all of the data to the actual physical disk file based on the changes recorded in the redo log buffer. Some good articles can be found here and here. 

As you are looking for the Maximum value for Sequence for each Booking_ID you could write some SQL similar to that below: 

The use of Save Transaction provides you with a mechanism for rolling back portions of a transaction. For example SP A starts a transaction which then calls SP B. At the start of SP B, a Save Transaction Start Processing could be created. If an error then occurs in SP B you could just rollback the change in the SP B allowing the changes in SP A to be committed. 

Using the above example a new table, #tmpTable will be created with the data from coulmnA, columnB and columnC from tableA. The data types in #tmpTable will match the datatypes of tableA. You can of course create a true physical table and have multiple tables in your select statement. 

Allowing even trusted users to login to what appears to be a production database is in my mind a big no-no. The solutions you mention above may provide you with a sense of security/relief but they their own have side effects. I would be particularly worried about your statement 

The Connection timeout is always set by client. The timeout is normally configured when the connection is created. In order to prevent recompilation of client code the timeout interval is often stored in a config file, registry entry etc You cannot configure the timeout on the database itself.