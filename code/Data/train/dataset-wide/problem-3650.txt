Is there an easy way to find out what exactly is going on? My understanding of Apache's innards isn't that good, but I would have thought we wouldn't need this many concurrent processes to serve up a page like this, with this sort of traffic. We did inherit the app, so we don't know much about it's insides, but it's a fairly basic CMS-type site, showing a few search results, I didn't think it would need this sort of grunt. I did run ab against the site, I was getting a fairly lousy request rate (well under 50 a second), but that may have been my poor choice of settings - a lot of those requests seemed to fail. Where should I be looking for information on what's happening, or any troubleshooting tips I could try? Cheers, Victor 

I'm attempting to follow the guide on the Ubuntu wiki for installing and setting up Kerberos. I am running Ubuntu 14.04 (LTS) 64-bit. I have setup avahi-daemon in order to provide .local DNS names. I have then run: 

This just writes out the output from every second to a textfile, which is stored on the NFS server. On Bob, I do: 

There should be other volumes - e.g. "datastore/joo", or "datastore/photos" etc. Any ideas of what's going on? Could the pool version somehow have borked something? =(. The fact it reports 12 TB free is pretty worrying - but I am hoping the data is on the disks there somehow....any suggestions on getting to root cause, and/or recovering? $URL$ Update - output of : 

I'm trying to install memory into a Dell R610 server, and am honestly at wit's end. The DIMMS I have are: 

I then tried putting in the 4 x 4GB sticks all on one CPU - A1, A2, A3 and A4. With the 2 x 8GB sticks on B1, B2. However, it then complained about: 

For this you need to make another virtual host that has the different settings, and for the redirects you want under "this situation" use that vHost instead. This is not PHP code, it's Apache language. Sorry. There are no if-thens. 

Why not make all the SSL on port 443 and use vHosts to use multiple domains? You're trying to do that, but you're overlooking it. Try this: 

Just be sure to uncomment mod_proxy and its reverse from the apache modules section of the config. And change the 127.0.0.1:port to the port and local IP of the "game boxes" And for the port thing, all of these requests are served on the external IP/port of the apache server. If you are trying to serve non-HTTP requests (a real game server) then you are better off using a NAT or DNS load balancer for this, and the client should connect to the default port. (Example: connecting to a minecraft server at mc.domain.com with no port specified, assumes port 25565. Http traffic assumes port 80, ssl is 443, etc.) 

and MAKE SURE you allow indexes and the goody-two-shoes options directives. That should fix you. EDIT: You can pretty much remove the "ifmodule" tags. Try it that way too. At least explain what your problem is, it's a little bland. 

It can never be confident of others. For example: website $URL$ (test.com is lookup one, then www.test.com is #2) has 20 images, all hosted on different sites. DNS can not predict this without loading the HTML first, but thats what the client is for. The client's browser will decode the HTML and request all the sources to the images, usually all at once. What if the page changes and they update all the links of images? BL: It's never going to be able to PREDICT dns requests, but it CAN cache prevoius requests. The TTL of the record (expiration time) will let the DNS server when to purge that record from the cache. That's just about it. 

We have a number of applications that generate fairly large (500Mb a day) logfiles that we need to archive/compress on a daily basis. Currently, the log rotation/moving/compressions is done either via custom bash scripts and scheduled via Cron, or in the application's code itself. What (if any) are the advantages of using a system daemon like logadm? (These are Solaris boxes). Cheers, Victor 

I have an IoT device that communicates with a remote server via HTTPS over port 443. I would like to intercept the HTTP/HTTPS communication - e.g. using Charles or mitmproxy. If it was a desktop or Android device, I could setup the proxy's SSL certificate in the certificate store. However, this device doesn't really have any HTTP settings - so I'd need to setup a transparent proxy right? However, how do I get it to trust the SSL certificates presented by Charles/mitmproxy? Thanks, Victor 

I have a FreeNAS server setup at my parents place. It was previously running FreeNAS Coral. This had a single ZFS volume called 'datastore'. It's a RAIDZ-1 volume, comprised of 4 x Toshiba 5TB disks. For some reason, that installation seems to have borked itself. Anyhow, I re-installed the boot USB with the latest FreeNAS 11 and booted up. I then went through the wizard to import my old ZFS volume. It seemed to import the volume fine - the GUI also prompted me to update the ZFS pool version, which I did. However, I now notice that I seem to be missing several ZFS volumes - yet no errors are being reported. Output of : 

On the front indicator panel of the drive, there is a steady amber light (i.e. critical failure). I have a SATA-to-USB adapter, I know the physical adapter is the same for SAS, but I'm guessing this won't actually work for connecting to a SAS drive. From a quick search, it seems I'd need to get an enclosure and an external HBA. I'm wondering, what are the chances of any data recovery at this stage? My understanding is that in general enterprise drives are more prone to simply reporting as fatal, rather than returning possibly corrupt data. Would we be able to get any data at all from the failed drive? And if we can't, I suppose, with the striping, depending on the stripe size would you be able to get small chunks? 

You could just install it with Xen without changing the ISO, then edit the files you need after it's installed, then make a clonezilla image for future use. 

This means, wherever the virtual host's files are taken from (root directory) you should put that specific favicon.ico file. 

Perhaps there are a limited number of network threads to connect the virtuals to the host, and uploading large files takes up all of them eventually and the rest of them lose signal. I've got no other guesses. Sorry. 

Maybe this would help? $URL$ or this: $URL$ EDIT: And .htaccess files are supported by most server types. Well, any server if they are configured to look for them. 

Notice the two different document roots. Also notice that they listen for a specific domain. For the other part of making all subdomains go to x.y.com, use mod_rewrite: Pop this in the .htaccess file: 

You'd still need to maintain a list like my original answer (above) to disallow the bots that ignore robots.txt. 

You don't want to set your INDEX page to a 404 page, otherwise it will not load. Simple. Try this: If you want to have a www-redirect, make a v-host with just your external IP. 

Well port 21 is supposed to be FTP unsecure. Not sure if you're running that on a "game box." If you're using Apache as the external server and trying to map it to other servers and ports, your best bet is to use apache mod_proxy and mod reverse proxy. Here's a sample config: 

Here's the deal: Fix the C:/'s in my example and update the paths for yours. Make sure you have the "NameVirtualHost" at the top of the vHosts section. 

Are too many "accounts" trying to log in for every single time a database is accessed? Do they not carry persistent sessions? 

The dialog asks me for a realm, as well as the hostname of the server, both of which I enter. However, it then seems to hit an error: 

I've noticed that there appears to be a delay of ~ 3 seconds for Bob seeing events. The tail -f will echo a logline, then pauses for around 3 seconds, then it spits out 3 seconds worth of log lines, then another 3 seconds of nothing, then outputs another set etc. However, if on Bob I do: 

It seems to be waiting at that point for quite some time. I thought of using vnc to connect to the box to see what's going on. Even though I've used , my understanding is that the VNC display should still get created by default. However, this is the output of : 

I have a system that was recently reformatted/reinstalled. However, somebody forgot to check the backups...sigh. Backup drive is a write-off (faked flash drive capacity) Anyhow, there were two partitions on the harddisk that we're concerned about, one NTFS 30Gb, and one BTRFS 30Gb. I assume the BTRFS partition is a write-off, since there's no few recovery tools for BTRFS. With the NTFS drive, the original partition was formatted (in the Windows 7 installer), and Windows installed on top, so the first 15Gb are essentially gone. I've just made a image of the partition as it currently stands, using dcfldd. I assume that's enough to do a recovery from, as opposed to the whole drive, since I just did a reformat, and re-used the same partition? Any advice on particular tools to re-enact the recovery? Basically, I'd like to scrub it for any files at all that I can pull. Opensource tool that work under *nix are preferred, but quite frankly, right now, I'll take anything...lol. Thanks, Victor 

The site seems to go longer than before, but still dies. Checking the list of processes, I have (third column is physical mem, fourth column is virtual size): 

For virtualizing an OS in windows, VMWare's Workstation is a great tool. You can run as many machines as your computer can handle (depending on if you set each OS to use a certain amount of RAM or processor usage). It supports linux, mac, windows, and solaris all the like (Though it can be tricky installing mac, if you so choose). Storage wise, you will need: - A RAID controller (can be an internal card like so: $URL$ 

Try FileZilla Server for Windows: $URL$ That will let you create password protected users with some of these settings: $URL$ (PDF file with screenshots). It does work on allowing you to attach network drives if you first map them to a letter under the Host. (In the server-computer, map folder XYZ to letter A-Z (one that's not in use) so that accessing drive X (or whatever letter you choose) will indeed open the folder of choice. Then under filezilla, grant a user access to drive X (or which you choose). If you need more help, ask. 

Incremental backups simply add files or remove files or apply changes to pre-existing files from an original backup. If this drive gets full (assuming it's only backup) then just take it out, pop in a new one, full backup, then do incremental. 

You can't do that without vHosting. If you can wait till 4pm eastern, I'll give you the code to do so. I'm not around my server now. 

If it's a windows machine, you can run "WinAudit" (free, just search for it) on a scheduled job. It generates HTML or text reports and such, of which you can save to a network drive and view from a remote computer. OR: Remote desktop. You connect, and use it as if it were a local machine. Works in *nix too. 

Why not store one large file and have the server convert them to requested sizes on demand then store them in the cache? Consider also running several frontend servers (Through load balancer) to serve the requests, then maybe using NAS or several other servers to serve the static content. The number of frontends you need depends on how much traffic you'll get (youtube capacity or just storing the content for occasional accesses).