Are there any more solutions? Are there lots more solutions?! (Note that if you take non-integer $n$ in 2. or 3., you still get involutions but they no longer have power-series expansions as desired.) [My motivation is a fixed point equation for probability distributions which comes from considering minimax functions (of the sort that arise for example in the analysis of combinatorial games) on random trees. In particular, suppose $G=1-A$ is a pgf, suppose $M$ and $N_1, N_2, N_3, \dots$ are i.i.d. random variables with the distribution whose pgf is $G$, and suppose $Y_{ij}, i,j\in\mathbb{N}$ are i.i.d. real-valued random variables with any common distribution. Let $$ X=\max_{1\leq i\leq M}\min_{1\leq j\leq N_i} Y_{ij}. $$ The property that $A$ is an involution is equivalent to the property that for any common distribution of the $Y_{ij}$, the distribution of $X$ is the same as that of each of the $Y_{ij}$.] 

As suggested by Robin, we would need to prove that if $X$ and $Y$ are independent random variables with the same distribution, the inequality $E|X|\leq E|X-Y|$ holds. This is certainly NOT true in general; for example suppose $X$ and $Y$ have some constant non-zero value; then the RHS is 0 and the LHS is positive. However, if you add the condition that $E(X)=0$ then the result is true. (In your notation, $\int_{-\infty}^{\infty}xf(x)=0$ ). In that case write $p=P(X\geq 0)$, and write $X_+$ for the positive part of $X$ and $X_-\leq 0$ for the negative part of $X$. We have $0=EX=E|X_+|-E|X_-|$, so $E|X_+|=E|X_-|$. Hence also $E|X|=E|X_+|+E|X_-|=2E|X_+|$. Now $E|X-Y| \geq E(|X-Y|I(Y<0)I(X\geq 0)) + E(|X-Y|I(X<0)I(Y \geq 0))$ $=2 E(|X-Y|I(Y<0)I(X\geq0))$ $=2 E(|X_+|I(Y<0)I(X\geq0)) + 2E(|Y_-|I(Y<0) I(X\geq 0))$ $=2(1-p)E(|X_+|I(X\geq 0)) + 2p E(|Y_-|I(Y<0))$ $=2(1-p)E|X_+| + 2pE|Y_-|$ $=2E|X_+|$ $=E|X|$ (using at various times the facts that $X$ and $Y$ are independent and that $X$ and $Y$ have the same distribution) 

In two dimensions the probability that 0 is outside the convex hull is $N(1/2)^{N-1}$. Let the points be $x_1, x_2, \dots, x_N$. The origin is outside the convex hull of the points if there is some half-plane, with the origin in its boundary, such that all the points are on one side of the half-plane. Equivalently, for some $i$, $0<\text{angle}(x_i, x_j)<\pi$ for all $j\neq i$, where the angle between two vectors is measured at the origin. For any given $i$, this event has probability $(1/2)^{N-1}$. The events for different $i$ are disjoint, giving total probability $N(1/2)^{N-1}$. 

Related: Blumenthal's 0-1 law says that, for fixed $t$, for any event $A\in\mathcal{F}_t$, there is an event $\tilde{A}\in\mathcal{G}_t$ such that the symmetric difference of $A$ and $\tilde{A}$ has probability 0. However, this on its own is not enough. For example, let $U$ be a uniform random variable on $[0,1]$, and define a process $C_t, t\geq 0$ by $C_t=0$ for $t\leq U$ and $C_t=t-U$ for $t\geq U$. Then a similar 0-1 law holds, and $U$ itself is a stopping time for the filtration $\mathcal{F}_t=\bigcap_{\epsilon>0}\sigma(C_s, 0\leq s\leq t+\epsilon)$, but there is no stopping time $V$ for the filtration $\mathcal{G}_t=\sigma(C_s, 0\leq s\leq t)$ such that $U=V$ with probability 1. 

Consider a Galton-Watson branching process, with offspring distribution $\mathbf{p}=(p_0, p_1, \dots, p_n, \dots)$. Let $O$ be the root of the branching process. Write $\eta=P(\text{process survives for ever})$ and $\mathcal{H}=\{\mathbf{p}: \eta>0\}$. Also write $\beta=P(O \text{ is the root of an infinite binary tree contained in the branching process})$ and $\mathcal{B}=\{\mathbf{p}: \beta>0\}$. Then it's fairly well known that in some sense the phase transition from $\mathcal H^C$ to $\mathcal H$ is typically "continuous" while the phase transition from $\mathcal B^C$ to $\mathcal B$ is typically "discontinuous". e.g. suppose the offspring distribution is Poisson with mean $\lambda$ and consider $\eta$ as a function of $\lambda$. Then $\eta(\lambda)=0$ for $\lambda\leq 1$ and $\eta(\lambda)>0$ for $\lambda>1$, and $\eta(.)$ is continuous everywhere, including at 1. On the other hand, consider $\beta$ as a function of $\lambda$. Then there is a critical point $\lambda_c\approx 3.3509$ with the following property: $\beta(\lambda)=0$ for $\lambda<\lambda_c$, and $\beta(\lambda)>0$ for $\lambda\geq \lambda_c$. In particular, $\beta(.)$ is discontinuous at $\lambda_c$. (In fact, at $\lambda_c$, $\beta$ jumps from 0 to approximately 0.535). My question: how has this been written as a general statement? (rather than just for particular parametrised families as above). My guess is that one would want to write it something like the following: Let $M_0$ be the set of offspring distributions with the topology induced by the metric $d_0(\mathbf{p}, \mathbf{q})=\sum|p_n-q_n|$. Similarly $M_1$ and $M_2$ induced by $d_1(\mathbf{p}, \mathbf{q})=\sum n|p_n-q_n|$ and $d_2(\mathbf{p}, \mathbf{q})=\sum n^2|p_n-q_n|$. Write $\mathbf{p}^*$ for the degenerate distribution with $p^*_n=0$ for $n=0$ and $n\geq 2$ and $p^*_1=1$. Then: (1) $\mathcal{H}\setminus\{\mathbf{p}^*\}$ is open as a subset of $M_0$. (2) $\mathcal{B}$ is closed as a subset of $M_2$. (Probably also $M_1$?) Of course (1) is basically trivial, since $\mathcal{H}$ is just the set of distributions with mean greater than 1, along with the single point $\mathbf{p}^*$. (Note that it's NOT the case that $\mathcal{B}$ is closed as a subset of $M_0$. For example, consider a sequence of distributions indexed by $k$ with $p_0=1-4/k$ and $p_k=4/k$, all other $p_n=0$. Then $\beta(k)>0$ for all $k$, but $\beta(k)\to 0$ as $k\to\infty$, and the sequence of distributions converges in $M_0$ to a limit in which just $p_0=1$, for which of course $\beta=0$.) I don't think it's hard to write down a proof of (2) above, but I don't want to reinvent the wheel. (I don't need to use the result directly, but I would definitely like to refer to it to illustrate a point). So: anyone know where such things have been nicely developed before? 

One can sample from the uniform distribution on permutations on $n$ elements with $k$ cycles in expected time $O(n\sqrt{k})$. For large $n$ and $k$ this may be more feasible than the method Herb Wilf refers to in his answer, which, if I understand right, requires the generation of the Stirling cycle numbers $S(m,r)$ for $m\leq n$ and $r\leq k$. The idea is as follows: consider a Poisson-Dirichlet($\theta$) partition of $[n]$. The blocks of such a partition have the distribution of the cycles of a random permutation of $[n]$ from the distribution which gives weight proportional to $\theta^r$ to any permutation with $r$ cycles. In particular, conditioned on the number of cycles, the permutation is uniformly distributed. (Once one has a partition into blocks corresponding to the cycles, one can just fill in the elements of $[n]$ into the positions in the cycles uniformly at random). Choose $\theta$ in such a way that the mean number of cycles is around $k$. One can sample a PD($\theta$) partition of $[n]$ in time $O(n)$ (see below). Keep generating independent samples of the partition until you get one with exactly $k$ blocks. The variance of the number of cycles will be $O(k)$ (see below) and the probability that the number of cycles is precisely $k$ will be on the order of $1/\sqrt{k}$, so one will need to generate $O(\sqrt{k})$ such samples before happening upon one with precisely $m$ cycles. So this is really not so far from what you suggested in the question (generate random permutations until you find one that fits) with the twist that instead of generating from the uniform distribution (which corresponds to PD(1)) you choose a better value of $\theta$ and generate from PD($\theta$). Here are two nice ways to sample a PD($\theta$) partition of $[n]$: (1) "Chinese restaurant process" of Dubins and Pitman. We add elements to the partition one by one. Element 1 starts in a block on its own. Thereafter, when we add element $r+1$, suppose there are currently $m$ blocks whose sizes are $n_1, n_2, ... n_m$. Add element $r+1$ to block $i$ with probability $n_i/(r+\theta)$, for $1\leq i\leq m$, and put element $r+1$ into a new block on its own with probability $\theta/(r+\theta)$. (2) "Feller representation". Generate independent Bernoulli random variables $B_1, \dots, B_n$ with $P(B_i=1)=\theta/(i-1+\theta)$. Write a string of length $n$ divided up into blocks, with the rule that we start a new block before position $i$ whenever $B_i=1$. So for example if $n=10$ with $B_1=B_5=B_6=B_9=1$ and the other $B_i$ equal to 0, then the pattern is (a b c d)(e)(f g h)(i j). (Note that always $B_1=1$). Then assign the elements of $[n]$ to the positions in the blocks uniformly at random. The expected number of blocks is $\sum_{i=1}^n \mathbb{E}B_i$, which is $\sum_{i=1}^n \theta/(i+\theta)$, which is approximately $\theta(\log n-\log \theta)$. If this is equal to $k$ with $1 << k << n$, then the number of blocks will be approximately normal with mean $k$ and variance $O(k)$. For details of some of the things mentioned here to do with Poisson-Dirichlet partitions, random permutations etc, see e.g. Pitman's lecture notes from Saint-Flour: $URL$ 

The version of the LLL that you wrote out above is stronger than the one on page 8 of the paper you linked to. The one you link to is sometimes known as the "symmetric form" of the LLL and the one you wrote out above as the "general form". To see that the general form implies the symmetric form: restrict to the case where $|\Gamma(A)|\leq d$ for all $A$ (as assumed in the symmetric form), and let $x(A)$ be a constant $x$ such that $x(1-x)^d \geq 1/(4d)$. (Check that you can always find such an $x$). Then the condition $P(A) \leq 1/(4d)$ in the symmetric form implies the condition $P(A) \leq x(A) \prod_{B\in\Gamma(A)} (1-x(B))$ which is the hypothesis of the general form. A standard reference is Chapter 5 of the book of Alon and Spencer, "The Probabilistic Method". Of course it doesn't cover Moser's new approach! 

I don't know the answer, but I don't think that Poisson($\lambda$) is best. What's the word for one minus the total variation distance? i.e. the maximum over all couplings of the probability that two random variables agree? Let's call it the "agreement probability". The agreement probability between Poisson($1$) and Poisson($\lambda$) decays at least exponentially fast in $\lambda$ (since the probability that Poisson(1) is at least $\lambda/2$ decays faster than exponentially, and the probability that Poisson($\lambda$) is at most $\lambda/2$ decays exponentially). I think you can do better than this by a distribution that puts more weight at 0. For example, suppose $\lambda$ is an integer, and look at a distribution that puts weight $p$ at $0$, weight $p$ at $2\lambda$, and weight $1-2p$ at $\lambda$. This has mean $\lambda$ as required, and to get variance $\lambda$ we need $\lambda = (1-2p)\lambda^2 + p (2\lambda)^2 - \lambda^2$ which gives $p=1/2\lambda$. So for large $\lambda$, the agreement probability with Poisson($1$) is then at least $1/2\lambda$ (because both distributions have weight at least $1/2\lambda$ at $0$). Anyway, this is just an observation; probably one can do much better than that. 

One way to think about this sort of problem is to embed in continuous time. Take $N$ independent Poisson processes of rate 1. (Think of $N$ independent Geiger counters, each going off at rate 1, if you like). A point in the $i$th process corresponds to picking the $i$th ball. Since the processes are independent and all have the same rate, the sequence of ball selections is just a sequence of independent uniform choices, as we desire. Let $M_i(x)$ be the number of points in the $i$th Poisson process up to time $x$. Then the distribution of $M_i(x)$ is Poisson($x$). In particular, $P(M_i(x)\geq 2)= 1-(1+x)e^{-x}$. The time of the first point in such a process has exponential(1) distribution, so its probability density function is $e^{-x}$. So fix one ball, say ball 1. Consider the event that when ball 1 is first chosen, all the other $N-1$ balls have each been chosen at least twice. To get the probability of this event, integrate over the time that ball 1 is first chosen (i.e. the time of the first event in process 1): $\int_0^\infty e^{-x} P\big(M_i(x)\geq 2 \text{ for } i=2,3,\dots,N\big) dx$ $=\int_0^\infty e^{-x} \big(1-(1+x)e^{-x}\big)^{N-1} dx$. The same applies for any ball, and the events are disjoint, so an exact answer to your question is $N\int_0^\infty e^{-x} \big(1-(1+x)e^{-x}\big)^{N-1} dx$. I don't know if it's possible to get an exact expression for this integral, but it's easy enough to bound it. For any $K$ $N\int_0^\infty e^{-x} \big(1-(1+x)e^{-x}\big)^{N-1} dx$ $\leq N \int_0^K e^{-x} \big(1-e^{-x}\big)^{N-1} dx + N\int_K^\infty e^{-x} \big(1-Ke^{-x}\big)^{N-1} dx$. Take $K=\frac12 \log N$, for example; it's easy to evaluate both integrals exactly (substitute $u=e^{-x}$) and to show that they both tend to 0 as $N\to\infty$.