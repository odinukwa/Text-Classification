Unsurprisingly, the Wikipedia claim misstates the situation. OT is a theory of "derivational mechanics", and autosegmental phonology or linear phonology are theories of representation. One therefore find many analyses conducted in OT which assume autosegmental representations, and ones which assume SPE-style representations. Wiki is creating a false trichotomy. Every OT analysis is either "linear" or "autosegmental" (and actually, many analyses are mixed / non-commital). A rule-based account on the other hand would be in opposition to an OT account. As far as I know, there are no "good" papers which give an actually balanced rule-based vs. constraint-based account. You can find papers that assume one theory and make disparaging remarks about the other account. 

The question seems to be about where one would put such materials (not how one would handle the typesetting issue). There are at least three places which don't require content review: Zenodo, Figshare, and the SOAS Endangered Languages Archive (I can't say for certain that the later is appropriate, so read their material). 

The standard Aristotelian understanding of "true", of a proposition, is "describes reality" (and "false" is "does not describe reality"). "Prove" then means "show to be true", which again in the standard Aristotelian approach means to show that there is a reason to judge the proposition to be true, and there is no reason to doubt the proposition. Owing to the influence of Kant and more proximally Karl Popper, the classical understanding of the concepts "prove" and so on have been widely though not universally replaced, owing to his stipulation that a "universal statement" cannot be proven, but it can be refuted. Thus all statements are either conjectures, or false. The possibility of observational refutation is undermined by the fact that the logical connection between an ostensively refuting event and a predictive theory is itself an unproven conjecture. If a theory W predicts a reaction temperature of X under conditions Q and the observed reaction temperature is Y, then at least one of W, Q or Y is false. It is conjectured that the device thought to measure temperature, which relies on physical theory Z, has value for establishing temperature, but that is at best an unfalsified conjecture. We may now actually have observational evidence that falsifies foundational theory Z. In other words, when something goes wrong, all you know is that something has gone wrong. Returning to the Aristotelian approach, the basic methodology of proof amounts to identifying reasons, which are observations or conclusions that are proven based on observations. The practical problem has been that the inference from observation to conclusion has not been automated, so people take invalid shortcuts. Statistical methods, for example, rely on the premise that a given sample is drawn at random from the population, and that is never actually the case in linguistic research. The reason why linguists cannot agree is fundamentally that there are vastly different epistemologies in the field: this Aristotelian / Popperian split is just one example. 

There is no realistic prospect for comparing languages in terms of number of ways to express a thought, because every language has infinitely many ways to express any thought. "A thought" isn't a quantifiable units. You might, however, be able to quantify the synonym-density of a language, taking a clue from your example: how many words are there for "couch", "dog" and so on. That is, how many expressions are there that refer to more or less the same thing. Your specific example also points to the problem of defining what you're looking for. You left off an infinite set of expressions which express that same thought: "yupparoonee; roger; ok; mhm; yeah no yeah; but of course; naturally; is the Pope Catholic; that should be obvious; and that's why were here...". In other words, are we counting communicative uses, or literal meanings? Let's say we exclude questions about the Pope's religion, which are used to communicate "yes". Then we have to decide what level of officiality to insist on. Apart from the normal word for "dog", we have "hound, mutt, mongrel, canine, fido, bowser, pooch, flee-bag, shit-factory. Once you pry into the Urban Dictionary repository, you discover that English has literally zillions of words that you probably didn't know, like zab (I'm not zab, so I didn't know this). If you allow this set, then English almost certainly wins hands-down, because the world spends much of its free time populating UD with new putative English words. The competing Kamusi ya Jiji project (sorry about the 404) hasn't gotten very far off the ground. If you insist on a fairly high degree of officiality to purge zab and the like, a possible competitor would be Javanese (there are other languages that do this, so it might be that Balinese wins the count-contest). This is because there are a number of official social register variants. There is no scientific answer, because the question isn't yet made precise enough that it can be addressed, but I've indicated the main issues that need to be addressed: "same" in what sense, and level of officiality. 

Since Bantu has been mentioned, I won't mention it again, much. I'll mention Athabaskan, Ket (not Athabaskan but probably related), Semitic, Berber, Coptic, Bongo, Krongo, Nilotic, Nyulnyul, Gooniyandi, Tiwi, Lenakel, Camsá, Cayuvava, Seri, Nahuatl, Lakota. You can get more examples here. That said, if you mean "only prefixing, with no suffixing at all", then that may be hard to find. The situation in Bantu is that tense-aspect-mood-polarity and sometimes subject number are marked suffixally, and tense-aspect-mood-polarity and subject are more marked (more distinctions are made) prefixally. In Semitic, subject can be marked in part prefixally (in the imperfective), but also suffixally. Athabaskan languages are mostly prefixing, but I don't think any are completely devoid of suffixes. 

IPA is used to render the pronunciation of a word, so NATO is [ˈnɛɪtʰɔʊ], JPEG is [ˈdʒeɪpɛɡ] and so on. Some people pronounce NATO as [ˈnɛɪɾɔʊ], in which case you'd use a different transcription. IPA isn't a spelling system / general-purpose writing system, it's used to represent pronunciation, thus it doesn't matter if you write IRA or I.R.A (but it does matter if you pronounce it [ɑɪɑɹɛɪ] or [ɑɪɹə]). 

A more accurate characterization of the history is that McCarthy did not adopt Goldsmith's radical desegmentalization, so from M's POV this was maintenance of the status quo. M does not address the question, so one can only speculate. There are a few prominent reasons that I believe most explain this. First, G's idea was rather speculative and rationalist, and the details implementation of radical desegmentalization remained to be worked out, so that it wasn't practical to actually adopt the proposal right out of the box. Second, recall that M is a strong proponent of the OCP qua universal constraint, which prohibits adjacent identical segments – but not features (G on the other hand rejects the OCP). The OCP facts (especially the *ssm fact of Arabic) necessitate a robust notion of "segment". Third, M had available to him the now expired additional theoretical construct "projection" which gives the appearance of featural autonomy without denying the segment. Forth, M's concern is fundamentally about syllable structure, which is maximally remote from the kind of feature concerns that inspired G. Segment lengthening and shortening, which are at the center of his concerns, are most problematic for a radical desegmentalized theory, lacking a unified thing that you can have two of. Related to this is the fact that most of the time, a segment can have only one specification of a feature (tone are major exceptions), and without a hardcore segment, this fact is hard to encode in the theory. Finally, M introduces a competing theory that each morpheme defines a separate tier, which if crossed with featural decomposition, would make the head explode (though he does not explicitly reject feature decomposition, still his focus directed is away from feature autonomy). 

A true complete and formal grammar would only hinder your desire to learn a language, so your motivation for asking for such a thing is ill-placed. You also make a false assumption about the nature of "grammar" in linguistics, that a "grammar" is only about enumerating allowed word orders. A complete grammar must include all aspects of the computation, ranging over semantics, syntax, phonology, morphology and phonetics. Also note that no grammar can be self-defining, that is, you must have some knowledge of a theory-external metatheory of the formalization, which means you need a pairing of a grammar and a meta-grammar. You can find specific formal accounts of fragments of Greek and Latin, such as Sommerstein's book on Greek phonology, Lightfoot's work on Greek semantics and morphology, Robin Lakoff's dissertation on Latin Complementation, Redenbarger's work on Latin phonology and morphology. These works do not all use the same metatheory. If you were interested in Sanskrit, I would point you to the one extant albeit nigh impossible to use complete formal grammar of a language, the Aṣṭādhyāyī. 

Under clause (b) [beŋ] is opaque, because ŋ is derived by P (place assimilation) but the context "Ø_k" is not present in the surface form (the left context is null, the triggering element on the right is not present on the surface). Which is basically what musicallinguist said, with definitions and references inserted. As it turns out, OT "accepts" the concept wholesale from rule theory, but rejects it as a theoretical concept which epiphenomenological (as it was in rule theory). 

The answer depends on your metatheory. In standard feature theory, there is no feature "place" or "manner", so "αplace" or "αplace,βmanner" is meaningless. However, that doesn't stop people from writing rules like that: essentially what they do is redefine the notation and redefine what a feature is (I disapprove, but it happens). The basic problem is that in the linear theory, there isn't anything that refers to classes of features, so you'd have to specifically list all of the changing features, and the rule gets to be pretty big. So this is one of the advantages of autosegmental theory. This topic actually comes up here, somewhat, in a discussion of what the right formal account of assimilations is. It's clear that we need some mechanism for talking about classes of features, like "place", but there is more than one way to actually get that -- and there is an alternative (first hinted at by McCawley decades ago) to the SPE feature variable notation. [Clarification] A feature is understood to be a well-defined acoustic state or articulatory action, which either is or is not. Neither "place" nor "voice" satisfy that requirement, so "place" can't be a feature. Ladefoged did propose multi-valued features that would include place, but those were features in name only, since at the time the only concept that was available for representations was "feature". [On numbers] Use of the numeric transformational format as found in early statements of reduplication is another possibility for total assimilation, if and only if your theory allows such unrestricted rewrite rules. Phonologists generally hate them. The general format for Arabic-like rules would be 

This is a common pattern across language phyla. I disagree with Dominik Lukes' implication that "why" questions in linguistics are intrinsically bad. It's true that our models are not precise enough that we can predict with 95% certainty what structures will exist in a language, but that does not invalidate the quest for scientific understanding of the underlying mechanisms of language. Since honorific plurals are found across the globe, we can reject the hypothesis that they result (in general) from a localized quirk of Latin (possibly true for Europe), for instance. This pretty much leaves the explanation in the domain of pragmatics and figurative speech. The circumlocution hypothesis is the only one that comes to mind which, in my opinion, is at least plausible -- that is the idea that it is more polite to speak obscurely, so as to not impose a social burden on the addressee. By speaking obscurely, you leave open the possibility of the addressee not understanding (and it's your fault, not his), which avoids imposing a burden to act on the addressee. Additionally, using a form of address that refers to the actual addressee plus others creates the interpretation that the actually addressee can simply do nothing, since the request is really made of "others". 

[v] and [ð] are acoustically very close together. Jongman et al. 1998 note that the frequency of the spectral peak for labiodental and interdental fricatives is not distinct (but at other fricative places, for z and ʒ, spectral peak frequency is different). They are also the same in terms of duration (they are shorter than the sibilants). Noise amplitude is what makes [v] and [ð] distinguishable -- [v] is louder. A frequent form of sound change is where two sounds are acoustically very similar, meaning that they can be hard to tell apart in sub-optimal conditions (i.e. normal life). Once these changes start, they can spread like wildfire. 

It's still not clear what fact of "understand" you want, because you haven't given us a definition of "meaning" (asking for "actual meaning" just confuses the matter, since it implies that there are non-actual meanings that you don't want). Here are some notions of "meaning" that I think you might be interested in. (1) a translation matrix into other languages or the same language, e.g. "understand": "comprehend": "comprendre": "fahamu": "vorstå". (2) an reduction to defined terms of the states and events which are described by the word, in normative usage. (3) a statistically-adjusted reduction to defined terms of the states and events which are described by the word, in observed usage by all speakers of English. (1) is usually just wrong, and seems to be the thing you don't want. (3) is too complicated and is of dubious empirical value (sampling problems, for example). I surmise that you want something like (2), where you state the necessary and sufficient conditions for felicitously using "understand" in Standard English. A non-circular account would distinguish "understand" from every other word (unless you have a situation like "sofa" and "couch" where there is no difference referents), and is built on other well-defined concepts which ultimately are based in perception. Only a consciousness can "understand", and what the consciousness understands is a fact of reality. This does not distinguish "understand" from "think" or "know", but you can add more distinguishing features, so that you can differentiate "understand" from "know". To do that, you could assemble pairs of sentences which differ in word choice, like "I know Spanish" and "I understand Spanish", which should lead you to see some differences in the situations described by the two verb – you may understand Spanish yet hardly be able to speak it, but it would be infelicitous usage to say say that you know Spanish if you can't speak it. Similarly, the situations described by saying "I know Jack" and "I understand Jack" are different, though there is some intersection. If you apply these two verbs to somewhat different objects, the difference becomes sharper: you can say "I understand this contract" but it is almost senseless to say "I know this contract" – unless you mean "I have encountered this contract before". It appears, then, that you are looking for a characterization of how "understand" is distinguished from all other words of English, in terms of felicitous usage. Since you mentioned being a programmer, it is likely that you are not interested only in "literal meaning", you would also be interested in metaphorical extensions. When A says to B "We understand each other", A is not just saying that A literally understands B and vice versa, he is saying that they are in agreement (so B could respond "I understand your position, but I don't think we do agree"). It depends on the nature of the programming task, but it is likely that it will have to deal with "actual usage", which expands the range of situations that have to be dealt with substantially. So while I encourage approach (2), from a practical perspective, ignoring "actual usage" (figurative language, especially) may be a short-sighted and mistaken choice.