Below are some good links from Microsoft which tell you how to move databases specially content database of Sharepoint server. You can select appropriate sharepoint version. Move all databases in SharePoint 2013 Move content databases in SharePoint 2013 

You are correct, but this is actually caused by the fill factor value. You cannot say that now fill factor is not 100 (assuming your case) that would be an incorrect statement because fill factor was 100 this page split event was forced. Suppose you have index page which was almost full and new row needs to be accommodated now this would cause page split and move half rows to new page there by causing fragmentation. So what caused this: it was precisely the 100 fill factor value. That is why you must be cautious with what fill factor value you choose A low, nonzero fill-factor value may reduce the requirement to split pages as the index grows, the index will require more storage space and can decrease read performance Also note that whatever fill factor value you use you CANNOT avoid page split even if you have ever increasing columns. There are 2 types page splits good and bad. The bad one is when data is inserted in middle creating a gap. Some information about Full Factor in Paul's Myth Series about Fill Factor Read more about fill factor 

The account which user selected on Server Configuration page window ( during installation) is somehow not able to bring SQL Server database engine services online. Either is lacks privilege or it is corrupted. During installation of database engine services SQL Server tries to bring online database services as a internal process but due to startup account either corrupt or not having appropriate privileges it fails to do so and ultimately installation fails. Other reason is when installation fails first time due to some reason and user uninstall failed installation from add remove program, the un-installation leaves account in broken state so any further attempt to install flashes this error message. The reason can be also that SQL Server installation was successful in installing SQL Server and its services but due to some other limitation in system or other system related errors SQL Server is not able to come online. The same has been discussed in section 'Looking at SQL Server errorlog' 

No this IMO is not correct inference like I said above before sending log records containing commit principal first hardens it on disk. SQL Server would not rollback the transaction if it does not receives confirmation from mirror transaction would still be open A very important point failures to commit on the mirror will not cause a transaction rollback on the principal. I would like you to read Table 9 Section in This Link 

This is not an official statement but I got this from my MS friend. It would be supported subject to condition both CU and SP are supported. But the risk and burden lies on you. I have worked with such environment for very short period, to be precise for 4 days and after that I made sure all the replicas are on same SP/CU level. If you really need it, I would suggest to try to limit this to couple of days to be on the safer side. The correct approach would be to first test the SP/CU on UAT and then if you are happy proceed on production. But sometimes due to constraints this may not be possible. If replicas are on different SP/CU failover may cause primary replica to be on node which is not at latest SP/CU and you may see issues which was fixed in recently released SP/CU so this is very risky. The solution here is to keep such environment as short as possible. 

Its has one row for begin tran one for commit and one regarding update operation. You can see for beginning of transaction and for commit of transaction. says a row was modified like we did in update statement. is in picture because table had CI and row must have been exclusively locked for update Then comes hexadecimal page ID which actually says what page had row which was modified Then last column Includes the lock Information. The lock which was taken on Database file number and page . You would also see KEY value There is also a log sequence number for every transaction and column Log Record Length which would tell you LSN and size of log record for particular operation 

There was notable change in SQL Server memory code in SQL Server 2012 but no where its related to what you are seeing. SQL Server memory allocation is dynamic in nature when SQL Server boots up it would grow its memory notification till the resource notification is revoked.SQL Server will keep allocating memory based on its need as long as there is memory available I.e. as long as notification is signaled in widows and will scale down its usage when there is signaled in windows. When available memory is between the low memory and high memory SQL Server will try to maintain the memory usage stable with some exceptions. With 48G memory you have set max server memory to 44G and only given 4 G to OS. IMO this is not correct you must give at least 6 G OS. You should give enough memory to OS so that it would have enough to allocate otherwise it would start signaling low memory notifications and SQL Server would have to trim its memory consumption. Does SQL Server service account has Locked pages in memory privilege. If not you should give it, but before doing that please leave 6-7 G for OS( I am assuming here that this is dedicated system for SQL Server) If you want SQL server to consume more memory create a dummy table and then start inserting large number of records in it at same time start running multiple statements you would see SQL Server memory consumption rising. If you want to check SQL Server memory consumed by SQL Server use query posted above 

After launching the setup and providing necessary details you would reach to Server Configuration page as below. When you reach this page which is also called as Local System account. Please see screenshot 

12 PM Sunday full backup Differential backup taken before the disaster strike. Note this is just one diff backup you have to restore no matter how many you have taken because its cumulative The log backups after diff backup (in point 2) stopping just before the disaster struck. 

Not only buffer pool and query plans but almost all components which require memory will now be catered from max server memory setting. Specially SQLCLR, memory to this is now allocated from max server memory setting previously in SQL Server 2008 R2 and before memory was allocated from outside max server memory setting. 

Located HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Microsoft SQL Server in registry Right click and go to Permission Click on Advance Tick on both check box (I. Inherit from parent the permission... II. Replace permission entries on all child objects...), click OK Click OK again 

You should have waited to restart the machine after it prompted for reboot. I am sure it would have prompted but you would have selected for reboot then only it went ahead and rebooted. Avoid rebooting SQL servers when restore process is going on this would disrupt restore process and you would have to start all over again, which is waste of time. 

Correct solution would be to upgrade it to version which does not have limitation on data file like developer, Standard and Enterprise depending on your requirement. If there is space in your database try shrinking it and see if that helps. But this is temporary solution in no matter of time you would again face space issue. 

Please note that recovery model controls amount of logging in transaction log file and in case of disaster how much data you can recover. It has no relation to amount of inserts or updates or number of column in you table. You can keep simple recovery model if you do not require point in time recovery. You can take full and differential backup in simple recovery model. If you want to change recovery model to full then make sure you take regular transaction log backups after changing recovery model. Changing recovery model should depends on how much data loss you can afford or agreed between you and client basically the RPO and RTO. 

So at least the query will start running but during runtime its quite likely the intermediate result is spilled to Tempdb making it slow. I strongly suggest you read Understanding Query Memory Grant 

When you detach database you dont delete it you just remove it from SQL Server instance. The mdf and ldf files would be present at location where it was before, if you have not deleted the files. Detaching a database removes it from the instance of SQL Server but leaves the database intact within its data files and transaction log files. This link has code which can be used to find which database is not attached to SQL Server. Or you can simply go to windows serach functionality and search *.mdf and if you can see your database name listed it was detached. 

Use this Microsoft BOL document to rebuild system databases. You can also use this documents its more easy to refer because it has screenshots. See if this helps Please contact Microsoft CSS ( Customer support) they are the best person to analyze dump which got generated. The location of dump file is C:\Program Files\Microsoft SQL Server\MSSQL11.MSSQLSERVER\MSSQL\LOG\SQLDump0001.txt. 

I have not read the blog but above one seems correct approach. Differential backup is mainly there to reduce Recovery Time Object(RTO) it reduces the time taken to bring database up and running after disaster strikes. Example: If you have 

But note the above commands . That is what you are experiencing. After running above query you can change SQL Server max server memory to lower value( may be 4-5G in your case) this WILL force SQL Server processes to release memory and thus bring down memory consumption again I am suggesting this as machine in picture is DEV machine. Changing max server memory does not requires restart. The other thing is restarting SQL Server service this will definitely clear caches and memory held by SQL Server process would be released. But after you restart SQL Server, it might, after some time take back all memory. 

Can you please use Ola Hallengren Maintenance solution for index rebuild please also read the FAQ's I am sure it would help. Online rebuild do produces lots of logs as it is fully logged in full recovery model from SQL Server 2008 onwards. Rebuilding an index will update statistics with the equivalent of a full scan. Do not update statistics on indexes that have just been rebuilt. Paul has some information on the same in this Blog Basically you are rebuilding all indexes whether it is fragmented or not and then updating stats for all the columns whether stats is outdated or not this is sledgehammer approach and IMO this is what causing the resources to be utilized to maximum. If you use Ola script it would only rebuild index which is fragmented and only update stats for column which requires updation this would reduce overall amount of work that needs to be done by SQL Server 

You dont need to to give db_datareader and db_datawriter permission if you have given db_owner permission. As per BOL 

I am talking about database mirroring as I am not sure whether this discussion holds same for AG which I assume it will be. I am talking about Synchronous mirroring and as per my knowledge actually following happens Following happens when suppose DML is started on Database principal 1.The transaction Log record from DML transaction would be inserted into transaction Log buffer. 2.The transaction log buffer would then be written to disk that is hardened and at same time the Log buffer would be sent to Mirror server and principal will wait for confirmation from mirror server. please note the commit has still not been given for transaction in Log buffer 3.The mirror will receive log records in its transaction log buffer and it will write to disk and the notify principal that it has hardened the piece of log record 4.Principal would receive acknowledgement and then COMMIT for transaction would be entered in log record buffer As you can see now commit of transaction is entered into log buffer and hardened but SQL Server still does not confirms this as committed transaction 5.Now same process for commit would be followed as above it would be hardened to disk and log record containing commit would be sent to Mirror and then it would harden it send acknowledge and replay transaction logs. Now transaction is actually committed I dont agree below 

When you delete/drop a database using SSMS GUI and while doing so you select SQL Server will remove backup history of that database. What it does internally is it will run a system stored procedure called sp_delete_database_backuphistory this stored procedure will delete backup information about the specified database from the system tables. Now if you still see the information you can execute the stored procedure and remove backup information related to this specific database. For example below command will remove the information 

If it completes successfully its great now restore this backup on DIFFERENT server again using option continue_after_error(read official documentation) if all goes well and you are restore of database completes successfully run 

If you read complete article it was trying to point out the commands or operations in SQL Server which would require additional disk space and others that would not. It was trying to do so because when Index is REBUILT(with ONLINE OPTION) it used extra space because during first phase of rebuild system metadata preparation happens which creates the new empty index structure then snapshot of the table is defined. That is, row versioning is used to provide transaction-level read consistency. This snapshot would take space this is what books online meant by extra space but in Reorganize of Index no such operation happens no such snapshot is created just it only requires 8KB of additional space in the database which is not much of a concern.Also note that reorganize operation is fully logged. Index reorganize is always online operation while index rebuild can be both ONLINE(present only in enterprise edition) and OFFLINE. 

and see for possible data loss. This would give you great idea about how much data loss could happen. And then you can plan accordingly about informing client about how much data loss occur. A very important point missed is did you tried finding reason why database went corrupt this should be on top priority because recovering from data loss is just half job done finding the cause and mitigating it will be complete solution. You should refer to SQL Server errorlog and windows eventviewer for more information/clue as to what could have caused possible corruption. Looking at error message it could be because of underlying disk corruption. PS: In this scenario I wont think Third party tool would help more than apart from what is mentioned in both answers