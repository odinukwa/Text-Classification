I don't have Hurley's book, so I can only proceed from what you've quoted, but I take him to be saying that it is the quantity of the occurrences of the methods that is most significant, not the type. He is not saying that one type of method is intrinsically better than the others, only that the more occurrences you have of any the better. Roughly speaking, this is like saying the more data you have, the more confidence you can have in the inferences you draw from it. Having said that, such a claim is fraught with difficulties. Merely having more occurrences, or data, is not in itself a guarantee of anything. To make plausible inferences from data one must attend to all kinds of considerations about how the data is gathered, the quality of the data, whether the data sources are independent, what possible biases might be present, what confounding variables might exist, etc. In particular, the sentence 

Goodman's claim is that Hume has missed the main point about how observing past examples provides confirmation of laws. To appeal to the uniformity of nature is either vacuous or false. The future always resembles the past in some respects and does not resemble it in other respects. The important question is which predicates are projectible and which not. Which are the properties that we can reasonably be confident will continue to hold in future? Goodman's 'grue' predicate is highly artificial and this makes it difficult to understand. Your question seems to suggest that you interpret Goodman to be saying that a grue object will change color at some future time t. This is not the case: nothing magically changes color in Goodman's example. Grue simply means an item that is observed before time t and is green, or is not so observed and is blue. One could even perhaps eliminate the reference to t and imagine a predicate that describes an object that has been observed to be green or has not been observed but is blue. The reason we find the example bizarre is that we are convinced that our eyesight and other faculties are well adapted to distinguishing blue from green, but not grue from bleen. It might help to consider a very different kind of example. In the realm of finance and investment, analysts often come up with an investment thesis and back test it against financial data from the last 50 or 100 years. Well and good, but what reason do we have for supposing that this thesis will continue to work in future? It is not a question of uniformity, just a question of why this property and not some other. There are, after all, many different theses we could come up with that would give different predictions about the future while being consistent with the past data. Projectibility depends on other characteristics, such as whether we are correctly grasping some fundamental natural kinds. Incidentally, Goodman's choice of emeralds for his example is unfortunate. An emerald is a piece of green beryl. If beryl is blue then it is aquamarine, not emerald. This means that 'emerald' is itself a color term, which complicates the example. I suspect Goodman didn't know this and would have selected a different example if he had. 

Kripke's Naming and Necessity, definitely. If any of Quine's books counts as a magnum opus, it would probably be Word and Object. Other seminal works in analytical philosophy include Austin's How to Do Things with Words; Gilbert Ryle's Concept of Mind; Strawson's Bounds of Sense. I'd like to include David Lewis' Plurality of Worlds too, but I find his modal realism weird. 

People want things. There aren't enough things for everybody to have as much as they want. Therefore: People have to prioritize their wants. People have to make decisions in accordance with their priorities. The decisions of lots of individuals interact with each other in complex ways and create conflicts. Societies need rules and structures to manage those conflicts. 

Try to look at an issue from as many different perspectives as possible. Find out what other people think about it. Look at what people in different countries, different languages, different cultures, different times have to say about it. Don't assume that because Aristotle, Marcus Aurelius or Montaigne lived a long time ago they have nothing useful to contribute. When you have what you think is a good explanation or account of something, keep looking for alternatives. Many people when they encounter an issue sieze upon what they consider to be the most obvious explanation and then cling to it like a dog with a bone. There are always alternatives. Try to falsify your beliefs, not just confirm them. It is tempting to look for evidence that you are right, but it is just as important, if not more so, to look for evidence you are wrong. Don't be afraid to challenge the prevailing concensus, or the assertions of the prevailing authorities. Even experts get things wrong. The history of medicine, for example, is full of false claims and harmful remedies that persisted for a long time because nobody dared to challenge expert opinion. Remember that theories and models are just useful fictions, so don't get too attached to them. Reality is always more messy than theories allow for. Guard against groupthink. It is common to spend most of your time with people you agree with. You should spend more time reading and talking to people who disagree with you. If others disagree with you, don't assume they are just ignorant or have vested interests. Imputing a vested interest to someone is an easy game to play, and anybody can play it, but ultimately it proves nothing and is unhelpful. Challenge your own preconceptions about things. Be realistic and honest about your own knowledge. It is easy to have a strong opinion about something you know little about. Self-criticism is difficult to do but essential to avoid bias. Don't be afraid to change your mind about something. It is not a sign of weakness. Equally, don't suppose that because you used to believe something and changed your mind that it must be false. "I used to think that..." is not a good argument. Never stop learning. Learn from a wide variety of different fields and subjects and don't over-specialize. Learn about the common forms of cognitive bias, so you become less likely to fall victim to them yourself. Always bear in mind that your own views, no matter how strongly you believe them, may be false. It is extremely easy, and common, to become overattached to your own opinions. Being sure you are right is positively correlated with low intelligence and ignorance. Smart, knowledgeable people know their limitations and are more cautious in their claims. 

The fundamental meaning of a fallacy is that it is a defective piece of reasoning that conforms to an identifiable pattern such that all instances of that pattern are defective. If you are asking, is there a single kind of pattern that all fallacies conform to, then no, there are many different types of defective reasoning. What fallacies do have in common is that they are not cogent, or that the truth or probability of their premises does not provide support for the truth or probability of their conclusion. It is important to note that the defective nature of fallacies is not the same as invalidity. Being invalid is neither necessary nor sufficient for an argument to be a fallacy. 

I don't disagree with Virmaior's answer, but it is worth making the point that the validity of contraposition depends on which logic you are using. If you are assuming classical logic, and if the '→' connective is intended to denote material implication, then contraposition is valid, which is to say that A → B entails ¬B → ¬A and vice versa. This can be demonstrated syntactically and semantically. The syntactic demonstration is to construct a proof, e.g. using natural deduction, while the semantic demonstration can be achieved by constructing truth tables and showing that the two sentences share the same values. This works for classical logic because it is bivalent, which is to say that it assumes there are only two truth values, and any sentence that is not true is false and any sentence that is not false is true. If we choose to move away from classical logic, things get more complicated. Intuitionistic logic, for example, is not bivalent (nor indeed n-valent for any n). We can still construct a proof that A → B entails ¬B → ¬A but not vice versa. But without bivalence we cannot simply appeal to a boolean truth table to get a semantic demonstration; we would need something more complex, such as a Heyting algebra. More importantly, there are logics in which contraposition is not valid at all. For example, in David Lewis' logic of counterfactual conditionals, A ◻→ B does not entail ¬B ◻→ ¬A. Also, in Ernest Adams' probability logic it may be highly probable that B given A, but not highly probable that ¬A given ¬B. So in general, when speaking of ordinary English conditionals, one cannot always expect contraposition to be safe. A noteworthy corollary is that in both the Lewis and Adams logics, while contraposition is not valid, modus tollens is valid. Some accounts of logic incorrectly run together contraposition with modus tollens and treat them as the same thing. While both are classically valid, they do not agree across all logics.