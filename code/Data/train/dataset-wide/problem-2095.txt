If we can make the assumption that the operation will always return a set where a member "id" is listed followed by its role in a single column of entries, then I've written up a quick solution for you. I made a quick data set as: 

I encountered this problem myself once, and for my case, I was able to fix it by setting the to a low value for just the query, as 

I'm going to add my answer, which was correct in the comments, as the answer here. Be specific about your types! In your master table, you have as a . In your constraint, you have used . Then, in your query, you just use , thus using a type in your predicate. To fix your problems, make sure that all of these entries are referring to a consistent type. In your comments above, you indicated that you changed all entries to refer to the type , and that it fixed your issues. 

Refer to the Postgres documentation on within for more info. A word of caution: By applying , it will keep only the first row, regardless of how the order is returned by your query. Unless you apply some criteria, you may end up with inconsistent results! In effect, you could run the query once and get as , but run it later and get as . I would highly recommend some additional predicate criteria to ensure proper and consistent results are returned. 

I'll try my best to answer your question in brief, but since I'm not really aware of your level of comfort with PostgreSQL, and I don't have a lot of time to go into an in-depth explanation anyways, I'll keep the answers simple, and you can ask for clarification if you'd like more info. 1) Why is it faster in batches? Due to the structure of PostgreSQL's write ahead log, the amount of shared buffer space in RAM, and the attempt to perform the entire in a single transaction, my guess is that you simply don't have enough computing resources to efficiently handle the update to nearly a million records in a single transaction. PostgreSQL has a well-built concurrency control system, essentially meaning that it has to keep the old copies of your pre- rows available during your operation. This is so that, in case another client tries to access these rows while you're updating, in case the update fails, or in case you cancel the update, you don't lose the old information. If you perform a large enough , PostgreSQL will load pages into memory and modify them, but will eventually run out of memory to work with, so it is forced to immediately copy these pages temporarily to disk if it wants to be able to load further pages and continue the transaction. Rather than being able to amortize the disk writes over a period of time, you've just forced your database into a bottleneck. 2) Scripting the updates You absolutely can script the updates, by creating a function in PL/pgSQL. There's a lot to learn about PL/pgSQL, including a lot I probably don't know, but generally speaking, you could do something like this 

In Cassandra, as far as I know, for filtered by non-key attributes, you only have three options: (1) Application side filtering. That is, if you get your results from a CQL , use your application to filter the results. For all but the smallest data sets, this is ill-advised. (2) Bite the bullet, and create those secondary indices. (3) Probably the most common option, duplicate your data by having rows composed of keys. That is, for whatever filter condition you want to apply, create a new entry in your database where you store the keys of all the relevant row entries which match the filter. Note that while the third option is most common, you will almost eventually develop some data inconsistency due to the inherent de-normalization. Apache Cassandra is not a cure all, it simply handles some applications very well. Good luck! P.S.: Here's a couple decent blog entries which can explain a bit of the theory at the logical data model level. Part 1 & Part 2 

is completely redundant. You appear to be re-checking for already checked conditions. For example, try just running 

I'm sorry, I don't have a corrupt table to help perform any verifications. My answers here must, by necessity, be heavily based on speculation. With the limited information we have, I am guessing that you are attempting to do a full database , along the lines of 

OK, so you've listed a few questions here, so I'll try my best to answer them. Is your query correct? First, you said: "I am interested in finding out how many (pl_namespace,pl_title) pairs in the pagelinks table show up in the page table as (page_namespace, page_title)." Is the query you're performing correct for that, or did I misunderstand the description? If you run 

Indexing Expressions I believe that the solution you are looking for pertains to building an index on an expression, rather than on the original data itself. For outside references, you can consult Wikipedia or the Oracle documentation, where the link here contains more info and subsequent links in the 'Function-Based Indexes' section. There are associated costs and benefits to using such an index, so it is up to you to see if it matches your use case. Contriving a generic example, let's say you had a table , having a column named and a column name , but you want to frequently query based on the month of the order. Then in this case, running 

If you want to test the performance, use the link to the SQL Fiddle below! SQL Fiddle EDIT: So, a commentor below asked why I used . It's because of the . In this case, you are asking the query to return rows , but the ensures that entries are returned whenever there is no match. In these cases, you can't know if a particular will be in table , table , or in both, so by using you ensure that so long as a one of the tables has the field (which is guaranteed, since you did a ), this will be filled in. WARNING: One of the alternative posted responses is an incorrect solution. Applying that erroneous query 

I think you should really look into using inheritance to manage this particular issue. Inheritance Table inheritance does many things in PostgreSQL, and I'd highly recommend you refer to the online documentation about inheritance to get a feel for what it does and how it might work for you. Since you stated that your are all structurally similar, then you can make a parent table as 

romeo, before I craft a reply, I want to ask: Is there any chance you have the opportunity to edit the schema in this case? If you can, you definitely should, because the problems you're running into are because of denormalization. Normalization of the schema From your example table above, it appears this should be split into at least two separate tables. From the information you've given you should have an table and a table, with (pseudocode) definitions as 

and then materializing those results. I then, once again use the expression to try to manually manipulate the plan. In these cases, you probably can't trust just the results of , because it already thought that it found the least costly solution. Make sure to run an to find out what it really costs in terms of time and page reads. 

With respect to these approaches, I have no idea if this will help, as I've simply never encountered a failing for any other reason than a syntax error, or something else relatively benign. Use to make CSVs As a last ditch effort, if you aren't able to pg_dump your schema or tables, in a compressed format, you could always extract the data using the command to make comma-separated variable (CSV) files. 

Then, when you run a query such as above, the planner should be able to make use of the index for much faster querying. Hope this helps! 

This way, you save on storage, and you can easily update/modify any category or subcategory entry in the list without needing to modify your entire table. Further, you can simply permit the column of the table to permit entries, if need be. Hope this helps! 

where in this case is simply the result of the above . The second argument indicated to that the items are comma-separated. This will yield a field containing your desired entries. 

Again, as expected, the inclusion of reveals to us some very important information. In the low case, we see that there are rows removed by the index recheck, and that we have heap blocks. Conclusion? (or lack thereof) Unfortunately, it looks like on its own is not sufficient to know whether an index recheck will actually be necessary because some of the row id's are being dropped in favor of retaining pages during the bitmap heap scan. Using is fine for diagnosing the issues with moderate length queries, but in the case that a query is taking an extremely long time to complete, then running to discover that your bitmap index is converting to lossy due to insufficient is still a difficult constraint. I wish there was a way to have estimate the likelihood of this occurrence from the table statistics. 

I think you're going about this the wrong way. First off, I don't think you need a procedure/function to solve your problem. Second, I definitely don't see how a cursor should come in to it. Try the following window query using the OVER() clause: 

To me, it looks like you're on the right track, if I'm understanding your question clearly (which I'm not sure that I am. :P ) To me, it looks like you simply need a block, where you are declaring variable values which will persist throughout the function block. Add 

as in this SQL Fiddle. So, I'm either misunderstanding, or your originally given data set doesn't reflect the significance of your sub- in the original question. IGNORE EVERYTHING BELOW HERE... Until we get clarification from the author... I think I've got your solution. At the very least, I've made a SQLFiddle with the results, and it appears that it will be much less costly. sub-queries in PostgreSQL I love chances when a sub-query can be used to save some time in your query. Unfortunately, I think I'm pretty bad at explaining when and where it should be used, and I'm only OK at recognizing instances of when to use it. It just doesn't come up too often in my particular query designs. Take a look at the Postgres documentation on keyword for some ideas, and also I really like this SlideShare presentation by Markus Winand for helping to explain a bit better. In essence, it has a flavor of a "for each" statement in typical pseudo-coding vernacular. The reason I looked into it for your case was: you were building the table, and then using the attribute of in your sub-, where you checked if was the distinct returned set of values. Using attributes in the clause of the sub- was the red flag for me. OK, so I realize the explanation of my motivation isn't so hot, sorry! :P On to the results... New Query Without further ado, here it is: 

I believe this can be solved relatively quickly, by adding a clause to your , and adding an additional equality condition, namely, adding . 

Presumably, though you should check against your own data, you can see the number of bytes which the index key is using by checking the field. I am betting that you can use this to check against your pre-determined limit which you said was 2712 bytes, and in that case use this info to collect all the s which violate your condition. Broadly speaking then, you need to create a PL/pgSQL function to iterate over the pages of your index(s), checking the , and collecting the s of the heap entries, and once completed, since you state it was OK to drop violating rows, then wrap it all up with a 

This may not seem like it is all that helpful at first, but if you have for example thousands of concurrent trips, then they may all be taking measurements once per second, on the second. In that case, you'd have to re-record the time stamp each time for each trip, rather than just using a single entry in the table. Use case: This case will be good if there are many concurrent trips for which you are recording data, and you don't mind accessing all of the measurement types all together. Since Postgres reads by rows, any time you wanted, for example, the measurements over a given time range, you must read the whole row from the table, which will definitely slow down a query, though if the data set you are working with is not too large, then you wouldn't even notice the difference. Splitting Up Your Measured Facts To extend the last section just a bit further, you could break apart your measurements into separate tables, where for example I'll show the tables for speed and distance: 

Upgrading a PostgreSQL Cluster Postgres official documentation I've only ever migrated using and , which is perfectly acceptable, but the docs also refer you to methods which involve , , or via replication. In general you want to apply one of these approaches since the data format used in successive versions is subject to change. 

OK, good. Back in it's right place in the schema, and the functions are still accessible to all schemas in the database. TL,DR; Just use the default schema for extensions. 

I'm concerned that we may not have a clear picture of how a group is defined. I only say this because, depending on some unstated conditions, the dates above will either form one giant single group, or 3 groups where one group dominates the set. Missing grouping conditions? 1) Does this 15 day rule cascade? If a record starts 10 days after another record , and then there is another record started 10 days after that, then does this form one group of three records , or two groups each containing two records and ? I made the assumption that the 15 day rules cascades to form larger groups. 2) Are the dates inclusive? For example, if one record has a start date and then a dead date many months later, do all days within that range get merged into the group? I treat both possibilities in my quick analysis below. Potential Groupings So, if we begin with id , we see that the start date is 1/27. Clearly, the next entry starting on 1/28 falls in this group. Notice however that ends on 5/15, so anything which starts within 15 days of 5/15 must be added to the group. Thus, through get added to the group, which via cascading leads to being subsequently added to the group. The cascading has consumed nearly all entries in the set. Call this . If, however, the grouping is date inclusive as well, all entries from up to must also belong to , and now all ids are in a single group. If the dates from aren't inclusive, but actually strictly adhere to the '15 day after' rule with cascading, then instead you would have a second group composed of through , and a third group with a single entry, . Essentially, my question is, do any of these match your intended grouping, or is there some other information we're missing in the group definition? I'm sorry for such a long-winded answer, but it doesn't appear that your tentative requested output meets your grouping definition. With clarifications, I'm sure that we can sort this problem out.