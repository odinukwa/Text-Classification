I wrote a TSQL script that will update the description based on the current version of the report. GitHub Gist 

Will not handle out of SQL references, but you might want to check out Redgate's SQL Dependency Tracker. It's a nice visualization tool. 

The LDF is not a log backup, so I think you're stuck with restoring the db as it was during the last full backup. If you had transaction log backups since the last full backup, you could restore those as well and get you up to your most recent one. Unless someone answers that knows of some tricks to pull, afraid you've lost 15 days worth of data. 

Have you tried using Adam Machanic's sp_whoisactive? There's an option to get the outer command to see if it really is within a proc. It could be the application is holding open a transaction instead of committing it. Try looking at DBCC OPENTRAN as well. 

$URL$ shows that -Output expects a path, not an integer as the error message says. Pub & Sub are both v9.0.4211, Dist is v10.0.2723 My Script (run at distributor): 

Yes, most analytic functions can be rewritten with semi-join. In your case it would probably not be very efficient: 

You're applying on a datatype. It is not only superfluous, but it actually causes unexpected behaviour. Here's how Oracle analyses the expression : 

If you shutdown the database normally (or transactional or immediate), Oracle will wait for the process to complete so that the database is left in a consistent state. If you shutdown , the database will be left in an inconsistent state (uncommited data written to disk) and Oracle will resume the rollback as soon as the database is restarted. The rollback is handled by smon, I wouldn't try to kill it. 

you insert a row in table A a trigger on table A (for each row) executes a query on table A, for example to compute a summary column Oracle throws an ORA-04091: table A is mutating, trigger/function may not see it 

Locks in Oracle are managed at the row level. Concurrent disjoint transactions should not interfere with one another. Unindexed foreign keys are an exception, since it can result in a complete TABLE LOCK. You should get the SQL in the trace file of the deadlock and that should help you narrow down which table / foreign key is responsible for the lock. Once you know which table is affected by the deadlock, make sure that all foreign key references to this table are properly indexed. E.G in your example should be indexed if it points to . Alternatively you could use Tom Kyte's script from the above link to determine if you have any unindexed foreign key. 

You could use parameters to solve this problem with the added benefit of execution plan reuse. Set the data types to match database. 

You can only reference servers that are listed under Server Objects -> Linked Servers as well as the local server via what you get back from @@SERVERNAME. Four part naming does not trigger a NETBIOS / DNS lookup. If you are referencing the local machine anyway, why not just use three part naming? 

We are in the process of removing a previous dba login and he owns all the endpoints and event notification objects. Endpoints were easy to change; Event notification objects not so much. I found this thread about changing the owner of an event notification object (you have to drop and recreate). I don't want to go through this process again if I can avoid it. I doubt it's possible, but outside of logging in as another user, can you create an event notification that runs as sa, etc.? 

It depends on your environment. I would setup a test using both methods and see which works best for you. Personally, I would page on the server. Less data over the wire and less data in the client's RAM, the better. If you can control the client machine specs, all traffic is over a nonsaturated LAN and clients always page through multiple pages quickly, then you might want to page at the client. 

One way to force data to actually be overwritten would be to update it to a meaningless value before deleting the row. This wouldn't work with indexes since updates are translated to delete+insert in a b*tree index. 

I don't think Oracle keeps track of past closed queries. However, you can find out what cursors a session has opened with . Since many applications cache the cursors for later reuse (this is automatic in PL/SQL: a cursor won't be completely discarded unless you reach the maximum number of open cursors), in many cases all past queries will be in this view: 

If the client itself has already left or the network communication has been severed, the killed session may stay indefinitely until you restart the instance. I don't think these sessions take resources. 

You can't really list all rows that are being locked by a session. However, once a session is being blocked by another, you can find which session/row is blocking it. Oracle doesn't maintain a list of individual row locks. Rather, locks are registered directly inside the rows themselves -- think of it as an extra column. You can find which session has acquired a lock on an object through the view, but this will only list general information, not at the row level. With this view you can also find if a session is being blocked by another. In that case, if a session is blocked by another session, the row information is displayed in the information. You can retrieve the rowid, let's build an example with 2 sessions: 

You could use a Document-oriented database for this. You could then create a program in your preferred language to import the existing documents into the db, parsing the folder structure for the metadata (customer, job#, etc). 

You could further normalize and have a row for each unique combination of game, team, & inning. This would allow you as many innings as the InningId datatype would allow. 

The simplest solution is to buy something like Red-Gate's SQL Doc. There are a few people who have written scripts that you can find for free. 

Using SSMS, you cannot chain a restore of the backups in one operation. You would have to do multiple restores. You'll want to use T-SQL in order to be more efficient. 

If your database is in FULL or BULK_LOGGED recovery mode, you need to backup your database and log files on a regular basis. If your database is in SIMPLE recovery mode, then you only need to backup your database on a regular basis. Please read the following articles for more info: 

Here the row in sale will be replaced (updated) by its first split component and the additional components will be inserted. 

Your materialized is not defined with a NEXT clause, therefore it will only refresh when you ask for it explicitely. You can use either DBMS_MVIEW.REFRESH directly or create a refresh group with DBMS_REFRESH. In order to automate the refresh, you could program a job with DBMS_SCHEDULER or DBMS_JOB (dbms_job is deprecated in 11g). You could also define your MV with a NEXT clause, for example this will refresh the MV every hour: 

You should never rely on implicit conversions because the behaviour is context dependent. Your interval filter is wrong: two intervals and will intersect if and only if: 

There is no difference in treatment in Oracle between a java and a standard : both are treated as cursors. They are both parsed (="compiled"), executed and (for queries only) fetched. The difference between the two kinds of statement is that : 

Most tools (PL/SQL Developer, Toad...) will display these comments in appropriate fields when you browse the database schema. The comments can be queried directly with the dictionary views, such as . 

Edit: I know PostgreSQL uses Sequences instead of IDENTITY, I don't recall the correct syntax off hand, so translate accordingly. 

I have a 2 node cluster (NODE-A & NODE-B) with 2 SQL instances spread between them. INST1 prefers NODE-A, INST2 prefers NODE-B. INST1 started generating errors then, failed over to NODE-B. Migrating INST1 back to NODE-A generates the connection errors after it logs a "Recovery is complete." message. Win 2008 R2 Ent. SQL 2008 R2 Ent. Errors from the Event Log after first failure: 

Composite keys as primary keys also run into index size issues that can affect disk usage, io speeds, and backups. You might want to review Kimberly Tripp's posts about primary keys and clustered indexes here: $URL$ I too would suggest a surrogate key in this case instead of a natural one. 

I would say consolidate your indexes in this case as for every insert, delete, and appropriate updates would require index maintenance * n indexes. Also, you can use the WITH (DROP_EXISTING = ON, ONLINE = ON) to reduce outage if you have Enterprise Edition. 

So cleanout (full with redo) will be performed during commit if the blocks are still in the SGA. In active systems, it is common for blocks with uncommited transactions to be written to disk and flushed from the SGA. In this case, the block is left as is and the next query that touches the block will perform delayed block cleanout (your point 5 doesn't happen in all cases). 

Edit: By definition, describes materialized views owned by the current user while describes the materialized views accessible to the current user (the user needs to be granted SELECT on the mview either directly or through a role). 

This is an interesting question: When does Oracle really delete data physically ? The unit of data in Oracle is a block. Let's see what happens when we delete a row. Here's an example with a simple table on 11gR2 (see "How to dump Oracle Data Block?"): 

If your table is updated concurrently, a bitmap index with a unique value will be a point of contention and shouldn't be used. 

Microsoft has done this with the next version of SQL Server, codename "Denali", as well as SQL CE 4. Check out the OFFSET & FETCH parts of the ORDER BY clause here: $URL$ 

I would suggest adding a (set of) staging tables to the destination or an intermediary that can be better controlled and more stable. Place the tracking info there. Then do all the transformation from the staging to the final destination, either carrying the tracking info with it or discarding it. There are several ways you can generate a tracking key from multiple systems, as long as they all follow the same algorithm, it doesn't have to be an INT. It could be a 2 char prefix with a sequential number. For that matter it doesn't have to be just one column. 

If the parameters are defined as the correct length, SQL will throw an error when calling the procedure. If you always define as (max) then you'll have to manually handle passed parameters that are too long for the destination table column before any processing in the proc happens. So in a nutshell, more work on your end since you'll have to add error handling to the proc you didn't need before.