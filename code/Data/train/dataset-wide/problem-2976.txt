Your implementation is really nice, however you could get way better performance with an array of bool! generateSieve with a : 

The function should not exists either. Intead, as @Josiah already said, intantiate the generator with a seed: 

An other easy optimisation would be to create an array with the correct capacity to save an allocation at each iteration, as we know that will have the same length as : 

After a long try I've been able to create a script in vba which can successfully handle webpages with lazy-load. It can reach the bottom of a slow loading webpage if the hardcoded number of the loop is set accurately. I tried with few such pages and found it working flawlessly. The one I'm pasting below is created using site. It can parse the title of different news after going down to a certain level of that page according to the loop I've defined. Now, what I wanna expect to have is do the same thing without using hardcoded delay what I've already used in my script. Thanks in advance for any guidance to the improvement. Here is what I've written: 

I'll go for the second method as there is a simple way to handle all connection in the main goroutine: instead of , let's handle incoming connection synchronously. There's no need for channels here, we can rewrite it like this: 

Some results on my machine (ubuntu16.04) , with a similar dataset ( 100 000 000 docs in data, 20000 in datagroup) New script runs in less than 5s : 

I've written a scraper in python using asyncio library to exhaust the name, address and phone number of all category from a webpage. Few days back, when I created a scraper to parse a webpage recursively, I got a suggestion to use asynchronous process to crawl a webpage with enormous data for the sake of optimum performance and avoiding blocking nature. However, this time I tried to follow that process so that my crawler can scrape the webpage asynchronously. It is running specklessly now. I used css selector to make the parser more readable along with acceleration of speed. This is my first time to work with this library, so I suppose there are scopes to take it to the next level whetting it's performance. Here is what I've written: 

Don't panic The code shouldn't panic on every error. For example, if the user running the program don't have the permission to read a file in , the program should not crash but rather log the error so avoid method like this: 

Almost two order of magnitude faster ! And the code is way easier to understand Also added a small test to make sure that all implementations have the same output: 

I've written a script in python with selenium to scrape different table data lie within different dots on a map in a certain website. Each table is connected to each dot. The table is activated once either of the dots is clicked. However, my script is able to open that webpage, traverse the map, click each dot to activate each table and finally parse the data of each table available on that map. Any input on this to make it more robust will be highly appreciated. Here is what I've written to do the whole thing: 

Concurrency Maps are not safe for concurrent use, for more details see map documentation Here you're modifing the map from multiple goroutines, so two solutiions to fix this: 

There is no need to create a new string at each iteration. You can directly append the subslice to the array like this: 

Then, the datagroup collection is filtered using and . On really big array, this can be really expensive as javascript is quite slow. Instead, we can let mongodb do this operation with a simple query using : 

I've written a script in python with POST request which is able to send email using send button and then catch the response of that email and finally parse the ID from that. I don't know whether the way I did this is the ideal one but it does parse the email ID from this process. There are 4 email sending buttons available in that webpage and my script is able to scrape them all. Here is what I've tried so far with: 

Work on []byte rather than on string This is a general advice regarding performances: always prefer working on instead of working on to avoid extra allocations. The regex package has methods to work on string or on byte slice, so instead of 

from now, we can check that we don't break anything when modifying the code. We can also accurately measure the performance gain... 3. Results We can see a clear performance improvement with the new code: old code: 

These are the five links out of thousands which are supposed to store in a csv file named containing a header : 

I've written a script in python to scrape e-mail addresses from different pizza shops located in los-angeles available in yellowpage traversing multiple pages. It is able to go one-layer deep and dig out email addresses. I believe, this crawler has got the ability to parse all the emails from any link no matter how many pages it has spread across. Just needed to adjust the last page number in the crawler. Here is what I've written: 

Don't use a to get the number set, because iteration order is not garanteed from one iteration to the next (see go maps in action for details) This can be a problem if some value have the same weight: the order of randomly change from a run to another. Intead, use two slice of A new version of the could could look like this: 

All errors (type error, required flag missing ect...) are handle directly by the library Now the complexity of the function is 3