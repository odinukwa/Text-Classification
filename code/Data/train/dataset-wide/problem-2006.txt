For some reason, the new CE is using different indexes, both of which give an estimate of 1 row, hence the ill-advised loop join. Are you in a position to update statistics on both tables WITH FULLSCAN? If that doesn't help, please post the table and index definitions (with only table/column/index names anonymised). Asides: The ObjectX aliasing in your sample code is very confusing, you may want to reconsider next time you need to anonymise, especially for a more complex query. AliasX for all aliases, with X matching the ObjectX, could be an improvement. By the way, if you haven't carefully considered the consistent use of nolock, you may want to read this: $URL$ 

Given the low rate of inserts, your proposed index will be completely fine, and perfect for the usage goal. Given a fresh index with fill factor 100%, and sufficient history for each organisation to fill a page, there will be a page split on the first subsequent insert for each organisation. But then there won't be another page split for that organisation until the new page is filled up. Even these splits and fragmentation can be mitigated by starting with a <100 fill factor, and regular reorganise. Aaron's thought regarding using non-clustered was probably because that would allow you to cluster on the always ascending identity column, ensuring the only splits and fragmentation you will get is in a very compact separate index. But I suspect he was only mentioning that in the context of you wanting very much to avoid fragmentation, and not as something really necessary in this scenario. 

You have to consider whether or not you actually want your trigger doing all that though, and I can't speak to the stability. For the comparisons, any particular reason why CHECKSUM instead of say i.colname=d.colname OR (i.colname IS NULL AND d.colname IS NULL) ? 

What you've described is really it. Self-signed keys are just fine for TDE in production as they will not need to be verifiable remotely. Having said that, using EKM apparently adds another layer of security. See here and here. But for self-signed, you can test your multi-domain requirement (which I'm quite sure makes no difference) using this info. 

PWDENCRYPT isn't officially deprecated yet, so it will almost certainly be around for quite some time. My concern would be that it "uses the current version of the password hashing algorithm" which means the behaviour could change without notice. Who's to say it will continue to be sufficiently random, perhaps instead going deterministic like Hashbytes? 

The questions of whether/how to directly measure CPU core usage etc. are beyond my understanding, but here's what I'd consider trying: Run a standard profiler trace with database name added, during your normally busiest period. Total up the CPU column for the SQL:BatchCompleted and RPC:Completed events by database, and you'll get a rough idea of how much CPU resources (which may be spread across multiple cores) each database is consuming. (Perhaps also total up the CPU column for the other events to see if anything major was missed. And save the trace "as trace table" for analysis.) Exactly how to translate that to how many cores you'll need, I can't say. But if you also measure the total system CPU usage during the profiler run you might be able to estimate against the specific database's ratio of the total. Note: If your server takes less than a few hundred batch requests a second (see SSMS activity monitor), then a standard profiler trace even across the network will almost certainly not affect performance. And if you instead script a server-side trace then more requests a second can be handled without slowing anything, but I make no promises for your environment. For RAM, I wonder if $URL$ might help you determine if your instances need less/more. I don't think there's any way to do this by database though. 

For something like this it would be much safer to script an update of just the column(s) that need updating in your user table. Cloning any part of your test environment user data into production could be disastrous. Is it not possible to use the script with which you (presumably) updated your test environment in production as well? 

Ok, with only a nightly large insert even nolock can only be an issue during that presumably minimal-activity period, so all I would suggest to change in your latest query version is to remove the "[varchar1] ASC," as it's unnecessary (since there will only be one selected) and will actually interfere. The indexing is going to be key. I'd try replacing your nonclustered index with the following: Create Index [i_table_index] ON Table1 (date1, varchar1, varchar8) INCLUDE (varchar9, varchar4, varchar5, varchar6, varchar7, xml1) This will satisfy your query without cluster key lookups. Why date1 first? Because only the first index column can be used for range scans, and clearly if your users are going to need up to 65k rows at a time they will need largish date ranges. However, it's possible the selectiveness of varchar1 may outweigh the range scan benefit, or that I've not fully understood the first-index-column-only rule (for which I can't find a good source at the moment--and it's possible the rule is actually range-scan-on-any-column-but-only-one). So since you've apparently got real-world data to test on, I would certainly also try this index instead (just the first two columns are swapped): Create Index [i_table_index] ON Table1 (varchar1, date1, varchar8) INCLUDE (varchar9, varchar4, varchar5, varchar6, varchar7, xml1) Also, beware that having date1 first may make the first query page (i.e. OFFSET 0) work nice and fast, but bog down with high OFFSET values. So test both index options with both high and low OFFSET values. Note I'm assuming that date1 ascending will be usable for the sort and fetch, but that depends how it's implemented internally. You may have to add DESC after date1 (in either index proposed so far), which I believe will make a internally fragmented mess of newly inserted pages in the nonclustered index--but perhaps you can run a reorg after the insert job. Finally, the long INCLUDE clause will duplicate nearly all the data in your table, which may be an issue especially with large XML. (Disk space may not be such an issue, but the added memory caching may make a difference.) So you may want to test with just varchar9 in the INCLUDE clause (kept there to efficiently satisfy the filter), as it's possible the non-filtering lookups may perform acceptably. And you might even want to try keeping the PK as is, having no non-clustered index, and clustering on (date1, varchar1, varchar8) which would satisfy the queries in the same way as the long include, but with no wasted space. However if it turns out you need DESC after date1, clustering in this way would possibly cause too much fragmentation pain. 

Ok, at this point I would be considering "manually" replacing statistics to tide me over. I haven't done this myself, but it should work in theory... You can confirm that this would work as follows: 1. Restore the pre-upgrade database to your LIVE system and confirm that the query is fast in that database, and delete that database once done. (This test is to eliminate any additional environmental differences as the cause. If the query is slow, then the rest of this proposal may be useless.) 1a. Restore the pre-upgrade database to your test system and confirm that the query is fast. 2. Update statistics, and confirm that the query is slow (cancel after a couple minutes of course) 3. Set compat to 120 and confirm that the query is still slow 4. Set compat back to 100 and confirm that the query is still slow 5. Restore another copy of the pre-upgrade database (I'll refer to this as Rest2, and the earlier as Rest1) 6. Extract all statistics from Rest2 using the techniques at $URL$ for all tables involved in the problematic queries (or all tables if that's simpler) 7. Apply the statistics to Rest1, and see if query is now fast (you may need to dbcc freeproccache first). If it works, then it's almost certainly safe to apply the statistics to your live database--just make sure you have ONLY statistics scripted. And also set its compat level to 100. You should then see the queries run fast (though you may need to dbcc freeproccache first--but consider the possible effect on live performance). Note I'm assuming here (based on your original post) that you do not have statistics autoupdate turned on, and that your data changes slowly enough that your old statistics will do until you figure out how to get your workload working with compat 120 and/or updated statistics (and you may as well sort both at this point).