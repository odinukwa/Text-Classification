I'd like to know whether somebody would suggest changes to the above numbers or even further settings that I do not know of. I'd appreciate any helpful remark. Steve EDIT: I have two queries involving joins across 10-20 tables and ran them on my Lenovo notebook and the new PC. Query #1 took 3m36s on the new machine vs 9m11s on the laptop; Query #2 took 22.5s on the workstation vs 48.5s on the laptop. So the execution speed was improved by roughly the factor 2-2.5. On the workstation, not even 50% of the RAM was used. The average CPU load across the four cores (as reported by Windows Task Manager) was only about 13%. The load on a per-core basis (as reported by Core Temp) was about 25-40% for ONE core, while it was <=10% for the others, indicating that MySQL does not make use of multiple cores for a single query. 

I'm running a MySQL 5.5 server on my workstation for scientific data analysis and wonder how to configure MySQL in order to get the most out of it performance-wise. The types of query that I typically run involve joins of 10-20 tables and can run for quite long, one to several minutes being no exception at all. Only very few users access the database at the same time (5 being the maximum). I moved the server from a Lenovo Thinkpad T61 with a 2.2 GHz Dual Core and 4 GB of RAM to the following brand-new machine with hand-selected components: 

My deletes in Oracle 11 are very slow. The tables are linked by foreign keys and every foreign key constraint has a delete cascade set on it. If I hit the explain button on a statement like this it only shows my the involvment of the even if 30 other tables are involved. How can I get a more realistic result? 

My question targets Postgres, but answers might just be good enough coming from any database background. Are my assumptions correct: 

I am using Postgres 9.2.1 and am saving my archivable WAL over a NFS share. I just use the basic command, given as an example in the postgresql.conf 

I'm setting up a pair of PostgreSQL servers. Because cash is short I can only afford one high class server and some crapy backup box. If the expensive high end box should catch fire, operations can live with the slower backup system for a while. I was wondering how much performance does the backup server need? It receives WAL and only has to apply it (I prosume that is the way syncrhonized commits work?). Is WAL always easy to apply, or would for example a delete with many checks on foreign key constraints, also have to do these constraint checks on the slow backup box? Is the PostgreSQL WAL logical or image based? I'm not looking for absolute numbers, more a kind of answer like: "Applying WAL over TCP in sync commit mode on the hot backup will be cheap in any szenario, except for blahblah". 

Then I want to run join queries against this table and multiply the mb_transferred field with a weight factor. I also want to add fields like "transfer_duration" to the logs table to calculate the speed of downloads and run queries to get statistical data how how well/bad the connection between certain networks, or certain servers for certain hosters is. The point is. The data structure is simple, its just a huge amount of rows. I have a lot of aggregation functions. This makes a light bulb in the "map reduce" section of my brain flashing. I thougth about doing vertical shards and use client_id as a breaking point. For example if I have 10 server send every user to its userid mod 10 server. This would be easy and relieve the load. But scaling will probably be awkward. So i think with the size of the project that I am expecting to reach soon with the current growth I cannot do anything but turn towards a distributed database system. I already tried to examine cassandra, project voldemort, amazon dynamodb and hbase but no matter how much I read I seem to run against walls. I think the long years of relational thinking are somehow blockading my mind. Can someone point me into the right direction on that? What database system would be suited for my use case and why? 

I've got a master and a slave postgres 9.1.3 database. I would like to init the backup database for the first time. I did the following steps: 

Does the cp command somehow secure that my backup server, which reads the WAL archives, doesn't read half copied WAL files? Do I have to manually define a command that lets the file end with and then call afterwards to give it its proper name? Has someone got a example of an save ? 

When setting up a system is it best to have all blocks at 8k? Or do the settings not real matter? I was also wondering if some "wrong" block size settings could endanger data integrity in case of a crash? Maybe if a Postgres 8k block has to be split onto multiple disk blocks? Or does nothing get batched together, and therefore I loose disk space with every mismatch between defined block sizes? 

Disable sync commits restart master master pg_start_backup stop master copy files start master master pg_stop_backup (docs say "don't do that when sync commit = on", I guessed the master resends all files in pg_xlog when it starts?) Enable sync commit restart master create recovery file on slave start slave 

This process seems to work, but I've got the feeling something is wrong, because it's been a few hours now and there are still and processes on the slave. The slave should be up after a few seconds because it already consists of all needed data... I don't archive my WALs. Do I have to use before copying the files to the slave? I tried this: 

I run a service where I deliver a lot of downloads that go over 1 extra hosts between the destination and origin host. They are represented by the integer interpretation of the 32 bit IP address. My system currently handles about 500 inserts/second during peak hours. I run a master-slave system. The master has an apache webserver with a PHP file that gets called from remote hosts and inserts a line into the log table. Then the changes get replicated to the slaves where queries happen. My queries are primarily aggregations over the mb_transferred field over a range in the time field filtered by client_id. 

The maser server runs an apache webserver with a simple php file that does the insert and is called by other servers. My server is now almost at the limit. I already upgraded to big hardware. I thought about using a GUID as primary key and using master master replication, that will for sure relieve something, but I think its short sighted, because it does not decrease the insert amount per server. I am expecting higher trough-puts in the future and I am also worried about database size. Also in future I plan to have a second table which defines "weights" for certain services. Something like: 

I am currently running a MySQL Database for logging and analyzing those logs. My current table schema looks like this: