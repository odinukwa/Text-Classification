I haven't thought about the example that you describe, but here is a different kind of example. Consider a topological group $F$, and let $M$ denote the monoid of weak equivalences $F\to F$, so there is an evident inclusion $F\to M$. Let $X$ be any based space, and let $i\colon X\to CX$ be the inclusion of the base of the cone. Now suppose we have a based map $f\colon X\to M$. Take two copies of $CX\times F$ and identify $(i(x),u)$ in the first copy with $(i(x),f(x)(u))$ in the second. This gives a space $E(f)$ with evident maps $F\xrightarrow{j}E(f)\xrightarrow{q}\Sigma X$. I think that this is always a fibration up to homotopy. If the map $f$ does not factor (up to homotopy) through $F$, then we have a fibration that is not classified by a map $\Sigma X\to BF$. Note here that $f$ is supposed to be a based map when we take the identity as the basepoint in $f$. One way to define such an $f$ is to take a based map $m\colon X\wedge F\to F$, and put $f(x)(u)=m(x\wedge u).u$. For a minimal example of this, take $F=S^3=SU(2)$ and $X=S^1$. Let $\eta\colon S^3\to S^2$ be the Hopf map, and take $m=\Sigma\eta\colon X\wedge F\to F$. The resulting map $f\colon X\to M$ is nontrivial in homotopy, but $[X,F]=\pi_1(S^3)=0$. 

For any finite poset $P$, we can consider the poset $sP$ of nonempty chains in $P$, ordered by inclusion. There is a morphism $m_P:sP\to P$ sending each nonempty chain to its largest element. If you modify the category of finite posets by 

I will first refine kantelope's analysis slightly. We want $(ad-bc)(af-be)(cf-de)=1$, so the three factors must be $\pm 1$. For the moment I will consider the case where $(ad-bc)=(af-be)=(cf-de)=1$. This means that $a$ and $b$ are coprime. Let $(u,v)$ be the smallest pair of strictly positive integers such that $av-bu=1$. Any other such pair must then have the form $(u+ia,v+ib)$ for some $i\geq 0$. Thus, there must be integers $i,j\geq 0$ such that $(c,d)=(u+ia,v+ib)$ and $(e,f)=(u+ja,v+jb)$. This gives $$cf-de=(u+ia)(v+jb)-(v+ib)(u+ja)=(i-j)(av-bu)=i-j,$$ so we need $i=j+1$. Thus, the number of solutions starting with $(a,b)$ is the number of $j\geq 0$ such that $u+(j+1)a\leq m$ and also $v+(j+1)b\leq m$. This makes it easy to write code to calculate the number $G(m)$ of solutions for any given $m$. It appears from the numerics that $G(m)/m^2$ converges to a limit which is about $0.6$. However, I have only calculated as far as $m=1000$, which is not far enough to see whether there might be logarithmic corrections. 

Is there a method to find a tight upper bound on the given integral? Note that the integral is upper bounded by $\sqrt{\pi/2}$, and thus converges. I first thought about applying Laplace's method. However, the function $-x^2/2-a(1-e^{-x})$ is decreasing and achieves maximum at $x=0$ which is an endpoint of the domain of integration. As a result, I don't think using Laplace method is a good idea to find an upper bound. Numerical evaluation indicates an asymptotic of $\sim \frac{1}{a}$, but I am not sure how to proceed to say anything about an upper bound in terms of $a$. Any ideas? Thanks in advance. 

So basically I am asking, is it always best to evaluate the gradient at the point $x^n$ itself, or there can be transformations $h$ which might depend on the structure of the function $f$, that may help in faster convergence of the method? Can anybody refer me to any result in the literature that might have a link with this question? Any help is appreciated. Thanks in advance. 

Any reference to literature and existing techniques will be highly appreciated. Thanks in advance. Edit: Another observation that I have made is that, if we have some $x^\star$ such that $\nabla f(x^\star)=0$, we can write the following $${x}^{n+1}-{x}^\star=({I-AG}({x}^\star,\ {x}^n))({x}^n-{x}^\star)$$ where $$G({x}^\star,\ {x}^n)=\int_0^1 \nabla^2f(x^\star+\tau(x^n-x^\star))d\tau$$ Then, analyzing convergence of the sequence $\{x^n\}$ is equivalent to finding suitable conditions on the minimum and maximum eigenvalues of $AG({x}^\star,\ {x}^n)$. Does the function $\|x-x^\star\|_2$ then qualify as a Lyapunov function? Even if it is true, I can not find an analog of $f(\cdot)$ which acts a Lyapunov function for the general case. I have read a few sections of the paper that @dohmatob referred in the comments, but I think I cannot find a Lyapunov function for this problem using the techniques introduced in that paper. The matrix $A$ is creating the problem. 

Let $\{x_n\}_{n\ge 0}$ be a sequence of reals such that $x_{n+1}=g(x_n)$, where $g:\mathbb{R}\to \mathbb{R}$ is a continuous function such that $0$ is a fixed point of $g$. My question is the following: 

It is a fairly formal argument to show that these are all the same, and so $$ X_nY = \text{colim}_{i,j}\pi_{n+i+j}(X_i\wedge Y_j) = \text{colim}_i \pi_{n+2i}(X_i\wedge Y_i) = \text{colim}_i X_{n+i}Y_i = \text{colim}_i Y_{n+i}X_i. $$ If you want to work exclusively with naive prespectra, you can use the "handicrafted smash product" $$ (X\wedge Y)_{2i} = X_i\wedge Y_i $$ $$ (X\wedge Y)_{2i+1} = X_i\wedge Y_{i+1} $$ It is unpleasant to prove any properties based on this definition, but it is adequate for the calculation that you asked about. 

I have put some notes about operads at $URL$ They are not finished (in particular, very many references are missing), but sections 13 and 14 are in reasonable shape, and they should answer your questions. They are written in terms of symmetric operads, so my $K(n)$ consists of $n!$ copies of the Stasheff polytope. I have also put another note at $URL$ This one shows that various things I had hoped to do with the Stasheff operad are probably not possible. 

Note that any continuous surjection from a compact space to a Hausdorff space is automatically a quotient map. Also, there are 'space-filling curves', which are continuous surjections from $[0,1]$ to $[0,1]^2$. This means it is not very difficult for a space to be a quotient of $\mathbb{R}^n$ or even of $\mathbb{R}$. In particular, any connected Hausdorff second countable manifold will be such a quotient. 

I think that the simplest nontrivial case comes from the maps $$ S^5 \xrightarrow{\Sigma^2\eta} S^4 \xrightarrow{2\iota} S^4 \xrightarrow{\Sigma\eta} S^3 $$ Both two-stage composites are trivial, so we get a Toda bracket which is a subset of $[S^6,S^3]$. This group (or at least the $2$-primary part) is cyclic of order 4, generated by an element called $\nu'$. One way to describe $\nu'$ is to note that $S^3$ can be identified with the group $SU(2)$, so we have a commutator map $S^3\times S^3\to S^3$. This sends $S^3\times 1$ and $1\times S^3$ to the basepoint, so it induces a map from $S^6=S^3\wedge S^3$ to $S^3$; this is $\pm\nu'$. Alternatively, using $SO(3)=S^3/\{\pm 1\}$ we see that $\pi_3(SO(3))=\mathbb{Z}$, and we can apply the unstable $J$-homomorphism to a generator to get a map $S^6\to S^3$, which is again $\pm\nu'$. Toda showed that the above Toda bracket is $\{\nu',-\nu'\}$ (so the indeterminacy is $\{0,2\nu'\}$). If you are happy to work stably, then the relevant group is $\pi_3(S)=\mathbb{Z}/8.\nu$, where $\nu$ is the quaternionic Hopf map, and $\nu'$ becomes $2\nu$, so $\langle\eta,2,\eta\rangle=\{2\nu,-2\nu\}$. However, if you want to build intuition then then you may well be better off thinking about Massey products; these are closely analogous to Toda brackets, but you can write down examples by pure algebra. 

a. Euler's computation of Zeta(2), (at first with only very weak, handwaving, or no convergence arguments), as redacted in Polya or here. Obviously amazingly ingenious, and interesting for artists, connecting numbers and circles. Requires unerstanding that a polynomial is equal to the product of its first degree factors, possibly it is too well known, however. It also allows one to then revisit the convergence argument and show what mathematicians actually worry about, after the "flash of ingenuity" b. A lovely agument I heard a long time ago on sci.math relating to Sagan's book "Contact" in which a message is encoded in the bits of Pi. Someone asked whether a deity could arrange for Pi to be a different real number. Opinions went back and forth relating to spacetime, etc. Then someone asked, in light of any of the familiar series expansions, "If Pi were different, which natural number would be missing or duplicated, and how might that be?" Leads to a discussion of the Peano axioms. Everyone goes home wondering. :-) 

Stellar aberration, the change in the apparent positions of more or less most of the night sky with the seasons, directly due to the velocity of the earth in its orbit. Note - aberration, which is related to the idea that raindrops appear to fall slanted in a moving vehicle, is not the same as parallax, e.g. has no relation to the distance of the stellar source, etc. :-) See $URL$ . Predicted by Bradley in 1725. First measured > 0 by Bessel in 1838, with unpublished successful observations by Henderson 5 years earlier. See $URL$ . 

Isn't the point that human reason is generally frail altogether, especially when making conclusions by using long serial chains of arguments? So in mathematics where such extended arguments are routine, we want their soundness to be as close to ideal as possible. Of course, even generally accepted proofs are occasionally later seen to be lacking, but to give up proofs as the ideal changes the very nature of mathematics. I heard that the example of parts of the Italian school of algebraic geometry of the 19th century was an important example of this overextension of intuition. Furthermore, it is only in the attempt at proof that the real nature of the reasons why a statement is true are finally exposed. So the reformulation and refoundation of algebraic geometry in the 20th century is said to have exposed revolutionary new ways of seeing mathematics in general. Finally, it is only by proof that the limits of applicability of a theorem are really understood. This comes into play many times in physics, say where some "no-go theorem" is elided because its assumptions are not valid in some new realm. 

For example, let $g(x) = x^2$. If $x_0<1$, then we have $x_n = x_0^{2^n}\stackrel{n\to \infty}{\to} 0$, and the rate of convergence is quadratic. Note that if $g$ is Lipschitz, then I can always find an initial condition suitably so that the sequence converges to $0$ linearly. However, this estimated rate would only be an upper bound on the actual rate, as it was seen for the $x^2$ example, the actual rate is quadratic and is thus very fast. Please provide some references to relevant literature. Thanks in advance. 

Let $\{X_k\}$ be an ergodic process. I know that if $f$ is a smooth real valued function then by Birkoff's ergodic theorem, $$\lim_{n\to \infty}\frac{1}{n}\sum_{k=1}^n f(X_k)=\mathbb{E}(f(X_1))\ a.s.$$ Is there any similar result for the $\limsup_n$ or $\liminf_n$ of the sequence $\{f(X_k)\}$, i.e. results which involve expectation? More specifically is there a way to find $$\limsup_n f(X_n),\ \liminf_n f(X_n)$$ almost surely? Though I have asked the question for the general ergodic processes, even results for stationary processes like Gaussian processes will be helpful to me. Also, if they are not available, it will be very kind if someone can give me some references that I can use to find methods to find these results. Thanks in advance. 

Let $\{X_n\}$ be an ergodic sequence of random variables, $X_n:(\Omega,\mathcal{F})\to (S,\mathcal{S})$ where the target set $S$ is a matrix ring. My question is, 

You can try applying gradient ascent method and run it for a few iterations to get an initial guess for $\mathbf{t}$. As the function is strongly concave, the gradient ascent is sure to converge to its unique global maxima, and so running it for a few iterations will produce a good approximation for the true global maxima. Actually, by analyzing the gradient ascent steps, you can estimate how many iterations you need to run it for the approximation to meet a prespecified approximation error required for the Newton-Raphson/Halley method to succeed. See this for such a discussion. 

Just an idea. I think the best way to start is to expand on the structure of the matrices $D, C$. For example, $D$ can be readily seen to be expressible in the form $$\alpha M_1\otimes I_2=\alpha\begin{bmatrix} 0 & \mathbf{0}^T\\ \mathbf{0} & J \end{bmatrix}\otimes I_2$$ where $J=diag(1,2,,\cdots)$. I think the structure of matrix $A$ can be expressed as $\beta M_2\otimes B$ where $$M_2=\begin{bmatrix}0 & 1 & 0 & 0 & \cdots\\ 1 & 0 & \sqrt{2} & 0 & \cdots\\ 0 & \sqrt{2} & 0 & \sqrt{3} & \cdots\\ 0& 0 & \sqrt{3} & 0 & \cdots\\ \vdots & \vdots & \vdots & \vdots & \ddots\end{bmatrix},\\B=\begin{bmatrix} 0 & 1\\ -1 & 0\end{bmatrix}$$ Now the problem becomes finding $(\epsilon\beta M_2\otimes B+\alpha M_1\otimes I_2 )^{-1}.$