Potential pitfalls to watch out for are the data types, size and collation discrepancies between data_table.field1 and geography.Area_Code. If geography.Area_Code is unique and must always be present make sure you stick a primary key on it. It is probably worth adding an index to data_table.field1. Again, if it is mandatory and unique make it a primary key. 

Failing that try putting an index across timestamp, experiment_id and bucket_label. Again, you had best do this in quiet/down time and make sure you have the physical resource available to do it. On a separate point be very careful using field names that are reserved words such as timestamp. You can get some peculiar exceptions thrown in applications that are very hard to track down. 

It depends how complex your spreadsheet is, how many worksheets, whether someone has put all sorts of merged cells, fancy titles etc I've had some success using Apache Tikka as a content extraction tool with basic Linux bash utilities such as grep, awk, sort etc. I've had to do this to determine which spreadsheets might contain GDPR sensitive data. Tikka can extract data from over 1400 file formats and is a JAR file that can be called just like any other Java program. The useful output from a spreadsheet will be tab delimited. The name of the sheet will without leading tabs. Cells will be separated by tabs and the first column in any sheet will be prefixed by a tab. This makes it really easy to grab what output you need and use the MySQL COPY FROM statement to ingest it. 

If you want to see the number or purchases per buyer and date, you would add the column to the SELECT as well as the GROUP BY: 

As an alternative to @AaronBertrand's solution (if you can't or don't want to create an indexed view), I would recommend you to create an index on . If this type of question is very common on your table, this should probably even be your clustered index. I would not generally recommend high-selectivity indexes as a general "best practice", but rather look at what index will give your query the best performance. An index on will give your query a highly optimized, non-blocking query plan with Stream Aggregates. 

Trying to stay database-neutral: Reading, filtering Indexes radically speed up ordering and filtering operations on a table - often by a factor of 1000 times or more. Compared to a phone book, an index lets you look up a single person up directly, because it's alread sorted alphabetically. If the phone book were just an unordered list of a million names with their phone numbers, you'd spend a month to find a single phone number. Inserting As a natural consequence of keeping an index organized, it adds overhead to any change you perform on the data. To continue on the phonebook analogy, if you add a name, you're going to have to insert the name in the correct alphabetically order, and this takes more time/work than just adding the record to the end of the table. Updating An index will vastly improve the speed at which you find your data, but if you change a value in an indexed column, the data will have to physically move in the table in order to maintain the correct order. Deleting Again, the index will help you find the record very quickly, compared to looking for the correct record in the entire table. Normally, a delete won't reorganize the index - it'll just leave a hole where the row was, though this may be different between database servers. In summary Changing data in an indexed table will take longer, while selecting data will be quite much faster with proper indexing. Like @ypercube says, over-indexing not only slows down change operations, it also forces the server to choose the correct index, which will take a long time if there are a thousand choices to go through. There are fringe cases where you may not want to index a table: For instance, when you need to insert a large number of records, and you have no interest in filtering or ordering those records once you read them. I would, for instance, consider this for a fact table used (non-incrementally) for an OLAP cube - it gets populated once, and read in its entirety once without any particular sort order. 

The majority of the unmatched objects will be Undo. You can join with to identify the Undo segments involved. 

Given your query, you have the following: Owner (Schema Name): Table Name: The user will not just be able to select from unless there is a Public Synonym pointing to it. So, to select from the table you have to fully qualify the name by prefixing the table with its schema: 

.. to reload the kernel settings. That'll set the maximum shared memory segment size to 2Gb & should solve your issue. 

The answer to your question is that it won't let you. Oracle is stricter than other RDBMSes and you'll get an if you try a with a reserved word. However, you can force it to do so by surrounding the name in quotes, for example: 

In other words, one of your control files might be corrupted. They are supposed to be identical. To fix this, we'll try each control file in turn to see if the database starts OK with it. Follow this step-by-step, to the letter. MAKE SURE YOU BACK THEM UP, AS BELOW. You have been warned. Make a backup of BOTH control files to a location of your choice. EG: 

"Efficient" could apply to log file usage, I/O performance, CPU time or execution time. I would try to achieve a minimally logged operation, which would be fairly efficient from a logging perspective. This should save you some execution timeas a bonus. If you have the tempdb space, the following might work for you. 

This should give you a nice, efficient seek (range scan, really) on (SoftwareId, SampleDate). I removed the expression and just sorted by the expression instead. It'll make your code a little more readable and perhaps even save you a few microseconds of CPU time. :) Just crossing Ts and dotting Is, here's an index recommendation on as well, although I suspect this table doesn't really contain that many rows: 

Explanation, using an example: assume : The common table expression () will match this with three rows in . 

I think that this is a business-driven question that your client/management should decide. Most well-designed databases can scale a lot larger than you may think at first. It's good that you're thinking of an archiving strategy, but you should really only build this on an actual, outspoken business requirement. I'm almost certain that with a little index tuning, your server will cope for decades to come. On a side note, don't confuse archiving with data warehousing (sorry if you think I'm nitpicking). You can archive old data from your e-commerce database, perhaps into a database that has more or less the exact same schema as the production database. A data warehouse could function as an archive as well, but it has a different schema and purpose than the production database, in that it is optimized for reporting and analytics. Remember that whether a data warehouse functions as an archive will affect your choice of backup strategy, etc. 

The grant worked just fine. You can't DESC a package or procedure, which is why you're getting an error. Documentation for UTL_FILE is here. It provides sample PL/SQL that shows you how to use the package. A short code snippet that sums up its functionality is here. 

The proper way to do this is using and . Note that for this to work properly, your query will need an clause so that the rows are fetched in the same order each time. Your first query would be: 

Can probably be done with some simple maths along with the source data, but my brain is a little slow this evening ;) 

There's a massive difference. TimesTen is an dedicated in-memory database. Oracle in-memory is just an added optional extra for Oracle 12c that lets you mark data, given certain indexing rules (you drop them), as in-memory. Oracle then caches the data in memory in both a row and columnar fashion, enabling fast retrieval for both OLTP and reporting workloads. It requires no code changes for existing Oracle-based applications - completely transparent & can offer significant performance improvements. 

We faced exactly the same scenario. Our solution was to have a SQL Server Agent job with a steps to call stored procs co-ordinating the delete/archive. To make our lives easier we had the application that persisted the data in the first place write a DateTimeCreated field in each related table with exactly the same date/time. For example an ProductEnquiry record and associated ProductResults record would get exactly the same date/time. Our clustered index was on that DateTimeCreated so DELETE FROM ProductResults WHERE DateTimeCreated BETWEEN... made use of the clustered index. As the database grew we found that the purge jobs had to have a pre/post step to disable/re-enable the FK constraints. We also had to start using a loop so we purged 50,000 records at a time and kept going until there were no more qualifying records to delete. When we move to Enterprise edition we started using partition switching which had a dramatic performance improvement on purge activity and massively reduced IO on our SAN. 

A VARCHAR(8000) column won't be indexed as you can only index up to 900 bytes prior to SQL Server 2016 so you don't need to worry about indexes. You need to consider who and what will be accessing the table when you make that change. As far as SQL Server is concerned such a change is considered a change of data type so will take longer than a change in VARCHAR size which is considered a metadata change. What is happening under the hood is that the data in your VARCHAR(8000) column will be shifted out of row and your record will now have a pointer to the data represented by VARCHAR(MAX). Time to execute will depend on many things such as 

This assumes that you've already partitioned the presented disk(s) (and will be using ), and that you're using . There will be a kernel module loaded if you are: 

.. it's already configured to only listen on . If there are lines with other IP addresses before the , it means that it's listening on those interfaces. To change MySQL to only listen on , edit your configuration file (usually ), add the following: 

It only allows access to data dictionary views, so the only security implication is that the users(s) would be able to see which objects existed in the other schemas. They would not, however, be able to actually view the data in the other schemas. I said there was only one security implication, but another is that they would be able to view the source of any stored procedures/functions in all schemas, by viewing etc. 

To change the behaviour, download the source for the mysql CLI and modify the handler to behave as you see fit, then recompile & install. 

You can do this using the MySQL Workbench Migration Wizard, which was introduced in MySQL Workbench 5.2.41. There's a blog post on the MySQL Workbench site here that details how to perform a migration. Note that it will not be able to migrate more complicated database features. 

As a BI consultant, my view on datawarehousing is that it provides (primarily) non-technical users with an easily accessible set of facts and dimensions. Often, you'll see the following features in a data warehouse: 

For the third one of those four to use an Index Seek, you're going to need a second index on , though. Here's how your query might look with these changes (including my refactoring of the query to make it more readable). 

The order in which tables are joined isn't defined in the query (unless you use a join hint, which I wouldn't recommend), so you can't be sure that A is joined to B before B is joined to C. To answer your question, in most cases SQL Server can tell from the statistics on the table (or other criteria) that the query won't return any rows. In that case, another JOIN won't make any difference from a pure performance perspective. 

defines a partition that spans over all of the rows for the current ItemCode and FiscalYear. Within this partition, you define a window using "rows unbounded preceding" that spans from the first FiscalMonth up to and including the current FiscalMonth. Those are the rows for which you the starting quantity and the period change. All of this assumes that the starting quantity is given for the first month of each year and zero for all other months. Now, all you have to do is apply a to this in order to get the output in neat columns. If you're on SQL Server 2008 R2 or Azure SQL Database, ordered window functions aren't supported. You'll have to resort to a much less pretty join-based solution, which also comes with a potentially hefty performance penalty. Something like: 

db_datareader is a built in role that grants read access to everything. db_datawriter is its equivalent for writes. If you want to grant limited CRUD access then you need to create a new role and use the GRANT statement to assign permissions to that role. After you have done that you can use the statements above to make your user a member of your new role as well. 

For the stored procedure part of your question I would set up an explicit role for stored proc access. I would keep this separate to MyLimitedCRUDRole as the visibility of what MyLimitedCRUDRole is of increasingly high importance in a GDPR world. I would also advise having roles that have clear and single purpose for clarity. 

It is possible to generate the GRANT statements dynamically but on any database with a security sensitivity I would be very careful doing so. 

Let us suppose that you have built a reference data set that has two (or more) fields. For the sake of argument lets call the table geography 

The technical limit on the number of columns depends on the engine. InnoDB allows 1017 in MySQL5.7 Those 1017 columns can cover a maximum of 65535 bytes. Records are stored on "pages". InnoDB allows the page size to be configured to 4, 8, 16, 32, 64Kb. Your record must fit on a page so you can't stick a 5K record on a 4K page. The problems with having wide records is that when the DB engine retrieves records it does so in pages. You can get few wide records on a page so retrieval performance decreases. DBs pull the results through memory so subsequent retrievals will see if the data remains in memory before falling back to storage. Having many records on a page means that the first physical retrieval of a page is more likely to load into memory records which can be logically (and much faster) read from memory. From a design perspective it depends on what your use case is. In an OLTP system then I would feel uncomfortable with 450+ columns. A database is not a dumb store. It can be used to enforce rules on the structure of information and the relationships between different data entities. This is an incredibly powerful weapon to have in your arsenal. In a data warehouse supporting certain analytical systems 450+ sounds like a lot however I have seen some wide denormalised tables used to feed OLAP cube technologies. If I saw a 450+ column table I should also ask questions about security. When I grant access to that table do I want everyone with access to have access to all 450+ columns? In addition to storage efficiency/performance normalisation can also factor in a security design. Consider performance. Of those 450+ columns which ones get retrieved the majority of the time? Do you really want to have the expense of retrieving 450+ columns if only 32 are used on a regular basis? The answer I have given assumes that InnoDB (the default) is used.