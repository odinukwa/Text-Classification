This is a really tough problem in the US. Names are not unique and often change during a person's lifetime or are presented differntly (Rob versus Robert for instance), so they can never be used to identify the patient except in conjunction with some more realiable information. Health insurance number and provider changes much more frequently and may be the same for multiple members of the family. SSN is supposedly unique, but there is fraud around it. Same with Driver's liscense number which of course not everyone will have. Personally, I would start with insurance policy number and date of birth and name combination, then ssn and date of birth and name combination. I would check address and phone to give me additonal assurance when they match but not much weight if they don't. Additonally I would use blood type as a rule out factor if it is known (and we all know the hospital vampires will be taking blood samples) as that doesn't change. Name matching would have to be fuzzy match due to the name varaition problem. Other things should generally look for an exact match first themna fuzzy match if the name confidence is really high (could have been a typo entering the SSN). 

Well we tend not to care what the originators table structure is, but only if it meets our requirements (which we send to them). If you are trying to figure out how to design a way to store the data permanently because you don't currently have a structure, then this is the method I use. Import the file into a staging table (not the final permanent table, I highly recommend you do that anyway, so you can clean the data before moving it to its final location) that has everything defined as varchar(max) or nvarchar(max). Now you can examine each field and see what is in in, look for the max lentgth of each field, check to see if numeric fields contain only numbers etc. Then you will know from the data what types of fields you need in the production table. Since this is only the first file, I would tend to create my final table with slightly larger fields than the data indicates, so it doesn't fail on the second file you get. After you have some history, you can tighten them. If you use SSIS instead of bcp, there is a data profiling task you can use to see what the data is really like. 

MySQL queries are not case-sensitive by default. It is possible that you have created case sensitive tables when importing data. Check if you have collation, that makes it case-sensitive. Reimport your data then using . Also, if you have collation, it will make queries case sensitive. Your collations changed when re-importing data. 

For inserts, you can use . This lets you update certain fields if primary key is already used. The syntax would be something like: 

Secondary indexes (non-primary keys) in MongoDB and MySQL are very similar. Secondary indexes declare fields or columns to be sorted separate from the rest of the data, and use row identifiers to reference the rest of the row for a query. 

is file in your directory. The meaning of tilde in unix is current user's home directory. The values from different configurations are overwritten. So will overwrite was what defined in , and will overwrite what was in previous files. 

When you are copying these files without detaching the database first, you are risking to corrupt your backup in the event that will be synchronized during the copy procedure thus rendering the backup broken. 

I don't think there will be a noticeable difference. Query optimization has very little to do with the syntax of your query and a lot to do with the RDBMS query optimizer. The optimizer pulls apart your queries and optimizes it as it sees fit. 

This is likely because your User has status on your system. Users without administrator access should not be able to access these files. If you enable Guest User, you can login and try same thing with Guest. You should not get access to the files as guest or normal user without administrator rights. 

The only time I would ever consider a delimted list is if there is no chance you will want to look at the data separately. Then indeed it is faster. This is a very rare case, however. And in what you are doing, there is an approximately 100% chance you will want to look at individual payments separately. Delimited lists are harder to query (and updating if someone made a typo, ugh) for individual elements (and generally slower for this than a related table would be) and harder to maintain the data integrity. Databases can easily handle many millions of records in tables that join with correct indexing and design. If you need more performance at that point typically, you partition the database. Sit down and read some books on database design and performance tuning. The rules for good design are different in databases than applications. Please take the time to learn them. 

Deploying the stuff you need to move a job is pretty much just like packaging the stuff you need to move application code assuming that you correcly put all the new items and changes in source control. This includes your SSIS package, any object creation or alteration scripts such as tables, views, UDFs, stored procs, CLRS (Never ever create database objects using the GUI if you want to deploy later), any scripts to populate tables (such as lookup tables). You may need to number the items in the deplyment folder to ensure they are run in the correct order. Usually I write a deployment document as well because some of what we are deploying will go to differnt servers (Our ssis server is is differnt from our database server). You can also script out the job, but you will need to review and change the script for the new environment. I often find it just as easy to set up the job on the other server manaully (but I have dba rights to all servers, if you do not, you will probaly need to this). 

Otherwise you will have to drop the table (with the command above) and recreate it manually or from a backup. 

Yes this is normal. When RAM is no longer needed it is not freed at the same time. It is kept as cached in case the server would decide that it needs to access it again. This would save you extra time that you would otherwise need for data to appear in RAM. Cached memory is freed only when new applications request more RAM. 

What I would do, is compare both files with tool. can come in help here as it has ability to diff word by word and if you pass it option don't even have to place them in git repository. This command can help 

Every process under linux runs under specific user privileges. Services (like MySQL) usually need to open ports and access various system resources during startup, so they are required to be started as user. However, it is not safe to have all the processes run under as it is not required for continuous operation of services, thus it is recommended to create a special user, which will be used to run MySQL service. MySQL will only be able to access what special user can, and this is going to be limited to MySQL files on the system. This is usual practice in linux. If you, however, use you distributions built-in package manager to install MySQL, this will be done for you automatically (in most distributions at least). 

If you use , MySQL will use timezone which is configured in . If no value is set in , then MySQL will determine your operating systems' timezone and use that upon startup. You can see how your server is configured with this query: 

When you are building complex queries, you should build them in stages checking the results as you go, not build one whole huge query and then try to figure out what is wrong. Here is what I do. First I list all the columns I want on a spearate line and comment out all the columns except those in the first table. Then I add the from clause and any where conditions on the first table. I note how many records are returned. Then I add each table one at a time and check the results again. I am especially concerned when the number jumps up really high unexpectedly or goes down unexpectedly. In the case of the number jumping up high, you may have a one to many relationship that needs further definition. This is especially true if the field or fields you are getting from the table are almost always the same. You may need a derived table or a specific where condition to resolve. You might even want to do some aggregation. Now I'm not saying it always bad if the record counts go up, only if they go up when you didn't expect them to or when the result set looks suspicious. In the second case, you generally have an inner join where you need a left join. The number of records went down because you did not have a matching record in the joined table. I often check each inner join with a left join to ensure that I return the same number of records. If it does then the inner join is appropriate, if it doesn't then I need to decide if it is filtering records I should be filtering or if I need a left join. When I am not sure why the record counts are off from what I expect, I use a select * (temporarily) just to see all the data, so I can determine why the data is off. I can almost always tell what the problem is when I see all the data. Do not ever use select * in a multiple join query like this or you will be returning much more data than you need and thus slowing the query as at a minumum the join fields are repeated (plus it is truly unlikely you need every field from 20 joins!). To find you issue, you are going to have to repeat this process. Start with just the from clause and the where condtions on it and add tables until you find the one (s) which are causing the query to eliminate all records. Don't stop with the first find, it is possible in a long query like this that you have multiple problems.