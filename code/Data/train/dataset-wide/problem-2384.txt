mysql install my.cnf as template with all sections and commented help information for start work You generally will need only [client] section for connect use command line like 

user mysql - have not rights for execute plugin compare with other plugins(default rights after installation): 

Main idea of SSIS (and similar tools) - not only load data from server to server, but transformation and check on the fly, and some of this transformations hard (or impossible) made by SQL only tools. Bulk insert in SSIS mean load from text file, when You not need any transformation Linked Server is good solution if Your servers have good network connections, it allow You not worry about 3rd party tools (SSIS as well 3rd party relate to SQL Server engine). Other way - unload result of complicated SELECT into text file and use bulk load resulted flat file into destination server. 

You can search some other. Unfortunately at least my experience show - this products far from "ready to use" state. But this is possible direction for feature research. 

First of all, on 32Gb machine You give for InnoDB (which I hope You use primary, if not - time to start :-) ) 

not "ready to use" solution, but may be is what You need: $URL$ description as I explain in comment, this is framework, it not ready to use solution, but it has very good set of code examples, as well as live-demo. Live demo 

as correctly mentioned in comments - LEFT JOIN not proper choice in Your case, even if You fix the syntax (JOIN require ON condition) You need use INNER JOIN: 

main reason for this - triggers created by one user (for example root) and select run from other, which not have full rights - You must have Trigger privilege for display triggers from database. in other case - result will be empty. 

Add: Example with Names look like not realistic, but real situation - when You need request not name which is really 1 per person, but actual postal address for client with 10 years history. He can have 20 addresses, and You need return most resent 

and than continue have a huge buffers and cache - it more bad than good just add: do not trust mysql tuner, it is not know Your real loading and often give wrong advices. 

Think - the spatial indexes would be right, but just some tricks which we use on one search project with MySQL 5.5 Look for problem from other side - how many cities could be on 1 degree lat/lon? IF exclude some countries like Malta (where my son walking from Home to School cross 3 cities for 15 minutes) - common answer will be not too much. So, with index for lat and longitude queries like: 

In case when You do not want use MySQL bud scheduler, You always can call this stored procedure from PHP, cron, any other SQL script and etc 

correctly select other columns corrected for result, if table have more than 1 row with same MAX(column3) - query return 1 row for each MAX(column3) result. Example data: 

it automatically create dump with master position for more information You can check - $URL$ most important - NO DDL command must be run on server if You use single-transactions From Your description, You miss FLUSH TABLES WITH READ LOCK; before note Master position Add - if make dump from slave, You must be sure - no temporary tables open on this moment. For prevent this You as well can force use RAW replication format 

so, when You test it as described - it will use 30+- bytes per row, and this is will be similar with result additional source of information: 

As mentioned in comments (it is really answer for original question) - using function in WHERE - prevent any indexes using. First of all, You can check - how many of Your queries use form 

Server tuning it always iteration process, and affects of tune queries more proper and long term I can suggest You start from: 

Exactly NOT 10k! - it always bad idea, for each open table MySQL will spend memory, cpu time, disk resources and etc Much better give all this resources back to MySQL and it will manage queries for big table. With proper queries and indexes - millions and billions of rows in production databases. One single table or may be few depend from other logic. Possible solutions: 1 Table for active events Second for archived Other split ways like event types - also possible. But generally - for open table MySQL can spend same or more time than for search single event from VERY big table 

or just properly handle all queries: include in column list only really necessary column, request BLOB data after by second request with access by PK by the way add second biggest issue when BLOB stored in db - 

I not test pre-calculated columns on 1B rows tables, but use it on 50M+ work fine, so it must be tested 

What parameters are You use for Talend tMySQLOutput? What MySQL settings? It could not be faster then INFILE, but still really depend from settings. You can post Your question on TelndForge forum with screen short of Job components, will try to help For example one of my regular Talend-MySQL Jobs (billing prepare) - transfer from->to mysql 2M of records, 400Mb, it take 6-15 minutes depending from servers (we have 2 configuration) edit, because always better 1 time to show, than 100 times explain 2.3M rows, 300Mb file, speed of tMySQL with 100 rows per insert: 

JSON have optimised storage format compare with old method (save as TEXT), but no any other information You will not find, because JSON size always depends from JSON data, some of them could be more optimised, some less, plus different length, plus ... 

create user 'bhuvi'@'%' identified by 'bhuvi'; - You are create user with password grant all privileges on . to 'bhuvi'@'localhost'; - You create 2nd user without password 

DELETE data on linked server if user rights maped properly run the same as normal SQL DELETE command, with full object names: 

so not reduce number of rows in this case best choice let mysql use default key for JOIN and filter of rows will be by 

You can manual check password for HR (if You want - force SET it oracle) Firewall is not important moment, because Oracle return You exactly error text if all correct, it must be: 

ApexSQL Search - $URL$ free tools, search database object (for this tasks) - search code for columns name 

If You already use MySQL Workbench, You can use menu: Database -> Synchronise with any Source Before apply changes it show You script, which You can copy and if need edit You can test it on copy of databases Also there are many other tools: 

in additional to the answer about support utf8mb4 from MySQL 5.5, the problem with collation You can resolve by edit(replace) all utf8mb4 to other utf collation. If file huge, You can use sed command for change string without open the file - 

By default (if it not disabled) MySQL will save file to same folder as database, in my case - sakila 

Create tables in order - from parent to child (not always possible) Create all tables without constraints and add all of them after 

Add: Because Author again and again not trust to community :) let try to explain - why all suggest the same: variant 1 - as author want: substring + compare all other speed depend from substring, for example VARCHAR(200), it mean for huge database with long URL it on second step can compare thousands and thousands values variant 2 - using HASH any hash - will make hash from full URL, so 2nd step will work only for database where hash will have duplicates - trillions of rows by other words for 99,99999% of cases hash will return single line after first step - lookup over short column 

Uninstall latest MySQL - by windows tools Install 5.6.30 from previous step Copy folder c:/ProgramData/Mysql/data saved on previous step Start MySQL 

Persona XTrabackup - $URL$ Stop - copy - Start (preferred) Attach Slave server with ROW based replication, than at any moment stop slave - copy files - start slave 

must work for You When You use subquery, You must control - subquery must return 0 or 1 records, this is mandatory, for example like this: 

This is a trigger, which is same as function could be called multiply time at unpredicted for You time so the code like 

illegal for this type of code. Just think what would be if 2nd session start insert at the same time? (100% realistic situation in any multi user environment) For prevent collisions - this is not allowed. 

so, in table with 2M records, big part of them will be have Unix date > 0 not many domain was created before - 1/1/1970 and You sort all this records without indexes You can create or