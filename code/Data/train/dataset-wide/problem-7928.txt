The Leibniz integral rule, in its multivariate form, deals with differentiation of the following sort: $$ \frac{\partial}{\partial t} \int_{D(t)} F({\bf x}, t) \, d{\bf x} \, , \qquad D(t)\in \mathbb{R}^d \, .$$ I am looking for a fully rigorous formulation of this theorem, as well as a proper proof. So far, I could only find: 

Generalizations to broader function spaces then the analytic functions Generalizations to Lebesgue integrals with respect to other measures. Reminder theorems for continuously differentiable functions. 

You are right! the $sX\frac{P}{k+P}$ term is precisely the one that saturates. As $P\to \infty$, this terms goes to $sX$, and so $\frac{dX}{dt}$ remains bounded. Another way to look at it, is to solve for $X(P)$, yielding $$\frac{dX}{dP}=\frac{a+sX\left(\frac{P}{k+P}\right)-dX}{rP-hPX} \, .$$ For a given $X(0)$, As $P\to \infty$, you get that $\frac{dX}{dP} \sim O(\frac{1}{P})$ and so the growth in $P$ hardly effects the marginal growth in $X$. 

The Biharmonic nonlinear Schrodinger equation is of the form $$i\psi_t (t,x) + \Delta ^2 \psi + |\psi |^{2\sigma} \psi = 0$$ for some $\sigma >0$, with $x\in \mathbb{R}^d$ (d=1,2,3) and usually an initial condition $\psi(t=0,x) = \psi_0(x) \in H^2$. For $d=1$, you look for solitary waves in this equation, then by the solitary wave ansatz $\psi(t,x) = e^{i\omega t}R(x)$ you get an ODE of the sort you were looking for - $$-\omega R + R^{(4)} +|R|^{2\sigma}R = 0$$ Sometime we also look at the Biharmonic NLS (BNLS) as a pertubation for the NLS, something of the following form: $$i\psi _t + \Delta \psi - \epsilon \Delta ^2 \psi + |\psi|^{2\sigma} \psi = 0 \, ,$$where $\epsilon \ll 1$. In this case, solitary waves can also be considered in the same manner as before. 

Question: Given $f\in C^2 [a,b]$, and $s$ its "natural cubic spline" interpolant on some grid/knots $a= t_0 < t_1<t_2 < \ldots < t_n = b$, is there a bound on the number of extremum point of $s$? This bound may depend on $n$ and/or the number of local extremum points of $f$. If not, are there stronger coditions by which this is true? An easy condition on $f$ - If $f\in C^4$, then $|f'-s'|<C h^3$, where $h=\max\limits_{j=1,\ldots n} |t_j - t_{j-1}|$. So, if $|f'|>\alpha >0$ on $[a,b]$, then for sufficiently small $h$ we have that $|s'|>\frac{\alpha}{2} > 0$. So, to Sharpen the original question: Can anything be said about such situations where $f'0$ on finitely many points in $[a,b]$? Intuition: The natural cubic spline minimizes the curvature $\|s''\|_2$, and so it seems that the arc-length of the graph of $s$ is small. Therefore, it might imply that $s$ doesn't oscillate "too much", i.e., more then the function it interpolates, $f$. However, I could not find or prove anything of that sort myself, and a counter-example may as well exist. 

Upper and lower bounds on the smallest positive root. Upper and lower bounds of all real roots. Upper and lower bounds for all positive roots. Bounds on the roots of a general polynomial. Bounds for the specific case $q=p+1$. 

Inasmuch I collected from Szego's book on Orthogonal Polynomials and some talks around campus, the answer to (1) is only the classical polynomials, and to (2) is no. But I didn't find any evidence of it. EDIT - English Reference request: I was referred in the comments to Bochner's "Ãœber Sturm-Liouvillesche Polynomsysteme" for a complete answer. Can anyone refer me to a translation or a book-chapter that gives its own version, in English? Thanks 

I'm looking for a good introductory to Sobolev, preferably with an emphasis to their relationship to PDE's analysis. I have only seen thus far Giovanni Leoni's "First Course in Sobolev Spaces" which seems to me, from a first glance, more on the pure functional analysis side. Thanks Amir 

Consider the unit interval $I$ with a continuous probability measure $\mu$, and consider a smooth random variable $f:I\to \mathbb{R}$. We can define its cumulative distribution function and probability density function in the regular way. There is a number of known way to obtain the latter two functions using histograms, kernel smoothing and so on. Both methods rely on the statistically motivated assumption that we randomly sample $f$. Assume now that we have sampled $f$ on the Gauss-Legendre quadrature point $\lbrace \alpha_{i}^N \rbrace_{i=1}^N$, which are the roots of orthogonal polynomials w.r.t. $\mu$, $p_n (\alpha)$. We can now obtain an $N$-th order approximation of $f$ using $p_n$, $$ f \approx \sum\limits_{n=0}^{N-1} p_n (\alpha) \hat{f}(n) \, .$$ My Question: Is there an elegant way of approximating the CDF and/or PDF of $f$ only with the spectral coefficients $\hat{f} (n)$ and the value at tha quadrature point $f(\alpha _j ^N )$? Note that generally speaking, $f$ is not the same as its PDF. 

I can give partial answers to 3-4. One definition to finite-time blowup is: there's a $T_c>0$ so that $\lim\limits_{t\to T_c ^-} \|u(t,\cdot)\| = \infty$. What is the appropriate norm? When $F(u) = |u|^{2\sigma}u$, for example, there's both an $L^{\infty}$ and $H^1$ blowup for certain values of $\sigma$ and initial conditions in the respective normed space. For more details, see "The nonlinear Schrodinger equation: Self focusing, singular solutions and optical collapse" by Gadi Fibich, specifically chapters 5.5-5.6. From here it is apparent that there might be local well posedness in the aforementioned norms and a finite time blowup for the same initial conditions. So, it seems to me that the answer to question (4) as I understand it is that you can't generally use local well posedness to prove the global one. Edit: However, in the defocusing case $F(u) = -|u|^{2\sigma}u$ as well as the focusing subcritical case, $F(u) = |u|^2 u$ and $u:\mathbb{R} \to \mathbb{R}$, it is exactly the same local well posedness with which one proves global existence, see the reference above for details. 

Given a PDE, is there a general method to show that it is not solvable using the inverse scattering transform? Specifically, for the perturbed 1D NLS or the 2D cubic NLS, where was it first shown that these equations can not be solved using any form of the inverse scattering transform. 

Given a finite measure $\mu$ on $\Omega \subseteq \mathbb{R}$, under what conditions does the respective set of orthogonal polynomials are a solution of respectve Sturm-Liouville problems? In these cases, can we go back from $\mu$ to the differential equations, rather then from $\mu$ to the polynomials? 

consider the $(1+1)D$ or $(2+1)D$ NLS: $$ i\psi _t (t,{\bf x}) + \Delta \psi + |\psi|^2\psi = 0 \, ,$$ $$ \psi (t,|{\bf x}|\to \infty) =0 \, , \quad \psi(t=0,{\bf x}) = \psi_0 ({\bf x}) \, , $$ with $x\in \mathbb{R}^d$, $d=1,2$, $t\geq0$. Usually, the discussion about solutions or blow-up of the NLS is for $\psi_0 \in H^1$, and there is some work about "rougher" initial condition. My question: is what sort of analysis is done for the case of $\psi_0 = \delta ({\bf x})$. Obviously, this can not be an initial condition in the strict sense, but I do wonder if some work was done in this direction? I was referred to this work, but I will appreciate further references. 

I'm looking for a good introduction to the critical generelized KdV equation $$u_t +u_{xxx}+5u^4u_x = 0 \, , $$ $$ u(t=0,x) = u_0 (x) \, , \qquad x\in \mathbb{R} \, ,$$ and its blowup solutions. There are some papers on the topic, but I'm looking for more of an overview - lecture notes, video, book chapter or a review. Any ideas? 

In Leveque's "Numerical Methods for Conservation Laws", Ch. 3.1.1., he says that given a system of Hyperbolic PDE's and a point $(\bar{t},\bar{x} )$, its domain of dependence $D(\bar{t},\bar{x} )$ is always a bounded set. The domains of dependence here is defined by the minimal size of an initial data set needed to determine the solution's value at a given point. My question: Wikipedia defines hyperbolicity of a system by its eigenvalues. How can I proceed from this definition to prove the boundedness of the domain of dependence? Thanks 

I'm trying to find upper and lower bounds of the smallest positive root of a polynomial, stated in terms of its coefficients. As I appreciate it might be a very general problem, My specific interest is in polynomials of the sort $$ -ax^q + bx^p -c = 0 \, \quad a,b,c>0\, , \quad q>p \, .$$ I know that, under some restrictions, it has real positive roots, and so I'd be interested in either- 

Let $s(x)$ be the natural cubic spline interpolant of a function $f\in C^4$. There are known bounds on the $L^{\infty}$ error, $\|f^{(r)}(x) - s^{(r)} (x) \|_{\infty} $ for $r=0,1,2,3$. See Hall & Meyer, 1976, J. Approx. Theory. My question: Are there any known optimal bounds on $\|f^{(r)}(x) - s^{(r)} (x) \|_{2} $? Under what conditions? Could you refer me? If there are none, optimal rates could also help considerably. 

The one-dimensional case (see e.g., Courant calculus book). Physics-flavored proofs, where the normal speed $v_n ({\bf x})$ is not well defined (see e.g., here). While insightful, this is not what I need. Differential-geometry and form-based proofs (see the above paper). While these are valid, they are far more general than what I need. 

The Clenshaw-Curtis quadrature rule approximates an integral $I=\int\limits_{-1}^{1} f(x) \, dx$ by $$I\approx I_n = \sum\limits_{j=1}^N f(x_j)w_j \, ,$$ where the $x_j$'s are the roots of the $N$-th order Chebyshev polynomial, and and $w_j$'s their respective weight. To prove the accuracy of this integration formula, one usually goes by either Fourier representation of $f(x)=f(\cos (\theta))$, or by the "Fourier" expansion of $f$ in the Chebyshev polynomials. See e.g., in the Wiki page. My Question: Is there a way to prove the accuracy of this formula, which does not rely on spectral/Fourier theory? Specifically, to show that it is exact ($I=I_n$) for polynomials of degree $\leq n$, and to bound its error for $f\in C^n$. 

Is there a sense of distance between $\rho _1$ and $\rho _2$ that does not involve their explicit computation, but only of the functions $f$? Does that answer changes if both $f$ are smooth? If both their images are bounded? If $f_1(x) = x$, and $d\mu = dx$, and so $\rho _1$ is the uniform distribution? 

Background and details: The cubic 1D nonlinear Schrodinger equation (NLS) $$ iu_t + u _{xx} + |u | ^2 u = 0$$ and the KdV equation $$u_t -6uu_x+u_{xxx} = 0$$ are both known to be integrable, and solvable via the inverse scattering transform. So, given the initial condition $u(t=0,x)=u_0 (x)$, one can compute these constants and solve an inverse, linear, auxilary problem to find $u$ for all times $t$. For example, for the cubic 1d NLS this is the Zakharov-Shabat equations, and for the KdV it is the linear, time-independant Schrodinger equation. The 2D cubic NLS, or almost every perturbation of the 1D case, e.g., $$iu_t +u_{xx} + |u|^2 u -\epsilon |u|^4u = 0 \, ,$$ is known to be not solvable using the inverse scattering transform, i.e., not integrable. I didn't find any reference that explains why, however. 

Consider $f\in L^2(I)$, where $I$ is the unit interval and $L^2$ is w.r.t. Lebesgue measure, and consider an approximation of $f$ denoted by $\tilde{f}\in L^2$. The error in approximated the moments of $f$ by those of $\tilde{f}$ can be readily bounded by $\|f-\tilde{f}\|_2$, e.g., denoting the respective expectancies as $\mu$ and $\tilde{\mu}$, one has that $$\left| \mu - \tilde{\mu} \right| \leq \|f-\tilde{f} \|_2 \, , $$ and if we define the variance in the usual way, as ${\rm Var}(f) = \mathbb{E}[(f-\mu)^2]$, then $$\left| {\rm Var}(f) - {\rm Var}(\tilde{f}) \right| \leq \left(\mu +\tilde{\mu} + \|f\|_2 + \|\tilde{f}\|_2 \right) \|f-\tilde{f}\|_2 \, .$$ Questions: 

I'm using the Euler-Maclaurin formula in a research I'm working on. However brilliant is the elementary proof found here, I need and want to know more about it. Namely Specifically, I would like to get an integral-residue kind of formulas for functions which are continuously differentiable only on open intervals. To be precise: Consider $f:(0,1) \to \mathbb{R}$ a continously differentiable function, and define $$R^N_f := \sum\limits_{m=1}^{N} f\left( \frac{m}{N} \right) - \int\limits_0^1 f(t)\, dt \, . $$ If $f$ is continuously differentiable on $[0,1]$, then the Euler Maclaurin gives a precise value for $R_f^N$. If the integral on the RHS exists, but the function is not continuously differentiable on the closed interval - what can be said about the error term? More generally speaking, if no such result exists, I'm interested in 

Question: Let $X_1, \ldots ,X_n$ be $n$ iid uniformly distributed random variables, i.e., $X_j \sim \mathcal{U}(0,1)$ for each $j=1,\ldots ,n$. What is the PDF of the maximal distance between to nearby elements? More formally, if we denote $X_{(k)}$ as the location of the $k$-th smallest element, what is the PDF of $\max\limits_{k=1,\ldots ,n-1} d_k$, where $d_k := X_{(k+1)} - X_{(k)}$. The motivation for this question is approximation and interpolation on random grids. What I know: Each $X_{(k)}$ is distributed by a Beta distribution, but so far I was not able to go on from there. 

My motivation: Numerical computation of $\rho$ is notoriously inaccurate and problematic, and so KL divergence or even $L^1$ distance are problematic as well. I obtain $f$ anyway, and I really only need to know how "far" is $\rho$ from being uniform. Disclaimer: Having a "good distance measure" is vague, and can be interpreted in many ways. I know. But the ways to define it I already know involve the computation of $\rho$, so I'd rather leave it open-ended for now. 

Are these bounds tight? Is there a known formula for the error in $\mathbb{E}[(f-\mu)^n]$ for all $n\in \mathbb{N}$? Is there a known textbook reference for these inequalities? It seems like I'm hardly the first to have discovered them. 

Consider an interval $I$ with a smooth probability measure $d\mu (x) = c(x) dx$ and two known real measurable functions $f_1(x)$,$f_2(x)$. Both functions define a distribution on $X = {\rm Im} \, [f_1] \cup {\rm Im} \, [f_2],$ the distributions are denoted $\rho_{1,2}$, respectively. My questions: 

Background: Give an increasing set of points $(x_i)_{i=0}^n \subset \mathbb [a,b]$, a cubic spline $S(x)$ is a piecewise cubic polynomial with continuous second derivative. One can also prove, roughly, that if $S(x)$ is the natural cubic spline interpolant, it also the minimizer of $\int\limits_a^b (u''(x))^2 \, dx$ over all such $C^2$ functions. Question: I'm looking for a variational Euler-Lagrange kind of proof for theorems of this kind, i.e., how to build a $C^m$ interpolant that minimzies $\|Ku\|_2$ for some linear operator $K$. I did find this proof, but it is not variational and is only true for the natural cubic spline interpolant. 

If $\left( p_n \right)_{n=0}^{\infty}$ is a family of orthogonal polynoamials with respect to a measure $\mu$ on $[-1,1]$, and $\left( x_j, w_j \right)$ are the quadrature points and weights for the respective Gaussian quadrature rule, we can easily prove that $$ f_N (x) : \, = \sum\limits_{n=0}^{N-1} \sum\limits_{j=1}^{N} p_n (x_j) f(x_j) w_j p_n (x) \, ,$$ is the interpolation polynomial of degree $N-1$ for $f$ at the quadrature points $x_1, \ldots x_N$. My question: While I could prove it, I couldn't find a reference for this proof in any textbook. Could you help with that? This is cross-posted from this post in MSE. 

In General Relativity, one uses the Riemann Tensor in its coordinate form $R_{abcd}$, and proves the Second Bianchi Identity- 

It is known that for the classical orthogonal-polynomials there exist a set of Sturm Liouville problems. E.g. , the Hermite polynomial of order $n$ is a solution of $$y''(x) -xy'(x)+ny(x)=0 \, .$$ My questions: 

Background and notations: Given an interval $I\subseteq \mathbb{R}$ and a continuous finite measure $d\mu = w(x)dx$, and denote $p_n(x)$ the orthogonal polynomials with respect to $d\mu$. We have the following recurrence relation $$ p_j (x) = (a_j x +b_j)p_{j-1}(x) +c_jp_{j-2}(x) \, ,\quad \forall j\geq 1\, \quad p_0(x) \equiv 1,\,p_{-1}(x)\equiv 0 \, ,$$ where the constants are determined by the measure. In the famous Golub-Welsch paper, section 4, they give a numerical method to calculate the constants. However, it requires the numerical value of $\int\limits_{I}x^\ell w(x)\,dx$ for all non-negative integers $\ell$. The problem is that for numerical integration we usually need a quadrature formula, for which weights we need the recurrence relation (see the other sections of the same paper, for example). Question 1: Is there a way to compute $\int\limits_{I}x^\ell w(x)\,dx$ without any quadrature formula? Question 2: Is there a way to compute the recurrence constants without evaluating integrals? Remark: We can always evaluate $\int\limits_{I}x^\ell w(x)\,dx$ using the Gauss Legendre quadrature. This means that we need only the well-known recurrence for the Lebesgue measure to compute these integrals for all other continuous measures. I'm looking for something else, though. 

Extra Details: I've learned that the functions $e_k (t)$ are, in fact, the eigenvectors of the following integral Kernel over $L^2\left(\mathbb{R} \right)$, $$Ku(t) = \int\limits_{\mathbb{R}} \mathbb{E} \left[ X_t X_s \right] u(s) \, ds \, . $$ For more details, see this coincise introductory here. Thanks 

In short: For a given smooth or continuous function, how can we obtain the best $L^{\infty }$ approximating polynomial? Jackson (1911) proved that there is a best approximating polynomial in the $L^{\infty}$ sense. The proof Can be found in the references below. The theorem is Let $I=\lbrack -1,1]$, then there is a constant $C>0$ such that for all $f\in C\left( \bar{I} \right)$ we have: $\| f - \Psi_{\infty, n} (f) \|_{\infty } \leq C sup_{\left| x-y\right| < \frac{2}{n}} \left| f(x) - f(y) \right| $ Where $\Psi_{\infty, n} (f) $ is the best interpolating polynomial in the aforementioned sense. However, I couldn't find anything about what would be this polynomial, or how to build it. I would assume that some advance was made since, but I couldn't find it in these textbooks. Thank you References [1] Funaro, Polynomial Approximation of Differential Equations, theorem 6.1.2 [2] Davis, Approximation Theory, theorem 13.3.7.