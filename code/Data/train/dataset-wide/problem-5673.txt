From a genetic point of view focussed on group psychology, humans are not evolved to deal directly with nature. They are evolved to live in groups of people that selectively isolate them from nature. $URL$ Although there are strong survival forces toward correctly interpreting your environment, and those can favor a lack of objectivity on their own, social benefits and anthropomorphism of nature can more readily explain irrational biases of many kinds. $URL$ (So if you want a name, this is a component of what some economists and sociologists call 'Gesellschaft' - the focus on the goals of one's groups and society over individual goals as a determinant of who we are.) This perspective predicts several psychological biases that we can easily confirm experimentally. Three that I think interfere directly with our intuitions of probability include these: 

I agree this may not belong here. But I also have a guess to offer. So I am going to answer as though it fits. We are equipped with redundant identical machinery for almost all of our sensory apparatus -- two ears, two eyes, etc. It may be that it is harder to break the symmetry than to allow it. If we want to be able to transfer the acquired skills of the dominant eye easily to the other eye in case an eye goes blind, the easiest way is to set the whole system up symmetrically. In that case it is not work to acquire a sense of symmetry, it is work to create the distinction. I would conjecture that the less-evolved behavior is not to be able to tell sides apart in memory. I think we see this in folks who have a hard time learning and remembering right from left (myself included). And I think it is also a factor in referred pain, where one limb goes numb and the other one hurts in the same location. Also, some reflexes fail symmetrically, as where you see something peripherally and jerk the wrong way. Further, it is easier, once you know how to write with one hand, to learn how to write backward than forward with your other hand (try it sometime) -- the backward left-handed motions match the forward right-handed motions. Such failures are more likely to be falling back on the simpler system than to be over-correction by an acquired ability. So there is evidence for the idea that your internal sense of space is originally symmetrical, and you actually do work to keep it lateralized. Once the sense of symmetry is there, we know that humans use it as a major factor in our measure of beauty, as do other species. Since diseases and other hardships are less likely to affect the stronger side of your body as much as the weaker, good symmetry implies either good immunity from major diseases, or good skills of self-protection. Once deployed, the accidentally afforded ability would never be cleaned up as useless functionality. 

If we never chose anything we should not, by some objective standard, how would that be "will" of any variety, or "free" anything? The existence of suboptimal choices seems to put a lower bound on free will, rather than an upper one. We are free to be stupid, because we are free. On the other hand, the continuation of a total war is perfectly logical. If you stopped fighting to ask the question as to whether to sit down and talk, you would most likely be killed. You are on a treadmill with a minimum speed, and you can't get off unless you break it completely and you would most likely die in the process. In your example, WWII, at least one side was well-prepared for mass genocide. Making peace with them too readily might just get you collected up and done away with. There was no better option for them, since their entire agenda was total safety for a given race of individuals, in a way that would be infringed by the mere existence of anyone safer. When you have two choices, and annihilation is one of them, the other one is probably optimal. The ability to evade that logic and actually end the war despite the apparent logical loop you are in seems to indicate free will much more than the fact that the deterministic loop holds control as long as it does. 

In the broader symbology of logic, at this point one often whips out the 'box' which indicates some special interpretation of 'necessarily', in this case 'is believed'. ('Necessarily' because it is what seems necessary for this to be true in the mental world of the person in question.) So there is a distinction between 

The opening is unfair to Kuhn. He characterizes most of science as 'normal science', which is constituted by 'puzzle solving', and is therefore a form of cumulative progress. And it is clear that the candiates in a revolution are only those that already address a majority of the solved puzzles adequately. The result cannot be anything other than cumulative progress, with the occasional step sideways taking us a little bit back. He does not posit a driving force, but he does not rule one out. Kuhn was refining Popper, not denouncing him, basically by adding detail to the overly-abstract notion of falsificationism. He emphasizes over and over again that they 90% agree, but that the bald notion of a single judgement criterion does not fit with historical reality. Normal science is falisificationist in principle, even though it fails to be so in practice. And revolutionary science is done by normal scientists. So by sheer force of social psychology, it inherits all the biases of normal science that can be accomodated without leaving too few candidates. Feyerabend was right to characterize Kuhn as historicist, though perhaps in a weaker sense than dialectical stances promote. Any adopted method must be somewhat teleological. Dogma needs a selling point, because freedom is traded for something. That something, in the case of sciences, is the quasi-mythological promise of eventual convergence. (I put in 'mythological' because I think it takes on some religious character, not because I doubt its truth -- I am a convert.) You can see punctuated equilibrium and the selfish-gene theories as revolutions within Darwinism. The original theory involved a notion of gradual accumulation of adaptations and a sort of 'convergent design as sculpture' that became less tenable when we realize how discrete genetic information was. The modern version of plate-tectonics based upon subduction is also a (slow and low-impact) revolution that eradicated the two contending camps of geological theories that had been at war for centuries and replaced them with a genuine peace. So one might say that plate tectonics was the foundational revolution in geology, before which it was not a fully-formed science, but still had two rival schools and no single paradigm. The assertion of this paradigm was underway during Kuhn's life, but it took decades. It finally came together during the 1970's, after the publication of "Scientific Revolutions" and in the face of famous holdouts with sterling reputations. The creation of the discipline of psychology started an ongoing revolutionary process, which we are still in the middle of. Freudianism almost succeeded at being a paradigm briefly, and so did Behaviorism. So we have firsthand experience currently of how revolutionary processes take place and how normal science manages to assert itself during the process and coalesce shared results across paradigms, despite the lack of genuine traction. To my experience, the history of psychology to date backs up Kuhn. But it also suggests that the process has a sort of fractal, self-similar nature to it, where potential paradigms create local sciences with miniature revolutions as they seek to expand themselves onto contested territory to gain traction in the larger ongoing revolution. I also would contend Kuhnianism as a sort of 'percollational* synecdoche' (where the process writ large is made up of approximations to itself on various levels, so that each level provides a modelling metaphor for other levels), is an excellent model for politics, group dynamics and individual human thought -- a moderate improvement on sheer dialectics. * the word 'percollation' is Benoit Mandelbrot's word for processes like the growth of a sponge, which form self-similar solids within a large range of scales. (We probably need a better term, less open to coffee puns.) 

Since it has an 'only' in it, the normal form of the Categorical Imperative lends itself most convincingly to proofs by contradiction, so he mostly proposes a number of maxims that do not universalize. Most famously "Lie." fails to universalize because it is not possible for everyone to lie all the time. If you told only lies, you would not be able to establish the truth, in order to make sure you were actually lying. So if you made this truly universal, people would forget how to do it. Others of these include "Kill people." "Make threats." "Undermine other cultures." The example he chooses to open with is one of the few positive cases: "Increase your holdings through any safe means." 

There are matters of degree here. If you can accomplish what you have to contribute to the world without isolation, and be equally effective, it is probably more moral to do so. But there is a difference between 'not perfectly moral' and immoral. Most folks have argued that there are times when isolation is necessary, or where it pays off, and therefore there is nothing wrong with it. That does not follow. From most viewpoints, there is nothing wrong with eating steak, but one can still question whether it would be more moral not to. Or whether there is an optimally moral amount that most of us exceed. I would argue that connection with society has an inherent moral value, whether or not it promotes 'progress' of some variety. Simply being available and contributing your perspective to the mainstream process to the degree this is possible creates a different society, and any given perspective may in the end be a deciding contribution. To the extent that your perspectives are largely redundant, or simply balance out other existing perspectives, this is not a concern. But I would argue that being a person who contributes no genuinely unaccounted perspective is probably doing less than one might as a moral individual. There is an obligation, when born into a society to offset one's burden upon it. Part of that burden is intellectual, even if you are a common person with no vaunted intellectual ambitions. This is basically the "The existence of democracy, and the sacrifices implicit in maintaining it, imply you should vote" argument at a more detailed level. (So I can see how that puts me in a minority, given how Americans vote.) You cannot participate in the process if you aren't there, and the process itself has value, even if we cannot discern it. (That last proceeds from basically religious motivations, so I cannot defend it well. But I think a lot of moralities implicitly assume it.) At the same time, if society, its structure, or the availability of its products makes you less able to express your own gifts, or if you are the kind of person who degrades society inadvertently and cannot help yourself (I have known a lot of drug-and-or-sex addicts for whom both of those things are true, and an equal number of 'schizoid-construction' deep thinkers) you may actually be more obligated to stay away. Isolation is also not necessarily physical. I feel that modern society, although at the same time insanely gregarious, produces too much personal isolation for the good of the whole. Too many subtle contributions are suppressed. Physical isolation that produced less emotional and intellectual isolation would not be isolation in the sense of this argument if the result had any chance of making it back into society. 

Since one's own life has no measured financial value, the only price that is fair to everyone is zero -- which is what naloxone costs in my area. It is distributed by charities that support the families of overdose victims. This is the Salvation Army approach to alcoholism -- leave it legal, be clear it is stupid, and let people learn from experience, hopefully without destroying themselves too thoroughly. Given reliable overdose prevention and a modern view of the mechanics of addiction $URL$ there is no good reason why this remains a heavy moral question. There is still no sure-fire way of preventing alcohol poisoning that is as accessible as naloxone. (At least in Chicago's suburbs, but I think this is not uncommon.) And we still haven't cured cancer. So to some degree, I would argue, milder, but more addictive substances like alcohol and tobacco are more immoral to produce than heroin. 

Neither is direct substitution. Both create new rules or new interpretations, and substitution simply does not do that. But in both cases the new information is made out of old information, so you can look at it as a substitution under some kind of remapping or transformation. The difference is that metonymy uses the properties of other words to change the meaning or interpretation of a word, and metaphor uses the actual properties of things referred to, instead. An analogy from another discipline might help: A type declaration in a computer language is 'metonymy' -- it combines words to change the behavior of other words for clarity or convenience. It controls how things get combined at a single level. It never generates any meaning at a more realistic or deeper level than the empty realm of algorithmic encoding. The analogy between the class' use and the human process it represents is metaphor -- it means that when we think about the code, we are really thinking about the modeled behavior. It substitutes more realistic, more comfortable thoughts for more abstract and potentially disorienting ones. It channels meaning from another level into our thinking about our empty algorithmic encodings. So Lacan is not reversing things: metaphors substitute earlier material for the new material, only under some transformation. Metonymies combine context into a new definition that does not really use other information, but only gives us better leverage over it. But you are saying 'the metaphor combines information from the two different domains, whereas the metonymy just substitutes a simpler form for a more complex expression.' This is akin to saying that if P is the set of possible worlds where p is true and Q is the set of possible worlds where q is true, then the set of possible worlds where both p and q are true should include both P and Q. Obviously, it doesn't. Language contents and their interpretations obey dual and not parallel rules. Interpretation dualizes notions like combination: the interpretations of p and q are the intersection of P and Q, not their combination. So substituting interpretations from domain A into domain B combines the domains, and recombining interpretations within domain A can be done by literally substituting strings of words. You need to have a consistent perspective, and keep verbal content and interpretation in the appropriate relation. 

I would still suggest, (even if you keep pointlessly deleting this): 1) No (or very rare) Miracles: Experiences present an underlying consistency that allows for similar actions to regularly result in similar results. (Thus, things 'stay known' long enough for progress to be made. We are not permanently stuck in the stage of 'pre-science' on any topic.) We do not need an absolute moratorium on miracles, just a high degree of overall consistency. (So, claiming this is an anti-religious sentiment is abusive, whether you are religious or anti-religious. So is demanding that it must be stronger than it objectively needs to be in order to work, just to force it into a disagreement with religion.) 2) Distrust the ad hoc: Simplicity is a valuable quality for causal explanations to have, and there is a shared, if vague, human intuition for what is and what is not simple. (Thus there is a limited range of causal principles to consider at any juncture in normal science or in any one of Kuhn's revolutionary periods -- the ones that look forward and seem simple enough to pursue. So no anomaly can produce an infinite quantity of research, and a given revolutionary period can never be too long before it converges on a new paradigm.) 3) We can talk about truth: Logic and math, particularly probability theory now that we have invented statistics, work and are reasonably the same for everyone at some basic level. (Thus Popper's principle of falsifiability indicates explanations in normal science either will converge or will be overtaken by ones with better odds.)