It is known that, for $n \ge 3, 2 < p< 2^*$, the imbedding $H^1(\mathbb{R}^n) \hookrightarrow L^p(\mathbb{R}^n)$ is not compact. Let $G=O(n_1) \times O(n_2)\times\cdots\times O(n_k)$, with $n_1+n_2+\cdots+n_k=n, n_i \ge 2$, and $k \ge 1$. Define an action of $G$ on $H^1(\mathbb{R}^n)$ by $g.u=u\circ g^{-1}$, and denote by $H^1_G(\mathbb{R}^n)$ the subspace of $H^1(\mathbb{R}^n)$ which consists of the fixed points of that action, i.e. $g.u=u$ for all $g \in G$. Then the imbedding $H^1_G(\mathbb{R}^n) \hookrightarrow L^p(\mathbb{R}^n)$ is compact. The question is whether there exists a space $E \subsetneq H^1(\mathbb{R}^n)$, with $H^1_G(\mathbb{R}^n) \subsetneq E$, that is compactly imbedded into $L^p(\mathbb{R}^n)$ ? 

either $u_k \to 0$ in $H^1(\mathbb{R}^N)$, or, there exist $r,\delta>0$, and a sequence $a_k$ in $R^N$ such that $$ \liminf_k \int_{B_{r} (a_k)}u^2_k \ge \delta. $$ I know how to prove that if 2. does not hold then 1. holds. I need a hint on how to prove that if 1. does not hold then 2. holds. 

Hi I'm stuck with the proof of a concentration-compactness lemma. We have the following equation in $\mathbb{R}^N, N \ge 3$: $$ -\Delta u +u=|u|^{p-2}u, $$ where $2 < p < 2^{*}$. The functional associated to that equation is given by $$ J(u) = \frac{1}{2}\|u\|^2_{H^1}-\frac{1}{p}\|u\|^p_{L^p}. $$ Because the Sobolev imbedding $H^1 (\mathbb{R}^N) \subset L^q (\mathbb{R}^N), 2 < q < 2^{*}$ is not compact, $J$ does not satisfy the Palais-Smale condition. So one considers the family $J_k$ of functionals defined by $$ J_k(u) = \frac{1}{2}\|u\|^2_{H^1 (B_k)}-\frac{1}{p}\|u\|^p_{L^p(B_k)}, $$ where $(B_k)_k$ is an open cover of $R^N$; $k$ positive integer. Each $J_k$ now satisfies the PS condition. The "lemma" says the following: Let $u_k \in H^1_0(B_k)$ be uniformly bounded in $H^1(\mathbb{R}^n)$, i.e. $\|u_k\| \le \Lambda$, with $\Lambda>0$ independent of $k$, and such that $J'(u_k) \to 0$ as $k \to \infty$. Then, along a subsequence, one of the following holds true: 

Let $(M, g)$ be a non-compact smooth Riemannian manifold of dimension $n \ge 2$, and $G$ a subgroup of the isometry group of $(M,g)$, say with $G$ contained in the component of the identy. Let $W^{1,2}_{G}(M)=\{f \in W^{1,2}(M)| \quad f\circ \phi =\phi \quad \forall \phi \in G\}$. Is there any known result concerning the compactness of the Sobolev imbedding $W^{1,2}_{G}(M) \hookrightarrow L^p(M)$ for some subgroup $G$? 

Using standard notation, we refer to $H^s(\mathbb R) = W^{s,2}(\mathbb R)$ to be the Sobolev Hilbert spaces. As is often the case, it's natural to then consider properties of functions in $H^s(\mathbb R)$ by looking at how it behaves in the Fourier domain, more specifically, we have that for any $s\in \mathbb{R}$, $ \begin{align*} H^s(\mathbb{R}) = \{ g \in L_2(\mathbb{R}) : \int (1+t^2)^{s}|\mathcal{F} g(t)|^2dt <\infty\}, \end{align*} $ where $\mathcal{F}$ is the Fourier transform. In particular to note is that this works for $s<0$ as well, and so negative order Sobolev spaces are easily defined. In particular, one can define the norm of $H^s(\mathbb{R})$ to be defined by the relation $\|g\|_s = \int (1+t^2)^{s}|\mathcal{F}g(t)|^2dt$. $\textbf{My first question:}$ I have seen, in particular the book Sobolev Spaces by Adams, Fournier (page 64), that another way to define the norm of the dual space of $H^s$, which appears to be $H^-s$ is given by the following: $$ \begin{align*} \|g\|^*_{-s} = \sup_{h\in H^{s}}\frac{\langle g,h\rangle}{\|h\|_s}, \end{align*} $$ where $\langle g,h\rangle$ is the standard $L_2$ inner product. I am having difficulty figuring out whether the norm defined by Fourier transformations or the one defined by using the dual are equivalent or not. The furthest I have been able to show is that $\|g\|^*_{-s} \leq (2\pi)^{-1}\|g\|_{-s}$ by Parceval's relation. $\textbf{My second question:}$ If there appears to be a relationship, then I would like to restrict attention to spaces of the form $H^{s}(A)$ where $A\subset \mathbb{R}$ with defined norm $\|g\|_{s,A} = \inf\{ \|g^{'}\|_s : g^{'}_{|A} = g\}$ for all $g\in H^s(A)$. Then is there a relationship between $\|g\|_{s,A}$ and $$ \begin{align*} \|g\|_{-s,A}^* = \sup_{h\in H^s(A)}\frac{\langle g,h\rangle_{A}}{\|h\|_{s,A}}, \end{align*} $$ where $\langle g,h\rangle_{A}$ is the standard $L_2$ inner product restricted to $A$? I am having difficulty trying to relate the two, because there seems to be a natural relationship globally, which I would hope to think there is a local relationship as well. 

Sometimes when doing regression analysis, we estimate our function $g(x) = E(Y |X =x )$ using an orthonormal series, and in particular we use an approximate series $g_{p_n}(x) = \sum_{k=1}^{p_n} \alpha_k e_k(x)$. In practice, we cannot expect to observe the true predictors $ X_i$, and so we observe the pairs $(Y_i, W_i)$, $W_i = X_i + U_i$ for some independent error $U_i$. This is sometimes known as classical measurement error. One might suggest estimating $\alpha_k$ by using a modified least squares regression estimator $$ \begin{align} \hat{\alpha_p} & = \arg\min_{a\in\mathbb{R}^p}\sum_{i=1}^{n}(Y_i - a \cdot E(\boldsymbol e^P(X_i)|W_i))^2f_W(W_i)^2\\ & = \arg\min_{a\in\mathbb{R}^p}\sum_{i=1}^{n}(Y_if_W(W_i) - a\cdot[f_W(W_i)E(\boldsymbol e^P(X_i)| W_i])^2, \end{align} $$ where $E(\boldsymbol e^P(X_i)|W_i) = [E(e_1(X_i)|W_i),... E(e_{p_n}(X_i)|W_i)]^\text{T}$. Naturally, our estimator should be based on what we observe $W_i$, which we call our error variable, since we do not observe the true predictors $X_i$, and the weights $f_W{W_i}$ just get rid of the denominator in the conditional expectation terms. Then when we do use this method (for the time being let us assume we know the densities of $X$ and $W$, in practice we estimate $f_X$, $f_W$ but we assume we know $f_U$), we have that $$ \begin{align} n^{-1}\boldsymbol{A}^{\text{T}}\boldsymbol{A} \hat\alpha_{p} = n^{-1}\boldsymbol A^\text{T}\boldsymbol Y, \end{align} $$ with $\boldsymbol Y = (Y_1,...,Y_n)^\text{T}$ and $\boldsymbol A \in \mathbb{R}^{n\times p_n}$ with $\boldsymbol A_{ij} = E(e_{j}(X_i)|W_i)$. Now what we know is that the rows of $\boldsymbol A$ are independent, but this is not the case with the columns, there are a strong dependency because the entire column is based on one single observation $W_i$. $\textbf{QUESTION: }$ I want to get some asymptotic understanding of the smallest eigenvalue of $n^{-1}\boldsymbol{A^\text{T}}\boldsymbol{A}$, and I know there is a vast amount of literature on random matrix theory. Unfortunately most of the literature is based on iid entries of a random matrix which is what mine is not about, but there is a special area of random matrix theory which studies in detail the 'sample covariance matrix', which my matrix in interest has some resemblance of. Most of the work derived on the empirical spectral distribution leads to Marchenko - Pastur type distributions, and there is work on dependence structures, but I cannot seem to find anything on such strong dependencies as in my case. Is there any information / any ideas out there that might help tackle this scenario? 

I was trying to construct a category of measurable spaces and 'nondeterministic functions' (not the usual category of measurable spaces); specifically a map $(\Omega, \Sigma)\rightarrow (\Omega', \Sigma')$ is a function $f$ from $\Omega$ to the set of all probability measures on $\Sigma'$ such that for each $\sigma'$, the function $\omega \mapsto f_\omega(\sigma')$ is measurable. The idea is that $f(\omega)$ is in $\sigma'$ with probability $f_\omega(\sigma')$. So after defining composition: $$ (gf)_\omega(\sigma'') = \int_{\Omega'} g_{\omega'}(\sigma'')df_\omega(\omega') $$ (here the notation $\int f(x) d\mu(x)$ indicates that $\mu$ is the measure and $x$ the dummy variable), the condition that composition is associative is equivalent to $$ \int_{\Omega'} \left(\int_{\Omega''}h_{\omega''}(\sigma''') dg_{\omega'}(\omega'') \right) df_\omega(\omega')= \int_{\Omega''}h_{\omega''}(\sigma''') d(gf)_\omega(\omega'')$$ Here $f : (\Omega, \Sigma) \rightarrow (\Omega', \Sigma')$, $g : (\Omega', \Sigma') \rightarrow (\Omega'', \Sigma'')$, $h : (\Omega'', \Sigma'') \rightarrow (\Omega''', \Sigma''')$, $\sigma'''\in\Sigma'''$, $\omega \in \Omega$, and the other omegas are dummy variables. Is this known in the literature? I'm not very familiar with measure theory. I strongly suspect that it is true, because all of the other axioms have worked so far (e.g. the composition of two measures really is a measure, identities work out properly). This will probably have a longish, boringish proof involving approximation by simple functions, which I want to avoid going through if someone else has. Indeed, if someone else has proven this, perhaps the category I am considering has already been invented. Does it look familiar to anyone? Perhaps a version of the chain rule, but for integrals? Edit: The proof of this is actually shorter than I had thought. However, the maps have already been studied; see the answer(s). 

In the article Voevodskyâ€™s Univalence Axiom in Homotopy Type Theory, an example is given of how types are not like sets: the existence of a nontrivial (nonzero) type $X$ such that $X\rightarrow X\cong X + 1$. However, I am unable to find this counterexample in the references in the article. Does anyone know this example? Is it a special case of some more general universal property, like maybe a type $X$ with $Y\rightarrow X \cong Y +1$ for all $Y$?