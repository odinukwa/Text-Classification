The Apache webserver is unable to run the script because it is unable to find an interpreter to use. This means that it does not recognise as a compiled binary, and so is treating it as a script, and therefore wants the first line to be for some interpreter. Since you claim the script is a compiled binary, but Apache does not recognise it as such, it is likely compiled for the wrong architecture. Try the command to see what type of file your OS claims it to be. Possibly it is compiled for 64bit when you're running a 32bit system, or it is a Windows executable when you're running a Linux server. In any case, you need to find a compatible binary for it to work. Try running the script directly form the command line, to verify that it can execute, before running via Apache. 

If you use the Routers2 frontend for MRTG/RRD, then it comes with the ability to make a 4hour graph, if you are collecting data at 1min intervals. At standard resolution, 4h is the graph width with 1pixel per data point. You can enable this in the routers2.conf file. 

Running NTP in a virtualised environment, you'll be luck to achieve 20ms accuracy (that's what we've done using VMware). The virtualised clock skew is bad, particularly in a virtualised environment with resource contention. It depends on how accurate you need to be. If you only care about to the second (EG for web servers) you'll likely be fine, as long as you dont have resource contention. If you want millisecond accuracy (such as a busy database, log server, research project) then forget virtualised time servers. NTP servers should always be on physical hosts. You should have at least 3 of them peering in a pool (so that one rogue server gets voted down by the pool); and if possible, get their time from GPS or other local tier-0 source rather than over the Internet. 

Dovecot doesn't know where to put the mail, because you have set the maildir to be but you have not told it what the user's home directory is. Try something like this, which sets the as well as : 

You need to make sure you set the relevant spaces correctly in order to make everything line up correctly. For example: 

Recipient domain checked for local: If the recipient mail domain is handled by this MTA, then any aliases are expanded. If it is still local, it is delivered, and the process stops. Recipient domain checked for explicit route: If an explcit SMTP route is defined for this recipient domain, then the mail is passed to the defined server using the defined method and the process stops. Smart host: If a 'smart host' SMTP route is defined, then all mails are passed to this server and the process stops. MX Resolution: The recipient domain is checked for MX records. If any are found, then they are tried in order until one accepts the email. Then the process stops. A record resolution: The recipient domain is checked for an A or possibly AAAA record. If one is found then the mail is passed to the MTA at this address and the process stops. Bounce: If it gets this far, the message is undeliverable and is bounced. 

Note that the sender domain has no effect on how the message is delivered, and is only referred to if there is a bounce. Assuming the initial MTA is the one that handles the sender's mail domain (though it does not have to be), then the difference between your two cases is whether or not step 1 above handles the delivery or not. 

Example: A similar method can be used to protect a location using a different method, such as or . If you use , you can return the email address in the same query, from where it will be loaded into the environment. You can then use in your Apache config to push it into the HTTP headers to be picked up by the definition. See here for the documentation. 

dsync is 'idempotent' and will synchronise from whatever state you are currently in; there is no queue of pending changes as there is with something like MySQL replication. This means that, when server B comes back up, the next time a dsync is triggered, you will end up with both servers back in sync. No manual intervention is required, and they will get back into sync even if all the mails filesystem on B was wiped (though it might take a bit of time to achieve this). You would probably want to have users normally access only via server A, and in the event of A failing and your proxy redirecting them to B, remove A from the proxy pool until your dsync has completed and A is once again in sync. 

Since you are using MRTG with Routers2, there is a generic cfgmaker host template available at $URL$ which will automatically generate MRTG configurations for many things, including the storage OIDs. These take advantage of the Routers2 additional features to give you combination graphs. It should work with any SNMP-capable host. You can use it with standard MRTG cfgmaker like this: 

In your example, you should be using the namevar, . For example, if you define a resource like this: 

This example sets 400 rows; you may wish to use more. When you have a 5pdp RRA, then you can use , otherwise if you only have a 1pdp RRA, you can only use a resolution of 60 (one step). You may also like to take a look at which (in the same way as ) allows you to define output calculated values which can be summaries. 

This is often caused by a setuid CGI script that hangs; it exceeds the IOtimeout, and apache tries to kill it, but is unable because of the change in uid, resulting in the error. You may want to increase the FcgidIOTimeout or FcgidProcessLifetime to allow the thread more time to complete. Another workaround is to make the Apache server run under the same UID that the setuid script is chaning to. This allows it to kill the process, though it may not be advisable for security reasons. Similarly, running apache as root is also a workaround but not very secure. If you do this, note that your fcgi sock directory (under /var/lib/apache2/fcgid/sock or similar) and process table file need to be writeable by the apache process owner. The root cause, though, is the CGI script itself taking too long. The cause for that depends on the CGI code which I have no visibiilty of. 

In ecelerity, the sieve++ function can optionally be passed the IP of a custom DNS server as an argument. This is used, for example, when querying an RBL server. We have multiple RBL hosts for resilience. How can I ensure that, should one server be down, sieve will fail over to use one of the others in a timely manner? When querying normal DNS, the is used and we get failover to our backup DNS host. However, in RBL lookup, it seems that only one DNS host can be defined and so there is no failover. Example: 

Other people have reported a similar issue with FCGID processes hanging and being unkillable in other systems, such as Wordpress and Sympa. A suggested fix was to add the option 

This filesystem is used by our application for rapidly accessing large amounts of ephemeral data. The filesystem size displayed in matches the zram size as reported in Copying test data into an empty filesystem, we verified that a 2.2 : 1 compression ratio is achieved, and so the filesystem fills before we hit the zramfs memory limit. The value matches the usage reported by the filesystem: 

Sympa authentication is configured by the file. This can contain one or more stanzas defining alternative authentication methods, such as the internal database, LDAP, cas or generic_sso. Sysmpa identifies users by their email address. The first two (internal and LDAP) take the user email address and password, and authenticate directly. CAS authentication uses a CAS service. Generic_sso authentication uses the Web server's own authentication to return a userID, and then obtains the user email address either from metadata or via an LDAP lookup. One example would be using Shibboleth (via mod_shib) and pulling the email address from the Shibboleth metadata. However, any web server authentication may be used, so you can easily use mod_mysql or similar to authenticate against an external user database. In order to get the email address, you can either use an assosciated LDAP lookup, have your web server authentication module return metadata (as an HTTP header), or ensure that the authenticated userID is the same as the email address. In short; use generic_sso, and then configure the necessary authentication in your web server, making sure to return the email address in the metadata if you cannot map user to email via an LDAP lookup. The (admittedly poor) documentation on this is here : Sympa authentication Example: This stanza uses to authnticate via Shibboleth; if the metadata is returned then it will be used, otherwise an LDAP lookup will be performed to obtain the email address. In order for the authentication to work, the location is configured in the web server to be protected by Shibboleth using . 

Then restart Sympa. This will ensure Sympa uses the correct domain name, even if the hostname and DNS have been changed. 

Your Ironport is failing the set up an encrypted TLS session for mail transfer. Possibly, either your Destination Controls or HAT Policy is mandating encryption, or the remote end is mandating it. You need to have a Certificate installed and configured in order for incoming SSL/TLS to work. This may be self-signed, but if it is then a remote side cannot have a policy of verification. If you have a Destination Control mandating verified encryption to a remote domain, and their certificate is expired or has an invalid chain, then you will not be able to set up TLS. Similarly, remote sites mandating validated certificate will not be able to connect to you if you have an expired or self-signed certificate. If you have 'encryption preferred', as most domains so then it should fall back to an unencrypted link and so your email will still flow. 

Within templates called from the define, these can be simply referred to a and (but watch out for reserved words!). 

However, when the application is running with live data over a longer period, we find that this no longer matches. 

In the Ironport web administration page, under Network/Certificates, you can define the various certificates you wish to use for SMTPS and HTTPS. When defining a new certificate, you can upload the (last step) PKCS#12 certificate. After this, you can edit the existing certificate via the same page. At the bottom of the page is a collapsed section "Internediate Certificates (optional)". Open this, and it will allow you to upload as many internediate certificates as are necessary to complete the chain. Now you can assosciate this certificate with your HTTPS service on the Ironport, and it wil send the full certificate chain. We do this here using QuoVadis certs which require 3 intermediate certificates, and it works.