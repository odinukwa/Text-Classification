The official documentation covering unusual errors describes DIANA: PL/SQL is based on the programming language Ada. As a result, PL/SQL uses a variant of Descriptive Intermediate Attributed Notation for Ada (DIANA), a tree-structured intermediate language. It is defined using a meta-notation called Interface Definition Language (IDL). DIANA is used internally by compilers and other tools. At compile time, PL/SQL source code is translated into machine-readable m-code. Both the DIANA and m-code for a procedure or package are stored in the database. At run time, they are loaded into the shared memory pool. The DIANA is used to compile dependent procedures; the m-code is simply executed. I see why you are interested. Access to the compiler underlying PL/SQL is tantalizing if you like looking under the hood. Nonetheless I would be reluctant to invest a lot of time when the same goal can be accomplished by parsing with PL/SQL. As someone who has used your site many times I can only thank you for having an inquiring mind. 

This business model is for a case management database. This is closely modeled on the idea of a file folder representing the phase and a sequential checklist representing the stages. A case consists of a phase that can have one or more stages. A phase can only have one stage that is "Current" or open at any one point in time. A case can only start from one type of stage but can progress to any one of a number of stages that are end types. In this business model there are many different types of phases and stages An example: you apply for a license. The process always starts with you submitting a form but can have different endings: the application is approved or rejected or sent back for more information. Edit: @Colin 't Hart asks what a phase is in relation to a case. Here is where trying to simplify a question can omit details. The complete schema structure is: - one case can have one or more phases but only one phase is open or "current" at at time. - each phase can have one or more stages but only one phase is open or "current" at a time. - there are different types of cases/phases/stages and transitions from the current unit to the next unit require adding a close date to the current and inserting a new record with an open date. An example: a production line for widgets 

If you do not have source control on the DDL statement that created the view, or a development or production database which mirrors the database with the issue then you don't have many options. The logminer can be started using a dictionary file, redo logs or the online dictionary. Have you started the logminer before the original view was created? try select * from V$LOGMNR_CONTENTS; to see if it was running 

This question is caused by the assessment of the team that editioning is not used and does not offer any benefits when we have generous time periods for scheduled downtimed. It prevents any materialized views being created by the editioned user if they select non editioned object from another user. This is required for one project due soon and a data warehouse in the near future. 

This solution does not require triggers but requires setup and causes a performance hit if you enable it for all tables. Auditing has been built into Oracle for many releases. There is an article here which goes into some detail. Basically, you turn it on, tune it for what level of detail you want and the output is available as a dba view or as XML. Check the system parameter AUDIT TRAIL, it sounds like you want the fine grained auditing available from 9i onwards so you can access the SQL_BIND and SQL_TEXT. There is an SCN number so you can flashback to the state of the data when the change was made. Keep in mind that there is a performance hit for all this. You must have the hardware and resources available if this is a production system. 

This could be done this way. I don't like it much as it is not very elegant and looks fragile if unexpected things happen. 

@a_horse_with_no_name is correct in recommending the 11g express and you would have more certainty that the project would not run out of room. You should consider a proviso when you supply the database that you make no representation as to how long it can be used. Even though you think you will only have ten tables software grows and requests to change or tinker with things never stop. For example: 

In your script to import set these variables NLS_LANG ORACLE_SID= your database name impdp a_user_who_exists/your database name directory=DATA_PUMP_DIR network_link=original database name schemas= user1,user2 LOGFILE=DataPump.log TABLE_EXISTS_ACTION=REPLACE 

Another solution is to create a package that does your CRUD operations on CAL1. When a value is changed, inserted or deleted the procedure that is called can create a job to update CAL2, or insert the values on an advanced queue to be propagated to subscriber databases. Arwen asks for more details: There are a number of ways to implement this and the best way must take into account your databases and network. 

In part this is driven by the application. It creates a pooled connection to the database as required. Users log onto the application but connect to the database as a shared user who has all the permissions required. To go into a more detailed example: User X logs in. They have a username and password that matches what is in Active Directory. After authentication they are taken to the main page. During that time the application queries the database for what permissions this user has and caches them in memory. The main page loads each sub work area. Each area asks the cached permissions: - "Do you have authority to see this area?". If so show the work area. - " Do you have permission to edit this area?" If so show an edit/create link. Edit2: The original poster comments that 

I have to continue maintaining an Oracle 8i database that receives information (about 20 Inserts and Updates a day) from a 9i database. I am upgrading the 9i database to 11g which does not support database links to 8i. The plan is to link from Oracle 11g to Oracle 10 XE and then to the 8i and hope that one day I won't have to do this. I have already found articles noting that installing XE after any other version is not recommended. Has anyone installed XE and another Oracle version side by side? What order was used and what problems were encountered? Edit: @George3 some forum posts indicate that you can use JDBC drivers to connect 11g and 8i. Please add some detail and post it as answer. 

I use this solution but it is not an elegant solution. It converts all accented french characters to english equivalents and strips out odd characters. order by translate(LibelleCourant,'ÉéÀàÇçÈèÙù''/*;"<>() ','EeAaCcEeUu') Edit: if you want to keep the special characters to sort by then change the translate function. Example: you want to keep the < and > symbols, just add them to the end string order by translate(LibelleCourant,'ÉéÀàÇçÈèÙù<>''/*;"() ','EeAaCcEeUu<>') What is happening is that any single character which is present in the first string and has a corresponding value in the second string will be replaced. Extra characters will be removed. These characters will be replaced ÉéÀàÇçÈèÙù<> and these characters will be removed: ''/*;"() See the Oracle docs for more info. 

The application I maintain does something similar that is database OS independent for around 100 users but in a much simpler fashion. Users are authenticated by the application and authorized from the database. Here are the details: 

There are a number of other ways to do this including renaming the database using DBNEWID but a new install on a new machine is easy and allows you to increase memory and storage at the same time. Note: 11.2.0.3 is already out of support, why not go to 11.2.0.4 which is supported for a little while longer? 

I would certainly write this differently today but it works.... The key problem with AQ for me is that I have never gotten queue to queue messaging going between different databases. This was described as a primary value for it. Yet I still like having the certainty that the initial transaction can complete without regard for the state of the destination database. If the message fails to be inserted when into the destination database an error is logged and an email is sent notifying the developer. 

Log in as sys, create an account for yourself and grant that account just the privileges you require. Give yourself a reasonable password. "Password" is not a good password. Here are the top ten most used passwords and even if it is your intention that only you log on as DBA these are still not good passwords: 

If the data files are growing then any file space you reclaim will be reused as the files grow. Reasons that cause me to think about re sizing are: 

Either way, do not take anything from my answer or anyone else's answer as valid. Oracle licencing is complex and a final word can only come from them. Contact your representative and ask for clarification. 

So you should be able to see what changes are invalidating the MV. Next you should examine the alert log to see if anything has been logged. I wonder if you have a complex chain of materialized views? Does one depend on another? Is only one becoming invalid and others are not? Try this query to see what depends on what dbms_snapshot.get_mv_dependencies( list IN VARCHAR2, deplist OUT VARCHAR2); I would not recommend your suggested workaround until you understand what changes are invalidating the view. Edit: Metalink note 264036.1 says: "This is expected behavior. When there is a DML on the master table, all the MVs based on this table are marked as INVALID. Though the status is INVALID, you will be able to query the mview. However, the query on MV will not return latest update done in master table unless MV is refreshed." From the way you are using the Materialized Views I am not sure why you cannot issue a refresh of each view that is marked invalid. 

I have seen this become necessary if the database is being used by a poorly coded application. It's surprising how many "Enterprise" grade front ends are not able to let go of a session when the end user finishes. If there are problems that this is addressing you should be able to see this in the application error logs or the database logs. If you can't see any errors then there is no need to restart Oracle. @miracle173 If you restart the application it releases the sessions but causes other application errors which usually result in the application's index of files needing to be rebuilt. If you kill the sessions on the database the application does not release these sessions and you have to restart it after a while or it will crash from too many sessions. This is a quietly acknowledged bug. We can upgrade the application or...restart the database. @Phil madness might be preferable to the abuse of a database system on the altar of sloppy programming. Enterprise application, yes, built with good standards and an understanding of what databases do well; no. 

I confirm that a full or incremental backup during business hours can make your database inaccessible to end users and web services. Factors that can make things worse: 

In order to do RMAN backups you need the SYSDBA or SYSOPER roles. In order to grant those you need to be able log on as a user with those roles: ie the SYS role. There are variations where you use OS authentication or if your OS user is a member of the ORA_DBA group. Easiest is if you can do this: 

Best practices may be different for sql server than for oracle however my experience is that the less schemas, the better. I like to have a schema for the dba/programmer that has special privileges and some code to do maintenance. All the business data should go in one schema so you know where it is. Naming conventions are enough to differentiate between the use of the table or stored code. I can see a case where you have multiple business units owing different data with little overlap each having their own schema. Otherwise keep it simple.