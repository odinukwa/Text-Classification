If this works, try to take a mysqldump and keep it safe side. And recreate a fresh installation with correct settings and remember to remove the entry innodb_force_recovery. If this doesn't work, increase the value and see if it works. Since I'm not sure about the history of this server. You may need to verify your innodb_log_file_size set correctly as per recommended values. 

But looking at your table definition you have secondary indexes almost on all. You might want to verify on performance of all your selects. Or tune them inorder to add the partition name or partition key as a mandate clause in where clause. So its better you take perf against selects and updates you use and then think whether is it feasible to go for partitioning or archiving the table. Also if you are anyway going for partition, you need to define the partition in such a way older partition can be removed. So I would suggest to go for 

Well if your search criteria always be stable for 8 or less characters i.e. starting from char 1 to 8 from long text field meta_value, then you may add a new column saying meta_key varchar(12) and add index to it. Remove index on meta_value long text. Now whenever you insert a row add first eight characters to meta_key. In this case your selects on varchar with 8 characters would be faster. 

What I'm I missing here? I need data greater that ISODate("2015-01-11T00:39:40.121Z") I know many forums have different answers for the same issue. But non resolved. Need a different point of view on this. 

Both the servers are ntp synched with GST. And they remain same after Master reboot too. It took almost 2 hrs to bring up Master Server. How come only slave can have the data that Master is not aware of? Please have someone come accross such a thing? 

I haven't tested this but this seems to be a working command, ensure that you have $URL$ - Build 14332 fixed to have prctl system call supported on Windows. 

I would like to understand the differences between innodb_autoinc_lock_mode options 0,1 and 2 when parallel load data infiles are given. I see in "0" option, it locks the entire table and does the First transaction requested for N number of records say TX1. So when next transaction say TX2 is raised in meantime when first transaction is still getting uploaded using "load data", then it has to wait in the queue for the first one TX1 to complete. And then it sets the max(AI_column)+1 value from the table and does upload for the next set of load data. In this case it doesn't jump the Auto Increment numbers. Also I see in "1" option, it locks the entire table and does the First transaction requested for N number of records say TX1. So when next transaction say TX2 is raised in meantime when first transaction is still getting uploaded using "load data", then it has to wait in the queue for the first one TX1 to complete. And then it sets the max(AI_column)+1 value from the table and does upload for the next set of load data. And then it sets the max(AI_column)+some_creepy_jump. But I see in "2" option, it doesn't lock the entire table. Instead it keeps inserts for each process simultaneously and inserting records for which ever request falls in randomly and ends all threads with average time around (1.21 sec for 7 parellel threads using load data having 1000000 record each). In this case it has multiple transactions in mixed order. And then it sets the max(AI_column)+some_creepy_jump atlast. I'm using mysql 5.1.61 . 

is incorrect, since both BCNF and 3NF produce decompositions that have the Lossless-join Decomposition property, that says that: 

is: both of them are correct, since both of them satisfies the definition of the 3NF. You have simply discovered that the synthesis algorithm for decomposing a relation in 3NF can produce different solutions. A different question is: which is “better”, and of course the solution with a single relation is “better”, since you do not need to join tables when making queries. Of course, if one could check at the beginning if the relation is already in 3NF, than he can avoid to apply the algorithm. But this in general cannot be done, since the check requires the exponential computation of all the keys, to find the prime attributes of the relation. 

so the relations respects this definition too. For your second question, the relation has two keys, and , and it is already in BCNF as well as in 3NF. Note that the analysis algorithm to decompose a relation in BCNF should return the original relation, since no dependency violates its definition (each determinant is a superkey). 

is lost. The reason is that the attributes of this dependency are in two different tables, and there is no way to obtain it, since the tables do not have attributes in common. A decomposition that preserves the dependencies can be obtained by applying the analysis algorithm to produce the Boyce-Codd Normal Form, or the synthesis algorithm to produce the Third Normal Form. 

make a group for each different combination of employeeId and date in the Sales relation, and for each group produce a tuple with employeeId, date, maximum sale price of the group. 

Your problem can be solved with the following set of tables: A table location, with attribute id, and attributes common to all location (like position, etc.) A table for each type of location, with all the attributes specific with that table, and an attribute location_id which is a foreign key for the table location. Then you could consider of adding, if necessary, an attribute to the table location that represent in a compact form all the types associated to that location (for instance an array if you use PostgreSQL, or something similar). 

The important thing about the 2NF is that in each (non trivial) dependency the determinant should not be a proper subset of a key. In the example, the determinant of AB->C is the full key, while the determinant of C->D is C, which is no part of any key. So the schema is obviously in 2NF. 

Why do you think you need to compare the data? Anyways it has to go to DB I guess. You can have a current_timestamp as one of the columns to DB and value can be derived when the Device generates its local data using device NTP. 

Not sure if this is some default behaviour of Mariadb or lack of our knowledge to figure how Mariadb picks its my.cnf. I have worked to resolve similar issue and made mysqld_safe to point to the my.cnf that we want to pick it, in below fashion. 

This ensures your backup data consistency and can avoid situations invalid backup data at the time of urgent restore. 

Have you tried mysqldump using --single-transaction ? In this way it doesn't lock the table for threads doing SQL,DML operations provided no DDL statements should be issued and you will get the backups for conistent states only. This will be efficient only for innodb tables. Below url might give some insights about the option. $URL$ 

You may switch the table from innodb to memory, but ensure the size of the table isn't that huge so that it doesn't eat up entire of its memory. Yes, you may safe guard or recover the table during system crash. Have master/slave replication setup and once the table is created on master with memory as engine type and change the table engine type to innodb (on slave only). So that even at the instance of master crash/restart you will get the data from slave. 

From Mysql 5.6 onwards you can explicitly mention the partition to be queried and it can go on further querying the data based index within the partition. I.E. 

By this way DB performance will look good, quick recovery and data availability 24/7 and less time spent in-case of any maintenance activities. 

As only you are aware of the word you may keep that word as a seperator. And at your application end split this word, if you get any other data after seperating 'clops' from the return type then treat it as a real return data. By this way you can ignore/suppress the warning that you get. 

By this way you can collate details when your usage goes peak. And you may check when you are awake in the day time. 

the key now is only , and the relation has redundancies (since for each phone related to the same person, you have to repeat the ID, TFN, and Name of that person). The relation is not in BCNF, and to bring it in such form you have to decompose it in the two relations: 

The only candidate key of your relation is {D DA HA L NF} (perhaps with R you mean D?) You can verify this by calculating the closure of those attributes, {D DA HA L NF}+, and seeing that it contains all the attributes, while, if you remove any one of them, the closure of the remaining set does not contains all the attributes (this is the definition of a candidate key). The relation is only in first normal form, since the second normal form requires the absence of partial dependencies, that is of dependencies in which non-prime attributes (i.e. attributes not belonging to any key) depends only on part of a key. In this case only the last functional dependency is not partial. 

Your decomposition is not correct, since in R2 you still have dependencies that violates the BCNF, for instance ( is not a key of that relation). The problem is that your algorithm is not correct. When you find a dependency that violates the BCNF, you should decompose a relation in two relations, the first with X+, not XA, and the second one with T – X+ + X. Then you should repeat the algorithm, if you find in one of the two decomposed relation some other dependency that violates the BCNF. So, in your example, a correct decomposition is: 

A multivalued dependency A->->B means that each value of A determines a set of values of B (and not a single value of B as in functional dependencies). For instance, suppose that have an attribute and an attribute , and each programmer can know several languages, you have the multivalued dependency: programmer-id ->-> know-language So, supposing that the programmer with program-id 7 knows SQL and Ruby, this means that in a table in which you have both attributes and , every time there is program-id 7 there must be two different rows, one with language 'SQL' and one with language Ruby, and all the other attributes equal: 

You may give it a try using percona backup alpha version for Windows - Download_Link . Below are the steps after installation in Windows Bash. 

Restart mysqld. Every four hours let cron to exec a script by doing below steps one by one. 1). "flush logs" on the server. Get file names from binary logs generated from restart to latest - 1. (Make a note of latest - 1 file name for later reuse). 2). Convert those logs to .sql .ie., mysqlbinlog mysql.0000021 > mysql.0000021.sql ; mysqlbinlog mysql.0000022 > mysql.0000022.sql ... so on until latest - 1. 3). Now replace string "use A" to "use B" like. sed -i 's/use A/use B/g' mysql.0000021.sql 4). Now apply them to your DB instance. mysql -uwill -psmith << EOF source mysql.0000021.sql ; EOF 5). Capture errors in the below manner at the starting of the script. exec 7>&2 exec 2> ERR_FILENAME.txt 6). And now if [ -s ERR_FILENAME.txt ] then sendmail ; fi 7). For the next iteration use the binary log file that you took a note at step 1 as a starting file name. IMPORTANT 

Doing by this way the SELECT statements are performed in a nonlocking fashion, but a possible earlier version of a row might be used. Thus, using this isolation level, such reads are not consistent.When you say not-consistent it means recently changing records i.e.. DML transactions that are currently in process will not be read. I assume which is in your case it is acceptable. This is also called a “dirty read.” Otherwise, this isolation level works like READ COMMITTED. If I were to be you, the below order is what I follow. 

Click here for the source of this question. - Is it because the blog is created @2001? UPDATE Jun-30-2017: As per "Evan Carroll" response and I have personally tested the blog experiment on 5.7.18-enterprise. The results obtained from the experiment seems to be Mysql is Not an ACID Compliant. 

You may choose the part_token to remove old partition if decided not to have any more. And try to run backup on slave before removing old partition. 

What had happened?: Your backup would have just started and there was a query that was going on running on a particular table for a longer period of time before even FLUSH TABLES command and didn't release the lock on the table and must have been waiting for that thread to get completed or keep trying to flush until the revision_version of that table is same as all tables. Thus you get other threads blocked for other tables as well. As this is entire DBs*.Tables* level lock while flush tables was going on. Finally it would have got accumulated every new connection in processlist and piled up until and not allowing anyone to login. Let's say if you have managed to login to the terminal and tried to kill flush tables, I don't think there is a way to pull back or rollback the flushed tables that has been done and release its own thread connection. So it might in a for longer time. And thus you might have reached the last option that is to restart the server. How to fix it?: At the time of issue, when admin managed to login to mysql prompt. Instead of issuing kill command on FLUSH TABLES thread, if kill was given to the thread running for long SELECT.There are chances that SELECT would have dropped and table is left FLUSH TABLES to acquire and update revision_version and release the lock for new queries. And backup would have continued. Since I don't think anyone expecting an answer on the other end waiting for the result for query running long hrs. What is the long term solution?: