I need to make my tabular model case sensitive. Currently my tabular model is case insensitive, how do I change my existing cube to be case sensitive? 

I have a composite primary key, comprising of a datetime, a location id and a 2 character code. This is the key that uniquely identifies each row of my table. I know that the usage of this table will insert a week of data from a year ago, then a week of data from this year. Therefore, it's likely the table will get fragmented, as the primary key jumps back and forth. I would like to know what sort of indexing strategies can be employed in this scenario. My gut feeling is that the default clustered index is a bad idea and I should replace that with a non-clustered index, which would make rebuilds easier. 

My problem was that when setting up the credentials for the remote server in windows credential manager, I had not specified the domain alongside the username. I had 

I have the same query that shows as running in batch mode when run as a select statement, but row mode when the same query is the select part of a CTAS operation. Both tables (plus the resulting table) are all distribution aligned. Why is this? Obviously I would like it to run in batch mode if possible. Causes row mode execution: 

When this user is logged in via ssms, there are no databases displayed, as they either don't have permissions or the server is totally empty. What should I check? Do microsoft publish a list of required windows polices that be be required for an unprivileged user? Edit: without changing the configuration in SSAS, giving the user windows admin privileges seems to fix it. I wonder what's required that the user currently does not have :/ 

Obviously this only gives you an identifier to use to track changes, you would still need to create a table to track it. 

Querying sys.partitions can return an approximate row count for a table. I've noticed that this is returning the same row count for all partitions, regardless of the actual content (even for empty partitions). The table has a clustered columnstore index and statistics have been created on almost all of the columns. Statistics are updated daily after each data load. The table is partitioned by date. sys.partitions query: 

I have a report that generates PDF's using a data driven subscription process. When the job is started, it shows a success status shortly afterwards, however I believe that this is really asynchronous and therefore cannot be used as an indication of the jobs status. Is there a reliable indicator that can be used? Ideally this would be something that would allow me to automate a check in a stored procedure. 

I have a tabular cube with a large partitioned table. I'd like to truncate some of the partitions (freeing the memory space), but retain the metadata, so that I can quickly rebuild them if required at a later time, without having to recreate the partition. Is this possible? If so, how can it be done? 

Imagine I am running multiple batches through management studio, separated by the GO command. I'd like to know how implicit transactions will behave - is the transaction committed on a per batch basis or once for the entire execution. 

I'm using to monitor the health of my columnstore indexes. I've noticed that I've got one index that looks strange. As far as my understanding goes, the columnstore index will fill a rowgroup before starting the next rowgroup. For best index performance, fuller row groups are better. Having said all of this, I have one table that I'm having trouble understanding. Even after an index rebuild using the largest resource class (to give the maximum amount of memory possible to the build process) the view still shows this index as being spread across many partially full rowgroups. 

Unfortunately, this is really slow. Everything is distribution aligned. Analysing the estimated query plans shows that a columnstore index always uses hash match to perform this operation. A clustered index will allow one of the queries to use a stream aggregate, although I was expecting a few more ( see plan below) 

Which is bizarre, because as SalesItems_Breakfast is inside the AVG function. However, the below query runs fine, the only difference being the over clause has been removed: 

My question is, are there any reasons why this is not permitted in an update statement, should I avoid using an updatable cte as a workaround? My concern is that there are issues when using window functions with update statements and therefore I'd like to understand if this is an acceptable method or should be avoided. 

I have two tables that are identical and hold the same rows of data. one uses a varchar(255) for every column of the table, the other is correctly typed for each column (using date, int, decimal etc.). Using Management Studio's properties window, I can see that the table that stores all data as varchar is far smaller, which is not what I expected to see. My understanding was that an int would only use 4 bytes of space and therefore use much less space than an int stored as a varchar. What should I look for to understand what is happening here? Both tables have a primary key on an int column and no other indexes. For testing purposes they have been populated with identical datasets from the same source query. 

How can I set up a subscription, when the server name does not resolve from the server I wish to subscribe from. 

Using the ReportServer database, I have a script that appears to work. Querying the same table as my data driven query 

I have a skewed dataset, where most rows fall into the largest 10 values of my best candidate distribution key. My data is made up of two large tables, that only share two keys - my best candiate key, plus one other, but that one is null 80% of the time, so I have discounted it as an option. Conventional wisdom says that if the data is skewed, I should use a round robin distribution. Looking at the explain plans produced by joins on the tables, I see my candidate column is the shuffle key for the shuffle move. This makes me question if I should change the distribution from round robin, to hash distributed, saving the time it takes to move data on every execution. Is my logic correct? I feel like this is against the conventional wisdom when working with distributed sql. I don't expect any queries where this join isn't required, so that may be where others would see the benefit. 

I've got a query that emulates the CUBE function. This is many different aggregations unioned together. 

(I know this also removes the filtering performed by the join, but this doesn't explain why excluding those rows changes the order) 

My plan for this, is to add a rowversion to each of the source tables, and display the max rowversion from the source tables. This is making the assumption that rowversion is unique across the entire database and is not incremented in each table (I need to check if that assumption is correct). 

Probably a stupid question, but I wanted to know if it was possible to switch a whole table into a partition of a larger table? I'm guessing not, as check constraints aren't available to constrain the table on the partition boundary. 

So, specifying the fully qualified domain name in the username field of credential manager resolved the issue. 

I have what I believe to be a correctly written sql query that calculates the average using an over clause to window the results. However, I'm getting an error when I include the over clause. 

I have a merge join in azure data warehouse. My estimated execution plan currently shows it as a many to many join. I'd like to know if it is possible to achieve a many to one join. Currently I'm struggling to think of a way to do this as primary keys and unique constraints are unsupported. Is there anything available that would allow me to tell it that one of the tables will always contain unique values. 

I have a server that I wish to set up a subscriber on, however the publisher lives on a different domain controller and is not in the primary DNS. To access the server, a FQDN needs to be provided. so becomes when trying to add a subscription, I get the following error: 

It turns out, I had done something really stupid. My SSIS package, as it turns out was perfect, I had named the transactions and set "RetainSameConnection" to true. Begin/commit/rollback commands were all specified correctly. My problem was that one of the stored procedures called as part of the SSIS package had a unnamed transaction in it, causing SQL Server to commit everything once the commit in that stored procedure was called. Resolution was to modify the stored procedure to name the transaction, so that SQL Server treats that transaction as a nested transaction (In theory, I would have thought the second "begin transaction" statement would do this, but for some reason this was not happening). 

I am trying to use explisit transactions within SSIS. The database I am working with does not have the DTC enabled, so I cannot use SSIS' inbuilt transaction handling. I have execute sql tasks to initialise, commit and rollback the transaction, however despite having set "RetainSameConnection" to true, I am still getting the below error: 

Running explain on the query showed that something underneath was setting to . Forcing the query to use 0 overcame this, as columnstores require DOP >= 2 Therefore adding the below hint enables batch mode 

Azure data warehouse supports both clustered and non-clustered indexes in addition to columnstore (which is the default for any new table). I know that having a large clustering key is normally discouraged, as it forces sql server to internally generate a larger key to guarantee uniqueness (and unique constraints are not supported). Of course there are loads of benefits of using columnstores, such as batch mode and segment elimination, however columnstore will use hash match and hash join operations for analytical queries. Experimentally I've seen situations where a clustered key performs better where all the grouping columns are included in the clustering key (4-5 columns, some varchars), because the guaranteed order allows a stream aggregate to be used instead. My tables are distribution aligned and the estimated execution plan shows that the queries against the columnstore indexes are indeed using batch mode. What are the potential pitfalls with this approach, is this scalable for billions of rows of data? 

I have two very large tables that need to be joined to produce a data extract. Both tables have clustered column store indexes and are partitioned. DW Sentry advises me in the plan view that it thinks a NC index should be added, but without compression, it's likely to be a very large index. Does anyone know about the join characteristics for a columnstore? 

I can set up the subscription if I add the server name and ip into the hosts file. Not pretty, but it works. 

I've got a really expensive join in data warehouse that isn't distribution aligned. Unfortunately the join key is nullable, and only half the data has a value, meaning that it wouldn't be a good candidate for the distribution key. Would be be possible to created a partitioned view where the null half of the table is round robin distributed and the other half is distributed on the available key values? 

Edit: Also appears to fail with the same error if the constraint is created inside the table declaration. 

Azure SQL Data Warehouse has limits on the maximum size of a transaction (as documented here). As CTAS operations cannot be performed inside a transaction, I was wondering if these limits still applied? 

The select into syntax will create a new table to insert data into (I know that types may not be consistent etc. but it gives you a rough copy). Is it possible for a delete output into statement to also create a new table in the same way, without having to define the table first? 

I don't understand why this is happening - my understanding is that CTAS and building a new columnstore should always be parallel, but this resolves the issue. Edit: This appears to be the default setting as it's aiming for the best optimum columnstore quality. So you have to make your choice between read performance and index quality or use an intermediate store, with each query having a different maxdop hint. 

So this turns out to be because of the shape of the data I am storing, IN THIS INSTANCE a varchar genuinely takes up less space. Many of the columns in the table are ints and numeric(38,12) (this is so huge because the data source uses this type). However, many of the values in these columns are 0 or whole integers. As a result a varchar(255) would store this value as 3 bytes (1 byte, plus 2 the two overhead), whilst a int would use 4 and the numeric would use 17. This results in using the correct types to be less storage efficient over the whole table. To decrease the storage space used, I would need to check if I can change the typing (from int to smallint or numeric(32,12) to a numeric (19,8) etc.) alternatively it looks like sparse columns would be a good solution where there are lots of 0's or null expected. 

I've created a new user that I would like to have access to a cube. The cube definitely exists and admins can see it and process data etc. I have a new user account with reader and data processing permissions, which gets the following error when trying to connect: 

I'm wondering if it is possible to use row level security based on a users AD group membership. I would love to have something that could support users in different groups and some super users who have membership of all groups. I suspect the answer is no, but any tips would be appreciated! 

The problem in this case was that I was trying to replace an existing column also named 'Date Offset' - this was not a calculated column, but taken from the source table. Resolution was to do the deployment in 2 phases, first remove the old column, then deploy again to add the new calculated column. 

I'm trying to deploy a tabular model to a server using the "Analysis Services Deployment Wizard". When attempting to deploy,I get the below error. 

If I have a rowversion on two different tables in a database, are they guaranteed to be unique across both tables? If not, is there any way to set this behaviour? I have a view that joins two tables together, I'm wondering if this behaviour is guaranteed, so that I could take the largest row number from either table, to form a rowversion for the view. 

I have a query that calculates . The table has a clustered index on the same columns (and order) as the partitioning and ordering for the row number. When using a merge join (many to one), a sort is required, even though the clustered index is in the correct order. Removing the join also removes the sort operation. The clustered index that should power the row_number calculation: 

Azure SQL DWH / PDW have the keyword to show the parallel query plan. This is really useful for seeing the data movement operations, however I was wondering if there was an equivalent of the traditional sql server query plan. I'm working on the assumption that just because there isn't any data movement, it doesn't necessarily mean that query is well optimised.