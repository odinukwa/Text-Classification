This is a very good question and not just because it shows effort on your part to answer the question. Thank you for asking it and please do not remove it. The answer as you have already discovered is -- You can't do that. You have correctly identified several similar concepts including... 

Unfortunately this requires the swapping the existence of both the public and private database links before and after any deployment that creates materialized views using links. The temporary creation of a public database link with embedded credentials is also a security risk and is in fact a problem we are trying to solve. 

To know how many tickets are active on a particular day, you need to know both when a ticket first became active and when it was closed. Your design already contains this data, so the count can be generated without adding any data. This can be done by calculating the number if days between the open and close and then joining that with a data set large enough to accommodate your longest open duration. Here is an example in Oracle doing something conceptually similar. 

Here are three methods of solving this in Oracle. The first is an Analytic SQL solution that only has to do one full table scan (assuming no indexes). The second is Nick Chammas's CTE solution converted to Oracle syntax and the third is a solution using a PL/SQL cursor FOR loop inside a Pipelined function. The solutions all produce the results expected. For these few rows the analytic solution performs best followed by the recursive CTE. Which is easiest to work with will be in the eye of the beholder. Analytic Solution 

What are the benefits? - Faster data access with less disk access. How will the client side cache be kept in sync? From the Oracle Call Interface Programmer's Guide: 

It will take marginally longer to generate the plan when there are more indexes to consider, but I doubt the difference would be measurably significant. The reasons for dropping an index does not list query performance. On the other hand, in general you shouldn't create indexes unless you know they will be used to make a query more efficient. From the Oracle Concepts Guide, here are the criteria for creating an index. 

Here is what the function might look like if it were done in PL/SQL. It was just thrown together and is only meant to illustrate building the SQL statement and is not meant to be an example of good coding. 

If the query does not return ARCHIVELOG then follow these instructions to turn it on. The following expert is from found in 10g Backup and Recovery Basics (strange place, I know) in a section called Limitations and Restrictions on Flashback Drop: 

You could store the catalogs tables in a separate schema on production. test could then have a schema with the same name that uses views that select from the production tables using a database link. This would make production fast because the data is local without requiring extra space. If you decided that a particular table was too slow to access over the database link or having too great of an impact on production, then you could create a materialized view on test instead of a view. You could create as many or as few of these as necessary to give the best balance of space and performance. On the other hand, 1GB isn't that much space, so you might be better off importing into both databases even though it would double the space requirements. It would have a speed advantage and would make test more like production. 

A mutating table occurs when a statement causes a trigger to fire and that trigger references the table that caused the trigger. The best way to avoid such problems is to not use triggers, but I suspect the DBA didn’t take the time to do that. He could have done one of the following: 

If you don't have a field like ID that is unique across the two tables, then you will need to compare more fields, perhaps all of them. 

Apply Lag will notify you when the apply is slow, not just when it has stopped. This may give you too many notifications, but it seems like you could tweak the threshold to limit these. See also: Oracle® Data Guard Concepts and Administration 10g Release 2 (10.2) Oracle® Data Guard Broker 10g Release 2 (10.2) 

If for whatever reason you don't want to use ik_zelf's solution (preferred), you could change your sequence to use negative values. This would keep it out of the way of the Add 1 code until you can change it. 

Some additional information to go along with the other answers that relates to your question (although not to your specific issue). When you can't build a fast refresh-able materialized view and sometimes even when you can but the same values are being updated in the same rows many times, you need a different technique to keep the replication current while limiting the undo and redo volume. One possible solution is to do a merge statement from your source table to your replicated table. Since the merge is base on the current values without regard to how many times they have changed, a compromise can be made between how up to date the data is and how much redo/undo is generated to maintain the relevancy of the data. For example, if a particular value in a particular row changes an average of three times an hour, a full refresh run once an hour will generate redo/undo for that row even if there are no changes to it. A fast refresh will maintain all three changes and apply them sequentially, but a merge would only apply the latest change and only if the data changed, yet still be as up to date as the other solutions. As a caveat, it may take longer to run, so you will have to test it to ensure it meets required refresh window. 

There is much more information that could be discussed on this subject, so please don't use this information here as your sole source in making your decision. 

The number of rows in this table would be mostly dependent on how often the rates change. Four building units with four categories that change once a year would have eight rows per year. If the rates changed weekly for every building unit and category there would be 832 rows per year. The table shouldn't need the Building ID as long as the BuildingUnitId is unique. It also shouldn't need the category as that can be derived from the BuildingUnitId. A rentals table would have a building unit and category columns that could be joined with the rates table to derive the cost based on the period of time the building unit is rented even if it crosses rate start/end date boundaries. 

Bad Data Field length provides an additional mechanism to catch/prevent bad data. An interface shouldn’t attempt to insert 3000 characters into a 100 character field, but if that field is defined to be 4000 characters, it just might. The error woudn’t be caught at the data entry stage, but the system may have trouble further down when another application tries to process the data and chokes. As an example, if you later decide to index the field in Oracle you would exceed the maximum key length (depending on block size and concatenation). See… 

I found this just before posting. The difference is the linux environment setting for NLS_LANG. It was set to on the system it added padding to. Unsetting this on one and setting this on the other swapped where the behavior. 

There is a point where the queries using the index are accessing enough partitions that a local index will be slower than a global index. For example, if your table is partitioned by date, but you are querying only on a status column. 

Then you would simply select the data before you delete it so that it would all be in the spool file. 

Use a materialized view for T2 as BillThor suggested. Create T2 as a view of T1. Create a job that periodically merges from T1 into T2. 

If a smaller value will work for 98% of the cases, but it takes a Varchar2(4000) to work for 100% of the cases, then you have little choice but to use the larger value. Creating a separate table for 2% of the values and then coordinating inserts/selects etc. would add complexity that would obliterate any memory or performance benefits from not extending the field. 

If you are in SQL plus and just want to know what instance or database you are connected to, you can use the following: 

As a programmer understanding the database better made me a better programmer. When I became a database administrator this became even more important, therefore I believe education is the key. DBAs should patiently guide developers treating them as competent professionals. Few programmers when shown the difference between a set operation and a row by row operation on the client side will balk at the idea. We share some of the same goals - application speed, data security, maintainability, etc. This applies not just to the application logic question, but also to all aspects of database interaction. Programmers want to use their tools better and the more the DBA can show them how to use the database tool better the more they both will benefit. 

Oracle implemented monitoring templates a bit differently than you might expect. When you apply a template to a database it doesn't associate the template with the database, but instead applies the template settings to the database. This is why modifying the template after it has been applied to the database has no effect until it is re-applied to the database. The 11.1 Oracle Enterprise Manager documentation explains how to use the "Compare Monitor Template" feature to determine how closely the template matches a particular target. In this way you can determine if the template has been applied to the database. 

With Oracle's Advanced Queuing is there a way to dequeue messages in a LIFO (last in first out) order? There is an indication through a lack of information that this is not an option, but perhaps there is a way to do this such as queuing in a different order. 

Here is another answer that works for nine rows, but not for more. I'm posting it in case it sparks an idea. 

Justin Cave is correct.+1 If you would like to learn more about roles you can get a good overview from the Concepts Guide. The Security Guide has more in depth information including limitations such as roles not being enabled in Definer rights methods. 

After creating the link you can run this statement in Toad, SQL*Plus, SQL Developer or any number of other SQL apps. 

You are doing a differential incremental backup rather than cumulative, but the syntax indicates that you plan to incrementally update an image file backup. Phil was on the right tack with the missing . Since you are not running this each incremental must be able to recover the un-updated tag 'sometag' causing it to grow each day as though were specified. See Backup Sets and Image Copies in the Concepts Guide and see Incrementally Updating Backups in the Backup and Recovery User's Guide (11.2). If your intention was to use incrementally updated image copies you will want something more like this: 

When building a new standby database, is it possible to have it start shipping archive logs from the primary without having the datafiles on the standby yet? 

DML can be considered to exclude statements. The Wikepidia.org entry for “Data Manipulation Launguage” describes it as follows: 

It really comes down to cost vs. importance. The more important the files are, the more beneficial storing them in a database becomes. Here are some things to consider. Pros of database file storage: 

You could insert the results of a flashback query or flashback versions query into a table and export that table or you could do your own versioning. You could also look at the Change Data Capture feature which may meet your requirements. The real answer to your specific question though is... No, you can't do that. 

No, there is no direct replacement for PRAGMA RESTRICT REFERENCES… WNDS, however, the feature has not been removed, only deprecated. Looking further though, your question assumes that these developers can’t be trusted to not add write commands, but can be trusted not to remove the PRAGMA or create another package without it. Although your question is legitimate on a technical level, your real problem seems to be a code review and/or management issue. Here are some recommendations that could help with that. 

As Martin Smith said, you need to pivot the data, whether with an explicit PIVOT as referenced or something like this (SQL Fiddle): 

Is there a way to know if a has been done on an index? Doing one doesn't seem to update the , , or attributes. Note: I am not referring to the that returns the first non-null expr in the expression list, but the one that is used to make free space in a block contiguous like this: 

There are however several types of statements that as of SQL Server 2012 do not correctly short-circuit. See the link from ypercube in the comments. Oracle always does short-circuit evaluation. See the 11.2 SQL Language Reference. Or compare the following (SQLFiddle): 

You should implement this on the database side in the method that can be called by your create schedule time and modify schedule time methods. 

We recently increased the LOG_BUFFER value from 8 MB to 16 MB with only a small reduction in the retries. It is still increasing at about 1,000 a day. Should the LOG_BUFFER value be increased further? Since the system is 64 bit I can take it up to 256 MB, but I don’t want to use the memory for this buffer if it won’t be beneficial and I don’t know if there are any downsides to increasing this value that significantly. 

Just to supplement Kerri Shotts excellent answer(+1), the Oracle documentation has an excellent guide on the steps required to duplicate a database. $URL$ 

Using the where clause that works and wrapping each field in a to_char returns the following interesting results: 

Instead a person has to remember the magic value. With each datatype used they have to remember more magic values e.g. 1/1//1900, "Z", -5000. Furthermore, when the magic value is in the data they must also remember alternate magic values. So, for one specific case it makes the code simpler at the expense of other cases, not to mention disk space, index size, query parsing, consistency, etc. 

To update a column using a sum of values from other tables that may have nulls, do something like this: 

Look at the DBMS_FLASHBACK package and the ENABLE_AT_TIME procedure (12.1 Documentation). This allows you to do something like the following: 

What should happen? Should the update fail on the local database but succeed on the replicated database or should it not run on the remote database because it failed on the local database? If you don't want changes by batchjobaccount to replicate, then why give them permission to make the changes on the local database? If they need a local playground, then can you just make a local copy of the tables and only give batchjobaccount access to those? 

I have several Oracle servers that exhibit a behavior I do not understand. I have identified a two node RAC in which one node exhibits the behavior, but the other node does not. RAC is not involved in the issue, but it dramatically reduces the differences between the servers experiencing the different behavior. Why is the following output different? Node 2 (expected behavior): 

Getting the max(ora_rowscn) will require a full table scan each time you do it. It may be faster just to refresh the entire cache each time. It sounds like you need a way to notify the other service that a change took place and what the change was. You could maintain a log table with a column that indicates which system needs to consume the change. The column could have two function based indexes one for each service so that each index contains only the entries that need to be consumed. Then as they are consumed they can make the value NULL to remove it from the index. Or you could just use Oracle's Advanced Queuing. 

Not using a built in command, but... If an really can satisfy your requirements as your title implies, then no inserts should be necessary and the primary key must be in sync between the databases. Your requirements don't allow remote updates, but if you can remotely query the database, then you can join the local table to the remote table to create the statements that will need to be run the remote system. Here is a demonstration: