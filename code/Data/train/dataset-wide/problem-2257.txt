I hope this is the right place to ask this question. I'm a user at StackOverflow, and when I perused the StackExchange websites, this was the best match I could find. So, if I've posted in the wrong place, please let me know and I'll move the post. I'm working with a group of people and we're creating a start-up investing firm. I'm the "tech guy" in the group (I can build PCs and program), and so it's up to me to figure out what we need for our database server. 

For the OS and SQL Server application installation, a RAID 1 array of 2 60 GB SSDs. For the SQL Server data volume, a RAID 5 array of 4 medium-sized, fast HDDs (I'm thinking 300GB Velociraptor drives; So far, 9 YEARS of data takes up only about 100GB). For the SQL Server log file volume, a RAID 1 array of 2 small-to-medium-sized, adequate HDDs (non-Velociraptor here). For the SQL Server temp database volume, a RAID 1 array of 2 60 GB SSDs. 

We want to index and search "text" on log_info column, so we tried Oracle Text 11g . Problems: If we use "data_type" is "clob", we can use "context index" and it have to synchronize after DML . ( we can not use this way because of (**) ) If any, how can we index and search on "log_info" column (max_length < 10000 and data is changed every second) ? 

Machine 1 (slave) and machine 2 (master) are in a cluster (streaming replication). Sometime, I see "FATAL: terminating walreceiver due to timeout" in slave log. Here is full detailed logs: Slave 

We want to search on "c1, c2, c3" coloumns on dtsc_search_data table with conditions: if seach_value is found on "c1" then return ; if search_value is not found on "c1" then finding on "c2", if search_value is found on "c2" then return ; else return (c3). Example: 

What I'm mainly interested in is where to spend more money and where additional power is less useful. For instance, when it comes to drive set up, I was thinking of something along the following lines: 

Is the latest i7 Quad Core useful for this, or is that overkill with processing power? Can I get away with an i5? An i3? A Core 2 Quad? I'm assuming that a 64-bit OS with 64-bit SQL Server is better because it will allow me to go beyond 4GB RAM. If that's true, is 8GB enough or do I actually get performance benefits at 16GB? Does DDR3 RAM offer significant performance enhancements? I know that the graphics on this machine are extremely unimportant, but would a dedicated graphics card off-load work from the CPU, increasing performance, or is that a waste of money? 

Description: We are using PostgreSQL 9.3 - Centos 6 x64 . We have a dtsc_search_data table as below: 

Here, you can use sequence to increase value. Please noted if the sequence reach to its , you will face this error . For example 

After testing with 1 million row, here's the best way from @Julien Vavasseur combine with Index on "user_id, grade, grade_date" . We should consider using window function when having a lot of rows. 

Question: Our solution: write 3 SQL queries search on "c1", "c2", "c3" are followed by conditions above. How can we do that with lowest performance ? 

2/ QUESTION: What is about "FATAL: terminating walreceiver due to timeout" problem ? How can I fix it ? 

The OS/SQL Server application volume will be a software RAID using the motherboard; the rest of the RAID arrays will be controlled by a dedicated RAID card. From what I've read online, the above setup is likely to give me great performance and stability. Now, when it comes to other components, what's necessary? 

We're going to use SQL Server because it's what I know (so, please don't suggest MySQL, Oracle, etc...). I'm familiar with purchasing components and assembling a PC, although I've never done it specifically for a high-performance database server before. Assume we have a several thousand dollar budget (my co-investers are doctors). Assume we have a constant stock market data feed (something like 100 stocks at a time) that needs to log the data to a database as it comes in and also use the current data to query for similar past data and perform comparisons. 

Problem: I tested over 10 times, each of times when data is inserted about 2,000 rows (2,000 records is commited into machine 2) , with insert time about 3 minutes , I got errors: 

From @Craig Ringer, you should re-format the data. Based on your data, on Linux, I used & to format CSV file before importing. For example: suppose that file as below 

If not, It causes an error . Secondly, please remember that you cannot have when on the trigger. If not, It raises an error . Finally, please take a look at the example as below: 

You can use Jailer tool, it will find all rows of child tables that reference to master table. Example: I have 4 tables : employee, employee_detail, phone_address, relationship. I want to delete employee which name="JOHN". With Jailer, it will find rows in employee_detail & relationship which FK to "JOHN" (by id). And because of phone_address reference to employee_detail, so Jailer will find rows in phone_address. 

Most of the work I've done with SQL Server has been for small companies. As a result (due to price), we've mostly done installations of SQL Server Workgroup. For everything I've ever done, that's been just fine and dandy, and I'd like to continue using that version. Is there any practical benefit to upgrading to Standard? Now, I'm guessing that for our needs, a server like this will cost about $5,000-$6,000. Am I better off buying the components myself and building the system or buying a server from Dell/HP? Also, (and I know this is a big wrench in things), if, during the initial, testing phase, we decided that we didn't want to spend $6,000, what cuts would you make to bring this down to a $2,000 system? 

Question: After researching , I found 4 solutions. Are there any better solution (not using copy data from files) ? 

Until now, pgBouncer doesn't support rotate log. Hence, you have to do it by yourself. You can refer to sites below: How to rotate PgBouncer logs in Linux/Windows ? How To Manage Log Files With Logrotate On Ubuntu 12.10 

Erwin said "You probably don't want to hear this, but the best option to speed up SELECT DISTINCT is to avoid DISTINCT to begin with. In many cases (not all!) it can be avoided with better database-design or better queries" . I think he's right, we should avoid using "distinct, group by, order by" (if any). I met a situation as Sam's case and I think Sam can use partition on event table by month. It'll reduce your data size when you query, but you need a function (pl/pgsql) to execute instead of query above. The function will find appropriate partitions (depend on conditions) to execute query .