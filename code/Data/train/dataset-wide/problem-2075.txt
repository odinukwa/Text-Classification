This will grab you the information from the active database[DB1], and then information from the other database [DB2] because you specified to go there to find it 

Firstly as mentioned the Ola scripts are safe and very thoroughly tested across a lot of people. Your fill factor should only really ever be set to 100% (0) if the index will only ever add onto the end and delete from the beginning (aka a bigint identity field is fine) If you have data that will insert into the middle of data sets so in order of a person then when you're adding to that index you will want to insert into the middle of your dataset. if the fill factor is 100 then there is no space and it will create / add to a new record completely out of place fragmenting the index. There's no 100% definitive answer to what is the right fill factor to use, and it can change for different indexes on the same tables. if something is constantly fragmenting then lowering the fill factor can help alleviate that. NOTE reducing fill factor while can reduce fragmentation, WILL increase disk space usage. if you have an index that is FF100 and uses 100GB and you change it to FF50 then it will now be using 200GB of disk space for the same amount of data. 

you cant have multiple databases 'in use' at one time, as there are a lot of things which change between databases even within the same instance. You can access databases outside of the currently accessed database like so: 

For clarification from gbn's answer, if you want to designate the locations when you first create the database do it with the CREATE DATABASE command eg. 

All of these commands are accessible for all users (all are views) but the views are restricted to just your current spid unless you have the permission 'view server state' which can be granted on a database by database level. A user is granted the basic permissions to view their own connection as it is information about yourself (Note that this will always report back that you are running a select command as that's what you're doing at the time) If you have the 'view server state' permission then you can run a query such as: 

TLDR; if you can do your backups on your secondary, run them there, but system databases need to be done per server. Our main consideration for this is licensing, if your secondary node is passive then you can't take backups on it, if you're using it as a read replica then that's not an issue. Second consideration is what synchronisation method are you using if your actually in synchronous mode then your secondary node is up to date as the data is written at the same time. So if you're on an active secondary with synchronous commit then you can happily run the backup on the secondary node to release pressure on the primary node. The AG group is sensible enough to tell the Primary replica that the backup has been taken and will clear the transaction log out on the primary accordingly (all of our backups are done on secondary) hereby no backups never need to be done on the primary node. Any databases that aren't in the AG (aka master, model, msdb and any other locals) need to be backed up independently still 

The method of insert doesn't appear to be the problem, more the data itself The error you are receiving is because one of the columns you have in your database table is too short (see your column 'wikipedia_link') see what length it is and if you can increase it, note it is quite possible to have more errors on other columns that are similar, judging by the data contained in that field the longest field is 128 characters, so I'd make the field nvarchar(130) as a minimum If you load the csv in excel you can find out the max length of a column by using {=MAX(LEN(Q:Q))} (NOTE to get the {} array calculation after typing the query press ctrl + shift + enter) Check all your fields are long enough and try again 

You've created the login for user1 and user2 to the master database not the database you want. rearrange to: 

This could be expected behaviour especially if you're deleting based on an index (aka where [primarykeyfield] < targetID) since that delete will take in the ms of time you'll almost always catch the process in the wait time between the two batches Say for example the delete takes 10ms (i imagine it will potentially take less than this but its just an example), the wait time is 2000ms So for every time you run the command there is a 10/2010 chance of hitting that in the period of time the delete is running (less than 0.5%) and a 2000/2010 chance of catching it during the wait for delay. If you want to observe it directly find the spid that its running over and run the profiler over it checking for the search and you'll see the delete occurring every 2s and going into the waitfor (note if you wan to see the start of the waitfor as well check the but the completed one will give you the duration of how long it takes to run each part. 

SQL Server is designed (along with most db engines) with security in mind the main areas you'll look at for viewing users are 

As you can see when ran naturally or the code is selected the output is returned as expected, however once it is passed through sp_executesql it returns as ? characters. Its worth noting if you attempt to send a varchar instead of an nvarchar it errors out so it defiantly uses unicode. Have tried a few different collations for the input but that doesn't seem to change anything 

(And you can do other values in there aswell) There will be many other things you can try, but as mentioned initially the problem is when a query is ran, the SELECT part almost right at the end so you can't use that anywhere else in the query unforutnatly EDIT: Select doesn't come dead last in a query, full execution order for reference is: FROM-ON(JOIN)-OUTER-WHERE- GROUP BY-CUBE/ROLLUP-HAVING-SELECT-DISTINCT-ORDER BY-TOP 

If you run it'll show you all the connections that are running against your DB. Running can show how many connections are active You may think that a high number of connections would increase the CPU but ultimately it depends what those connections are trying to accomplish, if they aren't doing much but are preventing you're main application from running their connections then the normal processing level will go down while you are not running your normal processes The annoying thing is to be able to run the show processlist command you need to actually be connected to the DB which you wont be able to as you're at max connections. A slight work around is to increase your max connections (if you're able to) and set up some monitoring somewhere that runs every so often (every 5 minutes would have caught that) and when you're over 50 connections, run a command that dumps out all the active connections somewhere so you can review later. 

There's two main forms of failover you can use with SQL post 2012, The Windows Failover Cluster (WFC) which has been around for a while and the Always on High Availability Group(AO) WFC uses a single database which can move between multiple machines this method requires the database to be installed on shared drives (such as a SAN) so that when the cluster moves the node the other machine has direct access to the information that makes up that database AO uses two completely seperate instances of SQL Server, both instances run at all times, (depending on how close they are you can set them up to be synchronous or a-synchronous, (if they're on the same network synchronous is better if they're separated globally a-sync is a better choice), due to there being two databases you do not require any shared storage, the two machines work completely independently of each other, we currently have ours set up with a quorum witness which is actually on a shared drive but is not technically required (it is advisable to have a witness however) Worth Noting about using AO, if you use the secondary node for any data access / backups (which you can as it can be accessable through readonly) you will require the extra licences to access it, if it sits there purely as a redundancy for when your primary node fails you do not require any extra licences as only one database will be accessible at any one time, with WFC since there is only actually one instance you only require one set of licences for that instance as it moves, you do have the disadvantage of only one disk failure will cause problems but using a RAID 10 on those disks will limit this from becoming a problem Ste 

Table for each user would be wildly inefficient as whenever a user leaves or joins you need to re-arrange your table structure. having an field in the table and then allowing the general users to be identifiable meaning that when someone is logged in the query can run Be worth having a table to link the ID to the logged in user, and then you can also set up managers (Table User_ID, Manager_ID,name,detail...) which means you can allow managers to view all data of their underlings (if this is needed) Its worth noting doing a structure like this to consider indexing the table in a structure that organises the data according to the field to improve performance for each user as they will unlikely overlap 

Thoughts I know the above is not possible strictly like that and it would need to be done on a table by table basis of the data to move across. but not sure where to start or how to do it. Theoretically(in my head) this would be done with splitting the clustered index on a filter across the filegroups (and the files just deal with it), there's options on a table properties for storage for filegroup or partition scheme, however on a clustered index this seems to always be greyed out so I'm not sure if I'm just clutching at straws there. NOTE: it is potentially possible to upgrade from SQL 2008R2 to 2012 if that is absolutely necessary(but not desired) EDIT: The query for this would ideally be based on data in another table, so DataToOldSAN = where userid in (select id from users where active =0) Thanks Ste 

Presumably your connection is secured so only your office IP can access the database. With that in mind a secure VPN tunnel that only directs out to the database area would provide security from them accessing any other systems And from the database perspective his access rights should be secured down to read-only which will prevent him from modifying anything. See $URL$ for more details on creating users, note the GRANT command, GRANT SELECT will allow them select access, so they cannot update, delete, insert or perform any other such actions. See $URL$ for more details on RDS authentication if that is required As for VPN that depends on the infrastructure you have setup, speak to your IT Admins about this 

Then any queries notice that the linked server is there, but has no access to anything within it. you'll find quite often that the queries for linked servers(as far as I'm aware) just check for the linked server (since you're declaring it as not as SQL database it doesn't seem to check in as much detail it presumes you know what you're doing (If you do it in the GUI you will get an warning but can still create the linked server without it registering that it can connect. Not sure if this is an acceptable workaround for what you're after since you are actually creating a linked server, but it is essentially a dummy as it goes nowhere, if you ever tried to access it you'd get a login error 

I get a report of deadlock preventing it, but nothing is being reported to sp_readerrorlog Any clues as to what's going on? 

NOTE: I've put section Y and Z as your additional sections as they don't appear in your example with what they are If that doesn't work directly here's some external links with more detailed examples: 

to catch up with the backlog from the night before (when most of our deletes take place) I'm wondering if there's any way to change the default settings from 5 seconds and 10 pages to say every second or run over 20 pages, is there any way of doing that or should I continue just spinning up multiple cleanup procs to clear out the data, or if there's any other actions that can assist with this Re-indexing runs on most effected indexs atleast once a week (most are every other day) SQL Server 2012 Enterprise SP3_CU8 (upgrading to CU9 tomorrow) on AlwaysOn High Availabilty Cluster also with replication (distribution on a separate server) 

You want a pivot table (which isn't a inbuilt function in MySQL, but there are workarounds) A rough example (I cant test this here): 

I may be understanding this slightly wrong so I apologise if I am. If you have two databases that you need to be identical and are on 2014 then use the AlwaysOn High Availability Group. Since you're data centres are at separate locations use the Async mode This will mean the database is kept completely up to date (all be it possibly with a few second delay) and you can have the secondary node as a read-only replica, this means that your alarm system can read into that database run all the checks etc you would normally. the Always on system keeps everything up to date, so if the connection drops, when it comes back online it will merge over all changes It also means that if your main centre goes down you can set it to automatically failover to the secondary, when the main datacentre comes back online it will re-sync with the (now) primary node, at which point you can fail it back over to your main centre. You can run this on multiple databases, so we have our main DB and our Admin DB synced across our nodes, however what runs all the jobs and direct actions on each side is not replicated so can stay independent of each other 

Am trying to use to run commands but I've come across an issue where in non-english characters (aka Chinese & Japanese) are changed to ??? when ran. sql server 2008R2 & 2012 Example Code 

Depending what the colum is for your userID (i've declared it as TeamMemberUserID which you are using later) Simply Join the timesheet table to the User Registration table on the TeamMemberUserID and select the user.name field End result looks something like: 

To answer question 2 first 2 A primary Key is NOT always the clustered index, it can be the clustered index and in the majority of cases is the way things are done, but it isn't always the best for your data. The culstered index is the order in which your data is physically stored on the disk whereas the primary key (which can be composite) designates the field to be a unique field and is beneficial for not inserting duplicate values and for foreign key lookups if you wish to do joins (In summary for 2, yes you can add a primary key without a clustered index) NOTE: sometimes if you want an identifiable row you can add a new field to the table to simply act as the primary key (not always advisable, but can sometimes be the a solution to improve performance) 1 Adding a primary key can change performance, however the only true way to know if it's going to improve performance is to test it, if you have a pre-live environment consider adding it to there and running your queries across it. If you have queries that run as joins to this table on ID,AID,BID all together (Sorry I'm not 100% sure how these are all coming together) the Potentially create a composite primary key across all three which means when anything wishes to get data with this table comparing all three of those it can find that row with ease. (Hope this makes sense) 3 Adding a Clustered Index completely depends on your data, once again a pre-live environment would be an ideal situation for testing. A few things to consider when creating a clustered index, what data are you retrieving and what are you inserting (this is a general example)If you're inserting and retrieving data that is the most recent data then a clustered index on the date field sounds the best idea, however if there is a LOT of data going in and out you will have very high contention on the most recent pages in your table, an alternative would be to have the clustered index around a category that those dates are on, eg client, this would mean that the data is grouped by a client which is more likely to have data gathered by, and spreads the read write load across the disk / disks If the data retrieve is very random then a clustered index is quite pointless, if the data you get back has no real order to it then a Heap is completely acceptable. Ultimately there is no be all answer to should I add a clustered index or a primary key because every situation is slightly different and will react in different ways. If you have a pre-live environment (even a cut down version) can help make your decisions. Personally we have tables with primary clustered composite keys, and some tables that are simply heaps. Hope this helps (and makes sense, sometimes I find I ramble)