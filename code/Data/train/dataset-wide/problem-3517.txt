I've run into the exact situation and used an external DVD drive, which shows up differently to the BIOS even when plugged into USB. When one wasn't available, I managed to plug a regular one directly into the motherboard and an external molex power supply, but that's a pain in the ass and you'll hate the damn thing before you even start the install. (Plenty of time for that later!) They had no PXE set up back then. PXE is really just tftp and dhcp, all you need is a client, so even if all you have is Windows you can still use tftpd32. If you can't configure your DHCP server for the proxy requests, you should temporarily turn it off and use tftpd32's. 

If you want to get deep into the registry, you can use the new Preferences control to define your own registry key, using: 

Download dd-wrt.v24-18000_NEWD-2_K2.6_mini-WNDR3400.chk Use the Netgear factory software to "Update" to the above download. Boot the router. Breathed sigh of relief when it came up. Configured the Radio and WAN. Downloaded dd-wrt.v24-18946_NEWD-2_K2.6_big-nv64k.bin Updated router to the big version from above. 

Not sure if I understand the question but you wont be able to have high availabilty of your virtual machines running on a host system. If you're already running an OS, then you need to add on a virtual machine service to windows. this would include: - virtualbox (free) - vmware server (free but being discontinued) - vmware player (free) - vmware workstation (paid) You can run VMs on each desktop, however you WILL notice a slowdown of the host system even if the VM isnt doing anything. CPU is only one concern - HD activity, network activity, ram, and most endusers will notice if you "sneak" a vm onto their workstation. If you're looking for high availability - to move vm around from machine to machine seamlessly - you're looking at commercial software from vmare/xen/microsoft. Using existing workstations is a bad idea as it will be slow and hassle to admin. What you're asking about isnt done in the real world. If you're wanting to play sure go for it, but if this is for a business/school/nonpersonal network it's bad idea to try to harness the "spare" cpu power, cannot stress that enough. It doesnt scale. 

If you don't have the intermediate chain certificates on your system, you'll have to swap for instead, which the provider always has available. 

The proxy server gets the certificate and all SSL configuration, and the Apache server doesn't need any. You can think of the proxy as unwrapping the SSL and forwarding plain HTTP back to the Apache box. (You can forward a new HTTPS session as well, if you're really paranoid about your own network, but then you have to put a cert on the Apache box too. It's better to set up an encrypted tunnel in that case.) You can do everything from the IIS manager, beginning with creating a request and ending with importing the pfx file from the provider. If the provider doesn't issue a pfx, then you'll have to manually combine it all into one file with openssl: 

My solution was to create an environment variable on every workstation, %PROGRAMFILES32%, that maps to either Program Files or Program Files (x86) as required. (I also created links to system32/syswow64 for the same reason.) As you've seen, there's no built in way to fix it. Alternately, since you need it primarily for one app, you could have the installer or a post-installation step add an environment var pointing directly to it. I did it through Novell Zenworks, but you could just as easily create it through a group policy startup script. 

I'm trying to migrate a dns server that has several thousand zones loaded on it. The named.conf file has about 17 different includes, and some of those files also has includes in them, and lots of commented out etc. It's a fricking mess! I'm wanting to get a list of all the zones currently loaded into BIND. I looked at rndc dumpdb but it doesn't show me just the zones. Instead of following the messy include files, is there an easier way to get a list of the authorative zones inside BIND? Thanks! 

Unfortunately this is a vague issue and it needs to be narrowed down. One of the first things that needs to be done is to make a chart clarifying what the problem is, and for whom. Chances are fixing the wireless network will improve your network greatly, but that might not be the real problem. One of the main questions would be , are the people who are physically wired in experiencing problems? If so, then that shifts the focus from the wireless issue to a server/application level issue, perhaps upgrade from older switches to gigabit etc. Wireless is not as good as a physical cable. Especially with consumer wireless routers/access points. They're designed for a few people to surf the internet, not a business network. Chances are the wireless network isn't operating as well as it should be. Few options: 

I get below error that it is looking for virtualenv==15.1.0 but installed version is 16.0.0. Is it possible to let certbot-auto use version 16.0.0? What can I do to resolve this error. 

I want to prevent my robots.txt file from syncing to the front end server. Here is my /etc/lsyncd.conf file: 

I'm having problems with the expires header. When I set the expires and access the file through the browser it becomes Not Found 404. Here is my virtual server setup on nginx.conf 

I carelessly did on my Amazon Linux AMI and updated Perl. Now my Movable Type install doesn't work. I did some google but could not find any solution except downgrading Perl to 5.14. Current version of the Perl is 5.16.3 Here is the error log for the Movable Type. 

In my Google Analytics realtime access I have about 700~800 users which is isn't many comparing during the day. So I don't think its due to high traffic. 

I am trying to the EC2 server but fails. This is first time it happened what do i need to do to resolve this? 

I would just have two mail servers, an internal and an external. Have the servers continually append outgoing emails to a file, and every so often rename the file, copy it on a USB key, and drop it in an incoming folder on the other server. This is how many installations perform air gaps in network servers. If it's too important to delay, it can be sent from one of the outside clients. 

There's only one group policy to do this, and it affects the entire action center, not just the firewall notifications, but if that's OK with you, then you should use it. 

Looks like it's because you can't contact the DNS server set at all, and it's trying to use the root hints (which, in my experience, are very unreliable for end-users). Can you ping it? Are you set to bridged mode, with no firewalls getting in the way? 

There's no chance of running mysql. If there's a way to use it with SQLite, it might actually be feasible. You'll have to put DD-WRT on it, following the instructions on the forum thread for the WNDR3400, recorded here for posterity: 

Hello I am using Amazon Ec2 with Nginx. I recently setup nginx and when I for example access blog.com/index.php it shows up in the browser correctly but when I access other file extension like JPEG, PNG, JS etc the file becomes 403. Here is the error log. 

I am new to Nginx. When I checked the error log I see this alerts going on for very long. Should I be concerned? 

I use Amazon EC2 large instance (Memory 7.5 GB) for my http server. Recently I noticed my ssh commands running slow so I checked the memory usage. 

So something is consuming a lot of memory. I twice a day restart httpd to free up the memory and about in 12hrs or so it gets full again. The daily average CPU utilization is below 20%. Here is the top comannd sorted by memory usage. 

I just installed Nginx 1.6.1 from source but it didn't seem to installed correctly. Nginx is running if I but when I do it outputs command not found. Regular HTML page shows fine and there is no error in the error logs. I am on AWS Ec2 linux AMI. Here is my script 

It's not pretty, but it works. Besides a GP Pref, you can also apply it with a login script. {E8433B72-5842-4d43-8645-BC2C35960837}.check.100 is the built-in Windows Firewall, check.101 and up are additional third-party firewalls. The registry keys in this folder are the ones that get changed when a user clicks "Turn off messages about (whatever)." in the Action Center. 

Perhaps either the script or the exe isn't visible to the computer executing the policy, for whatever reason. The GPO seems fine. Checked the event log for Policy errors or tried a traced log on? 

It's a RAID 1 mirror, right? Just unplug the bad drive, plug in a new one, and rebuild the RAID. No need to hyperventilate about data loss - as you would if you had no RAID. Obviously you should be backing up as a matter of course, even a free online backup of just your most important stuff, but that disk could go down in flames and you'd still be OK working off of a single one for a while. It's smart to replace early than take your chances on a double-failure though. 

So... was trying to add in a 2008 DC to a single 2003/exchange 2007 setup. ran adprep and updated schema, and joined the new DC to the AD... then 6 hours later noticed everything was not working. Restore tapes are offsite and not available for a few days so no easy option. The gist is the GC is not locatable and the sysvol isn't being shared. If you connect directly to the DC you can query all objects inside the AD properly, but nothing that queries the root domain itself works. went through the dns tree and eveyrthing seems proper. the server is pointing to itself for dns. dcdiag shows: Starting test: FsmoCheck Warning: DcGetDcName(GC_SERVER_REQUIRED) call failed, error 1355 A Global Catalog Server could not be located - All GC's are down. Warning: DcGetDcName(TIME_SERVER) call failed, error 1355 A Time Server could not be located. The server holding the PDC role is down. Warning: DcGetDcName(GOOD_TIME_SERVER_PREFERRED) call failed, error 1355 A Good Time Server could not be located. Warning: DcGetDcName(KDC_REQUIRED) call failed, error 1355 A KDC could not be located - All the KDCs are down. i have done ntdsutil and seized all roles anyway, confirmed under sites that the DC is a GC. it should work...google doesnt show what i want.... i'm good with AD but not good enough ;) Where do i go from here? 

I have EC2 m4.2xlarge running Nginx 1.6.1 and serving static HTML files. Recently my server started to give 503 server temporary unavailable. When I check the server process the CPU was all 100% load. Is this mean I am lacking server power or my nginx.conf configuration is not set right. I have this in the conf currently. 

I am having trouble having installing CGI module in Amazon EC2 Linux AMI. During the CPAN install process it fails. 

On my process monitor I see command running but its slow. If I ran another command will the deletion process be faster? Or this will make things slower. I am running this on 2 core machine. 

I am trying to install ngx_pagespeed to fresh AWS Ec2 Amazon Linux AMI but I get this below error.I don't understand what the error saids. 

I am not a Perl wiz but there where some dependencies issues? so rewriting the CGI.pm file is not a option. I tried but I receive dependencies error.