turn off CPU affinity first and then bind the ports to see if it works. This works Fine, and I am able to see NUMA 0 listening on one port and NUMA 1 listening on second port. This is what I want, but I dont want to turn off CPU Affinity. Am I in the right direction ? CPU Affinity ON. Try NUMA assigning to ports. This does not work in the sense that Instance1 is bind to NUMA 0 and Instance 2 does not show in error log that it is listening on NUMA 1. 

Doing HA in a mixed environment is a recipe for disaster because during disaster once you failover from lower (2012) to higher (2014) version, you cannot failback. Since you are using Enterprise edition of SQL Server, you can leverage AlwaysON technology wherein you can configure writes occurring on Primary and reads on the secondary configured as . Thinking a bit more on what you mean by mixed, I presume that transactional replication would be much better from sql server 2014 as publisher and 2012 acting as subscriber. The performance would be dependent on how big is the database, if you are replicating only the tables that you need or the entire database, the latency between the two servers as well as the amount of transactions that are occurring on your publisher. Note that if you go with T-Rep, doing schema changes will require a new snapshot. Also there will be a distribution database created as a part of setting up T-Rep. 

This has is_dynamic = 0 in the . This means that when you run , the VALUE will be set to , but it wont take affect until you restart sql server. You should check value in use for or run_value of the output. 

IT depends on what you want to actually monitor. Also do you want to log the data you have collected for baselining or performance trend reporting as well ? 

You can use tablediff utility to find out the difference in data between your Publisher and Subscriber server. Check How to run TableDiff utility for ALL replicated tables ? article by Chris Skorlinski on REPLTalk. This blog has all the required scripts that will help you out. If you have 3rd party tools like redgate schema and data compare, then your life would become much easier. As to find out what went wrong, you should check if 

If this is just one time issue and CHECKDB run without any errors and you have good backups, do not bother to investigate it (unless you want to spend time and resources to what is not reproducible). If you are able to reproduce the issue, then I would recommend you to 

What Neil has suggested is a viable option, but it is geared more towards Cloud environments and will be more complex than what you are trying to achieve. OPTION 1 : The simplest solution would be Transactional Replication (if you want near to real time data) or snapshot replication depending on your NEED. You can use Replication from Enterprise Edison to Web edition. Important to note that web edition does not support PULL Subscription. It only supports PUSH Subscription. Depending on your business needs, you can schedule to sync every 1 hr or 1 day. Replication support matrix is here OPTION 2 Use 3rd party applicaitons like Redgate schema and data compare for syncing data between 2 databases. It also has command line options, so you can write up your own custom scripts that will periodically sync data. You can get started here SQL Server also has an inbuilt data sync functionality which is commandline called "TableDiff". As you see, there are many options, which one will suffice your need depends on your Business requirements. 

Make sure that the Virtual Directory parameter is set to ‘ReportServer_’ (‘ReportServer_SQLEXPRESS’ for the SQLEXPRESS instance), and that the TCP Port is set to 80. Click Apply. In the Reporting Services Configuration Manager left pane, select Database. 

Apart from the common ones Full, Differential and Transaction log backups, you can consider below ones 

This way, you have a very less downtime and the risk of deployment failure on a live system (which is in maintenance window, since you are doing upgrade) will be highly minimized. Also, following the Blue-Green approach, you will be oscillating between LIVE and PREVIOUS version which will be staging for the next version. Again, this will require more hardware/licensing as well as planning and testing. Most of the steps can be automated using DACPACs and PowerShell. Also, if you are installing multiple instances on one server, make sure to re-balance the Memory settings when switching between Blue and Green. The LIVE environment gets more memory than the Passive environment. In my current environment, we have implemented Blue/Green Model for Agile Code Deployment that allows us to promote code every 2 weeks with ample amount of time for testing and business sign-off. Also, its a breeze to rollback in case something goes horribly wrong. We have automated majority of the deployment stuff using Dacpacs and PowerShell. 

A much better way and more scalable (when you have to do it on many servers) is to use Get-DbaDatabaseFreespace command from dbatools 

This is your problem. There were many fixes introduced in the consecutive CUs / SPs after the RTM build. Can you patch the sql server with the latest SP2 ? After patching, check if the issue still happens or not. Your problem might be because your database might be in simple recovery. If the model database is set to SIMPLE recovery model and user database is created with the simple recovery model from model database template, SQL Server does not truncate its log automatically like it suppose to (after a full backup). It appears that somehow SQL Server is treating it as if it is in full recovery model. you can execute below log backup (even though your database is in simple recovery - as since model is in simple recovery - due to a bug in RTM, sql server treats it as being in FULL recovery) 

You can see what types of locks are generated using DMV. Doing a reseed is very instantaneous and is not a big overhead. You just have to thoroughly test your idea for all possible cases to avoid any surprises. Also, out of curiosity, how is your solution more efficient than T-Rep ? As T-Rep is tested since many many years to be more efficient than any custom/home grown solution. 

As per KB 2728419 You have to install atleast SQL Server 2012 + SP1 + CU1. On a sidenote, CU8 is out as well which is the latest one for SQL Server 2012 SP1. Also, you need to be a member of SYSADMIN role. DB_OWNER wont work as per the above KB. 

Dont try to reinvent the wheel as there is a cost to reinventing it. Use Ola's maintenance scripts - they are tested well enough and follow best practice. You can even use LockTimeout in Ola's solution. I would not worry on the fragmentation levels where the page count is less than 5K. 

When you do locally, sql server will honor it, but when you use Linked server, sql server has to initiate a local transaction and promote / esclate to a distrubuted transaction. From --> How to create an autonomous transaction in SQL Server 2008 

The gist of Blue-Green Concept is to divide your production into 2 environments and they are identical all times (data synchronization) wherein 

You can happily do ONE-WAY logshipping from lower version to higher version. The only caveat is that once you failover to higher version, you cannot failback to lower version... same applies to Database Mirroring. This technique is normally used when you want to do migration from lower version to higher version with minimal downtime. 

As I mentioned in above answer, it depends on how much data you are replicating and the amount of transactions happening on publisher database. Better to follow your DBA recommendations if he/she has proper stats or baseline that will show the current behavior. 

Remember that the schedulers are not bound to cores and SQL Server does its own thread scheduling. For some reason, if a non SQL process maxes out CPU 1 which is running a thread on scheduler 1, then SQL server has the ability to put that scheduler onto any available CPU e.g. CPU 4. SQL Server has its own load balancer that will move a thread from one CPU to another. If you set processor affinity, then you remove the ability from sql server to switch scheduler. So scheduler 1 is bound to CPU 1 and it has to run there only. Some Excerpts from my book library ... From SQL Server Professional Internals and Troubleshooting Book : 

Is there a possibility for you to partition the table and purge the old partition instead of doing deletes ? FYI .. SQL Server 2016 and up allows you to truncate individual partitions as well. Also, you can (TEST and implement) - disable ghost clean-up (trace flag 661) and then option since you are using Enterprise edition. If you are using AlwaysON (with async) with replication, make sure to enable Trace flag 1448 - allows the replication log reader to move forward even if the asynchronous secondary replicas have not acknowledged the reception of a change. Make sure to read Deletes that Split Pages and Forwarded Ghosts from Paul White to see if your tables have triggers or LOB columns that would slow down. 

You can look into approach like incremental load using SSIS from staging to PROD database. You can apply different techniques when loading the data as well. 

!! CAUTION !! (use a careful batch size - its advisable to use this parameter if your table is massive): 

No tool is perfect and you should not rely implementing the suggestions without proper testing your entire business life cycle. Native tools like DTA are OK, but I have seen many problems with using it. I would not rely on it. That being said, SQL Server exposes a wealth of information using DMVs and Extended Events. I would suggest you start with : SQL Server Diagnostic Information Queries - Glenn Berry - choose as per your version of SQL Server. Also, highly recommend this FREE book from RedGate - Performance Tuning with SQL Server Dynamic Management Views You have to baseline your servers and then you can compare what is good and bad for your environment. Remember performance tuning is an art, not science. It will take you some time to digest the resources I have cited and this is how I have learned. You should be cautious when creating Missing indexes exposed by DMVs Other blogs that contain a wealth of information are (in no particular order):