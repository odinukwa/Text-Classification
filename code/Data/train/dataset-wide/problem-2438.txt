As you can see, is returning row numbers instead of row names; this yields an identical reordered data frame: 

Additionally, this part of the code hard-codes the choice of delta=60/61, which may make it harder to see what the code is doing or update to another delta value. It would be best to pass to the function and then use instead of in this expression. Here is the updated function: 

In addition to being a lot less code, this will result in sensible speedups for larger datasets. For instance, consider the performance with 10 million elements and 10 classes: 

The number of combinations of size from a set of size can be computed with in R, and if you have multiple values you can just pass in a vector for . Therefore your code can be simplified to: 

I think the task you are attempting to accomplish (obtain indices to reorder a data frame by several factors, one after another) is already handled well by the built-in function. Let's consider your sample data: 

These sorts of operations are tough to vectorize because you need to compare every element to all later elements. If the counting operation is a performance bottleneck in your code, then one possibility would be to implement it in C++ using the Rcpp package: 

It seems that your function takes as input a data variable name and rolling mean length and outputs information about the rolling mean with the indicated length, computed for each fips value separately. I see two key issues with the code as currently written: 

It seems that you are looping through seeds to find the one that causes a randomized procedure's output to match the output from a previous run. If you had set the random seed immediately before running the randomized procedure and have simply forgotten the seed you used, then this in theory would work; all you need to do is loop through the billion or so possible input seeds until one matches. There's no real way to speed up the process (beyond parallelizing, which would be easy because the problem is embarrassingly parallel). is just a wrapper on a loop, so that would not speed up the process. Unfortunately, more likely than not you did not set the random seed immediately before running the code. Therefore you would really need to test all the internal states of the pseudorandom number generator (PRNG) that you used to find the one that matches the results. Unfortunately there are intractably many internal states; for instance, the most popular implementation of the Mersenne Twister, which you are likely using, has a period of 2^19937 - 1, meaning it has at least that many possible internal states. Clearly it's impractical to test this many states, so it's probably hopeless to try to match an exact PRNG state if you hadn't set the seed immediately prior to running your randomized procedure. 

Another advantage is that the built-in function is more efficient than your implementation. Let's see on a resampled version of your dataset with 1 million rows: 

You are summing from j=1 through j=i by looping through the values individually and adding them up. In R you can get significant speedups by using the vectorized function: 

In two places you are growing objects element by element. Please see Circle 2 of The R Inferno for why this is an inefficient way to grow objects. A vectorized rolling mean function should be much more efficient than one you code on your own with a loop in R. 

I've tried refactoring it into smaller functions, but oddly, I felt that it would actually reduce readability, and I also found some parts hard to factor out well, because early returns are needed for error handling (and unless I'm willing to pollute my API with , which I am not, I can't just return to two call stack frames above the currently executing function). Do you have any advice, a technique, etc. that could be applied to this method (and potentially more of them, given that the cause of these problems, I feel, is similar throughout the code)? My second problem is way less specific - despite the fact that a colleague of mine and I spent several days coming up with the overall idea and with designing the details of the API, I just have a gut feeling that the code I've written is ad-hoc. It might not come through too well based on one single method, but I'm starting to think that I've written way more code than I should have for such a simple task. Even when my methods are not too long and not at all repetitive, I'm concerned with how many smaller or bigger internal helper methods I've created, how many properties I added to system classes through categories, and the like. How could I remedy that? 

I'm working on an XML-to-model-object serialization and deserialization library. There are three main design goals for the code: 

To meet these goals, I've wrapped the library in my Objective-C code, and used mostly functional style to implement the transforms (which are basically a of various functions and their composition over the tree nodes). The problem with the quality of my code is two-fold. My first concern is that I have several long methods. I like to have my functions fit in one screen (which is 25 lines for me - no jokes!), but here is, for example, my serialization (tree-to-XML-text) method which is over 80 lines long! 

It should parse XML into and serialize from Cocoa-native collection objects such as , and . Creating a custom representation (e.g. classes) should be avoided. This is mainly done in order to embrace the philosophy of JSON whereby the parser is able to perform a simple mapping from the string representation to a tree made out of the most common data types. XML is a very verbose format in that it has many ways of expressing the "contains" or "is child of" relation. E.g., unlike in JSON, both attributes and contained text nodes can but need not express the same semantics. Therefore, in order to preserve all the potential information in the original XML structure, we need to define a "canonical" representation for its syntax tree instead of just trying to be smart and somehow make sense of it in general. The format I've come up with is an for each node, where the type of the node (e.g. element, text, comment, etc.) is registered, and depending on this type, the children, attributes, etc. thereof are also stored in a collection object. However, the resulting tree is way too verbose to be either human- or easily machine-readable in the sense of being able to simply and immediately map it onto business logic model objects. So, the idea is to introduce two layers of "transformations" or "mappings". One of them takes the canonical (hence very regular) but verbose tree and transforms it into a more compact and readable representation which the user can interpret using semantics specific to their concrete use case. This, for one, gives them an opportunity to use already-existing object mapping libraries to generate model object from the collection-based tree. The other layer of transformations performs the inverse operation, whereby a simple collection-based structure is generated from business objects, and the transforms morph it into the canonical format from which textual XML can be generated. 

2.) Don't use abstract base classes for function objects 2.1) Use instead of abstract base class 2.2) Use instead of abstract base class [ Note: Or just take these as template parameters only. ] The rationale is that the way you do it is intrusive and a client has to define adapter classes to use your library. Since your abstract base classes only require virtual you are better of with which is just such a wrapper around any function-like type. Defining abstract base classes here might also introduce a lot of subtle misuses and errors. For example: you do not define a virtual destructor! Since you do not store your function objects polymorphically it seems to be okay in this case (and might leak otherwise), but on the other hand you take pointers to such objects without -checks... and your classes invite users to misuse them by letting the pointers dangle. I have no experience with fluent interfaces but I do not like this particular example. You gain pretty much nothing but a code bloat -- one selector-class for each parameter plus it seems to be easy to misuse. Maybe one can generate selector classes and avoid code repetition with Herb Sutter's announced metaclasses. IDK. 3.) Don't use an abstract base class to require a ForwardIterator Just rely on substitution errors or constrain your Node type with Concepts / + . This adds again unnecessary coupling for the client to your library. 4.) Issues in A*- 4.1) is not a good name. I suggest something that indicate what you search for (an element? a path!) Suggestions: 

always copies the arrays , deeply can be misused by passing and trusts the user that is chosen correctly. 

is an additional comparison in each step. Imagine your are not looking for s but in a vector of large s. Just make an -like search and test this bound. Schematically: 

Why is that? Whenever you move a pointer from to you move its ownership to close, but you never free any pointers form , only from . This 

If you want to initialise your data from any range use templates and constrain its type to guard for errors at compile time. Prefer regular functions for computations This means, that an implemented function is also a function in the mathematical sense. Same input data produces same output data. I found that this almost always leads to evaluations which do not change an inner state. Thus it smells to me that 

Prefer value type semantic Try to make classes default constructible and equality comparable whenever this makes sense. Currently your class doesn't fulfil neither. The following code will not compile 

You will see that implementing is very simple and beautiful in comparison to a direct . Edit: What I call here "upper_bound"-like search is implemented in the STL as not to confuse with . 

Note that copying is very cheap since its only a (pointer, integer)-pair and doesn't have any ownership associated with. 

1.) Issues in 1.1) You require nodes to be output streamable in You did not state this requirement in your description. Visualising nodes is not your concern here. 1.2) Don't store naked pointers to objects, they might dangle You only store a pointer to the nodes but you do not control at what time this exception is being caught. These pointers might dangle at the time this exception will be accessed. Since you do not require Nodes to be copyable (it might be generally too expensive anyway) you have no chance to store a node here.