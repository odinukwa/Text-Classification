As pointed out, your model isn't completely specified. Yes $n_1$ and $n_2$ are influenced by $N$ and $M$, but how exactly depends on the participants. For instance, you could decide to model each participant $i$ as having an estimate $p_i$ of the true probability $p$, drawn from a Beta distribution $B(p R, (1-p) R)$ for some factor $R$ and posit no risk aversion. That is however just one possible model among many. 

I'm looking for a distribution $P_{\theta}$ with pdf $f (t,\theta)$ over $\mathbb{R}^{+}$ such that there exists functions $\mu(\theta)$ and $\sigma(\theta)$ such that for all $t>0$: $$\mu(\theta)\frac{\partial f}{\partial \theta}+\frac{1}{2}\sigma^2(\theta)\frac{\partial^2 f}{\partial \theta^2} = \frac{\partial f}{\partial t} + f(0,\theta)f(t,\theta)$$ $P$ represents the probability distribution that an event will happen $t$ seconds from now. $\theta_s$ is a stochastic process representing a changing parameter. The constraint expresses the reflexion principle. There are of course infinitely many solutions, but I'm looking for a non-trivial one. If $f$ is an exponential distribution, for instance, the only solution is $\mu = \sigma = 0$, not a particularly interesting case. I tried a multivariate generalization on the gamma distribution and the Weibull distribution and their two parameters. It didn't work either (I think). 

Let $X = (x_1,\ldots,x_n)$ be an i.i.d sample from distribution $F%$ and let $y = \prod_{i=1}^n x_i$ Can we derive a randomized, unbiased. estimator $\hat{y}$ of $y$ that on average considers only a subsample of $X$? Weak conditions may be imposed on $F$, for instance we may assume it has finite mean and variance, we may also assume that $\forall i, x_i >0$, but otherwise nothing is known about $F$. 

A good way to think about a uniformely random point on the sphere is to think of it as the normalization of a vector of normally distributed i.i.d random variables. In fact, you don't even need to normalize. Consider hemisphere $v_i$ and $v_j$, form the $2 \times N$ matrix $V$, $v_i$ and $v_j$ being the rows of $V$. Let $X$ be a random Gaussian vector with covariance matrix $I$ (identity). $V.X$ is a vector with two components, whose sign represent membership in the hemisphere $i$ and $j$... $V.X$ follows a multivariate normal distribution with mean $\left(\begin{array}{cc}0\\\0\end{array}\right)$ and covariance matrix $n \left[\begin{array}{cc}1 & 1-2h/n\\\1-2h/n & 1\end{array}\right]$ where $h$ is the hamming distance. Notice that any covariance matrix can be approximated, up to a multiplicative factor, for $n$ large enough. The measure of the intersection of the two hemispheres is the integral of the the probability density over an orthant. For the intersection of two hemispheres, the orthant is a quartant and the intersection is given as $$\frac{1}{4} + \frac{\sin^{-1}(1-2h/n)}{2\pi}$$ (note that this is not the area but the fraction of the $(n-1)$ sphere area that is in the intersection) There is also a formula for the trivariate case, i.e. if you're considering the intersection of three hemispheres $$\frac{1}{8} + \frac{1}{4\pi}( \sin^{-1}(1-2h_{12}/n) + \sin^{-1}(1-2h_{23}/n) + \sin^{-1}(1-2h_{13}/n) )$$ Unfortunately, it is known that there is no closed-form expression of the probability of the orthants of a general multivariate normal distribution above the trivariate case. In your case, the covariance matrix have a certain structure, so it is not definite evidence that there is no formula for the area of the intersection of more than three hemispheres, but it is pretty good evidence. 

If you're looking for the asymptotic behavior, it may be of interest to approximate this process as a product of uniform distributions. The product of $t$ uniform random variables has probability density $$p_t(x) = \left|\frac{\log^{(t-1)} x}{(t-1)!}\right|$$ In particular $$\int_{0}^{1/n} p_t(x)~\textrm{d}x = \frac{\Gamma(t,\log n)}{\Gamma(t)} = \frac{1}{n}\sum_{k=0}^{t-1} \frac{\log^k n }{k!}$$ might be a decent estimate of $P(x_t=1)$ For instance, $P(x_2=1) \simeq \frac{\Gamma(2,\log n)}{\Gamma(2)} = \frac{\log n}{n} + \frac{1}{n}$... not quite $\frac{\log n}{n} + \frac{\gamma}{n} + O(\frac{1}{n^2})$ but not bad either. I conjecture (strongly) that the actual series is $$P(x_t=1) = \frac{1}{n}\sum_{k=0}^{t-1} E_{t-1-k} \frac{\log^k n}{k!} + O\left(\frac{1}{n^2}\right)$$ with $$E_j = \frac{(-1)^{j}}{j!}\int_{0}^{\infty} e^{-x} \log^{j} x ~\textrm{d}x$$ 

First, I want to establish the result properly If there are $N$ dates ($N = 365$) then the probability that the person in position $k$ wins is the probability that persons $1\ldots(k-1)$ have distinct birthdates and that person $k$ has a birthday matching one of these $(k-1)$ distinct birthday, thus $$p(w=k) = \frac{N!}{(N-k+1)!N^{k-1}} \frac{k-1}{N} = \frac{N!(k-1)}{N^k(N-k+1)!}$$ As you suggest, we can look at the ratio of successive terms, for $k>1$ $$\frac{p(w=k+1)}{p(w=k)} = \frac{k(N-k+1)}{(k-1)N}$$ It starts at a maximum of $2(N-1)/N$ and decreases monotonously to 0. The maximum is achieved for the lowest $k$ for which $k^2-k>=N$. For 365 days, this gives $k=20$ as you point out. I can't get the first quadratic exactly from a glance, but we should expect the answer to be $O(N^{1/2})$ as the number of potential collisions increases roughly like the square of the number of people considered at first. 

With m and n so small (about 10), either the change is large enough that it's going to jump at you by looking at the data or it's small enough that you won't be able to say anything very conclusive with a statistical test. If you insist on a formal approach nonetheless, MDL provides a framework. Write the shortest program $P$ that outputs an infinite time series that starts like the $m+n$ values and let $x = |P|$ the length of $P$ in bits. Then write two short program $P_1$ and $P_2$ that output infinite time series that start respectively like the first $m$ value and the subsequent $n$ values such that $y = |P_1| + |P_2| - |P_1 \cap P_2|$ is minimal, where $|P_1 \cap P_2|$ is the longest prefix of code shared by $P_1$ and $P_2$. If $x < y$ you can't really justify treating the two time series as different. Otherwise, you can look at the mean implied by $P_1$ and $P_2$ and see how they differ. 

The KL divergence is given by $$\hbox{Tr}(\Sigma_0\Sigma_1^{-1})- \ln(|\Sigma_0\Sigma_1^{-1}|)$$ Let $\mu_{-}$ be the smallest eigenvalue of $\Sigma_0\Sigma_1^{-1}$ and $\mu_{+}$ the largest, then $$\mu_{-} = ||\Sigma_1 \Sigma_0^{-1} ||_2$$ $$\mu_{+} = ||\Sigma_0 \Sigma_1^{-1} ||_2$$ $$\hbox{Tr}(\Sigma_0\Sigma_1^{-1}) > n\mu_{-}$$ $$-\ln(|\Sigma_0\Sigma_1^{-1}|) > -n\ln(\mu_{+})$$ $$KL > n ||\Sigma_1\Sigma_0^{-1}||_2 -n\ln(||\Sigma_0\Sigma_1^{-1}||_2)$$ With the lower bound on the KL you can get a lower bound on the distance you're looking for. It doesn't seem you can get away with looking at the norm of the difference between the matrices. 

$$N(\mu X,X^2) \sim XN(\mu,1) \sim \mu X + XN(0,1) \sim N(\alpha \mu, \beta \mu^2 + \alpha^2) + \sqrt{\beta}N(0,1)N(0,1)$$ You thus have a sum between a normal distribution and a normal product distribution. 

Using the pareto distribution $f(x) = \frac{\alpha}{x^{\alpha+1}}$ ($x > 1$) , the ratio $\frac{E(||X-X'||)}{E(||X||)}$ approaches a $2$ as $\alpha$ tends to 1. To find such a distribution, consider that all else equal, you want to maximize the difference $||X||-||X'||$ since the angle between the two is independent. This means that you want a distribution that extends to infinity as flatly as possible. 

f you use the notation suggested in the mathexchange question: o is the number of baskets with one ball, s the number of baskets with more than one ball and n the total number of baskets, then the Bellman equation gives you $$E(o,s) = \max\left(o-2s, \left(1-\frac{o}{n-s}\right) E(o+1,s) + \frac{o}{n-s}E(o-1,s+1)\right)$$ Now define the optionality function $V$ which represents the value of continuing shooting instead of just stopping $$V(o,s) = E(o,s) - (o-2s)$$ Thus $$V(o,s) = \max\left(0, \left(1-\frac{o}{n-s}\right) V(o+1,s) + \frac{o}{n-s}V(o-1,s+1) + \left(1-4\frac{o}{n-s}\right)\right)$$ Lemma $V(o,s) \geq \max(V(o+1,s),V(o,s+1))$ This can be verified by induction or by noting that having more balls in the baskets does not open up more options. Thus, the optimal strategy is to stop playing when $4o+s\geq n$ The expected value of the game can be computed with dynamic programming. 

Let $\Sigma$ be a hermitian positive definite matrix and $L$ be it's Cholesky decomposition so that $LL^\ast=\Sigma$. Furthermore, let's diagonalize $\Sigma$ as $\Sigma = P\Lambda P^\ast$. $\Lambda$ is a diagonal matrix containing the real, positive eigenvalues of $\Sigma$, let us denote as $\sqrt{\Lambda}$ the diagonal matrix whose diagonal elements are the square roots of these eigenvalues. What are some non trivial relationships between $L$ and $(P,\Lambda)$? The one I have is: Since $(P\sqrt{\Lambda}P^\ast)$ is the unique positive square root of $\Sigma$ then $U = L^\ast P \Lambda^{-1/2} P^\ast$ is unitary. ... and that's about it. Maybe it counts as trivial? Are there other interesting relations, maybe relations that take into account $L$'s triangular structure? In particular, I'd be interested in algorithm that derive $L$ from $(P,\Lambda)$ or vice-versa (and obviously which aren't merely the trivial composition of two algorithms). 

What you're looking at is a Gaussian Markov Random Field (GMRF). Here's a presentation on the topic that explains how to take advantage of their sparsity. The trick seems to be to work with the precision matrix, not the covariance matrix. 

Not an answer yet, but some thoughts that may lead to one... Let $$G(z) = \int \mathbb{1}_{\pi(y)\geq z} ~dy$$ then $$I = \int \int_{x,y} \min( \pi(x), \pi(y) )~dx~dy = 2 \int \pi(x) G(\pi(x))~dx$$ $$I = 2 \int_{0}^{\pi_{\max}} z G(z) \int \mathbb{1}_{\pi(x)=z} dx dz$$ $$I = 2 \int_{0}^{\pi_{\max}} zG(z)G'(z) dz$$ edit: seems there's already an elegant answer 

You could model the problem by assuming that your $n$ numbers are drawn from a multinomial distribution with a parameter drawn from a flat Dirichlet prior. Draw a fixed number $p$ of integers, then run a MCMC on the space of parameters of the multinomial distribution. Integrate the random variable $N$ over this chain. 

The problem is that even though you may obtain a unbiased estimate $\hat{S}$ of the stochastic matrix $S$, $\hat{S}^k$ is not an unbiased estimate of $S^k$, the k-steps transition matrix. To account for the convexity, you need to put a prior on your transition, for instance an independent Dirichlet prior on each column. Observing the Markov chain will give you a Bayesian update to that distribution (which is conveniently a conjugate prior). You can then sample whole chains, or even marginals within the chain, but every single chain will unfortunately represent a point hypothesis over $S$. It's important to realize the posterior of the continuation of your chain is in general not Markovian. 

A sufficient condition would be that the tails of $f$ decay as $O(x^{-1-\epsilon})$, which is most distributions you might encounter over $\mathbb{R}$. That being said, the differential entropy of a continuous pdf isn't really a meaningful physical, or information theoretical, quantity. The equation isn't homogeneous: changing the units in which you measure $x$ changes the differential entropy (dimensional analysis can lead to a surprising amount of mathematical insights, even outside of physics). What does make sense is the KL-divergence of one distribution with respect to another. For a pdf with a compact support, you're implicitly looking at the divergence with respect to the uniform distribution. However, there is no "uniform" distribution over the real line and thus the concept isn't meaningful. 

The cross-entropy method will easily allow you to approximate $\mathcal{P}_{q,\epsilon}$ as an ellipsoid, which is likely reasonable if $\epsilon$ is small enough ($q$ is a global minimum so the hessian is semi definite positive around $q$) The idea is to iteratively find a multivariate normal distribution that minimizes its KL-divergence to the distribution $\mathbf{1}_{\mathcal{P}_{q,\epsilon}}$. This will then allow you to efficiently generate random samples from $\mathcal{P}_{q,\epsilon}$. Note that the C.E method uses KL-divergence, but it has nothing to do with the fact that the problem is about KL-divergence. The answer would be similar for many other types of balls. 

Consider the problem where we want to find a maximum likelihood estimate of $\theta$, given $X$ and $$P_\theta(Y) = \sum_z P_\theta(Y,x)$$ where $x$ is a latent variable. I know that the soft EM algorithm guarantees that every step will not decrease the likelihood of the observed data. I'm pretty sure it is also true for the hard EM algorithm, but I've been stumbling on a test case where a step of the hard EM seems to decrease likelihood function. EDIT: Yeah, hard EM definitely does NOT converge... Toy example $x \sim {0,1}$ is a latent random variable, (a coin flip) $y \sim N(x, s)$ is the observed random variable $s > 0$ is a parameter There's only one observation, -0.75 Let's run hard EM, shall we? No matter how we initialize s, the likelihood is maximized for $x=0$. Assuming $x=0$, the maximum likelihood is given for $s = 0.75$. But what is the real likelihood of this model? it's $p(-0.75 | (x=0,s=0.75) ) * 0.5 + p( -0.75 | (x=1,s=0.75) ) * 0.5$ that's about $0.179$. Yet, the maximum likelihood is achieved for $s \sim 1.11$ and it is about $0.195$ or $8.94\%$ better. It's not a local maximum problem either, we could have initialized s at 1.11. The reason hard EM works in practice is that when you use the Viterbi algorithm on a long stationary chain in state space, you effectively sample the whole distribution. EDIT:The more I look at it, the more it seems that the "hard EM" does not guarantee convergence to a local maximum of likelihood! Wikipedia somehow seemed to imply it did (I just stuck a big citation needed on that), but looking at $URL$ I read "Convergence is guaranteed; note that the algorithm (unlike Baum-Welch) does not maximize the likelihood P(Xm | Π) but contributions to P(Xm | Π, Q(Xm)) as a function of the parameters Π." Yes, that makes much more sense. The HARD EM maximizes the likelihood taken at the best point. Can anyone confirm that with more sources? By hard EM I mean $$x_t = \arg \max_z\left(P_{\theta_{t-1}}(Y,x)\right)$$ $$\theta_t = \arg \max_\theta \left(P_{\theta}(Y,x_t)\right)$$ I've verified manually that $x_t$ and $\theta_t$ is computed correctly, and I've verified the likelihood function... If you'd like to try, the problem is the following, I'll write the parameter $\theta = \Sigma$, $Y = (A,B)$ and $x = X$ $$P_\Sigma( A,B, X) = \mathbf{1}_{AX=B} \frac{e^{-\frac{1}{2}X^tS^{-1}X}}{(2\pi)^{NM/2}|S|^{1/2}}$$ where $S$ is the block diagonal matrix, each block of size $M$. $\Sigma$ is always symmetric positive definite. $$ S = \left(\begin{array}{cccc} \Delta_1 \Sigma & 0 & \cdots & 0\\\\ 0 & \Delta_2 \Sigma & \vdots & \vdots\\\\ \vdots & \vdots & \ddots & \vdots \\\\ 0 & 0 & \cdots & \Delta_N \Sigma \end{array}\right) $$ Here's an example (copy paste from html won't work well, so if you're a kind soul who wants to give it a try, you can get the data here as well $URL$ ) Set $M=3$, $\Sigma_0 = I$, $N=10$, $\Delta =$ $[1.19571,0.98553,1.48534,1.72635,3.4422,0.96689,0.01366,0.53092,1.1134,0.9718]$ $$A^t = \left(\begin{array}{cccccccccc} 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\\\ 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\\\ 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\ 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\ 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\ 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\ \end{array}\right) $$ $B^t = (-2.46269,-1.06192,0.16578,-3.390909,1.619429,3.33237,-1.00818, 0.90752)$ I find that $P$ is maximized when $$X_1 = \left(\begin{array}{c} -0.546023113539513\\\\ 0\\\\ 0\\\\ -0.450044162425061\\\\ 0\\\\ 0.645924400969046\\\\ -0.678282922487939\\\\ -0.616831349169687\\\\ 0.973504460926985\\\\ -0.788339891896596\\\\ -0.716918568383787\\\\ 1.11304689180543\\\\ -0.829046405178143\\\\ -1.42947697785166\\\\ 2.21932425211845\\\\ -0.232873327013359\\\\ -0.401529911986066\\\\ -0.371369961109884\\\\ 0.165779960962481\\\\ -0.00567269366556661\\\\ -0.0052466806358363\\\\ 0\\\\ -0.220480684975234\\\\ -0.203920096563427\\\\ 0\\\\ 0\\\\ -0.427643477547802\\\\ 0\\\\ 0\\\\ 0.907519813639624 \end{array}\right)$$ And given $X_1$ $P$ is maximized for $$\Sigma_1 = \left(\begin{array}{ccc} 0.339228113 & 0.098120669 & -0.17565\\\\ 0.098120669 & 0.140817533 & -0.15471\\\\ -0.17565382 & -0.154705723 & 0.444462 \end{array}\right)$$ However, I find that $P_{\Sigma_0}(A,B) = e^{-10.8816}$ and $P_{\Sigma_1}(A,B) = e^{-12.843}$