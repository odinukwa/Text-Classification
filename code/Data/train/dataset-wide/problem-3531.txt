I use Webalizer and AWStats. Both will analyse the W3C Extended Log Format, both are licenced under the GNU General Public License (GPL). There are differences between the two in the format of displayed results and if you are producing stats for many sites some users may prefer one to the other. I have also used analog in the past but its main advantage for me was raw speed and I don't now have log files big enough to make it worthwhile using a third analyser. There are differences between the three in the format and extensiveness of displayed results and if you are producing statistics and graphs for many sites some users may prefer one to the other. Many web hosting companies provide a couple of these for online use by customers. The platform you are using it on can be an issue. All three (and more!) are available in GNU/Linux distribution repositories, e.g. Ubuntu 9.04, and are usually ready to run when installed. It may require more work to get the one you want working on other platforms (e.g. I think awstats needs perl; webalizer comes from the author as C source code or as a Linux-x86 or solaris executable). To make a choice between the many analysers available you need to decide which reports you need, which you would like and which you don't want and compare that list to what the various tools offer. Tailoring the output to what you want makes it easier to get out the information you need. You may want to consider several runs through the data to provide different reports, possibly using different tools. For example, a fast run through just to identify unexpected errors (missing files or graphics (404) referenced by other pages on the site and unexpected error codes) can be helpful to the site administrator. Data providers are less interested in those reports but may want to know which pages are most popular, search strings used and numbers of visitors. Network administrators may want to know average and peak total load and which pages generate most load so that they can ask when they have been optimised correctly. Eventually people start to ask questions that none of the tools answers well but experience with several different analysers may postpone that day for a while. Not analysing the logs from the server but relevant to the area, Google provide Webmasters tools that give information about the site from Google's perspective, gained from site crawls. As well as showing the sites ranking within Google on certain search terms and which other sites link to yours it has other information such as which pages Google is not indexing (e.g. because of robots.txt restrictions) and which pages it cannot find. These are a useful adjunct to the log file analysis on the server when looking for errors and missing material on the site. 

Apart from resilience, multiple nameservers can provide latency reduction (e.g. where the nameservers are, for example, on different continents) and many domain registries require that a domain name has more than one authoritative nameserver. Two is a minimum for registration and this should be maintained subsequently. Many domains and registrars will use more. A typical maximum is 13. 

If you don't think you need a firewall, think again. ufw is simple but designed for Ubuntu and based on iptables Update the packages: as a minimum apply all the security patches Document what you've done to secure the server and why. Include configuring (automated) processes to monitor logs, test the configuration and report security updates that are needed. 

If you want to refer to part of the hierarchy of one site as another, this is possible in the same way, e.g. the second entry above could be 

A few years since I had to do this and there must be better solutions available by now: I used a custom expect script to fetch and backup the live switch and router configurations and diff'd for changes. 

First network transmission (typically a BOOTP/DHCP request) Time for network load/DHCP set-up and server(s) being used Time for DNS responses and server being used Time to end of start-up network activities (e.g. licence/update checks) Time for browser home page start/end Time to quiescent (if ever) 

Some techniques to get more information. Write down what you know about the network settings on each device, draw a diagram of how things connect. As you check things and run tests consider whether this information needs adding to or if any of it is wrong. With two DHCP servers on one subnet how do you know that all the devices will get the settings you expect? For example, if the allocation is by MAC address, check the the DHCP server settings carefully and check that PC0 and PC1 consistently get the details you planned for. To distinguish between a DNS and a routing problem, try pinging between PC0 and PC1 and between each of these and each of the DNS servers and routers by IP address using ping -n , or its equivalent in your OS. [ping -n tells ping not to look up the names of hosts when displaying its output, by using this and pinging to an IP address we can avoid the need for DNS in this test]. Some of the responses may not happen: this could be security features on the device or security constraints working correctly or a problem: write down what you tested and what the result was and then think about it. Check the setup of PC0 and PC1 including which nameserver(s) should they use (these must be specified as IP addresses, not as names) and domain names (if the two subnets have different domain names then these need to be specified explicitly or be in the domain search list in the client DNS setup). Check that there are no settings in the hosts files on the PCs that could clash with names that should be resolved using DNS. Try pinging between devices by name: even if the ping fails it will report whether it could resolve the name to an IP address. Use dig or nslookup to query the two DNS servers to see whether they can both respond with resolutions for the names of PC0 and PC1. These queries should be done to both nameservers from devices on both subnets (for example dig @ will attempt to use to resolve ). Check the routing tables of PC0, PC1, the two DNS servers and the two routers. See if all have routes to the 172.16.2.x and 172.16.1.x subnets. I assume, but you should check, that these are all /24 networks (subnet mask 255.255.255.0) but you need to check that all the devices have the same subnet mask setting and that this is reflected in the routing tables. traceroute between PC0 and PC1 (both ways) will confirm that traffic is taking the route you would expect. Somewhere in here I would personally be using a sniffer. You might not be able to interpret the output from that yet and, hopefully, some of these simpler checks and tests will be helpful. This should all contribute to a clearer picture of what does work, what doesn't work, whether these states are stable (i.e. it either works or doesn't rather than works sometimes) and, probably, why. 

seems to support this view but I expect you are probably more familiar with the code than I am and there will be folk on the mailing list who know it in detail and could be more definitive. 

I don't use Microsoft windows so can't advise directly on reliable sources for tools on that platform (I use dig and nslookup under GNU/Linux) but there are plenty of online gateways to dig and other tools, including nslookup, that may be sufficient if you are looking at Internet-based problems. A Google search will turn up lots. I just tried $URL$ and $URL$ and both seemed to work well. Additionally, ISPs often provide these tools on their websites for customers who don't have native tools on their own computers. 

Server A is a database server that you want to monitor in detail. You can obtain relevant stats on this machine that you want to display using MRTG but you do not want to run MRTG on this server. Server B is a server running (possibly multiple instances of) MRTG and Apache. MRTG will collect stats from a variety of servers and network devices using SNMP and other protocols, update its databases and the display pages for each statistic monitored. This server would usually also display pages relating to the structure and configuration of the network (some static, some dynamic content). There are multiple HTTP clients wanting to see the MRTG web pages on server B. If there was only one client then Apache is not needed on server B as a suitably privileged client could display the MRTG HTML files without the need for a web server (e.g. using )