Whenever a number matches the LHS of a rule, it can be replaced with the RHS. The Collatz conjecture is that we can always get to $1$ with the above ruleset. Conway proved that a similar generalization is universal. But the ruleset here doesn't have to be a single function — the "relaxed" Collatz conjecture can be expressed this way too: 

And we can construct some very short rulesets describing various number theory problems. For example, here is the Goldbach conjecture: 

My question is about when we extend this language with a multiplication symbol $\times$, subtraction symbol $-$, additional natural number variables $y, z, \dots$ and variables $p, q, r \dots$ restricted to taking prime values. Now we can also represent this prime-bifurcating Collatz-like function: 

Yes, because any chess position can be translated into Presburger arithmetic. For a fixed initial combination of piece types, let's define a position to consist of an (x, y) location for each piece as well as a bit (c) to indicate whether or not it has been captured. Multiplication of these parameters is not required to describe the legality and the effects of any one move, so in Presburger arithmetic we can recursively define the proposition "White cannot capture Black's King in fewer than t moves starting from initial position X.", then apply the axiom schema of induction to get an expression meaning "White cannot ever checkmate starting from initial position X." Since Presburger arithmetic is complete we will always be able to prove either this statement or its negation. EDIT: Summary of how this is supposed to work: 

I'll try to answer part 2 with a reasonable exponent. I claim that a proof can be verified in $\tilde{O}(n^2)$ time on a multi-tape Turing machine: define a proof to consist of a list of statements such that each statement is an axiom or a tautology recognizable in essentially quadratic time (including instances of all the usual rules of inference expressed as implications), a conjunction $A \land B$ of two earlier statements $A$ and $B$, or the conclusion $B$ of an earlier instance of modus ponens $A \land (A \rightarrow B)$. I believe the adequacy of this definition follows from the completeness theorem, but I've left open exactly what constitutes a recognizable tautology or axiom schema instance and how to recognize it. This certainly depends on the theory, since simply being effectively axiomatized doesn't mean we can recognize the axioms quickly enough, but I believe this can be accomplished for theories like $\text{PA}$ and $\text{ZFC}$ on a multi-tape Turing machine in essentially quadratic time. Without going into details, the verification proceeds one statement at a time from beginning to end, scanning backwards to find the earlier statements, which should make for an essentially quadratic time algorithm. Translated to a single-tape Turing machine this algorithm runs in time $\tilde{O}(n^4)$. Now we can rephrase the search for a proof as a bounded halting problem on a non-deterministic single-tape Turing machine yielding a $\tilde{O}(n^8)$-bit instance of $\text{SAT}$ according to the linked paper. There is a quadratic lower bound on the time required to verify a proof on a single-tape Turing machine, which can be proved by a crossing-sequence argument applied to the problem of recognizing any formula that substitutes the same formula more than once, for example $A \lor \neg A$. So we won't be able to get better than $O(n^4)$ bits using the same reduction to $\text{BHNTM}$. It's unclear to me whether or not it's possible to achieve essentially quadratic verification with one tape, but with multiple tapes it seems straightforward enough, and that suffices for the $\tilde{O}(n^8)$-bit upper bound. Assuming $\text{P} \ne \text{NP}$, no polynomial-time reduction to an $o(n)$-bit $\text{SAT}$ instance is possible. That's because there's a polynomial-time reduction in the other direction, from $\text{SAT}$ instances to $O(n)$-bit proof searches. If one existed we could make a sufficiently large $\text{SAT}$ instance at least one bit smaller in polynomial time by transforming it into a proof search and back again. Likewise, $\text{ETH}$ implies there's no $2^\text{o(n)}$-time reduction to an $o(n)$-bit $\text{SAT}$ instance. For part 3, I know that RH has been reduced to a $\Pi_1$ sentence, and the instances of this sentence have essentially linear proofs, so a refutation won't be much longer than a counterexample and we can use the above method. Of course this is not at all a practical approach as with discrepancy, unless the refutation turns out to be much shorter than the counterexample. 

Equivalently, the weak version is that there is a deterministic algorithm to find a prime $p > 2^k$ which runs in time $F(k)$ on infinitely many inputs $k$. Another formulation of the weak problem is to determine the values of $\displaystyle\liminf_{p\rightarrow\infty}{\frac{L(p)}{\log_2(\log_2(p))}}$ and $\displaystyle\liminf_{p\rightarrow\infty}{\frac{L(p)}{\log_2(p)}}$ where $p$ is prime and $L$ is the Levin complexity. I consider both problems to be defined with respect to the class of models of computation possessing a time-translation to a single-tape Turing machine satisfying $T' \in T^{1+o(1)} \cdot S^{O(1)}$, where $T$ is the time used in the other model and $S$ is the space. This class includes the various RAM models and multi-tape Turing machines. However, the equivalence is sharper than a polynomial time-translation, and we can witness a specific value of the time exponent with an algorithm satisfying $S \in T^{o(1)}$ in any model in the class. For example, searching an interval $[2^n, 2^n + 2^{\epsilon \cdot n})$ for a prime can be accomplished in time $2^{\epsilon \cdot n + o(n)}$ in all of these models because the AKS test is simultaneously in subexponential time and subexponential space. But we won't be able to place the AKS test itself in a particular time class like $n^{6+o(1)}$ unless we can adapt it to simultaneously use space $n^{o(1)}$, until then the best we can say is that its runtime is $n^{O(1)}$ in every model. If either finding primes problem can be solved in polynomial time, assuming model-invariance won't make it any harder to prove that, and (suspending disbelief) it can only help to prove a lower bound on the exponent of exponential-time algorithms. It's plausible that it would present an obstruction to improving the exponential upper bound — for example, an algorithm that finds a prime $p > 2^k$ using $p^{\frac{1}{3}}$ time and $p^{\frac{1}{3}}$ space on some particular machine wouldn't qualify as an improvement under model-invariance. That's because the time exponent doesn't translate when the space is exponential — $\frac{1}{3}$ becomes $\frac{b+1}{3}$ after a $T' = T \cdot S^b$ time-translation. The exponential-time algorithms I refer to below are all in subexponential space anyway so this issue doesn't seem to actually come into play. For the ordinary version, all we know is $F(k) \in 2^{0.525 \cdot k + o(k)}$, and I haven't been able to find any better bound for the weak version. My understanding from the polymath page is that we don't know if having factoring for free can help us find primes. Can it help us find primes infinitely often? An attractive aspect of the finding primes problem is that there are many conjectures which imply improvements. For example, the Riemann hypothesis puts $F(k) \in 2^{\frac{k}{2} + o(k)}$, Cramér's conjecture implies $F(k) \in k^{O(1)}$, and $\text{P}=\text{NP}$ implies $F(k) \in k^{O(1)}$. For my version of the problem, we additionally have that infinitely many Mersenne or Fermat primes gives $F(k) \in k^{O(1)}$, infinitely many $n^2+1$ primes implies $F(k) \in 2^{\frac{k}{2}+o(k)}$, and Bunyakovsky's conjecture implies $F(k) \in 2^{o(k)}$. Some other conjectures have similar implications. A world where we can't find primes infinitely often would be a very weird place, even weirder than one where we just can't find primes! That is my main motivation behind studying this problem, to try and understand what that world would be like. I'm looking for more information about finding primes infinitely often. In particular, is there any better time bound than what is known for the ordinary version? I haven't found it discussed in the polymath threads or elsewhere, and I haven't identified any open problems that any improvement implies, despite all the open problems that imply improvements. So for all I know, there's a simple and provably fast algorithm that I just can't think of myself. 

As mentioned, this follows from an effective form of Cramér's conjecture, so we won't be able to disprove it. But we won't be able to prove it either, because it implies the existence of an algorithm to find an $n$-bit prime in polynomial time, another open problem. The algorithm searches for a prime using the AKS primality test by starting at $2^n+1$ and counting up until it finds one, requiring total time $n^{8+o(1)}$ if your conjecture is true. But the state of the art in finding an $n$-bit prime is an algorithm requiring $2^{0.525 \cdot n + o(n)}$ time — in fact, it's the same algorithm, it's just not known to run any faster than that. 

I spent some time working this problem and discovered the following generalization. There's no new information here about the $2^{x-1}+5$ problem, so this is not much of an answer to that specifically. But we can say some similar things about some similar functions. Let $F(x)$ be a composition of functions $x$, $c$, $c^\square$, $\square + \square$, $\square \cdot \square$, and $\square!$. For example, we might have $F(x) = 2^{6^x + x^2} + (x !)^2 + 3^x \cdot x - 3$, but not $F(x) = x^x$. Let $F^k(x)$ denote the $k^\text{th}$ iterate of $F$, so for example $F^2(x) = F(F(x))$. Lemma: $F^k(x) ~\text{mod}~ m$ is eventually periodic in $k$. Proof: First re-write $F(x)$ so that all the bases are factored into primes, for example $F(x) = 2^{6^x} = 2^{2^x \cdot 3^x}$. Now with $m = p^a \cdot b$ and $(p,b) = 1$, define $g_p(m) = \text{ord}_b(p)$, and observe that $p^{a+x} \equiv p^{(a + x) ~\text{mod}~ g_p(m)} ~\text{mod}~ m$. Assume as an inductive hypothesis that $F^k(x) ~\text{mod}~ n$ is eventually periodic in $k$ for all $1 \leq n < m$. By taking all the exponents $\text{mod}$ an appropriate composition of $g$ functions we get a function $G$ such that for sufficiently large $x$, $F(x) \equiv G(x) ~\text{mod}~ m$ — for example, if $F(x) = 2^{3^x+x}+x^7$, consider $G(x) = 2^{3^{x ~\text{mod}~ g_3(g_2(m))} + x ~\text{mod}~ g_2(m)} + x^7$ — then for sufficiently large $k$ we have $F^{k+1}(x) \equiv G(F^k(x)) ~\text{mod}~ m$ and combined with the inductive hypothesis and $g_p(m) < m$ this implies $F^k(x) ~\text{mod}~ m$ is an eventually periodic function of $k$. Factorials are allowed too since they are eventually equal to $0 ~\text{mod}~ m$ and we can remove them from the expression, but I'm not sure how to handle general $\square^\square$ power compositions or primorials. $\square$ Corollary: If $x$ never occurs outside of an exponent in the expression defining $F(x)$, like $F(x) = 2^{x-1} + 5$ and $F(x) = 2^{7^x+x}\cdot 5^x +3$, but not like $F(x) = 2^x + x$ or $F(x) = 2^x \cdot x$, call it restricted. Then $F^k(x) ~\text{mod}~ m$ is eventually fixed (periodic with period $1$) for all $m$ if and only if $F$ is restricted. However, this doesn't really explain why it should be that $F^k(x) ~\vert~ F^{k+1}(x)$ as requested. It's simply a generalization of the observations in Update #1, but it applies to a much wider class of functions than I originally suspected. Corollary: For all $x, m \in \mathbb{N}$, there exists a $q \in \mathbb{Q}$ such that for all $k \in \mathbb{N}$, $F^k(x) \equiv \lfloor q \cdot m^k \rfloor ~\text{mod}~ m$. If $F$ is restricted then $q$ has a denominator of the form $m^z \cdot (m-1)$. An idea I have is to compute some of these rationals for various $F, m, x$ and find cases with the same $m$ and two different functions $F$ and $G$ where the corresponding rationals have some of the same base-$m$ digits at the same positions. Since it is impossible to evaluate iterates of $F$ and $G$ beyond small arguments, if there is no obvious reason for this relationship, then determining for exactly which $k$ is it the case that $m ~\vert~ F^k(x) - G^k(y)$ may turn out to be a good puzzle problem requiring essentially the argument above plus calculations.