I have a 1.2Tb SQL Server 2008 R2 database which is more than 50% empty, because I migrated a lot of functionality to PostgreSql. The database is still in use, and still somewhat growing, but I do not think it will grow to use all 1.2Tb in the next 3-5 years. At the current rate of growth, we are not going to use up 1.2Tb in more than 14 years. More to the point, we keep migrating functionality to PostgreSql whenever we need to make serious changes. In a very dynamic agile environment, this means we frequently migrate data away out of this SQL Server database. So, this database is likely to grow even slower in the future: it is being phased out, eventually. As such, we'd rather use this unused empty space for other databases and such - moist likely it is not going to be needed for growing tables/indexes ever. I recall that the rule of thumb used to be "never shrink databases". Does it apply in this case? Currently the server is running 2008 R2 EE, but we plan to upgrade to 2012 very soon. I think because this shrink functionality exists, there should be valid use cases where it makes sense to shrink. Otherwise why does it exist at all? We can afford a few hours of downtime during a weekend. Edit: the problem we are solving is as I said above: "we'd rather use this unused empty space for other databases". Also we'd rather prefer a solution that does not require investing too much time in learning the technology we are migrating out of. 

This seriously depends on the expected half-life of your project, on how agile your environment is and so on. If your project is there to last, and changes are possible, I would not assume that your composite PK is never going to change. I have seen too many projects burned by such assumptions. One good example would be the use of Social Security Number. Long ago it was a common practice to use it as a natural PK. Later on, many systems had to restrict its use for privacy reasons, and it became not possible to propagate SS#s all over child tables. The systems that used identities adjusted to the change easily. The systems that used SS#s in child tables had to go through a major overhaul. So, in many cases it is cheaper to use identities as PKs, adding a UNIQUE constraint on what currently is considered to be unique, and referring child tables to ParentID. This way we have to do less work to adjust when the situation changes. 

If you switch to snapshot isolation, this effect should be gone. The following repro script shows how COUNT(*) running under REPEATABLE READ returns wrong results with high concurrency. Prerequisites We need a table with data and a function that provides random integers: 

Here is your description of the problem: "The CTE portion of the query returns almost instantly, only when we start joining to the clients does the performance issue appear". One possible, and likely, explanation is as follows: the optimizer fails to estimate the cardinality of your subtree, and chooses an inefficient plan. With your way of storing hierarchies, this is no surprise. How would you yourself estimate the size of a subtree without actually retrieving it? Can you use materialized path? Getting a subtree using materialized path is essentially one range scan, fast and simple, and the optimizer can have a good cardinality estimate off the statistics on one index. In my experience, your way of storing/reading hierarchies does not scale up. I have never been able to make it work fast and use resources efficiently. 

The following SQL Server solution uses only constraints. I am using similar approaches in multiple places in my system. 

Be careful: when we turn on READ_COMMITTED_SNAPSHOT, we can break existing code. I wrote a few examples here: When Snapshot Isolation Helps and When It Hurts 

To reuse code and reduce maintenance costs, I would have both selects utilize an inline UDF to keep common functionality in one place. Besides, as demonstrated in Martin Smith's excellent answer, UNION leads to a possibly unnecessary sort, which may be very expensive. In general, it pays to write shorter, more specific queries - you have a better chance to get a good execution plan. 

THat done, you can use your second subquery to group data points into interval, or just run it on a client, where is can be solved as one trivial loop. Second approach: instead of intervals I would store sequence of events. there woudl be two kinds of events: interval start and interval end. With every I would store a running total, the number of open events after this event has happened. Essentially this running total is the number of prescriptions active after the event has happened. As in the previous approach, much of the data you calculate on the fly every time you run your query is pre-calculated in this table. We can use trusted constraints to ensure the integrity of precalculated data. I can describe it in more detail later if you are interested. 

Everything necessary to complete a project must be in version control, in a branch. To take over somebody else's unfinished task, all we need to do is pull a branch from git. This applies to test data etc. as well. This will allow the continuous integration server to build/populate a new test DB from scripts downloaded from version control, and run the test suite I am not sure why was this question migrated from stackoverflow - this is IMO strictly programming. 

If you denormalize like this, you do not need to traverse up your tree at all. Note that I added two constraints, a foreign key and a check, to ensure that BillToCustomerID is always correct Another way is to use materialized path. I can post it tomorrow if you are interested. 

The following is just a few examples regarding "actual evidence for or against reliability in the first version of any new release", as requested. This is not meant to be a complete analysis, but rather a suggestion on what you might want to research. You can google up "List of issues that are fixed by SQL Server 2008 Service Pack 1" and "List of issues that are fixed by SQL Server 2008 Service Pack 3" on MSDN website. Compare the number and severity of issues in both lists. IMO the first list is longer, and it has more items that could ruin my day, such as: 

If your priority is speed of selects, the following approach allows for extremely fast selects. Instead of storing a period, you want to store two events (period start and period end). Change column is 1 when the period begins, and is -1 when the period ends. If more than one event occurs on the same day, they must have different EventNumberPerDay. RunningTotal is the number of open peroids after the event has happened: 

Note that your design does not prevent cycles. We can easily enforce that via constraints as well. I can elaborate if you are interested. Regarding the efficiency of finding qualifying descendants, we can add some redundant data and get much better speed. For example, E is a descendant of A, but not a direct one - B is between them. We can store the following rows: 

This is a very simple solution - it took me 10 minutes to develop. A recent college graduate can come up with it. On the database side, execution plan is a trivial merge join which uses very little CPU and memory. Edit: to be realistic, I am running client and server on separate boxes. 

The fastest solution is as follows: you create an additional column, IsLastID, and build a filtered index or an indexed view using it. You can use constraints to ensure the integrity of IsLastID, as described here Grant Fritchey wrote up a detailed comparison of various solutions here 

This means that there can be no overlaps. As you have seen, for every time window, there can be at most one preceding it, and at most one following it. The following interval cannot begin before the end of its previous one. Together these two statements mean that there can be no overlaps. **