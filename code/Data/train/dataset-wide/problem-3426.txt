DynamoDB is built for high throughput low latency. So the core feature of this Database is providing answers within single digit milliseconds. As you already figured out you pay per second which means if you don't need nearly the same amount (within the ability of scaling up and down) of reads/writes over a period of multiple hours DynamoDB is expensive as you pay for queries you don't need. 

In EC2 you should take a look at cloudinit ($URL$ and the user-data ($URL$ With this you're able to provide scripts which will run during the instance launch based on data you can send to the AWS API. But besides that: Why are you doing a RAID and copy over data from the boot volume to the RAID on launch? Without knowing your exact use case but that sounds wrong ;-) Maybe you can elaborate more on this so we're able to actually provide a better solution without the need of doing launch scripts and so. 

There is a read preference value "nearest" which actually uses the latency between the MongoS and MongoD to determine which is the best/fastest set member for the query. You can find the documentation about it here: $URL$ The selection is based on the "member selection". For a sharded cluster like yours you can find the way how the MongoS selects the node here: $URL$ The most important thing to notice here is, that "nearest" doesn't care about the type of a node. So even if you do a read query it is possible that the MongoS selects a primary (which sometimes isn't the way you would expect it). To fix this the only way is to use the tags you already mentioned. Hope that helps! 

I see the confusion now, you actually want to know how the backend server can read the x-forwarded-for header from the logs? Try these instructions for Windows IIS XFF or Apache XFF. 

Re-posting from my LVS mailing list response: Usually for MASQ/NAT mode the real server would be in a different subnet with the LVS server set as the default gateway. If you want to do one-arm i.e. same subnet MASQ then the test client needs to be in a separate subnet OR you need to have special routing rules on the real (backend) server. $URL$ 

DDOS is never an easy problem to solve but it sounds like the type of low level attack you are experiencing would definitely be helped by the queuing facility in HAProxy i.e. don't flood the Apache server with too many requests (maxconns). You could also try and prioritise your existing customers using a cookie facility, like some busy sites do black Friday protection. 

Alexis, This sounds exactly like what agent-check is meant to be used for. Willy kindly added the patch we use for the Loadbalancer.org appliance and we have open sourced our Windows based server agent. The instructions including a basic Linux server example are here: $URL$ The Windows agent can monitor a combination of CPU, RAM & TCP connection counts (i.e. for RDP sessions). Feedback is always welcome. This reminds me - it would be nice if someone implemented some kind of server response time algorithm, then you wouldn't necessarily need a server based agent. 

Andy, The trick is to add another backend that you only use for the extra stick table. You can only have one stick table per backend - BUT you can use them in ANY front/back end... So I just add one called Abuse that you can then use as a global 60 minute ban for any backend... You will need to change my example but try something like this: 

Security groups in AWS are way more comfortable AND even better from the security point of view (as the traffic you want to block doesn't reach your instance at all). So turn off the iptables and configure the security groups as limited as you can. 

Is there a reason why you don't use EBS snapshots? You can use those to save the whole EBS device (incremental) with a simple API call and the snapshots are saved within S3. If you need an old version back just create a volume from this snapshot and connect it to your instance instead of the broken EBS. 

It is not possible (even for the AWS support afaik) to use Elastic IPs which are not for VPCs with VPC instances. So you're stuck here - the only possible way to do things like this is not to rely on a fixed IP address (you will get the same problem if you try to use ELBs or more than one instance). Your customers should NOT point to an IP address but they should use CNAME records with the given subdomain you provide for them. With that architecture you're able to migrate the whole domain with all subdomains to a new IP address if you need to and with the CNAME records nothing changes on the customer side (as the subdomain they're pointing to has the new IP address). The only solution for you now would be to send out an email to all customers which use the IP in their DNS records to change it to a CNAME and after migrating all customers to CNAME you can switch to the new Elastic IP and change your own DNS records. UPDATE: As pointed out below it is now possible to move an Elastic IP from "classic" to "VPC" - you will find the details here: $URL$ 

My first question would be why? If mod_security is on the actual server it will be transparent. If mod security is on a gateway then the client will only ever see the gateway server address. Just use x-forwarded-for to see the client IP in the server logs. It may be possible to use TPROXY in the linux kernel in a two subnet configuration where the servers default gateway is translated through the mod sec box (servers would NOT be able to have a public IP address). But I'm not sure if apache/mod_sec even supports TPROXY it needs code specifically to support it (like in HAProxy). 

DSR is implemented in the Linux Kernel (IPVS) which only works with LVS (Linux Virtual Server). You will also need a health checking daemon like keepalived or ldirectord. BTW DSR does not work in Amazon AWS or in Azure due the to the network virtualisation security that they use. It won't work in things like Docker either. $URL$ 

I'm not sure that I would call DRBD easy, awesome maybe but not easy. As suggested by Jeroen: Round robin DNS or HAProxy can be used to route the traffic to both nodes, but yes your actual problem is that your database is only in one place. So hitting both servers may not be what you want. DRBD would ensure storage is replicated between the servers (solving the problem). Or the standard architecture is 2*HAProxy in front of N+1 Web Servers with the database on a separate and highly available platform (maybe master/slave maybe shared storage maybe DRBD or maybe memched). 

Yes, Traffic is configured to hit a Floating Virtual IP, when the master load balancer fails the slave detects this, brings up the Floating IP and sends a load of gratuitous ARPs to the network. You can achieve this with VRRP, Keepalived, CARP etc. The Loadbalancer.org appliances use HA-Linux & HAProxy. 

Debian Lenny is very old and I assume that's the reason why Oregon doesn't provide the kernels you need to run your image. The only thing I can think of is to update your PV-Grub and create new AMIs. Maybe this helps: $URL$ If you don't find matching AKI in the other region there is no way afaik. 

There is no "Attachment History" - so just start a new instance (default AMI with the Distribution of your choice) and attach all volumes to different /dev/* then mount them readonly on the machine and check for yourself what is on the volumes and if you still need the data. So you can make sure you won't change anything on these devices/volumes (even booting from these volumes will change logs etc.) 

From the connection point of view "something" needs to answer your requests (GET, POST, PUT, everything). First of all you have a TCP connection and "something" needs to make sure it is understanding layer 7 and making sense out of the bytes the client is sending. Only at this point it is possible to handle GET requests differently than POST requests or one URL than another URL. So in the end you need a service which is capable of understanding and routing HTTP. The following services are capable of doing this: CloudFront ELB/ALB API Gateway (limitation comes later) API Gateway uses CloudFront internally (without giving you the chance to actually configure anything on the CloudFront level) - that means there is no way to run CloudFront and API Gateway side-by-side as in the end this would mean you run CloudFront with CloudFront side-by-side. CloudFront gives you the chance to select different origins based on patterns - but you can only select S3 or ELB/ALBs as an origin - not Lambda functions (besides the Lambda@Edge functionality). ALB/ELB can only use EC2 instances as a backend - no Lambda or S3 here. The only ways I can think of which might do what you want to do are these: 

I'm glad that you seem to have solved the immediate problem with fail2ban, and it does make sense to block at the iptables level, but you can do the exact same thing in your HAProxy config: You can use acl's with src_http_req_rate() or even src_http_err_rate(Abuse): I've used them in some examples of haproxy configs for DDOS mitigation here. Longer term: It sounds like you might want to implement a double login for that page, if the users will accept it then you could just put an extra htaccess password in from of the login page, so that they need to separate logins (pretty hard to crack with a script). Or you could use mod_security to do the same thing or some kind of honey trap. 

It is simple enough to use HAProxy to load balance any TCP connection including telnet (most protocols are very similar to telnet anyway). But you should enable persistence by source IP and for long lived connections you should make sure you set long timeouts and keepalive i.e. 

If you have MySQL listening to BOTH ports then simply leave the backend server port out of the HAProxy config i.e. let the client decide the end port to connect to. OR if your MySQL is only listening on 3306 then force all the connections to go to that port by specifying it on the backend config. If still stuck try posting your config. 

That is really an application design problem. You could split the download requests out to a different IP i.e. downloads.myapp.com and set maxconn=1 i.e. 1 request at a time? Or you could balance on URL so the same download request goes to the same application server: But it sounds like you just need in memory caching of content or more servers or faster I/O.