You may want to use SPF records for the domain and MX. TXT records were originally used and some tools only look for them. Creating both SPF and TXT record should minimize lookups required. 

Not all applications are updated to TLS 1.2. My SMTP logs indicate about a 50/50 split between TLS 1.0 and TLS 1.2. That likely overstates the adoption of TLS 1.2 as my internal serves use TLS 1.2. Try running your tests with , , and as well as . For now only should fail. That should enable most modern clients connect. I disabled SSLV3 on my servers last week-end. I run Dovecot, Apache, and Exim, so I don't have instructions for Postfix. For Dovecot I used: 

Routers typically don't run servers. OpenWRT and DD-Wrt based routers can provide this functionality if they have sufficient memory. This is because they are running a Linux kernel. Web server capability is done by running a web server or proxy such as Apache or Squid. If you have several small sites to server, you may be able to use a Rasberry Pi or similar device to run your web sites. If you require more capacity, a small server running Linux can provide both firewall and Web Server capability. I started with a recycled desktop system with a second network card running Linux as both a Web Server and firewall. I do this separation at the web server. The major web servers handle delivering content based on the Host header. The Apache web server calls this functionality VirtualHosts. It is possible to proxy some or all of a Host's traffic to a second server. Multi-domain HTTPS is more difficult as certificates are associate with a hostname. If the domains are related, a multi-domain or wildcard certificate will work. Some servers can handle negotiating the Hostname before providing the certificate, although I have never successfully implemented this. You will want to have your router forward HTTP requests to your web server. Most routers provide this capability by port and/or as a DMZ address. 

If your email server is set up correctly, there should be little if any penalty. I have only had to manualy whitelist incorrectly configured severs. 

Using an image server allows you to serve the images from shorter requests. All the domain specific cookies get dropped. It also makes it easier to set caching policies. If the static content is on a separate server, you can set a caching policy for it without caching your dynamic content. Securing the static content is simpler as well. The server can be restricted to answering only GET requests. Referrer restrictions can be added as well, although they are easy to bypass. Log analysis is also easier as you don't need to filter or process log entries for images and other non page content. 

It looks like you have some access to the server. There are various approaches you can try. You should have a working ssh client, try using ssh to either fix the permissions, or login with a different shell. 

There are various options included in the apache2 mass virtual host documentation. In your case a solution may work for you. 

I've run into this issue with different applications on different platforms. It is a common coding problem where the code that uses files does not close the file when it is done with it. The code should always close the file when it has finished working with it. One common method of securing temporary files is to open the file, delete it and then work with it. This may be what is being done here. However, the file remains in existence until it is closed. A work around would be to limit the number of requests an Apache child will handle before shutting down. The files will be closed when the child exits. Setting MaxRequestsPerChild to somewhere in the range 100 to 1000 may mitigate the issue. The default value is 10000. 

The timetable I use for scaling down TTL is to reduce it by half whenever the time until the change is twice the current TTL. Keep the TTL at least five or ten minutes. Add the new addresses to the active nameservers 10 to 20 minutes in advance of the change. Remove the old entries after the change has been accomplished. 

What you should put after the slash depends on the subnet that contains the mail servers. This will depend on the subnet that was allocated for the mail server. If you are using a responsible provider, they will have an SPF record you can include before your policy, which normally should be . Policies specified in includes are ignored so don't worry if they use a or other permissive policy. You may want to look at the SPF records for Gmail which use multiple include records. There is a limit to how many DNS lookups that will be done before your SPF record is considered invalid. Consider implementing with a reporting address if you want to track issues with and/or . Several large providers provide reports to domains with configured. There are multiple services that can provide reports on how well you have configured your email domain. You may want to consider using a separate domain for bulk emails. This should reduce the damage to your mail domains reputation, if there are problems with a bulk mailing. I've blogged about Securing your Email Reputation with SPF among other topics. You may also find the resources in my posting Detecting Email Server Forgery useful. 

I don't think you will likely have resource issues related to CPU scheduling or memory mapping. These won't happen unless needed. When they do happen, they are relatively low overhead compared to the process that is scheduled. What I would look at is: 

Don't break the key. Just add it as a single string. It will wordwrap in most editors. My key looks like the following. 

If you disabled SSL_3 by removing ciphers, you have likely also disabled TLS1 and TSL1_1. This leaves TLS1_2 which is not yet supported by all applications. As I documented in my post on Disabling SSLv3 to block Poodle, you can block SSL3 in Apache with the configuration line. 

I have never seen any network structures for holding time a connection was established. The information can be logged by stateful firewalls. However, they only track when the last activity occurred. In some cases, it is roughly derivable from when the process to servicing the connection was created. 

You should have a routing to the end point over the interface device. This will take preference to a wider routing to the servers behind the tunnel. Lower metrics are higher priority. Metric 2 won't be used if metric 1 or 0 is available. Failover routing needs to be configure with some sort of monitoring software which will change routing. Mixing routings to the same IP address is difficult to get right. 

If you implement BATV (Bounce Address Tag Verification), you can block them as they come in. Most of these messages are likely to userids which don't exist on your system, and should be refused on that basis. If you set and publish strong policies (SPF, DKIM, and DMARC), many servers will refuse the spam and you should see less of it in a few days. (Some of the servers sending this "backscatter" spam, may stop sending based on these policies.) 

Only and will resolve. will not resolve as it isn't defined. Unless designates other name servers for , its name servers will be used to resolve . Unless the domain is misconfigured there will be at least two name servers such as and . However, the name server could just as well belong to another domain such as and . (Note: rather than .) The NS record you have is redundant and likely incorrect. Just configuring or any other name server will not work. In this case would need records for pointing to the name servers. 

This will permanently redirect all requests not matching the canonical name to the the canonical name. Include it in the virtualhost definition for your SSL (HTTPS site). The following rule set should force any non-HTTPS traffic to HTTPS. 

Having reviewed your assumptions do both. Use a CD/DVD based media to dump the current state. Primarily you will want to be able detemine how you were compromised. You may also want to attempt to recover user data not on your backup images. Yo Then rebuild the system from the latest backup media that is not contaminated. Verify this with checksums if possible. Alternatively, reinstall the software from installation media, and reconfigure. Reconfiguration should be automatic if you have been using Puppet or cfEngine to configure your server. Reload user data and scan for root kit components. Verify you have no setuid or setgid programs in the data directories. Determine and close off the method of access used to infect the system. Stage reactivation of the server allowing time to verify applications are running as expected. Carefully monitor for new infection attempts. This all assumes the infection is at root level. Web infections can be done by altering the code run by the web server. Allowing the web server write access to its code may make this more easily done. You may still want to treat this case as if the root account has been compromised. If root on one system has been compromised on one system, you may have more systems compromised. Carefully consider which systems can be accessed without a password from the infected system. 

Run to check for memory, swapspace, and process counts. Do you have any swap. If not, add at least file based swap. If your apache process counts continue to grow, then you may want to cut the process counts temporarily. Could be a denial of service attack on Apache. You can use netstat to watch connections inbound and outbound. should have a relatively stable count. Try searching for denial of service, and memory leaks. 

Outgoing email goes from the user's UA to , its MSA. The MSA sends the email to an MX (possibly or ), or directly to an MDA like . The MX may tranfer the email through additional MXs until it reaches and MDA which accepts the email for delivery. If at any step along the way, the email is determined to be undeliverable, it will be returned to sender if possible. At some point a UA may be used to read the email. 

It appears you are inside network which limits access to ntp servers. Of the six time servers you have specified you are only getting time from one. However, all is not lost. Try using local time sources. I find reliable NTP sources are common. Often your router and DNS servers will provide a reliable time source. If an dedicated ntp server is available, it is often called or . If the local domain is then you would find it as . For your master server consider using its clock as a time source. Setting is stratum in the range 8 to 12 makes it a useful backup but won't override real sources. This may not be an accurate time source, but can be used as common fallback time source for all your systems. Even if you can't get a good time source, you can at least sync all your systems to the same time source. 

Setting owner to make the directories and files writable by your web server. Set this ownership only for directories you want the web server to write to. Configure the server not to run active code from these directories. The injection likely comes from the active code you are running on your system. You may want to limit access to javascript files only to directories that can NOT be written to by the web server. Schedule regular malware scans for the directories that can be written to by the web server. This should only be the cache and upload directories. 

These are a couple of other options to use the command to run the file. Java uses the CLASSPATH to find classes and other resources. The classpath can contain jars and directories. Java will search the classpath for resources. Define an environment variable containing the path(s) you want searched. The path seperator is ; on Windows and : on most other systems. This will work for classes on the command line, but not jars. If you read the of you can write a short class which invokes the main class. Alternatively, just invoke the main class directly as specifed in the manifest instead of (as specified below). You may already have a classpath defined. Try to see if it is. Try 

Once you have a basic install up an running you can do everything over the network cable. The basic install could be an installation on a USB key or PXE as described above. I typically manage most of my ubuntu installations over ssh. Doing verision upgrade over ssh is risky, but possible. If you end up with sshd not running and loose all your ssh connections you will likley need a keyboard and monitor. Otherwise, the ethernet cable is fine. I don't think the default USB key installation includes running sshd, but the new releases allow persistent changes. You could install sshd on the key and then connect it to a system without a keyboard or display. 

If the mail isn't getting delivered to the clients you should be getting bounce messages. Your email server log should give you some hints as too what is happening. In most cases, it should show the message being delivered to their mail server. If their server is receiving the email, then the problem would be on their end. Try to contact their administrator. Some servers don't reliably send bounce messages. Unfortunately, many servers accept messages and then determine if they will deliver the message. As a result it is highly probable that a bounce message will be back-scatter spam rather than an appropriate notification. You may want to check if your server is listed on any blacklists. 

You should disable recursive queries on any Internet facing DNS servers. It is likely not a good idea to allow incoming Internet access to your domain-controller. Blocking recursion will break DNS for any host using your domain-controller as their DNS servers. Configuring firewall rules allowing only outgoing access (and replies) on port 53 would be appropriate in this case. If you are publishing addresses on your domain to the Internet, it should be done by a separate DNS server. It is common to use your domain registrar for such purposes. Some DNS software is capable of running in split brain mode where they provide a limited set of services to Internet, and full recursive access to the local network. I don't know if this is possible in your case. 

If should tell you if if loaded any modules in which case you will need to restart apache. Copy the error/include sub-directory for each of the sites. Edit the files in these files to modify the error pages. 

Server specifications are likely to be less of an issue than the file system you are using. Different file systems have different approaches to storing directory data. This will impact the scanning speed at various sizes. Another important consideration is the lifecycle of the files. If you have frequent addition and deletion of files you may want the leaf directories to be smaller than they might otherwise might be. You may want to look at the cache directory structures used by the Apache web server and Squid proxy. These are are well tested caches which handle relatively high rates of change, and scale well. EDIT: The answer to your question depends significantly on the life-cycle and access patterns of the files. These factors will significantly influence the disk I/O and buffer memory requirements. Number of files is likely to be a less significant factor. Besides file system chosen, memory, disk interfaces, number of disks, and raid setup will all impact disk access performance. Performance needs to be sufficient to requirements with some leeway. Disk setup tends to be more important as writes and deletes increase. It can also be more important as access to files becomes more random. These factors tend to increase the requirement for disk throughput. Increasing memory generally makes it more likely that files are accessed from disk buffers than disk. This will increase file access performance for most systems. Access to many large files may result in poorer disk caching. For most systems I have worked with, the likelihood a file will be accessed is related to when it was last accessed. The more recently a file was accessed the more likely it will be accessed again. Hashing algorithms tend to be important in optimizing retrieval in these cases. If file access is truly random, this is less significant. The disk I/O required to delete a file may be significantly higher than adding a file. Many systems have significant problems deleting large numbers of files from large directories. The higher rate of file additions and deletions, the more significant this becomes. File lifecycle is an important factor when considering these factors. Backups are another issue and may need to be scheduled so they don't cause disk buffering issues. Newer systems allow IO to be niced so backups and other maintenance programs have less impact on the application.