You can prove that the two theories are, in fact, equivalent. By induction (NB: 'meta-induction') on the number of parameters, we can reduce the claim to the case where you have a theory $T$ with the usual induction schema and a theory $T'$ that is an extension of $T$ by a one-parameter induction schema. So assume $T$ and $T'$ are not equivalent. This implies that there is a model $\mathcal{M} \vDash T'$ and a two-place open sentence $\phi(x,y)$ such that $$\mathcal{M} \vDash \phi(0,\beta) \wedge (\forall x (\phi(x,\beta) \rightarrow \phi(x+1,\beta))) $$ but also, $$\mathcal{M} \nvDash \forall x \phi(x,\beta)$$ for some $\beta \in \mathcal{M}$. But note now that the above is equivalent to $$ \mathcal{M} \vDash \exists y (\phi(0,y) \wedge (\forall z (\phi(z,y) \rightarrow \phi(z+1,y))) \wedge \neg \forall z \phi(z,y) $$ and if you call this last sentence $S$ then you get that $S \wedge \phi ' (x)$ violates the usual induction-schema, where $\phi ' (x)$ is the one-place open sentence we get by by closing $y$ under the existential quantifier in $S$. That is to say we have $S \wedge \phi ' (0)$ and $ \forall x (S \wedge \phi ' (x) \rightarrow S \wedge \phi ' (x+1))$ but not $\forall x S \wedge \phi ' (x)$. But that is a contradiction since $\mathcal{M}$ is a model of $T$. Hence the two theories are equivalent. 

I think some of the confusion in this question comes from the use of the phrase "joint distribution"; the slogan is that freeness of two variables allows to determine the joint distribution of the two variables, given the distribution of each of the variables. This is correct, but "joint distribution" has here to be understood as the non-commutative joint distribution, which is, by definition, the collection of all moments of the two variables. If the variables commute this can be identified with the classical meaning of joint distribution, i.e., a probability measure on $\mathbb{R}^2$; if the variables do not commute then this identification does not work any more. If variables are free, then (apart from trivial cases, where one of them is a constant) they do not commute, hence there is no joint distribution in the classical sense for two free variables. The search for a good analytic meaning of the joint distribution of non-commuting variables is at the moment actually a quite active direction in free probability. See, for example, my survey article Free Probability theory. The physical significance of freeness usually comes via its link with random matrices; then large $N$-limits of physical theories might have a formulation in terms of free probability theory. Some literature with a more physics flavor are for example: 

On leafing through some papers of John Nash (available online on his webpage) I found this intriguing little observation: 

I suspect his terminology might be idiosyncratic, so I'll point out that by an $\omega$-model he means "a model of set theory in which the natural numbers are ordered as they are 'supposed to be'; that is, the sequence of 'natural numbers' of the model is an $\omega$-sequence." My first (not-so-interesting) question is this: 

Please don't get put off by the length, all the questions are quite simple, but given the quasi-mathematical context I tried to be precise with the formulation. The more mathematically interesting title question (and the one that's most important for my purposes) is the last one, so if anything please take a look at the end. Here goes. In his paper Models and Reality (1980) pg. 468 ($URL$ Hilary Putnam states and proves the following theorem: 

In general it seems to me obviously not - any version of the strong LST uses some notion of cardinality which surely cannot apply to $L$. What am I missing here? Putnam does add that strictly speaking the Skolem hull construction is also needed, but I don't see how that would help. Can it? I will tag this as a reference request too, in case someone knows whether this theorem has been published elsewhere - by Putnam or otherwise. (His footnote says that he proved it in 1963 but provides no more information.) 

I think it is a bit misleading to contrast cumulants as combinatorial quantities with the characteristic function as analytic object. The characteristic function is an analytical device containing information about the moments. In the same way there is an analytic object (namely the logarithm of the characteristic function) which contains information about the cumulants. So I would say that there are moments and there are cumulants, dealing with them has often a combinatorial flavor, and there are analytic reformulations of moments and of cumulants which allow the use of more analytic tools (and, in particular, allow to deal with situations where no moments/cumulants exist). In some cases moments are better suited for the problem at hand, in some cases cumulants are. In the classical case, the closeness between the analytic avatars of moments and of cumulants (the first is the characteristic function, the second is the logarithm of the characteristic function) might be a reason that one usually does not talk so much about the analytic version of cumulants. In free probability theory the difference between the Cauchy transform (the analytic function for moments) and the $R$-transform (the analytic function for free cumulants) is much bigger and the parallelism between the combinatorial and the analytical side of moments/cumulants is more visible. 

Has anyone ever tackled this conjecture? Would it be an 'easy' conjecture? Let me make the conjecture precise, for the sake of clarity. Let $N$ be an even number and let $s(N)$ stand for the smallest prime such that $N-s(N)$ is prime (if such a prime exists, i.e. if $N$ is expressible as the sum of two primes.) Define the following set: $$S_m = \lbrace N \vert N \text{ is even and } s(N)^m > N-s(n) \rbrace $$ With this notation, Nash's conjecture asks: Is $S_3$ finite or infinite? Nash calculated the first member of $S_3$, which is 63274 = 293 + 62681. What about other values of $m$? If indeed $S_3$ is infinite, is there an $m$ such that $S_m$ is finite? (I'm tagging this as a reference request too since I know very little about the Goldbach literature and would be intrigued to read any related papers.) 

The definitions are indeed equivalent. The idea of 'local smallness' is to get for any $X$,$Y$ in $\mathcal{C}^I$ an object of your indexing category to represent, as it were, all (vertical) morphisms between $X$ and $Y$. Both definitions describe this fact, although Johnstone's is, I guess, slightly more 'general' than it needs to be in that it applies the above property to any $X$ and $Y$ in $\mathcal{C}$ (and not to $X$, $Y$ in the same fibre), but that's OK by Theorem 10.1 in Streicher since $\mathcal{S}$ has finite limits. The equivalence of the definitions can also be proved as exercises 8.8.9 and 8.8.10 in Volume 2 of Borceaux. To see that they are equivalent let for simplicity $X$ and $Y$ lie on the same fibre $I$ (WLOG bearing in mind what I said above.) Then Johnstone's definition says that there exists an arrow $\alpha \colon J \rightarrow I$ and a morphism $f \colon \alpha^*X \rightarrow \alpha^*Y$ such that for any $\beta \colon K \rightarrow I$ and any morphism $g \colon \beta^*X \rightarrow \beta^*Y$ there exists a unique $u: K \rightarrow J$ such that $$u^{*}(f) = g$$ and $\alpha \circ u = \beta$. But now if you write $J$ as $H_{X,Y}$ and $\alpha$ as $h_{X,Y}$ you'll see that the last sentence says exactly that there is a bijection between morphisms $f \colon \beta^*X \rightarrow \beta^*Y$ and morphisms $u$ such that $h_{X,Y} \circ u = \beta$, i.e. between morphisms from $\beta$ to $h_{X,Y}$ in $\mathcal{S}/I$. And this is exactly the second definition.