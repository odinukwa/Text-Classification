Notes on performance With small tables (less than 1000000 rows), any solution will work. is slightly faster: 

in pg_hba.conf, in the lines with localhost IP, replace "ident" by "md5" and restart - then you will be able to use password logins if (1) is not acceptable, sudo to "postgres" user and create another superuser login - then use this login to connect from PgAdmin. 

notification ID will be a monotonic bigint never going down and store only one the last ID read (which means all previous notifications for given user are also read) - this will be just one field in table. 

Only thing I do not understand in your question is "database agnostic" - what does it mean, precisely? 

RULEs or triggers are a performance overhead, and can be avoided. Consider something along these lines: 

You do not need any triggers / rules to maintain it. There are open ends here - that's just a draft... Some issues: 

Decide your vertexes (nodes, objects) and edges (relationships). Convert relational data to cypher, declaring all items and all relationships explicit. 

If you want to see also query duration, you can set instead of . This is very useful for query tuning. Then reload config (restart or HUP) and collect enough log to estimate traffic. Note: neither method will include queries embedded in user-defined functions. 

Why async? I would avoid doing this in trigger due to locking issues under high load. Also, easy to DDOS so permissions should be separate for form insert and form create. 

You can always calculate higher-level information from lower-lever aggregate. For example if you had an aggregate on (app_id, day, collection_id) you could use it instead of (app_id, day). You can materialize your aggregates with feature. But this is not an only way. If old data is static, it could be enough to insert new rows daily, with something similar to 

Use double backslashes [] in the folder path to separate folders rather than just one to escape the first backslash since by default a single backslash is used as a special escape character and ignored when used alone 

I also wanted to share a db_DDLAdmin_Restriction role you may want to consider to consider creating otherwise with explicit to restrict what give access to so you could at least create this on the DBs where you grant them this role and set the explicit for the actual object types, etc. you don't want them to have access to. For example, if you know they will definitely be creating stored procedures and functions, you can exclude , , . 

A Limit to the number of MySQL Databases a User Account can Create At the MySQL level as per the Limits on Number of Databases and Tables there is no limit on the number of databases MySQL can contain at this level. If you give a user account the global CREATE permission to create new databases, then you give them just that and you cannot restrict the number of databases it can create at this level. Limiting the number of Database a MySQL User Account can Create To control the number of database you allow a user account to create you could just create the databases per an "approved" request and not grant them permission to CREATE databases themselves. You'd grant the user account Database Privileges at this level once created. Additionally, as some third party hosting services utilize, if you only allow access to manage MySQL instances and databases via an application, it is possible to have rules at this level keeping track of the number of databases an account creates, and enforcing rules to set such restrictions. 

Yes you can hide those messages in the log. In the calling session, before running the statement, issue this statement: 

Interesting question but also very open one. I'm putting a list of recommendations here - hope it helps. 

No it's not. Unless your field set is very dynamic (no single authority, people can invent fields on the fly). 

Exception blocks are meant for trapping errors, not checking conditions. In other words, if some condition can be handled at compile time, it should not be trapped as error but resolved by ordinary program logic. In Trapping Errors section of PL/PgSQL documentation you can find such tip: 

I understand that you want to go with single database (as it is good from management & maintenance point of view), but maybe it's too much integration. I am assuming that: 

Solution 2 Using more ANSI-compatible SQL, like UNION and LIMIT. It will work on MySQL, DB2 and some others. Similar solution can be done on Oracle, just replace LIMIT with ROWNUM. 

I'm not yet sure if this is doable in pure SQL (probably - yes), but here is the brute force solution using cursors. This is just a (working) draft, with some twiddling and dynamic SQL you could add more parameterers, like function name. 

Yes. If you cannot get rid of MandatoryData object, you should have one such object per book. Otherwise I would simplify the model and move any mandatory attributes to the Book object, which will reduce ne04j database size. Modeling relationships which are always 1:1 is a bit silly. 

Make sure you have a full or log backup on your primary DB first, and then do the shrink operation from the primary DB. It'll probably grow again due huge transaction so you may want to see if throwing in a few log backups may help reduce the growth before your full backups occur or else determine what's causing the logs to grow so much bigger than the data file -- typically it's due to poorly written queries or HUGE transactions or maybe not doing enough backups to free up space since your in FULL recovery mode. 

So do a FULL backup on your primary DB Run the SHRINKFILE on the primary DB of the mirror without ALTERing to SIMPLE RECOVERY mode 

I have to agree with Max's comments about running the stuff that would normally run when the issue occurs as a simple step to confirm whether or not those items/processes are causing the issue -- process of elimination should be simple enough. Since you pretty much have the day and timing down to a science when the issue occurs, you could schedule a SQL profiler trace to run via a SQL agent job and give it a stop time (@stoptime) to stop the trace to see what details the trace will provide. Since you asked about where else you could start to troubleshoot, I think a SQL profiler trace would give you a lot of detail to go by actually. I'll paste what I have on this when I did it on SQL Server 2008 R2 below. SETUP DETAILS Follow the below instructions for scheduling a trace with a specific start time and a specific end time. You'll go to 'SQL Server Profiler' and build your trace criteria as usual but be sure to save to a file somewhere valid on the server itself, check the 'enable file rollover' option, and specify a trace stop time. Once all the criteria is selected and filtered for what you need to capture, run it, stop it, and then navigate. . . FILE | EXPORT | SCRIPT TRACE DEFINITION | 'select' the SQL Server 2005 - 2008 R2 | and then save the file somewhere you can open later. Go to the file, open it and you should get something similar to the below. KEY POINTS 

To calculate TPS (transactions per second), run the query several times and calculate difference over time interval. There are ready made tools for that, one of them is $URL$ More info: $URL$ 

This answer assumes that you want to connect via TCP to localhost. (for local socket connection see Erwin's answer) Two options to ease your pain: 

All operating systems and all applications use a concept called "caching". It means - when the data is first read from a slow memory device (like, a hard disk), it is saved in a fast memory device (like, RAM) for some time, to facilitate faster lookups. The same applies to RDBMS. First time the data blocks that build up your query results are read from disk, second time they are read from memory. Details can be explored using OS and database tools. If you specify what RDBMS and what OS you are on, we can help you get the details. For PostgreSQL it's about EXPLAIN command. 

Update: Konrad corrected my misunderstanding of his question. The goal was to count queries, not transactions. How to count queries? Method 1 Use pg_stat_statements contrib. Method 2 Enable full logging of queries for a representative period of time. To enable full logging, for PostgreSQL 9.0 - 9.3, change following settings in 

I try to saturate the server using pgbouncer. I was running a Select-only test, with 1000 clients for 5 minutes. (). PgBouncer was initializet with scale=100 (but a SELECT-only test should not suffer on it). During the test: 

For better effect please post results of and . Also let us know the dataset size and resource configuration parameters (, ) as well as database host parameters (OS, memory, disks). 

Whenever user requests data, Solr is queried and resultant XML is returned (Image is only fetched from Mongo when explicitly demanded otherwise not). Now data has grown too much and search query is taking considerable amount of time to fetch the results. A proposed solution for this is to store all data in mongo DB as - 

We are experiencing huge logs creation in our MongoDB shard setup. Log settings used are default values - 

User may request above data using keyword(s) that exist in any of the field above. For example - search 'technology' in title, description and meta-data field. The current solution used for above scenario is to store images in mongo DB and store the mongo-ID (_id field value) along with other fields (title, meta-data, decription etc) in Solr. Solr has index on fields - title, meta-data, description, category and tags with index properties as 

We have a mongoDB cluster with 2 shards, 3 config servers and 5 mongos instances. We create new database for each day and drop database(s) that are older than 10 days. On any one mongos, when I list all databases, there is one database shown that is occupying 40 GB space. This database should have been removed by timely scripts, but that did not happen. When I try to drop this database, error Occurs as - "database does not exist". Find below console output of step carried out - 

Issue is that Strict mode query is not providing results but query without strict mode provides the data. I am using mongo version - 3.2.7 

Condsiderations for security options in SQL Server (Two things to mention for typical simple configurations) 

You can also run this adhoc this way without SQL Agent scheduling specifying the correct DateTime variable when you want it to stop. Once the process has run, you just go to the file and open it as usual. See script logic at bottom with comments on how to confirm this isn't running any longer, etc. -- any traces for that matter and how to stop them from running if they are. NOTE: In my case we scheduled this job start with SQL Agent job at 7 AM and then put the DateTime variable value for the @stoptime argument passed to the sp_trace_create object so it quit running at 8 AM on this day. Once in the office and reviewing this, we were able to sift through and determine the issue and correct. We filtered our trace criteria down as much as we could though beforehand when we built the script file that saves to disk to give the TSQL for scheduling. 

The above TSQL will grant explicit VIEW DEFINITION access to the specific DB object and to the specific security principal as specified in the applicable TSQL logic 

Can you point the 'full path' to the UNC path i.e. instead and see if that works? Just like when you map the "X" drive to just use that in the full path of one of your packages and run to see if it'll work. If one of your jobs work like that (assuming most all are setup the same and this way, etc), you can probably script out the SQL Agent jobs through SSMS by pressing F7 (once SQL Agent jobs is highlighted), selecting them all from the right pane window, right click, then create to new query window, then do a mass CTRL+H and do a find and replace to replace with , and then run that. Just be sure the SSIS proxy account or the SQL Server Agent account has appropriate NTFS and SHARE permissions where the SSIS packages reside to read them.