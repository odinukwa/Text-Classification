It really depends how your HR/, Payroll application presenting it to the users. I can imagine that can be done in completely ergonomic way, so basically it would be only integration problem for some kind of external analytics. And it probably may have advantage, past year databases may be put on read only mode, and that current year databases would be smaller than big, aggregate database.. But saying this, I would like to say, I've never seen any good system for such kind of database model. Some random thoughts: 

Storage of the files on Linux filesystem. From my experience cames that even opening and listing directory with 100 000 or more files is a task which Microsoft cannot do in efficient way. Just using more efficient operating system here may give you at least some ergonomic gains. Maybe you should research etl tools like Apatar. It works with many files by GUI, so in some cases you may have a visualization of your process, which probably makes it easier to automate or analyse. It may be surprising in relation to this problem, but on the market there are very efficient data collection tools. Maybe thinking outside the box, and using clustered tools for system log acquisition, like Graylog log server, will be interesting choice. It has pipelining system, which takes data from network port, or through remote agents monitoring particular text files on remote system, and crunch it through various regex collectors, then has ability to divide that streams into output depending on given regex results. Output may be defined by you, and sent to another network device. Even on single machine acquisition of several hundreds of thousands records takes a seconds. It is very, very efficient. I believe at least part of your work may be automated here, for example it may test patterns and produce stream with wrong format entries ( or with good ones). And it gives you some basic statistical informations like dashboard with graphic representation of error types etc. 

Certainly you are person who knows a lot of technology tools etc. But I will just point here, that two things are probably worth of research: 

Sql Server account has to be the same on both machines, it should be Domain account ( not local user). It should have particular rights in domain, at least during installation, and it should be local administrator. SQL server browser service should work. You should not mess with reporting services nor "install all" option, because it may lead to slightly harder configurations scenario. It is better to focus on bare database engine services installation, and after add what you want. 

It is troublesome in every day use, as it is quite common some operations have to change databases records even in the past, at least for short time during years change It is very hard to use such systems in external analytics, based on normal tools: instead writing queries on database with time periods as filters, you have to manage several database connections and aggregate statistical data outside, probably using temporary tables or even additional data warehouse Some reports may be required, for time periods which not agree with years change. For example for accounting purposes. Exporting duch data into other systems, for example when changing HR platform to other products, is nightmare in such scenario. Upgrade. Usually is performed on present database. Will you upgrade historical ones as well? If yes - what I advantage of having it separately? If not - you will end with several incompatible database schemas. Maintenance. It may be a lot of effort put to maintain several databases instead of only one. It is known issue. 

Hm, so the CTXCAT index uses a trigger to know when it needs to update the index for a particular entry. All I need to do is tweak the trigger and recompile it so it does what I want. To get the content of the trigger: 

I'm using Oracle Text (available on 9i) with a CTXCAT index on the compound_name field. CTXCAT is a transactional index, meaning that it updates any time there is a change to whatever it's indexing. This is in contrast to a CONTEXT index, which requires a manual or periodic command issued to it, at which point the index updates. Index creation code: (forgive the capitalization) 

The trigger works properly and populates the alright. But it seems as though, even though CTXCAT is supposed to be transactional, it's not smart enough to detect that I've manipulated the value in my trigger. In the tests I've done so far, only a direct manipulation of that field with an update or insert query will trigger an update to the index. Any solutions to this? I would like to more or less keep the set up I have, with the field being populated by triggers rather than in my DAO layer, but I need to force the CTXCAT index to update itself whenever that field changes, even if the trigger is changing it. Alternative methods of handling this are welcome as well, but I'd prefer a simple tweak or annotation or something to just get this working. Thanks! 

I never make changes directly to the column. Instead, I have a Before Insert Or Update trigger, that composes that value out of and . The trigger looks like this: 

I'm running Oracle DB 9i. I have a table with various constraints to ensure data integrity. In addition to the constraints, I have triggers on and to ensure that necessary data goes into the table, in some cases allowing the application layer to omit the information from their queries, and in some cases forcing that it be present. The actions that I need to do on my are different than my . My question is this: Can I have one common trigger that uses , or should I make separate triggers? For example: 

(Slightly different than what I said in my question, I apologize.) The effect here is that, each time I used one of these statements, either the or would be null. So when we get to this condition: 

You can see that the 12c Oracle Text version's AFTER trigger does actually compare the and values of the indexed column to see if it needs to update--not the case back in 2008. So...if I'm updating the value in my BEFORE trigger, that should be reflected in the AFTER trigger, and the comparison would kick off an update to the index. What gives? Well, here are the two SQL statements I was using: 

TLDR: The CTXCAT index is supposed to be transactional, but Before Insert triggers that modify the indexed column don't seem to induce an update. 

I'm running on Oracle 9i. (Actually, I'm running on 12c, but building for 9i.) I have a table like so: 

Aha! I've found the answer. Talk about an edge case. First, I found this post from 2007, where someone says: 

We are doing an equality comparison against a null, which evaluates to UNKNOWN. Hence, no index update. This is actually an extreme edge case, because you have to be indirectly modifying via a trigger, and either or must be null. I would never have discovered it it I hadn't used those exact SQL statements. So, we have a slight update to the trigger: 

Make sure that SSH is allowing forwarded ports; if you get a message that states the forward was not permitted or was refused, then that is why. To fix this, you'd have to change the server configuration; add this configuration to the SSH server: 

I'm trying to understand how to compare a DATETIME value (ostensibly inserted as GMT) and a TIMESTAMP value (automatically generated at INSERT time). As I understand it, the DATETIME is set to whatever it is set to - and the reporting comes back with the same value, no matter what time zones are in place on the server or client. In the MySQL documentation, it sounds like TIMESTAMP values are reported in the current timezone, but stored in UTC. In this situation then, subtracting the DATETIME value (earlier event) from the TIMESTAMP value should be possible without a lot of timezone conversion - because the DATETIME is GMT and the TIMESTAMP value is UTC. However, doing this accurately - and understanding it - is proving to be a problem. I have situations where I need in minutes and in days. If we assume that column is DATETIME and column is TIMESTAMP... So far, I've tried: 

Our time zone here is US Central Time - currently Central Daylight Time - GMT-6 for CST and GMT-5 for current CDT. It also looks like DATEDIFF converts both values into dates then compares them - instead of comparing time between the values and dividing by the number of 24 hours in each. Thus, I don't think DATEDIFF will work directly for me anyway. The specific questions, then are: 

UPDATE: Tried using the function with the values of and . However, the function with such specifications appears to read the setting of Daylight Saving Time from the current system time and not from the time as read from the database field. This means that when the data should be taken as a date in CST, it is read as CDT instead. I keep thinking it's got to be simple to get the UTC representation of a field - which would simplify things, seems to me. UPDATE 2: Looks to me like I mispoke regarding . I'm using this function and it seems to work: 

The reason that 127.0.0.1 is being contacted is because the tunnel connects a port on your local machine to the remote host. The message seems to suggest that an SSH connection is not being made. Try this from the command line: 

Why are these calculations so different? I am assuming that it is a lack of understanding of TIMEDIFF, but what am I missing? 

All three of those "variables" are FLOAT fields in the (temporary) database. I would expect that and would never differ by more than 1 or 2. However, there are certain values that seem to generate a wild answer - and they tend to be for certain values of . Calculating the value from shows that the calculations are right. Conceptually, day2 should be difference between the dates from the DATETIME values, and day1 should be the number of 24-hour periods in the same time frame. The biggest differences are at the maximum value for timediff: using calculations, the number of 24-hour periods is just over 97. However, using the DATEDIFF function shows a time span of as low as 60 days. This makes no sense. There are also big differences when TIMEDIFF results in 26 24-hour periods: DATEDIFF reports 10 days instead - or when TIMEDIFF results in 30 24-hour periods, and DATEDIFF reports 11 days. There are also differences when TIMEDIFF reports 79 days, but DATEDIFF gives 29 days. Here is example output - from a Ruby script (and in Ruby array format) - with columns d1,d2,TIMEDIFF,24-hr periods,DATEDIFF: