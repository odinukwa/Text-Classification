Short answer: Foucault avoids programmatic statements and rarely passes judgments. It would be safe to say that yes, indeed, Foucault simply offers a descriptive analysis without value judgments and "solutions". Foucault does not believe that the role of the intellectual is to tell others what to do. However, we may also add that by choosing to focus his research on madness, sexuality or biopolitics Foucault does show a certain bias: that these issues need attention, or, as Foucault usually puts it, that there is something "dangerous" about them. Long answer: What is biopower? Biopower is the power over life itself. It will be helpful to contrast biopower with the sovereign power, as Foucault does himself on several occasions. Before the 18th century in the West, or so the argument goes, power operates primarily by "deduction" (HoS1, 136). The sovereign takes stuff away from his subjects: land, produce, and ultimately life. Sovereign power shows itself and asserts itself in these brief moments when a life is to be taken away (usually as a matter of public spectacle). The right of sovereignty is "the right to take life or let live" (SMD, 241 / HoS1, 136). Biopower is the reversal of this relationship. It actively promotes life or lets die. Not that "bare life" has never been an issue before, but it was around the 18th century, argues Foucault, that it became an issue for politics: 

So your professor's answer is actually not precise enough to determine whether he is using the purely rigorous definition. There is a third possible definition, where a polynomial is a function of the form ( x ↦ a + b * x + c * x^2 + ... + d * x^n ), where "+" and "*" can be overloaded and "x^k" is just a short-form for the k-fold product of "x". Such a function will be undefined or crash if given an input x for which any of the multiplications or additions are not defined. In this sense "x" is not really meaningless but is in fact a parameter name. Of course, one can use a different parameter name (as long as it is not used elsewhere) without changing the meaning. The disadvantage of this approach is that it is very difficult to formalize with all the overloading, but it actually corresponds quite closely with how we informally reason. It is actually not too hard to use the formal definition to capture this notion, via the evaluation map on univariate polynomials over R (where R is a ring), that maps each such polynomial p and object x in R to the value ( a + b * x + c * x^2 + ... + d * x^n ). Similarly for multivariate polynomials. The only thing is that we always have to use this evaluation map to 'apply' a polynomial to some object. People naturally create the short-hand "p(x)" for the above value, despite this conflicting with the formal definition! (By the formal definition, p(1) would be the coefficient of the linear term, but by the convenient short-hand p(1) would be the sum of all the coefficients!) 

Deleuze picks up the concept of multiplicity from Riemann and Bergson and develops it in many of his works and in a variety of ways. In general, he rejects the One-Many dialectic and proposes multiplicity instead: 

Substance theory from Aristotle to Spinoza operates freely with the One-Many dyad (e.g. monism reduces the variety of things in the world to the unity of one). Deleuze opposes this and that is why it can be said that his metaphysics replaces substance with multiplicity: 

The closest thing to the "study of naming objects" today, it seems, would be semiotics or semiology. As a sustained field of interest it has begun with Ferdinand de Saussure at the beginning of the 20th century. He convincingly and very influentially argued that language is a system of signs and that it does not pertain to "the world in itself" but is rather arbitrary. Of course, Saussure had many predecessors interested in the very same question, but his posthumously published lectures were the main impetus behind turning this interest into a full blown discipline (which today one will find very far away from the original Saussurean conjectures). In Cratylus, for example, Plato is already pondering whether the names of objects pertain to the "essence" of these objects or if they are simply arbitrary. Plato in this particular case is either on the essentialist side or takes no position. Aristotle, on the other hand, argued for conventionality of language: if a word had some sort of intrinsic connection to what it signifies why, then, even such basic concepts as bread and wine are called differently in different languages? 

Firstly, mathematics is circular even in the notion of natural numbers. Worse still, there is not only no viable alternative but also no apparent physical model of Peano Arithmetic. Moreover, the generalized incompleteness theorems can be proven in weak meta-systems such as ACA, which imply that there is absolutely no way to pin down the natural numbers by any useful formal system whatsoever. Hence even if we assume that PA is correct about natural numbers (whatever they are), it is really a strange kind of 'blind faith' because we cannot even mathematically specify what the natural numbers are, and yet we are making assumptions about them! Also, you may be interested in this brief account of the increasing philosophical assumptions needed to express more or prove more. So, yes, all mathematics is based on faith in the sense that there is no valid justification for PA being completely correct about some structure in the real world, and yet formal systems are based on the essentially equivalent string manipulation properties. But, no, PA was originally designed to capture what we thought was correct about what we conceived of as natural numbers, and seems to work at human scales, so is that really 'blind faith'? If it is, how can you explain why RSA decryption works? Secondly, your question about philosophical theories is quite answered by the above considerations regarding formal systems. Let me put it explicitly. Every formal system needs the basic properties of finite strings to hold, otherwise one cannot even affirm that the very notion of logical deduction is valid. But there is no viable substitute for formal systems in rigorous logical reasoning, so any philosophical theory that has objective proof validity already relies on the same circularities as mathematics. And if a philosophical theory does not have objective proof validity, it cannot be claimed to be objective, and is arguably worse than having 'blind faith' in classical arithmetic. The reason is that any claimed proof in classical logic can be checked syntactically and its validity over the deductive system is decidable and unambiguous, so anyone who thinks their semantic interpretation of the axioms are true, then they are forced to accept the semantic truth of the proven conclusions. In contrast, any non-syntactic system is no better than just arbitrary opinions, since there is no precise delineation of valid arguments. 

Biopower operates on the level of populations. It is, in a sense, myopic in respect to particular individuals. The issues of concern for biopower are, for example, hygiene, health, birth and mortality rate. All of the above become objects of active manipulation: 

Not really. At least for two reasons (but I'm sure there are more). First, Hobbes' "war of all against all" is a specific view of the "state of nature": there is no regulation, no sovereign, no law. Everyone is equal in the sense that everyone is free to take what they can, notwithstanding moral or any other regulative principles. Might is right. Perfect competition, on the other hand, is based on a certain vision of order - market economy. Moreover, perfect competition means there are no monopolies, which is not exactly in accord with "war of all against all" where the strongest is free to take all. Second, Hobbes' state of nature brings no positive outcome for the community as a whole (well, there is not even a community). Perfect competition, in contrast, supposedly results in a desirable situation where consumers benefit from lower prices and better quality. To put is somewhat differently, in Hobbes' state of nature scarce resources are distributed inefficiently - they just go to the strongest, whereas under the conditions of perfect competition scarce resources go, supposedly, to those who can make the most of them at the lowest price. All in all, however, the two concepts come from two very different places and I doubt they can be compared meaningfully. 

There is a significant philosophical problem with the common viewpoint of many set theorists. Namely, the notion of "set" was supposed to capture the notion of "collection". If really there is some set-theoretic universe that satisfies ZFC, then that universe itself is a collection, and clearly the ZFC axioms do not correctly capture that. MK (Morse Kelley) set theory does not solve that, because again there is no class of all classes. In any case, there is no non-circular philosophical justification for ZFC, so ZFC is in fact a red-herring in discussing Russell's paradox. Discarding axioms on reaching contradiction Finally, I want to point out that it is not viable to simply discard axioms that lead to contradiction. For a simple example, if PA is consistent then PA+¬Con(PA) is also consistent, but proves a false sentence (under the standard interpretation of natural numbers in the real-world). This clearly shows that mere consistency is nowhere near enough to make a logic or formal system meaningful, and we must have some kind of soundness. At the least, we ought to have arithmetical soundness (at least at human scales). 

The concept of "hegemony" is usually associated with Gramsci (especially his Prison Notebooks) and "interpellation" with Althusser (Ideology and Ideological State Apparatuses). The latter was influenced by the former. Both concepts are motivated by the same question: Why is it that the proletariat does not revolt as Marx predicted? Hegemony could perhaps be conceived as a state of affairs: the success of the dominant class in making their view of the world (ideology) accepted and internalized by others as their own. Interpellation, on the other hand, is a process of such internalization (subjectivation). Althusser writes that 

Phenomenology, was, perhaps, the most sustained effort to excise this division from ontology. It begins with Husserl's notion of intentionality and his famous dictum that "consciousness is always the consciousness of something", but finds its full force in Heidegger's Being and Time and his notion of being-in-the-world. Perhaps even more explicitly, Merleau-Ponty's Phenomenology of Perception is the criticism of the mind-body distinction. Although phenomenology is a movement, not a set of doctrines, the main idea shared by phenomenologists is that 'things' (objects) cannot be separated from those who perceive them. Hence the emphasis on examining meanings rather than things, and hence the erasure of the subject/object distinction. Phenomenology does not so much resolve Cartesian dualism as shows how it is a superfluous distinction. 

"Only Xs are P" never means "All Xs are P", but rather "The only things that are P are Xs", which is equivalent to "All those that are P are Xs". However, "Only those Xs are P" does mean "All and only the Xs are P". This is because "those" somehow makes the phrase refer to the whole group of "Xs", and so none are left out when asserting that they are "P". In this case, therefore, "Only those who ignore the facts are likely to be mistaken." means "Those who ignore the facts, and no one else, are likely to be mistaken." This is of course false in the real world, since there are people who do not ignore the facts but are mistaken anyway because they are not aware of the facts. But it is exactly what is stated by the English sentence. You then see that the argument is valid, but unsound. So you can say that this question is just a matter of understanding English, not logic. Those who disbelieve my answer can check the Corpus of Historical American English for themselves. To prevent bias, I provide here the first 20 results for the search string ". Only those" in the past 50 years or so. Almost all of them provide unambiguous evidence for the semantics I claimed. The rest should also be interpreted in the same manner. 

References: Deleuze, G. (1983). Anti-Oedipus : capitalism and schizophrenia. Minneapolis: University of Minnesota Press. Deleuze, G. (1994). Difference and repetition. New York: Columbia University Press. 

One example. As it was mentioned above, biopower is exercised across populations (regularization of birth rates, longevity etc.) and is myopic to particular individuals. It promotes life or lets die. An interesting example of the problematic nature of this approach is the "left-to-die boat" case. In 2011 sixty three Lybian migrants died after drifting for fourteen days in an area heavily monitored by NATO. They were noticed, of course, but all kinds of complex maritime jurisdiction trickery was used to avoid responsibility for rescuing the Lybians. Abstaining from rescue can be as potent a killing as targeted murder. Governing migration, and especially when it comes to refugees, is the prime and telling example of biopolitics in action. References HoS1: Foucault, M. (1978). The History of sexuality: An Introduction. New York: Pantheon Books. SMD: Foucault, M. (2003). Society Must be Defended. (M. Bertani, A. Fontana, F. Ewald, A. I. Davidson, Eds., & D. Macey, Trans.) New York: Picador. 

Why the so-called definition is circular It is circular because "repeatedly" cannot be defined without essentially knowing natural numbers. You cannot use the natural numbers to do counting because you have not defined them yet! You are stuck; you must already know what are natural numbers before you can talk about iteration. This is why in mathematical logic the meta-system must already have the collection of natural numbers to be able to define what it means for a formal system to be arithmetically sound (prove only arithmetical sentences that are true in the 'true natural numbers'). And for a slightly less brief account of various assumptions needed in the foundations of mathematics, see this post. Why there is no viable alternative The problem comes right at the beginning even before you can talk about arithmetic. Think about just pure propositional logic. What is a well-formed formula in propositional logic? To even define that, you necessarily must assume the existence of finite strings in the real world, otherwise you cannot even have a precise syntactic form for sentences, not to even say a full logic system. Furthermore, it is not enough to have a constructivistic view of finite strings in the sense that you can recognize them while not necessarily generating them. This is because propositional logic has deduction rules that permit deducing "A ⇒ A ∨ A" for any propositional formula "A", which clearly implies that one must be able to generate arbitrarily long strings. Effectively, to even describe the deductive system for propositional logic one already must accept the existence of the collection of finite strings. One might naively think that perhaps we do not need formal systems at all. But the only way we can precisely and objectively describe something to another person is by a finite sequence of symbols in a common language. Pictures do not work because they are subject to interpretation unless they are in an agreed fixed format, in which case they could easily be encoded by symbol strings anyway. And if we want to have logical reasoning, the mere notion of mathematical proof involves finite sequences of symbols, hence by accepting any formal system whatsoever as being meaningful, we already must accept the basic properties of string manipulation, which amount to accepting TC (the theory of concatenation). But TC (despite having just the concatenation operation and no arithmetic operations) is essentially incomplete, so we cannot pin down even the finite strings! So we do not even have hope of giving to anyone a description that uniquely identifies a collection of finite strings, which naturally precludes doing the same for natural numbers. This fact holds under very weak assumptions, such as those required to prove Godel's incompleteness theorems. If one rejects those... Well one reason to reject them is the following... Why there is no apparent physical model of PA As far as we know in modern physics, one cannot store finite strings in any physical medium with high fidelity beyond a certain length, for which I can safely give an upper bound of 2^10000 bits. This is not only because the observable universe is finite, but also because a physical storage device with extremely large capacity (on the order of the size of the observable universe) will degrade faster than you can use it. So description aside, we do not have any reason to even believe that finite strings have actual physical representation in the real world. This problem cannot be escaped by using conceptual strings, such as iterations of some particular process, because we have no basis to assume the existence of a process that can be iterated indefinitely, pretty much due to the finiteness of the observable universe, again. Therefore we are stuck with the physical inability to even generate all finite strings, or to generate all natural numbers in a physical representation, even if we define them using circular natural-language definitions!