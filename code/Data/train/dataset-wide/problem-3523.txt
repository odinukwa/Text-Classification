However, this retains my $HOME variable with the same value as my initial user, so mercurial could not find the required trusted information in /root/.hgrc When I established a root shell with 

We are in the process of moving our MySQL DB from AWS RDS to EC2. RDS provides us with with a solid HA solution, which we want to replicate in EC2. We've looked at MySQL Utilities and MHA, both of which present problems for us. As a result, we're thing about a DIY solution, which will work as follows: We will have a Master and a Slave replica (Slave 1). We will have a Slave replica of the Slave (Slave 2). Master and Slave 1 will be behind a HA Proxy cluster. We will monitor the Master very closely. If the Master monitor detects a failure, we will run a script that shuts down the Master EC2 instance and stops the Slave process on the replica. Slave 1 will now become the Master, and Slave 2 will become Slave 1. The new Slave 1 will be added to the HA Proxy cluster. We will then rebuild Slave 2 from a backup of Master (backup every 3 hours). We obviously need to be careful to ensure that all steps in the process exit cleanly, and that we have rock-solid error handling. Can anyone offer an opinion as to the viability of our proposed DIY solution? 

I have an issue with routing. I have 2 public subnets: 172.31.1.0/24 and 172.31.100.0/24 In each of these I have a NAT instance. Each NAT instance is an OpenSwan VPN peer to a remote location. This allows the following VPN connectivity: 

Where pclean.sh is the name of the locally saved script file, and it is saved in the same directory as your working directory (otherwise include the absolute path). With this in place, each time you launch a new instance, regardless of its initial hostname, it will revoke any existing cert that has the same hostname, and generate a new one. Obviously, if you are launching hundreds of instances at the same time, you may have concurrency issues, and some other solution will be required. 

There are a variety of solutions for this. OpenVPN will work, but my preferred solution is to set up an L2TP over IPSEC VPN, using an OpenSwan VPN server inside your AWS VPC. IMHO, IPSEC VPNs are a better option that SSL/TLS based VPNs (eg OpenVPN), given all the issues with SSL over the last few years. This allows you to create a local VPN connection on your desktop computer that you can fire up when ever you need access to your VPC. You can choose to direct all your traffic through that or just particular routes. The only qualification I would add is that getting that connection to work in Windows 7 requires a Registry tweak. It works out of the box with no additional software in Mac/Ubuntu. $URL$ Remember to disable Source/Destination checking on the EC2 instance you use for terminating the VPN inside EC2. 

regardless of what the NS records in the zone at Provider 1 are set to. On this basis, I've concluded that the NS records within the zone file are irrelevant to the migration, and that only the name servers records at the TLD level are important. However, I can't get a read on how long it is likely for a change there to propagate, either forward or back. Is it possible to query what TTL are am working with for records emanating from the TLD root servers? 

Skype will only use the Proxy Server configuration if it cannot find another route to the Internet. To force Skype to use your Proxy, you have to edit the Registry settings for the Skype client. Its covered in the Administrators Guide: $URL$ If you want to confirm this, setup a firewall rule to block outbound access from everything except your proxy server. That's how I figured it out. 

Loop through your servers in Bash and then set you exit status based on Bash arguments you pass to the script, then set this up as a Nagios command. The beauty of Nagios is that you can create your own monitors, for whatever purpose. 

The $1 is everything in the REQUEST_URI, and the QSA flag will append the existing query string (the GET vars) to the new URL. 

$URL$ If you're paranoid, just update the yum command to report only. You can also rollback changes if issues occur. 

(172.31.100.102 = Nat Instance 2) This suggests that while traceroute may be aware of a specific route to a specific network, it will only report an attempt to follow that route if routing is allowed on the default gateway for that route. If not, it will attempt to follow the default route and report success or failure for the default route only. I'm sure this is consistent with the design of traceroute, but probably signals that traceroute may not be the best tool to debug routing issues (its more a tool to debug network issues). 

httpd doesn't know what the hostname in the host header is until the decryption process takes place, so if the original request doesn't match the hostname in the certificate, you will get an error. If you want to have both domains work with https, you will need either: 

My company has reached that tipping point where what we are spending on EBS storage warrants a serious look at moving our storage to a SAN/NAS in a Co-Lo location connected with AWS Direct Connect. Pricing Co-Lo and Connectivity in this regard is straightforward, but pricing hardware is very complex at this early evaluation stage. 1TB of storage in AWS EBS costs $1,200 per annum ($0.10 per GB per month). In the order of 300TB, and assuming 3 year depreciation, is it reasonable to think we can get Co-Lo SAN/NAS for < $3,600 per TB? We don't need All-Flash. IOPS requirement would be about 2k IOPS IN+OUT. 

The issue here was that the sebp/elb image contains a timeout parameter that was expiring before elasticsearch could start. The is controlled by the ES_CONNECT_RETRY environment variable, which is set to 30 secs by default. Elasticsearch takes longer than this to start the first time, so when I set this to 300 secs, it worked. You can add this as an environment variable for the elk container in your docker-compose manifest. 

Everything works fine for the former, but no matter what I do, the route table entry for the later does not work. No route that I setup for NAT Instance 2 works. When I traceroute to any address in 192.168.100.0/24, packets are sent directly to 192.168.100.0/24 (and therefore fail) rather than routing via NAT Instance 2. I thought maybe there was a limit on the number of concurrent NAT instances in a Route Table, but even when I delete the route to 192.168.1.0, so that the only route that exists is the route via NAT instance 2, it still doesn't work. I've checked all the usual stuff (Src/Dst check etc) but nothing seems to be out of place. All of this was created with CloudFormation, so manual error isn't likely. 

I run a number of standalone Logstash servers to allow review of log files from web application servers. One of these recently reported a Yellow cluster state due to unassigned shards. This is a common enough occurrence, which I usually deal with by deleting the most recent index and restarting Elasticsearch. In this case, it didn't work. When I delete the indices (either via the API or simply by deleting the files from the file system) and restart Elasticsearch, the cluster state initially is green, but as soon as the first index is created, it turns yellow with precisely 5 unassigned shards. This server was working fine for several weeks, and is not at all loaded. I've also checked that there are no other Elasticsearch servers in the CIDR (Its in a VPC in Amazon AWS anyway). I've turned on debugging in the logs, but its double dutch to me. There are no references to shards not being able to be assigned. 

Why not write your own command/script that checks each server certificate, aggregates the data, and then alerts if a percentage of checks fail? You can check Cert expiry with openssl from a command prompt: 

I would recommend the following considerations: If you creating an IPSEC connection between your corporate LAN and your VPC, use a CIDR that is different than that on your corporate LAN. This will prevent routing overlaps and create an identity distinction for reference. For very large networks, use at least different 16-bit masks in different regions eg 

From what I can see, from looking at the actual files that contain the logs in the target S3 bucket, you will never see a log entry that is < 1 hour old. You will see logs entries that are precisely 1 hour old, which explains they you see entries right up to the hour mark. As such, both Elasticsearch and Logstash are performing as designed, and the issue is with AWS S3.