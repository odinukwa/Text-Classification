In any case, the code you have only checks if is a power of with an exponent less than or equal to twenty, it won't return true for for example. The code to do it a a less limited way is a stunningly simple: 

I'm going to assume you mean integers since that's what your test data suggests. I'm also assuming the powers are non-negative integers so as not to involve roots (a). In that case, there's no need to do all those power calculations, you just need to realise that, if is a power of , then continuously multiplying an accumulator (initially ) by will either: 

My advice is to not mess about with something that's been well and truly tested :-) No, not really: if you find an algorithm that's better then, by all means, use it. However, in this case, for general data, this isn't going to be an improvement. The power of binary search, and other type algorithms, lies in the fact that you dispose of half the remaining search space with each iteration. In other words, if the initial search space (array size) was 1000, the first iteration removes 500 of them. Any change to the "midpoint" (the divider between what you're keeping in the search space and what you're disposing of) that you select during an iteration has the potential to improve or degrade the performance. For example, putting the midpoint at 25% has the potential to reduce the search space even faster (if you're right) or slower (if you're wrong). Now, if you know of some property of the data, you can use that to your advantage to improve an algorithm. In fact, it's the "extra" knowledge about your list (the fact that it's sorted) that allows you to optimise what would normally be a sequential search into a binary one. So it all comes down to how good your extra information is. In this case, the values of the two end nodes alone are not really indicative of where the midpoint should be. You only have to look at the list: 

There's may well be other issues that haven't sprung immediately to mind, I won't think too much about them as, in the absence of further information, they're probably unnecessary. 

to see this in action. If you were looking for in that list, you might decide that, based on the first and last elements being and , it would be in the middle somewhere, which is clearly not the case. Similarly, if you were looking for , you might first check elements around the 1.4% mark (14/1000) which would probably be the first element, despite the fact it's right up at the other end. Of course, that's not to say that other extra information would not help. If you knew that the data was fairly evenly distributed over the range, then your improvement may be worth it. You also need to be aware that it's usually only going to be important with large data inputs, so it may not necessarily be worth it even if it turns out to be much better. Even bubble sort is blindingly fast for 100 elements :-) 

Before you ask: The code was written in times of .NET 3 and is still required to work in .NET 3.5, so was and is no option. Anyway the code works fine but now looking back in retrospect the design strikes me as a bit non-optimal. So I'm looking for some input regarding design/best practices/style. Base cache class implementation This is the base cache class. The cache entry holds the value to be cached and also some meta data, mainly time of last access and an index reference. The index reference is a node in a linked list. Whenever a cache element is accessed it is being moved to the front of the list. This allows for an easy LRU eviction strategy for a size based cache. The access time can be used by the age based eviction cache. My main quarrel with this is that the base class contains knowledge about both strategies although it does not implement any of the eviction strategies itself. 

I would use instead of as it conveys the intention better. Instead of using you should use . This avoids having to iterate over the entire sequence just to find out if there are any items in it. Also you are not interested in the entire you're just interested in the first one if it exists so you should make use of which also provides a predicate overload. This loop: 

The way this works is: First we try to find the node which should be removed and we also keep a pointer to the previous node. Once the loop finishes there are several cases to check: 

In addition to what @lol.upvote already said: I don't really like the way you pass the column names and values around for several reasons: 

You don't need to capture the result of a function call in a local variable to return it outside of the lock. You can simply do: 

Apparently passing in for the release action is not valid so in your constructor you should check and throw an if that is the case. I might be missing something, but shouldn't this be called the other way round: 

Your users will only ever get the but your implementations ( and ) will implement . Update Looking at it again it don't really see the point of the . If the user will have to call the manager in order to save the preference then why don't make the method public in the first place - so you end up with only one interface. Then the user can call (the arguments he'd have to supply anyway when calling the manager). 

Bug: you assume that the result will never be longer than the original string while in fact it could be twice as long. For example would yield . Instead of using to initialize you can also initialize it to 0 like this: 

For the LINQ query at the end is your friend. You should be able to achieve your desired results in a single query: 

Which will save you a lot of rewriting if you decide to change the names of any of the properties down the line 

Then you can have your validation method return a state, and you can do your checking block after, like this: 

As this part of the question has been skipped so far, I'll take it: Do you think this is too hard? I think it is. Similarly to your game, in classic Mastermind, the player has to guess a combination of 4 non-unique coloured pegs, and each time is given a mark for the number of correct position and colour combinations. 64 = 1,296 possible combinations However, with your game (to borrow the nomenclature), there are 10 possible pegs, and as they're unique the combinations are: 10 x 9 x 8 x 7 = 5,040 possible combinations Which is an increase of 288.8%, furthermore you're only giving the user 5 attempts to guess, as opposed to Mastermind's 6, 8 or 12. The reason Mastermind plays with a minimum of six, is that you get to test every combination, e.g: 

I don't really like the method of exception pseudo-typing you're doing, I'd use different objects to represent the type of exception rather than the content, like so: 

Additionally, I'd suggest reading up on your SQL for a start, this whole thing screams out clause. In every case, your database will be more efficient at sorting the results than whatever programming language you're using them in (and certainly PHP). Such as: 

The first thing I spotted was this: . I'm not sure if that's an error with your page name or code, but whichever it is needs to be fixed. 

You can increase the readability and reduce the number of checks required in your if block by performing each of the four booleans you get at the start, then checking against them instead of performing it over and over as well as separating out the check then reveal logic into its own method, like so: 

There's a couple of issues with it, for start your nested if blocks are unnecessary: instead of manually checking and returning true or false, just return the result of those methods, like so: 

The best optimisation is only doing what's necessary. When checking the login, you don't check whether they've even been specified until after you hash it, consider reordering it like so: 

It's secure, but it's really chunky and tedious to write. I'd do this by overriding the magic method to trick GSON into thinking the methods exist, like so: 

Another thing you should be careful about is ing the same thing over and over, as it's usually unnecessary and can be replaced with a which will ensure that it's only loaded if it hasn't been in the current session. 

If the code will be distributed, read or maintained by others, you might want to look at your naming conventions as they're a bit inconsistent, for example sometimes you use underscore separated names such as , but other times you use sulkingCamelCase such as and in class case. Either one is fine, as long as you use the same throughout. Aside from your single line getters and setters, the function names you've chosen are not always indicative of what the function does. Ideally you shouldn't have to read the actual code to know what a function does, consider these changes: 

I think and are more descriptive names than and . In C# the standard naming convention for methods is . You could save several iterations by doing the following things: 

Your list does not have a nice API to use. You can see that in which is cluttered with handling of many special cases. Some methods like ( would be a different candidate) are inefficient as you have to iterate the entire list first. You could avoid that by keeping a separate pointer. 

is not a good name for the function as it does not say what it checks. It returns false if all numbers below are present and otherwise. So a better name might be . You can make your loop body a bit shorter by inverting the condition: 

Now design: I find it a bit odd that your class represents a descriptor for a status code and acts as a container for all status codes at the same time. This looks to me like an SRP (Single Responsibility Principle) violation. I would split it up and maybe put some generics in the mix (unfortunately enum can't be used as generic type constraint). So something along these lines: 

The violates the general expectation of methods in the .NET framework. A method should return to indicate whether parsing has succeeded or not and return the actual result as an parameter. Your implementation will simply throw (which is what the methods were meant to avoid) if the fails. Magic numbers like and should be avoided - introduce named constants for those like and . Instead of just blindly waiting in the you should create another you can wait on - it should get set when the main loop has quit. Instead of dealing with threads directly you really want to have a look at the Task Parallel Library. Instead of polling you should make use of the asynchronous socket API and (examples can be found on MSDN). Or in these days you probably want to be more looking at to make use of the feature in newer .NET versions. Those are far more effective since they won't waste CPU cycles trying to read data which isn't there. Especially when you are targeting low power devices similar to Raspberry Pis you really want to be as efficient as possible. Since you have no guarantee that you will read the entire message in one go you should consider supporting partial reads. UDP gives you no guarantees about packet delivery which means packets may be dropped and/or delivered out of order. Your application protocol will have to deal with this. 

The main issue is that the key event handler and the timer tick will run on different threads which means that someone could press a key while the timer tick fires - in which case the could be modified while you are trying to read it from another thread. You will need to safeguard against that by protecting the access to the with a You can probably get away with it because the runs on the UI thread where the / events are also being raised. The idiomatic way of subscribing to event handler is to let the user subscribe to the event rather than passing the event handler via the constructor. What happens if the user has shorter lifetime than the event provider? In your case it would keep it alive because you can't unsubscribe. When raising the event you need to check if the vent handler is null - subscriptions to an event are considered optional which means no subscriber might exist. Also you should not pass null as sender or EventArgs. The idiomatic pattern in C# goes like this: