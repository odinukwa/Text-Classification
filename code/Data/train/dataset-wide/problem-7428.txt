Expanding on the second open problem: Given $A \subseteq \mathbb{Z}_6$, define an $A$-$MOD_6$ gate to be one which outputs 1 iff $\sum x_i \in A$ mod 6. The most embarrassing open problem in circuit complexity may be the following: Show that for all possible subsets $A$, the AND function requires superpolynomial-size depth-2 circuits of $A$-$MOD_6$ gates. [PS: Thanks to Arkadev Chattopadhyay for explaining some of these $MOD_6$ problems to me.] 

The following two statements are really "obviously false", but are still open: $EXP^{NP} \subseteq$ depth-2-$TC^0$ $EXP^{NP} \subseteq$ depth-2-$AC^0[6]$ Just as a reminder: 

The first way I try to solve questions like this is to "ask Maple" (or Mathematica). If you have access to, say, Maple, then you can type "sum(x^k/k!^2, k=0..infinity)" and it will report BesselI(0,2*sqrt(x)). [It's impossible to tell from the font, but that is "Bessel" followed by capital-i.] If you're me, that's when you search for Bessel functions on Wikipedia. And furthermore, you can then type "asympt(BesselI(0,2*sqrt(x)),x)", and it will report that the leading term in the asymptotic expansion is indeed $\frac12 \frac{e^{2\sqrt{x}}}{\sqrt{\pi} x^{1/4}}$, as others have said. I'm not sure what resource will immediately explain "how Maple knew that", but at least one knows the answer at that point. 

In this paper -- $URL$ -- we needed a non-iid "Berry--Esseen"-type result for vector-valued rv's with singular covariance matrix. Because of the singular covariance matrix possibility, we couldn't seem to use anything from the literature in a black-box fashion, so we had to make our own proof. We used the Lindeberg method, rather than the characteristic function method. The final statement had a slightly unusual conclusion, with error being measured with respect to unions of orthants, because that's what our application needed. But anyway, for what it's worth, here's what we obtained: "Let $X_1, \dots, X_n$ be independent ${\mathbb R}^d$-valued random variables satisfying $E[X_j] = 0$. Assume also that each is "$(2,4,\eta)$-hypercontractive", meaning $\|a + \eta X_j\|_4 \leq \|a + X_j\|_2$ for all $a \in {\mathbb R}^d$. (This is a 'niceness' condition, saying that the $X_j$'s don't have very skewed distributions. An example case where it holds is if $X_j$ is of the form $x_j w$, where $w \in {\mathbb R}^d$ is a nonrandom vector and $x_j$ is a random variable satisfying $\|x_j\|_4 \leq C\|x_j\|_2$; then the condition holds for $\eta = \frac{1}{\text{const}\cdot C}$.) Let $S = X_1 + \cdots + X_n$ and write $M = Cov[S]$ (possibly singular). Assume that $M$'s diagonal entries are 1. Finally, let $G$ be the Gaussian random vector with mean 0 and covariance matrix $M$. Then $S$ is close to $G$ in the following sense: For any set $A \subseteq {\mathbb R}^d$ which is a translate of a union of orthants, $\Pr[S \in A]$ and $\Pr[G \in A]$ differ by at most poly$(d/\eta) \cdot (\sum_{j=1}^n \sigma_j^4)^{1/8}$, where $\sigma_j^2 := \|X_j\|_2^2$. (One expects this last factor to be "small" if no one of the $X_j$'s is ``too dominant'' in terms of variance.)" As I said, the "translate of union of orthants" way of measuring is somewhat weird, but I expect that you can allow a larger class of test sets. 

Doesn't quite answer the question, but in this paper -- $URL$ -- it is stated that Tessler and Louidor showed that in the infinite d-regular tree with d even (and random tie-breaking), for uniformly random initial spins, almost surely there are vertices that change their spin infinitely often. I couldn't actually find the paper being referred to, and it doesn't clearly answer the question because of the random tie-breaking. 

Here is a simple proof which should give exact optimizers for "most" choices of n mod 4 and max/minimizing; and, near-sharp values for the remaining cases. I'll do it for upsets instead of downsets. -- Identify the discrete cube with $\{-1,1\}^n$ and let $f : \{-1,1\}^n \to \{0,1\}$ be the indicator of $\mathcal{D}$ which is monotone since $\mathcal{D}$ is an upset. Up to a factor of $2^n$ we are trying to min/maximize $E_x[f(x) \chi(x)]$, where $x$ is uniformly random on the cube and $\chi(x) = \prod_{i=1}^n x_i$. Let $1 \leq j \leq n$ be uniformly random and let $x^{(j)}$ denote $x$ with its $j$th coordinate negated. Since $x^{(j)}$ is also uniformly distributed, $E_x[f(x) \chi(x)] = E_{x,j}[\frac{f(x) \chi(x) + f(x^{(j)})\chi(x^{(j)})}{2}] = E[\chi(x)\frac{f(x)-f(x^{(j)})}{2}]$ where we used $\chi(x^{(j)}) = -\chi(x)$. Thus in absolute value, the quantity is at most $E[\bigl|\frac{f(x)-f(x^{(j)})}{2}\bigr|] = \frac{1}{2n}E_x\left[\sum_{i=1}^n \bigl|f(x)-f(x^{(i)})\bigr|\right] = \frac{1}{2n}E\left[\sum_{i=1}^n f(x)x_i\right]$, where the last step uses that $f$ is monotone. But $\frac{1}{2n}E\left[\sum_{i=1}^n f(x)x_i\right] = \frac{1}{2n} E\left[f(x)(\sum_{i=1}^n x_i)\right]$ is clearly maximized among $0$-$1$ functions $f$ by the "Hamming ball" which is $1$ when $\sum_{i=1}^n x_i > 0$ and $0$ when $\sum_{i=1}^n x_i < 0$. (If $n$ is even and the sum is $0$, it doesn't matter what $f$'s values are there.) -- For the purposes of checking sharpness, note that the maximizing $f$ (or $f$'s) there happens to be monotone. The only inequality used is in fact sharp if $n$ is odd and $n$ has the "right" (vis-a-vis min/maxing) remainder mod $4$; if $n$ is even then I think you can get a sharp inequality for any remainder mod $4$ by suitably making $f$ equal to $0$ or $1$ on the middle Hamming layer. I hope a little trick can handle the case of odd $n$ congruent to the "wrong" value mod 4 but I didn't think about it. -- For people familiar with "analysis of boolean functions", I think this result could be considered "folklore". For example, it essentially appears at the end of page 8 here: $URL$ 

The answer to this question should be well known, but it's a hard question to search for online. Suppose we want to approximate the function $x^n$ by a polynomial of degree $d$ in the $L_\infty$ norm on $[-1,1]$. What is a good estimate of the error of the best approximator, in terms of $n$ and $d$? I know this question was solved exactly by Chebyshev for $d = n-1$ (the error is $2^{-d}$ I think). The range of interest for me is $\sqrt{n} \leq d \leq n$ and I don't mind log factors in the estimate. Thus I would be happy to have an estimate for the error of the Chebyshev expansion truncated to degree $d$. (A bonus would be an answer to the same question for $(1-x^2)^d$.) Thanks! 

Clarification: by "piecewise", I mean a finite number of pieces. I'm sure this must be true, but my search for a citation was in vain (although I did learn the new term "polyconvex"). Thanks! 

I interpret this as being about (Berry--Esseen-style) closeness of $X_1 + \cdots + X_n$ to the $d$-dimensional Gaussian distribution, when $n$ is fixed and when the dependence on the dimension $d$ is carefully taken into account. It depends on what class of 'tests' you use to measure closeness, but the best general result along these lines I know is from Bentkus, "A Lyapunov type bound in Rd": $URL$ He shows closeness for all convex test sets, with the dependence on the dimension being $d^{1/4}$. 

Short version of the question: Presumably, it's poly$(t)$. But what polynomial, and could you provide a reference? Long version of the question: I'm sort of surprised to be asking this, because it's such an extremely basic sounding question. Here are some variants on it: 

Write $\beta = \lambda_n$, the top row of your GT patterns. It's a theorem of [Baryshnikov] that if we choose a uniformly random point in the polytope GT${}_\lambda$, it's equivalent to choosing a Haar-random Hermitian matrix with spectrum $\beta$ and then taking its "principal minors". (I've also seen this fact credited to Weyl, and others.) More precisely, let $B = \mathrm{diag}(\beta)$, and form a matrix $X = U B U^\dagger$, where $U$ is a random unitary. Then let $\lambda_{11}$ be the top-left entry of $X$, let $\lambda_{21}, \lambda_{22}$ be the eigenvalues of the top-left $2 \times 2$ submatrix of $X$, ..., and let $\lambda_{n1}, \dots, \lambda_{nn}$ be the eigenvalues of the top-left $n \times n $ submatrix of $X$ (namely, $\beta$). Then $\lambda$ is uniformly random in the polytope GT${}_\lambda$. This probability distribution on $\lambda$ is basically your integral, but we have to divide by the volume of the polytope, which is $V(\lambda)/[(n-1)! (n-2)! \cdots 2! 1!]$. I guess this is standard? If not, it's also in Baryshnikov. Having done so, your identity is the Harish-Chandra--Itzykson--Zuber identity applied to the matrices $A = \mathrm{diag}(\alpha)$ and $B$. This follows by inferring the diagonal entries of $X$ from the Gelfand-Tsetlin pattern $\lambda$, which you can do because the Gelfand--Tsetlin pattern gives you the traces of all the top-left submatrices. (By the way, I think the [Faraut] paper referenced below has a good exposition of some related things.) Baryshnikov, Yu., GUEs and queues, Probab. Theory Relat. Fields 119, No.2, 256-274 (2001). ZBL0980.60042. Faraut, Jacques, Rayleigh theorem, projection of orbital measures and spline functions, Adv. Pure Appl. Math. 6, No. 4, 261-283 (2015). ZBL1326.15058.