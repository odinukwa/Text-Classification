No, MongoDB does not create backups by default. Recommended backup methods are described in the MongoDB manual. I would consider the MongoDB manual the definitive reference source as it is kept current with product changes. Blog articles may be outdated or contain incomplete/incorrect advice. 

The parameter controls the base port that a or server listens to for TCP connections. The HTTP interface is accessible at a port number that is 1000 greater than the base port, so given a of 28017 you can connect via port 29017 if you have enabled the HTTP interface. You can see both ports listed in the server log on startup: 

There are several online testers for YAML syntax that can be useful to troubleshoot problems (eg: YAML Lint). 

The value is in microseconds and the indicates how many operations were observed since this last restarted. For an unused collection both of these values would be low or 0. You can sample metrics over time to see if counters continue to increment due to usage. If you want to check which commands are being run against a database, enable the Database Profiler with profiling level 2 (all operations): 

When you add a new shard using you are providing a seed list of members which will be used to discover the current replica set configuration. You can specify as few as one member of the replica set or as many as all, and you do not need to specifically include the arbiter. FYI, the shell helper wraps the underlying command (which has a few additional options not currently exposed via the helper). 

Is the collection that you must shard stressing your current deployment? Before moving to a larger deployment with more moving parts I would encourage you to try to understand and tune your existing deployment. There may be some easy wins that could improve performance ... or conversely, unfortunate design choices that won't scale well in a sharded environment. Helpful starting points include the MongoDB Production Notes, Operations Checklist, and Security Checklist. If you have good understanding or estimation of your data model and distribution patterns, I would suggest sharding in a test environment using representative data. There are many helpful tools for generating fake (but probabilistic) data if needed. For an example recipe, see: Stack Overflow: duplicate a collection into itself. 

This indicates the BSON type and size (in bytes) of that object including nested fields/objects. Potential use cases for confirming underlying data types might be checking for consistency (eg. a given field has the expected type) or confirming underlying data types in order to do a query using the operator in MongoDB. You can perform a similar size calculation for a document/object in the shell using . 

Although you've excluded third party solutions, there are currently no built-in features or feature requests matching your description. You could certainly raise one or more feature requests with a description of the use cases, but I think both of these approaches sound more suitable for implementing in an application layer with your business logic rather than embedding that into a distributed database. As a more general solution to enable the use cases you've described, I suggest putting an API layer between your applications and your MongoDB deployment. The API layer could intercept queries and perform the desired actions based on your use case, and is an option available to you without any dependencies on MongoDB server features. 

It generally makes sense to have a shard key that supports your common queries so they can be targeted at a subset of shards with relevant data, but this doesn't appear to be possible in your case as both and are optional fields. If your field provides good cardinality (i.e. large number of values) but is monotonically increasing (eg. default ObjectIDs) you could consider a hashed shard index on the field for good write distribution. The hashed index wouldn't support your common read queries (unless by specific values) so you would need a secondary index for your queries on and (i.e. {}). The recommended secondary index(es) and order will depend on your common queries and sort order. For further background information I suggest reviewing: 

The method returns a cursor, so what you have calculated is the size of the cursor object rather than the size of a document. You should use to return a single document if you want to measure the BSON size: 

MongoDB (as at 3.0) only supports a single primary per replica set. Replica sets can have up to 50 members, with up to 7 voting members. The 2-node replica sets you have described should have a third member (either a data-bearing secondary node or a voting-only arbiter) to allow for failover. Replica sets require a strict majority of votes (n/2+1) in order to elect and maintain a primary, so a 2 node replica set requires both members to be healthy (see: Replica Set Elections). Replication Topology In terms of replication flow, the deployment of a replica set could be described as either: 

A 2dsphere index in MongoDB 3.0+ will allow additional values in the coordinate array but only the first two values will be indexed and used in a 2dsphere query. This allows GeoJSON objects with additional elements (eg. altitude) to be saved in MongoDB, but interpretation or use of those extra values is up to your application code. The MongoDB manual has information on supported GeoJSON objects. There's also a relevant feature suggestion you can upvote/watch in the MongoDB issue tracker: SERVER-691: n-dimensional geospatial search. 

These storage metrics are informational counters and expected behaviour (i.e. unrelated to data integrity). There isn't any obvious actionable outcome based on these metrics, but if a collection wasn't getting much overall benefit from compression the counters might provide insight into how often pages aren't compressed (as compared to compressible pages that don't have a significant level of compression). 

As at MongoDB 3.2, replica sets can have a maximum of 7 voting members with up to 50 members in total (see: Replica Set Limits). 

You could estimate how long the file transfer will take and ensure the oplog is a reasonable multiple of the worst case file transfer time. I strongly recommend avoiding any approach which leaves you without a viable secondary while you are copying/syncing data; racing against the oplog duration is risky if something goes amiss in the copy/sync process and it takes much longer than you planned for. 

As at MongoDB 3.4, there is no server-side functionality for query listeners or triggers. There are some open feature suggestions for triggers in the MongoDB Jira issue tracker, but these all appear to be associated with operations that update data (insert, update, delete) rather than queries. 

To ensure there are no migrations in progress while taking your backup, you should stop the balancer via . Any in-progress migrations will be completed before the balancer is disabled. If you want to ensure the balancer is stopped before taking your backup, you can run the following in the shell, which should return : 

In general MongoDB only supports mixing adjacent major versions (eg. 2.6 and 3.0, 3.0 and 3.2, 3.2 and 3.4) in a replica set for the purposes of a rolling upgrade to a new version of MongoDB. Mixed version requirements are currently only mentioned in the upgrade notes but I expect the underlying assumption is that new deployments would always use start with the same version. If you create a new replica set deployment using MongoDB 3.2, the replica set will be using a new election protocol ("version 1") that older versions of MongoDB do not support. For more information, see: Replication Election Enhancements. The proper solution is to add a MongoDB 3.2 arbiter to your new 3.2 replica set. For any new deployment I would definitely recommend using identical versions of MongoDB to avoid unexpected issues. 

There isn't enough information to determine if your disk is the most limiting factor, but running a sharded cluster on a single computer with a slow HDD will certainly add resource contention challenges. Choosing appropriate shard keys should minimize the need for data migration unless you are adding or removing shards for your deployment. Assuming you are using a recent version of MongoDB with the WiredTiger storage engine as default (MongoDB 3.2+), you will definitely want to explicitly set to limit the internal cache size for instances. See: To what size should I set the WiredTiger internal cache?. 

With a 32-bit O/S the total addressable file size for MongoDB's MMAP storage engine is limited to ~2GB of data including indexes (and journal, if enabled). Given the 32-bit limitations for memory-mapped files, journaling is off by default on 32-bit MongoDB builds as otherwise the data limit would be halved. This is not a recommended configuration for any important data: unclean shutdown is very likely to result in data integrity issues. The WiredTiger storage engine, which is the default storage engine in MongoDB 3.2 and newer, only supports 64-bit operating systems. From the log provided it appears that you have exceeded the maximum storage size that can be memory-mapped for a 32-bit server: 

A replica set can only have one primary, so you wouldn't gain anything by swapping the roles of the current primary and secondary. The data still needs to be replicated. 

In general, most replica set deployments have sufficient members to allow for automatic failover (i.e. minimum of three members). If high availability is not a concern (or you prefer to have manual intervention), a two member replica set is not disallowed. However, a more typical option to allow failover would be to add a third voting-only member (aka an arbiter) to your replica set. An arbiter is a lightweight voting-only member that will provide the additional vote needed for a majority in the event one of your two data bearing members is unavailable in a three-member configuration. The arbiter helps with availability but is a compromise over a third data-bearing member. When a three member replica set with an arbiter is in a degraded state (i.e. one of your data-bearing members is unavailable) you will still have a primary but no longer have data redundancy or replication. An arbiter will also prevent your application from being able to rely on the write concern to ensure that data is committed to a majority of replica set members. My personal recommendation would be to deploy three data-bearing members as a minimum for a production replica set, however fewer data-bearing members could be considered for a development or non-critical deployment.