I had this same question a few months back, and ended up contacting AWS (I have Enterprise support). This was the result: 

You could probably do this in a disaster recovery situation, where getting the database up is important, and you don't mind a rollback. Something like: 

EFS runs on a system of IO credits, these credits are generated constantly throughout the day based on the amount of space you are using in EFS. Every time you read or write to an EFS volume you consume these credits. If you have no IO credits in your balance, reads and writes will wait until you do. I think it's somewhat likely, you've managed to use up all your credits, and the background IO keeps depleting your credits, keeping performance horrendous. You can check the number of credits an EFS volume has by checking cloudwatch. Just to clarify this a little more. If you were only using 1GiB on an EFS disk, any time you read/wrote at a rate exceeding 50KiB/s you would burn through credits, and any time you read/wrote a rate lower than 50KiB/s you would generate credits. In comparison by having 10GiB in an EFS disk you would be able to sustain 500KiB/s. I'm utilising EFS with one service, and found I had to generate roughly 80GiB of raw useless data, which just sits on the EFS disk to generate enough IO credits to allow my application to use the share. I set up a cloudwatch alarm to notify me if my credits ever fall below a threshold, to give me time to add some more 'useless data' to the EFS disk to allow me to sustain usual performance. I recommend reading Amazon's official suggestions on EFS throughput scaling if you think this effects you. 

With ALB, Targets are defined in the same way. The main difference is that you define the port you want to connect to your instances on, in addition to which instances you want to connect to. The most common way to configure this, is to point at the instances on port 80, as this is what most webservers are running on, although like in the ELB case, to enhance security, you might want to use HTTPS on the instances using self signed certs to keep everything encrypted end to end. Similar to with ELB, your most important bit of information here is the target status, in this instance, you want to see 'healthy'. If you don't see this, wait a little while, or investigate your health checks. 

Where members[0] is a known 'good' server. A safer way to do this would probably to install a small management service on each mongo node that you could hit via rest api. If at all possible, admin commands should be relayed to the primary. Automating this sort of thing would be quite difficult, as there is likely no easy way for your applications and servers to agree on what servers are 'good', and which servers are bad. One common model for mongo clusters is to run nodes in multiple locations, some of which may have less reliable routing. In these cases I would recommend modifying your node priorities. If you have servers you would never want as a primary, set their priorities to 0, and set nodes in your preferred location as priority 2 (or higher). For more information force reconfigure, read: $URL$ Edit: I think I'd be tempted to play about with mongo arbiters. By running mongo arbiters in the locations where you run the clients, they would be able to vote in the election process. Servers that can't be seen by the majority of voting servers cannot be a primary. 

If the instance needs to stay online while you attempt the recovery, or want to do a practice run through before doing it to the live volume you can use a snapshot of the problem volume. Take a snapshot of the running volume, and restore that to a new volume and follow the steps above with that volume. 

You do not need to share the top level folder. If you share this folder, you will be exposing the /share/private folder as you highlighted so do not do this. I think your problem is the typo in your client fstab. You have: 

If you are using ALB, the listeners tab will again map 'what people request' to 'where it is coming from', but instead of just mapping ports, it maps to 'Target Groups'. In most situations, the effect will be the same as ELB, you are piping both port 80 and 443, to a single target group. 

This is an IP belonging to AWS, so I'm assuming this is going to the right place. I would check to see if that matches the EIP, but based on what I'm seeing below, I'm going to assume this is correct. Using curl against that IP, I get: 

Which would look to mount /foo from 192.168.101.254, but you are sharing out /share/foo, so that would need to be: 

If the issue doesn't show up after working your way through those steps, it's usually caused by some sort of process or understanding issue. For example, did you actually copy the file to that location on each machine first, as that path is local to machine running the policy, not where it is written. You can use GPO to deploy files from DC DFS or network shares to the machine, but you need to be aware of authentication issues. IE if it is a computer policy, the computer must be able to access that share without a user's grants. Remember you can force refresh GPO's on the users machines using 'gpupdate /force', as GPO's aren't applied immediately. Logs will show up in the results tool, after a refresh. 

If you have problems with GPOs not applying the way you think they should, the best way to diagnose this is by using the 'Group Policy Results Wizard' in the Group Policy Management app. Start the wizard, and point it to a machine of the a user who isn't getting the background and select the relevant user. The results wizard will display 3 major things: 

Alternatively, another way to do it would be to create a database snapshot, and spin up a new instance from the snapshot. You might want to look at both approaches. Both methods will probably cause you some downtime unless you are able to run your application in read only mode for a while, or have a method of replaying transactions on the restored snapshot. 

In my experience, it's been fairly standard practice to make a policy that just grant's access to the bucket and contents. I'd typically use a policy like this (don't want to allow this user to override bucket permissions, or delete the bucket, etc): 

You can use this to set default values for almost all command line flags, utilising wildcards, etc. I personally combine this with simple bash aliases so say typing "sw" is an alias for "ssh web" just to make my life that little bit easier. Just to clarify a few things above. You only need to override the values you need, so you can likely save time. If the only thing you need to override is the username, for example, you could put most the hosts under the same host, separated by spaces. In my ssh config file, some definitions are 10 lines long, configured with port forwarding, and special keys, and x11, other definitions are only 1 line long, and matches thousands of hosts. In the example above, the "db?" would match "db3". Using comments (#) you could still build the concept of labels (or just merge the same server types together). If you want the server list for audit reasons, etc you can also use the 'LocalCommand' (and 'PermitLocalCommand') options to perform actions on the local machine, such as write out to a local text file. 

DNS Specifications state that the 'root' domain, ie "domain.com" cannot be a cname (or at least not with a standard setup). Some hosts will allow you to configure it as such, but your 'mileage will vary', as DNS relays, caches, and clients shouldn't be expected to support it. (The reason for this is that cnames work for all record types, and you will want additional records at the root domain: MX, NS, etc) Some DNS providers knowing that this is a 'common request', have a few workarounds, such as hosting a redirection service (you set the root domain to a set IP, and they handle the redirect), or offering 'forwarding' (effectivly the same thing). These generally don't work for HTTPS though as it would require a valid SSL Cert on the redirection server, and that's more than most redirection services are willing to invest (it's also fairly bad practice, as it's a single point of failure, as you can bet most providers won't use anycast). If you want to have the root of your domain pointing to an ELB and work correctly with HTTP and HTTPS, the recommended way to do this is switch your DNS nameservers (you don't need to transfer the registration) to AWS Route53, as they support 'alias' records, where AWS will look up the IP address of your ELB (these change periodically, so don't try this yourself), and reply with the ELB details as a standard A record response. A more generic way to do this, is to use a DNS provider that supports 'ALIAS' or 'ANAME' records, which work similar to the way Route53 works, but a little less intelligently (Route53 doesn't need to nslookup ELBs hostnames). 

This shows to me that I'm indeed hitting a webserver running on , but the webserver I'm reaching thinks I should be somewhere else, and is forwarding me to (This is not Route53 related). This means that somewhere on the webserver located on there is some code that is triggering a redirect. This could be in your webserver (apache) config file, it could be a .htaccess file, or could be in the application itself. A lot of websites during the setup phase, make you input a 'primary' website address, which you get redirected to if you visit in any other way. In the past trying to solve this with Wordpress (if this was miss-configured on initial setup) you needed to modify a setting in the database before you can visit the website without being redirected. I can confirm that we are getting to a working webserver and DNS is working, because running the following command: 

Network interfaces are exposed for many service you spin up within your VPC, this includes things like load balancers, databases along with normal EC2 instances. Network interfaces can be considered the same as standalone Ethernet card in the real world. When you move a network interface between machines it can keep its public, private and mac address. Under normal circumstances assigning an EIP to an instance, is actually just assigning an EIP to that instances primary interface. A primary interface is created automatically when creating an instance. [AWS Docs] This does get a little more complicated when you include 'ec2-classic' mode, as these instances don't have the concept of network interfaces, as they are pre-vpc. In this mode, EIP's are attached to the instance, not the interface. More details on the differences between the 3 'vpc modes'. One other thing to note is that you can assign multiple interfaces to the same instance, so there will be some situations where you will need to explicitly search by interface, and not instance. 

Amazon doesn't grant you anyway to log into a VM via any virtual KVM or remote host console. If you are unable to SSH in at all, which sounds like the case, one way you can recover an instance is by mounting the root partition on another host. To do this you would: 

Firstly, don't feel as if you have to go down the Amazon Certificate Manager (ACM)/Load Balancer route. It is a good solution, but it is designed for situations where you have a few servers behind load balancers, rather than a single stand alone instance. Another potentially cheaper option could be to use Cloudfront in front of your site, with an ACM certificate, or use Lets Encrypt on the instances itself. That said, you asked about LBs, so here we go: If you are new to load balancers in AWS, you first must understand there are two types: ELB and ALB. ELB (Elastic Load Balancer) are the old 'classic style' load balancer, which basically relays the connection through the balancer, without doing any fancy logic. You throw it in front of any number of instances, and you go through to a random server in the pool. ALB (Application Load Balancer) are a little more complicated, in that they can do some application logic routing. When using ALB you define Target Groups, as well as routing rules, which mean you can send traffic to different sets of instances depending on the requesting path. ALB can be used in a similar way to ELB, and AWS appears to be pushing their use. Regardless of which type of LB you use, you are still putting a balancer in front of the instance, which relays traffic to the source machines. HTTPS/SSL complicates things a little bit when talking about AWS LB, as they are configured in slightly different ways. If you are using ELB, the listeners tab in the AWS interface lets you configure the port mapping. This is where you map 'what people request' to 'where is it coming from'. In this case, you would probably want 80 -> 80, and 443 -> 80. As you want to listen for both http and https, but only connect to http on the server, as it doesn't have https. For a more advanced and secure configuration you could install an self signed certificate on the server to encrypt the connection end to end, and then use 443 -> 443. 

When you provide a 3rd party service via an AMI, you don't have to leave account provisioning intact. Meaning that unless you tell them the SSH or RDP details, it can be somewhat difficult to gain access to the EC2 instance. I've rented a service from a 3rd party before, and was not given any credentials to ssh or rdp to the box, and was only able to manage the box via a web interface. If you haven't come across it before, the AWS Marketplace is designed to allow 3rd party software to be run on your AWS account, and AWS builds in some protections to the 3rd party, such as preventing users from detaching EBS volumes from the official AMI and attaching it to another, or cloning AMIs for personal usage. Sadly, if you want to use AWS Marketplace, you do however need to allow users access to perform OS-level administration. I don't know how far that actually extends to however, you are required to lock down the root/admin accounts, and allow access via a normal user. Perhaps chrooting this user could offer some code protection. A good example of that is some of the professional security software: $URL$ You can also use the AWS marketplace to bill AWS users for SaaS.