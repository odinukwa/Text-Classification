First off, check your media converters and network links, and run a comparison when using them vs when connected directly at 1Gbps to the server over a short CAT6 cable. Is there a difference? If there is, you need to improve the network. For example, run cat6 directly without the media converters, or run fiber for the entire run length. Chances are though, your network is set up that way for a reason and you might not be able to change it. Outside of that, check your raw ping times between the server and clients, it should be 1-5ms on LAN and should traverse as few devices and networks as possible. MS RDP is a lightweight in terms of features, and remote desktops really REALLY aren't suited to intensive graphics at high resolutions. If this is a major problem for you, you should be looking into Citrix ICA with speedscreen enabled. You should see significant refresh rate improvements with that vs RDP. Here's a comparison: $URL$ 

Given the size of your organisation and requirements, consider Microsoft's cloud Exchange offering: $URL$ It'll meet your requirements. Your scenario appears to match up perfectly with the current 'ideal case' for cloud-hosted email providers. You could also look at google's professional mail hosting option. Again you'll get your calendaring, address book, device sync, spam filtering, IMAP connectivity, all as part of the package. Managment will love the low up-front cost (basically just man hours to migrate) and the monthly per-user spend. 

How are these conflicts occurring? Some kind of simultaneous access by more than one user, or by more than one machine logged on at the same time? If so, see this note on DFS being incompatible with concurrent offline files usage: $URL$ 

The snapshot feature on VMWare can be flaky if you keep a snapshot on a VM for too long. When there's a snapshot active on a VM, the system keeps a static copy of the VM hard disk & generates a kind of diff file to track changes from that point onwards. When you 'delete' a snapshot, what really happens is the diff file and base disk get merged together. The system works quite well but has limitations with: 

PlateSpin Protect is one product that can do this. However like all the solutions in this space, it's kinda messy, difficult to work with, and I wouldn't recommend it. I'd advise a case-by-case look at your replication requirements: 

In theory, the clients will check in with Windows Update, see they need some patches, check their cache, and realize they already have it. I haven't verified this, but it's worth investigating. We're based in NZ and do some work out of an Australian office. We hit issues with bandwidth caps and the speed across the Tasman all the time. 

The swap file is not simply a buffer to run into when you run out of physical RAM. That is an over-simplistic view of how the swap file operates. That said, you can operate without a swap file, as long as you're 100% confident that you'll never exceed your RAM's capacity, at any point in time. Because if you do, your system crashes. IMO, better to keep the page file and also keep tabs on how often it's being used by the system. If you're never thrashing, it's not a problem, and if you are thrashing, you have an issue that'd probably be even more serious if you weren't running a page file at all. It's true that we have more RAM now, and it's cheap. But that's also true of disk space. Unless you have a compelling disk space constraint, I'd recommend sticking with a decent size swap file. If you feel things are being paged to disk too often, you can look at tweaking your swappiness to be less aggressive. 

You can set ODBC sources by either System or User. Check the ODBC connection definitions under Control Panel -> Administrative Tools -> ODBC. You'll see seperate tabs for 'User' and 'System' DSNs. You can check out a users ODBC connections in their registry hive under SOFTWARE\ODBC 

If it's not been done already, load the x32 2003 drivers directly onto the terminal server. It will then pick that driver up and use it when printing for clients. 

There's 2 main components to templates, if you're running vSphere (ESX and vCenter). The template Vms themselves, and Guest Customization. If you configure guest customization (just by coping sysprep for each OS onto the vcenter), all the windows deployment steps (naming, network, license, time zone, owner info) is taken care of for you. So once that's running, all you have to do with the templates is provide a base OS configuration, patched, with a temporary network name. If you have 'standard build' apps that you must layer on your builds (e.g. monitoring agents, wallpaper) then you can do this to the template VM before marking it as a template. Beyond that, I don't think there's too much more to it, everything else you could add on is at your discretion. I find that keeping a relatively small number of templates/guest customizations, usually one per OS, is sufficient and saves a chunk of deployment time. 

Ultimately you can target a tiered storage solution that stacks up like this, in order fastest to slowest: - RAM - Pagefile - SSD - Fast SATA/SAS stripe - Bulk Disk array 

Disk throughput and latency are not often an issue on file servers serving up mapped-drive data (documents & images). It's fairly easy to scale a file server's disks up to accommodate more concurrent users (by adding disks to an array, using junction points to add additional arrays, splitting shares), and scale them out across multiple file servers (with DFS, junctions and a few other tricks). Usually you hit a limit in network congestion or latency before you hit a serious problem with disk throughput and latency. However, there are much faster use-cases for enterprise storage which are already around and have been in use for many years now. These are iSCSI and Fiber-attached Storage Arrays, and are capable of serving data for much faster usage than is needed for simple file servers. Typically, they are used to concurrently serve data on a dedicated storage network to multiple servers concurrently. Those servers could be fulfilling nearly any function, such as email, file & print, web, database or application hosting. The impact that SSDs are having on these systems is increased availability of automated Tiered Storage capabilities, even in relatively low-end storage appliances. Administrators will usually purchase a few SSDs and add them as the highest storage tier, effectively utilizing them as a large cache. 

Seperation of roles is an important concept to apply here. If you have a network share that is hosting user documents, you can apply certain backup, availability and security approaches to that share depending on things like the kinds of files it hosts, and what times during the day users may need to access it. You also know that if you have to take the server that hosts it offline during the day for maintenance, you just need to notify your users to close any open documents for the duration. Now consider that you've added some software into that file share, that users run directly from the share. Suddenly, you're backing up program data alongside user file data. You now have extra complications if you need to take the server down for maintenance (what happens if the server goes offline while the app is running?). This is one example of many, where your needs for administering the program and your needs for administering the rest of the file share may different conflict. You can't always predict these kinds of conflits ahead of time, but it's one of those things that more often than not leads to administrative headaches. So this is why you should seperate functions and roles out wherever feasible, if their characteristics are different, or ever likely to diverge in the future. It's more elegant and supportable, with less nasty surprises. For a real-world example: At a company I worked at previously, we had a general file & print server, and we ran Lotus Notes/Domino for groupware. The Lotus Notes installation for all users was hosted off a file share on the file & print server, and run directly from it. I believe this was originally done with the intention of being able to upgrade Notes once, and have all clients automatically update. Maybe at one point this worked. The reality of the situation though was that a single network blip, maybe once a week, would kick every Notes user out of email, and generate a lock file on the share which would need to be manually deleted by an admin. People really notice when 'email is down'. The software loaded slowly also, especially first thing in the morning when you had 150 users concurrently trying to load off a single .exe. To top it all off, Notes upgrades still required visiting each PC. The net benefit ended up being zero or negative, although I presume it looked like a great way to save some time, originally. As to your specific issue... what are you actually trying to achieve by doing this? If your .exe is one that's being created in-house and updated often, and your devs just want a quicker way of publishing their updates.. be careful. Swapping out an EXE while users are still accessing it can cause headaches and data inconsistencies. Also, loading apps on a terminal server should be done in a particular manner, using the set user /install command on the TS before installing, then set user /execute when installed and ready to run. Bypassing this process may lead to unpredictable results. 

Extract the user SMTP addresses & Names from LDAP. This is most certainly possible and you probably want to try and get it into a CSV format. You could do this with VBScript (check this out) or perhaps PowerShell (see here). Transform this into a suitable format for GMail contacts. Since GMail accepts CSV, you might be able to do this with no or minimal alteration to your export data. Here's the skinny on GMail's acceptable CSV format. Load your data into GMail.