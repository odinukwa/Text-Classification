Your "social reading platform" looks like what HTML and the WWW=World-Wide-Web was supposed to be when Tim Berners-Lee first set up a web-server and web-browser platform applications at CERN on a NeXT machine using Objective-C (I think he programmed it in objective C). Researchers were supposed to have their web pages listing and highligthing their research with hyperlinks pointing to the publications and datasets. If you look at the majority of academic webpages, the pulication and research interests are listed in that way. It's just that the majority of the internet world has gone into walled gardens such as the social media pages, with the cost of entry usually being the loss of any privacy or control over what can be done with your user-provided content. Look at the issues discussed on the meta website here at mathoverflow about why there hasn't been an upgrade to the StackExchange 2.0 software. Wiki pages (not just that encyclopedic site that everyone uses, but a wiki page and wiki server which you can set up for yourself) allow for multiple users to modify a text using html or internal markup language. The requirement that $\LaTeX$ be usable in the markup language may require the use of MathML, or MathJax, or the jsMath package. I think the correct answer is most likely an internal wiki server, with password-accessed accounts for modifying the wiki-pages. The problem is going to lie in placing a full copy of possibly copy-righted material, particularly in the case of wanting to do an "annotated version" of a research paper, or of a book chapter. If the author of a particular paper or book chapter, or the full book itself, wanted to do the experiment and set up their own wiki for the paper or book, and allow either free-for-all access or password-required gateway granted access to allow modifications and annotations, I would be very interested in taking part in that collaborative effort. 

This is not the correct use of Elo. Elo requires something with a transitive property that holds over the ordinality of the elements of the set. A subjective rating such as ranking or ordering of images (based on what? beauty, attractiveness, hi-resolution, high percentage of high-frequency components, balanced color, etc.) will have great variability due to individual differences and preferences. N/A, since the Elo rating system will not help you in this case. 

Simply represent each vector as a binary integer: $v_0 = "0 0 0 ... 0 0 0" = b_{n-1} b_{n-2}... b_2 b_1 b_0 = 0$ so the binary digit sequence representing $i$ is a decimal number $d$ $$d = \sum_{j=0}^{j=n-1} b_j \cdot 2^{j}$$ Given such a binary representation, you can transform the binary digits into the coefficients by mapping $0 \to -1$ and $1 \to 1$ $b_j = 0 \to r_j = (-1)$ and $b_j = 1 \to r_j = (+1)$ Then, given a $v_j$ as a binary digit, generate $v_{j+1}$ by adding $1$ to the binary representation. There is no shortcut around having to test each of the $2^n$ possibilities, so this does not make your overall calculations faster. It just makes it easier to generate the binary digits. A simpler way is to iterate your index as an integer (call it $z$) from $0$ to $n-1$ and generate the binary representation of each $z$. There are many ways to do that quickly which you can find with simple searches on the internet or by asking at stackexchange.com 

My conjecture would be that such a parametrization would not work. Try something similar for a simpler (in certain viewpoints) structure such as a Koch snowflake. Would your approach to parametrization allow you to generate a function based on $n$, the number of recursive iterations used to generate the snowflake to a certain depth? I would think not. You might be able to, at least for the Koch curve, parametrize the "rubber band" hull around it, but that would be trivial for most recursively defined objects. 

I've found that the best books are the ones that make me pause when I read a paragraph or point because I suddenly feel that I've understood something well or that I've suddenly slipped gears and am bogged down. At that point, I tend to walk away from the book towards a stack of blank pages and work out the problem as well as I understand it at that point. Either I conquer my mistake and return to the book, or I find another book or example which helps me. This is particularly true of my Differential Equations book from my undergraduate course, and I remember stopping at the catenary problem and marveling at the simple and elegant way of looking at it provided by differential equations. The other direction which helps me is in having (i) a problem to solve or (ii) a question to answer already in mind. I ended up rereading a book on Computation theory and Finite Automata because I wanted to refine the state space of a particular algorithm. Having a question in mind helped me re-state the examples in the book in terms of what I found myself interested in. Summary: 

Similarly, every numerical simulation in physics (or chemistry, biology, physiology, or medicine) always has to use finite precision representation of values, such that there is a limit to the largest and smallest integer represented by a fixed number of bits, and such that there is a limited amount of "floating-point-precision" available in dividing the bits of a floating-point representation of a real number into a set number of bits for the mantissa and a set number of bits for the exponent. For example, assuming that $d=64$-bits are used to represent "real numbers" as floating point numbers in computations, $m=48$ bits may be allocated to the mantissa, allowing the numerator to be $2^{48}$ yielding approximately $14$ digits of base-ten specificity to the numerator; this leaves $d-m=16$ bits to the exponent which may be signed (+/-) yielding a range of -32768 to +32767. In this case, the floating point number is in the range $n\times 2^{d-m}$, where $-(2^{47} \le n \le +(2^{47}-1)$, and ${-32768} \le d \le {32768}$. If the total number of bits is $d$, the number of bits allocated to the exponent, $m$, may be decreased while simultaneously increasing the number of bits, $d-m$, allocated to the mantissa, increasing the "precision" of the numerator while decreasing the range over $\mathbb{R}$ spanned by this particular approximating set of {0,1}$^m \times ${0,1}$^{d-m}$ (which is not equivalent to $\mathbb{Q}$, as I erroneously stated originally) Thus every numerical simulation is already, in a way, based on $\mathbb{Q}^d$ when models of $d$-dimensional systems are created and iterated using Euler or Runge-Kutta of whatever order. 

In 1-dimensions, the probability distribution at time step $t$ are the convolutions of $[0.5, 0.0, 0.5]$ with itself, and the envelope is the region of this resultant convolution where the probabilities are non-zero. It's also equivalent to the Binomial expansion, or every other row of pascal's triangle divided by the sum of the elements of that row: 

edit 1 $\to$ 2 ** This might or might not be the answer... (but I believe the answer is **no. to your Q1) : 

Whether you use $sin$ or $cos$ matters, as sine is an odd function because $sin(-\theta)=-sin(\theta)$, whereas cosine is an even function because $cos(-\theta)= cos(\theta)$. If you draw a parametrized lissajous curve with $x=cos(nt), y=cos(mt)$, with n=2, m=5, you will get two intersections as I remarked in the comments above. If you draw a parametric lissajous curve defined as $x=sin(nt), y=sin(mt)$ with n=2, m=5, you will get the 13 points of intersection that Will Jagy got above. Your question posits a parametric Lissajous curve with an even $cosine$ function. The website you point to draws out the example curves with an odd $sine$ function. You should clarify exactly what you mean in your question. Otherwise, the points I made in the comments above should help. Consider the intersections occuring at the simultaneous constraints for multiple values of $\tau_k$, $k$ varying from $1$ to the number of intersection points: 

For your specific example, starting at one of the points on the equilateral triangle $abc$ composed of the segments between pairs of the intersection points of the lines $A$, $B$, and $C$, with no lines draws other than the three initial lines, the three lines divide the $\mathbb{R}^2$ space into 7 regions, the center of the triangle, the three regions outside the triangle touching the line segments $ab$ (defined on line $C$), $ac$ (defined on line $B$), $bc$ (defined on line $A$), and the regions extending out from points $a$,$b$,$c$ away from the triangle and not touching the line segments. I am working under the impression that you have random direction defined as a uniform distribution over the possible angles to go in, say $0 \le \theta \lt 2\pi$. This allows you to define the direction you're going in with one random variable and still defines the line as the extension in the positive and negative directions. Picking a line and then picking "left" or "right" requires calculating two pseudo-random numbers, numerically using up twice as much computation time with each step of the simulation. It also leads to ambiguity in defining "left" or "right" clearly for all possible orientations of the line. I also assume that even though the probability of picking the exact same angle of the line which has been intersected is zero in mathematical terms, it is non-zero because of the level of numerical precision used in computations of the simulation. If the new line's angle is the same as the current line's angle, I assume that the next point is the same as the current point (rather than picking a random distance along the already exisiting line, the intersection occurs at all points along the line, and the starting point is technically the first point of intersection if you define it that way), and you pick another new angle. If you start at a point on one of the line segments of the triangle (rather than on one of the points of the triangle), the initial random line has a slightly greater than a $1/6$ change of escaping to infinity, and a slightly lower than $5/6$ chance of intersecting one of the lines. For example, starting on the line segment $ab$ on the right side of the triangle (along line $C$ and on the triangle, but not at point $a$ or point $b$), the only way to escape to infinity is if $0 \le \theta \le \pi /3$, and the ways to intersect a line occur for $\theta \gt \pi /3$. Similar statements can be made for each open region on the outside of the polytope created by the intersections of the lines. If you start at one of the end-points of the equilateral triangle, then each point allows three open outward facing regions in which a ray could escape to infinity, and a line below it effective blocking the $\pi$ radians of angular directions which can intersect a ray. For example, starting at $P_0$ as you've defined it (point $a$ as I've labeled it), the union of the three outward facing regions (each of $\pi/3$ in angular extent) is available as the escape region, while the region of $\pi/2 \lt \theta $ leads to an intersection on the line $A$. The three escape regions sum up to an angular region $0 \lt \theta \lt pi/2$ (excluding the angles $\theta=\pi / 3$ and $\theta=2\pi / 3$ of angular extent $\pi$. Since equal angular extents are available for escape and for intersection, the first step has $p=1/2$ probability of escape and $p=1/2$ probability of intersection. The first point of intersection $P_1$ (if it exists) is required to be on line $A$, with probability $1/3$ of being on line segment $bc$, and probability of $2/3$ of being on line $A$ outside of the line segment $bc$. Each new point of intersection will either be