It seems like a contradiction, but perhaps sign language is parole? (Even if sign language isn't as arbitrary as signs in language) 

Look, I understand the basics of phonetics and phonology. Phonetics is a physical science and phonology is a psychological science, sort of. Nonetheless, they both treat the same object: linguistic sounds. Is it possible to illustrate this interface to a naive and dimwitted undergrad? 

What's the explanatory value of metrical trees used to account for prominence relations or syllable stress? At first reflection, it seems to me like rules should be sufficient (indeed, rules and trees coexist in most accounts) - or are there some particular phonological phenomena that the trees are very good at modeling? 

The acoustic theory of speech production as worked out by Gunnar Fant depends on a correspondence between the vocal tract and elementary electrical circuits. But the quote below perplexes me. In what way could electrical circuit theory be informative? 

Linguists use the term "formant" to refer to refer to those frequency components that e.g. distinguish one vowel from another. Typically, we refer to F1, F2, F3, and F4 and their role in determining the articulatory mechanism used to produce a phoneme or phonemes. Obviously, there are no phonemes in instrumental music. In what sense, then, if any, is the concept of "formant" comparable? 

I'm curious about the history of the science of phonetics and looking to learn more about how technological innovation shaped the development of the science. Thanks a lot. 

Aymara also lacks gender in the third person pronouns. Both 'she' and 'he' are expressed with the word jupa. A sentence like 'talk to him' is given morphologically: talk-2subj.3obj. Stress the gender of the third person in this sentence can only be achieved by adding an additional noun to the sentence (inflected in the dative), e.g. 'man', 'brother', 'boy'. 

I’m sometimes confused about representations of speech sounds. Even if I know a spectrum is 2D and a spectrogram is 3D, and the axes are different, I often struggle to figure our what I’m looking at. It’d be great to find a couple images e.g. of CV syllables or even just phonemes as a spectrum and spectrogram, to better understand in my own mind how they are different and could be “read”. Maybe some of you guys know of some good pointers or instructive examples. 

I know that the only speech phenomena one can see in a speech signal are those that arise from articulator movement and which have features that aid perception (like energy as particular freqs+times and a/periodicity of a waveform). So, specifically, I guess you can "see" vowels, fricatives and stops in the waveform (even if you don't know what language it is). Is this accurate? Are there are other speech sounds you can see in the waveform even if you don't know the language which is spoken? 

The subdomains of discourse analysis, dialogics (Bakhtin), and the study of narrative discourse markers (Labov & Waletzky) may be of interest. There is significant literature on this topic, so you can start by googling some of the concepts above. 

I am doing a simple graduate-student research project with a colleague in the CompSci Department. We want to develop a program that automatically distinguishes a man's voice from a woman's. We combine linguistic know-how and good ol' fashioned machine learning. To these ends, we have identified a number of voice cues relating to e.g. f0 range. We tested a few people and get pretty good (but not perfect) results. My question is not about what parameters will further improve our results (I have some ideas already in that direction) but about a hypothetical: Let's say we continue to refine the algorithm, and we test it on 10 people and it performs perfectly. Then 20 people. Also: perfectly. At what point can we say that the device is "validated"? What's the standard accepted in the literature? Is there a standard metric used? We consider the data we have now as a training and a validation set so the size of this probably also depends on the number of features and variability in the dataset. Thanks a lot! 

In essence, I just wonder what the modifier "natural" means in front of "class". Is there a distinction between a "class" and a "natural class"? 

a "standard feature matrix" with phonetic features another feature matrix with diacritic features (which identify word classes among other things). 

I'm writing a cross-dialectical description with three distinct dialects, and would like to indicate with a superscript symbol which dialect a given word/phrase comes from to aid the reader. I thought of raised *s and daggers, but that looks strange. Raised numbers seem to indicate footnotes. Have you ever come across an intuitive solution to this in text? 

But obviously this is a "Linguistics 101" sort of understanding. The situation is probably somewhat more complex (e.g. Stress, rhythm, intonation, melody, and so on). But aside from the above-issues, is this an adequate and serious "working understanding" or does it miss some key points known best to linguists? 

I understan that with modal phonation the glottal wave has a triangular shape, whereas with breathy voice, the source wave is closer to a sine wave. This means that the higher harmonics are more prominent. Are there any other crucial differences between these glottal airflow functions? Forgive me my elementary understanding of articulatory phonetics! 

While this is a tremendous explanation of questions which were, at the time, crucial to understanding feature matrices, I have trouble understanding the logical reasoning that lead to the conclusion in bold at the end. Why is it the case, that in this framework, a segment must be interpreted as a collection of value-attribute pairs? 

Are there languages in which lexical tone can associate to semivowels or glottal stops, or does tone ALWAYS associate only to vowels when it is realized in a spoken word? 

In my search for Master's programs, I'm looking to work on speech synthesis or speech recognition. I wonder what sub-discipline of linguistics I should be searching in. My first guess was Computational Linguistics, but that sub-discipline seems mainly dedicated to (written) text analysis, automatic translation, and the like. How can I find a Master's program on these and similar topics? Is there a particular sub-discipline that is dedicated to this study? Thanks! 

I understand the distinction between phonemes and allophones, but why are phonemes considered as mental objects and not psychological objects? Isn’t everything mental also, in a sense, necessarily psychological? I wondered if there is a point to splitting mental from psychological stuff. 

I'm confused by what is meant by a "generation path". Does this have to do with articulation or something else? 

I'd be grateful for any information that could shed light on how these two disciplines connect. Thanks a lot. 

How was phonetics performed with a phonograph? From the Wikipedia excerpt below, I guess it went something like this: 

there's a well-known diagram made proposed by Fant (see image, passive perception is ABCDE and passive perception is ABCKFE) but it a quite anatomical and neurological account. (And I'm not even sure what the arrows represent, specifically arrow A!). Are there generally-accepted alternatives to representing the difference between the two kinds of speech perception? 

However, Leben suggests still another kind of feature matrix, which had features for e.g. Noun/Verb, and Past/Fut and other prosodic stuff (tone etc.) and godknowswhatelse. Is Leben's feature matrix distinct from the second kind of feature matrix above? To me they sound to be basically the same or anyway very similar. I'm not sure why he posited it. 

Infants and some mammals can apparently perceive VOT continuum categorically, which I guess is evidence for natural auditory sensitivities, and can explain, at least in part, the mechanisms through which phonemes are perceived categorically. But, even if I have an inkling of the impact, one thing I don't really understand after reading these and similar articles is what is meant by "perceiving the VOT continuum". What does it mean to perceive a continuum? 

To be specific, I am analyzing a phonological process like this /maNioN/ --> [mãn.i.õn] --> [mã.ni.õn] --> [mã.nyõ?] These intermediary forms make sense according to the phonology of the language. Does the transcription make sense, i.e. having intermediary forms between brackets ? Is it even correct to have intermediary forms? I guess not in OT, but maybe it's tolerated in a rule-based approach, or is it? Thanks! 

The student left, apparently happy, but I don't feel like I adequately explained the phenomena. Maybe some sort of visualization would help. Is there a graphical representation that exemplifies the different between stressed-timed and syllable-timed languages? 

One example may be the combination of Chinese characters with pinyin and Zhùyīn fúhào 注音符號 alphabet/syllabary, respectively -- although it is not completely symmetrical to hiragana and katakana 

How can I distinguish between elipsis type 1, 2 and 3, below? Type 1 and type 2 "nice day" and "sleeping dog" are both NPs. Type 3 "very sexy" in an AdjP. 

Of course stress is manifest in a variety of ways, not just increased f0, and f0 indicates much more than just stress - but is it accurate that stress always entails at least increased f0? 

It then goes on to describe those technologies, as well as a separate section on "HMM-synthesis" almost as an afterthought: 

Many languages have epenthesis and deletion, but obviously for very different reasons. Is there an OT overview with an account of the most common epenthesis and deletion processes? That way when I work on an OT account of a language with either of those processes, i can refer to comparable accounts given for other languages, to help guide and inform my analysis. 

As part of forensic linguistics practice, I must assess the relative age of multiple individuals based only voice. One crucial variable is VOT, which is known to decrease with age. So, let's say hypothetically that I classify the individuals into two groups: one with a longer VOT than the other. However, I also want to do some statistical analysis to make the nuance of my results clearer by the jury. What statistical techniques are optimal to these ends, in the case of (non)-normally distrubted data - how will normality be characterized? ANOVA, T, Kruskal-Wallis, etc. 

But I don't understand the explanatory value of this "gestural score". How does it support the claims of autosegmental phonology (or deny the segment)? I'd be grateful for any insight on this! 

I wonder if sign language would be "parole" according to Saussurian linguistics. After all, parole can be denied as 

Would appreciate any insight into brand names/types and/or additional things y'all recommend. Thanks a lot. 

The phoneme is only in the mind of the speaker /s/ The phonological segment is that which the speaker articulates [s] What is it that the speaker "hears"? Is there a name for this unit? It isn't spoken, so it can 't be the phonetic sequence, but it also isn't necessarily the "same" thing as the /s/ in the speaker's mind, so it can't be a phoneme. Is this an allophone?? 

Some parameters of speech analysis occur in the temporal domain (those are the ones I typically understand: f0, formants, etc.) - but others occur in the so-called cepstral domain. If I google "cepstral domain" or "cepstrum", I see mostly mathematical stuff. Not what I'd call intelligible. Perhaps one of you has some a handy heuristic to understand how the cepstral domain differs from the temporal one from a speech perspective. Thanks. 

I'm working as a forensic linguist at an upcoming trial. No details to share, but I need something highly professional. Praat is obviously a fine speech analysis program (and I know it well from my university days) -- but is that the program commonly used in industry? What other, more professional options are out there? 

Is it accurate to claim that the underlying level is (only) characterizable by a phonemic representaiton of a word and the surface level is (only) characterizable by a phonetic representation of a word? 

I'm a little unclear on what autosegmental phonology is vis-a-vis suprasegmentals. Although I understand the nuts-and-bolts of autosegmental phonology, it is not clear to me if it negates a division between segmentals and suprasegmentals. So far as I read, autosegmental phonological descriptions do not make reference to suprasegmentals... 

I read* that one reason that f0 varies over time owes to the quasi-periodic nature of vocal chord vibrations. But what does f0 variation over time have to do with the quasi-periodicity of the signal? 

I can name a few: 1. Tones as numbers 2. Intonation contour as a line above the sentence 3. Tones as lines above segments 4. Stress marks before stress syllables ['white house] vs [white 'house] But can you think of any others, common/accepted or esoteric and unused, it doesn't matter. I'm just looking for a general overview of symbols. My reason for asking is because I'm interested in how the representation of features can influence analysis. How were symbols for prosodic features different before linguistics became a science, for ex? Thanks! 

Altho a feature can be modeled as unordered value-attribute pairs (+cons, -syll, etc.), The matrices in which the features are attested must be ordered, correct? After all, The three matrices that comprise the word "sick" [sIk] are the same three matrices that comprise the word "kiss" [kIs]. Just the ordering is different. So it is accurate to reason as follows: 

According to Wikipedia, bandpass filters were used before the digital era began. Can anyone explain to me how they appeared or were analyzed? 

In the book Concepts, Ontologies, and Knowledge Representation, the author makes a distinction between syntagmatic and paradigmatic semantic relations. That's clear enough - but then he raises a third kind of semantic relation called attributes (see attachment). 

In a number of textbooks, and on the wikipedia page for natural classes, I see that "class" and "natural class" are used interchangeably, see excerpt below (with key words in bold) 

Our university is making a crash coarse in phonology for first year students so, while there is a dedicated phono module, there's also this streamlined overview of phonological theory. My job is to present on autosegmental phonology. Theoretically, no problem -- but most of the examples that illustrate the use of autosegmental approaches have heavy theoretical burdens. African tone, for example, may be a bit too complicated. Are there some good, visual-friendly, examples of how autosegmental phonology provides compelling explanatory power? Perhaps some harmonic process...? 

In what way are these different models of distinctive features more or less adequate in terms of language description? At least from my perspective, most schools of linguistics have converged on Chomsky & Halle's mixed approach. Does this come at a theoretical price? 

Reading over my class notes, I see that you can consider a spectrogram is being comprised of many "slices" (static spectra? Whatever that means) horizontally, making it so "time appears continuous". This sounds very strange to me, and I can't get my brains around it. I don't think I understand what "slices" are or what it means for "time to seem continuous" in the spectrogram itself. Can someone help me come to terms with this? 

I don't understand how to compare these three kinds of semantic relations. Attribute relations seem so unlike the paradigmatic/syntagmatic distinction. Furthermore, I also learned that semantic relations have an arity, i.e. the number of concepts a semantic relation can associate. But this doesn't connect to any of the aforementioned three semantic relations, or does it? Any insight would be appreciated! 

How do I recognize and distinguish non linguistic inhaling and exhaling in the waveform or spectrogram? I can hear it in the signal but I want to write a script that recognizes inhalation and exhalation candidates semiautomatically. Any advice appreciated! 

So I know that F0 need not be present for pitch to perceived (at least for men!) because the telephone filters out frequencies <300Hz (at least in the 90s, maybe there have been innovations in the filtering technique). So how is F0 expressed over the phone? 

As each language can be said to have an "inventory" of pitch movements which are felicitous (?) or anyway possible, I wonder what sequences of pitch movements characterize English (but not e.g. French or vice-versa). 

This is the study of "event co-reference" and is something of a challenge for computational linguists who may want to associate different texts describing aspects or perspectives of the "same" event. 

I just noticed, perhaps naively, that the representation of amplitude on the y-axis of a waveform is somewhat paradoxical. Although the space between each value on the y-axis is identical, the units of amplitude are a logarithmic representation of the intensity of the sound. I'm no acoustician, so maybe this is just "how it's done", but doesn't this influence perhaps in ways we just take for granted how we analyze a waveform?