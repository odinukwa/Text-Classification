INT (BIGINT) always will be smaller than CHAR(36) - as result less memory usage, more data in memory for index recreate operations (such as massive insert) Increment will be faster any other form of random generator in 99% of cases So in case of TAXNUMBER as primary key: 

You can not access data from this file from mysql inside You can open it manually by editor, or with specialised tools like - pt-query-digest If You want have this data in table, You need enable writing to table - $URL$ 

2 sec it is not significant huge time, but if it make collisions for other operations - You can to split operation for chunks 

It is like - what zip archiver ratio? Is it same for txt and jpg? some more information there - $URL$ 

it give You idea - which index better to use in this case without information about data, general ideas: - index for created_timestamp - good candidate You can or create index separate for created_timestamp only, or create index for 3 columns: core_id, hit_status_code, created_timestamp - columns in index must be in same order as used in query and last SELECT * not give ideas about data size, even You request 25 records, but before server must sort records by DESC 

Step#1 - stored procedure, which will run as many queries as You need - $URL$ Step#2 - Native MySQL Events - which will call this stored procedure - $URL$ 

p1 - if You plan rollback exactly for 5.6.30, p2 - if You plan go till the win, best to have both. then You must have old installer for revert back at any time, You can download it from official archive - $URL$ Then You if You decide revert back: 

I have a question about this query plan. We have a table in a test environment, Order_Details_Taxes, that has 11,225,799 rows. This table has a column, OrdTax_PLTax_LoadDtl_Key, which is NULL on every single row. This test environment is configured in such a way that this column will always be NULL. There is an index on this column. I ran some queries against this table using a NULL value for a column. A NULL INNER JOIN will never yield any results. 

My question is: 1) Why is the timestamp not always increasing with concurrent inserts? Bonus points if you can answer this question: 2) Why are the concurrent inserts overlapping the primary key instead of all being inserted at once? Each insert is running its own implicit transaction, so I expected the primary keys to be in order for a single thread's insert. I did not expect the primary keys to be interleaved. I don't know enough about replication to answer this one: 3) Do having timestamps out of order cause a problem with replication? In the above example, what if thread 2 commits its data first? When thread 1 completes, its timestamps are all lower than the records inserted by thread 2. I peeked at the executing requests and verified they are not running parallel, so I don't think parallelism is the problem. Note that this query was running in the default (READ COMMITTED) isolation level. If I increase the isolation level to SERIALIZABLE, I still get timestamps in reverse order when threads change. I am testing this on SQL Server 2008 R2. To check the timestamp orders, I was doing a , and I was also using the following queries: 

but You have only one true choice - start change code for proper form because if in case of hidden type conversion, You are just slowdown server (sometime dramatically), in case of suppress 

this query return data, but what the reason of this query if same result You can achieve without un-necessary JOINs? 

look like You try to run mysql(?d) from inside mysql console this command must be run from OS command line: 

remove DATE function from timestamp, in more and add index to timestamp column Insert also very slow, but it could be next tasks The main idea of changes: You have only 1 column in this query, which can reduce number of rows, and You not use this column. DATE() == full table scan. So we must use any possible chance for narrow filter conditions, and any chance for use indexes for query 

than after this look at loading again high cpu loading normally come from a lot of sorting operations 

with any new version of MySQL, MySQL more and more "SQL" Yes, MySQL allow many hints, which most of other databases reject with errors. it is and: 

and lookup for note_id Add example for Your question - why with similar plan FORCED INDEX query work better for bigger collection: 

CHECKSUM TABLE not warrant same checksum between MySQL versions, it always present as notice to this function in docs use text diff tools over dumps - it also not a best choice, The first and quick tests (very quick for understanding direction of feature search): 

Other possible reason - number of rows processed before send to client Query which return just 100 rows, could sort millions before calculate result. If You already identify this 4-5 queries - just go thru each and check: 

I wanted to share my experience with trace flag 4199. I just finished diagnosing a performance issue on a customer system running SQL Server 2012 SP3. The customer was moving a reporting database away from their production OLTP server onto a new server. The customer's goal was to remove competition for resources with the OLTP queries. Unfortunately, the customer said the new reporting server was very slow. A sample query run on the OLTP system completed in 1.6 seconds. The query plan did an index seek on a ~200 million row table that was part of a view. On the new server, the same query completed in 10 minutes 44 seconds. It performed an index scan on the same ~200 million row table. The reporting server data was a copy of the OLTP data, so it did not appear to be a difference in the data. I was stumped until I recalled that our software (which runs their OLTP system) enabled some trace flags on startup. One of them, 4199, I recalled was a query optimizer fix. I tested enabling trace flag 4199 on the customer's new reporting server, and the reporting query completed in 0.6 seconds. (Wow!) I disabled the trace flag, and the query was back to completing in 10 min 44 sec. Enabled the flag: back to 0.6 seconds. Apparently, enabling the trace flag enabled the optimizer to use an index seek into the view on the 200 million row table. In this case, the query optimizations enabled by trace flag 4199 made an enormous difference. Your experiences may vary. However, based on this experience, it definitely seems worth enabling to me. 

So far I have not been able to capture the query plan used at the time of the deadlock, and the normal ways I try to retrieve the query plan are not returning anything(sys.dm_exec_query_plan and sys.dm_exec_text_query_plan both return NULL). UPDATE 2013-08-29 The customer installed SQL Server 2005 SP 4, but they are still seeing this deadlock. I will pursue removing the deprecated (NOLOCK) on the tables being modified and see if this fixes the deadlocks. 

Possible or not increase speed of insert? it depends from Your current server settings and configuration - may yes, may be not Also it depend from other server loading: 

This topic is very interesting for me now, so because it Up, some last information: Replication is good, but what if You need replicate data to different technology Target System (for example Redshift) I found and test few implementations for change data capture: 1) MariaDB MaxScale CDC - my personal test was unsuccessful (and builded from source and binary distribution return link for missed plugin). Ask a questions, wait for answer. 2) StreamSets - simulate slave, ship changes to Kafka MQ. Tested on real loading, work perfect 3) Yelp 4) Maxwell Daemon - under test now, example with NiFi - NiFI examples 5) Python Library - also simulate a slave for remote server 6) Lapidus - NodeJS implementations Few other links also founded, some under active maintenance, some look like forgotten. From the tests, StreamSets look very good, Maxwell also very interesting MaxScale - sure it work, but may be available only under subscription support. From "other" side - Talend as primary data integration tools, Redshift, memSQL. 

"Standard" way - analyse indexes, table sizes and etc (Rick James already ask proper questions) But I want suggest other way: 

The biggest issue in above is "etc". Any editing operation on Slave can crash the replication process, and Your head of IT just absolutely correct want avoid regular restore of server. The possible ways: 

partitioning - not help from bad queries and can make situation more bad split operations - write on master, all read (especial users) - on slave on master optimise structure for insert update - only necessary indexes, which really need for improve updates On slave(s) - indexes for select operations query must reduce number of rows as much as possible - depend from logic, but it user_id, date, location and etc - need analyse queries and indexes cardinality then good idea to ask - for what reason in table with very simple structure: (phone_number,start_time,stop_time,time_lasting,fare)