After running this setup in production for 6 months, I can say there does NOT seem to be any stability issues with the VMs running on a SR serviced from DRBD in a VM. The biggest issue is you have two "hosts" to worry about that will affect all other VMs, Dom0 and the DRBD server effectively adding a second point of failure software wise (configuration errors, administration errors, bugs, etc). However this has not proved to be an issue thus far. I have not ran any comparison benchmark for performance of VMs on and off DRBD although we have not had any noticable performance issues and the majority of data served is not served by DRBD; the SR on DRBD only host the VM host disks. TL;DR We had some extra RAM available on the HOST so I did set the DRBD server to use all remaining available RAM so it caches the data it serves. Bringing up the secondary server is relatively easy. It involves setting the backup servers DRBDs service to primary, mounting the DRBD drive, then removing the primary servers SR and re-adding it on the secondary server from the SR on the DRBD drive, then utilizing XenServers builtin backup and restore to reassociate the metadata to the virtual disks. This means the VMs used during an outage on the primary server are not outdated in anyway because they were actively replicated via DRBD vs a script. It is rather critical we keep the metadata up to date to make this work easily. This is used like a RAID and the VMs are still backed up in case of some other failure or corruption. 

I am looking to begin a tape backup regimen and am looking to keep data flowing to the tape drive in a sufficient manner (120+MBs target sustained) but cannot figure out how to do so without a dedicated source drive/array that idles when not writing tapes. The documentation for our specific drive mentions no minimum throughput required. Enviroment 

I have been working at setting this up in much of a way you describe and it works great! (XenServer) I setup an old but capable server as the primary host, this runs a console only VM for DRBD. This VM then serves a "SharedDRBD" SR back to the Xen Host via NFS. The rest of the working VMs providing services run on the SharedDRBD SR. The VM's DRBD dev is on its own VDI on a MDADM RAID 1. This SharedDRBD SR hosts the rest of the VMs for various services with a local larger RAID10 array for bulk filestorage. All MDADM work is done by the host, but one side of the DRBD is in a VM. The DRBD ran in a VM gets synced with a DRBD service running on the file backup server; the file backup server is NOT virtualized purposefully so we have bare metal access to all files given XenServer is the biggest quirk we generally deal with. There is a secondary server that is virtualized but has no local storage except for what is required for the host. This server is part of a Xen pool with the primary server to simplify failover. Failover is currently manual but fast and easy. First, all VMs on the SharedDRBD SR are shutdown while the secondary XenServer host is powered on. The DRBD on the file backup server is made primary and mounted as needed. Then, the SharedDRBD SR is pointed to the file backup server and VMs are started up on the secondary server; XenCenter doesn't even realize it is serving the VMs from a new location because it sees the same SR with the same data. The VMs are fired back up and things are back and running. There is alot more to it in terms of configuration, and arrays, network topology, etc; but the jist is DRBD is served in a VM back to its own host. Overall it is HA enough for our SMB / Home use; down time during a catastrophic failure of the primary server is 10-20 min or less to fully back online and no loss of data; DRBD means the VMs are up to date! Plus, outside of the primary server which is pretty robust, there is a ton of overall redundancy. Most of the primary server is redundant in-and-of-it-self so it pretty much gives us triple redundancy or better for just about every piece of hardware you can think of (PS, RAM, CPU, HDD, Controllers, NICs, etc) besides the motherboard(s) which is only double redundancy (primary/secondary Xen Hosts). And yes, XenCenter is installed on windows sadly, the rest is all Linux. I know, this Q's is 8 years old. 

Only reason this could be is if your POP3 server is retaining (or restoring) the messages. Normally once your OE has downloaded the mail, it's deleted from the server. You can configure it to leave it on the server, in which case it keeps track of the UIDL of the last downloaded message so that it knows which ones it already has, and only downloads new ones. If you have it configured this way and the UIDL is being lost, that would cause it to download all messages again. So, the first question is do you have OE configured to leave a copy of the email on the server? If you do, and it's by intention (perhaps because you want any or all three workstations to get all emails) you might want to consider using an IMAP server instead. 

I've got a book called "Windows Server 2003/2000 Terminal Server Solutions", which is reasonably good. However I bought it after we'd done a lot of work with TS, purely as a reference and because it was very cheap. But it does seem to cover a lot of the stuff we went through. ISBN is 1-578-70276-3, published by Addison-Wesley and written by Todd W Mathers. 

If it's an application that runs from the desktop (i.e. not as a service) did you try setting the application to run in compatibility mode? I had a similar problem with a VB6 program (same function to read the registry called from two different parts of the program, one works, the other does not, but from the VB6 IDE both work) and just right-clicking and having Win7 test for compatibility and choose XP SP2 mode cured the problem. 

Is the server sitting behind a firewall that only opens port 80, not 8080? That would be my first guess, but hard to know without knowing how the server is configured, where it sits etc. 

Are you trying to run Windows 2000 on hardware that's just too new for it? We upgraded a customer to a new Fujitsu quad-core Xeon server, and have similar freezes and general problems that the same machine running Windows 2008 does not exhibit. 

In a small company, giving developers a free reign over their choice of tools can cause a problem if the developer then leaves the company and no-one else has experience with that tool. There's a learning curve involved that, if the developer had used the same as everyone else, would not be required. 

(This is not a question of how to run both machines from the same set of data at the same time; they would be effectrively running from the same set of data at different times. The files the servers each need to constantly modify would be on the local file system) We simply want to setup a redundant server and minimize the redundant administration. 

Looking at using DRBD or a clustered files system to help with up-time when downtime strikes in a small business environment. We currently use a server box for a file server using Linux and samba, then running the web server and Database in a VM. Was looking at adding a second server and putting the files and the VM onto the distributed file system. The base OS is more static and easily can be managed more manually (copy config files at time of change, copy base OS if needed from full backups, etc) Question is about the fail over scenario if manually done. If server 1 goes down and fail over is manually done, is fail over completed by simply setting the static IP of server 2 to server 1 (again server 1 is down and would be in a state of needing repair), starting Samba, and starting the VM which would have the same static IP's as they had when running on server 1, and starting the backup services? This sounds like a quick and simple process, almost too simple. Am I missing something? This could easily be automated as well through a script or something that someone with little proficiency could be directed to run in the event of a failure. Down time if we have a hardware failure could easily be days without the support of on call IT support and the parts needed without a second server, but with the the second server, down time would be at the maximum a matter of hours (if no one is the office proficient enough to perform such operations, minutes if someone was) 

Given XenServer (7 currently) is based on CentOS, does that mean it works just like CentOS in terms of updating, CLI, administration (non-Xen specific like mdadm and boot loaders) etc? Basically, if I want to use XenServer, then am I committing to using, learning, and working in the CentOS "way"? We have a new (to us) server on the way and now is the time to switch hypervisors and we are set on using Xen. Our current setup that I am familiar with and can administer efficiently is a Debian host with a couple VMs using Virtual Box which is less than ideal to say the least. Due to this, I am familiar to working in Debian and have made a conscious choice to use Debian for our servers. I administer only our servers for our small business so I do not have the diversity of other setups and other distributions to work from. From my understanding, the way Redhat does things is a bit different from Debian based distributions and would require a learning curve of unknown amount; but a learning curve for sure. So if I use XenServer, am I also committing to the Redhat learning curve? I am aware that I can install Xen with a Debian based Dom0 but the consensus I have read seems to say XenServer works the best overall. However there will be a bit of configuration I will need to do such as getting our local RAID arrays up and running for the Dom0, Xen, and Network Shares, along with getting boot loaders and grub in order. I can do this configuration rather easily in Debian so I am trying to weigh the cost in time of trying to do the same configuration the CentOS way which I am afraid will add a considerable amount of time to get the new server in to Production given IT by myself for our company slowly happens in the afterhours of business; hence the question.