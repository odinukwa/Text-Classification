Rsync can be confusing about selective copies like this. I use the following to do the task that you're asking for: 

You didn't explicitly state how Subversion is served, but based on your tags, I'm assuming Apache. Using Apache 2.2 to serve Subversion with authentication by AD is pretty simple. In your Apache config, you need to add the appropriate Auth parameters to the location block for the repository. For example (restricted read and write access): 

Basically you need to include all directories in the search, then add all files to the list, the exclude all other files. The option is handy to use as it excludes any directory that doesn't have a file. 

At this point, the pre-delete and post delete verifications still show the same disk blocks in use. I should instead see a reduction in the number of in use blocks. Waiting an hour (in case it takes a while for the TRIM command to be issued) after the test file is deleted still shows the same blocks in use. I have also tried mounting with the options, but that doesn't seem to help at all. Partition that was created from above (I keep the partition small so the verification can go faster): 

What do we need to do to make the disks be recognized? Any other things that we need to try? EDIT Here is the output of : 

I recommend reading the revision number from a file rather than reading from an environment variable. There isn't an easy way to ensure that your current environment is the same a the environment that Apache is running under. Also this ensures that the revision number in the php code survives reboots, etc. You should set Subversion to ignore the svn-revision file so you aren't checking it in. 

From reading your answer it sounds like this is a client-side issue. Subversion does not support client-side hooks (from what I know) so I would recommend writing a small script similar to the following to perform your update and also update the revision number in a file. 

I recently had a similar problem on a Debian Lenny box that was set to UTC when I wanted localtime. First you need to copy (or symlink) your correct zoneinfo file from to . For example I ran on my system. Second you need to edit to reflect your timezone as well. On my system, the file states . Once both of those files are taken care of, it is a good idea to restart crond to ensure the proper timezone is picked up. 

So after many days working on this, I was able to demonstrate that BtrFS does use TRIM. I was unable to successfully have TRIM work on the server that we will be deploying these SSDs to. However, when testing using the same drive plugged into a laptop, the tests succeed. Hardware used for all of this testing: 

If the name for the reverse address does not match the forward address, it is likely that the name you see was the name assigned to your IP address when the IP was used by a different customer of your provider. If the name does match, you should talk to your provider and make sure something else isn't going on. If possible, please revise your question with some examples of what you're seeing. 

I strongly recommend against having Apache run as any real user. If an exploit is found in your sites, a malicious user can read or alter your personal files. WordPress will run fine with the files being owned by your user account and with Apache running as a different user. There are a few files and paths which you should to the same group as the Apache server and make group writable. That way Apache can make the necessary changes without the risk of a user having full access to the rest of your files. You should read the WordPress document on Changing File Permissions. 

You need to include all of the parent directories down to the desired directory before using the exclude rule. For instance, I use the following in a backup script: 

There are some advantages and disadvantages to EBS and instance store. In general, I recommend instance store over EBS for most scalable applications. Some advantages of EBS: 

Intel AMT is a motherboard feature found in certain chipsets such as Q67. So if you are looking for a superMicro board with this, then just find out which intel chipsets support Intel AMT, and then look for supermicro boards with that intel chipset. There are some various other requirements as far as what CPU you use, etc. In a nutshell the motherboard keeps one of the ram chips powered, the northbridge powered(which runs the management interface), the onboard ethernet connection, and an extra flash chip to store settings. So you can connect to the system over ethernet and access the remote managmeent features/power-on/power-off etc. I am not sure though if this is available in any chipsets targeting servers though, but it is really powerful and would seem to be well suited for servers. 

If I have a mirrored pair of 250GB drives in a pool, and I later buy two more drives and add another mirrored pair to the same pool, can that second mirrored pair be 500gb? Such that my total usable space would be 750GB? Or do all the mirrored pairs in a pool need to be the same size? 

Scoped to a specific filesystem name :No header so that first line is a snapshot name : List snapshots (list can list other things like pools and volumes) : Display the snapshot name property. : Capital denotes descending sort, based on creation time. This puts most recent snapshot as the first line. : Says include children, which seems confusing but its because as far as this command is concerned, snapshots of TestOne are children. This will NOT list snapshots of volumes within TestOne such as . : Pipe to head and only return first line. 

EveryDNS used to have this, but I switched to DynDNS because I found out my router has support for DynDNS. 

So my understanding of one scenario that ZFS addresses is where a RAID5 drive fails, and then during a rebuild it encountered some corrupt blocks of data and thus cannot restore that data. From Googling around I don't see this failure scenario demonstrated; either articles on a disk failure, or articles on healing data corruption, but not both. 1) Is ZFS using 3 drive raidz1 susceptible to this problem? I.e. if one drive is lost, replaced, and data corruption is encountered when reading/rebuilding, then there is no redundancy to repair this data. My understanding is that the corrupted data will be lost, correct? (I do understand that periodic scrubbing will minimize the risk, but lets assume some tiny amount of corruption occurred on one disk since the last scrubbing, and a different disk also failed, and thus the corruption is detected during the rebuild) 2) Does raidz2 4 drive setup protect against this scenario? 3) Does a two drive mirrored setup with copies=2 would protect against this scenario? I.e. one drive fails, but the other drive contains 2 copies of all data, so if corruption is encountered during rebuild, there is a redundant copy on that disk to restore from? It's appealing to me because it uses half as many disks as the raidz2 setup, even though I'd need larger disks. I am not committed to ZFS, but it is what I've read the most about off and on for a couple years now. It would be really nice if there were something similar to par archive/reed-solomon that generates some amount of parity that protects up to 10% data corruption and only uses an amount of space proportional to how much x% corruption protection you want. Then I'd just use a mirror setup and each disk in the mirror would contain a copy of that parity, which would be relatively small when compared to option #3 above. Unfortunately I don't think reed-solomon fits this scenario very well. I've been reading an old NASA document on implementing reed-solomon(the only comprehensive explanation I could find that didn't require buying a journal articular) and as far as I my understanding goes, the set of parity data would need to be completely regenerated for each incremental change to the source data. I.e. there's not an easy way to do incremental changes to the reed-solomon parity data in response to small incremental changes to the source data. I'm wondering though if there's something similar in concept(proportionally small amount of parity data protecting X% corruption ANYWHERE in the source data) out there that someone is aware of, but I think that's probably a pipe dream.