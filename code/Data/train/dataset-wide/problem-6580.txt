(Poor Lampwick.) This can be transformed using the definition of implication (if A then B means B or not A): 

The Japanese letter ん represents a moraic /n̩/ sound, which always follows a vowel. Finally, it sounds like [ɴ], the uvular nasal, but elsewhere, its pronunciation assimilates to the following consonant's place of articulation. Wikipedia's article about the letter states that it "is followed by an apostrophe in some systems of transliteration whenever it precedes a vowel", in which case it is realized usually like [ũ͍] or sometimes [ĩ]. That article gives no examples of words containing n'. But off the top of my head, I remembered the name of the failed Shin'en space probe, which is four morae /si-n̩-e-n̩/. A bit more digging turned up the boy's name けんいち (Ken'ichi), consisting of four morae /ke-n̩-i-ti/. Draconis pointed out another in a comment: 恋愛 (れんあい, ren'ai), meaning "romantic love", is also four morae /re-n̩-a-i/. In each case, /n̩/ is a full mora but may belong to the previous full syllable depending on how your linguistics professor defines a syllable. See also the Japanese Language Stack Exchange question Difference between んい (n'i) and に (ni). 

Who introduced the notation e̯ a̯ o̯ (vowels with inverted breve below) for Proto-Indo-European laryngeals and when? Proto-Indo-European has been reconstructed with so-called "laryngeal" consonants, spelled *h1, *h2, and *h3. These were lost in branches other than Hittite but left traces on adjacent short *e as well as compensatory lengthening in descendant languages. Based on how they color *e, it's likely that these sounds were fricatives [x], [χ], and [xʷ], corresponding to the velar, uvular (formerly "plain"), and labiovelar plosives. Occasionally when these are syllabic, I've seen them spelled with the schwa indogermanicum: *ə1, *ə2, and *ə3. But lately I've been seeing reconstructed PIE with *e, *a, and *o with half-circles under them. I'm guessing that these correspond to syllabic allophones of laryngeals *ə1, *ə2, and *ə3. Am I right? What source introduced this orthography and why? Is it aesthetic, to make PIE look more like "words" and less like mathematical formulae, for the same reason that Pinyin uses accent marks instead of tone numbers? Or is there some new insight into the reconstruction of how the laryngeals were pronounced? And how do you type these, other than copying and pasting from elsewhere? 

It's common for someone to insist on particular terminology that he considers clearer, especially when he has a point to prove. In fact, it's so common, it's become a storytelling trope. I'll give one set of examples from my native language (English). The Free Software Foundation publishes a list of numerous English words that it encourages people not to use because it considers them loaded. For example, people say "photoshopped" to mean that a photo is doctored, but FSF encourages use of generic terms (such as "manipulated") so as not to advertise a particular proprietary software product. For the same reason, it discourages "PowerPoint" and "MP3 player". Other examples include use of "piracy" and "theft" to mean infringement, use of "consumer" to mean an individual, describing copyright as "protection", describing a uniform-royalty license offer as "reasonable and non-discriminatory", describing those Linux operating systems that use GNU C Library and GNU Core Utilities as anything other than "GNU/Linux", etc. 

The only coherent meaning of grammatical/ungrammatical I know of is 'complying/not complying with the rules of grammar G for language L'. If you accept that point of view, provided you formally specify an explicit, complete, and consistent grammar G of L, you will, by definition, be able to tell whether a certain string of symbols is or is not 'grammatical' in L modulo G (= whether or not it is in the set L that can be formally deduced from G). If you agree to further identify grammatical with correct (= well-behaved modulo the rules of G and, therefore, acceptable as an object of L), then you can also automatically tell whether a sentence is or is not correct relative to G. So far, there is no problem. The problem is that we have nothing remotely approaching a formally explicit complete grammar of any natural language, even if by natural language we meant just 'the state of the internal language L assumed by speaker S at time t,... (replace "..." with a list of parameters encoding all the factors that may condition the state of speaker S's internal 'grammar' G during a specific speech act). Actually, we do not even have anything remotely approaching an explicit grammar of 'the language of speaker S' tout court, with all such parameters abstracted away for simplicity's or idealisation's sake, i.e., we do not have any explicit grammar of any individual speaker S's idealised 'idiolect', approximately what Chomsky once called 'the language of Jones'), and the bad news is that we are not likely to ever have one of those either. Needless to say, we are still incomparably further away from having any formally explicit complete grammar of 'English' (or any other language) as people normally understand the terms English, Spanish, etc., i.e., as the would-be homogeneous 'public languages' millions of people across the world 'share' and use to express themselves, which is the way I presume you understand the object 'English' and the word English. Unfortunately, having the first such grammar of a natural language in that latter sense is not just a question of time and doing much more research; it is simply an impossible enterprise, for reasons clearly explained long ago by Quine in Word and Object and by many others since (Davidson, Chomsky). We will never have an explicit, complete, and consistent grammar of 'English' in that sense because 'English' in that sense is not a scientifically coherent, definable object; it is just a practically convenient fiction, and it is impossible to develop such a grammar for it. Since, unless you first do exactly that, no automatic verification procedure can possibly be designed and made to work, either, the answer to your question, as I assume you intended it to be interpreted, is: "No, it is not possible to automatically verify whether a sentence (of 'English', or any other natural language in the sense explained) is or not 'grammatically correct'." 

Perhaps this is why natural language processing is beating researchers' asses, so to speak: one has to capture the stereotypes associated with actors in a sentence in order to resolve this ambiguity. 

"Would it make it harder to recognize where a word stops and where the next starts?" This is the problem of segmentation of an utterance into morphemes, the difference between "experts' exchange" and "expert sex change". Languages have a couple strategies to solve this, which apply whether or not a language has CV syllable structure. One strategy is to adopt predictable stress patterns. In Toki Pona, for instance, the first syllable of a word is always stressed, and the modifier in the phrase ends up carrying the phrase's primary stress. For example the phrase jan pona, which means "friend", behaves as a compound noun (/ˌjanˈpona/ which may realize in running speech as [ˌjɐmˈb̥ɔnɐ]) with pona "good" as its root. Stress in Spanish is not as completely predictable (and it's slightly less CV anyway), but the majority of words that end in a vowel are stressed on the penult (second to last) syllable. In a language with long vowels or geminate consonants, stress may fall a given number of morae, or length units smaller than a syllable, from the end of the word, as in Latin and Hawaiian. Another is to relax the stress constraints, but bias the lexicon against roots that contain the same sounds as common inflections or particles. For example, if two phrases supa yuza and su payuza invite confusion, people start rephrasing their utterances to avoid them. This is also fairly common and plausible, but it does result in occasional puns, as shown in the "Mondegreen" and "Pen Island" page on the literary analysis wiki All The Tropes. 

The English sentence "every man who owns a donkey beats it" can be interpreted as "every man beats every donkey that he owns", which makes no implication that anything exists. It is true even if there are no men, no donkeys, or no donkey slavery. This means a rigorous wording of this sentence uses only universal quantifiers ("for all" or "for each"): 

This also works with the slang interpretation of "beats it", with "x beats y" replaced with "x stimulates himself". Another user left a comment pointing out a third interpretation of the sentence, which I find less likely, that leaves at least some donkeys owned by richer men unabused: "every man who owns a donkey beats at least one donkey that he owns." This uses a third free variable: one for the man, a second to identify him as a donkey owner, and a third to identify a particular donkey as a beaten one. This third takes an existential quantifier. 

That trait of elliptical answers is simply 'Focus'. Wh-questions have a Focus, and the answer to them must be an expression that denotes something in the category of entities that the asker has decided ask about, to focus his question on. Violating the syntactic Focus principle can also be considered a violation of principles like 'relevance' (in Grice's sense), since it entails offering information the question presupposes as known and that is, therefore, redundant if repeated, or even as a violation of 'Economy'in Chomsky's sense, and, depending on which theory of syntax you adopt, the unacceptability of examples like your (b-h) above will be explicable as a breach of all or of just some of those principles. 

Adjuncts are invariably interpreted as nth-order predicates of heads of different ontological types (for n > 1; a first-order predicate is never a syntactic adjunct, it is a syntactic 'predicate'), whereas complements are interpreted as arguments of the predicates that constitute their heads. That holds whether the head is a noun or expanded noun, a verb or expanded verb, an adjective, a few adjective-derived adverbs that do take a complement, or, in fact, any other category, including 'functional' ones. If, on the contrary, you try to distinguish complements from adjuncts by syntactic criteria, under current Larsonian/Cinquean 'all-in-spec' approaches there is no way to do so, and criteria like optionality/obligatoriness (or inclusion of complements in phrases substituted by pro-forms like one or do so) are notoriously problematic, as well, as certain complements are arguably 'optional', whereas would-be adjuncts like very well in e.g. They treated us very well are obviously obligatory. Depending on your syntactic assumptions, of course, there may be 'locality' criteria that also distinguish complements from adjuncts, but in surface structure such locality constraints may be cancelled by 'displacement' due to information-structure or by related PF constraints. In general, the above-mentioned semantic criterion is, as far as I know, the easiest and most consistent heuristic tool in this respect. 

You ask two questions of very different orders of magnitude. The one you raise in the title and in your first sentence, the main one, is about a really huge (ultimately ontological) issue. The other is about the proper syntactic analysis of the DP the man next to him and should perhaps have been formulated as an independent question. Of course, it does concern a case of adjunction and so is not an irrelevant example, but it is a relatively unproblematic one that does not really show the complexity of the issues involved in answering your main question. To your main question, there is as yet no complete answer, and, to judge from what little has been achieved so far in this area, we are not likely to perhaps ever? have any. Most generative syntacticians offer essentially meaning-based classifications of adjuncts. Cinque and Ernst, for example, demonstrated the need to posit more than forty different types of 'adverbs', subsequent 'cartographic' studies by Gary John Scott and others have also established the ordering of about twelve different types of adjectives that may precede nouns, and, of course, all descriptive grammars have long distinguished about a dozen different types of 'modifiers' of manner, instrument/means, place (several types), source, goal, path, time (several types), purpose, cause, etc. within VP's and, less systematically within NP's, AP's and PP's, too. However, a) such labels do not yet remotely allow us to unambiguously clasify every property we can find accompanying N's, A's, P's, or even V's (the best studied case): there are indefinitely many properties that, simply, do not fit easily into any of those ontological categories and call for more and more discriminating ontological classification in a process that, as far as we know, may have no end. But, especially, b) the linguistic ontology we would need to predict and explain which properties are predicable of which entities and why has remained ridiculously inadequate for decades. To give you the obvious example: Cinque shows that the forty+ adverb types he distinguishes are hierarchically ordered and must correspondingly appear in a certain surface order, but only about five of the syntactic heads they are supposed to be 'predicable' of bear convincing ontological labels; Cinque simply cannot identify even the 'entities' such adverbials must modify, not to speak of the reasons why they can modify them. The answer to your question 'What can be adjoined to what and why', in other words, would require the elaboration of a) a full linguistic (not extra-linguistic!) ontology, and b) a listing of all the properties predicable of each of the entities so discovered, and, regrettably, we have nothing remotely on that scale in either respect yet. That is why we can at best offer rough descriptive generalizations about the order in which, e.g., prenominal adjectives must be adjoined to nouns or NPs, but no real explanation of the reasons why that must be so. As to why, in the specific example you cite, the AP/PP next to him must be adjoined to N or some 'extension' of N (say N', in X-bar terms), rather than to the DP, the problem is an old one. Back in the 1960's, Chomsky himself claimed the structure of such NP's (at the time) to be [NP [XP]], and about a decade later Emmon Bach tried to revive that analysis (which had in the meanwhile been abandoned after the objections raised against it by Barbara Partee), but with no success. I cannot here review the arguments in detail, it would be too long to do so, and, apart from that, most have aged since and would no longer fit current assumptions, but one of the constituency tests usually brought to bear at the time in defence of the [NP[PP]] analysis was that in cases like The library is much better than that in my home university or The computers in our offices are much slower than those in the university library, that and those seem to be able to stand for the full NP's the library and the computers, respectively, which, indeed, would support the [NP[PP]] analysis against Partee's (and most subsequent formal syntacticians') [Det [N [PP]]] one. Conclusions derived from such simple constituency tests have later been proved to be less compelling than was thought at the time, though, but, again, as reviewing the literature would be impossible here, I will content myself with offering you a different (and more theory-independent) kind of argument against the [DP[PP]] analysis that you contemplate as a possible alternative to the 'standard' [Det [N PP]] one: Since a DP is a 'closed' projection (in Frege's terms, it denotes an 'object', not a 'function'), if the analysis were [[DP]+[AP/PP]], the DP the man would have to be interpreted as the external argument of the one-place AP/PP predicate next to him, and the result would be a 'saturated' AP/PP which should have a 'propositional' interpretation (as in head-line style expressions like The President Furious/in London). Recall that the combination of the man - semantically, type 'e' - with the adjunct next to him - semantically, type [e, t], a first-order predicate under this analysis - would necessarily yield a type 't' expression, i.e., a proposition, whereas, of course, the man next to him does not support a propositional interpretation. If, alternatively, next to him were considered type [[e, t],[e, t]], i.e., a second-order predicate, then it could not compositionally combine with the man, obviously type 'e', and the expression should be ill-formed and uninterpretable, but it is not. On the contrary, if next to him is assigned type [[e,t], [e,t]] (a second-order predicate) and predicated of the noun man alone, type [e, t] (a first-order predicate), the resulting phrase man next to him would have the type representation [[[e,t], [e,t]] . [e,t]], i.e, it would yield an [e,t], and, of course, assuming that Det itself is a function of type [[e,t], e], functional application would correctly yield for the man next to him the type 'e', i.e., an expression referring to an entity (a man, in this case), which is exactly the way the whole DP is interpreted. Hence, on formal semantic grounds, only the 'standard' syntactic analysis [the [[man][next to him]]] is tenable, and that is probably a (or even 'the') major reason why it has been adopted, not only in mainstream TGG/GB/PPT and 'minimalist syntax', but also in all other non-Chomskian 'generative' approaches to syntax of the last four decades (e.g., LFG, GPSG, HPSG, etc.), as far as I know.