A suspended is thread which is waiting for some resource and is currently not active. The wait can be I/O, network etc.. See the blog I have shared for more details. 

Buffer pool is always there starting from when plan is created to when query executes. Buffer pool is allocating memory for any . 

Buffer pool only caters to request which require pages <=8KB for any pages or memory request which require pages >8KB memory would not be allocated by buffer pool but would directly be done by windows API. This memory allocation can cause SQL Server memory consumption to go beyond limit set in max server memory configuration page. Following can take memory outside buffer pool 

I would like to post this as answer. You can use various methods to make sure you are able to knock out any connecton to database when you are trying to restore but there are repercussions when you use command 

Yes there would be some outage during cluster failover.. During failover SQL Server is stopped on current node and failedover/moved to other node. After this SQL Server is brought online on the node to which it is failed over. Sometime this is so quick that application users might not notice. But the fact that SQL Server is restarted means you should always keep in mind that there might be some outage 

Yes this is correct. Although starting from SQL Server 2016 you have Distributed Availability Groups in which replicas can reside in different WSFC 

List of SQL Server features deprecated in SQL Server 2012 List of deprecated Database engine features in SQL Server 2012 Breaking changes to DB engine feature in SQL server 2012 

You can see you are creating temptable with same name.You cannot create temp table with same name in single query or batch this is documented as per BOL document. From Bol document 

No you cannot do this without breaking Logshipping. You cannot convert recovering database to read only without bringing it online and moment you bring it online Logshipping breaks. Instead configure logshipping with secondary in standby mode. Note that when you check the box disconnect user when restoring database while configuring Logshipping it would disconnect all users running query on secondary DB for restore. This is one of the drawback. 

Yes that is correct. Undo happens when database goes through crash recovery or recovery. There are 3 phases of recovery 

Next question would be why are you shrinking Log file every day . Shrinking of log file is also a evil operation because when log file grows process running has to wait for it to grow and since instant file initialization is not there for user database log files every time log file grows it has to Zero out space and write the the information there. This could be a time consuming on busy system where disk I/O is heavily utilized so moral is don't shrink log files. by shrinking you are just allowing it to grow again and it will grow again because SQL Server has to write information in log files related to queries running on system You can pre-size it and provide necessary space to log files so as to avoid autogrowth events. Make sure you keep proper value for Autogrowth setting don't keep it in percentage as it will cause unnecessary growth. More about autogrowth can be read from this link. See by how much SQL Server is growing and you would allocate that much space for a month to database log file so as to avoid and autogrowth events. 

I don't think there is any issue because of this, this is totally normal looking at this you cannot draw any conclusion. 

This is excellent article on data compression, its big, but would help you in understanding data compression You must remove all SQL Server related folder and files from Antivirus check specially if you have McAfee Antivirus. 

Its has one row for begin tran one for commit and one regarding update operation. You can see LOP_BEGIN_XACT for beginning of transaction and COMMIT for commit of transaction. LOP_MODIFY_ROW says a row was modified like we did in update statement. LCK_Clustered index is in picture because table had CI and row must have been exclusively locked for update Then comes hexadecimal page ID which actually says what page had row which was modified Then last column Includes the lock Information. The lock which was taken on HoBt 72057594041860096:ACQUIRE_LOCK_IX OBJECT: 8:613577224:0 ;ACQUIRE_LOCK_IX PAGE: 8:1:344 ;ACQUIRE_LOCK_X KEY: 8:72057594041860096 (8194443284a0) Database ID=8 file number 1 and page 334. You would also see KEY value There is also a log sequence number for every transaction and column Log Record Length which would tell you LSN and size of log record for particular operation 

I quote from same Support Article Update to expose maximum memory enabled for a single query in Showplan XML in SQL Server 2014 or 2016 

No it should first go to node 2 because the Node 2 is read only replica and Node 1 is primary replica which will server to read write operations. As per BOL 

EDIT: Restore process also depends on amount of transaction which needs to be rolled forward and rolled back. If Log file have to many VLF's restore process would take time. Restore will be faster if disk from which restore process is reading data is not facing I/O contention. More details in this microsoft Link $URL$ 

Automatic failover is not allowed if availability replica is hosted on SQL Server FCI. Quoting MS Arvind Shyamsundar This is simply because the SQL Server team did not wanted to add more complexity to failovers scenarios. You already have automatic failover provided in SQL Server FCI and introducing automatic failover with availability group in FCI would make it too complex and may result in unwanted situations. If you go ahead and try to configure automatic failover you would get below error message. Copied from this blog 

Since you mentioned you want to copy table data only I guess SSIS would be perfect. I have been using it and it works just fine for me. It also depends on level of competency you hold with SSIS. You can go for backup restore but it would also restore tables whoes data you dont want to change. If you want complete refresh then of course no better option than backup restore. You can go for import export wixard as well and its more easy than SSIS because it almost does all data conversion(if required) for you. You should use OLEDB provider always when data transfer is to be done within SQL server.There are tools but personally I found SSIS more relaiable than tools. Have a look at below link $URL$ Best practice for SSIS packages $URL$ 

Stop looking at Buffer Cache hit ratio to determine memory pressure. This is because with read ahead mechanism in SQL Server more than enough pages are alredy in buffer pool to satisfy the query so BCHR does not gives accurate figure about memory pressure. You might even see that BCHR even would not drop when there is memory pressure. All these has been explained in Great SQL Server Debate about BCHR PLE output you posted seems really low but we cannot just use one counter to gauge memory pressure. PLE is more indication of I/O activity on server. It could be possible that due to heavy I/O activity PLE plummeted. If you note Target and Total server memory still remains same. Which is good sign. For . You can use below counters 

You need to find out what query is running after 2:00 PM and it was still running so it makes me think there is process.job starting at 2:00 PM. This process/job/query is trying to read too many pages which is causing lazywriter to flush too many pages because query is requesting space for such pages It can be that such query is missing index, can be that its creating a bad plan due to skewed statistics, it can be that query needs to be written to get/read only subset of data not whole data. 

You can see the for large number of queries are approx 5G, this is large memory grant, ideally it should be few MB's. Do note that is just around 5 MB and is NULL this is because due to memory pressure SQL Server is just able to provide minimum memory to start the query but not able to provide additional memory for query execution resulting query to fail with OOM error. The query costs for queries requesting huge memory is also high which leads me to believe that either statistics are skewed or queries are written poorly. Other possibility would be query not supported by proper index. Number of queries requesting such a huge memory grant is good in number. 

Their cannot be a comparison drawn as Snapshot is "photo copy" of data file. The difference lies in amount of activity both does and consistency of product arising out of both the operations. Full backup "Is Always" more reliable than snapshot backup Full backup includes all committed and uncommitted transaction when full backup has finished. Read more about this in Understanding SQL Server Backups full backup also includes few amount of transaction log, if required, this is not the case with snapshot backups. Backing of transaction log is required so that recovery can run successfully when backup is restored. Read about Database snapshot A database snapshot provides a read-only, static view of a source database as it existed at snapshot creation, minus any uncommitted transactions. Uncommitted transactions are rolled back in a newly created database snapshot because the Database Engine runs recovery after the snapshot has been created (transactions in the database are not affected). 

First thing if transaction log is not damaged you should go for tail log backup. That would cause minimal data loss. Then restore would be 

You must note that latch only protects physical integrity of page while it is in memory so latch would be taken when page is in memory. Suppose a record is being inserted and for that page needs to be fetched. First, page would be locked and brought into memory then it would be latched and information would be written. The process after this would be 

IMHO you it would help you in PIT recovery as well as its documented and recommended in BOL Article. It wont do any harm and you will have a failsafe. Please also note its advisable to change recovery model when load on database is relatively less. Although you can change it in peak hour but changing recovery model does takes lock on database and if load is high you might face delay. No transactions/changes would be lost, if you change from full recovery to simple. Changing recovery model would force a checkpoint which would commit transactions which can be committed. After changing recovery model to simple automatic checkpoint would truncate transaction logs(if no long running transaction is holding the logs) Point 3 and 4 are totally not required, do you have specific reason to do it ? Point 5 is necessary and you must at least take full backup daily of database in simple recovery. However backup of database should be according to RPO and RTO agreed. You can also take differential backup in simple recovery to reduce RTO.