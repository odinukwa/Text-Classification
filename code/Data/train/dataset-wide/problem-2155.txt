Writes to the journal are fast append-only operations. As you've noted, these provide durability and crash resiliency. If MongoDB stops unexpectedly, it can recover from the journal and data will be in a consistent state. Without the journal, your data is in an unknown state after an unexpected shutdown and you need to validate it by running repair or (in the case of a replica set node) resync from a known-good copy of the data. 

An important caveat is "since the last background flush". Multiple updates affecting the same pages within a given sync interval will effectively be batched. If you're trying to get to the bottom of performance issues then consistently high background flush times (particularly as a large or increasing percentage of the default 60s flush interval) are definitely of concern, but should be reviewed in the context of other metrics such as page faults, I/O stats, and lock percentage. I would also review the MongoDB Production Notes for general tips and upgrade to the latest MongoDB production release for your major version (i.e. latest 2.6.x or 3.0.x if there's a newer than your current version). 

If you've restored a sharded cluster from a backup, the shards will already be configured. If host names have changed from the original deployment, you will need to update the information via a rather than adding new shards (and then restart the instances in your sharded cluster to ensure the new names are used). See the MongoDB 3.0 Restore a Sharded Cluster tutorial for more specific details. 

If the problem is easily reproducible, I would raise a bug report in the MongoDB Jira TOOLS project including more details such as the specific versions of and and command line parameters used as well as how long you waited. It seems reasonable for to have some sort of timeout or error handling for this case rather than hanging indefinitely; I also wouldn't expect to create files that the same version of cannot handle. 

GridFS is for storing large binary data chunked into smaller documents (by default, 255KB each). The access pattern for GridFS is different from a sharded collection where random document distribution might be more desirable (for example, with a hashed shard key). With GridFS the documents relating to a single file are normally read sequentially: identified by unique and ordered by chunk number . The supported shard keys for GridFS enable range queries based on the order that drivers will reconstruct a GridFS byte stream. A hashed shard key does not support range queries so would be extremely unhelpful for read performance ( chunk lookups would be required and data would be randomly distributed). 

The option for specifies the source database to dump. The option for specifies the target database to restore into. 

Mixed-version replica sets are only supported for consecutive major versions (i.e. 3.2 and 3.0) and expected to be used transiently during a rolling upgrade to a new major version of MongoDB. A mixed replica set with 3.2 and 2.6 members is definitely not supported. 

If the reason for your performance challenge is a lack of resources, you should be adding dedicated resources (such as more RAM or faster storage) rather than sharing existing resources. Note that even with replica set members on multiple servers, secondary reads are not a panacea. For some considerations, see: Can I use more replica nodes to scale?. Before adding additional server resources, I recommend starting with optimising indexes to support your common queries and aggregations. See: Index Strategies in the MongoDB documentation. 

MongoDB drivers and clients use the command to describe the current role of a instance, monitor round trip time, and verify replica set configuration/status (if applicable). Frequent commands are expected behaviour, although the occurrence will vary depending on how many clients/drivers you have connected as well as the driver versions. For more information you might be interested in the Server Discovery and Monitoring (SDAM) specification which details the behaviour for officially supported MongoDB drivers. 

Start With all three config servers are available & upgraded, changes to the sharded cluster metadata can now resume. Re-enable the balancer so normal balancing activity & chunk migration can resume. 

Sharding is initially enabled at the database level but the partitioning happens at the collection level based on a shard key. You can have a mix of sharded/unsharded databases and sharded/unsharded collections in the same MongoDB deployment. 

The utility converts BSON data (which is binary) into human-readable formats. Common usage would be to inspect data formatted as MongoDB Extended JSON. This can be useful for previewing data contained in BSON files as created by , replica set rollbacks, and other features (eg. auditing in MongoDB Enterprise as per your example). The output is generally only relevant for a developer who wants to see an annotated view of the BSON (as per the BSON spec). 

Does the same data take more storage space in version 2.6? As noted in the MongoDB 2.6 release notes, the default storage allocation strategy changed from using exact-fit allocation with padding factor to . The powerOfTwoSizes allocation may take up more initial space, but generally results in less storage fragmentation and better reuse of space from deleted documents. One notable exception is if you are using GridFS and have data saved with older drivers. The original GridFS chunk size was 256KB (an exact power of 2) which resulted in rounding up to 512KB (the next power of 2). Drivers were updated to lower the GridFS chunk size to 255KB, but this is a common reason your database size to grow if you resync with the powerOf2 allocation (see SERVER-13331 in the MongoDB issue tracker). Changing the allocation strategy In MongoDB 2.6 you can change the allocation strategy on a collection level using the command, or on a server level with the configuration parameter. If you change the allocation strategy for an existing collection, this will only affect new documents created or moved in storage after the change. You would have to resync, compact, or repair the database to rewrite all documents with new allocations. If you are using GridFS and have older documents with the 256K chunk sizes, it would make sense to change the allocation strategy for any gridfs collections to not use PowerOf2Sizes. Similarly, if your data use case for a collection happens to be insert-only (or does not grow in size via updates) then the exact fit allocation will be more efficient. Otherwise, the powerOf2Sizes allocation is recommended. WiredTiger storage engine supports compression in MongoDB 3.0+ If storage size on disk is a concern, you should also consider testing the new WiredTiger storage engine in MongoDB 3.0, which includes support for index & data compression. Upgrading to MongoDB 3.0 & WiredTiger will require you to use updated drivers & tools, and as with any major version upgrade you should review the upgrade procedures relevant for your deployment as well as any compatibility changes. 

A self-configuring tree topology, with the current primary as the root node. By default MongoDB allows replication chaining so secondary nodes can choose to sync from other secondaries based on ping time (for example, to mitigate bandwidth usage when multiple secondaries are in a remote data centre). A self-configuring star (or hub-and-spoke) topology, if you disable replication chaining. 

Disable the balancer. Disabling the balancer ensures any active migrations have completed. Stop the last config server listed in your mongos' setting (will call that for the purpose of these steps): At this stage you should have: 

The context you are missing is that failure of a replica member does not affect the number of configured members the replica set has (or the required voting majority to maintain a primary). Any changes in fault tolerance or replica set election requirements will involve a re-configuration of the replica set (eg. adding/removing members or changing voting members) rather than a replica set member state change (eg. DOWN, RECOVERING, ..). A three node replica set with one down member still has three members (with a strict majority of two). If you think of it as analogous to a RAID configuration, a three node replica set with one member down is running in degraded mode and cannot tolerate the failure of any further voting members. This allows for continued availability, but you will want to recover or replace the unavailable member to return the replica set to a healthy state. You can check the current replica set configuration with and the current state of members with . 

Depending on the driver and/or framework you are using, there may be helper functions to make it easier to populate related data. These helper functions will ultimately result in multiple queries as there is currently no server-side support for joins. Some examples of frameworks with helpers (not intended to be a comprehensive list): 

Since the only possible update was to the array, indicates that "mango" already existed. Updating with more fields If you combine multiple field updates, you won't know which field was actually updated. In this case, you could specifically match documents where the array value isn't set already: 

This a generic warning that a WiredTiger file could not be read because it was being used exclusively by another operation (for example, if a checkpoint happened to update a file at the same time as it was being validated). The "Not treating as invalid" message is very opaque but is trying to describe "not success but not an error" (the file was busy). This should be a transient error and the command will likely succeed if you retry. I created SERVER-27670 in the MongoDB issue tracker to make this message more user-friendly. 

MongoDB 2.4+ can build multiple indexes in the background. For more information, see: Background Index Builds. You may also wish to upvote/watch SERVER-20960: Default index build option support in config/runtime in the MongoDB issue tracker, which is a feature request to make the default index build type configurable (eg. default to background index builds). 

This query can be improved using projection to exclude the field and achieve a covered query using only the index. The covered query no longer needs to fetch a ~10MB document into memory, so will be efficient in both network and memory usage: 

Stopping one of the config servers ensures there are no changes to the metadata in the cluster (chunk splits or migrations cannot be committed without all three config servers available). Use to export the config database from config2 After running the you should have a directory with bson files. Create a new data directory on . The storage format for WiredTiger data is different from the existing mmap data, and cannot use the same dbpath as mmap. Restart the server with the WiredTiger and appropriate storage options: 

As part of the MongoDB 3.0 upgrade it is strongly recommended that you upgrade to the SCRAM-SHA-1 authentication scheme. New users will have their credentials created in this format by default; mixed authentication schemas will likely lead to confusion. You will need to upgrade your client driver & admin tools (if you're using a GUI) in order to use the WiredTiger storage engine. Drivers/tools that have been updated for MongoDB 3.0 will also support the new authentication method. As per the note on the SCRAM-SHA-1 documentation (referring to a future major release which would like be 3.2.x): 

Each shard contains a subset of data for the sharded cluster. If the replica set backing a shard becomes read-only you will get exceptions trying to write new data to that shard. The mechanics of failover and high availability for an individual shard are governed by your Replica Set configuration and deployment. 

As at MongoDB 3.4, when a primary transitions to a non-primary state (eg. as the result of an election) all active connections are dropped so they can be re-established on the new primary. Cursor state is specific to a given , so you cannot resume a cursor on a different member of the replica set. A recommended area to investigate would be why your primary was heavily loaded and why a change in primary would have reduced the load significantly. Generally electable secondaries in the same replica set should be identically provisioned in terms of hardware resources, so exchanging server roles should have pushed similar load onto the new primary. If the load was coming from suboptimal queries that were terminated on the former primary (or due to other resource contention), you could perhaps have avoided reconfiguring your replica set by finding and addressing the root cause. The MongoDB manual has some information on how to Evaluate Performance of Current Operations. You should also implement a monitoring solution (if you haven't already) in order to capture a baseline of normal activity and help identify metrics that change significantly when your deployment is under load. If you have long running queries which are likely to be interrupted by a restart you could consider: