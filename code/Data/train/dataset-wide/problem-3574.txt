But how do I identify the data to move? I cannot go by age, because the first 10 TB was filled the same day using . I have tried: 

As far as I can see Linux has RAID6 and RAID0, but not RAID60. So RAID60 is emulated by creating RAID0 on top of some RAID6s. It is possible to add a hot spare to each of the RAID6s, but is it possible to add a global hot spare shared by all the RAID6s? If so, how can that be added to existing RAID6s that are currently assembled automatically (no mdadm.conf)? 

GNU Parallel is a general parallelizer and makes is easy to run jobs in parallel on the same machine or on multiple machines you have ssh access to. If you have 32 different jobs you want to run on 4 CPUs, a straight forward way to parallelize is to run 8 jobs on each CPU: 

It gives 77 workers. GNU Parallel is a general parallelizer and makes is easy to run jobs in parallel on the same machine or on multiple machines you have ssh access to. It can often replace a loop. If you have 32 different jobs you want to run on 4 CPUs, a straight forward way to parallelize is to run 8 jobs on each CPU: 

GNU Parallel instead spawns a new process when one finishes - keeping the CPUs active and thus saving time: 

There are several solutions. The workers are behind NAT. No access to a jump host. No access to firewall. For this you need some sort of VPN that can traverse the firewall. A TOR hidden service for port 22 on the workers can be used here. If all the workers are TOR-enabled: 

works between two machines that are both NAT'ed behind different NAT-walls does not require modification of NAT-wall (i.e. port forwarding is no-go) works as both client and server on both Windows 7+XP and GNU/Linux is free (as in freedom - not price) software 

I have made an external script (snmp_max_io written in Perl). When run with and IP-address, it logs input to syslog and it prints a single 64-bit numeric value and set the exit value to 0. Example output: 

I have a RAID60 that I want to expand. The current is: 2 axles each having 9 disks + 2 spares. The future is: 4 axles each having 10 disks + 1 spare. So I need to do some --grow to reshape the drives. I thought this would be enough: 

and have autofs lookup the DNS name my.nfsserver.example.org, mount exporteddir automatically, and unmount again when the dir is not in use. But this only works on my Unix-machines. Does an NFS-automounter exist for Windows (7 or 8)? So I can do something similar to: 

It seems there is a solution here: $URL$ It gives 5 wan links, and 400 Mbps is plenty for my use. Finally the price seems reasonable. $URL$ 

Everything up to the last is OK. Only stuff written after the last sync is flakey. Add some spares and do the rebuild. When the copying of the existing disks finishes tomorrow I will test out the above. If it works, then the above is an answer. Otherwise a new copying of the original set will be started, and new ideas are welcome (but please test them on the test scenario). == Spares are now added and rebuild started. Every 1000th file was copied to a dir on the file system and this did not cause issues in the logs. So it seems the filesystem is OK. It remains to be seen if the users miss some files. == No users have reported missing files so far, so it seems to work. 

The workers are behind NAT. No access to a jump host. Access to firewall. If you can forward ports, so that port 2001 is port 22 on host 1, port 2002 is port 22 on host 2, 2003 is host 3 ... then you can use -p: 

I need a remote assistance tool (i.e. tool that makes it possible to control another persons screen) that: 

If the servers are world wide (i.e. not on your local 10 Gbps network), then $URL$ might be a solution, too. 

My software RAID can write 800 MB/s sustained. I see that happening when returns > 2 GB. However, most of the time the writeback is round 0.5 GB which gives a performance around 200 MB/s. There is plenty of data to be written. says the dirty cache is 90 GB. As I understand Dirty is what needs to be written, whereas Writeback is what is actively being written to disk. So there may be blocks in Dirty that are located on the disk just next to blocks in Writeback, and these will not be written in the same go. This can explain why I get much worse performance if Writeback is small as the time spent seeking is much longer that the time spent writing a few extra MB. So my question is: Can I somehow tell the kernel to move more data from Dirty to Writeback more aggressively and thus increase Writeback? -- Edit -- This is during low performance: 

After a while Zabbix changes the status to: 'Not supported' and "Received value [] is not numeric". The command works fine when run on the command line as root or as zabbix. But the command is never run by the zabbix server (nothing is logged). How do I find out why Zabbix thinks the command returns [] while it clearly never runs it? The solution is in the comments: The script MUST be a bash script. So a perl-script must have a bash wrapper. 

The workers are behind NAT. Access to a jump host. If you have access to a jump host from which you can reach the workers the obvious would be: 

Installation If GNU Parallel is not packaged for your distribution, you can do a personal installation, which does not require root access. It can be done in 10 seconds by doing this: 

I have changed the controller with an identical controller (SAS2008), reseated all cables, exchanged external SAS cables, reseated all disks. I have no problem reading disks individually using 'dd', but when used in a RAID6 the disks drop offline often. 

So and are clearly busy taking up 64% and 53% of a CPU respectively, but not near 100%. The chunk size (128k) of the RAID was chosen after measuring which chunksize gave the least CPU penalty. If this speed is normal: What is the limiting factor? Can I measure that? If this speed is not normal: How can I find the limiting factor? Can I change that? 

I use "Template SMNP Interfaces" to monitor switches. It gives me keys like: ifOutOctets[16] I would like to have an item that covers all ports: 

Redirecting port 80 to 443 is trivial with iptables, but from your description you clearly do not want to run http on port 443: You want people to be redirected from http to https. You are clearly aware that this can be done in Apache for each virtual host, but you prefer not to use that to avoid configuration oopses; otherwise sgtbeano's solution is what I would recommend. Based on that I would recommend you stop using Apache on port 80 and that you run another webserver on port 80 that does the simple redirect. You could even run a separate instance of Apache on port 80 with a different configuration dir. This will also avoid the configuration oopses, but may be harder to maintain (not many people expect 2 different instances of Apache on the same host with 2 different sets of config files). In this case you can use sgtbeano's config for the port 80 instance. 

For other installation options see $URL$ Learn more See more examples: $URL$ Watch the intro videos: $URL$ Walk through the tutorial: $URL$ Sign up for the email list to get support: $URL$ 

I have recently transferred 10 TB through a 1 Gbps connection. The major problem was keeping the 1 Gbps filled at all time. That was no problem when transferring big files, but proved to be a problem when transferring small files as the sender could not seek fast enough. The solution was to run multiple transfers in parallel. A few with big files and a few transferring the rest. It was based on: $URL$ If your files are compressible, make sure to compress the transfer (rsync -z). In theory you should be able to use GNU Parallel and rsync on Windows 7, but even if you cannot you can probably use the idea of transferring big files in parallel with small files. 

But I only seem to get the basename of the file and not the full path to the file. And since a lot of my files are called the same (but in different dirs) then that is not very useful. Also it seems to give me more information that just inode 1. I have a feeling that I can use and get that to tell me what files are using blocks in the first 1 TB, but I have been unable to see how. (By using the mount option the file system will not give No space left on device, but if you later forget to use mount option then you will get No space left on device again. I would like to avoid the mount option as the filesystem may be mounted by other people on other systems, and they will forget that and thus get a surprising No space left on device). 

So we can read sequentially at 140 MB/s in the outer tracks and 82 MB/s in the inner tracks on all the drives simultaneously. Sequential write performance is similar. This would lead me to expect a rebuild speed of 82 MB/s or more. 

But this does not work because the command is dequoted by ssh twice where as GNU Parallel only expects it to be dequoted once. So instead use again: 

Installation A personal installation does not require root access. It can be done in 10 seconds by doing this: 

says the disks are not 100% busy (but only 40-50%). This fits with the hypothesis that the max is around 80 MB/s. Since this is software raid the limiting factor could be CPU. says: 

The two servers are the same model. They have the same modules loaded, but they are not using the same modules. shows that the two servers of the same model use very different RAID controllers (I hate it when vendors do that: F*cking change the model number if it is not the same model!). On the server, where the disk works, you do not need to setup anything to access a disk that is not in a RAID. But on the server, where the disk does not work, you need to set the disk up as a volume in the RAID controller before Linux detects it. So I have now done that, and the disk is now accessible. Thanks to Falcon Momot and Sergey Vlasov for pointing me in the right direction. 

Consumer grade DSLs are way cheaper than getting at leased line, so for my building complex I am considering getting a bunch of DSLs instead of a leased line. I feel confident that I am not the first to do that, so I am wondering if there is a Linux router that will somehow distribute the traffic over these DSLs. We are not talking about real bundling as the ISP should not be required to do anything. My initial ideas are along the lines of computing a hash based on sender IP and recipient C-class and based on this hash choose the DSL, so all packets between a given client and server will always take the same DSL. And if a DSL goes down it should of course not be chosen. But as I said: I cannot be the first to do this, so I feel confident there is a tested way of doing this. 

For other installation options see $URL$ Learn more See more examples: $URL$ Watch the intro videos: $URL$ Walk through the tutorial: $URL$ Sign up for the email list to get support: $URL$ 

The template runs a portscanning on the host to see which TCP ports are open. This all works very well, but portscans every single host every 7 minutes (my guess is that it takes in total 7 minutes to ping all hosts in the network and do a port scan of all the hosts that are up). I would like it to only run once per day or so. The discovery rule has 'Delay (in sec)' 200000. The 'Template TCP service discovery' has 'update interval' set to 86400. How do I tell Zabbix to cool it? Don't run the discovery+portscan every 7 minutes. 

This took 20 minutes on a 50 TB filesystem. Oddly enough most of the time was CPU time and not waiting for disk I/O. It used in the order of 100 GB RAM. Now the file system is usable again: 

I have just installed a SAS disk into a Debian server. It was detected correctly and everything was fine. Then I moved the SAS disk to a different Debian server, the same hardware model and running same version of Debian, but here the SAS disk is detected as /dev/sg7 and not /dev/sdb. works fine, but and hang. I tried putting the SAS disk in another slot: Same problem. How can I force the SAS disk to be detected as /dev/sdb?