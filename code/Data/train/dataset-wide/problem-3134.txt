Update: checked how these ratios are calculated, here is a more efficient answer that avoids a lot of checks between pairs: 

As for performance: in the worse case(element is not there), will have to check all values of the array to check if the value is in it. And doing this for all values of the second array, means O(n * m) where n is the size and m is the size. If n = m, we can consider that it takes O(n*n), which is quite expensive. You could improve this by sorting both arrays and doing a smarter check, but even that will have the cost of the sorting, which is O(n * log n) with a good implementation Having said all that, what you really want is a structure to check if you have seen or not an element that can check it really quick. In javascript, we can use objets to do this. As its native code we don´t really know the complexity of access, but with testing a bit it is clear that it´s much better (in theory access should be O(1) or O(log n)) The pseudo code is like this: 

For every row in the csv, generate 2 events: subscribed with and unsubscribed with Sort all these events by date in ascending order Set , and Preprocess data: keep amount of unsubscriptions total up to each day and amount of actually subscribed users on each day 

Performance review: Your biggest bottleneck is the , which is turning your algorithm from being O(n log n) to O(n^2). A simple way to fix this is using a new vector and input the unique keys intead of removing duplicated ones from your old vector. 

The most expensive part of the algorithm is the sorting, which I´m not sure if you needed it. If you do not care about order, no need to sort them. The important part is that we only add each number to the result array once, and we process each number once, so except for the sorting, the complexity should be linear on the amount of numbers in input. 

The main point about breadth-first search is that when you visit a node, you are visting it with the shortest distance possible. That´s why visiting a node twice does not make sense, as on your second visit you are visiting it with a higher or equal distance. In your approach, although you are setting your current node as visited using , The correct approach is to mark the ones you add to the queue as visited, and not the node that you removed from the queue. Think that until a node is popped from the queue, it is not marked as visited so while it´s true that from the moment it gets popped it won´t get visited again, it´s entirely possible (and highly likely) that your queue has multiple of this same node already stored. I would remove the line , change your : 

Here is an alternative way of calculating the churn rates which would be optimal efficient wise without altering the data you have. Note that maybe pandas queries may outperform python loops used here so your solution could run faster even though its less efficient. Also, I am assuming no user can have multiple subscriptions on a same day (intervals for user subscription don´t overlap) and that an user startDate - endDate range is at least 1 month 

I went full performance here, so no forEach and such. The idea is to use associative arrays and assume they cost O(1) for speed improvement. The steps of this algorithm: 

In your case, you start with 10-50 and 0-15. It did not give enough overlap. Is it worth comparing 0-15 with the other slotA ranges? No, because the others will start at more than 50, and they won´t overlap. Is it worth comparing 10-50 with other slotB ranges? Yes, because there could be a 20-30 range afterwards. Basically, 50 is greater than 15, so thats our hint that we can keep growing that 15 going to next range from slotB and as long as its less than 50, we might keep having an overlap. If you keep doing this, you will arrive at desired answer. There might be data structures suited for this, but its still simple enough to not need one. 

After searching the original problem on codechef, it becomes clear that B can be 1 and N be 1,000,000,000. In that case, your code will iterate all values and it will take long, as its O(n). This can be reduced to O(1). You need to see this problem as a function, and once you find the correct function, you want to maximize it. In the first example, N is 10 and B is 2, so we can do at most 5 clicks on second button. If we decide to click first button, we will always need to click it 2 times, because if we leave 1 energy remaining we can´t use it for second button. Another important observation is that you always need to click the first button X times, and then spend the rest on second button. I leave that for you to think why. So in this case, you can do the following: 0 * 5 (click 5 times second button and 0 the first) 2 * 4 (click 4 times second button and 2 the first) 4 * 3 (click 3 times second button and 4 the first) 6 * 2 (click 2 times second button and 6 the first) 8 * 1 (click 1 times second button and 8 the first) It is clear that our function in this case is f(n) = n * (10 - 2n), with n being the amount of clicks on first button. Expanding, we get -2n^2 + 10n. Deriving that, we get -4n + 10. If we consider the maximum to be at 0 (which it is), n = 2.5 would archieve that maximum. If we replace on the ecuation, 2.5 * (10 - 2*2.5) = 2.5 * 5 = 12.5 However we can´t actually make 2.5 clicks on first button, so we need to consider either 2 clicks or 3, and take the best one. In this case, both cases will give us the answer : 12. In cases where N is not a multiple of B, your first steps will always be clicking first button until remaining energy is a multiple of B and then you have the same problem as before. More generally, f(n) = N % B + n * ((N - N % B) - B*n) and the n that maximizes the function is the solution of 2Bn = (N - N % B), so n = (N - N % B) / 2B. Be careful that n can be decimal, so you should consider using floor. This way you can compute the answer in O(1) and remove your loop. 

You can then combine this with @scnerd solution to add multiprocessing If you know your data is all the same type, you can further optimize it: 

Your code is pretty good, except for using , as that can be quite expensive: . Here is how I would do it to avoid using that. Assuming hashmap access is and n being the max length between both strings, my implementation runs in 

Be careful that the random can be between 0 and 4 when it should be between 1 and 4. If you want to make it more object oriented, I would separate the interface (alert and prompt) from the game logic. As this is just a game with 1 ship, I would only do a Game and Ship objects to represent it, but for more complex versions other objects may apply. Here is how I would implement it having custom columns: 

This problem is much easier looking it backwards: Instead of processing the input in the order given, process it in reverse order. You know the last number in your answer will be 1, because no numbers follow it. Now the previous one could be a 2 if the sign is different with the last one or a 1 if its the same sign. So basically, each step back can either add 1 to the list of consecutive numbers with alternating signs or reset the count to 1. The code: 

I will assume all your words only have a-z characters. With that, an efficient check can be made by preprocessing your dictionary: Pseudocode: 1) Preprocessing: 

There are a lot of queries, but a small amount of cities. It would seem more important to optimize answering a query time, which is usually done with some precalculation or caching. In this case, we can think that as there are at most 3000 cities, there is at most 3000 distinct city types. So all queries that don´t ask for an invalid city type (no city has that city type) will have one of those 3000 types. We can precalculate for all cities, the query of going to right or left for each of these types, ending with a O(cities * distinct types * 2) complexity, which would be 3000 * 3000 * 2 at most, much better than the original 500000 * 3000. Then for each query, if the type of city is a valid type (at least one of the cities has it), we have already preprocessed the answer and can answer in O(1). If it isn´t, we just answer with -1. Another hint is the topic of the problem, which is for practicing multi dimentional arrays. We use this to store the calculation of all valid queries to then answer in O(1). I advice you to try coding the problem again with these hints. If you get stuck: 

First of all, checking in a range if you can write a letter or not can be improved to a O(1) check, in my case I used for that. Another thing that can improve yours is adding a cache, because the longer the string (and the more * it has), the more repeats you are making in the recursive calls. In my case I used for that. With just those 2 changes I think your code would be greatly improved. Another observation is that it is not necessary to try for each index writing the 3 letters. When we have a wildcard, we only need to try using the letter from previous index and the letter that is more ahead () because the third letter has no inference in that range. When we don´t have a wildcard, there is only 1 choice of letter to make. My final observation is that if there is "nothing for a long way", it is not necessary to try writing at several indexes thinking it is necessary to "leave space" in case we have a different letter "soon". This completely cuts the branching in these cases, which further helps on performance. I defined "long way" as block_size * 2 as I am sure that is enough, but maybe it can be less than that. Took me a while but can´t find anything else to improve. Although my code works for your test cases, more testing should be done as there might be some bugs. Your code took me 12s to solve, haven´t meassured mine but its definitely under 1s . Here is my own solution with all these ideas: 

Now, reading some special properties about this numbers, we can make a much better algorithm: we generate only the possible numbers that satisfy necessary conditions to be highly composite and then we process only those to filter the ones that were really highly composite: 

Looking by the way you make these levels, as a sublevel has the same prefix as its parent, if you sort the data by , you will have data sorted in a way that if you see a level node, the last level node you processed will be its parent. For example when you iterate after sorting and is 13010190, you know that 13010100 was already processed (because the number is lower) and that any nodes processed between 13010100 and 13010190 will be of level 4, because the next possible level 3 node (13010200) would appear after 13010190. With this in mind, we can first sort all data by and then just add each node to its lower level last seen node, which will be the right parent for it. To avoid border cases with level 1 nodes, I chose to make a fake level 0 node called root which will have all level 1 nodes. Its children would be what you need called in your first example. Also, I didn´t create the properties and as that can be easily checked by . If its in your requirement to have them, should be easy to add them. This code would also work for more than 4 levels as long as it follows same format. Here is the code: