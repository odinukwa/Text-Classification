I have been hearing the term "bounce the database" lately, and researching it, it seems that people take it to mean different things. Most recently is was an Oracle DBA saying: 

In Oracle, I know we have a built-in function which is more or less equivalent to SQL Server , and I have built an Oracle bit-wise function to mimic SQL server's bit-wise operation () as follows: 

Side Note: I hear lots of arguments for not allowing nulls which relate to "oh, what if users writing queries don't know how to properly account for s in joins or where clauses, etc... This is never a good reason to support a design decision at the data model/etl level. This issue is best addressed at the data presentation or reporting layer. E.G. some BI tools have built in handling for this or abstract it away, etc. 

Can do this easily in regular expressions, but I do not have access to add regex capabilities to my instance. So, How can I search using LIKE to find something like this? Presumably I would need to find where the first character of the line (in a multi-line blurb) is a character and 2nd is a dot. 

What is the best way to index a column for an efficient look-up based on a bitwise comparison in the WHERE clause e.g. 

Are there any easier simpler ways of accomplishing this conversion? Even better would be something I can use natively in T-SQL instead of creating a function dependency. 

If you don't have a thorough understanding on the business meaning of the data point, you should NEVER put in a dummy or made-up value. If this is a fact table, you will need to work with the business teams to understand the scenarios in which the can occur. If the column represents a monetary value, using zero works in most cases, but again, understanding the data point is necessary here too. If you determine with the business teams that can/should be represented by something else e.g. $0, or -1, etc., I recommend working with the folks responsible for the source data, and trying to affect the change there if possible. Then the appropriate business values flow through to the DWH in your ETL. If the business determines a field/data point can legitimately have no value, then let there be nulls! If the column is an identifier e.g. a primary key to another dimension or reference table, then it is a different story. In your dimension or reference table, there would typically be a row representing 'no value', which would have an id of it's own... This key would go in your fct table. Regardless of what you work out with your business teams... ALWAYS document it all in your data dictionary, so that users of the data (and your ETL/DWH developers) have a reference back to what it was done that way. 

I am exporting a table to a TSV file using sqlcmd, and I am running into issues. The table has 16+ million rows and about 55 columns. The problem is that it does not export the full table, but seems to stop randomly at various points (i am guessing a timeout?) Each time a different number of rows are exported and each time the file is of a slightly different size (indicating that I am not hitting any row or size limit). I am not using any timeout switch (meaning the default of "as long as it takes" is used). Here is my command (with most columns removed for simplification and illustration purposes): 

I am in a SQL Server 2008 environment. I am trying to use pattern matching in the clause to find rows where a certain column's value contains characters that are not alpha-numeric, underscore, dash, period, or space. This is my code, and sample data, but I am not getting the results expected. In the example data, I want to return rows 7, 8, 9, and 12, but I am getting rows 5 and 6. If this isn't the best way to achieve the goal, I am open to hearing other methods. I am not in an environment where I can implement regex so my solution limited to out-of-the-box functionality. 

There is one clustered index and one non-clustered index, both on columns which are not involved in any way in the computed column definition. What is it about the column being used in a computed column that causes this error? 

I work with an application that, in many places, uses stored procedures to return data to the application, kind of like a view, but these procedures take in parameters (which isn't possible in a SQL Server view). I believe this was done due to it being more efficient for the given queires to have values passed deep into the inner queries, than it was to use a view with a clause. In these cases, none of the other features you get by using a stored procedure are required e.g. they are all only select statements... no transactions, it's a single result set, no error handling, no calling to other procedures, etc. My question is - for this specific use case - which is basically a "parameterized view" - are there efficiencies to be gained by using an inline user-defined function that returns a table data type instead of stored procedures for this? 

I have a stored procedure that updates the source column used in a computed column. That stored procedure has . When the procedure executes, and tries to update the source column, the following error message is received: 

I wonder if it could have something to do with timeouts or the use of ISNULL() on all of the columns (although when i run the query in sql server management studio I get the correct number of rows returned e.g. 16 million + )? Again, I get about 4-8 million rows each time, but never the full amount. I am in a sql server 2k5 db, and running sqlcmd from a remote machine with sql server 2k8. 

Is there a standard definition? In the context of Oracle databases, I have seen people use it to refer to a shutdown normal, shutdown immediate, or shutdown abort followed by a restart. Would it mean the same thing in the context of SQL Server? e.g. stop the service, restart, or kill the service abruptly, restart, etc. Perhaps in all cases it just means to shutdown and restart a database (or instance?, or server?) and depending on the context, can be achieved in a variety of ways. 

On SQL Server (2008), I have a column which is varchar(max). I want to query for all rows where the blurb contains a lettered list: 

Here, ASP_SESSION_ID should be considered a new instance of a session when > 20 minutes passed since the last request. So, how can I group by or rank the same ASP_SESSION_ID differently as they get re-used over time? e.g. if the next request by that ASP_SESSION_ID is > 20 minutes from the last, group it/rank it differently? I am just not sure how to attack the problem. Here are some statements to generate the data above: 

In SQL Server 2008, why does this return the row, even when I add a whitespace, or two, or more, to the end of the where clause? shouldn't zero records be found in the following example? 

Is there a way to determine (from within a stored procedure) which parameters were passed into a stored procedure (vs. those which just took the default value)? 

...so means membership was not active on those days. With this data structure, I would like to get to a "collapsed" set, showing "spans" of time inactive/active: 

I have three columns, , , . One, two, or all columns may be NULL. With the following logic, a single string is made using the SQL below. It is messy, and I am wondering if there is a simpler way to achieve the output? I've considered CONCAT() but I don't think I can use that and still get the separators in there. 

Both seem to return the correct Date when I run it today (2017-06-17). My question is - how does this work? It seems to be getting a count of weeks since or weeks after ? And why does it work with both a and ? Also, are there edge cases one would need to be aware of where it might not give the expected results? 

We stop turning off, or; The source column (the one used in deriving the computed column) is removed from the update statement. 

BOL documentation says sysadmin permission is required to run debugger. I'm 90% certain there is no workaround to this requirement, but thought I would ask just in case someone found a way to grant Debugger permission without granting sysadmin permission. What do people do when you have a team of developers needing to step through a complicated cursor loop with variables, etc to debug some aspect of that? Most shops don't allow developers to have sysadmin permission even on development servers, and many wouldn't allow devs to keep a copy of enterprise data on their local machine with their own developer sql server edition e.g. due to PII and data security reasons. Not sure why the debugger would be set up this way. So, I'm curious how other people handle the requests for Debugger permission in a similar scenario. What do you do in your environment? 

I am trying to find a way to convert an rfc822 string date (with timezone) to either a GMT value of data type DATETIME or a unix timestamp of data type INT. For example: 

I have a new database (powers a new website) in a dev environment. I have everything scripted out to create the DB, objects, and data in my production environment. In that DB, auto create and auto update statistics is turned on by default. Is there any point in scripting the statistics from all of the DEV tables, or will they auto generate on their own in production? I should have specified that I am not migrating an entire db file, but rather, I will be re-generating tables, procs, views, data, etc, in production via scripts... 

At first I thought to just group on the date (at the DAY level), but for the example above, notice how the 2nd instance of the session id spans across midnight. That would cause a 3rd group, when really it's the same session. So if I could rank them properly, it would be: 

Just swap out for your column name, or you can replace with if you want to use a declared variable e.g. if you are making a function or something. It makes use of an XSL transform using built-in XML functionality (since SQL Server 2005) 

I am trying to figure out all of the common bit-wise operations in Oracle. In SQL Server we have some very simple bit-wise operators to use against a bit-wise value: 

In learning how to get the date of a day of the week from the previous week (e.g. the date of Monday from the previous week), I found the following two expressions coming up commonly, where you change the very last number (e.g. 0=Monday) depending on which day you want: 

I will add that if a CITY and/or STATE value exists, then COUNTRY will never be NULL. Here is the code so far. It is messy but I believe it meets the output requirements listed above. I am working on SQL Server 2008. 

I have a column in a table in SQL Server which hold a base64-encoded text string which I would like to decode into it's plain text equivalent Does SQL Server have any native functionality to handle this type of thing? A sample base64 string: 

I recently posted a question looking to find an alternative to granting devs privileges just to use the sql server debugger. I am finding that there really isn't a feasible solution to this. My question here is, WHY does the sql server debugger require sysadmin privileges? I want to understand the reasons behind designing it this way. In a dev environment, developers should be able to do dev activities such as using the built-in debugger tools without granting sysadmin, instead of being told to basically build his own debugging mechanism using print statements, etc. SQL server has privileges for so many specific roles/tasks, etc, it's odd not to see a separate one for this e.g. which could be granted only in a dev environment, etc. I don't think this question is a duplicate of the others, since this question is not about how to use the debugger if you aren't a sysadmin, but rather, it is so I can understand why it is designed this way. 

I want to normalize these all to UTC. I can strip off the leading 5 chars, and trailing timezone string, and they cast to datetime just fine... but my question is: Does SQL Server have any built in function where I can pass in the datetime, the source timezone string e.g. and a target timezone string e.g. and it will handle the conversion? I am thinking of something similar to what Oracle has: 

I can not get this UPDATE statement to work in an Oracle environment. It was written for SQL server. I am looking for some guidance on how to convert it. 

I believe the "UTC" and "GMT" portions actually mean the same thing, and for my particular use case, I believe those are the only two timezone values I will see. I am working in a SQL Server 2008r2 environment. The only thing I am finding to do this is the function below, but I want to avoid that for a few reasons: 

I am hoping to be able to do it without having to maintain a timezone offsets table to map the string values to an offset. I also don't want to hard-code the offsets in some big case-statement. I am on sql server 2014, so I can't use the new for timezone info that comes in 2016. Any thoughts? 

Lets say we have an table where each row is a day, and it is ordered by this day column. Then we have left joined a membership data set showing which day the members were active (and not). Lets say our current data set looks like this... Membership was active from day 3-5, inactive from 5-8, and active from day 9 onward etc. 

I am in a SQL server 2012 environment. I understand that an index on the FLAGS column won't be used in this type of query. I know the other option is to break the FLAGS column out into individual columns, but I wanted to avoid that because in some cases, that would mean 25+ new columns, each of which would need a combination of indexes to support the common queries. Are there any ways to index the single column so that the bitwise queries are more efficient? I tried using a computed column with an index as suggested by dudu below, but when I used in on a table which has a clustered index, and start selecting columns (instead of just select count(*) it no longer uses the index on that new computed column.