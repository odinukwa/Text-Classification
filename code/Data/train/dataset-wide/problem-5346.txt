Your confusion is understandable. Material implication is useful in mathematical contexts and in some scientific contexts where propositions are understood to be certainly true or false, but it is far less useful when it comes to representing ordinary everyday conditionals. One might even go so far as to say that it only works in the special case where the A and B are either certainly true or certainly false. In the real world this is seldom if ever true, and this is why material implication fails to represent ordinary conditionals very well. As soon as things are uncertain, material implication gives completely the wrong answer to simple questions. Suppose I roll a regular 6-sided die and ask you, what credence do you attach to the conditional, "if it comes up even, it will be a six"? Nearly everyone will say one third. This of course is the value of the conditional probability P( six | even ). By contrast, the probability of the material implication P( even -> six ) is two thirds. The example generalises completely. Pick any typical conditional you like, just choose one where the A and B are not certainly true or false, and you will get the same result: the credence you attach to the conditional is the conditional probability, not the probability of the material implication. This has been tested experimentally in numerous trials conducted by cognitive psychologists: by and large we understand conditionals to mean that it is probably the case that B on the supposition of A. I have to qualify this with "by and large" because conditionals are very messy and unruly and there are many strange uses of them in English. This approach to understanding conditionals was pioneered by Ernest Adams in his books "The Logic of Conditionals" and "A Primer on Probability Logic". He showed how this serves to explain the so-called paradoxes of implication, including the one you refer to in your question, i.e. that it is not generally plausible to deduce "if A then B" from ¬A. In my view, introductory logic textbooks do a disservice when they introduce material implication by not immediately warning the reader of its limitations. 

These are straightforward examples, where sentences are being paraphrased to avoid being misleading. By extension, they are part of the whole process of abstracting, generalising and looking for a satisfactory logical form, which is what makes up philosophical analysis. The purpose being to find a form that is simple, powerful and expressive, while avoiding contradictions, confusions, paradoxes and unwarranted ontological commitments. This approach resembles what Quine (Word and Object, section 33) calls paraphrasing, though interestingly Ryle claims that the analysans has the same meaning as the analysandum, while Quine says not. 

(Q => P) => R ¬Q ∨ S ¬S ¬Q ...................2,3 DS ¬Q ∨ P ............4 DI Q => P ............ 5 MIR R ..................... 6,1 MP ¬R ................... Assumption ¬R ∨ T ................ 8 DI T ......................... 9,7 DS ¬R => T ............ 10,8 discharging the assumption. 

This means we don't need to suppose that a ball is remembering the previous falls. Each ball is independent, and the resulting curve (a binomial distribution) emerges spontaneously from it. This is one of many examples of how apparently orderly behaviour can emerge even when there are lots of disorderly things going on at the micro level. Another is radioactive decay: we cannot predict when one atom will decay, but with a large mass of them we can predict very precisely what proportion of them will decay in a given time interval. Another example arises from the kinetic theory of heat: we cannot predict how individual molecules move around, but put enough of them together and we can say all kinds of useful things about their thermodynamic properties. So the gambler's fallacy is a real fallacy, even though it is perennially tempting. My favourite way to test peoples' intuitions about it is to ask them this: suppose I decide to play the lottery every week and my preferred strategy for picking the numbers is to look up the numbers that won last week and choose those. You will find many people who think this is crazy because the chances of the same set of numbers winning two weeks running is tiny. But of course the probability of any set of numbers winning is all equal: it is not affected by the previous week's win. 

This is an interesting and important question and merits a long answer. I shall be as concise as I can consistently with being helpful. The question asks whether we should understand validity in terms of proof, which is a syntactic concept, or in terms of models, which is a semantic concept. Proofs are powerful and able to solve complex problems by reducing them to the application of a few rules. Models are potentially complex and nebulous and involve coming to terms with truth and meaning. So, why not stick with proofs? 

There has been a fair bit of discussion of this statement from Wittgenstein. Kripke in Naming and Necessity famously disagrees entirely and offers "the standard metre in Paris is 1 metre long" as an example of an aprioi contingent statement. A priori because we don't have to measure it to know it is 1 metre long, but contingent because that particular rod could have had a different length from the one it actually has. I'm not an expert on Wittgenstein so I wouldn't presume to interpret him, but he may be making the point that to say (1) the standard metre is one metre long; is quite a different kind of statement from saying (2) a piece of wood in my hand is one metre long. (2) ultimately means that the piece of wood is the same length as the standard metre, while (1) cannot simply mean that the standard metre is the same length as itself, because that is vacuous. Rather, (1) is part of a language game of measuring and has a special status in that game. As to what the father should say to his son, I don't see any problem with the father giving the answer: it is one metre long - that is how the metre is defined. (Incidentally, although the metre was defined that way at the time Wittgenstein and Kripke were writing, it isn't any longer; it is now defined in terms of the speed of light.) 

Your use of "because of" is slightly odd. If we write it using the conventional notation of conditional probabilities, then it becomes P(A|B) is high, P(B|C) is high, therefore P(A|C) is high. In the general case this is not true, so an argument of this form is not guaranteed to be cogent. It may be, for example, that the truth of C renders A highly improbable. One way to understand this is to think in terms of worlds in which the propositions are true. It may be that most C worlds are B worlds and most B worlds are A worlds, but few C-and-B worlds are A worlds. Try this for an example: If Alice spends lots of money on expensive luxuries she'll probably become poorer. If Alice wins the lottery, she'll probably spend lots of money on expensive luxuries. One cannot conclude that if Alice wins the lottery she'll probably become poorer. Another way to express this kind of argument is using default reasoning. By default, if B then A; by default, if C then B. C, therefore A. In many cases this will work, but not in general, because C may be a defeating condition for the conditional if B then A, i.e. it may be the case that if B and C then not A. 

Probability theory can be understood as an extension of the propositional calculus, and even of Aristotelean logic, but not of predicate logic in general. To clarify, propositional calculus is basically the truth-functional calculus of 'and' 'or' 'not' and material implication. Aristotelean logic permits quantification in one variable, so that one can capture statements such as "all A's are B's". Predicate logic is a great deal more expressive and permits quantification in any number of variables in a single sentence: for example it allows one to capture the difference between "there is some girl whom every boy loves" and "every boy loves some girl" and prove that the latter entails the former and not vice versa - not something one could do with Aristotelean logic. If one uses an epistemic interpretation of probability, then one may speak of the probability of a proposition being true. Probability calculus provides a way to deal with ands, ors and nots that is compatible with the propositional calculus, but it does not allow one to peek inside an atomic proposition and say that individual components of it are more or less probable. If a proposition contains multiple quantifiers, there is no completely general way of accounting for how the probability of the expressions within it relate to the probability of the proposition as a whole. Some progress has been made into what is called probability logic and it remains an active area of research. Jon Williamson has written about it in the paper "Probability Logic" in "Handbook of the Logic of Argument and Inference" (Elsevier, 2002) and in his book "In Defence of Objective Bayesianism" (Oxford, 2010). 

Spurious precision. It would be impossible to have percentages that precise unless there were tens of millions of professors in Northville. Correlation vs causation. A declining percentage of female professors over time doesn't mean there is some definite cause that can form the grounds for projecting the correlation into the future. Without some plausible story about why this is happening, it could be just natural variation rather than a trend. The fact that you have just three data points doesn't help either - it is not enough to be significant. Even if you can identify a causal story about why it is happening, it does not imply that it is projectible a long way into the future. People and societies are complex and not so predictable. The reference class problem. Why pick on just philosophy professors over a three year period? Is the same trend observable for other faculties? For other College employees? For other residents of Northville? Cherry picking. How was this data obtained? Was it mined or randomly selected? Was any attempt made to ensure it is representative? Have any other falsifiable predictions been made that could be used to test the hypothesis? 

A question like, How can I prove something exists? must be placed in a context. Who is asking, and what will they accept as proof? In an ordinary everyday sort of way, it might be answered by saying that something is the object of our senses: I can prove this apple exists because I can see it, touch it, smell it, taste it. In less simple examples, one might use indirect evidence - I can't prove that Australia exists by sensing it, since it is a long way away from me, but I could point to photographs, books about it, people who have been there, beer that has been brewed there, etc. Now if the questioner responds, But that's only evidence, not incontrovertible proof, he is upping the measure of what we ordinarily count as proof and playing a kind of game of radical skepticism. There have been several kinds of radical skepticism - one can be skeptical about the existence of other minds, which leads to solipsism; one can be skeptical about the past and believe that the entire universe came into existence a few seconds ago with all my memories preformed; one can follow Descartes and try to be skeptical about everything. Ultimately there is no way to prove such a skepticism to be mistaken, one can only say that it is pointless or unhelpful. Incidentally, you say that you can prove that you exist, but are you sure? Descartes' cogito was famously criticised by Hume who said that he cannot discover in his own experience what "I" refers to. If you are going down the route of radical skepticism you cannot help yourself to concepts like "I" without proof, which will lead to a regress. 

Within aristotelean logic, the difference is that a contrapositive is a categorical, rather than a hypothetical. As your example shows, the contrapositive of the categorical "all A are B" is the categorical "all non-B are non-A". The term transposition is reserved for hypotheticals, so "if A then B" transposes to "if not B then not A". In predicate logic, this distinction is unimportant, because propositions such as "all A are B" are interpreted hypothetically to mean "for any x, if x is an A then x is a B". This is an important departure from its treatment in aristotelean logic, because it means that "all A are B" has no existential import, i.e. it does not assume that any A exist. In consequence, "all A are B" is trivially true if there are no A, and "all A are B" does not entail "some A are B". 

Frege's logic even allows us to prove that the second sentence entails the first. These sentences cannot be written using Aristotle's logic. A noteworthy corollary of Aristotle's approach is that it takes the sentence "all S is P" as having existential import, i.e. it assumes that there are some S. Aristotle has no use for the sentence "all sheep are mammals" if there are no sheep. By contrast, Frege's logic takes the universal quantifier 'all' to be hypothetical, so a sentence of the form "all S is P" might be glossed as "anything that is S is also P". This is highly useful, but it does have the unintuitive consequence that the sentences "all unicorns are white" and "all unicorns are not white" are both true, because there are no unicorns. 

I share your dislike of this kind of argument. If someone wishes to say that they used to believe X and now believe Y and proceed to offer the reasons why they made the change, then that's fine - the reasons are what matter. But it is all too common for people merely to say, well I used to think X but now I believe Y as if that magically makes Y more plausible than X just because they changed their minds. Tell me the reasons or go home. As to whether it is an instance of a fallacy... I'm not a big fan of hunting for fallacies - to me that is just kindergarten logic and if you progress in your understanding of logic then you should grow out of it. But as it happens, it is an example of non-sequitur. The fact that your opponent used to believe something else does not of itself make Y more plausible. 

The way you have chosen to express the rules implies you are assuming a non-monotonic form of reasoning. Rule #1 as stated has no exceptions, while rule #2 expresses an exception to rule #1. In a monotonic system of logic (which includes classical logic) this would lead to a contradiction: if Bob hits Charlie, rule #1 says Charlie may not hit Bob back, but rule #2 says he may. In non-monotonic systems, rules may allow the inference of propositions that hold by default but may be defeated or overriden by the addition of other propositions. In such cases you would need some meta-rules that tell you how to apply the rules. For example, the rules might have some explicit priority value that tells you when one overrides another, or there might be a general consideration that more specific rules override general ones. In your example, rule #1 might then be assumed to hold by default but be defeasible where rule #2 applies, because rule #2 is more specific. You don't need to infer that the rule applies, you only need to check that no defeating conditions are present. If you wished to avoid using non-monotonic reasoning, an alternative approach would be to attempt to express the obligation in a single rule, e.g. "no man shall hit another man who has himself never hit others". You can then infer that if Charlie is a man who has never hit others, then Charlie should not be hit. The kind of reasoning we are using here is called deontic logic - the logic of obligation. Obligation can be treated as a propositional modality and attempts have been made to define formal logics for it, though it has proved highly problematic. The Stanford Encyclopedia has an article on deontic logic.