IQ tests are pretty specific to certain types of intelligence. Just like my mom would destroy me on a "musical IQ" test and I'd be much better on a "logic IQ" test, your species could very well be vastly superior to humans in some kinds of processing, while being sub-par in other kinds. If your stalkers are really good at tracking prey, predicting movement, ambushing their victims, etc., they wouldn't need to evolve highly complex methods of building tools, traps, buildings, and other things humans are good at. To us, they would seem primitive, but the truth is we're just stupid in those regards and need the extra help technology provides. Eventually though, technology would outpace evolution, and we would actually surpass them even in their fields of strength. At this point, they might see a need to duplicate at least some of our education and technology to enhance themselves. This assumes they feel a need to compete with us, however; if they're content to just have their part of the world in peace, they might not change much at all. Your general idea of having different components in the brain is roughly accurate. The brain's mass (or neuron count) will be roughly sub-divided into different regions of expertise. For example, humans have different parts corresponding to vision, memory, spacial awareness, and so forth. Devoting neurons from the "creativity" center of the brain to vision would give your stalkers what they need to see more of the spectrum, while reducing their ability to invent new tools. Be aware, however, that brains aren't really individual components. Each "part" of the brain is highly interconnected, and the distinction between "GPU" and "memory" is a blurry grey area. Also, many parts of the brain have multiple functions, and some functions are distributed to multiple parts of the brain, further confusing the spatial notion of "parts". So it probably makes more sense to think of logical parts, which are loosely correlated to spatial zones. 

If you have a minimum distance between objects (caused by some kind of small-scale repulsive force), the sum is finite (this is why we don't have infinite gravity from standing "on" the ground in 3D) Although it might still go to black hole levels if you add much mass at all with molecular-sized minimum distances. 

Does it reproduce? If so, there would quickly be more than one plant. If not, there are a few problems. First, it's going to die if it's remotely like Earth-life. So it won't evolve, it will just go extinct. Second, a plant isn't one living thing. It's trillions of them, called cells, that reproduce constantly. Supposing the plant has a naturally-indefinite lifespan, those cells would eventually evolve and either destroy the host plant (that's what cancer is), or turn it into something new. The cells could evolve into distinct microbes which could, in turn, spawn entirely new forms of life. 

Count Your Heartbeats. My resting heart-rate, as measured by professional nurses with actual medical equipment, while I'm as calm and rested as I can get, has ranged from ~70 to ~110 BPM. It tends to be in the 80-90 range. I don't know what a typical deviation is for a single person, but it's probably at least $\pm$5 bpm. So 85 $\pm$ 5 is an error of $\pm$ 5.9%. After a year (I'm using 365.2422 days, although I doubt the extra precision matters here), that 's a $\pm$ 21.5 day deviation. That assumes the person going through knows their own average resting heart-rate. It also doesn't take into account the incredibly significant effects of stress (you just got tossed through a stargate and have to survive here for a year), the alien environment (different gravity, oxygen levels, air pressure, etc. will have an effect on your biology), or, as days and weeks pass, the different nutrition you'll be getting, your physical condition, etc. Realistically, most people probably won't be able to be certain of an error margin of anywhere near the $\pm$ 5 BPM range, driving the deviation even higher. Measure Your Period. It's quite typical in normal, everyday Earth life for a woman's period to be off from average by $\pm$ 2 days or more. Average period length is around 28 days, which gives an error range of $\pm$ 7.1% or $\pm$ 26.1 days. Wikipedia puts $\pm$ 4 days as more typical, which is an error range of $\pm$ 14.3% or 52.2 days. The highest typical period length is around 45 days for adolescents, or 31 days for adults. 45 days $\pm$ 2 days gives an error of $\pm$ 4.4% or $\pm$ 16.2 days, while 31 days $\pm$ 2 days is $\pm$ 6.5% or 23.6 days. Like heart-rate, period length will be greatly affected by stress, diet, etc. It's actually a bigger issue here, because the next period will be days to weeks away, meaning your body will have a lot of time to adjust to the new conditions, while the heart-rate measurement could be taken within minutes of arrival. And, period length is usuable to less than half the population. About 50% of the planet is male, and while I didn't bother looking up numbers, young girls and older women don't menstruate, so actual numbers will be less than 50%. Another issue my room-mate mentioned is birth control. Anyone currently taking birth control will have substantial hormonal changes going on once they hit the new planet and have no more pills. Use Your Favorite Music. Obviously, not everyone has heard of John Philip Sousa, let alone memorized any of his songs, but most people know a decent number of songs. Great! But does that help? Not really. I highly doubt the average person can tell you the BPM or running length of any particular song they own. I can guarantee that only a (relative) handful of people are capable of singing through an entire song without missing a beat. Then they have to get the exact right tempo. Certainly there are plenty of musical professionals who could do a pretty decent job. But even then I doubt they'd hit the 24-hour mark. That answer says 120 BPM $\pm$ 2-10 is reasonable for someone with moderate to little musical training. That's an error margin of $\pm$ 1.7% to 8.3%, or 6.1 to 30.4 days. Even the tight end of that scale misses the mark by quite a bit, but is looking good compared to other answers. I'm not a musical expert by any means, and had no luck looking up statistics relevant to this, so I can't comment on how accurate that $\pm$ 2 BPM estimate is. But something tells me that's after sitting in front of a metronome or similar device, and most people probably program that timing with a musical instrument. The lack of any such tools would probably throw the estimate off by more. Again, stress and local factors could have a huge impact on the perception of time and the ability to maintain a consistent rhythm, even for the pros. Measure Hair or Nail Growth. To add to the idea, when I went through basic training the new toe nails were much thinner than my old nails, and I could visually see the time passing. Hair grows around 6 inches per year, while nails grow around 1.44 inches per year. It's probably a lot easier to measure hair than nails in this case, but it's easier to cut into nails to keep them marked in more precise intervals. And there's a big difference between the length of neatly-brushed hair and caveman hair. However, as Wikipedia points out, actual growth time can depend on many factors, such as the stress, etc. on our alien planet. More importantly, I highly doubt many people could tell you their personal growth rates right now with any significant accuracy. It's hard to put an estimate on the accuracy here, but it's probably at least $\pm$ 10%. Hair on different parts of your head doesn't grow at the exact same rate, so you'll have to average it out a bit. Not everyone will have just gotten a haircut when they got pushed through, so the unevenness will be even higher. Plus the biological factors. From freshman astronomy, you remember that the period of Delta Cephei (a cepheid variable) is 5.36 days, doubling its brightness at maximum. This is a pretty good trick, but that requires the person to have learned this fact (or one like it) at some point, still remember it to an exact enough value, and be able to find the star in question. Given that stargates could dump you all over the galaxy (or even other galaxies), this is a very niche technique. If you can use the trick, the $\pm$ 0.01 day accuracy is good to about 2.4 seconds at the end of the year, although you'd likely be off by at least a few hours in trying to determine the center of the maxima and guessing at how long it had been between arrival and the first maximum. But certainly a very good technique. But if you can't use the trick, it's worthless, and the odds of the trick working are very slim. Practice a Speech or Clapping Your Hands. This sounds like a neat idea on its surface, although I doubt it's terribly accurate. Remember that alien worlds could easily affect your speech and muscles by a considerable margin. But the worst problem is it requires preparation. If I was going to prepare, I would bring something with me that was far more accurate, like a watch. Even simple things like calibrated springs, weights and rulers would do a better job. If preparation is allowed, it becomes trivial to calculate time. But that's not what the question asked. Dial a Random Gate for 38 minutes. I thought of this one, and it's not a bad idea. There are still problems. First, this narrows the assumption from "Stargate-like" to "this is the Stargate universe", which is probably too narrow. Second, most people probably never knew about the 38 minute limitation, let alone remember it. But more importantly, it took trained professionals (in-universe, which we need to consider meaningful if we're using the 38-minute rule) decades to get one correct address. The odds of successfully dialing out in the entire year you're there are vanishingly small. However, if it worked and you happened to get a gate open, $\pm$ 1 minute translates to about $\pm$ 2.6% accuracy or $\pm$ 9.6 days. So we've still missed our window, but it's not a bad estimate. Calculate the Period of a Pendulum. For starters, your height is probably much less variable, and a longer length translates to better accuracy in the long run. So I'd use that rather than an erection. But can we actually do it? Not really. The formula for the pendulum makes several assumptions already, so we need to be careful. But let's pretend we can meet those assumptions using weeds and sticks and so forth. We still need to find both L and local g. L is easy enough with something like your own height. Pretty much everyone knows their own height, which is a huge plus, but it's not ridiculously accurate. Best case measuring accuracy is around half an inch, and most people are probably between 5' and 6.5' (60-78") tall. That gives us an error margin of $\pm$ 0.8% to 0.6%, which is quite good, leading to a $\pm$ 3.0 to 2.3 day total error. But we still need to be able to replicate that height on the alien planet, which is probably going to double our error margin to around $\pm$ 6.0 to 4.7 days. But we're not finished. We still haven't found local g. The proposed method won't work once you're on planet. It requires you to know the initial velocity of an object, which requires you to measure distance over time, and time is the things we're searching for. Two unknowns, one equation. If we happened to know how far and how fast we could throw a particular object on Earth (as suggested), it could work. But there are several variables to consider. According to this random site, MLB pitchers (they're literally pros at throwing things) throw a baseball about 92.15 mph, $\pm$ 2.6 mph. That's $\pm$ 2.8% or 10.3 days of error. Then remember that most people aren't pros. And we're throwing random rocks or sticks that won't be the exact same mass and shape. Then there's throwing distance. I can't find any good references, but we're adding at least a few percent here. Finally, there's throwing angle. According to Wikipedia, MLB pitchers had an easy time hitting a strike zone between a batter's shoulders and knees. Let's call that a 4 foot height, which is at a distance of 60.5 feet from the pitcher. That's an angle of $\pm$ 1.9°. Using the $d = \frac{v \cos \theta}{g} \left( v \sin \theta + \sqrt{v^2 \sin^2 \theta + 2gy_0} \right)$ given, with a velocity of 70 mph (we're not all MLB pitchers), $y_0$ of 6 ft, Earth gravity, and a horizontal throw, we get distances between 52.8 and 74 feet. That's 63.4 $\pm$ 10.6 feet, giving an error margin of $\pm$ 16.7% or 61.1 days. Now, the errors given so far have been per measurement. We need to add them all together to get the final measurement. With a tall person (6.5'), that's a total error of $\pm$ 20.8% or 76.1 days. And, yet again, local factors will cause more deviation. (Of note, adding them together means you get a more condensed curve, so it's actually a little better when accounting for whatever arbitrary probability cutoff we're using. $\pm$ 15% or 54.8 days might be a better estimate.) Drop an Object Off a Cliff. Measure Echo Distance. These suffer the same problem that they require a precise, working clock to make your measurements, which negates the purpose of the experiment. Measure Time to Run a Distance. The problem here is it assumes people can accurately determine a half-mile distance and know how fast they run it. But both of these assumptions are flawed. First, normal people don't do that much running to begin with, and can probably guess their half-mile speed to $\pm$ a minute or something. But anything substantially better than that isn't likely. Second, I've met many people who couldn't judge 100 yards within 20%, let alone 1700. This second problem can be somewhat overcome with time (use your height to measure a length of wood, then use that to measure half a mile to decent accuracy -- probably $\pm$ 2-5%). Additionally, even professional runners are unlikely to maintain close to their normal pace when you put them on a random alien planet on rough terrain. Even if they can easily tell they're slower than normal, they won't have any particularly reasonable way to determine how much slower they are. You're still going to be off by at least several percent, and probably 10-20% depending on just how rough the terrain is. This is another place where it's pretty hard to make any estimates, but probably a minimum of $\pm$ 10% or 36.5 days. Find an Atomic Clock on the Planet. This would certainly be an excellent method of telling time, because it's the same frequency anywhere in the universe. However, you'd have to remember off the top of your head that Cesium oscillates at around 9.19 billion cycles per second, and hope the aliens are using Cesium, and that you know enough chemistry to verify that they're using Cesium (they aren't likely to call it Cesium). And there's no guarantee there are people on the planet, let alone an advanced civilization. Using my "Stargate-like" assumptions, we know that most of the gates lead to extremely primitive planets, if the planet is even habitable. So this is another case where the timing is more than sufficient if everything lines up, and worthless if it doesn't. Use the Dimensions of a Dollar Bill. Assuming you have any such items on you, this is certainly an excellent suggestion. But you have to know the length, weight, etc. of such items for them to be useful (I have no more idea of the length of a dollar bill than my own finger). And you have to have these components on you as you get shoved through. This isn't an answer so much as a "how to help with other answers" suggestion. It's certainly good advice, but the question doesn't specify we have these kinds of tools so we can't presume we do.