I have run into a problematic situation. I oversee the administration of 5 php web pages on one server, each having their own database. All pages work like interactive forms drawing and storing data into the DB. They all have the same structure but with different variables and screen outputs. A few days ago between 22:00 and 06:00 (exactly the night when the leap second was applied), one of the web pages did not store any entries into its DB. All the other worked fine. From the error and access logs in apache it looks like that during that night, there was no traffic from the webpage at all, so it looks like a connection problem. The users report that everything was working fine which we later confirmed with camera footage and also several temp files on their computer. These are rests of generated data that was sent into their printer. I tried to check the cable and it is ok. Page also informs the user if there is a problem with the DB. And I do not think that they could work from cached site data for all night long since there would be problems with the DB connection. The data was not deleted because the auto-increment ID's are ok. I also thought of the leap second but all pages use the same date time function and there were no other problems reported after it was applied. Also data was missing even before the leap second was applied. After 06:00 the storage of new data functioned as normal while there was no one that did anything either with the site or the database. Me and my boss have run out of ideas what could be wrong. Anybody experienced something similar? And some server info: 

So i think the best thing without rewriting most of it would be to edit $query_2 to include 0 values 

That is because i display the results using php. The values from the first query are stored to two different arrays in a while loop and the second query to a third array in another while loop. Than they are displayed through a for loop Hence the current queries would result in this 

The optimizer bases much of it's decisions on the statistics for the tables. If they are not up to date then SQL can't make a good choice. Whenever you see poor behavior from queries, when you know that the number of rows involved is very small compared to the overall size of the tables, the first thing you should do is make sure that the statistics are up to date. This is quickly done. The usual method is reindexing the tables, but you can get similar results with commands that specifically update the statistics. The number of times that specifying hints has actually helped queries I've worked with has been extremely small. 

You have yourself quite an interesting project. I've never directly seen anyone try to implement something that large, at least on SQL Server. The more I read your post, the more questions I come up with... Worst case scenario infrastructure-wise (which is actually the best case scenario, business-wise), you need 10K databases times 2k users. That's 20,000,000 users. You aren't going to be successful in trying to manage 20 M SQL Server logins. IMO. Just the sheer number of them, dealing with moving them from server to server, watching out for ID collisions and mismatched IDs, plus I'm not sure how SQL Server would behave with 20 M rows in sys.server_principals. Additionally, your web app is probably going to want to connect as a single, or very low number of, users. IIS can't pool connections unless their DSN strings are identical. One of the attributes of a DSN string is the user name. Different users means no pooling. You will need to roll your own user credential scheme. It will have to be able figure out what tenant a user belongs to and then your web code will need to select the proper database. That user metadata is critical, it going to need to be stored somewhere, it's going to need to be clustered or mirrored, it's going to need to be fast and it's going to need to be well-protected (from a security perspective. IOW, encrypt it.). Assuming that SQL is even a good idea here, I would keep this database away from the instances that server tenants. This helps from a security standpoint and from a load standpoint, though I would guess that once a users is validated and the web app is steered to the correct database on another instance, there will not be any more querying of this user metadata related to that user. Quick question: should two different users, who belong to two different tenants, be allowed to have the same user name? Another quick question: If I tell you that I work for FuBar, Inc., how do you know that? Is FuBar going to give you a list of users and you give them back a list of user names, or are they going to self-provision? You are going to need to go multi-instance. If even a fraction of those users decide to hit the application at once, a single instance will melt. It won't have enough worker threads to run all of those requests at once. If only 1000 users hit your instance at the same time, it will probably run out of worker threads and request will start to stack up and wait. I've seen this happen; the proximate symptom is that new connections will not be able to log into the instance because there are no available worker threads to service them. If this is very short-lived behavior, you app might survive. If not, or your app is fussy, users will get errors. Even if you will not have many tenants to start, you should start thinking about the future and automation because when you see that your server is bogged down and there are 10 new tenants to bring online, it's pretty much too late and your service (and your clients, and your soon-to-be-ex-clients) will suffer until you write your way out of the problem. You are going to need a way to move databases around, from overloaded servers to lightly loaded (or new) servers. Whether or not you can get a window of downtime will depend on your SLA. Are you providing a specific application, like SalesForce, or are these databases just containers for whatever your tenants want to put in? How big are the databases? If they aren't very big, you could just restore from a backup file that provides a template. (This isn't much different than what the model database does, but I haven't seen anyone really use model in a good way since my days with SQL 6.5.) Once the template has been restored to the new database name, you could then customize the new database as necessary for a particular tenant. You can't do the customization before you have the tenant, obviously. If the database is big, you might follow the same basic procedure except you do the restore ahead of time, before any new tenant needs the space. You might keep a couple of these databases around, maybe one per instance. If you keep too many around, this will force you to maybe buy more hardware and/or storage than you need, plus you will have make allowances in your backup/reindex/checkdb maintenance window (or change your code to avoid databases that are 'deployed', but not 'in use'.). If this is your own app, how are you going to handle updates to the schemas? How are you going to keep versions of the database straight with versions of the code, if you are using a single URL that gets to your web app? How do you detect and destroy databases that aren't in use anymore? Do you wait until your A/R group says that someone hasn't paid their bill for three months? If tenants are managing permissions, that implies that they have some understanding of the inner workings of the app, or that your app has a very simple role structure. Using something like Blogger as a rough example, users can (read posts), (read posts and make comments), (... and create posts), (... and edit other's posts), (... and can reset other users passwords), or (... and whatever). Having a role for each of those different sets of rights and assigning a user to one role or another shouldn't be too hard, but you don't want your app running 'GRANT' statements. Watch out for roles that have a hierarchy and depend on inheritance, it can get confusing. If you are promoting or demoting a user, I'd say pull them out of all of the associated roles and then add them back to the one role that they need. Oh, and since you probably can't use SQL's native credential scheme, you are going to have roll your own code here. I think that I've only scratched the surface here, and this post is already too long. What you really need is a book, or at least a whitepaper from someone who has done this. Most of those guys won't be talking, if they view it as a competitive advantage. 

Q 1). Why a simple integer data is taking 4 actual bytes of data + 7 Bytes extra = 11 Bytes. Q 2). Can anybody explain how does a record store in a page. Kindly find the images below: 

I had a question on Log shipping Manual Fail over Let me explain about environment, Primary Server A (with Cluster setup A-P Mode) Secondary server B(DR server standalone). We had enabled Log Shipping on a database TEST from Primary Server and now we would like to test by doing a manual fail over and i had few questions. Log Shipping Jobs are disabled on both A and B Servers and Log shipping is not disabled or removed, Once the Server B is brought online by restoring the all the logs from Server A 

I have a few questions on tables and Indexed tables and its storage, Here is my first image where i have taken an example with Northwind database and Table name is Orders. Orders Table is with 9 indexes and Orders1 table has no indexes which i had replicated with statement(select * into orders1 from orders). 

I had checked the space of table Orders and Orders1 where i found the index size is of Orders Table is more than data size and in Orders1 Only i can find Data Size is less. Question 1:Is index size in orders table is cumulative of 9 indexes? or Its a single clustered index(as shown in below Fig.2) Question 2:Each index partition shown below in Fig.2 has 830 rows is that mean to have duplicate page data on disk or memory? Question 3:Does Non-Clustered index occupy space on disk or only Clustered Index? Question 4:How does fill factor effect on storage of these clustered and non-clustered indexes. How does index occupy size on disk(any references links/books/blog)? 

I have a question on SQL Server page, I have created a new database with only one new table as below Here is the Code: 

I have seen the 8KB (8192 = 96 Header + 36 Row Offset + 8060 Free space) page architecture in SQL Server. When it comes to storing a data record in Page i am confused. In Below table i have create Integer for column ID it should take 4 Bytes but each record size/length is showing in DBCC PAGE Command 11 Bytes. I have created a simple table as below: 

I see that you found that TableDiffGui has bugs. Have you tried using TableDiff directly? You might not run into the same problems. 

I have always found that trying to shrink a lot of space out of a data file in one go either takes an inexplicably long time (with low I/O on the files while the command is running) or completes after I've lost patience or blown through my timebox limit on when I can do the operation. Instead, first get an idea of how many MB you will want to keep (that's going to be data space plus some extra to handle reindexing--if you shrink out all of the free space, it will be painful when SQL auto-grows the file back out during the reindex that you will have to run to fix the fragmentation). Next, write a bunch of SHRINKFILE commands (to save time, you can use a text editor with a good macro or scripting facility or write a small script in tsql, powershell or vbscript) that will shrink the file in small chunks, maybe 1 GB or 500 MB at a time. Start with the current size of the file and slowly move down towards where you want the file to be. As a dumb example, if you have a database file that is 500 GB and you want to shrink it to 400 GB, you would want to use statements more like: 

The versions of Visual Studio that support "Database Projects" should provide a schema comparison tool that can be used against two different "live" copies of the database (as opposed to a "live" copy and the copy that you are working on in Visual Studio.) You don't need to create a full-blown project (although Microsoft pushes you to do so since that is really the point of supporting database projects and it does have it's advantages) just to compare a couple of databases. The comparison tool will show you what is different and can generate a script to "merge" the changes. You don't have to manually keep track of your changes and you shouldn't have to write a significant amount of code to "upgrade" your production database to match your development database. The trick is that Database Projects support for older Visual Studios was provided as a download, while newer versions included it. Some editions of Visual Studio don't support it. They've changed the name of the feature every time they have released it, it seems. "Frankly, I have found the licensing surrounding this feature one of the most confusing things about it. They key thing is that you want to look for support of "Database Projects". TL;DR: If you are using Visual Studio for other things, you might already have a comparison tool via the "Database Projects" feature. If you've already paid for Visual Studio, that makes the comparison tool effectively free.