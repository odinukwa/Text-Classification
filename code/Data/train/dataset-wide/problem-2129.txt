According to the official MySQL Documentation (How MySQL Optimizes WHERE Clauses) MySQL will optimize the WHERE clause as follows: 

If you would adapt that to your 2 Million row table, then the statistics would update roughly after on a SQL Server 2014 or older version. It's slightly different for SQL 2016 and newer: 

The answers provided by previous posters are all correct regarding the syntax to create a new UNDO tablespace and to switch system to the new UNDO tablespace. The Oracle documentation states that the following statement will switch to the new UNDO tablespace: 

You could implement the following solution. Game "Table" This table stores the auto-generated values of the (playing) Tables for the game: 

Solution: Turn off (disk) cache mechanisms. Reference: Disable Windows Disk Write Cache for Data Integrity and Better Performance (Long White Virtual Clouds) 

Have you compared the permissions of the files on master with the files on slave? Have you compared the owner of the files / directories on master with owner of the files / directories on the slave? Have you compared the contents of the directories on master with the directory contents of the files on slave? (Missing files) Are the mounted filesystems on the slave still Read-Only? 

You could try using Paul Randall's script to capture the Wait Statistics occurring during the execution of your script. Based on the results that are returned you can then search top Wait Types and interpret the performance issues. Sample results could look like this: 

You could always combine the two queries into one query and output the required information in one go: 

We have various installations at our shop and we tend to start low with four tempdb files and then add additional files if there is tempdb contention as pointed out by @Kevin3nfs comment, where he references an SQLSkills.com search. Microsoft has a knowledge base article: Recommendations to reduce allocation contention in SQL Server tempdb database 

You can't install the pgbench utilty without installing some required packages. These can be seen in the link you provided in your question: Package: postgresql-contrib-9.3 (9.3.21-0ubuntu0.14.04 and others) [security] Each package that has a depends (red dot) in front of it, is required to install the package you are installing. In your case the postgresql-contrib-9.3 package depends on: 

Hints Just a quick summary of possible ways to provide you with additional information. Job History You might get additional information if you query the job history: 

And even the index creation is modified as can be seen in the documentation at 9.2.1.7 Use of Index Extensions 

Backing up a database involves quite a few parameters, even if you let SQL Server Management Studio do it with your assistance or if you schedule a Maintenance Plan to do it for you. The easiest solution is to verify that the following parameter is set, if you are using the same filen name for every full database backup: 

You might want to consider opening an issue on MongoDB's Issue Tracker and providing the file in the issue. 

So in a really far-fetched sense a Threat combined with a Vulnerability could lead to a breach of data in your Table_Data. And as a result you have a Security Risk. Now having all the relevant data referenced in your central table could pose a heightened security risk from your teacher's perspective, but that is very far fetched. 

...but there was no mention of the share and what it does or what it is for. You enter the name and SQL Server will create the share under-the-hood. FILESTREAM-enabled database When you create a FILESTREAM-enabled database, the database references a filegroup which references a directory (recommended on a separate drive) that has absolutely nothing to do with the share, which was initially created during FILESTREAM configuration. Screenshot FILESTREAM-enabled database script 

2nd instance Because the 2nd instance has a dynamic port for the SQL Server instance and a dynamic port for DAC, the ports have to be retrieved from the ERROR log file in advance before connecting to the SQL Server. See 7 for more information. In this example I will assume port 63390 is used for the SQL Server instance and port 63389 is used for the Dedicated Admin Connection. Microsoft recommends to fix the IP port, thus allowing you to properly configure a firewall 6. 

You can verify the account by executing the following statement on your MySQL instance after connecting with your root account: Prior 5.7.6: 

You might want to consider just bringing the statistics up to date for the indexes that have a high level of fragmentation. Yes, the indexes are fragemented, but they are still indexes and they will still work, regardless of the level of fragmentation. However, the statistics are possibly not up-to-date until you... 

Depending on the wait statistics you find in the results, you can find hints of what may be the issue in the SQL Server Wait Types Library on Paul's site. Then go from there. Depending on your wait stats, you should see what your SQL Server is waiting for. If you don't find any big issues, then your application might be the limiting factor. Disk Alignement Seeing as you had many read aheads without the covering index, you might want to verify that your data disks are formatted with 64k block size. SQL Server reads in Extents which is explained in the Reading Pages information on Microsoft's site. (1 Extent = 8 Pages = 64kB) You can also find information about disk alignment on Microsoft's Disk Partition Alignment Best Practices for SQL Server. This document will also explain how to retrieve the block size with the command: 

Source: Split an Access database (Microsoft Office Website) ...If I use a SQL command within excel and send it directly to the back-end, then I am using excel as the front-end (and that clears the issue, am I right?) Same as above. You are minimizing the risk when splitting an Access database into a front-end (form, queries, etc.) and back-end (data), but you are not eliminating the risk of data corruption. Consider using a database system like Microsoft SQL Server, Oracle RDBMS, PostgreSQL or MySQL to further reduce the risks of data corruption. 

I guess the worst case would be if you were to lose all your data. Consider performing a simple mysqldump to have a backup of your data. Then consider implementing a backup strategy. 

From a (MySQL) database perspective: It depends. If the InnoDB page size parameter is set to 4KB then the maximum tablespace size is 16TB. For 8KB page size the tablespace can grow to 32TB. For more information on the limitations of the tables / tablespace / OS limitations visit these links: 

If a user is reported as orphaned you can relink the SQL Server Login with the Database User by issuing the following command: 

No, when considering the FULL and DIFF backups. No, when considering the TLOG backups, but it will free up the VLFs inside the TLog file, if the database engine has some time to spare. 

3. Add User to Database Role Once you have granted the permission to delete data on all of your tables, all you have to do is assign the users to this database role. 

...you will see that the index is well and truly not used. In the documentation for the EXPLAIN command here 9.8.2 EXPLAIN Output Format they write for the Extra information that: 

You might have to run to alter the permissions of the files in the databases directories. You might have to perform a with a recursive option to change the group / user owner on the file structure(s). Re-install MariaDB on the slave and then perform a restore of the backed up database from the master instance. Mounting the filesystem on the slave with Read-Write access. 

Note that the user (109) belongs to the group (117), which matches the ID from the command. Please Note: Running the and commands will only reset the permissions correctly if the user and group exist. If the user and group exist, then you might be able to reset the permissions and owners to the correct values. 

Reference: BACKUP (Transact-SQL) Depending on your error message, you might find that because you ommitted one TLOG backup file, that the restore is failing. Reproducing Issue Inserting data / creating backups I reproduced your situation on my local SQL Server with the following steps and was able to restore to a point-in-time with a transaction still open during the FULL and TLOG backups: 

But we still don't use individual instances, because if you have to install the Service Pack then you have to do it for each instance. 

You might want to have a look at the following article: Configure and Manage Stopwords and Stoplists for Full-Text Search (Microsoft Docs) 

If you want to use the standard features provided by Microsoft then you might want to consider using the available DMVs (Dynamic Management Views) which are an integral part of Microsoft SQL Server. Best starting point Try out the starting page for System Dynamic Management Views (Microsoft Docs). This page provides an overview of the DMV grouped into categories. The benefit of using these DMVs is that you get to know SQL Server better, just by using them on a daily basis. You don't have to rely on updated third-party tools, because you always have the up-to-date information already at hand. That said, even the DMVs will change over time which is noted in the following statement found on the DMV overview: 

For the Dedicated Admin Connection (DAC); really depends on Browser Service status a) Browser Service ON 

It is essential that both the TCP Dynamic Ports and the TCP Port settings are empty! When done click OK and restart your SQL Server Instance SQLINSTANCE. Your SQL Server instance SQLINSTANCE will now pick up on both IP addresses. Good luck! I wrote a Q & A style article What are valid connection strings for SSMS login box? which explains some of the settings you might encounter when connecting to SQL Servers and their instances. It also briefly explains that the SQL Server Browser service is only required if you don't know the actual port you are connecting to. Added security anybody? 

Restart database instance step by step Then restart the database instance step by step and watch for any issues. 

I have posted answers on two occasions that relate to your issue. Will VSS backups break logchain? - My answer here How can I backup an SQL Server database using Windows Server Backup? - My answer here Checking the 3rd-party backups Basically you have to check the backup history in the database to see how the database backups were created with the 3rd-party software. With the following script you can retrieve some of the information relevant for further investigation: 

Then RMAN is your tool of choice. Yes, RMAN is pretty complex and has a steep learning curve. However, once you have mastered it, you will benefit from a large variety of possibilities that RMAN provides you with, e.g. central RMAN catalog for multiple Oracle instances. Export / Import Capabilities Do you want a dump of a database .... 

It isn't possible to change a Windows Authenticated SQL Server Login to a SQL Server Authenticated SQL Server Login. One reason being that a Windows Authenticated SQL Server Logins retrieves the SID from Active Directory and stores it in the master database. Another reason is that the backslash () is not a valid character for a SQL Server Authenticated SQL Server Login. 

Your calculation for the number of VLFs should be correct. Is your database set to AUTOSHRINK? Are you executing any other maintenance tasks that could be resizing the TLOG size? Auto-growth: yes or no? If you manually grow your TLog file then indeed you could turn off the auto-growth setting. But be aware: What will happen if you are unavailable and the database has to grow? The database will grind to a halt with an error message. Baseline Size At some point in time your database will have a TLog size that will have adequate space for the general imports that occur and the everyday workload and the TLog/Diff/Full backups occurring. Use this baseline size as a starting point when you create a new TLog from scratch. If the database's TLog is really huge then create a TLog with a base size of 8 GB and manually grow in 8 GB steps until you have reached the "baseline" size. Then leave some additional space on disk to allow for one or two "auto-growths" of 8GB size. This will leave you with the best settings according to the article Transaction Log VLFs – too many or too few? where they recommend VLFs to be 512 MB in size. Disk Space If you don't have infinite disk space, then ensure that the TLog file has a maximum size set, that will not fill you disk to the maximum. Monitoring The most important point is to have a monitoring tool in place that monitors the database's "free space", because you never know if a rogue process is going to do things you hadn't anticipated. Transaction Log Physical Architecture There is a good article on TechNet that explains the physical architecture of the Transaction Log. This gives you some insight as to why the TLog rolls over or why it might have to grow. References Important change to VLF creation algorithm in SQL Server 2014 Initial VLF sequence numbers and default log file size Transaction Log VLFs – too many or too few? Transaction Log Physical Architecture