Let $M$ be any $n\times n$ matrix. We define the usual cofactors: $C_{i,j}$ is $(-1)^{i+j}$ times the determinant of the submatrix obtained by deleting row $i$ and column $j$ of $M$. We can write the determinant of $M$ using Laplace expansion along column $p$ as: $$\det M = \sum_{q=1}^n M_{q,p} C_{q,p}$$ Now, for any $k, p\in \{1,...,n\}$ consider the sum: $$W_{n,p}(k) = \sum_{q=1}^n {M_{q,p} C_{q,p}^2 \prod_{j\ne q}{C_{j,k}}}$$ Clearly $W_{n,p}(p)$ can always be factored, with $\det M$ as one factor: $$W_{n,p}(p) = \sum_{q=1}^n{M_{q,p} C_{q,p}^2 \prod_{j\ne q}{C_{j,p}}} = \sum_{q=1}^n{M_{q,p} C_{q,p} \prod_{j=1}^n{C_{j,p}}} = \left(\det M\right) \prod_{j=1}^n{C_{j,p}}$$ But in all the specific cases I've examined, $\det M$ appears as a factor of $W_{n,p}(k)$ even when $k\ne p$. For example, with $n=3$, $p=1$ and $k=2$: $$W_{3,1}(2)=\sum_{q=1}^3{M_{q,1} C_{q,1}^2 \prod_{j\ne q}{C_{j,2}}}=\\ \left(\det M\right)\left(M_{2,2} M_{2,3} M_{3,1}^2 M_{1,3}^2+M_{2,1} M_{2,3} M_{3,1} M_{3,2} M_{1,3}^2-M_{2,1} M_{2,2} M_{3,1} M_{3,3} M_{1,3}^2-M_{2,1}^2 M_{3,2} M_{3,3} M_{1,3}^2-M_{1,2} M_{2,3}^2 M_{3,1}^2 M_{1,3}+M_{1,2} M_{2,1}^2 M_{3,3}^2 M_{1,3}+M_{1,1} M_{2,1} M_{2,2} M_{3,3}^2 M_{1,3}-M_{1,1} M_{2,3}^2 M_{3,1} M_{3,2} M_{1,3}-M_{1,1} M_{1,2} M_{2,1} M_{2,3} M_{3,3}^2-M_{1,1}^2 M_{2,2} M_{2,3} M_{3,3}^2+M_{1,1} M_{1,2} M_{2,3}^2 M_{3,1} M_{3,3}+M_{1,1}^2 M_{2,3}^2 M_{3,2} M_{3,3}\right) $$ It might be worth noting that the second factor here cannot be written as a linear combination of products of the cofactors, with purely numeric coefficients. This is in stark contrast to the case $k=p$, when the quotient is simply a product of cofactors. I am seeking a proof that $\det M$ always divides $W_{n,p}(k)$, and a general formula for the quotient in the non-trivial case $k\ne p$. 

Let me precise my question by saying what kind of answers I would hope for. There is a well-known description of $Res_G^H Ind_H^G V = Res_G^H W$ as follows. Let $S$ be a set of representatives of the double classes of $H \backslash G / H$. For $s \in S$, let $H^s$ be the group $sHs^{-1} \cap H$ and $V^s$ the representation of $H^s$ on the space $V$ where $x \in H^s$ acts by $s^{-1}xs$ on $V$. Then $$Res_G^H W = \oplus_{s \in S} Ind_{H_s}^H V^s.$$ Using this description, one gets using Frobenius reciprocity twice that $$Hom_G (W,W) = Hom_H(V,Res_G^H W) = \oplus_{s \in S} Hom_H(V, Ind_{H^s}^H V^s) = \oplus_{s \in S} Hom_{H^s} (V,V^s)$$ which implies the well-known criterion of Mackey: $W$ is irreducible if and only if for all $s \in S$, $V$ and $V^s$ have no $H^s$-irreducible subrepresentations in common. So assuming we can compute $S$, the groups $H^s$, the representations $V^s$, and how $V$ and $V^s$ decomposes as representations of $H^s$ (this may be hard in practice, but let's assume we can do that), then we know when $W$ is irreducible, and in general we know how to determine the dimension of $Hom_G(W,W)$. (Of course all of this is standard cf. Serre Linear Representations for instance). Yet this falls short of answering the question, because if for example you know that the dimension of $Hom_G(W,W)$ is $4$, that doesn't tell you if $W$ is the sum of 4 non-isomorphic irreducible representations, or of 2 copies of the same irreducible representation. What I'd like would be a way to tell us, by looking at the $H^s$ and the $V^s$ etc, of determine which one it is. Or more generally, how to "read" the decomposition of $W$ into irreducible reps. in terms of the $V^s$, $H^s$ etc. If it is not possible, I would also like an explanation why. 

This answer is an attempt to slightly rephrase Ilya's argument; I would have written it as a comment, but there's not enough room. Given a path $\gamma$ with endpoints $v_0$ and $v_n$, we have a 1-chain $c(\gamma)$, which is an integer-linear combination of edges of the graph. Let $E_{c(\gamma)}$ be the set of edges with non-zero coefficients in $c(\gamma)$; this need not include every edge that appears in the path, since some might cancel to zero in the 1-chain. If we think of these edges as decorated with the integer coefficients they inherit from the 1-chain $c(\gamma)$, we can associate a 1-chain with any subset of $E_{c(\gamma)}$. It's possible that $E_{c(\gamma)}$ as a subgraph of $\Gamma$ contains several connected components that share no vertices with each other, but since the boundary of $c(\gamma)$ is only non-zero on $v_0$ and $v_n$, and it's impossible for a 1-chain to have a single-point boundary, these components would need to include exactly one whose 1-chain's boundary was non-zero on $v_0$ and $v_n$, and all the rest would have to give 1-cycles. Since they're all disjoint, any one of the 1-cycles $\chi$ would satisfy $\langle c(\gamma), \chi \rangle = \langle \chi, \chi \rangle \gt 0$. Assuming now that $E_{c(\gamma)}$ is connected as a subgraph, we can build up a 1-chain $\sigma$ with a positive inner product with $c(\gamma)$ as follows. Starting at $v_0$, pick an edge $\epsilon_1$ incident on $v_0$, and put $\pm \epsilon_1$ in $\sigma$, with the sign chosen to be the same as the coefficient of $\epsilon_1$ in $c(\gamma)$. Then advance to the other vertex of $\epsilon_1$, and choose an edge $\epsilon_2$ such that $\epsilon_2 \ne \epsilon_1$, and $\pm \epsilon_2$ with the same sign as the coefficient of $\epsilon_2$ in $c(\gamma)$ gives a boundary for $\pm \epsilon_1 \pm \epsilon_2$ that is zero at the current vertex. This must be possible (assuming we haven't ended up at an endpoint of the path), since the boundary of $c(\gamma)$ is zero at the current vertex, so the signs of the edge coefficients in $c(\gamma)$ can't all give boundaries of the same sign here. We continue this process until we reach either $v_n$, the endpoint of the path, or a vertex we've visited before. If we reach a vertex we've visited before, we can drop any earlier edges from $\sigma$ and obtain a 1-cycle with a positive inner product with $c(\gamma)$. If we reach $v_n$, there are two possibilities. If $\sigma = c(\gamma)$ then $\sigma$ describes a simple path $\gamma'$ from $v_0$ to $v_n$ with the same 1-chain as our original path, and we can proceed to use that simple path in place of $\gamma$. If $\sigma \ne c(\gamma)$, it nonethless has the same boundary. Since $\sigma$ is supported on a subset of the same edges as $c(\gamma)$, and its coefficients are of the same sign but never greater in magnitude (and must be less on at least one edge), $\langle c(\gamma), \sigma \rangle$ will be strictly less than $\langle c(\gamma), c(\gamma) \rangle$. So we'll have a non-trivial 1-cycle $\ell = c(\gamma) - \sigma$, and: $$\langle c(\gamma), \ell \rangle = \langle c(\gamma), c(\gamma) \rangle - \langle c(\gamma), \sigma \rangle \gt 0$$ Finally, suppose we have a simple path $\gamma': v_0 \to v_1 \to \dots \to v_n$ with the same 1-chain as $\gamma$. (The following refinement of Ilya's argument is something that John described to me in correspondence.) Since the edge $e_n: v_{n-1} \to v_n$ cannot be a bridge, there must be a path joining $v_n$ to $v_0$ that does not include that $e_n$. If we follow that path only as far as the first of the $v_i$ it reaches, we will have a path $\rho: v_n \to v_i$ which uses no edges of the simple path $\gamma'$. We can then append the portion of $\gamma'$ that goes from $v_i$ to $v_n$, call it $\gamma'_i$, to obtain a 1-cycle $c(\rho) + c(\gamma'_i)$ that must have a positive inner product with $c(\gamma)$: $$\langle c(\gamma), c(\rho) + c(\gamma'_i) \rangle = \langle c(\gamma'_i), c(\gamma'_i) \rangle \gt 0$$ 

If you have no idea at all of finance you should begin by reading a book of finance, with only elementary mathematics, which tells you what it is all about. A very standard, well-known text book is "Options, Future, and other Derivatives" by John C. Hull. Now to begin with the mathematical treatment of finance, I recommend for example "Methods of Mathematical Finance" by I. Karatzas & S.E. Shrieve. 

Yes, this is possible. The more natural method is by using Chebotarev's density theorem. If $G$ is the Galois group of $P(x)$, and $p$ is a prime not dividing the discriminant of $P$, the number of roots $k$ of $P(x)$ in $\mathbb Z/p \mathbb Z$ is the number of fixed points of the Frobenius element $\sigma_p$ of $G$. So let $D_k$ be the set of elements of $G$ which have exactly $k$-fixed prime. Cheobotarev density theorem tells you that the number $N(k,x)$ of primes up to $x$ with $\sigma_p$ in $G$ is $\sim \frac{|D_k|}{|G|} Li(x) \sim \frac{|D_k|x}{|G|\log x}$.If you want more precise result with an error term, you need to apply an effective version of Chebotarev's density theorem. Serre's paper "Quelques applications du théorème de Chebotarev" is a good place to start, if you read French. 

Consider the set of integers $A$ which are in between $10^{n^2-n}$ and $10^{n^2}$ for some $n$. Then the upper natural density of $A$ is $1$, because among the $10^{n^2}$ first integers, at least $10^{n^2}-10^{n^2-n}$ are in $A$, so a proportion of $1-10^{-n}$ are in $A$. The lower density of $A$, on the other hand is $0$, for the number of integer up to $10^{n^2-n}$ of $A$ is at most $10^{(n-1)^2} = 10^{n^2-2n+1}$ so the proportion of those integers that are in $A$ is $10^{1-n}$. Now let's compute the logarithmic density of $A$. Each interval $[10^{n^2-n},10^{n^2}]$ in $A$ contributes $\log(10^{n^2}) - \log(10^{n^2-n}) + O(1) = n \log 10 + O(1)$. So if $m$ is any number between $10^{n^2}$ and $10^{(n+1)^2}$, the number of elements of $A$ up to $m$ is $n^2 \log(10)/2 + O(n)$, and when divided by $\log m \sim n^2 \log(10)$, one gets $1/2 + O(1/n)$. Hence the logarithmic density of $A$ is $1/2$. Hence there is a set whose logarithmic density exists and is non-zero, hence whose Dirichlet density exists and is non-zero, but which have lower natural density 0 and upper natural density 1. Of course, your question was about a set of primes, but then it suffices to replace $A$ by the set $A'$ of primes into $A$. But just applying the prime number theorem, we see that the computation of densities above are the same, and that proves that having a non-zero Dirichlet density says nothing about the lower or upper natural density. 

I've found a somewhat nicer proof than the version on the Visual Insight blog. This new approach doesn't entail a huge conceptual breakthrough, but it does avoid having to deal with 3072 individual cases. We have some freedom in choosing $R$ and $S$, but this makes no difference to the resulting sets of axes. We will pick: $$\displaystyle{R = \frac{1}{4} \left( \begin{array}{ccc} 2 & 1-\sqrt{5} & -1-\sqrt{5} \\ 1-\sqrt{5} & 1+\sqrt{5} & -2 \\ 1+\sqrt{5} & 2 & -1+\sqrt{5} \end{array} \right)}$$ $$\displaystyle{S = \frac{1}{4} \left( \begin{array}{ccc} \sqrt{5}-1 & -2 & -1-\sqrt{5} \\ 2 & 1+\sqrt{5} & 1-\sqrt{5} \\ 1+\sqrt{5} & 1-\sqrt{5} & 2 \end{array}\right)}$$ All the powers of these matrices can again be written with a denominator of 4 and numerators taken from $\{\pm 2, \pm 1 \pm \sqrt{5}\}$. We can simplify things a bit by working with integer matrices in 6 dimensions. For each of the four powers of $R$ and $S$, we will multiply the matrix by 4 and then write it as a linear map between 6-dimensional spaces with separate components for the rational and irrational parts of each component of the original vector. For example, for the first power of $R$ we get: $$\displaystyle{R_6 = \left( \begin{array}{cccccc} 2 & 0 & 1 & -5 & -1 & -5 \\ 0 & 2 & -1 & 1 & -1 & -1 \\ 1 & -5 & 1 & 5 & -2 & 0 \\ -1 & 1 & 1 & 1 & 0 & -2 \\ 1 & 5 & 2 & 0 & -1 & 5 \\ 1 & 1 & 0 & 2 & 1 & -1 \end{array}\right)}$$ and for the first power of $S$ we get: $$\displaystyle{S_6 = \left( \begin{array}{cccccc} -1 & 5 & -2 & 0 & -1 & -5 \\ 1 & -1 & 0 & -2 & -1 & -1 \\ 2 & 0 & 1 & 5 & 1 & -5 \\ 0 & 2 & 1 & 1 & -1 & 1 \\ 1 & 5 & 1 & -5 & 2 & 0 \\ 1 & 1 & -1 & 1 & 0 & 2 \end{array}\right)}$$ Suppose we have some unit vector $v$ of the form: $$v = (a + b \sqrt{5}, c + d \sqrt{5}, e + f \sqrt{5}) / 2^{N+1}$$ where $a, b, c, d, e, f$ are integers, with at least one of them odd, and $N \ge 1$. We will work with the integer vector: $$w = (a, b, c, d, e, f)$$ Because $v$ is a unit vector, the components of $w$ will satisfy the conditions: $$a^2 + c^2 + e^2 + 5(b^2 + d^2 + f^2) = 4^{N+1}$$ and $$a b + c d + e f = 0$$ If we multiply any vector of this form by each of the eight $6 \times 6$ matrices corresponding to the four powers of $R$ and $S$, then it turns out that precisely one of those eight matrices will yield a result equal to the zero vector modulo 8, i.e. a 6-tuple of integers all divisible by 8. To prove this, we take the lattice of vectors in $\mathbb{Z}^6$ equal to the zero vector modulo 8, and multiply it by the inverse of each of the eight matrices in turn, to produce eight new lattices: lattices which yield 6-tuples of integers all divisible by 8 when multiplied by the appropriate matrix. In concrete terms, for a given matrix $M_i$, the basis for the associated lattice is given by the row vectors of $L_i = 8 (M_i^{-1})^T$. The original claim can now be restated as saying that every vector $w$ that meets the conditions described above will belong to the union of the eight lattices $L_i$, but no such vector will belong to the intersection of any two of the $L_i$. The first part is fairly easy to show. We can obtain a basis for the union of the eight lattices by forming a matrix whose rows are the union of all eight bases, and then reducing that $48 \times 6$ matrix to a $6 \times 6$ matrix by putting it in Hermite Normal Form (the equivalent of reduced row-echelon form for integer matrices), and discarding all rows containing only zeroes. We will call that matrix $L_U$. The test for the vector $w$ belonging to the lattice whose basis is given by the rows of $L_U$ is that the vector $(L_U^{-1})^T w$ has all integer coordinates. When we carry through these calculations, we find: $$(L_U^{-1})^T w = (2a, b-a, 2c, d-c, e-a-c, \frac{a-b+c-d+f-e}{2})$$ Since $a, b, c, d, e, f$ are integers, the only thing remaining to show is that $a-b+c-d+f-e$ will always be an even integer, given the conditions we've placed on $w$. We have the conditions: $$a^2 + c^2 + e^2 + 5(b^2 + d^2 + f^2) = 4^{N+1}$$ $$a b + c d + e f = 0$$ It follows that: $$a^2 + c^2 + e^2 + 5(b^2 + d^2 + f^2) = 0 \mod 4$$ $$a^2 + c^2 + e^2 + 5(b^2 + d^2 + f^2) - 4(b^2 + d^2 + f^2) - 2(a b + c d + e f)= 0 \mod 4$$ $$(a-b)^2 + (c-d)^2 + (e-f)^2 = 0 \mod 4$$ It's not hard to check that the sum of three squares can only be a multiple of 4 if all three of the numbers being squared are even. So we have $a-b, c-d$ and $e-f$ all individually even, so $a-b+c-d+f-e$ will be even, $w$ will belong to the lattice $L_U$, and at least one of the eight matrices multiplied by $w$ will yield a vector whose components are all divisible by 8. To prove that only one matrix yields such a result for a given $w$, we need to show that the intersection of any pair of distinct lattices $L_i$ and $L_j$ cannot contain any vector $w$ meeting the conditions we've imposed. There are 28 such pairs of lattices. Finding their bases is a bit more involved than finding the basis for a union. First, we need to construct the dual of each lattice. The dual of a lattice $L_i$ is the set of vectors $d$ such that for every $v \in L_i$, the dot product $d \cdot v$ is an integer. Its basis is given by the rows of the matrix: $$D_i = (L_i L_i^T)^{-1} L_i$$ We obtain a basis for the intersection of two lattices by forming their dual lattices, finding a basis for the union of those duals (by joining their matrices and reducing it to Hermite Normal Form), and then taking the dual of that union. If we do this for the 28 pairs of lattices, we find that in 16 cases the intersection of the lattices contains only vectors whose coordinates are all even integers. This violates the requirement that at least one coordinate be odd (which we impose in order that the corresponding vector divided by a power of 2 is in lowest terms). For the remaining 12 pairs of lattices, the requirement that at least one coordinate be odd can be satisfied if and only if one particular element of the lattice basis has an odd coefficient $\ell$ in the sum that decribes the vector $w$ with respect to that basis. But that in turn clashes with the requirement that: $$a^2 + c^2 + e^2 + 5(b^2 + d^2 + f^2) = 4^{N+1}$$ The contradiction appears if we require the equation to continue to hold modulo 8. In each case all but one of the lattice coefficients vanish, and what we end up with is: $$4 \ell^2 = 4^{N+1} \mod 8$$ which is impossible for $N\ge 1$ and odd $\ell$. Because $R$ and $S$ are rotations of order 5, the set of their first 4 powers can also be seen as the set of inverses of their first 4 powers. Because the corresponding integer matrices are multiplied by a factor of 4, a result that is a multiple of 8 corresponds to a factor of 2 in the original matrices. So what we have established is that, given any unit vector over the golden field with a denominator of $2^{N+1}$ for some $N \ge 1$, the inverse of precisely one of the powers of $R$ and $S$ will take us to another unit vector with a denominator of $2^N$. As we repeat this process, we will move back through the tree to ever smaller denominators, eventually terminating with the original cube. This means that we can reach every unit vector $v$ of this form as one of the cube axes or their opposites, and also that we can only reach it via a single path. 

Even without mentioning the relations with the arithmetic and algebraic geometry (motives, if you want), there are many reasons people have been interested in automorphic forms. One point of view is that the theory of automorphic form is a natural generalization of Fourier series. You can see the theory of Fourier series as the statement that $L^2(\mathbb{R}/\mathbb{Z})$, as a representation of the additive group $\mathbb{R}$ acting by translations, is the direct sum of the characters of $\mathbb{R}$ of the form $x \mapsto e^{2i\pi n x}$, $n \in \mathbb{Z}$. There are relatively straightforward generalizations for $\mathbb{R}$ replaced by any locally compact abelian group, and $\mathbb{Z}$ replaced by any discrete cocompact subgroup. The theory of automorphic representations arises when you want to move up to a non-abelian setting. Replace $\mathbb{R}$ by say, $Gl_n(\mathbb{R})$ and $\mathbb{Z}$ by $Gl_n(\mathbb{Z})$ and try to decompose in sum of irreducible representations (they are now longer characters) $L^2(Gl_n(\mathbb{R})/Gl_n(\mathbb{Z}))$. They are the automorphic representations. 

This is true. More precisely, any $E(f)$ has real multiplication by the field $K_f$ generated by the coefficients of $f$, and this field is a totally real number field of degree over $\mathbb Q$ equal to the dimension of $E(f)$. This is due to Shimura and proven in his book "Introduction to the arithmetic theory of automorphic functions" (see chapter 7). The arguments is also explained in more modern references, such as the book on modular forms by Diamond and Shurman and the text on Fermat's last theorem by Diamond, Darmon and Taylor.