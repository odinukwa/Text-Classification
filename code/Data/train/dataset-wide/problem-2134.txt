Don't confuse transaction log backups with differential backups, they have different purposes! What you're calling a "differential backup", whereby you "note all the changes to the cells", is in fact a transaction log. A differential backup's purpose is to keep the size of the resulting backup file small by only recording the information that has changed since the last full backup, and to keep the restore time within your recovery time objective (RTO). A transaction log backup's purpose is to let you replay the transactions to an arbitrary point in time - often, but definitely not necessarily to "the most recent anything to happen". What you're talking about is in fact possible - but you need to restore the full backup, and then restore the transaction logs. If you have the day 1 full backup and all of the transaction log backups between day 1 and day 5, there's nothing stopping you from restoring the day 1 backup and replaying the transaction log until you have the data as it was on day 4. You could also start from the day 2 backup, which would be slightly faster to restore, as you'd be replaying fewer transactions. You could also restore the day 1 full backup, the day 3 differential backup, and then restore the transaction logs to day 4. Edit: OK, your edited analogy makes a little more sense. The answer is then "because you can already achieve what you want with transaction log backups". A differential backup is merely a cheap and convenient way of recording a whole bunch of transaction log activity. It doesn't offer any data recovery granularity that a transaction log backup doesn't offer. There's only so many features that offer "mere convenience" that make it into a product. 

This is easily solved using sqlcmd.exe and scripting variables, if you can call an external utility instead of using You'd just have this in your Query.sql file: 

as the standard query template, which I've found super helpful. You could just get in the habit of doing that, but SSMS Tools makes it really convenient. The plugin isn't free, but it isn't expensive either, and it offers a trial period before you buy it. 

The only way I can think of is, using "Application Name" in the connection string. That is how SQL server recognises different applications. I think, you can append application name in the connection string at runtime using your v$session column. 

Just by looking at the query and predicate ("income in (32,33,34) and status_id = 6;"), you can't be sure that SQL will use covering index. It all depends on cost that SQL thinks is the lowest one. Not sure how many rows your predicate will have, but obviously, using Clustered index scan is cheaper than covering index. 

Changing the below setting and uninstalling some add ons for SSMS 2016 helped me - HKEY_CURRENT_USER\Software\Microsoft\SQL Server Management Studio\13.0\UserFeedbackOptIn Thanks @Kin. 

I am working on the optimization of a SP which contains some business logic using looping. I have removed the looping and converted those piece of code into some simple insert/update statements. Now I've to do benchmarking and compare old and new code in terms of execution time and logical/physical reads. My problem is because of the loop in my old code, how can I determine what is the total no of logical/physical reads. Because in SSMS, I can see thousands of IO stats statements like: "Table 'Employee'. Scan count 1, logical reads 3, physical reads 0, read-ahead reads 0, lob logical reads 43, lob physical reads 0, lob read-ahead reads 0." 

I am tuning an SP and re-written that to optimize. Now I am comparing old code and new code in terms of timing using "setting statistics time on/off". While setting time statistics off, my new code is performing well at least 4 times better than old code (Old code is taking 4 secs and new code is taking 1 secs to execute), but when I am setting time statistics On, my new code is taking approx 12 secs and old code is taking around 7 secs. Why new code is performing badly after setting time stats on? Looks like there is some cost of time stats also and in my new code, that cost is very higher in comparison to old code. Am I right? If so what is that? 

Since the recent patch where TLS 1.0 was disabled and 1.1, 1.2 Enabled , we are having issues where the SSRS in the server cannot make connections to the Database server. 

It depends on what your needs are. Re-organize is a light weighed operation ,generated lesser logs and always an Online operation. While rebuild is online only for the Enterprise edition of SQL Server and is more heavy operation. 

Try making the db to Multi-User from the GUI and not via script in query window.Also run sp_who2 and try to terminate the connection which is accessing the database now. Once the database is accessible to all users, it will not throw the error you have mentioned. 

I have a request to uninstall one of the test instance which is part of Always ON. I googled around for a clean decommission but did not find steps for Always ON Instance uninstallation. Would be great if Steps can be shared here or any link if you are aware of. As per me I would : 

Yes you can do that. Before hand make the log reader job disabled. This will make sure the jobs are not looking for the database. For more clarity look at the below link : MSDN : Replication and Single user Mode 

As per Microsoft, having same service account is the prerequisite : All server instances that host an availability replica for the availability group must use the same SQL Server service account. The domain administrator needs to manually register a Service Principal Name (SPN) with Active Directory on the SQL Server service account for the virtual network name (VNN) of the availability group listener. If the SPN is registered on an account other than the SQL Server service account, authentication will fail. $URL$ 

Index maintenance is very vital when it comes to query performance. So it is very important to have it properly planned and implemented. Along with Index maintenance comes the ballooned log file which is the result of Index rebuild and re-organize. Now planning for Index maintenance is important as it might just trigger another issue of 'Transaction full' unless proper precautions are taken. It depends on you how you want to do the index maintenance, however the solution from Ola hellengren is the best and freely available. Link below : $URL$ With above solution you get multiple options and all can found in the website. Coming down to your queries: