You will also need a corresponding (password database) specification. Using as the driver will use NSS lookups which should include NID data. As you have configured PAM you may want to try the driver. 

You could rsync / into another directory, which will give you a full backup of the production server. You will need enough space for the whole hierarchy. Also you will need to exclude things like /proc, /sys, and other such mount points. Use will exclude your data if it is on a mounted file system. Ideally for this kind if setup, I would identify the data and configuration directories for the applications being used with and cherry pick them. When planning a backup scenario, decide how much data you are willing to loose and plan your backups from there. Are you willing to loose log data? Backing up live databases with rsync is likely to cause problems. The approach I use there is to used the database tools to create a recoverable backup, and then copy that. Consider using to select the directories which need to be backed up. Use the option to see what will be backed up before your first run. 

If you have your content split between the servers using separate directory paths, then proxying would be appropriate. If you have pages from the same directory on both servers, use Lewis's solution. A simple forward proxy may be all you need. Consider caching on the front-end server. 

Try changing the path in /etc/environment. Put /opt/jdk6.0/bin in front of /usr/bin. However, /usr/bin should be a symbolic link to an appropriate Java version. On Debain or Ubuntu you can use update-alternatives to modify then entry. However, in that case you should consider installing the sun-java package and not run Java from /opt. 

These files are prefixed to rest of the configuration. Settings in these files can be used to override many configuration macros. Look of wrapping the definition in the configuration files to identify these. Reload your changes using . You will find the new file as . If your configuration fails to validate, the file will be in same location with different name. (There are are most two files so it should be easy to find. Documentation on the various Exim components can be found in . The directories for Exim documention have names beginning . Man pages are available and can be found with the command . For your case you may want to read the output of , the command that creates the running configuration from the templates. 

Also check the log messages generates when you restart bind. They should tell you what is and isn't being loaded. On Debian/Ubuntu these will be logged to . You should be able to use reload rather than restart to load your changes. Besides you can use the command to resolve names. 

I would put an Apache2 proxy in front of an application server. Apache2 has a specific module designed for this. It adds the appropriate headers to the request for your application to identify the remote user. Apache2 can be used to serve static content, and only pass application requests to the application server. Windows does not implement privileged ports (ports less than 1024). Other than running Jetty as root, some technique is required bind port 80 as root and pass the data to Jetty. Apache2 uses setuid after binding the port so that the process handling a request does not have root access to your system. The sites you have found show some of the options: 

If your processes are leaking memory, you can use to limit the amount of memory a server can contain. This will prevent servers from growing too large. The same effect can be achieved on a temporary basis using the command. It may be best to use to discover an appropriate size, and then set those values in the file. If your server supports it, drop a file into rather than editing . 

Use separate UIDs. Having multiple users with the same UID make it impossible to tell who is who. Any reverse lookups from UID to logname become unreliable. One standard solution to your requirements is to install and use . sudo allows you to grant the ability to run programs as a different user to any users or user groups you wish. The file allows significant flexibility in who is permitted to do what. Actions performed using are usually logged. Some distributions lock root and use to allow users to perform actions which need to be done as root. It is possible and common to allow users to execute , or use the option to become another user. This reduces the audit trail somewhat, but tied with process accounting you can get a very good audit trail. Remember that anyone with root access can usually work around your audit trail. Don't give people root access unless you trust them. 

Make sure the .ht* files are readable by the web server. I test unauthorized access by changing the user id in the file to one I haven't used. 

I would expect any servers that support SSL/TLS connections to support SSL only connections. If the server is listening on the the SSMTP port (465), this port is listening it should support only SSL/TLS connections. Servers should not be SSL/TLS only on the SMTP port (25). The SMTP protocol is not SSL/TLS only. The protocol has been enhanced to support StartTLS functionality. Requiring TLS/SSL would severely limit the server's ability to accept email. I would expect most servers listening on the Submission port (587) to require a switch to SSL/TLS before enabling authentication. This port should require authentication or a local connection before accepting messages. 

You need matching DNS entries. It appears both sites may be resolving to the same IP. The first VHOST defined on each IP address will be the default site, and handle any sites that don't resolve. If you want to use IP based domains, you need to match your DNS configuration. Check the results of and/or to see what your DNS configuration is. If you are using IP addresses as you show, I would expect each IP combination to show up as a separate NameVirtualHost entry. Prior to Apache 2.4 there was a NameVirtualHost directive that could be used to specify VirtualHost IP/Port combinations. If you are using Apache 2.2, try adding matching NameVirtualHost directives before the VirtualHost specifications. You may need to remove the existing NameVirtualHost *:80 directive. You can use the access log for debugging. Adding the local address () to the log along with the site and port () will allow you to ensure traffic is flowing on the the correct IP addresses. For a CGI script contains the local IP for the connection. 

After the dot is when the email is delivered. It is also when your email is scanned. Things that can/do happen include: 

The redirection will again block buffer data before writing it to disk. Much of this will use memory recorded as buffers. Modern file systems may keep several seconds worth of data in memory, before flushing the data to disk. 

This will allow the web server to write to the directory even if it doesn't own it. (I am assuming the group used by your web server is .) Once the file is created, you can change the permission to prevent writing new files to the directory with the command (again run from the directory). 

Any number of headers can disclose origin information. These include buy by no means are limited to: 

Names resolved from DNS are case insensitive. This is important to prevent confusion. If it was case sensitive then we would have eight variants of .com (.com, .Com, .cOm, .COm, .coM, .CoM, .cOM, and .COM). Country codes would have four. If name resolution is case-sensitive for Ping it is not being done by DNS. 

On Ubuntu these options are available but commented out in . You would need to uncomment the appropriate lines in this file as well as uncommenting the line including this file in . This would be an appropriate file to add the configuration to if you have it. It has been my experience that will accept configuration data in any order as long as it is in a file that is included in the configuration. The command will display the configuration data after includes and comment removal. 

For security reasons paths like are generally not allowed when the span outside the document route. For a URL like they should be acceptable; but for a URL like they should not. I get requests for paths like . If this was honored, the requester would have a list of valid user ids on my system that they could try to crack. As already noted, the error log is a good place to look for problems like this. 

ICMP shouldn't cause amplification unless you allow and respond to request on broadcast addresses. DNS amplification is possible and would likely be a DDOS attack on another site. You may be able to filter out those addresses. Try shutting down your DNS server to see if it resolves the output issue. If so there are a number of steps you can take: