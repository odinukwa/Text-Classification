I have a query on Oracle 11g of the following type, which results in a series of highly inefficient full table scans: 

to clear the cursor cache (an admin had to do it!). There are probably other ways to do this on user or SQL statement level (the above is for the whole database server), which I did not understand the quick way. An alternative may be: 

Imagine, you've tried to change this. In the beginning, you didn't know NHibernate and how it works, but then you came up with ideas how to adapt data access to it's real abilities, and avoid unneccessary database operations: map relations in NHibernate, keep sessions and transactions open for several object operations, do set/bulk operations, normalize the DB the way you've learned years ago, add foreign keys, views, maybe Materialized Views to it. But you keep being rejected, with arguments like: "nobody is going to pay for it", "the database can handle it, no matter how 'bad' the application is", and simply "it works". Disk space, memory, CPU power and network resources are cheap; refactoring data access would be much more expensive. Likely, standing by the code programmer's object oriented approach, rather than the DB programmer's set based approach, is preferred (including it's enforcement against the ORM implementation). What does it matter if the system could be 10 or even a 100 times faster, if it works sufficiently in the current way? Don't care about SELECT N+1 anyway, today's databases can handle it! That would only be gold plating! It might become different when databases grow into terabytes, but that's not for now. So, maybe, there is a solution in the "NoSQL" or "NewSQL" area. It might be possible to fetch objects from and store them in a database in a fast and efficient way. Even with many queries in a single object, rather than set approach, as long as it is a local DB without long distance latency. It looks like the current system uses the relational database just as an extended, persistent main memory, and all those "stone age relicts" of IT, like creating and maintaining tables and indices manually, or mapping objects to relational tables, just add a huge overhead. My idea is: A "NoSQL" document database is a good thing, because: 

SQL Server will auto update the statistics if the default is left enabled. The problem with that are the thresholds (less of a problem with your SQL Server 2016). Statistics get updated when a certain amount of rows change (20% in older Versions of SQL Server). If you have large tables this can be a lot of changes before statistics get updated. See more info on thresholds here. Since you are doing CHECKDBs as far as I can tell you can keep doing them like before or you use the maintenance solution for that as well. For more information on index fragmentation and maintenance have a look at: SQL Server Index Fragmentation Overview Stop Worrying About SQL Server Fragmentation Considering you storage subsystem I would suggest no to fixate to much on "external fragmentation" because the data is not stored in order on your SAN anyway. Optimize your settings The sp_Blitz script gives you an excellent list to start. Priority 20: File Configuration - TempDB on C Drive: Talk to your storage admin. Ask them if your C drive is the fastest disk available for your SQL Server. If not, put your tempdb there... period. Then check how many temdb files you have. If the answer is one fix that. If they are not the same size fix that two. Priority 50: Server Info - Instant File Initialization Not Enabled: Follow the link the sp_Blitz script gives you and enable IFI. Priority 50: Reliability - Page Verification Not Optimal: You should set this back to the default (CHECKSUM). Follow the link the sp_Blitz script gives you and follow the instruction. Priority 100: Performance - Fill Factor Changed: Ask yourself why there are so many objects with fill factor set to 70. If you do not have an answer and no application vendor strictly demands it. Set it back to 100%. This basically means SQL Server will leave 30% empty space on these pages. So to get the same amount of data (compared to 100% full pages) your server has to read 30% more pages and they will take 30% more space in memory. The reason it is often done is to prevent index fragmentation. But again, your storage is saving those pages in different chunks anyway. So I would set it back to 100% and take it from there. What to do if everybody is happy: 

Tasks has indices on Id, ObjectId; ObjectAffectingTasks has index on both ObjectId and TaskId. All joined tables have proper indices, too. The ObjectAffectingTasks table contains task IDs affecting the object, but having another in the ObjectID, so all Tasks affecting object with ID 12345 shall be selected. When analyzing the query, it seemed to be the OR condition that spoiled the execution plan. Where clauses with only ObjectId or only the subquery were using all the indices. Another workaround was to create a Union subquery, which also used the indices: 

Please replace 'insert FQDN here' and 'insert NetBIOS name here' with the actual FQDN and NetBIOS name keeping the double quotes. 

To be honest there is no definite answer to your question. If you do the last fallback to N1 you tested it on all available nodes. Within limits, more testing seems to be better to me. But to be honest it might not be absolutely necessary. Leaving it on N2 will get you the added benefit of using the N2 node for primary workload until the next update. It might even be feasible to do the fallback to N1 and then do another failover to N2 to test if N1 is running and then have N2 as primary until the next update. In conclusion: I don't think it is absolutely necessary to the fall back to N1 but if you can afford the time during your maintenance window I would consider just doing it (and maybe even do the failover to N2 afterwards). 

I am considering to apply a fix which will generate constant parameter sizes for (n)varchar parameters. The application is based on NHibernate with the old OracleClientDriver (using Microsoft Oracle driver, ODP.NET can't be used in the near future), and NHibernate runs on top of ADO.NET. This driver creates parameterized SQL with the size of (n)varchar parameters set to the actual string size, which, of course, varies: 

This is not the original code I used, so check for syntax errors first. I also checked for an existing sequence first and dropped it, if there. If somebody knows a fast working solution for SQL Server versions without sequences, that would still be nice to see! 

You could be dealing with a delegation/impersonation problem as DimUser suggested. If your SSIS is just fetching data from one DB server and delivers it to the other the solution is much easier and should be to set up an SQL Server Agent Proxy. If you are executing the SSIS Package from Visual Studio or SSMS by hand the package will try your Windows credentials to log on to the SQL Server. If your account has the correct rights it will succeed. If you set up an SQL Server Agent Job to execute that package the service account running SQL Server Agent executes this package. He does not have your Windows credentials so he will try to use anonymous login resulting in this error message. To enable SQL Server Agent to execute a package using a different account you have to set up three things: 

So, even a few results with all sub-elements quickly make hundreds or thousands of rows, and if the parent table has tons of heavy NCLOB/BLOB/NVARCHAR(MAX) columns, there's a HUGE amount of data to be transmitted as duplicates. Some object-relational mappers work around this by SELECTing child and grandchild rows by their parents' IDs (the good ones loading them in groups/batches, not single SELECTs for each parent row!), but this is likely not transactional (subordinate/child rows may have been changed after parent rows were selected, not possible in a join - and of course you don't want to do this in "serializable" isolation level). A possible solution would be to transmit join results in a different format: instead of duplicating parent rows, they would transmit them only once, plus a reference between them and subordinate rows (likely the original PK/FK). In the most simple case, multiple result sets from all joined tables would be transmitted, with each row (or row projection) only once, to be easily transformed into objects by the client application: 

You need to enclose CertStoreLocation, Subject, DnsName and FriendlyName with double quotes. There is no need to specify a location as it will default to "Local Computer/Personal/Certificates" where it needs to be in order to use it by SQL Server. This will generate a valid certificate on Windows Server 2016 that will be usable by SQL Server 2017: 

Create a credential Set up a proxy account Configure the SQL Server Agent step to use the proxy account 

It is provided by Tim Ford in his blogpost on mssqltips.com and he is also explaining why updating statistics matter. Please note that this is an CPU and IO intensive task that should not be done during buisiness hours. If this solves your problem, please do not stop there! Regular Maintenance: Have a look at Ola Hallengren Maintenance Solution and then set up at least this two jobs: 

One remarkable thing I found out with a similar query on SQL Server 2008 R2: The query plan contained an index scan on a completely unrelated, nonclustered index, like one on the pr.RecommendedPrice column only. My idea is that the unrelated, nonclustered index contains references to the clustered index rows (pr.ID, pr.CategoryID), and it's cheaper to get these from a nonclustered index scan, rather than from the actual clustered index. Am I right in my assumptions? 

perhaps somebody can add the correct syntax how to remove shared cursors for a table or user this way, and tell if full admin rights are required for this. 

Create a credential Connect to the SQL Server that should execute your package. Right-click Security and select "New Credential...". Enter a descriptive name (Credential name), select the Windows account you intend to use (Identity) and enter the password. Click OK. For initial testing, you can use your own Windows account. For production use, I would suggest creating a dedicated AD Service Account (with minimal permissions). Set up a proxy account Expand the SQL Server Agent folder. Open Proxies and right click on "SSIS Package Execution" selecting "New Proxy...". Enter a descriptive proxy name and use the credential you created earlier. Configure the SQL Server Agent step to use the proxy account Open your Agent Job, select the properties of your step executing the SSIS Package. Now you can select your proxy account in the "Run as:" drop-down list. Additional setup: If your package is deployed to the SSIS Catalog you need to grant the Windows login you used for the credential the "SSIS_Admin" role on SSISDB as well. For that, you need to create the account as a regular Windows login in SQL Server (Public) and map the user to SSISDB using the SSIS_Admin role. 

Edit: This question is about how to deal with many issues that arose from a overall system design, which made parts of the system deviate from common standards. For example, managing everything in the business model with own program code, even down to relational integrity. This gives the database and persistence layer a smell of bad design, by using it as a place to "dump something in and get it out again somehow", rather than a structured storage. I asked the question, because NoSQL document storages seem to me like an option to move an already schema-less (or very loose schema) database to something without schema by default. Also, I must note that the whole system is not a bad one at all, despite some of the flaws described here. Also, some of the problems, such as versioning, have solutions on the way or already implemented. Think of a software system you look at, based on classic, relational databases (SQL Server, Oracle), NHibernate as object-relational mapper (ORM), a business logic model layer on top and a huge number of modules (a few 100), mainly .NET based services and a few web services (with clients, max. ~100 per system/customer, company network, non-public). Operation style is mainly OLTP, with write/CUD access being an importand part of the workload. Productive databases are usually some 10GB, but always well below 100GB in size (thus no "big data"). It does it's work well, but to me, the database and ORM implementation smell of several anti-patterns (for relational databases). Maybe these implementations can be better with another kind of database â€“ document oriented ("NoSQL"), or in-memory database.