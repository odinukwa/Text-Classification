The process is writing to stdout, the actual output of this however is the input to the mail command. If we check this: 

Pretty sure I've seen something similar. If you've had a command run as part of a job that runs on that host (like a "Client Run Before Job" statement), if that command has paused, or stalls in some way, it breaks bacula even if you restart the agent. If this ends up happening, the "before job" command inherits the listening file descriptor that the agent runs on and keeps it open. If you restart bacula, Windows doesn't ever seem to care that there is already a process that is bound to the listening address bacula should listen on and lets bacula start up. However all the traffic that you send to bacula is actually being received by the stray command from an old instance of bacula, not bacula itself. To fix this. 

This alters the behaviour of the file using POSIX ACLS so that both your FTP user and apache can alter / remove / create files inside of that directory regardless of who the orginal author was. ACL masks are defined by the standard unix bits for group. ACLs can tell a directory is a diretory as it inevitably has a +x in it. A file will typically not so even if you have an ACL set as rwx inside of it the +x part will be masked by the group permission bit not being set. Another way is to use suPHP, suExec or some other 'change user on execution of program' method. But personally I find those methods less convenient than this. 

This stuff denotes memory fragmentation. This basically shows you how much memory is contiguously available. And here is your problem. Of free normal memory, you have no more than 32kb of available contiguous memory. Your memory is horribly fragmented. This means if any application needs to allocate more than 32k of space, there is no memory -- so oom killer arrives to kick something out to give memory to do this. So, what can you do. The clue to that is this: 

Interesting question! As a reference, this page from the VM subsystem author will give you a good idea what is likely to happen; $URL$ Note that this is actually very difficult to answer in its current form (if your are talking about file backed caches) because its dependant on how hot the cache is, how long you cache items for and the 'heat' of each object in the cache. Assuming the following:- 

Your problem is your running in the user_t domain as root. user_t does not have access to su. Change your user to the staff_u user, that should make it go away. 

There doesnt appear to be any relevent policy that permits this by default. This is because pam is loading modules in the sshd_t context and trying to do funky stuff not normally associated with something sshd_t types should be doing. To fix this stick the below in a file, probably calling it mysshd.te; 

It basically requires that the XFS block size be at least equal to the systems page size. This means two things. 

Yeah, it will stop the prompts being sent to the terminal when starting a web server. And yes it does pose a security risk because where before the certificate was encrypted it is now in plain text. This means it might be possible to steal a completely working certificate from the machine. Whether this poses a significant security risk to you depends on what the repercussions would be if it happened to you and what the you gain from doing it this way. If its more important to you that services should restart gracefully even if unattended than the security of the SSL system overall then its a straight forward answer. Personally, I find keeping decrypted copies of SSL certificates overall has more pros than cons for my typical workload, heres why; 

You need to relabel the folder to be writable by httpd. If http is the only thing (and say ftp or ssh) that will be writing to it then this should work. 

This command returns no results. No transitions out of init_t for a process. You should also avoid running this type in too since its not appropriate for the service. As a general matter of fact -- the best solution would be to properly write a confined policy for you're service, however this is beyond the scope of this answer. Instead, you need to get your process out of to another type. Because no policy exists for this application it is probably best to move this into , for which a type transition does exist. 

is used for this purpose and its typically ran from init scripts. Its basically without the PAM integration. 

If I had to guess, I would suspect that the vast majority of HTTP requests when the server is "choking" is blocked waiting for something to come back from tomcat. I bet if you attempted to fetch some static content thats directly served up by apache (rather than being proxied to tomcat) that this would work even when its normally 'choking'. I am not familiar with tomcat unfortunately, but is there a way to manipulate the concurrency settings of this instead? Oh, and you might need to also consider the possibility that its the external network services thats limiting the number of connections that it is doing to you down to 300, so it makes no difference how much manipulating of concurrency you are doing on your front side if practically every connection you make relies on an external web services response. In one of your comments you mentioned data goes stale after 2 minutes. I'd suggest caching the response you get from this service for two minutes to reduce the amount of concurrent connections you are driving to the external web service. 

Oh dear! No swap! So, memory thats committed -- just stays there. Newer kernels these days actually 'defragment' memory to make region of memory contiguous, older ones dont do it. You had 70Mb of memory that could has been swapped! Plus this wouldn't have occurred all in one go but gradually so would not have been a hit for you. But no swap, so no luck. You also have little memory for pagecache which is also bad and slow for your system. This potentially could of given a lot more free contiguous space too which would of been nice for you. My advice to you. Get yourself 768Mb of swap. Honestly, you really do your kernel a disservice by not enabling it. Swap is really important for releasing unused memory (a quarter of it in your case) and also would have avoided the nasty fragmentation problems you've experienced as memory could have been swapped out and released more contiguous space. And even if it did get swapped back in, it could have been put back into a region of memory which give you larger contiguous gaps. 

No, probably not. One would expect to see a derivative loss for all other hops thereafter (so around 10% loss on hops 5, 6, 7, 8 and 9) if it was the case node 3 really was dropping forwarded packets. 

The reason this works for SSHD is because after you have logged in you are being given the source context of for which this rule applies. This does not work for the FTP service because the source context of this service is most likely which has no matching rule. As such, you'd need to modify the policy to alter the behaviour of SELinux to also honour the named file rulings you see in the other entries for FTP too. In Fedora 23 at least, there exists a interface to permit this, a policy module like this would do it. 

Some of the characters in your command are shell meta-characters. You are effectively starting one task in the background. You need to quote your command. I've re-ordered the command since its a big ugly. 

I believe in RHEL6 (before systemd anyhow) session management is tracked by . Try and see if that shows you anything. For completeness, on Fedora 16 and 17 this feature was deprecated in favour of which you can list sessions with using . 

Number of calls Percentage of calls Amount of real time spent on all the processes of this type. Percentage. User CPU time Percentage System CPU time. Average IO calls. Percentage Command name 

This works because apache will change directories into the virtual hosts document root prior to doing any work inside of the virtual host. So whichever virtual host has that document root is the culprit. If you have more than one virtual host using the same document root you'll need to investigate both vhosts to see what they are up to. Since I have no apache workers taking up CPU time all of them are 0.0 and thus no ordering is taking place. If the problem is actually an SQL query dont expect this to show anything. 

There are a couple of possible ways you can do this. Note that its entirely possible its many processes in a runaway scenario causing this, not just one. The first way is to setup pidstat to run in the background and produce data. 

Bear in mind, trusting third party sources for your IP might be problematic especially if what your doing with that data has special meaning. A more trustworthy way is to pick a known, trustworthy DNS server (ideally running DNSSEC) and query the hostname of the box with it, providing your DNS server contains such entries; 

Bit of a shot in the dark but I've seen this before and managed to debug it. Its a real bizarre edgecase though. If your clocksource is set to jiffies certain processors do not stay in sync with one another, leading to a situation where the time on one processor differs from the time on another processor. It can be out be a difference of only 1 second for this to cause a problem in mysql. You can test for this by doing: 

The best way to acheive this level of seperation is to not use type transitions but category / MCS transitions. This acts a little like the implementation in the libvirt KVM stuff. OK, the first thing you're going to need to do is download a httpd module called . Its been floating around in the fedora repos for quite some time, but has never really made it into the EL6 systems unfortunately. In any case, you can rebuild the package from fedora sources. I did this on a fedora machine but you can just download the same package from a mirror. I used F16 as a base as it runs . 

So, now you know how, I must point out that your developer is doing it wrong. If non-blocking sockets are something they want to use, thats fine - however they should setup an on the socket and block on the poll instead. The program gains nothing from on a non blocking socket that produces -- as a matter of fact, the result is worse because nearly all system calls are a preemption point where the kernel can context switch you anyway. This developer is wasting power, CPU cycles that could be used for idling threads and is not actually gaining any benefits he/she things they are from doing it this way. If the developer wants to be 'cache-line' friendly, pin his tasks to a particular CPU and be done with it. 

This will mean that your problem (emulate buffered I/O on a non-buffered write) will be resolved. That is because the new 'limit' on your FIFO will in effect become the speed of whatever utility is writing what is in the pipe to disk (which presumably will be buffered I/O). Nevertheless, the writer becomes dependant on your log reader to function. If the reader stops suddently reading, the writer will block. If the reader suddenly exits (lets say you run out of disk space on your target) the writer will SIGPIPE and probably exit. Another point to mention is if the server panics and the kernel stops responding you may lose up to 64k of data that was in that buffer. Another way to fix this will be to write logs to tmpfs (/dev/shm on linux) and tail the output to a fixed disk location. There are less restrictive limits on memory allocation doing this (not 64K, typically 2G!) but might not work for you if the writer has no dynamic way to reopen logfiles (you would have to clean out the logs from tmpfs periodically). If the server panics in this method you could lose a LOT more data. 

Providing your not pinging yourself the TTLs that come back are the TTL values of the received ping packet. There is little you can do to manipulate that.