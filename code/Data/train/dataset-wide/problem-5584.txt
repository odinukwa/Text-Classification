I think that Catch-22s (Catches-22?) are different from paradoxes, contradictions, or circular reasoning, though they do resemble these things, and are interesting for this reason. It seems to me that a Catch-22 consists essentially of: 

What systems of inference rules (i.e. what formal systems of logic) seem to be robust to the Liar Paradox? What works consider such rules of inference? 

It should be noted that describing calculus as an analysis of 0/0 is, at best, such imprecise short-hand as to say virtually nothing — similar to saying that the two-slit experiment proves that electrons can be in two places at the same time. In each case, the description is certainly one way to hurriedly describe what's going on, but omits absolutely all of the machinery and crucial concepts which would make it instrumental — and therefore worth saying. With this in mind, if we are operating on the level of stretching of descriptions in which calculus is a theory of 0/0 (via the concept of a limit of ratios, e.g. in computing derivatives), then it is also a theory of 1/0 (by that same concept of limit, both in computing ratios and in integration). Much of the interest in foundations of mathematics was spurred on by what was considered to be weaknesses in calculus, indeed by the fact that it originally dealt in such concepts as 0/0, whereas it is a student's exercise to show that admitting 0/0 as a signifying expression immediately allows one to prove 0=1 (and is therefore problematic if you prefer to distinguish between such numbers). In place of limits, we may use a modern development of infinitesimals to recreate calculus in the original spirit of Leibniz and Newton: but while an infinitesimal is smaller than any positive real number and may be describable as 1/infinity (for a sufficiently precise definition of "infinity" as a number by which one may divide), an infinitesimal of the sort useful for calculus is still not zero in any non-trivialising classical model of logic. We may construct the negative numbers only by abandoning the notion of a lowest number, and the rationals by abandoning the notion of a smallest; whereas the complex numbers demand the abandonment of the idea that numbers can be ordered in any way at all, consistent with having any meaningful relationship to multiplication. A similar extension to allow 1/0 or 0/0 to signify would require us to abandon either the idea that multiplication and division are inverse operations, or a relationship of multiplication to addition. To obtain a mathematical theory with utility, it proves better to use subtler approaches than division by zero, and to leave 1/0 as a gloss as imprecise as the words "big" and "small". 

I have revised my answer somewhat, condensing in some places and adding other ideas, in response to your revision of your question. Consider first, numbers involving one "conceptual shift" as alluded to in the quotation by Hofstadter. You may well complain that while there least can manifestly be 300 of something (e.g. markings of hollow circles), the best of our knowledge there can never be measurably 10300 of anything. Let's grant for the sake of argument that the latter claim is true. All this means is precisely what is implicitly taught to students of physics in secondary school: that we have mathematical models of the world which, because they are simple, fail to capture complications which are inherent in the world. Just as we contemplate perfectly rigid spherical cows falling in vacuum under the influence of a perfectly uniform gravitational field, we can concieve of a number of objects which is so large that we cannot actually imagine concretely what such a collection of objects would even look like, and which are unlikely ever to represent phenomena that we will ever encounter. The reason for both is because of the simple formulation of the models in both cases: Newtonian mechanics on the one hand, arithmetic on the other. The notion of piling on layers of conceptual shifts, as suggested by Michael Dorfman in the comments to his own answer, is akin to Knuth's up-arrow notation. But the crux of this, and even of our familiar Indo-Arabic numeral system, is that we only deal with numbers through representations of them (even if those representations are through visual images of objects such as apples). A googol is, in very ruggedly practical terms, unimaginably large (in that a googol of objects is not something you can really imagine), and a googolplex is unimaginably larger than that (in that it is not really possible to imagine how many boxes of a googol objects each would suffice to make a googolplex). But the fact that we can represent them by 10102 and 1010102 means we can still talk about them, and somehow conceive of the numbers abstractly. Is being able to write such absurdly large numbers in such a way cheating — does it hide the fact that we cannot fully grok the significance of these numbers, somehow? Well yes, it does perhaps hide the fact that we don't really understand these numbers except to recite their names, to point out trite things such as that they are multiples of 2 and 5, and are prefect squares, etc. But this isn't cheating; we also understand numbers such as 300 less perfectly than we do the number 3, and use the same extension of our cognitive powers to try and come to grips with 300 by imagining three groups of ten groups of ten. Nearly all mathematics, even arithmetic, is indirect in this respect, and while some people may be able to grasp more numbers somewhat directly, we ultimately rely on highly compressed descriptions of numbers to reason about quantity. As such, we are limited in our contemplation of numbers to those which we can easily describe somehow; and we can only reason about those numbers as well as our representations allow. Multiplication was hard in ancient times for those relying on Roman numerals; and similarly our representation of a googolplex gives us little intuition as to e.g. what the next largest prime after a googolplex is. As with Borge's library, "most" numbers don't have a simple representation; and even those that do may have superficially similar representations which make them hard to meaningfully distinguish. In fact, if a 'simple' representation has to be of at most some length, then all but a finite number of numbers are beyond all human ability to reason. Does this mean they escape logic? Well, it certainly means that we can't reason with them; but it also means that we will never have to worry (or more to the point, we are unable to worry) about their properties in any productive fashion. Again, as with the books in Borge's library, most numbers are gibberish; they have no particular importance to us. If you suppose that 'logic' is a human concern about the structure of the world, and that reality simply 'is', then worrying about the potential illogicality of numbers which are so large that they cannot be represented in reality is to worry about a counterfactual, and so not of any importance except how much we are entertained by the question. 

We might represent time then by a pair of labels: a label E for the epoch, and a label t for the time within the epoch (as measured from some fixed event during that epoch). Most importantly, we must be able to describe an order on the labels of the epochs: for instance, we may label them by numbers. These labels might be drawn from 

What you indicate is that the tome which allows John to simulate communication in Chinese is a rather tremendous computational resource: one which is very close in complexity — assuming that its rules are complex enough to successfully years of conversation in the same way that a Chinese essayist might — to simply conferring with a Chinese person. And I think that you are substantially right. Even though the book itself is a static object, it encodes rules to simulate an interactive process (by its very construction!) and so is not very distinguishable, in terms of its value as a conversational resource, from a Chinese person. Obviously a book is not a conscious Chinese person. I think we can say with some confidence that a book on its own isn't any sort of conscious entity. But a book such as Searle envisions would be an informatic resource of tremendous power; of absurdly enormous power, in fact. (And someone who was patient enough to successfully use it would be no slouch at computation, or at least the reliable execution of complicated rule-systems, either.) One of the major missteps by Searle is to imagine that we can even conceive realistically of what this book (or its user) would have to look like in order to achieve the purpose he projects for it. It would essentially have to be as complicated as a rather sizeable part of a human brain, another thing that we don't understand on any level other than an essentially syntactic one. There's a reason why consciousness is such a mystery. We don't have the first clue of how to understand the world in such a way that we can have both predictive power, and also the ability to use that predictive power to describe what consciousness even consists of, aside from experiencing it ourselves and describing the symptoms of apparent consciousness in others. By saying that computers cannot be conscious because we know of no mechanism that would allow it to arise from mechanical evolution is also to summarize the problem we have with understanding our own consciousness. Scott Aaronson, professor of computer science at MIT, has very similar things to say. The following is an excerpt from a set of notes from a lecture of his on artificial intelligence, from an excellent and much wider-ranging set of lectures. 

What you're touching on, of course, is a couple of basic facts about epistemology — and how they impact the activity of proseletysation: the attempt to get someone to believe in an idea (whether religious or secular) which they not only did not know, but did not even concern themselves with, before. The short version is that because original discovery of knowledge is difficult, we have systematically done out best to substitute discovery with learning from others. This makes knowledge socially contingent, and therefore based on reputation. Discovering knowledge is hard. So hard, in fact, that we have developed several interoperating institutions whose purpose is to preserve knowledge which we have won, and to try to disseminate it much more efficiently than it could be independently re-discovered by anyone who might need it. Meanwhile, we pay researchers moderately generous salaries (much less than programmers, medical doctors, and financiers, mind you, but generous considering their outwardly apparent output) to take the risk of brain damage involved in spending long periods of time trying to discover things. Knowledge of sufficiently complicated topics is socially constructed. The ideas of 'trust', 'faith', and 'teaching' are predicated on the idea that we can take as candidate "facts" things which we have never experienced, but have been told. In this sense, we defer the obligations for knowledge to others — we make knowledge dependent of social relations. Application to proseletysation. Suppose you want to apprise someone of a new idea. In trying to provide them with 'knowledge' by social means, you must have one of the following two resources: concision, or reputation. If your claim is outlandish, but the explanation is short and sweet, I may humour you just out of curiousity because the cost to me is low. Otherwise, if your explanation is long and complex, I will only investigate it if I have some good reason to give you the benefit of the doubt: for instance, if you are widely recognised by society as a trustworthy speaker on the subject (e.g. you are a famous scientist, you are a teacher at the school and your audience is your class, etc.) If a conspiracy theorist wants to convince you of something, they should either present it very calmly and soberly (or convince you of short separate pieces of their reasoning which can stand alone) in order to build up your trust in them, or they must provide a very short and compelling explanation of their theory. Otherwise, even if their claims are true, they are asking you to make an unreasonable expenditure of your time and energy as an information-gathering agent for one very specific piece of knowledge — if indeed it is knowledge, i.e. a faithful representation of reality. This is true not just of conspiracy theorists, but also of religious enthusiasts, and technical enthusiasts — it is unreasonable to ask your friends to share one's excitement for (or vehement rejection of) Jesus Christ as your saviour; and unreasonable to ask one's relatives to develop highly specialized knowledge of the operating system on their computers, if they have computers. There are exceptions in both cases if it is highly relevant to their lives, but if it requires them to spend a lot of attention or to overturn a lot of ideas of how things work, then it will be labour-intensive for them, and one's request should be made giving this fact due respect. In short, anyone who is asking you to believe in something, is asking you to make an effort. They are requesting a favour from you (the aspects of what makes belief in an idea a 'favour' is itself an interesting idea, but never mind that) in the attention you spend on them. If they want you to do them that favour, the onus is on them to make sure their request is reasonable, in that it is not a social imposition. If they develop a reputation for social impositions, that's unfortunate — but repairable, to the extent that any reputational damage from social transgressions can be repaired. On noisy learning environments. On the case of penicillin and the Pasteur Institute mentioned by Michael in his earlier answer: we have in that case the unfortunate situation of an environment which was, for the Pasteur Institute, hostile to learning, in that it was subject to a lot of noise by (possibly even well-meaning) cranks. It is not immediately clear what they should have done in the face of such stimulus. There are similar problems in computational complexity and proofs either of P = NP or P ≠ NP: because there are so many well-intentioned (but poorly trained) people who continually attempt to prove it, and who cannot even be induced to learn from their mistakes, the very question has a taint upon it, so that there is a high reputational burden involved in being taken seriously. Thus we see a second-order ethical obligation to conspiracy theorism (or interests which diverge from the norm, generally): not only should you be respectful of the attention which your audience is literally paying to you, it is your social responsibility to be careful in how you engage in your conspiracy theorism, lest you poison the well for others who may come after you, and thereby inadvertently hurt society. Again, I think that this responsibility is not just upon conspiracy theorists and amateur scientists.