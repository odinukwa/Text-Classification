If in fact it isn't what Frenchie said, which it most likely is, it may be helpful to look at that users mail. Cron mails output from cron jobs to the user account of that crontab. That is why you often see STDOUT and STDERR piped to /dev/null, so they wont be mailed output they don't care about. You can use the mail command as that user to check for mail with helpful output. Also, the /var/log/cron file may include helpful information. 

As far as I know that will just allow snmpget requests for this data. How do I enable traps based off this info? Thanks! 

Transparent proxy that you can enable and disable on the fly would do it. Since some applications do not explicitly support proxies there are pieces of software that you can install that will force a piece of software to use a proxy. They basically intercept any network activity and instead route it over the proxy you configure. For instance some older IM clients do not support a Socks proxy so people use these pieces of software to overcome that limitation. First google result: $URL$ 

You could also run a script that periodically dumps the username:passwords into a file and use the pam_pwdfile. There are a slew of choices. $URL$ 

If this rails info is in a mysql dbase you can configure pam_mysql. There are pam modules for just about everything. Here's one for ftp that I have with mysql: 

I'd like to centrally remove all non-active snapshots on a vMA host before performing central backups of VMs. I am hoping such a script exists already??? Anyone know of one? Edit: Here is a script using basic vMA tools to do the job: 

which upon search path resolution becomes the same fqdn. Anyone have a way to expand shortnames before calling ssh, or an alternative approach? 

Why doesn't that answer work for you? Each business is different so 3sec load times may be a max for some where 8sec is acceptable for others. In the end "pageviews, queries, visitors" is going to be specific to the application being run and they all add up to load time metrics. Shared will work till you start receiving enough requests per second that the page load times become "unacceptable". Where unacceptable is somewhere over 6sec, when visitors start hitting the back button to google. How many requests/sec the shared host can handle before load times become unacceptable is dependent on the shared servers hardware and configuration. You are at their mercy. I personally don't see why more people aren't using VPS solutions like linode and slicehost. At $20 a month you have root access. What that means is you can run tests on drive IO performance, cpu performance, etc. Additionally, you are more secure than whatever chroot environment a shared hosting provider places your business critical application in. You can recompile php to make use of opcode caching with APC, etc. You can decrease load times by optimizing server performance instead of upgrading to higher tier shared hosting. Basically, from a math standpoint it would seem to me that shared is only a good value to those who wish to do little if any administrative work. The value is with VPS where solid, secure, flexible VPS solutions start at $20 a month. Perhaps I'm biased but this is serverfault.com so I'm in like company. 

I want to get an automated process for AMI creation going, and one piece remaining is automatically cleaning up the instance after image creation. The instance is booted with a user-data script that does the necessary setup, then kicks off image creation from self using AWS CLI. Then it shuts down. I could go with option and wait there until the image is ready, then terminate, but the docs state that "file system integrity on the created image can't be guaranteed", so I want to avoid using it. What's the best way to kill the instance from itself after image creation is completed? 

How can I tell (in ) if I'm running in interactive mode, or, say, executing a command over ssh. I want to avoid printing of ANSI escape sequences in if it's the latter. 

When creating an autoscaling group I can choose an ordered list of termination policies for its instances. Amazon's documentation states that 

It's single availability zone no backups not in a security group that's reachable from the outside world 

But it glosses over the specifics of how these policies are combined and when the "fall through" happens to the next policy in the list, i.e. under what conditions each policy fails and moves on to the next policy in the list. For example, I have a policy list in my group and yet after scaling up and then down, the scaling group proceeded to terminate by newest (and healthy) instance (newer by a large margin), and I can't figure out why. Additionally, according to the same doc, default policy is actually itself a combination of policies, and includes and as two of its steps. If I have a list that includes , does it evaluate and twice? Lastly, does the termination consider load balancer? For example, if my new instance failed to initialise properly and is not in-service with the load balancer, and is in effect, will scale-down action kill the unhealthy instance first even though it's newer? 

If a directory "foo" is owned by user A and contains a directory "bar", which is owned by root, user A can simply remove it with , which is logical, because "foo" is writable by user A. But if the directory "bar" contains another root-owned file, the directory can't be removed, because files in it must be removed first, so it becomes empty. But "bar" itself is not writable, so it's not possible to remove files in it. Is there a way around it? Or, convince me otherwise why it's necessary. 

Please, I am not looking for a rehash of what's stated in RedHat's documentation regarding emergency mode. I would like to know what steps are involved from the time grub hands off to the kernel to the time you get a emergency mode login prompt. I imagine /sbin/init is completely bypassed and therefore rc.sysinit bypassed as well. I don't however know what isn't bypassed or how emergency mode differs (intimately) with init=/bin/sh. What sort of steps does the kernel take when given the emergency argument at boot time? Thanks! 

That will list which services are listening on which IP:port. You may need to modify apache's listen directive like so: Listen 0.0.0.0:80 And verify iptables is configured to accept traffic destined to port 80. If the server is setting on an internal network where you do not have access or permission to modify the router...then you should not attempt to make that server available publicly. It would most likely be against company policy if you could get it to work. Also, instead of giving out your home IP address to everyone (since it's going to change/expire at some point) you can instead give out a dyndns address. See dyndns.org If this wasn't what your looking for then please give additional detail in your question. 

You can do reverse lookups on each IP in the range and search through the results for the domain you're looking for. However, this will only work if the person/s responsible for that domain created a PTR record for the IP. This is a quick and dirty working example: 

Fake DNS server pythons script used Also, I used the dig command and the host command. I verified they make use of the resolv library. I tried this on CentOS 5.6 as well as on my personal ubuntu with totally different versions of the related packages. I'm totally stumped here, need some help on this one. 

Does the originating/source IP show up in that log output? If yes does that IP show any valid requests in the http logs? Perhaps a monitoring system of some kind is checking http on your server, since you said it was in consistent intervals. Just throwing stuff out there. 

This means that you CAN just change the progname and it will look for a pam file of the new name. Not a good security practice and I am kinda surprised by this. Maybe someone knows something I don't..since the OpenBSD guys are a much smarter bunch than myself. :p UPDATE 2: Verified that PAM servicename is set to the basename by doing the following from the console: cp sshd to sshd2: 

I am trying to use AWS autoscaling lifecycle hooks in a template that encapsulates the following things: 

I cannot use UserData, because anyone can read it. I cannot use private S3 buckets for the same reason (metadata and hence credentials can be accessed by anyone on the box). I'd strongly prefer not to bake my own AMI, as it's quite a hassle. 

As I understand it, an instance needs to be granted access to resources in order to do anything with CloudFormation. But when I run this on a Beanstalk web server instance: 

What would be the best way to pass sensitive data to EC2 instance (on boot or otherwise) that only root can access? 

Basically what it ways on the tin, how can I create individual per-instance alarms inside an auto-scaling group created with a CloudFormation template? I can reference the ASG itself in an alarm and create ASG-level alarms, but cannot seem to specify dimensions to be "any EC2 instance belonging to this ASG". Is it possible or is my only option user-data script? 

I don't specify any access/secret keys in the command line. My instance role was manually created (by me) and definitely does NOT grant any permissions on resources. 

I want to forward TCP connections on a certain port of the machine A to another port on the machine B (which is actually the same that originated the connection to machine A) and simulate random or deterministic packet drops. How can I do it with iptables? 

Is it possible to buy an intermediate certificate to use it to sign subdomain certificates? It has to be recognised by browsers and I can't use a wildcard certificate. The search turned up nothing so far. Is anyone issuing such certificates? 

How come I can still read any CF metadata? I noticed that in the client code, the script goes to use instance credentials ( is True) 

Within plain EC2 environment, managing access to other AWS resources is fairly straightforward with IAM roles and credentials (automatically fetched from instance metadata). Even easier with CloudFormation, where you can create roles on the fly when you assign a particular application role to an instance. If I wanted to migrate to Docker and have a kind of M-to-N deployment, where I have M machines, and N applications running on it, how should I go about restricting access to per-application AWS resources? Instance metadata is accessible by anyone on the host, so I'd have every application being able to see/modify data of every other application in the same deployment environment. What are the best practices for supplying security credentials to application containers running in such environment? 

We have outgrown the use of a single server for monitoring our network so we are looking to add another monitoring server. Unfortunately, we were shortsighted when we initially configured the hosts being monitored. We configured the hosts on our network to only allow monitoring from a single IP. To add another IP to all the hosts firewall rules is not logistically possible. I won't go into why but suffice it to say the servers have a mix of owners and cannot be updated at will. So initially we were thinking of just having an OpenBSD box doing NAT with PF in front of the two servers. We're not thrilled about the additional complexity or, most especially, the single point of failure. So, can we do some magic to allow both servers to use the same internal IP address without adding another host? Also, the system needs to continue working if either one of these hosts go down. So doing the NAT on one of the two hosts and routing the second server through the first isn't really enough. We basically need failover source NAT in front of the shared IP...without adding two additional boxes. 

Every publicly accessible web server receives requests like this all day long. They're just blindly attempting known exploits against your server. What I often do is configure the web server to display a blank page when it receives requests to it's IP (i.e. $URL$ I only allow the sites to appear when the correct virtual host domain is requested. See what site appears when you access the web server by IP instead of by domain name. Most of the exploit scripts crawling the netter-tubes aren't performing valid virtual host requests(proper virtual host headers). You can also look into the various utilities that will automatically block IP addresses that attempt nefarious requests. 

SFTP is NOT a feature-rich solution comparable to an FTP server like vsftpd. It doesn't support chroots; which is what you are looking for. FTPS (not SFTP) would be the best solution since it supports encryption, chroots, etc. vsftpd supports this and it's easy to setup. In addition be sure to take advantage of the pam_listfile module to explicitly state which users are allowed to login via ftps. 

What is an open source load balancer that allows for hash based balancing? I would like to do hash based load balancing of a URL but first remove the user specific arguments from the URL. Basically want to increase Varnish's cache performance by adding URL to node persistence. Example: example.com/foo/usertoken/bar Where the hash would be based on: example.com/foo/bar.