Start reading the first 2 articles and then the third one has step by step method with screenshots. The high level steps would be 

It makes the log file reusable so that other transactions can use it or as per Brent's analogy someone else can use the drawers to keep there stuff. After going through the answer I strongly recommend you to read about transaction log on SQLSKILLS.com 

The number of such machines are high There is no application owner, so we are not sure where exactly changes needs to be done(a bit tricky situation). SQL Server cluster does not supports renaming if database is replicated. 

So you can see it is not the free space present in pages allocated to Heap but the varying sequence of pages that creates the fragmentation. This can be demonstrated by small test. Let us create a Heap Table and insert some records in it and then check the fragmentation. 

You also have SQL Server installation files located at Check file. PS: This whole process is waste of time I would suggest get SQL Server access and make your life easier 

You can use standard edition in SQL server database mirroring but you cannot use Snapshot functionality to get report as Mirror server is always restoring and you need to create database snapshot on mirror to run reports but snapshot is enterprise only feature. If you use transaction log shipping with Secondary database in standby mode you can run reports although when log backups will be restored users will be disconnected. 

This does not really indicates a problem. What it means is SQL Server has issues more number of physical read and writes than the threshold you have set. Now next question is how often do you get the alerts, if this is often you might need to find out whether you are facing memory crunch and what queries running are causing this. 

Yes you can take full backup on secondary replica but you have to make sure that they are COPY_ONLY full backups. As already pointed in BOL article shared above. 

This basically means there was meta data corruption and checkdb tried finding out page ID and index information but was no able to, so got the value as zero. 

What I can feel is there are other services running on SQL Server machine and sometimes there occurs situation where memory requirement increases and thus forcing OS to take hard step of paging. I suggest you also add more RAM on windows machine 

If you are using any such feature you would have to stop using it, take fresh backup and then restore. I know its hard to tell because starting from SQL Server 2016 Sp1 standard provides far more features support. So at the last the answer would be you need to restore and test. Good news is SQL Server 2017 RTM Evaluation Edition is released so you have the Eval edition for your testing. 

I guess previous users were taking file system backup which in my opinion is not best practice when you have important app running. You can never get point in time recovery. Your thinking is correct about creating a maintenance plan backup it gives you more control on backup. Plus if you don't want point in time recovery you can put database in simple recovery mode and can just have full and differential backups as per RPO and RTO. This would avoid the hassle of transaction log file backup and unnecessary growth of log file 

I must say first that using fn_dblog and fn_dump_dblog is unsupported as these commands are not supported by Microsoft. Jonathan Kehayias in his article even pointed out that using fn_dump_dblog multiple times has grave performance implication so use these undocumented commands on your own risk. There is also a way to track page splits, not perfectly though but will help, using DMV Sys.dm_db_index_operational_stats the columns leaf_allocation_count in output points to page split which occurred for index and nonleaf_allocation_count gives you page allocation due to page splits read the documentation please. You can include object(table) for which you want to track page split by editing the query Below is the code which may tell you page splits for index and heap does not have page splits they have forwarding pointers 

Yes very much and believe me its not a hassle it will save you from lot of hassles. As already pointed you are running on unsupported version of SQL Server. Let me tell you personal experience, suppose you face any issue which comes eventually as a MS bug and you plan to ask MS to pay you for your loss you cannot because you are running unsupported version. The legal agrrements are little more complicated and I will not talk about it. Also even if you raise a case with MS for some support related activities MS engineere will straight away say its unsupported version first please apply SP2, YES not even SP1 SP1 is also not supported, then only they will proceed further, after you have applied SP2. There was a critical bug fixed in SQL Server 2012 SP1. 

Please note that the backup history is kept in the msdb database of the replica the backup got executed on. This implies that backup history and chain cannot be retrieved out of one instance only. Differential backup can only be done on instance which is primary node. Restore will work as normal but I must tell you cannot just restore database which is part of availability group you need to first evict it out of AG and then perform the restore. At last I suggest you to read MSDN blog series on Availability Groups backup and restore by SQLGardner for more details If you are backing up lot of big databases it does create lot of I/O so your idea to configure backup when load is relatively very less is correct. Normally during midnight or during maintenance windows load is relatively very less you can physically look at server to see the load and configure backup at that time. 

You have standard edition which cannot take advantage of fast recovery which is present in enterprise edition. With fast recovery database can come online after second phase of recovery(the redo phase) however it is alwasy not the case so please read the article. As an additional reading you can read When is fast recovery used. Standard edition limitation could be one more reason why it is taking database time to come online. 

No this is a temporary solution. I guess you posted same question before could you please tell what is total size of database you have in your SQL Server instance. Size of tempdb depends on how much your queries are using it. It cannot grow by its own unless you use it. Links shared by Aron would help but you need to tune queries if they are heavily using temdbb or may be its the default requirement of your environment. I have seen few env. where 200 G of tempdb was acceptable because queries required that much amount of tempdb space. 

There is no LIMIT set as such for express edition. However when connecting from application connection pooling comes into picture, if enabled. In connection pooling, after a connection is created, it is placed in the pool and it is used again so that a new connection does not have to be established. 

You must run SQL server 2012 upgrade advisor and generate report before doing migration. This report would list out all features deprecated and which would not work moving ahead in SQL Server 2012. There is an option to allow few old features to work in SQL server 2012 and that would be by keeping compatibility level of database to 100 after migration. As a good practice you should change compatibility level of database to match that of Server. In case you want to change compatibiltity level please read Alter database Compatibility level. The good thing with compatibility level is it can be changed immediately if something goes wrong. About compatibility level please note that 

No there is no option or restriction which we can enforce so that a user can take copy_only backup when ever he tries to take any backup but what you can do is create a procedure which backups database with copy only option and grant execute on the procedure to various users and then remove backup database privilege from the user. BACKUP DATABASE and BACKUP LOG permissions default to members of the sysadmin fixed server role and the db_owner and db_backupoperator fixed database roles. So move accordingly. Its really not a good practice to allow general users to backup database as per there mood but I guess requirements vary as per different firms.For more details read Security section in This Link 

The owner running the job is removed from AD Someone changed the recovery model of database. Insufficient disk space 

What is type of mirroring Sync or Async Yes there would be impact and impact would be in terms of amount of transaction log being generated. Delete logs each row which is being deleted so logging will be heavy. Your approach should be to delete in batches this would minimize lock escalation, blocking and thus would not hamper concurrency much. You should also consider performing this activity when load is relatively less How strong is your network connecting principal and mirror. You are going to put heavy load on it if you delete 100G table so becareful with that aspect also its good to ask network team to have a look on network during this period of time you would not like your mirror to lag behind principal. Can you truncate the tables or selectively truncate tables, it is super quick and logging is very very less as compared to delete but NOTE there are limitations with truncate table and before proceeding please read below Microsoft documentation $URL$ 

REORGANIZE is always a online operation and single threaded operation but does not holds blocking locks. When you reorganize there is option to if you keep this value off you might get error like index cannot be reorganized as it cannot take page level lock so its better to reorganize index with option . So yes table is available for read write operation locks are taken as pointed in the option but as I said it does not holds blocking locks As per Microsoft Books Online Turning off page and row locking might or might not be acceptable because the weekly batch update will block the concurrent readers from accessing the table while the update runs. If the batch job only changes a few rows or pages, you can change the locking level to allow row or page level locking, which will enable other sessions to read from the table without blocking. If the batch job has a large number of updates, obtaining an exclusive lock on the table may be the best way to ensure the batch job finishes efficiently. 

I would suggest you to read this blog.msdn article by Bob Dorr. Some important points he emphasized is 

Simply because it does not have any idea about the status of transaction, like whether it will commit or rollback so it does its job of backing up enough transaction logs so that if transaction commits it can bring database to consistent state after backup is restored and if transaction does not commits before backup is finished the backups takes it as uncommitted and when it will be restored their would be no information about changed made by that transaction in restored database. Read The First Myth pointed out by Paul Randal 

The answer would be simple here, assuming you just have one clustered index on table and your query is like 

I believe it is possible to restore with same old answer that subject to condition you are not using any "enterprise specific" features. This you can get from 

300 mark should be completely ignore. The base PLE should be calculated like (Max server memory*4)/300. This should be your benchmark PLE anything much lower than that should bother you. Moreover if you have NUMA system you have to track PLE for 

Yes the changes can be done without taking users offline. But please note that this is command which will lock the database so if any schema changes are happening it would be blocked. It would be better to run this query when load is relatively very less, just so that it finishes very quickly. Also note that when you enable legacy cardinality estimation the procedure and plan cache will be flushed and new queries would be recompiled using old CE. This will force SQl Server to do some extra work and you might see slowness for a very small period. 

As I can read your question you got this message previous year and one occurrence now. Keeping its mind in occurrence I would suggest its not that grave issue something running on OS sometimes requests more memory and SQL Server becomes victim. Workaround The workaround would be here to provide SQL Server service account Locked pages in memory privilege. This will avoid paging out of buffer pool but non buffer pool can still be paged out. Solution. 

I am adding this answer as it could be helpful and related to Inplace upgarde First you need to check using the link Thomas gave whether inplace upgarde you are doing is supported or not. After you are sure you meet the upgrade matrix you can proceed with two considerations 

Please note that restore 'verifyonly' does not checks consistency of backup completely only a successful restore will guarantee that you backup is in consistent state. There is a option to use Continue_after_error can you please use that in your restore script. 

To start with you have SQL Server 2012 and memory architecture changed significantly in this version there is only one memory allocator (yes there is VAS allocator as well but I still would say there is only one 'Any Page' allocator) and this does Almost All memory allocations. Here is what BOL has to say. Please read this definition clearly 

Index rebuild will update statistics of column with a equivalent of full scan. Reorganizing index will not update statistics at all. For more detailed explanation Ben Nevarez has written more in his blog 1) By default, the UPDATE STATISTICS statement uses only a sample of records of the table. Using UPDATE STATISTICS WITH FULLSCAN will scan the entire table. 2) By default, the UPDATE STATISTICS statement updates both index and column statistics. Using the COLUMNS option will update column statistics only. Using the INDEX option will update index statistics only. 3) Rebuilding an index, for example by using ALTER INDEX … REBUILD, will also update index statistics with the equivalent of using WITH FULLSCAN. Rebuilding indexes does not update column statistics. 4) Reorganizing an index, for example using ALTER INDEX … REORGANIZE, does not update any statistics. The same can be verified from Paul Randal Post