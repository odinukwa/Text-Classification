The answer to the question as formulated in the last sentence is a resounding no. Language does in no way represent or reflect any underlying truths about the world. In fact, the very definition of language is that it represents one way of reflecting a particular way of perceiving the world. Different languages represent this reality in different ways. Perhaps the one universal tendency is that much of language reflects certain realities of the human body. However, even here languages will differ in the particulars. At best you can say that some language structures reflect certain social realities. But this will mostly be limited to vocabulary and phraseology. At any rate, any such correspondences can only be reconstructed, never predicted. Plus, you make a very simplistic link between knowledge, names and physical reality. You could say that all three are constantly interacting in a mutually constructive way. But there is no chain of mirror like reflections. Ultimately, you need to problematize both your notions of language and knowledge. Once you see them in all their complexity, the question will make less and less sense. 

The difference between polysemy and homonymy is often one of degree or the direction you approach them. They are difficult semantic relationships to fix with certainty even when it comes to lexical items let alone constructions with more abstract meanings (like tense or affixes). But the examples you give are examples of polysemy NOT homonymy. You have one suffix -an with 2 related senses of relationship to root. 

The term 'sense' is used in at least 2 (related but distinct) ways in linguistics. One in lexicography, it's simply used to refer to one of the 'meanings' listed under a lexical items. The second comes from the Fregean tradition of distinguishing between reference (the thing the word denotes, refers to, points to) and sense (what makes the reference meaningful in the context of a proposition - I'm being very liberal here). One synonym for sense is connotation. And from this perspective, the definition makes sense. Connotation can be thought of as a result of comparison to similar words (paradigmatic relationships) and contexts of use (syntagmatic relationships). Of course, nobody reading the definition without the headword would guess that that's what it was referring to. In the structuralist tradition that would simply be a definition of grammar (as @jlawler points out). 

The origin of using a plural form of address is hypothesized to be in the address of the Roman Emperor in the time when the Roman Empire has two emperors (East and West). This is also where the Royal We may come from. Speakers might also make sense of the form as indicating greater value in the plural (more means better) through a relationship of iconicity. (This could be a ex post explanation of some speakers). The plural form could also be seen as creating a form of distance or indirectness similar to the use of past forms and other distancing circumlocutions in polite requests (I wonder if you could be so kind as to help me). In this sense, using the third person (whether plural as in German or singular as in Polish) would make the same semantic sense. 

First, you need to understand the position of 'Universal Grammar' in linguistics. It is a particular theory that grew out of very specific concerns - mostly having to do with learnability of syntactic structures. It posits a set of universal innate constraints on the syntax of languages - and is not really a grammar. It is highly controversial and linguists don't generally refer to it to solve problems of comparison unless they work within the theory. While there is no 'Universal Semantics' that parallels Universal Grammar (in name or in theory structure), concerns with meaning universals go as far back as Plato (in the Western tradition). When Aristotle was outlining his principles of logic, he was in fact thinking of them as semantic universals. In this tradition, many people have dealt with formal logic and logical semantics as if they were in fact describing universal principles. Cognitive semantics, on the other hand, posits universals in the shared human experience of the world. We all walk upright, have a similar field of vision, interact with objects in our environment, are born to women, grown up in a social setting, have similar internal biologies, etc. These are all reflected in how language is structured both at the level of syntax and meaning. (Both the formal and cognitive approaches are described by Lakoff in Women, Fire and Dangerous Things). Some people have sought to go even further and posit universal semantic primes (most notably Anna Wierzbicka). 

I just needed a simple reference for a point about the teaching of phonics (it's built on the assumption that Br English has 44 phonemes while many if not most speakers in the UK do not distinguish between 44 phonemes in their speech). So I just wanted a simple number. 44 phonemes in RP, X phonemes in Wales, X phonemes in Birmingham, etc. The figure 44 phonemes is all over the place but nothing about the huge variation in that number by speech community (not counting the variation in what recognized as a phoneme in the first place). Here are a few places that address the complexity of the issue: 

I'm surprised that nobody's mentioned the form/function distinction. Phonetics studies the nature (acoustic and articulatory) of sounds that human's produce while speaking. Phonology studies their function in differentiating meaning in various contexts. Thus (very crudely), phonetics will notice the difference between the aspirated and non-aspirated /p/ in pin vs. spin. But phonology will be mostly interested in the fact that the meaning of both words will change when you replace /p/ with /k/ in exactly the same way as in lip vs. lick where aspiration doesn't enter the picture. But of course, phonology can only be successful when it has good phonetics to rely on. 

I would appreciate any hints at an answer, help with formulating the question better or even just suggestions for further readings. UPDATE: As I was thinking about this, perhaps a better way would be dealing with vowels and consonants separately and use John Wells' lexical sets as a way to count vowels. This is the standard way of comparing dialects, so maybe it's a better concept to use for the practical purposes I mentioned than the more abstract notion of phoneme. 

In some way, this question just underscores the fundamental problem with 'syntax' theory. It takes an idealized notion of 'a correct sentence in a language' and tries to come up with formal rules for generating all possible sequences of lexemes in such a way that all correct possible sequences are generated by those rules and no incorrect possible sequences are. The notions of dependency and constituency are just ways of arranging those rules with respect to putative relationships among the words within a correct sentence. But even if they were successful in that (which they aren't) they wouldn't get around the fundamental problem of the consequence of the initial idealization. If you assume the fundamental unit of syntax to be sentence (or even clause), you are mostly limited to written language for your universe of sentences. But even then you have all those 'imperfectly' formed sentences or varying judgements on acceptability among native speakers as to the acceptability of those sentences. In spontaneous speech, you don't even have sentences. So your next step is to posit an arbitrary distinction between competence and performance and ignore all the aspects of performance. But you then get the problem of bootstrapping your target set of sentences from this imperfect 'real' set of performed sentences, then rejecting them, and defining language only through the rules. Therefore, in a formal (constraint-based) theory of syntax, you can never account for the totality of the syntactic phenomena you encounter in the real world. The construction grammar framework was explicitly designed to deal with this issue. But it thinks of syntax very differently, so you cannot think of it as a third alternative to dependency or constituency. It simply does not think of sentence or clause structure in terms of heads but rather abstract schemas into which words can fit. So instead of S --> NP VP you get something like The ___ Verbed ____ into a _____. These constructions are considered to be part of an inventory alongside traditional lexemes. This means that you have a lot of 'rules' that are difficult to abstract away into structures (like trees) that are directly translatable into algorithms. Instead, you have to model them as they are. You get a lot more linguistic and cognitive plausibility this way but not a 'grammar' framework that can be evaluated on the same criteria - ie. positing of just the right constraints to produce only acceptable strings (sentences). 

This is really not the right question to ask. Writing is not just about the way of writing things down but also its own way of communication. People who read and write, don't just transfer speech into letters. They express themselves differently in the two modes. This is partly because of the different cognitive demands of reading/listening and partly because the two are used in different contexts. But in effect, all literate people are to an extent bilingual - they are competent in two codes and they switch between those codes with ease. But as in all instances of language contact, there is cross-pollination. So it's quite likely that complex multi-clausal sentences with embedding and hypotaxis developed in complexity alongside writing - although I don't think there is any definitive proof of that. In some cases, written language perpetuates older forms - e.g. in Czech or Arabic that would probably otherwise disappear. In English, there are many pronunciations influenced by spelling and vice versa. None of these examples constitute 'imperfections', just features. Although, it would not be difficult to argue that the English spelling system is completely flawed, the impact of these flaws on spoken English has been minimal.