Doing this with SQL Server would create a huge number of query plans, one for each combination of string sizes (when multiple string parameters), which is of course highly inefficient. I always thought it's the same in Oracle, but now I've got doubt. I applied a fix to the NHibernate driver, which would send always the max parameter size, e.g. if the above Name column was 32 chars wide, it would always send [Type: String (32)]. My code does not use dedicated Prepare statements, but sends parameterized SQL together with the values (similar to calling EXECUTE IMMEDIATE). I then looked at the statement (actually an INSERT) in Oracle Enterprise Manager, and the old version did not appear in the duplicate queries list. The statement itself showed a parse for each call (total of more than 1000 after some testing), but few hard parses. Thus, I could see no difference in performance between the fixed and the variable varchar length. Does this mean the fix is futile, and different query plans for different parameter sizes occur only in SQL Server (and maybe other DBMS)? Does Oracle check only for SQL string equality, but not for equality in parameter size settings? I also noticed that duplicate SQL was almost only found where parameter values were concatenated into the SQL string, instead of using bind parameters. EDIT: cursor_sharing in Oracle is set to EXACT. 

I finally worked around the problem using SQL Server 2012 sequences. Unfortunately, this won't work with versions prior to 2012. Speed was o.k, one or two minutes for about 20 million IDs to insert. 

Question currently affects C#/.NET, DB access based on ADO.NET and SQL Server 2008 R2, but I think it applies to other databases as well. I noticed some old modules of a system have non-optimal SQL queries, with multiple concatenated value strings instead of parameter placeholders. They do a polling on a table, like every 10 seconds, to get items added during the last few minutes, which generates a new query plan on every execution. Their performance is not too bad, no SQL injection risk (no web/user forms), they're old and it would be a lot of work to change their queries to correct parameterization. I suggest to do this change, but there's debate that it would be a waste of time, with other things being more important. Edit: The database is supposed to run with mostly parameterized queries (which all newer modules use), so I would like to avoid the "optimize for ad hoc" option. Partially parameterized queries create a plan anyway. Is there a downside when running in ad-hoc optimized mode, with mostly parameterized queries? To me, it seems like these old modules take a huge portion of the database resources, although they are few. Even a single module of this kind would create thousands of query plans over time, while all newer modules together have less. Is it importante to change these, or can I leave them in their state, with optimization/parameterized queries only in current/future modules? SQL is like: 

only 2 parent table rows and 5 child table rows. The whole thing as transactional as a classic join. If somebody wants a classic join result (for example, when using classic relational in-memory structures like ADO.NET DataSets), the person can easily transform it from the multi-table or "jagged" to the traditional, bloated "rectangular" join result. The data backend could still be relational and have tables, but object graphs and joins would be much better to handle. There have been multiple buzzwords around in the recent decades, including object databases, XML databases, graph databases, some of which claim to keep full relational integrity and ACID between all elements (I do not mean document only or non-/partially ACID databases in the NoSQL area - I mean with full ACID, also indices transactional, OLTP usable, but possibly limited scalability). Also, the DB should not be too close to specific programming languages, but basically provide sets of common data types, which can be loaded into or extracted from language specific data structures. The abovementioned database types may fit my definition, but in my idea, there should be a schema and table-like structures which allow both retrieval of object graphs and large numbers of rows and related rows by conditions. I am not sure whether object or graph databases are internally heap or table (relational) organized. Can anybody name me the DBMS type which fits my definition, possibly name products? 

Broken shared cursor (sql re-use) as cause of ORA-00942: I recently had this error, although the table to select from did exist and the user had sufficient privileges. Furthermore, select from SQL Developer or Sqlplus worked correctly, and even other queries on the same table within the application (using NHibernate). Only one specific, but important query to get an item by name failed again and again. The cause of this was a broken shared cursor (similar to a query plan in MS SQL Server) which didn't work anymore after I changed some user rights. My solution was to do an 

That index can help only for queries searching the properties in the given order. However, this help would still be spoiled with the given code! This is a typical example for beginners' mistakes when working with databases. It can help if you search for: FirstName FirstName, LastName FirstName, LastName, Country, ZIP It can NOT help for search on: Country, ZIP LastName, Country, ZIP because SQL Server would have to iterate through all first names and last names and, from there, look for country and ZIP. In the second case, it would still have to go through all FirstNames. Likely, a full table scan will be the lesser evil then. For good performance, you will have to create an index on at least each of your possible search parameters/columns, probably followed by others (a single column index may help little for many different queries, a multi-col index a lot for few or just one parameterization). A stored procedure is completely useless here! All modern DBMS "remember" the same SQL query if it is being sent repeatedly - make sure you use @parameters and not value literals concatenated into an SQL string (that's an error and security risk todays)! An equal SQL string (parameters remain equal, even if their submitted values vary) will cause the previous query plan to be re-used, saving much database resources. Your "one query for all" will seriously harm performance, even with good indices. It generates one query plan which is anything but optimal for individual parameter settings! For example, SQL Server will only decide once which index to use - and not use another, proper index when parameterization changes. Optional parameters in "OR @param is null" style are a disaster for performance, since they make the best selection strategy completely unpredictable. LIKE queries can be another problem. Full literals are o.k., a wildcard at the end also (LIKE 'Hunting%'), but querying with a starting or with enclosing wildcards ('%ington', '%ingt%') makes index usage impossible, at least for the column searched this way. If you really can't avoid using leading wildcards, put the name parts at the end of indices on Country and ZIP, so the full text scan can at least be limited to these! My advice is: Remove that stored procedure and create dynamic, parameterized SQL in your application. You will get a maximum number of 16 different queries, all in the DB server's query cache, as fast as stored procedures, but optimized for each individual parameterization. 

Most selective first is useful only when this column is in the actual WHERE clause. When the SELECT is by a larger group (less selective), and then possibly by other, non-indexed values, an index with less selective columns may still be useful (if there's a reason not to create another one). If there's a table ADDRESS, with COUNTRY CITY STREET, something else... indexing STREET, CITY, COUNTRY will yield the fastest queries with a street name. But querying all streets of a city, the index will be useless, and the query will likely make a full table scan. Indexing COUNTRY, CITY, STREET may be a bit slower for individual streets, but the index can be used for other queries, only selecting by country and/or city. 

then selecting the plan XML with a CROSS APPLY on sys.dm_exec_query_plan, with plan handles selected by query plan hash. Edit/Temporary conclusion: It seems like it is best to leave the very old applications as they are, even when creating tons of ad hoc queries. My greatest fear was that the flood of single use ad hoc queries would cause the good, multi-use parameterized and prepared query plans to be evicted from cache. This does not happen, because, when cleanup is done, ad hoc plans are evicted first, and others are rated by factors like complexicity, number of usages etc. So parameterized queries with a high usage rate will likely be kept, no matter how many ad hoc or partially parameterized plans flood in. Ad hoc optimization reduces plan size (actually, no real plans are stored at first use), but even more plans may be kept, with a similar memory usage (is this correct?). Even partially parameterized SQL (DateTime parameters to avoid local format troubles) will quickly get evicted if not used again, even when sent with sp_executesql, which forces parameterization and plan caching. Having a huge number (5000 to 8000+) equivalent ad hoc query plans is no good, but probably less harmful than having to dig through years old C#, C++, maybe even Visual Studio 6.0 code, to fix queries (nobody pays for that, and the stuff is still running without recognizable problems). 

The rows with "null" IDs are new, the numbered IDs are those already existing in the main, target table. The NaturalID can uniquely identify an entry (in fact, it's multiple columns). I want to set the "null" IDs to incremental values, following the current max ID, here: 5 and 6, increasing when more null IDs are found. Currently, I use a cursor to iterate over the rows with ID null and update each ID with a value, but sincce it's really a very big table, it would take days. I tried to do an update with row_number(), but it gives me an error "Windowed functions can only appear in the SELECT or ORDER BY clauses.": 

On SQL Server 2012, i've got an intermediate/staging table for merging existing with new data, where I want to insert numeric IDs for newly created rows: 

Yes, you can do a self join, to the same table with another alias. However, what you want to do looks like a hierarchy, or tree structure. Doing this through a parent or manager ID relation looks easy in the first place, but is a pain and heavy load when you want to retrieve for example the boss through multiple levels, or all employees under a certain manager. You then need multiple queries or self join levels, not knowing how many. Check for example Nested Sets or variants of Closure Tables as an alternative when implementing a hierarchy, then you have left/right values and between them all subordinates. Nested Sets may be good for trees with mainly read access, closure tables have a lot of data (no problem for a company hierarchy) but are best when updated frequently. 

I would, anyway, expect the original query and IN subquery to be translated into a semi-join with index lookup on both Id and ObjectId. However, I see the IN subquery is internally translated into EXISTS (select 0 from...) May this be only a statistics problem, or does the OR condition spoil it all? Is it possible to lookup the indices on Tasks and ObjectAffectingTasks with such an OR condition? Unfortunately, the queries are created by rather complex program code, so rewriting them would be way too much work. Another issue is that they're a bit generic, being used for both Oracle and MS SQL Server, so proprietary hints for index usage are not an option, either. 

where the values vary and the date is a few minutes before now. In a few cases, the date has been changed to a parameter like "@startDate", to avoid format problems, but ItemType and ItemCreator values are still concatenated strings. When monitoring query plans with DMV or Activity Monitor (Recent Expensive Queries - Plan Count column), I notice some of these queries have 8000+ equivalent query plans in cache: 

The WITH(NOLOCK) hint (or the equivalent READ UNCOMMITTED transaction isolation level) causes SQL Server to circumvent transactions and to read tables even when being changed by another statement, and to not block them during read process. SQL Server Management Studio also sends SQL statements to the database, likely in the NOLOCK / READ UNCOMMITTED mode to not block tables. If a change to the table occurs while being read in this mode, the SQL command may run into this error. Probably something has been changing the tables SSMS was about to read. SQL Server 2005 and later can be enabled for Snapshot mode (also referred to as Oracle style), where users read only rows already existing when their transaction/statement was started (row versioning), rather than locking against other reads/writes. The transactional locking, enabled by default in SQL Server, makes especially long running queries critical and adds risk of severe delays or deadlocks. For this reason, the NOLOCK hint is common for complex or long running queries on SQL Server. 

Many relational database and supporting ORM features omitted: Tables are strongly denormalized, foreign key relations missing or impossible, e.g. due to metadata tables referencing different main tables, with columns like . NHibernate has mostly no object relations mapped (and has generally problems with table structures it is not made for). Instead, these are implemented in business logic (sometimes leading to orphaned child objects or inefficient database access, see below). Denormalization down below the basics: Increasing use of non-1st NF data: nclob/nvarchar(max) columns with XML, comma separated lists, or composite numeric value columns (e.g. 123, 10123, 40123 for task type 123, but different module configs identified by 0,1,4 * 10000). The first two containing database relevant, logical "Foreign Keys" and data model relevant values, such as (to be checked with ). This is mainly due to many quick-to-release, short lived and customized values which shouldn't go into the main schema or are easier to implement through XML values. Non-2nd NF data, including table contents being copied by triggers, followup stored procedures or applications into other tables. E.g., a table column value copied to a "vertical" metadata table, this again copied to a "horizontal" or "pivoted" representation of the metadata (each metadata type a column), because some applications can only use the metadata or horizontal metadata. Frequent requests to use "rubbish bin structures" (dump collected data from various sources into one nclob/nvarchar(max) "rubbish bin" column and let an application search through it, instead of many different sources). The "One-Object-Disease" in business logic model and applications: Iterating and immediate load/save of single objects: The business layer uses mainly Load/Save() methods for individual objects and few bulk/set based operations. A common job is to get object IDs by SQL or it's NHibernate representations, then iterate over all retrieved Ids and fetch the objects one by one in the style of . This with all metadata, dependent objects collections etc., a classic SELECT N+1 situation. Furthermore, most of NHibernate's caching, persistence ignorance and combined operations have been forcilby disabled: Loading one object explicitly calls to prevent using the cache or deferred execution, but get a fresh object from the current DB row. is implemented to enforce immediate Insert/Update: NHibernate followed by immediate . The whole thing uses NHibernate micro-sessions: loaded objects are immediately taken out of session context and saved within a new session (preventing those "weird", undesired changes of unsaved objects in DB). Persistence ignorance and object relations through NHibernate seem undesirable, to keep control of the state of every single object. NHibernate is really considered a mapper (one row to one object) rather than a complex tool for relational database access. There's also debate about using a "fast" micro-ORM instead of NHibernate, which will materialize SELECT N+1 queries lightning fast into objects, but of course do nothing against N+1 itself. An important requirement is to get everyting working with everything, because it is too much to release all modules for every single change: a new module must work with an old database version, where certain columns and tables don't exit, and an old module must still work with a new DB version, with columns etc. added. This leads to new columns having default values if not nullable, and old tables/columns, abandoned for a long time, to be still in the data model, because removal might crash old modules. Another consequence is reluctance to add new tables/columns, which you can hardly get rid of, once released. Instead, XML (in text columns) and similar, denormalized stuff, or property values in the global metadata table, are preferred. Many modules receive tasks just for single objects, which would be no problem within a possible, set based approach, since a set/bulk data access method could also handle a single object/row, if needed so. On the other hand, there are the web servers, maintenance and background services, which handle many objects at once, need the business logic and run very inefficient in the current single-object way (the webservice using native SQL, or, newly, a Lucene based search engine to search the IDs of desired objects, but retrieving the full model objects one by one).