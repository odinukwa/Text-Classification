Requests to several websites on Media Temple's Grid Server (shared hosting; more than one account) are timing out for me today. Everything was fine earlier this morning, and for no apparent reason these URLs are timing out in the browser, via FTP client, and SSH. Clues/notes: 

I'm used to asking strictly technical questions, so I hope this "Dear Abby" style is appropriate. Potentially-Excessive Background I'm a web designer and developer, good at a lot of things and excellent at very few. I've used various VPS products as development servers, I've tried all kinds of shared hosting, and my high-traffic experience ends with CloudFlare and bumping up resources—no distributed computing, failover setups, load balancing, or big-kid networking. I've set up a mail server and web servers from base distros and know that I'd put everyone at risk offering to do those things for clients. (So I don't.) Let's say I have a client, a medium-sized company well-established on one of the thriving US coasts, and they run an off-the-shelf PHP/MySQL CMS along with a custom-built PHP/MySQL app. I can keep that software updated, healthy, and neatly deployed—no problem. Now let's say this client also has an internal IT department that doesn't seem up to the task of maintaining the site and server: inconsistent version control, questionable production "fixes", and with a reluctance to share access, ask for help, or take thoughtful advice. An otherwise stable VPS has been crippled, and the client knows but isn't sure what to do. I want to help responsibly and not just blame the current situation on iffy internal decisions, but I don't know what to recommend. The Question This is a client that benefits from the resources and availability of a good VPS product, with an IT department that may not be well-suited for managing such a server. Does this mean that managed hosting is ultimately an ideal fit? Management software updates (Plesk, Cpanel, etc.), security patches, and server maintenance should all be handled by somebody other than the client's IT, and ideally there'd be guaranteed, scalable resources with solid uptime and an SLA to match. I see that Media Temple is offering managed hosting now, but I'm interested in reputable suggestions if this is what I'm looking for in the first place. 

I've got VMWare ESXi (the free version) machine that is booting off a 9650SE RAID-1 drive setup (two 1TB mirrored drives). I'd like to replace the 1TBs with 2TBs. Is there a safe way to do this? 

I'm looking at some scripts, and a number of them use this what is essentially this command in their "" target: 

I've got a number of Windows (7/Vista/XP) based machines on a network that have no unifying structure (i.e. no ADC, PDC or for that matter a workgroup). I recently added a Linux box with a fairly sizable raid setup that I'd like to use as a file server. What would be considered a best practice for making the storage on the Linux box available to the windows clients on the network? It seems my options are: 

Data reliability is the most important issue; power comes next, and I could care less about speed. For the data hardware I've chosen a 3WARE 9650SE-4LPML which will be hosting two (2) TLES enabled WD RE4-GP drives running RAID-1. The decision I'm facing now is what file system should I use? My preference is for something Linux based, and a file system that attempts not only file system integrity but data inegrity as well. Please comment on which file systems, OS distributions and for that matter hardware chosen that you think may best fit my requirements. 

Will display the order for DNS resolution. Useful for when you're creating or debugging your Network settings. 

There are certain aspects of the LM UI that are available only to the System Administrator - is it possible to be assigned as a role to an LDAP, imported user? 

Linux DHCP Server, many Linux clients. One Solaris client. Some of the machines have their MAC addresses within the dhcpd.conf on the DHCPD server, and they all pick up the appropriate IPs. The Solaris machine though, grabs one from the pool instead. Ignoring its MAC->IP assignment. Why? Also, where is the dhcp lease file within Solaris, so that I could clear it - just to make sure. Thank you 

With the vSphere client, if you delete any VM, and there were say boot ISO's in the same directory. They stay, as VS Client only purges VM-ware files. Although is there a way to get rid of the whole carcass? ISO's inclusive? Thank you. 

VMware server is not the product that you'd wish to do this with. As mrdenny suggested, either go up to vSphere. OR. Re-evaluate what you are using VMware Server for, and if you need the failover feature-set, and obviously have access to a SAN - your best bet without spending actual $ would be to go with XenServer - which will provide you with HA. One thing to note is that it will require a dedicated machine (something to consider, as it seems whatever workload you're trying to virtualize is important enough that you're considering failover) 

I'm using Ubuntu 10.04 and have the directory setup as a BTRFS RAID 1 with two 2TB drives. I'd like to make the directory just a single drive; how do I safely go about doing this? 

I have a server that I use infrequently, so I'd like to a job to shutdown daily if no users are logged in via SSH and SAMBA shares. How can I determine how many active SSH connections there are, and how many active SAMBA connections there are? If both of these values are zero, the cron script will shutdown the server. 

With the value set to true, the module was trying to remove from the system, not, as I assumed, in the directory. 

I'm using a fairly vanilla Ubuntu 10.04 server installation, and I'm experimenting with BTRFS. How can I create a BTRFS RAID1 mount? I've got two (2) 1Gig drives that I popped in the server and after running the following commands, it appears I've got a 2 Gig partition, not 1 Gig as I would expect. 

The is actually a script function that checks the status code and calls exit if necessary, nonetheless, How is the above any different than this: 

I have a directory with every package I require. Don't really want to go through several dozen files and manually invoke pkgadd for them. Is there a way to automate this process or call pkgadd to have it install everything in the directory? Thank you. 

Anyone notice that the HS22 blades take at least 3-4 minutes to get to booting the OS? Is there anything that can be done to get it through the BIOS faster? 

everything I want gets picked up from the dynamo process. Although for some reason the CPU Utilization isn't being presented and recorded. Any ideas? Thank you EDIT: SYSTEM - Linux x86_64, Virtual Machine, running atop ESX4.0 on an IBM HS21 Blade 

Am looking at Puppet and Cfengine. Unfortunately neither can do Windows deployments. Can you suggest some alternatives? Thank you. 

Ended up adding a disk to a DS3400 array. Unfortunately it's been going for nearly 3 days now. Is there a way through CLI or otherwise that I can actually see what the completed percentage is? Thank you. 

Once there are several hundred adhoc machines being brought up, each with a unique name. Their hostname gets added to the DNS. Then upon TTL expiring, will the DNS entry still be reachable, or if the host has been down for some-time, it [dns-entry] will be cleared? What's the best way to manipulate these timeouts? Thank you. 

My active directory password changed, now I can't log into Redmine (0.9.4) via LDAP. I haven't installed plugins, so what gives? 

I'm putting together an file server which will have a very limited number of users (less than 5) and offer the following services: 

A lot of script simplification would be had if they are. EDIT: I should add these "processes" aren't daemons and don't actually record their PID in the FS anywhere ... 

I used a turnkeylinux.org otrs installation and I'm trying to configure the default domain of 'yourhost.example.com'. I tried the following: 

I've configured to automagically mount a cifs share when users login; the problem is if a user logs into multiple times simultaneously, the mount command is repeated multiple times. This so far isn't a problem but it's messy when you look at the output of a command. 

First, let me state, this was an IT goof up. They deleted an account, went "oops" and created a new account with the same credentials. I can't fathom why. Regardless, I've got a Linux (Ubuntu Server 10.04) box authenticating against an active directory server, and users logging in have local home directories and such. I'm stuck with one user name having a new UID (or SID in AD jargon). Over and above ing his home directory, what other interventions should I do?