What you are attempting to do is vague at best... But in Oracle, is analogous to .. All of this information can easily be found in the oracle documentation (which is probably why you have down votes .. if you spent a bit of time in the oracle documentation for data types, you'd have your answers), but I will explain the difference between varchar and char here.. datatype is for fixed length strings, period. All strings within a field are the length of the field. So a of would ACTUALLY be stored as (with buffering spaces to fill up the length). The only reason your test works above is because your is the same as . datatype is for variable length strings, period. All strings within a field are the length of the string that was given. So a of would ACTUALLY be stored as (with a tiny bit of additional information saying that the data is "x" bytes long). There are very few situations where you would prefer a field over a field. What @a_horse_with_no_name is trying to suggest is that you should be able to just declare your variable as and be done with it. If, for some reason, 20 is TOO SHORT, then just declare it to be a larger size. If you don't know how large it should be, then declare it maximum size ( for PL/SQL) 

In more details: [^1-5] boxes matches because 7 is not a number between 1 and 5 followed by a spaces and the word boxes matches because 4 doesn't match (regular machine not in the first step) -> 8 is not a number between 1 and 5 followed by a spaces and the word boxes the actual match is: [^1-48] boxes Doesn't match because the rules specifies now characters from the set of (1,2,3,4,8) and the string fails on this check. Fix your regex In your case probably you want to use 

Further down the road If you want to have better HA and/or more automated way of operation you can look at the following solution. (this list is not complete only some pointers to give you a start) Percona XtraDB cluster With XtraDB cluster you can ensure consistency and have true HA with even under 100ms failover time (depending on your failover technology). MySQL Fabric It's Oracle's in house HA solution. Requires client libraries so make sure your application can handle it properly. MySQL MMM One of the old player on the MySQL HA market but worth to mention. I hope this helps. 

You would then need to edit your MAIN.F_GET_GK_TEXT function to first look at the WEB_GK_STATE_OVERRIDE function and see if an entry exists for PRG_CODE/STATE .. if so, return the TEXT value .. if not, return the TEXT value recorded in the WEB_GK table. But your existing design and this one will be fragile.. They will work, but it's very much a bandaid instead of an actual good design that will be flexible in the long run. 

pg_restore has a --clean flag (or possibly --create) which will auto delete data before running operations.. The Excellent Documentation should help you greatly... Just to clarify, in case it's confusing: 

Essentially adding a 3rd parameter to your query .. one use case would use the 3rd param and set the 2nd param to FALSE, while another would set the 2nd param to TRUE.. 

COALESCE will let you get away with a lot of interesting things when you're dealing with the situation of "if it's null do one thing, otherwise do something else".. 

Your query uses which will select every column from the tables (in the order that they exist within the tables). So if you have: 

You probably want to build indexes if you want to have some lookups on them. Also a primary key may be beneficial. 

The behaviour you're describing is called MVCC (Multi version concurrency control). Strictly saying it's not delete + insert. It is more like: 

2015-11-22 is not older than 6 months compared to 2015-05-01 unless you truncate the response date to month also. 

You didn't attach the table schemas so I just assume this would work. You might need to fully qualify the field names (using tablename.fieldname). Also feeds_processing_data could benefit from a composite index covering the involved fields. 

As long as the bottleneck is the amount of RAM you can keep scaling up. MyISAM has some bug related to large key buffer size but InnoDB handles massive buffer pool just fine (even in the TB range). I see you already have 4 buffer pool instances. By increasing the buffer pool it's good to increase this too. The only "issue" you may experience is if you also have larger innodb log files and more dirty pages in the buffer pool then stopping the database may take longer and in case of a crash innodb recovery will take longer. For scaling out the most efficient simple measurement is the concurrently running threads: You can take a benchmark and see where is the tipping point for TPS on your system. (Usually with increasing concurrency you will see transactions / second increase for a while until it hits different contention issues and then drop usually quite quickly, you can see a sample benchmark and what the results look like here: $URL$ You want to keep it always under that level. Measuring this will give you a good indication of when will you need new server and how many. 

Lets say I have a number of tables that will have a type and status and I want the types/statues to be configurable by a end-user of the application (so I can't use enums here). There are 2 different ways I am think about doing this: 

I am build a web application (project management system) and I have been wondering about this when it come to performance. I have an Issues table an inside it there are 12 foreign keys linking to various other tables. of those, 8 of them I would need to join to get the title field from the other tables in order for the record to make any sense in a web application but then means doing 8 joins which seems really excessive especially since I am only pulling in 1 field for each of those joins. Now I have also been told to use a auto incrementing primary key (unless sharding is a concerns in which case I should use a GUID) for permanence reasons but how bad is it to use a varchar (max length 32) performance wise? I mean most of these table are probably not going to have at many records (most of them should be under 20). Also if I use the title as the primary key, I won't have to do joins 95% of the time so for 95% of the sql, I would even occur any performance hit (I think). The only downside I can think of is that I have is I will have higher disk space usage (but down a day is that really a big deal). The reason I am use lookup tables for a lot of this stuff instead of enums is because I need all of these values to be configurable by the end user through the application itself. What are the downsides of using a varchar as the primary key for a table not excepted to have many records? UPDATE - Some Tests So I decided to do some basic tests on this stuff. I have 100000 records and these are the base queries: Base VARCHAR FK Query 

I'm not sure I fully understand what you expect to happen with the SQL you pasted .. (add up the numbers from 1-99 ignoring all even values?) But this is a SQL to get around the syntax error you have: 

, as John pointed out in a comment, is for filtering on aggregates after they have been calculated, while is for filtering the rows that are gathered at the start. For example: 

With regard to your questions, please read the documentation completely, as it addresses all of your concerns. In terms of looking for when it doesn't exist .. the recovery process just parses all WAL files until it encounters "EOF" .. which usually happens when it tries to read from the next WAL file and it doesn't exist.. So it uses that case as a termination point for the recovery process.. The important bit here is that it reached a checkpoint in the recovery process as stated with . This means that your recovery is reliable. Continuous Archiving Documentation Section 24.3.5. - Timelines Timelines are a way for Postgres to handle "alternate universes" so to speak... For example, if you have a situation where your replication gets messed up in some way and suddenly you have slave promoted into master status (while your master never went down) ... some machines are writing to the master still while some are writing to the slave.. But it's ok, because the writes to the current master will all happen on timeline 1 and the writes to the slave will all happen on timeline 2 .. so you can, theoretically, still merge all the changes together (although I'm sure it wouldn't be easy).. I imagine it's similar to a branch in a version control system. Except there's no real "trunk". For the postgres gurus out there - if I have anything incorrect, please correct me. :) 

You can use replace into flag () and where condition () while also omitting the create statement (): 

So depending the logic what you consider expired the query changes a bit. I've included both version. In comment the date and active condition as per the example. Using Lateral join: 

In case of postgresql: You didn't write which version you have from postgresql. You can use LATERAL JOINs (available from 9.3) or window functions. Example for window function: 

As @jkavalik mentions in comment the literature you want to read up on is database normalization and normal forms. Here is a good, clear explanation on them: $URL$ Generally speaking you should thrive for at least 1NF. So in your case if user has only one phone number you can still store it together but try to avoid , column creation which pattern unfortunately can be seen in many places. is most likely unique so there's no need to split that from your table. On this page you can find a lot of sample database models: $URL$ You can browse and find which resembles to your problem the most but it's also good for learning about different solutions as well. 

My 2 cents, although I am not nearly as experienced as many of the others on this site. If it is related to data integrity (either from a relationship standpoint OR a standardized formatting standpoint), then I firmly believe that it should be handled by the database. If it's simply for a nice presentation, then I think that should be handled by the presentation layer. In the example you site where the value caused a division by 0 error in a calculation .. if that calculation happened outside the database, then I do not believe a constraint on the data in the database is valid at that point. The database shouldn't need to know what the code does with its values and the code shouldn't need to know what the database does with its values... The short answer (again, in my opinion) - the database should be responsible for delivering clean and reliable data in a standardized format ... what happens WITH that data is up to the person/code that is requesting it. 

would return one row (where type is EXAMPLE) and the count of how many rows contained that type. The filtering here takes place AS THE DB COLLECTS THE ROWS up front .. Any row that doesn't have type 'EXAMPLE' is thrown out. Only one COUNT() is calculated in this instance. One more complex example to show that the two are evaluated separately and can be combined in any query. 

One Table/One Field - Basically there is one table that store the history of all the table that need history storage. All change are recorded in one field as a text data type. Table Per Table/One Field - Same as above except the each table has its own history table (ie. Projects/ProjectsHistory, Issues/IssuesHistory, etc...). Table Per Table/Field Per Field - This is like the above in the each table has it own histroy table but also the history table has pretty much the same definition as the regular table with an additional of additional history related fields (updateDatetime, updateUserId, etc...). 

Have Types and Statuses tables for each table that needs them. Have one Types and Statuses table that every table that needs them uses 

What are some of the advantages and disadvantages to these different methods of storing record history? Are there other methods that I have not thought of? 

Now I don't know what configuration I could make to make one or the other (or both) faster but it seems like the VARCHAR FK see faster in queries for data (sometimes a lot faster). I guess I have to choice whether that speed improvement is worth the extra data/index size.