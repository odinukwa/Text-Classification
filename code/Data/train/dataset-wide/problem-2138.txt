For excluding a database from the backup script in Ola Hellengren's solution, you will need to add it like the below: 

This was a mere co-incidence. Both backup and restore are 2 different tasks which depend on various factors. 

In SQL Server restricting to view just one database is not possible. Either you see everything or you do not see anything (Except master and tempdb).Hiding all databases can be achieved by below steps : 

Azure SQL (PaaS) is not yet fully mature and there are few features in the On-Premises that will not be supported. IaaS will be much suitable for existing application for migration as you will have full control on moving the existing functionalities. Here are 2 links which give a better comparison of the two offerings: 1.Microsoft Documentation 2.MSDN Blog 

I used your data and was able to do it without any issues. Seems your first row of data does not have {CR}{LF}. The snap of data looks like there is an auto new line rather than you providing it.And that is why the 3rd column has ' Is_Active""1, so the wizard is looking for a comma now. Below is my test with data of similar type : 

Check for the space issues and also the permission on the Temp folder. If the permission is read-only then change it to Full Control. $URL$ 

The extension of a backup file is just to have a notation and easy identification. The script you are using is for Log backup and not for Differential backup. Differential backup backs up the extents that had changes since the last full backup. Below is the t-sql for that : 

Before doing this do test it in a Test environment and prepare a full proof plan. Also refer to this link : $URL$ 

No..Never!! This is why we have development/Test/UAT servers. The data from Production can be copied onto the test environment and the developers can go ahead with their testing. Select queries can be very harmful as well in case of a Production environment. It increases the load and at peak time can bring down the entire performance. Any information they need should go via the DBA who can run what they want to ( Select) and send them the results. That's how we do in our environment. 

For getting all backups to just one location, you will need to remove few parameters from the store proc [dbo].[DatabaseBackup]. There will be this part where you set variebles( Look for the comments) -- Set variables Go ahead and look for the below part : 

What is the frequency of the log backups ? Use dbcc sqlperf(logspace) to check how much % is free space in the logs. There is something that is holding the logs up. Check the log_reuse_wait_desc for that database and take action accordingly. Follow the below article : $URL$ 

sys.dm_os_buffer_descripters gives you the Buffer pool usage and not the total memory usage.DO NOT assume the results from this DMV to be the total memory usage by the database engine. There are lots of other things that SQL Server does and uses memory for. Total will be a sum of Buffer pool and Non-buffer pool usage. 

Google is there for helping you out in coding. Take this as a challenge start coding , it will help you out more than someone writing it for you. Have a nice day. 

It would be recommended to have the script scheduled during minimal operations window . Identify the frequently high fragmented tables and treat them separately as per your environment convenience. Also if possible always have a maintenance window defined for the database. 

You just need to change the Directory parameter in the Ola Hellengren script. Go to the job, click the Step --> Edit-->@Directory = 'C:\Backup', This will be enough. Adding to your particular requirement, I went into the Store proc and was able to figure out. This took me some time so apologies for that. Please mark this as answer once you are able to complete your task. Below are the steps to get your required directory : 

The remote query timeout option specifies how long, in seconds, a remote operation can take before SQL Server times out. The default value for this option is 600, which allows a 10-minute wait. This value applies to an outgoing connection initiated by the Database Engine as a remote query. This value has no effect on queries received by the Database Engine. What is the value set for Execution timeout in SSMS for you from where you are running the query: 

Log backups will help you control the log growth unless there is something that is holding up the logs from being reused. Configure the log backups to run every 15 minutes. (That's what we use) Coming back to our question : 

Its a normal behavior as the IntelliSense does not recognize the objects that are not cached. I use the Refresh cache option in Edit menu each time I see this. Edit -> IntelliSense -> Refresh Local Cache 

Change the collation of the instance back to what it was i.e SQL_Latin1_General_CP1_CI_AI. This will fix the issue. You can go through the below article on how to change the instance collation, this will need a downtime so plan it properly and go ahead. $URL$ 

How does it effect the performance? ( Did not find any explanation or proof of concept) Also confused as some articles say dropping them can cause performance issues too. Is it for all cases or any specific? 

Take database backup Take note of Availability group listener and database involved.(For future if they want it back) Remove the Databases from the AG. Remove the AGs Uninstall the instance as a normal standalone instance.(For all nodes). 

No. Index rebuild will generate logs irrespective of the way you run it because internally the t-sql will be same. 

In this case only the db_owner and sysadmin can see the databases. db_owner can see only the one he owns while sysadmin can see all. Again giving db_owner privilege is not a good idea as at db level it is the highest privilege. Also one database can have only one owner. Hopefully MS will have a resolution for this soon. 

The characted '\' will be identified as invalid character and it will error out if you try to create a SQL login using it. This is not possible. First of all please go through the two types of authentication and how that works. Windows Authentication is for AD users which goes by Domain\user_name and are added to the instance while SQL Authentication is for logins created inside the instance. Please go through the below link : Authentication 

Sql Server Management Studio is just an application used to manage the Sql server database. It is not to be confused with Database Engine. You will have to install database Engine which is actually the database and will come up in the configuration manager. 

I am playing around with SQL Azure and in my learning curve. I Have migrated a database from my on premises instance to the Azure server. Now my question is, how do I move a database from Azure to on premises instance. Source : Azure , Database name : Azuretest Destination : Sql server 2016 on premises instance in my laptop. Is Azure to On Premise database migration possible? 

Yes, however this depends on the page count. If the page count is too low then the index fragmentation will always be on a higher side and it wont effect the performance. In our environment we consider Page count > 500 during the index fragmentation analysis. Re-organize when Index fragmentation >5% and <30% Rebuild when Index Fragmentation > 30% The above numbers (%) is not a standard but yes its widely preferred. 

Do not use Index rebuild ALL but individually.Index maintenance has to be followed-up by regular T-log backups. This helps in controlling the log growth. Have a rough estimate of log growth and the log drive ready with sufficient space. Follow the best practices for transaction log management and Index maintenance will never be a problem. 

First of all find the session from which the backup is running. To find that use Sp_who2 and check for the command column to find "Backup...'. Now just confirm that it is the same one that you have initiated by taking a look at the query fired for corresponding spid.Check this by : DBCC Inputbuffer(SPID); /Use the spid here/ Once you are sure of the SPID just kill the session by using : KILL (SPID) And then wait till it disappears. There will be STOP button next to EXECUTE in SSMS. You can use it to cancel the query execution from ssms. 

With the points mentioned , it seems your database is already connected by some login and since it is in Single-User mode, you are not able to access it. 

Yes, you just need to have a different publication created for a specific Subscriber with the required articles. This will help you in customizing your subscriber as per the required articles. 

Index rebuild (Online) is a resource intensive operation. During the rebuild there are locks being held which for a very short period of time can make a table unavailable (Very short time).Below link provides guidelines for online index operations : $URL$ A paragraph from the above link : 

We use the entire Maintenance solution provided by Ola Hellengren i.e.Backup, Database Integrity and Index Optimization. What we do is create a new database for DBAs and then use it deploy the maintenance solution. This keeps the system dbs separate from any other user /DBA created objects. Just need to change the script a bit for that (Use databasename). 1.Important parameter Values which I need to modify? For Index Optimize check the SP and configure it according to your need for Index fragmentation in the script itself. 2.Where will these commands create the SP and related Command Execute and Command Log, all in Master Database or MSDB? Yes, if you use the script as it is it will use master to create all objects. This can however be customized as per your requirement. We do use a different database to get this deployed. 3.Series of code execution - Which code I need to execute first? Once the solution is deployed ,it will create jobs and then you can schedule it from there. This is the best available solution with lots of freedom to customize. Once you start using it, it will reduce a lot of burden from your daily activity. 

The timeout is a client side setting and below explanation from Microsoft will clear the air why you are still having the issue. Also this setting change does not require a restart. This value applies for an outgoing connection initiated by the database engine. 

You are appending the backup into same file so it is actually accumulating the backups. You need to give a new name each time you trigger backup from Tasks->Backup. Use maintenance plan as it has timestamp for each backup file or else go with Ola Hellengren's solution. $URL$ 

We had to re-enable TLS1.0 to make the connection successful. Is there something that can avoid enabling it as it was identified to be a vulnerability. What needs to be done to make it work with TLS 1.2? 

Delete the mdf,ndf and ldf files of the database from the locations they reside. Old backup files for the database needs either to be Deleted or moved to another server considering your retention period. 

Scheduling log backups is the best way to control the logs (considering the Full recovery mode).Make sure it is scheduled properly and more frequently. 

5.How is it different from appending backup files and daily individual backup files with Date Time Stamp appended to the filename daily basis . 

The first and the foremost thing that your database is already in Simple recovery mode so log will not grow much until it is held up by a single transaction. Once the checkpoint comes the log will be truncated itself. Now that you have limited the log size, SQL Server goes for a toss when the transaction is in the middle and there is no scope of log to grow. Simply kept, the transaction needs the log to grow. You cannot make the transactions complete without allowing the logs to grow when required. This is why the best setting is to keep the AUTOGROWTH enabled. SQL Server treats it as a crash and then goes for a recovery to rollback the incomplete commands as seen in the error log. 

This is a very annoying thing happening to the reports I am developing. The top breadcrumb sometimes works fine while at times breaks into multiple lines. I have only IE 11 in my environment, Is it the browser issue? If not then please help me fix this. Below is the screenshot. 

Script out the replication. Take the database backup for Publisher. Backup one of the subscriber (right after no.1). Copy subscribers backup (which is much lighter) to the future subscriber and restore Configure the new subscription accordingly. 

Check Event logs for any server resource utilization errors. Database response is one thing and server response is another. If you are facing issues even while doing simple tasks like expanding the database then it is mostly sluggish response . 

7.How better I would have thought than what I am thinking now . Which is more recommended and better , Overwrite / Append / Individual Backup Files with Date-Time Stamp ? 

No there is nothing wrong in linking two identical primary keys in different tables but the problem arises when the data type is declared as identity. When you declare some field as identity then the value is auto incremented and is decided by the seed if provided. Even though the value are in sync sql server will not allow you to go ahead with this as pointed out by Dan. Use INT and it will not be an issue. 

All major points have been already covered. Below are my 2 cents : Detaching a database is never a permanent solution as it was intended to be used for moving the database files within the server or to another server. Removing a database permanently can be done by Delete option in SSMS or the DROP database command as mentioned above. Usually the databases which are intentionally kept offline and keep generating Alerts are the ones we detach and keep it till it can be permanently removed(Deleted). Pre detach task : Run to know the file locations. Cleanup tasks : 

The database has corruption which is reported by DBCC CHECKDB and it is the only safe way it could repair it which did not happen in this case. Using repair_allow_data_loss will guarantee you some loss of data and you should not be going for that unless you can afford to lose data. As any DBA, you will have point in recover backup set up so use it to recover the database which will be more consistent. For any further troubleshooting use : $URL$ $URL$ 

It is always good to have backups done with individual files (With date stamp). This will not only help you save diskspace but also be easier to do the cleanup. The cleanup can be done as per your retention period agreement(5 days).One big disadvantage being any mishap where the bak files gets corrupted. In this scenario you will have no useable backup while in other case you will have at least one good backup (Keep checking the for validity of backups). Use Ola-Hellengren's scripts which are the best way to configure backups locally.This ha all the required parameters and also the time stamp issue will be taken care. Coming on to answering your questions : 4.What will be the disadvantage of my above plan . 

Check for any memory issues.A memory pressure can cause this kind of response. Check if the database Auto_Close property is set to True as this also cause slowness while expanding the database objects. $URL$ Evaluate the Disk IO as suggested by Pinal.