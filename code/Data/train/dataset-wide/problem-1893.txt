As early as the late 1960s / early 1970s, Nimbus-3 carried Iris-B and Nimbus-4 carried Iris-D, both infrared spectroradiometers with a moderate spectral resolution. Subsequently, I believe it was not until AIRS flew on EOS-Aqua (launch 2002), and subsequently IASI on EPS and CrIS on SNPP and on JPSS, that spectrometers were flown on (semi-)operational weather satellites. Why is this so? Were IRIS-B and IRIS-D considered unsuccessful, prohibitively expensive, or did agencies simply prioritise on other things? 

One method is through ice cores from the worlds ice caps. Each year, as small amounts of snow accumulate on ice caps such as on Antarctica and Greenland, bubbled of air gets trapped. As we drill through the ice, we can identify the air samples in those trapped bubbles, and measure directly the composition of the air in those bubbled. It tells us much more than the level of CO₂. For example, the ratio of isotopes of oxygen is a pretty good thermometer. We can also see pollen, volcanic ash, and other things. Other methods are listed at the Wikipedia article on climate proxies. Most famous are ice cores and tree rings, but other methods are lake and ocean sediments, corals, and others. Those methods are somewhat independent so if they confirm each other, that is good. Errors on timing may get larger if one gets further back, and sometimes a new analysis leads too a change in timing estimates. But for the period where we have comparable records (for example, the period for which written historical records exist), results compare pretty well! 

Lunar atmospheric tides are likely insignificant for weather, although Guoqing (2005) asserts that The lunar revolution around the earth strongly influences the atmospheric circulation. They don't seem to be studied terribly much. What insights can we gain from the study of lunar atmospheric tides? 

Figure 7.7 | Distribution of annual-mean top of the atmosphere (a) shortwave, (b) longwave, (c) net cloud radiative effects averaged over the period 2001–2011 from the Clouds and the Earth’s Radiant Energy System (CERES) Energy Balanced and Filled (EBAF) Ed2.6r data set (Loeb et al., 2009) and (d) precipitation rate (1981–2000 average from the GPCP version 2.2 data set; Adler et al., 2003). Figure reference: Boucher, O., D. Randall, P. Artaxo, C. Bretherton, G. Feingold, P. Forster, V.-M. Kerminen, Y. Kondo, H. Liao, U. Lohmann, P. Rasch, S.K. Satheesh, S. Sherwood, B. Stevens and X.Y. Zhang, 2013: Clouds and Aerosols. In: Climate Change 2013: The Physical Science Basis. Contribution of Working Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change [Stocker, T.F., D. Qin, G.-K. Plattner, M. Tignor, S.K. Allen, J. Boschung, A. Nauels, Y. Xia, V. Bex and P.M. Midgley (eds.)]. Cambridge University Press, Cambridge, United Kingdom and New York, NY, USA. 

The instrument measures microwave radiances (after calibration) If we know the sea surface temperature, we can use radiances to calculate emissivity. The emissivity at 1.4 GHz is itself a function of near-surface ocean salinity. 

Note that OSCAR has a lot more capabilities than this. For anything Earth-observation-from-space related, it is a superb resource. 

The most state-of-the-art knowledge at the time of writing is summarised in IPCC AR5 WG1 chapter 7 (PDF), which stated in 2013: 

The differing definitions need to be kept in mind when comparing models to measurements. Under normal circumstances, clouds are a necessary but not a sufficient requirement for precipitation (there are some exceptions). To answer your titular question: No, a cloud fraction of 0 for low clouds does not mean it was raining. Nor does it necessarily mean it was not raining. Remember: a total cloud fraction of 0 means a clear sky, a total cloud fraction of 1 means a completely covered sky. So a low level cloud fraction of 0 means there are no low level clouds. To answer the question you ask in the content of your post: Statistically speaking, yes. The quantities low-level cloud fraction, cloud optical thickness, and total cloud fraction do contain information about precipitation. If the total cloud fraction is zero, there is no precipitation. If the cloud fraction for low clouds is zero, but for mid-level or high clouds is non-zero, it may or may not be raining. Cloud optical thickness should correlate with precipitation occurrence and intensity. However, these quantities do not provide complete information about precipitation. You can't say with full certainty that a day is rainy based on these quantities alone — but you can say which days certainly aren't. 

The bad part about geo-engineering are the unknown unknowns, to paraphrase a certain US politician. Our climate models are wrong. All models are wrong, but some are useful.. Our models are useful, but not quite useful enough to trust them when they tell us massively spraying stuff into the stratosphere or the oceans is mostly harmless. Our models can't properly reproduce our current climate. They agree about some trends, but there's lots of stuff that they're missing. We used to use lots of toxic chemicals in our fridges. That was a bad idea, so we found some stuff that wasn't toxic. Great, no? Decades later, scientists realised this was destroying ozone above Antarctica. At first the measurements were rejected because they didn't make sense. Contrasted with our understanding of the atmosphere, and why would ozone suddenly disappear anyway? Our climate system is complex. Very complex. Mess with one of the symptoms, and who knows what side effects are going to be? It's a risk that we cannot quantify, and those are probably the scariest risks out there. If even with stuff that doesn't seem remotely connected to each other (fridge coolants and ozone) we almost messed up very seriously, who is to say actively messing with our atmosphere or ocean isn't going to have nasty unintended consequences? Better than offsetting one set of geoengineering (deforestation, ocean acidification, enhanced GHG emissions) with another equally radical one, it might be safer — from a risk management point of view — to try to not do any geoengineering at all. Certainly as long as our understanding of climate is just starting. Note that weather models are initial value problems and climate models solve boundary value problems, so mathematically speaking, the two are completely different. 

Total solar eclipses are rare. Globally, they only happen every 18 months. In any given spot, they are much, much rarer, with a recurrence period of many hundreds of years. Solar eclipses are localised in time and space. Although model resolution may be just about small enough (in time) to resolve the totality, it only has a limited impact. One could implement solar eclipses in weather forecast models, but there's plenty of more important things to worry about improving. Operational weather forecast models are complex beasts, so you really want to change things only if it may yield a significant improvement. The potential improvement of implementing a consideration of solar eclipses is minimal, so it's simply not worth the effort or the risk. Experimental models are a different category and as you have seen, eclipses have been implemented in those. But the risks involved are much smaller; nobody really cares if experimental models go totally wrong, but you really don't want to be responsible for breaking the operational weather forecast model. 

Yes, but this is not mainly due to the small intensity variation of sunlight with distance. Rather, the elliptical orbit affects the length of the seasons, which — along with other orbital effects — triggers the ice ages. Currently, in the northern hemisphere, summers are longer than winters, because of Kepler's orbit laws and the fact that perihelion is in the northern hemisphere winter. However, there is precession in the date of perihelion, and periodically over tens of thousands of years, it shifts into the northern hemisphere winter. This causes enough of a difference to trigger the ice ages: see Milkankovitch cycles on Wikipedia. 

Convective overshooting tops reach above the normally horizontal flat layer of the convective system, a layer that should coincide with the tropopause. If we have such an overshooting top, does this mean we actually have the cloud reaching into the stratosphere, or does it mean that the convective system locally pushes the tropopause upward, while the convection is still limited to the (temporarily extended) troposphere? Both alternatives are given as unsourced answers to this identical Yahoo! Answers question, so I would like to see a referenced answer that specifically addresses the distinction between "air parcel reaches stratosphere" or "air parcel pushes tropopause upward". (Note that this is not merely a matter of terminology, but should affect troposphere-stratosphere interaction in general) 

In a recent met-jobs job posting (different version here) for a researcher with a background in climatology/meteorology/hydrology or environmental sciences, I saw the phrase breadboard retrieval algorithm (emphasis mine): 

Official meteorological stations should have well-calibrated thermometers. In any comparison between models and measurements, the measurements would be the reference, not the models. One way I can see to validate ground-based in-situ temperature measurements is by carrying an SI-traceable thermometer to all the sites. Unfortunately, you cannot do this retroactively. I don't know to what degree thermometers at official meteorological stations are SI-traceable. Another way would be to look at spatio-temporal series, and search for outliers that way. This would not be a comparison between different independent methods, but would reject a particular measurement if it is too far off compared to measurements nearby in space and time. To find the right criteria to reject data is a delicate balance, and you will certainly end up either rejecting good data or accepting bad data. 

The one you are most likely to find on meteorology websites is the dew point temperature. Basically, if you have two of temperature, dew point temperature, and relative humidity, you can calculate the third. For example, see this University of Miami calculator. 

I'm not entirely sure how the calculations in the linked article are performed, but I recall calculating this as a student and ending up somewhere around 5 km, so it sounds right. Simple calculations often assume an isothermal atmosphere, which is of course not accurate, but good enough if you don't need to know the answer more precisely than within several hundred metre. This page at Stanford reaches the same number. 

Primarily because of inertia. This phenomenon is called seasonal lag. It is true that the December solstice is the moment that the northern hemisphere gets the lowest total amount of insolation. Conversely, the June solstice corresponds to the moment of maximum insolation. The oceans, and to a lesser degree the land, absorb a lot of heat. Due to the large heat capacity of water and the large amounts of water, the seasonal lag can be considerable. In general, mid-latitude areas near large bodies of water have the largest seasonal lag, to the extent that the warmest month of the year in most of western Europe is August. It is illustrated very well by this Warmest day of the Year map that NOAA NCDC (now NCEI) produced for the contiguous United States: 

Yes, mean cloud cover is routinely measured from satellites. Like all satellite data (and in fact all measurements), it does have an uncertainty, but for the purpose of this question the satellite product of mean cloud coverage is good enough. Personally, I would hesitate to trust the data at very high latitudes with frequent cover of snow or ice, because those are hard to distinguish from clouds with any form of passive down-looking remote sensing. The figure below is released by ESA and derived from the MERIS and AATSR instruments on the former ENVISAT satellite. Although the caption does not specify, I suspect that it uses visible channels and that it therefore only relates to daytime cloudiness. It would be interesting to see how it compares to 24-hour cloud cover! 

As a lecturer I had during my undergraduate put it: if you draw your information from BBC/Discovery Channel documentaries, you will believe we could get a 100-million-people-killing disaster every week. The collapse of the Cumbre Vieja is but one of those perceived threats. How do such documentaries get their info? Scientists develop models, then run them. Remember than all models are wrong, but some are useful. Using their models, they speculate that if the substance of the mountain has a certain shape, and if the volcano erupts in a certain way, and if this causes a landslide in a particular way, with a couple of more ifs, then they speculate that there might be a huge tsuname destroying the eastern US. Do they state this with certainty? Absolutely not. Do other scientists agree with them? Not really. Does Discovery channel care that it doesn't appear very likely? No. The already linked Wikipedia article Cumbre Vieja#Future threats answers this particular one better than I can do. Don't worry and sleep well ;-). Oh, and you must visit, it is spectacular! 

La Palma, 27 December 2012. Certainly, the inhabitants of the shores of the Cumbre Vieja are more at risk than the inhabitants of the US east coast. (P.S. I'm cheating, this is actually some 15 km north of the dangerous part of the slope. But still, if this mega-tsunami does happen, I think the TV networks will forget about the poor islanders :() 

You might be interested in this article on The weather’s response to a solar eclipse that my colleagues wrote. 

According to Rodgers (2000) equation 2.80 $$ d_s = \mathrm{tr}(\mathbf{A}) $$ where $d_s$ is the number of degrees of freedom for the signal, tr denotes the trace, and $A$ is the averaging kernel matrix. I'm trying to properly understand this. For example, is it theoretically possible for the averaging kernel to have mainly or even exclusively off-diagonal elements? In practice, this would mean that the retrieved state at height $h$ would correspond to the true state at height $h+1$, if we take remote sounding of an atmospheric column as an example. In such a (admittedly contrived) example, we could get $d_s=0$, although we do have information in the measurement — it's just displaced. Is this situation possible? If yes, does $d_s$ tell the whole story as for the information content in the measurement? 

Have we seen any changes in the flux associated with the recent "slowdown" in global warming? There is no evidence for such a slowdown, nor for a change in the associated flux. Atmospheric warming has not been significant in the past 15 years (nor can we significantly state that it hasn't warmed), ocean warming has continued. As the oceans absorb the vast majority of the excess heat, this net flux probably has not significantly changed during the "slowdown". Note that 15 years is in any case too short to state anything about global climate change. As for your other questions: estimates of the flux have certainly changed over the past 20 years. but estimates vary quite a bit, so I don't think we can significantly state that there has been a change in the fluxes over the past 20 years. Nor can we state the reverse; it appears further research is needed. For an excellent and still somewhat recent overview article on the topic, see: 

However, in reality, it's more complicated, because there are other factors that come into the equation, such as surface roughness, which depends on wind. Remote sensing problems are often underdetermined, and we need to make assumptions in order to retrieve the desired quantity. Measuring ocean salinity from space is a classical inverse problem. You can find details in the highly cited paper: 

This is not a complete answer. One aspect of weather models consists of Data assimilation or 4D-var. I agree that they are amazing, and the question how do they work is too broad to be answered. So I recommend you read up on data assimilation and in particular 4D-Var. Concepts are somewhat similar in inverse theory, but of much higher dimensionality. In a tiny nutshell: 

In general, climate is the statistics of weather over a long period of time. In general, an individual weather event cannot be attributed to global change / global warming. An increase in the frequency of such events could be attributed to climate change. But considering how noisy weather and climate are, it takes a very long time series to measure an increase in frequency, in particular for rare events. That being said, if an event occurs that is extremely unlikely under the null hypothesis of a stationary climate, but only moderately unlikely under a changed or changing climate, one could consider that this event likely means that climate has changed. But one needs to be very careful with such an analysis, because even unlikely events do happen, and on a global scale, the probability that this year there is a one-in-a-million-years event somewhere is far larger than one in a million. And we'd still need to be sure that we understand the present climate correctly — we could be underestimating the likelihood of an event in the prior climate regime, in particular if we have no prior experience with those events. So to summarise: for a single event, we cannot say if this polar blast is caused by global warming. If the frequency of those increases along with global warming, then we may be able to establish a causal relationship. Or not.