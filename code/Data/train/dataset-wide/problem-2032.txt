This is in addition to other IT concerns you'd want to keep in mind like maintainability, support contracts, and your space/power requirements. 

However I would like to have that integer replaced with a variable that is the RunID of the row updated (which is causing the trigger to fire). How can I do that? 

The total number of times it will iterate is 150, however it is stopping at 100 connections and I can't figure out why. I could adjust my script to just use a single thread, but I'd prefer to know where I'm missing a max connection setting as that will be more useful to know for future reference. Here's where I've checked so far: 

However I can't figure out how exactly I need to escape the quotes. It always wants to treat the as a separate column. How can I escape the comma properly? 

The way to gauge hardware performance is to be take a look at your current workload. So I would begin by quantifying that: 

After a test completes (on a SQL 2008R2 system), the system updates a row in the TestRun table to include an end time. I am creating a trigger that will take action after that update, and do some post-test analysis for me. Right now it's very simple. 

I'm not sure of the change that caused this (the logs don't show anything specific) but ever since Monday afternoon I've seen the following error repeated: 

Currently this takes something like 3 minutes to run. Is there a better way to get the information I'm looking for? 

This might make sense because the logic of "extending rows" by large objects is at odds with the post-9.0 large object permissions logic. The difference is that future rows in a table have access rights predetermined by GRANTS to the table, whereas the concept of access rights applying to not-yet-created large objects does not exist. Lastly, if there is a way to reconfigure your application so that it uses fields instead of large objects, that would be an ideal solution. 

Files below are supposed to be data files that are not meant to be edited. They are not live configuration files. The FHS spec at $URL$ defines /usr as: 

This forces plpgsql to avoid pre-processing the query, and it's somehow the generic workaround against this kind of errors. But anyway it looks like pgsql_fdw is quite experimental at the moment, it's not included in the PostgreSQL contrib directory, and its source code is suprisingly not available for download at: $URL$ where we would expect it. Currently, you may have more success with dblink, the "old way". 

Identifiers are limited to 63 bytes by default, which doesn't qualify as "extremely long". Quote from the manual: 

Yes, dropping the primary key constraint will drop the index. But it should be noted that serial is not a real type, it's a shortcut for , where is auto-generated. You don't say what is the current column type, but since you expected to be able to reuse the index for the serial column, let's assume it's INT. It's not obvious in the question if you plan to ultimately drop the old column, but if you just want to transform it into a serial column, dropping and recreating the associated constraints is not necessary. Just create a sequence, initialize it to the next value of the primary key, and let its nextval be the default value of the column, as in: 

So the first thing I did was to make sure that the instance is accessible via SSMS (it is), and that my CMD prompt is Administrator (it is). Then I checked to make sure the instance has TCP enabled (it does) and allows remote connections (it does). What am I missing? The commend I use, minus the script, is: 

Currently I run when I want to wipe information out of the buffer pool between running SQL queries. However, I was reviewing this Technet article referencing . What caches does wipe that doesn't? 

I'm running SQL Server 2008 R2 SP1, on a Windows Server 2008 box. I have a .NET script running from Visual Studio 2010 that does the following: 

I have the need to restart Microsoft SQL Server's group of services on a given computer, remotely, through some sort of programmatic method I can insert into a workflow. I'm looking for the best way to do this, I've seen a few different ways and I'm wondering if there's anything I'm missing: 

When I restore after dropping the database, the fastest I can get a restore going is 4 hours. This is when I put the .bak on a RAID 10 logical drive, with the data files being written to a separate RAID 10 logical drive on the same server. The log files go to yet another RAID 0 on the same server. However, if I restore with Replace, the restore process only takes 56 minutes (similar setup). Did I find some sort of turbo button? This is so fast that I am worried. 

The SQL Server Database Engine uses the following procedure to accomplish this change: 1 - Adds a new column to the table in the new storage size and format. 2 - For each row in the table, updates and copies the value stored in the old column to the new column. 3 - Removes the old column from the table schema. 4 - Rebuilds the table (if there is no clustered index) or rebuilds the clustered index to reclaim space used by the old column. 

and that worked for me too. to run ssms press the and on the ssms icon and that will give the options below: 

Missing Join Predicate the Missing Join Predicate has been discussed here and Here too. Missing Column Statistics I have fixed it, or at least got rid of the warning by simply running 

then I want to see all the permissions of the objects. when I check my stored procedure I run this script: 

I have inserted your values into a table variable just to so I could pivot from it later. The code came out like this: 

Run SQL queries from data server (you need to be remotely connected to the database server) Enable use of Kerberos on the database server Set proxy account for linked server, so that MDX queries are executed in its context instead of in context of the user that is issuing t-sql query: 

have fixed sqldwdev01 the reason I couldn't connect to UAT instance is because of a couple of reasons 1st was there was a dynamic port specified for the instance under SSNetworkConfig\Protocols for UAT so had to remove that and specify 1435 (as 1434 is in use) restart the service still cant connect remotely to the instance name but can connect using the port name eg sqldwdev01,1435 so it needed sql browser service to be running to direct the port for connecting with instance name i started sql browser and now can connect using sqldwdev01\uat 

I'm not sure where else to check, I know I have a lot of moving parts here but I'm getting the feeling I'm just missing a max pool setting somewhere. 

I'm gathering wait types on a load test environment, and the second biggest wait type is . The stats are coming from sys.dm_os_wait_stats, but I can't find any documentation that explains what that particular type means. I'm running SQL Server 2014, SP1 on a physical Windows 2008 R2 instance (no VM involved). 

So I have a database (on a SQL Server 2008 R2 SP2 instance) that is 2TB uncompressed, with a .bak file that is 500GB compressed. As I have been toying with the fastest way to restore this database after a series of tests, I've tried a couple of things: 

I have 2 instances of SQL Server installed on one server: SQL002. One instance is a default instance, SQL2008 R2, and the SQL service is generally not running except for specific tests. The other instance is a named instance, SQL 2014 (so SQL002/SQL2014). This is generally running all the time. Today I just had a developer tell me that when they connect to SQL002 in SQL Server Management Studio it will connect successfully. I replicated this from my machine and also see a successful connection, however when I look at the properties of the connection it is reporting that it connected to the named instance. What is going on here? Is there a redirect I'm not aware of? 

If you haven't found the difference is in the Login_From. Basically what I am trying to achieve here is to find windows_logins that belong to a windows_group login that are also present in the same server. Inside my procedure the critical line that get the windows group for an individual login is the following: 

when I run my script it shows me the next running time for the job in general, but I would like to see it by schedule too. 

Here is where I should have something to handle it. Let's keep it simple, and just say - if it breaks by a constraint violation - just don't delete the record. How can I get this done? 

I would like to drop all the jobs related to a set of specific databases. So I have added all the names into a table variable and I was trying to generate the scripts to drop the jobs. how can I achieve this? 

the problem is though, that sometimes, after for example a couple of day without running this procedure I get sometimes the following error message related to the linked server: 

is it because the inserted rows did not fit in a page of the clustered index, and therefore had to be created somewhere else allocating a new page for them? could this be an indication that the FILLFACTOR is too high? 

there were other things I did not include here: 1) before a restore - make sure all the processes that use that database are stopped. 2) before a restore - script all the permissions, so that you can re-apply them later on. 

You're right that SQL Server rarely bottlenecks at the CPU, but it's not impossible and you can tell whether you're seeing a CPU bottleneck by looking at some perf counters. Then you'll know whether or not you'll see a performance improvement simply by updating the CPU to a faster one or one with more cores. If your SQL isn't written to take advantage of parallel processing, it won't matter if you throw more cores at it. Additionally, there's more things I would want to consider in buying a system: 

That exception usually doesn't refer to the SQL Server itself running out of memory, but instead the client workstation that is processing the results running out of memory. You will generally see this if you are outputting a very large result set into the Output window. To troubleshoot the issue (and possibly resolve it) allow SQL to write the results to a file rather than trying to put it all in the output window. 

Take a look at Brent Ozar's perf tuning stuff Identify your bottlenecks (are you seeing bad parallelization, CPU chugging away without finishing, memory, storage) Determine what differences there will be between your new and old system. 

I found the answer elsewhere on the Internet, courtesy of Grant Fritchey. "When you look at the fragmentation percentage, you're looking at external fragmentation. It means the percentage of pages that are out of order showing an inneficient storage mechanism." $URL$ So it's how many pages out of the whole are out of order. 

Run to check the actual contents of the latest log file. If these warnings are the reason why the log files are huge, they can be muted by setting to OFF in See the compatibility notes in postgresql 8.4 docs for why this warning kicks in. The other default logging settings of PostgreSQL on Ubuntu are: 

(in more recent versions, this advice has disappeared because VACUUM FULL has been reimplemented differently). See Routine reindeing and the REINDEX command. The easiest way to reindex is to connect to the database with the db user that owns it and issue: 

(emphasis mine). This seems to support what you wrote in the question: a specific column of a specific row. 

With PostgreSQL 10 or better, variables are also expanded in backtick-executed strings, so we may also call an external evaluator through a shell: 

or to get a single byte as an integer. See Binary String Functions and Operators in the doc for all functions. 

The point of is to receive the data from outside the query. The closest syntax to what you're asking is 

Usage: the same as pl/perlu version. You still need to be superuser, unless the superuser privileges for this function only are granted to other users (which in the present case would be only OK if 100% trusting the users, otherwise it's disastrous): 

No, this whole log looks fine and tends to show that pooling is working. These entries with being tagged as , they concern communication between a client and pgBouncer. just means that the client used the connection for less than 1 second, which is consistent with the dates with milliseconds appearing in that log. A line 1, a client connects to pgBouncer. At line 2, there's a first connection made with a new postgres backend. At line 3, the client quits. At line 4, a new client arrives and no new connection is made with a backend, which implies that the previous one is reused as expected. At line 8, a second server connection () is made with a new postgres backend, which is necessary since there are two concurrent clients (see sucessful login attempts on the two previous lines within the same second). Then the clients quit, and there's no mention of connections to postgres backends being closed, as expected again. They should be kept open until they reach or when idle.