This recreates the table, and migrates the data and indexes into . If you are willing to do this 240,000 times (6000 X 40), be my guest. WHAT CAN YOU DO FROM HERE ? You could setup MySQL Replication and perform backups on a Slave You do the following 

Believe it or not, although the products went EOL December 18, 2009, the last versions of MySQL Query Browser and MySQL Administrator are still available for download for free. Hey, I still use them today. Click here and get it while Oracle isn't looking ... QUICK !!! 

If you have to do a bulk insert, you need to use along with a larger bulk_insert_buffer_size. Here is why: 

The first line tells the DB Session not to record the SQL that follows. Thus, when you run the DELETE on the Master, the SQL will not be written in the Master's binary logs. Consequently, the Slave will never receive the DELETE in its relay logs. This will only affect the session you use to run these two lines. All other DB Connections will replicate as usual. Once you disconnect, any new session will replicate properly. Give it a Try !!! 

Instead of checking to see if the data already exists and then doing an , try doing an and seeing if the came back with the same value or not on two calls. 

I have a little surprise you : Did you know that if you have the SUPER privilege, you can write to a read_only database ? According to the MySQL Documentation on read_only: 

Now, you have a static trendCutYear. You can now query using a static value in the middle of the index. The query can now be rewritten using trendCutYear instead of trendCut: 

Four and a half years ago in my first post to Any option for mysqldump to ignore databases for backup?, I collected the database names to memory rather than to disk. Here is the code from my first answer 

Your mission is to parse the mysqldump I just mentioned and get each table description loaded into this table, along with the database anem table name. Once you construct such a parsing program, you could do this process every hour and update your dynamic documentation. Give it a Try !!! UPDATE 2012-04-01 00:39 EDT Here is modified suggestion: Given the mysqldump suggestion I made, your should load tMySQL Schema attained from the mysqldump into another DB server that contains no data. Set it up as a replication slave using replicate-do-db=mysql. That way, no data will collect in the slave. You can use this slave as the source of the mysqldumping of the DB schema. This separates the data from the schema. You can use your documentation system fetch the schema from the slave. 

EPILOGOUE Suggestion #1 may be all you need. Suggestion #2 is an alternative means of deleting old rows. YOUR QUESTION If you have binary logging on the Slave and the Slave is not a Master, disable it. If you want to see which queries are expanding to rather than , use mysqlbinlog against the binary logs on the Master to see a text representation of all queries. The row-based ones will be a little obfuscated. The statement-based queries will appear as it was executed. 

If scaling out DB Servers is not your cup of tea, then Amazon RDS is OK to use because all bells and whistles come with it. Those who simply want moderate HA, backups, and scaling out benefit a great deal. On the flip side, if you want to scale up hardware, that is out of the question for RDS. What if you want to scale up MySQL's capabilities? Unfortunately, that is out of the question for many aspects one would want. For example, did you know that two fields are capped across all seven(7) RDS server models? 

after doing that mysqldump ? My guess would be yes. If neither of these are the case, then your table has some corruption in the . Try installing MySQL on another DB Server and loading the SQLDump there. If nothing errors out, the mysqldump is fine. This verifies some corruption. Try dropping the problem table, create it from scratch. Then, load it back. You could run myisamchk against the MyISAM table. 

mysqld looked for the grants, not in , but . You cannot yank grants on individual database from a user with global grants. 

Personally, I would not risk moving the installed binaries laterally to a newer OS because you may have to drag along needed libraries WHICH MAY NOT WORK IN THE NEWER OS !!! You should thoroughly look through MySQL's Archives, which goes back to MySQL 5.0.15. SkySQL also tries to fully accommodate users of older versions of MySQL, MariaDB, and more. Please go to their repository and find the version you need. They go back to MySQL 4.1. You should make sure SkySQL has RHEL6 builds, at least. You could also try compiling MySQL from source within RH9. DISCLAIMER : Never Built MySQL From Source 

There are those rare times when an InnoDB table gets stuck in the ibdata1 deeply that you must try forecing mysql to start up in spite of it. Try using innodb_force_recovery in /etc/my.cnf (Option values 1-6) 

Why is it important to have mysqld and mysqld_safe using the same MySQL version? Let me illustrate it this way: Percona Server sometimes has additional features in mysqld_safe for manipulating the OS. For example, I have seen in a Percona Server mysqld_safe. If that line was not there, the mysqld for Percona Server may run into issues with memory and swapping. The same scenario could possibly be the case for Oracle's (ugh, still hate saying that) mysqld and mysqld_safe. There could be improvements from one major release to another that would be removed if the mysqld_safe was older. Rather than exploring the possibilities of using a old mysqld_safe and a new mysqld (or vica versa), please make your life simple and reinstall MySQL 5.5.30 from scratch. Before doing so, please run 

indicates you used mysqldump from apple-darwin9.5.0 (i386) binaries shows the version of mysql you used mysqldump to dump from. You wanted to load the mysqldump file into 

In my particular case, the Slave would crash trying to interpret the binlog event. What would happen is the following: 

Your server class is too small for your InnoDB settings. On , I mentioned how the storage engines have distinct ways of caching. 

This is a very tricky question because of the internals of the ARCHIVE storage engine. People have asked this same question in the MySQL Forums 

In Query 3, you are basically executing a subquery for every row of mybigtable against itself. To avoid this, you need to make two major changes: MAJOR CHANGE #1 : Refactor the Query Here is your original query 

This will traverse the index one one key per query SUGGESTION #2 You should shrink the table after mass deletes. 

The second paragraph is not in the 5.1, 5.5, or 5.6 Documentation. This may have been left out of the Documentation when MySQL 5.1 went GA. If this is still happening, this is a big oversight. Let try this out on MySQL 5.5.37 

GIVE IT A TRY !!! It's better to insert 4 Million Rows than join 3,311,651,902 (396178 X 8359) rows. 

Why do two sets of heavy lifting ??? Let MongoDB do it. For this example, suppose IP of is with hostname STEP 01 : Enable Replica Set on the PRIMARY only Make the following changes in on the PRIMARY 

This being the case, you would really feel efficiency or deficiency in the following areas: DiskSpace Having the single auto_increment column as the keeps the overall size of the PRIMARY KEY smaller that that of having two columns. Why? BTREE pages would be twice as big if the was two INTs instead of one. This becomes even more painful if you use foreign key constraints and secondary indexes as they must subsequently blow up in size as well. In this instance, you would choose the second schema for better index usage. Insert Performance Inserting row data into an InnoDB table that has two UNIQUE indexes calls for twice as much BTREE management and unique checks. 

Log out and login to MySQL and you should notice the difference NOTES If you are logged out, you have to come back in an hour and try again. If you can't wait one hour and you can restart mysql, you could enable 

Since MySQL grants are checked in memory, then one of two things must happen SCENARIO #1 If you ran standard GRANT commands to change the password, the password change is recording on disk via automatically. (See Section 34.1.2 "The Grant Tables") SCENARIO #2 If you hacked the password in, as follows (shown in Section 35.5.1 Page 498): 

If the resulting script has no ALTER TABLE commands, you don't need anything done. If there are ALTER TABLE commands and if you are ready, run the script with this 

Let's say you want to make (mounted on ) the location of the database. Instead of doing you would do the following: 

You now have 6 snapshots of the data based on how much could be recovered. You would then have to load each MySQLDump into a separate instance of MySQL. You would have to then peruse the data and determine if enough of the data has been recovered. Percona has a Data Recovery Toolkit that would do all this way more efficiently than I am saying it. My answer is simply a poor man's approach to this. I hope this helps !!! 

Here is something you may find intriguing SUGGESTION #1 : Let mysqldump create the batches mysqldump already uses an option called --extended-insert. This will group a series of records that can be loaded in a single INSERT command. While you cannot use mysqldump to control the number of rows to be batched together, you can parse a mysqldump. Of course, these means you will have to script it in the language of your choice (Python, Perl, C++, etc). For any given table in a mysqldump of a single database, you will see the following paradigm: 

Subtracting 120 backs 2 hours away from midnight of the next day makes the last line's end 10:00 PM Give it a Try !!! UPDATE 2014-04-16 09:44 EDT I cannot really test this, but you can try this: 

SUGGESTION #3 After performing the mass delete, the index statistics need to be recalculated. does it for youm but you can do a as a separate step, like this: 

I don't see anything along those lines in the Percona Toolkit Documentation. There is something you can do to cheat Scenario 

You don't need since all the other indexes begin with The additional indexes may hopefully be used in each in the Give it a Try and See If It Helps !!! 

According the info you just added to the question, each tag has two fruits. Please note that each tag field (holding two fruits) is unique. You should be storing the fruits as separate fields. The GROUP_CONCAT function is for query aggregation, not set manipulation. The only way to use the view is to do this convoluted solution I reloaded the data like this first: 

Step 1 flushes everything InnoDB not yet committed to disk. That makes for a faster mysql startup. From here, you have the option of changing settings Give it a Try !!! 

Your first works well. I am sure it is benefiting from the unique index as I said earlier, but IMHO I think the UNION is simpler. With than unique index, it would appear to be six of one and half dozen of the other in terms of execution and output. You would have to benchmark your first query against my suggestion UNION and see. This was a good question you asked today. +1 for your question. 

These represent the Latest Binary Log Event that Executed on the Slave. That is where you pick up replication from. Running will erase the relay logs and start with brand new relay logs. 

This will perform the table repair with mysql runnng. This will perform a full table lock so no one can access the table. TECHNIQUE #2 : Repair Offline To repair a table offline, move the files making up the table to another folder and perform the repair there. For example, to repair mydb.mytable using the folder 

then you have in your my.cnf. You should comment it out and restart mysql What you want to see is this: 

What concerns me here is the use of both on the Slave and on the Master at the same time. There is no need to use both. I would remove from the Master. I would also look over any queries with fully qualified table names. Why ? If you replicate any SQL use fully qualified table names (dbname.tblname) and you use replicate-do-db, MySQL Replication gets confused if you execute . Percona explained this in conjunction with and back in May 2009. Percona also suggested reading the replication filtering rules thoroughly. 

The book I mentioned recommends getting away from ANSI error codes and using the direct MySQL error codes. The reason? Page 133 has a box "SQLSTATE or MySQL Error Code?", which says the following: 

or you may keep this old index if other queries can benefit from it. I would start with this before deep diving into InnoDB or SSD. 

Even though the statements are executed in sequence, if they exist inside the same transaction, you must issue some kind of checkpoint between queries or tweek the transaction isolation level before starting the transaction. There are four values for tx_isolation: 

The only situation I can think of is reloading a large mysqldump. Why ? Check out this Pictorial Representation of InnoDB (Percona CTO Vadim Tkachenko) 

What was net_store_data doing at that moment ? The book Understanding MySQL Internals says something about net_store_data and NULL values on Pages 74,75 under the heading Server Responses subheading Data Fields 

These will recommend the right sizes for the MyISAM Key Cache for existing MyISAM and the InnoDB Buffer Pool for InnoDB data and indexes. While running these SQL queries recommend the buffer sizes, the human factor must still kick in. We must plan data usage based on hardware and frequently accessed data. We must also ask: Of the amount of buffer recommended, how much of my working set will actually reside in memory? If you configure MySQL to use 75% of RAM for InnoDB, even on a dedicated MySQL server, the OS will get a busy paging to disk. Just because MySQL can use huge numbers for buffer sizes does not mean it's OK to push the limits. Two have two basic storage engines to consider. Use of commodity hardware merely demonstrates MySQL configurability with noticeable results. Unless you are using 32-bit RHEL (if you are, please stop using it right now, get plenty of coffee, go upgrade), raising buffer sizes in storage beasts can be well trusted. For example, my employer has a client with 3 DB servers dedicated to MySQL only. Each has 192GB of RAM of which 162GB is the InnoDB Buffer Pool. The working set is actually 170GB. While there is a little swapping and paging going on, the DB performance is astounding. OS has about 30GB of RAM for itself (16GB of RAM disk for temp tables). I am sure that these servers are just little lizards compared to the komodo dragon-sized DB servers out there. MySQL can be up to the task if configured properly. Keep in mind that InnoDB puts its own checks and balances in place because the current source code limits InnoDB log files to 4GB. This is the case because InnoDB was originally designed with commodity hardware in mind. In light of this, ACID transaction throughput could possibly bottleneck there regardless of the storage beast's hardware setup and the version of MySQL you choose to run. CONCLUSION It's OK to push limits on Big Beefy DB Servers. You should always do so with consideration to the OS in terms of Memory, Swap Space, Working Dataset, and number of transactions expected. 

The only place I can think of where these number would make sense would be in the network. These status values represent the amount of data passing in and out of DB Connections. These bytes would most likely be visible from another perspective: the Operating System. You could measure the amount on incoming/outgoing traffic in netstat against MySQL's view of it. If the amount of incoming data is low, or if the amount of outgoing data from MySQL is significantly higher than netstat says, check MySQL and/or the network. You may also want to look for any signs of dropped packets along any interfaces. In light of this, when it comes to tuning, the only thing I can think of that you may want to tune is setting two things: