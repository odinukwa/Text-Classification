There sure is a lot going on here so we will just have to see where this leads. First off, the difference in timing between SQL Server 2012 and SQL Server 2014 is due to the new cardinality estimator in SQL Server 2014. You can use a trace flag in SQL Server 2014 to force the old estimator and then you will see the same timing characteristics in SQL Server 2014 as in SQL Server 2012. Comparing vs is not fair since they will not return the same result if there are more than one matched element in the XML for one row. will return one row from the base table regardless, whereas can potentially give you more than one row returned for each row in the base table. We know the data but SQL Server does not and has to build a query plan that takes that into consideration. To make the query equivalent to the query, you could do something like this. 

If you are on SQL Server 2008 or later you can use the Table Value Constructor instead of as suggested by Vladimir Baranov in a comment. 

I see no benefit of having two indexes when one is enough to get the job done. Removing the extra index will save you disk space and IO cost when updating the table. 

If you don't specify the node, SQL server have to generate a plan that works with mixed content XML, concatenating all the node values from the sub nodes. 

You can generate the extra row(s) by doing an unpivot using a cross apply and the table value constructor. 

You need to trace the source of the data backwards to the producer to see where you end up with invalid XML. 

From Error message 1785 occurs when you create a FOREIGN KEY constraint that may cause multiple cascade paths 

If you want the index to be used for the function you should match the data type in the index with the data type used in the query. You have in the query and in the index so and is not used in your query. It is instead parsing the XML data using the table valued functions. To make your query use only the indexes (no xml parsing functions) you can change the query and the index to this. 

The query cost calculated for a query is an estimate. Even in the actual execution plan it is the estimated cost that is shown. For some reason SQL Server Management Studio uses those estimated costs and comes up with a cost percentage for each query in a batch. Generally speaking that percentage has nothing to do with the actual performance of the query and should (in my opinion) not be used. Estimates are based on statics of the involved tables and columns but XML variables (and columns) does not have any statistics that describe what shape for form they have so for XML queries the estimates are even less useful then for other types of queries. Measure the actual performance instead like duration and IO. 

Update: I took the liberty to execute the test rig provided by wBob on SQL Server 2014 with Compatibility level 110 (SQL Server 2012) and 120 (SQL Server 2014) Result: 

The XML you have is invalid in UTF-8 encoding. The accented characters needs to be encoded. For instance should be encoded as . Here is a shorter version that also fails. 

You can use the third parameter of that is used to specify where in the string the search will start. 

Update 2: If you know for sure that your value never contains a period and that it is always a four part name you can use parsename. 

The best workaround in your case is probably to get the values you need into a temp table and then query the temp table with a cross apply to the UDF. That way you are sure that the UDF will not be executed more than necessary. 

Some parts could do with some more information. This extracts the last number in . So will give you a . And this part returns all but the last number. For it will return . I shamelessly stole the test data generated by wBob (thanks and much appreciated) and found out that this version is faster on my machine. I use a really ancient 2 core laptop for the tests so the result could be different on a real server. Any way, for me it took 3 minutes to execute the code by wBob and my solution was about 20 seconds. 

You can use the the with an clause partitioned by to get the max date for each venue and then use that as the first column in the order by clause. 

You can use the Table Value Constructor in a cross apply to generate two rows for each row in the source table. 

Left join on dbo.CS here makes the query use batch mode and the trace flag 9453 disables batch mode. 

SQL Server expands the view and considers accessing the base tables instead. If the view or the base tables are used is a cost based decision by the optimizer. To force SQL Server to use your indexed view you should use the noexpand hint. 

If is not the first element under the query need to be modified to add all elements before first and all elements after after. 

You can use a full outer join between your tables and check against null values to figure out where the data comes from. 

Use value() instead. The value() function returns a scalar value and takes two parameters. The first is the xpath expression and the second is the data type you want. 

Not sure I actually provided a definitive answer for you here but I thought this was too much for a comment. 

The query plan for the XML query shows a table scan of and a nested loops join with a seek on the primary key into the system table that holds the value you are looking for. 

Yes there is. You remove the directive from the statement and you remove the call to the function. A side effect of doing that is that values you concatenate that contains characters that needs to be encoded in the XML like will be encoded in your result. 

The warning is there because of the XML function . The second parameter to is what you want the value stored in the XML to be converted to. You could argue that this is not in fact an implicit conversion but a very explicit conversion since you are asking for it to happen. Perhaps something for a connect item to suggest to Microsoft. Simplest way to reproduce what you see. 

Vote on this connect item for a change of behavior. You can use for xml explicit to build your XML instead. Something like this: 

Nothing comes to mind that would improve the performance of your query so until you actually see a performance issue with this query you could use it as is. Provided it does return what you want. 

Can't really answer that. Some internal workings of SQL Server creates a result like that, sometimes. The technique has been around for a considerable amount of time. Microsoft does not support the functionality and advises not to use it. From SET @local_variable (Transact-SQL) 

I really can't answer that but thought I should share what I know anyway. I don't know why a scan of PRODUCT table is considered at all. There might be cases where that is the best thing to do and there are stuff concerning how the optimizers treats UDF's that I don't know about. One extra observation was that your query gets a good plan in SQL Server 2014 with the new cardinality estimator. That is because the estimated number of rows for each call to the UDF is 100 instead of 1 as it is in SQL Server 2012 and before. But it will still make the same cost based decision between the scan version and the seek version of the plan. With less than 500 (497 in my case) rows in PRODUCT you get the scan version of the plan even in SQL Server 2014. 

You can convert the version string to XML and then extract the four parts replacing no value (nulls) with a 0. Put the version back together and compare to your original table. 

Speculation on my part. There is a cast on a column that is used in the where clause which make statistics of that column interesting. A change of datatype makes the statistics no good so lets warn about that in case the value from the field list might end up to be used somewhere. 

The syntax you have is correct from SQL Server 2005 and up. You are using SQL Server 2008 so it should work just fine unless you have compatibility level for your database set to "SQL Server 2000 (80)". Make sure you have compatibility level set to at least "SQL Server 2005 (90)". Right-click on your database - Properties - Options 

The reason your function takes ages is because you have empty values for in . The patindex expression returns 1 when you check for an empty actual so you never exit the inner loop. You can fix that by adding to the query against . Next issue is where you use as parameter to . The parameter should be an integer so if you remove your code returns something but I don't think it is what you are looking for. is translated to . Another approach you can try is to use the XML capabilities in SQL Server. XML in SQL Server is UTF-16 but it is able to load UTF-8 encoded strings and that can be used. Concatenate your string with a UTF-8 xml declaration and use the function to fetch the value from the constructed XML. I guess you eventually want to use this on a table so here is an example that uses a table variable. 

A selective index will not be used when using to retrieve the data. From Selective XML Indexes (SXI) - Supported XML Features 

For the future you can use a database trigger to log the DDL events. SQL Server DDL Triggers to Track All Database Changes 

SQL Fiddle Update: I am not really sure how you want to deal with different within a . The above query groups on and but I guess an alternative would be to use the lowest per in the function. 

You can do this with a loop one level at a time. Use merge with output to capture the generated along with the XML that will have that id as a . 

I have not been able to produce a query that use XML indexes using the method so I can't really tell you in what cases it may work. If you want to tackle a performance issue with your query I would recommend you to shred the XML using and and then reconstructing the XML using . It is usually faster than building the XML with and you could also probably make use of selective XML indexes if you need it.