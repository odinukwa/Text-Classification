William Lycan's book is easy to read, though perhaps a bit dry. I've used it for an introductory course once. On the other hand, you could also just start by reading original texts. Basic Topics in the Philosophy of Language, edited by Robert Harnish is one of the best collections of seminal articles. It seems to be out of print at the moment, but perhaps you're lucky and spot it in a used book store at a reasonable price. 

In other words, the so-called paradoxes of the material conditional are accepted, as the conditional is only a truth function, but their analogues for logical consequence are avoided. Confusing logical consequence with the material conditional is considered a grave conceptual error in this view. This work is only available in German, see this German Wikipedia link. Relevance logics have been developed in the Anglo-American tradition to alleviate the problems of the material conditional. Since they change the conditional itself, as far as I know adequate versions of the deduction theorem hold in them, so they do not speak against your point of view. 

The position you describe seems to be a brand of moral intuitionism, but I haven't heard a name for it yet. It's worth pointing out, though, that this theory does not work, because there are no such universal moral intuitions (e.g. what constitutes a 'good' reason for killing someone in one society is a 'bad' reason in another society, and vice versa) and the question of which moral theory is the 'best' is itself morally relevant. 

Trying to come up with 'reasonable' answers to empirical questions without studying nature is indeed fiction or scientific speculation, not philosophy as it is understood in the contemporary academic community and also not empirical science. Commonly accepted are thought experiments, all kinds of more or less counterfactual speculation, and similar kind of hypothetical reasoning. Also commonly accepted nowadays are philosophical theories that are informed by empirical results, and if they can be shown to be empirically adequate many contemporary philosophers would consider this a benefit. Substantial arguments based on purely speculative answers to empirical questions without further support by empirical studies are not acceptable. (Of course, you might always find a philosopher who disagrees. I'm describing a general consensus among professional philosophers, what you could call the 'mainstream' position.) 

This is probably not what you're looking for, but in classical economics a violation of Pareto efficiency could be considered a form of moral imbalance. (Pareto efficiency itself can also lead to constellations we consider immoral, though.) Then there are also theories of fair resource allocation in which people try to find principles for balancing resources in a fair way. For example a transfer that takes away an amount a of goods from a rich and gives this a to a poor and afterwards still leaves the rich at least as wealthy as the poor is called a Pigou-Dalton transfer. There are many similar principles and many interesting impossibility theorems in this area. For example, you cannot have social welfare based on Pigou-Dalton transfers that always remains Pareto efficient. Unfortunately the math behind this is not very easy. Of course, this tradition is mostly utilitarian and usually concerned with goods, commodities, and welfare in the sense of 'utility', not with moral values. However, a mathematically sound theory of moral balance would have to use very similar methods and face similar problems as this research in welfare economics and social choice. Some people work on this connection, e.g. Vlodek Rabinowicz in Sweden. 

Often authors omit these kind of details because they want to keep proofs simpler. Many proofs are by induction over the structure of formulas, the more rules you have the more cases you have to consider. That's probably the reason why Ebbinghaus et al. omit these details. On the other hand, when you 'use' the logic rather than proving meta-theorems, it can become cumbersome having to expand definitions all the time, so in that case you might prefer to introduce more expressions into the language directly rather than defining them. It depends on the purpose and in textbooks also on your pedagogical goals. 

Deictics are also called demonstratives and in a broader sense also sometimes taken to comprise both indexicals and demonstratives. Indexicals are expressions that need to be interpreted by taking into account the utterance situation and deictic center I, here, now. Demonstratives additionally need to be interpreted with respect to an accompanying point gesture. German diese is plural of dies, meaning roughly these. It can be used demonstratively, as in "Welche h√§tten sie denn gerne? - Diese (+pointing gesture)." (Which ones would you like? - These+pointing gesture.) Da is not a demonstrative on its own, it can be a conjunction meaning because or as. It has a deictic use as sentence adverb as in "Er ist da" (He's here/He has arrived.) and a use as particle where it often combines with demonstratives and then roughly means near but not close the speaker, e.g. the answer to the previous question could be "Diese da (+pointing gesture)". Agamben probably has the text-deictic use of such expressions in mind, which structure narratives. For example, in the phrases "as I have shown there already" and "here we have a nice example of", the indexical "here" and demonstrative "there" are used text-deictically, referring to some previous discourse. 

You ask two questions, (1) whether it is beneficial to think in a logically consistent manner, and (2) whether it is always beneficial to think in a logically consistent manner. To answer these, we have to take a look at two different aspects of thinking. First aspect: Belief Systems Regarding your beliefs, I would say the answers are Yes to (1) and No to (2). It is generally beneficial to have consistent beliefs and as a rational animal you strife to eliminate inconsistencies by further thinking, experimentation, belief revision, and so on. It would not be beneficial to always have consistent beliefs (and also not be possible, because people make mistakes), because you aggregate beliefs and form knowledge from various different sources: perception, your own thinking, and other people by testimony. At least for the latter, it is beneficial to keep track of possibly inconsistent information in order to evaluate it later. It would be harmful to immediately revise all your beliefs in light of contradicting evidence or ignore contradicting evidence in order to forcefully maintain a consistent belief system. Second aspect: Drawing Inferences Here again the answer is Yes to (1) and No to (2), but for different reasons. First of all, not all inferences are logical in the narrow sense of being 'deductive' (as in classical logic). You also need to make inductive inferences and possibly abductive inferences, where it must be mentioned that the existence and justificatory status of abduction is highly controversial. These are not logical in the narrow sense but the have been studied by philosophers, logicians, computer scientists and mathematicians in great detail, too. Secondly, there are general complexity considerations that speak against the view that one can or ought to always make perfect deductive or inductive inferences. With an increasing number of variables and background assumptions deductive and inductive inferences become computationally intractable. You'd be paralyzed by your inferential processes, which would take too much time to complete. Instead, humans make a lot of heuristic shortcuts, which only work sometimes and lead to erroneous results in other cases (cognitive biases). In computer programming, shortcuts are also used, or alternatively incorrect simplifying assumptions are made. (For example, for a Bayesian network to become tractable you need to make strong assumptions about the independence of random variables which are rarely fully justified.) Bottomline: No, logical thinking is not always beneficial, but it is so most of the time - at least if time permits. 

If that's what he says then he's completely wrong. Deictics are communicative shortcuts which always refer to something else than their own utterance. The key difference to ordinary expressions is that you need to take into account aspects of their own utterance - who said what when in which way?, i.e. the deictic center and pointing 'gestures' understood in a very broad sense - in order to fully grasp what they refer to. This is not self-referentiality. It has been discussed under the label token-reflexivity, where a token is a physical linguistic sign. (For completeness, it needs to be mentioned that most contemporary accounts of the semantics of indexicals and demonstratives are not based on token-reflexivity but on David Kaplan's Logic of Demonstratives which is based on a two-layered model where linguistic meaning+ context= content and content+ circumstances of evaluation= extension.) The term shifters has sometimes been used synonymously to deictics, e.g. by Jespersen. There is nowadays also a use of the term as shortcut for "context-shifting indexicals", which is a special linguistic phenomenon where indexicals shift their deictic center from I, here, now to some center of a reported speech act. This occurs in languages like Amharic and Zazaki. However, it is unlikely that the author has this special use in mind. He probably just means "Da and Diese as deictics". 

Formal ethics comprises more than just deontic logic. Deontic logic is primarily concerned with a logical analysis of notions such as 'ought', 'must', 'may', or 'being permitted', whereas formal ethics comprises all ethical and moral reasoning that makes use of formal methods. Apart from deontic logic, formal ethics is also concerned with the modelling of formal systems of norms and rules (which are usually conflicting with each other, sometimes also defeasible and context-dependent), the formal modelling of the permissiveness of moral rules as 'soft constraints', dealing with vagueness of norms, laws, and moral rules, game-theoretical explanations of moral norms, formal models of distributional justice, the formal modelling of values including value incommensurability, moral issues of decision making under uncertainty (e.g. the Precautionary Principle), problems of value and preference aggregation, and utilitarian ethics in general. Some of these topics are also addressed in AI research, formal epistemology, game theory, and economics. Research in this area is fairly interdisciplinary. 

Giving up (a) is tough, I'm not aware of any logic where this doesn't hold. (But I wouldn't be surprised if there is one.) (b) does not hold in intuitionistic logics and also not in partial logics. Many logicians consider a failure of (b) to be desirable. (c) is given up in paraconsistent logics, e.g. in a logic with with 4 truth values you can interpret the values as {}, {T}, {F}, {T, F} where the latter is the case A & not A is true - dialethists do this to "deal" with semantic paradoxes. 

Yes, there are such systems, but they are rare. Most logicians agree with you. They want the deduction theorem to hold. Among the logicians who disagree are German logician H. Wessel and some of his scholars. In his theory of strict logical consequence A|-B only holds if and only if 

Logic is used in all branches of philosophy except history of philosophy and, as far as I can see, aesthetics. Logic is definitely used in epistemology, metaphysics, ontology, the philosophy of language, and ethics. However, only a small fraction of philosophers uses logic and even less of them make substantial uses of other broadly-conceived mathematical tools. Since you define "fundamental tool" as one that is always used the answer to your question is thus: Philosophical logic and all "branches" with the attribute formal such as Formal Epistemology. It is, of course, debatable, or perhaps just a matter of taste, whether to count these as branches on their own or not. 

The first problem is that you cannot tell what free will is without giving a definition, but it is hard to give an appropriate operational definition without already implicitly answering your question, viz. taking a stance. The second problem is that randomness can be characterized in many different ways and these definitions are not fully equivalent. Unpredictability is an important criterion in cryptography, but does not work as a general criterion for itself, for it is too unclear. For example, if you record a sequence of random numbers (generated from radioactive decay) for later use, you may "predict" them before they are used, because you have already recorded them, yet they would still be random numbers. Vice versa, many deterministic processes can be unpredictable in practise. A slightly better practical definition of randomness is that a sequence of numbers is random if you cannot compress it and it satisfies some additional statistical criteria. (That practical measure is related to Kolmogorov complexity.) The problem is that a sequence of numbers might pass all these tests and still not be random, i.e. you cannot really tell from a sequence whether it is random or not by looking at the numbers alone. A random generator outputting characters may spit out all works of Shakespeare at any time, although this is highly unlikely. Third, non-determinism involves a random choice between alternatives. So what does this have to do with free will? Since a random choice would not be a choice of your free will but rather a choice made by the respective random source, a choice made of free will does not seem to be the same as a non-deterministic choice. Since on the other hand we do not consider a deterministic choice to be a choice of free will either - perhaps not even a choice at all - the concept of free will might be inconsistent after all. In any case, without a proper definition of free will we cannot give an answer to your question, and if you had this definition you would likely have answered the question already for yourself. There are two easy ways out of this dilemma: (1) Discard free will in the sense of accepting determinism. (2) Discard free will in the sense of accepting that free will consists of highly complex, non-deterministic, yet rule-based choices. I personally opt for (2), because it is more in line with modern physics than (1) and supported by the view that human brains are open information processing systems. (Notice that a computational system with the ability of making non-deterministic choices is not a Turing machine, although it might be an extension of one.) 

I partly disagree with Mozibur's answer. I agree that every mathematical object has a tacit context. However, if we change from one context to another we are strictly speaking not talking about the same object, because the meaning of terms involved is given by implicit definition within the mathematical theory as a whole. In the example given, the zero that is not the successor of any number and the zero that is the successor of -1 are two different abstract objects bearing the same name "0". My answer sounds a bit formalist, though, and I have no idea how to reconcile it with a more Platonist view. 

This is just an addition to the other answers from an analytic perspective, not an attempt to give an exhaustive reply.