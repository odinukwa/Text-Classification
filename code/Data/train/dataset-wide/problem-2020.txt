The comment: "To see non-NULL values on a mirror database, you must be a member of the sysadmin fixed server role" is found in the discussion of sys.database_mirroring dynamic management view at: $URL$ Try running the following statement: 

When a database backup is restored it almost immediately allocates the space needed for the database files. So a 300 GB database will quickly appear in the file system at full size, but the file will be essentially empty to start with. Once the space is allocated, the restore process goes on to read the backup file restoring data at the speed sustainable by the network and the servers. Once the backup is complete, restoring all the backup files, handling roll backs and roll forwards, then the database will be available. A few years ago we had a very big database with the backup files stores remotely that was going to take 12 hours to restore over the network. So, in that case we cancelled the restore. Then we copied the files locally (about 1 hour) and restored quite quickly after that. Note that if you database backups are compressed, the backup file will restore faster since there is less network traffic. 

Create a table (perhaps in a different database) for the BLOBs of fingerprints then drop that column from the. That way when you you will not need to read the BLOBs from the table. This should greatly reduce the I/O requirements. If you want to manage the storage all from SQL Server, this would be the choice to make. It keeps the fingerprints in the database, but separates the I/O issues for general queries. Store each FingerprintBLOB as a separate file in a directory structure on disk. Keep the BLOB's in the . If you would like to keep the database space smaller and prefer to store on the file system then choose this approach. You can implement this on your own, which means you will need to manage the two separate backups: Database and File Structure. Or, a hybrid option is that you can use the SQL Server which can be used to write files to non-database disk, but still provide a single SQL Server BACKUP and RESTORE process that backs up both kinds of data. 

Using a , this will produce a list of event ids for the . An invalid will return no rows. Since you are not getting any return on your queries, then there should be no traces running. 

If your changes are not frequent and your parent table does not have a lot of data, then you could just update your schema on that table. However, potential downsides are changes in the schema could propagate to changes in stored procedures and views. This could be because of adding columns or removing columns. (Of course, any change made in a database schema, will have some ripple effects that must be coped with.) I like the parent table to be as stabile as possible. Having a related table or tables to provide new requirements is a good general rule according to how I look at schemas. But it is definitely an opinion-based choice that will depend on what you are most comfortable with doing. 

I am sorry to say that this does not look like a good implementation of dbo.Eval. Since this script uses xp_cmdshell and osql, it means that for every call the xp_cmdshell command shells out to the operating system, executes the OSQL command line utility (which has been replaced by the newer SQLCMD command line utility) and inserts the answer into the dbo.Eval table where you then select it out for your use. That is a lot of overhead for a simple calculation. Likewise, there is nothing in the stored procedure to prevent multiple uses of the EVAL function from running simultaneously. Which would mean that you could easily get the wrong answer from a basic select due to the concurrency issue. You might look into using dynamic SQL as Uday Kothari suggested using sp_executesql in his response to that post. That certainly has less overhead. Note: Mikael Eriksson also mentioned the concurrency issue. Other problems exist as well. 

Restore a copy of the source database to a point just prior to when you experienced the failure. Name the database [dbnameEmergency]. Restore the logs incrementally to [dbnameEmergency], such as 1 minute, 5 minutes, 30 minutes, according to how finely you want to process the deltas in order to reduce the damage. Reading and comparing the incremental restores will require either creating snapshots, if you have Enterprise edition, or restoring the log to STANDBY mode so as to read the data. Compare the latest readable changes in [dbnameEmergency] against the state of the [dbnameReplica] (which is the database you are trying to set straight) and write code to apply the changes you are able to infer to [dbnameReplica]. 

In that same discussion Robert Davis (SQL Soldier) agreed that would resolve the problem for the moment. But he also pointed out that if you use the fully qualified domain name of the servers you are less likely to run into a name resolution error. UPDATE: When you reset the endpoints, it will take the database offline for the duration of the restart. However, it will not break the mirror. The mirroring will stop running when the endpoint is , but it will resume once the endpoint is again . The MSDN article: $URL$ suggests. 

EDIT: My comment on Excel formatting is simply that you also have to control the Excel columns. If the Excel spreadsheet is all defined as text column, the results are predictable. In the three columns below I typed the same strings in and columns, but see how differently they are displayed. Then I copied the values into column and got the values, but justified as . 

Remember though that the View or the Stored Procedure must implement your security plans, which you have not really explained. This tip is just showing you how to make use of a users identity, not how to create your system. 

Because there are several apparently semi-independent jobs running, I would suggest implementing a status table and creating a trigger for that table. It depends on how complex you want to be, but let's keep it to a very simple example. Perhaps a table: 

You have left out some information needed to help you. All you have showed is the @TableTest and the insertion of data. 

ALTER TABLE to drop the existing FK constraint. ALTER TABLE again to create the ON UPDATE CASCADE constraint for the FK in question. 

You can get the full logs by keeping the database in the FULL recovery model at all times. Also the statistics are not stored in the log files but in the database. 

Using either a or a as the parameter will produce a listing of all running traces. That list could look like this: 

We always run our Analysis Services on a named server, e.g. AnSrv123, and being run by a Domain Login such as DomainName\ManageAnSRV124. Regarding connecting to Remote Analysis Servers using SQL Server authentication please note the following. Connect from client applications (Analysis Services) $URL$ This says, in part: "Authentication is always Windows authentication, and the user identity is always the Windows user who is connecting via Management Studio." 

If you have a serious problem that requires lowering or raising the minimum, you can reconfigure the setting (without a restart being needed) and it will change up or down in fairly short order. The actual SQL Server memory use, if my memory serves me, can be a little higher than the minimum value, because of some OS overhead. 

It is just a fact of life. Different history, different beliefs, different goals. Your might find this post on PL/SQL and its goals helpful in seeing some of the differences: $URL$ It amusingly starts with: 

For snapshot and transactional replication: Distribution cleanup agent purges the non-latest folders For Merge replication: the snapshot agent itself will purge older folders. 

To get your DATE formatted in the way you want it, you have to insert the '/' delimiters then use the STYLE 3 in converting from the string to the DATE. (I am sure that there are other workarounds and conversion styles that would work as well.) Likewise when displaying the DATE as you desire, you need to use STYLE 3 

Your Primary Key is a unique key and should be sufficient to find the records that you need. However, if you need a sequential number for other identifying reasons, then YES you should make it a separate unique value. I personally would skip the GUID key and just use the Integer, but that is a choice you will need to make. Getting a sequential set of keys without gaps will require some code. Are you using Microsoft SQL Server or some other version. If MS SQL Server, look at: $URL$ This is a discussion of using Sequence Numbers, with some warnings about the limitations. 

Since all the details on backups are maintained in the msdb database, you should just extract the backup file name from the source server. You could create a linked server from your Dev Server to access the Production Server's msdb database. Or you can use OPENQUERY to query the same data. (OPENQUERY may be faster since the query is actually being run on the Production Server.) For example: 

I am not quite sure that I understand your question clearly, so please bear with me on the issue of Logins and Users. It appears to be, in your case, SQL Server logins and not Active Directory accounts, but they behave essentially the same within a server and database. Also, for what it is worth, it seems that some step or steps are missing from your question. No big deal overall. Login: (Server Level) You might create login Login1, which has a SID of and grant it some rights. These rights belong to the login. User: (Database Level) When you create User1 for Login1 the user inherits the same SID of . So, inside your database the server login Login1 is actually executed by the database user User1 so the User is accredited with the permission. Because, as you see, the two are really the same account. The name of the User is absolutely meaningless in terms of execution. It is the SID that matters. 

This means that your question about Clustered Indexes and Primary Keys is really about some of the following issues. Please note that not every table benefits from the same indexing plan. When would I benefit from the Primary Key being separate from the Clustered Index? Perhaps when the Clustered Index is Wide (for example, 5 columns of textual information, but the Primary Key is small (INT or BIGINT), such as you seem to be describing. 

I added a second answer because you asked another question entirely. What you can do is limited by all the physical factors that prevent an activity from happening instantaneously. Your customers need to understand that instantaneous does not exist, and that you latency is 1 to 10 seconds, or some such number. Then you endeavor to treat that as real limitation. The universe has rules, you know. If you want to have things seem synchronous (even though they are not) you will need to enforce a delay someway. [Kludge Alert!] 

My experience is that a useful index gets used almost immediately. It does change the information available to the optimizer after all. However, a new index may never get used if the optimizer rejects its value. You should note that a stored procedure may hold onto an existing plan for some time. If so, you can use to cause a stored procedure to (yes) recompile and assess the value of your new index. If you want to know much more about the optimizer and indexes, you should read through this article by Benjamin Nevarez: Index Selection and the Query Optimizer $URL$ This should give you some insight into how things work, at least enough to get you started on testing a potentially better index. 

I do not have specific experience with this, but I see a few posts that have not been answered successfully, so just pointing you to other resources. 

The Default profile which disables the new RSS features, leaving behavior very similar to the Windows 2008 R2 behavior. The NUMA scalability profile enables RSS CPUs to be assigned per NUMA node on a round robin basis. This results in "RSS assigns at least one RSS CPU per NUMA node before assigning the second RSS CPU for each NUMA node." This round robin setup spreads the task across more CPUs, thus likely improving the data throughput. 

As you go down the stack, things get more complicated, but also provide more powerful ways of scaling out. 

Microsoft has documented their "Description of support for network database files in SQL Server" at: $URL$ Trace flag 1807 bypasses the check and allows you to configure SQL Server with network-based database files. However, this is not without risk and you can fairly easily wind up with a corrupt database if the device does not fully support what SQL Server needs to achieve consistency. If you want to use this approach in a manner that Microsoft will fully support, they have a set of standards describing what they support. Following is a quote from the link: Windows Hardware Quality Lab (WHQL) qualified devices Microsoft Windows servers and networked servers or NAS storage servers that are Windows Hardware Quality Lab (WHQL) certified automatically meet the data write ordering and write-through guarantees required to support a SQL Server storage device. Microsoft supports both application and storage-related issues in these configurations. Note To be supported by SQL Server, the NAS storage solution should also meet all the requirements that are listed on the following Microsoft website: $URL$ 

Creating a Scale Out database at Internet scale is pretty huge step. You will face a lot of issues that are not critical on a single big database. From your notes I see that you understand some of the basic issues you face. Since Microsoft has papers on using SQL Server for scale out, I suggest that you study those first. Your scale out strategy will need to take into account the database server you choose. For Microsoft SQL Server you should first study: $URL$ This paper discusses the decisions that you need to make and why they are important. It offers 5 SQL Server strategies for scaleout: 

Using my non-sysadmin account, I get nothing. No rows returned, no ROWCOUNT, et cetera, just a delayed 'ding' sound. So, that note is just trying to tell you not to be surprised when you get no information back. (Except, perhaps, a 'ding' sound.) 

MariaDB comes in a few different forms: MariaDB is free and includes: Access all the features of MariaDB Server MariaDB Enterprise Subscription includes a number of other features and is priced per server. To look at the high-level features available see: $URL$ 

Do not use the /// keys at all to select your code to run. Assuming that a computer mouse can be used, do this: 

Of course licensing is a necessary part of Microsoft's business, so it is not surprising that the installation would enforce the limits that the purchaser is granted. Setting up a SQL Server Failover cluster for SQL Server 2012 (Standard as well as Enterprise editions) you must first: Create a New SQL Server Failover Cluster (Setup) Option 1: Integration Installation with Add Node (i.e. only 2 nodes permitted.) 

It depends on whether your local server is Enterprise Edition. If so, you can restore the database backup and leave it in NORECOVERY mode. You will not be able to directly access the database, but you can create a SNAPSHOT database which will be Read Only. That snapshot database can be dropped, the new TRL restored in NORECOVERY Mode, and recreate the snapshot. 

But please note that this only answers formatting a phone number for the 10-digit answers. If you are using internationally dialed numbers, you need to investigate the various forms of phone numbers you might need to handle. See: $URL$ 

Because FREETEXT freely interprets your data using a thesaurus it is likely that "trail" and "road" are interpreted as equivalent in meaning and therefore the two rows get the same score. If you need more precise answers you should use CONTAINSTABLE. 

If your goal is simply to create a Flat File have you considered using BCP.EXE to Bulk Copy your data out. Bulk Import and Export of Data (SQL Server) gives a link to the bcp utility. You do need to give directions on how to interpret the data. Here at Create a Format File (SQL Server) you can examine the two Format File type used to define the data format. (Naturally.) The format file that I use is a Non-XML Format File. This is just a text file and is fairly easy to type and make changes within. For me, this is a quick way to export a flat file. (Of course, if you read further down the page you will see an XML Format File, which might be more your style.) It is also possible to use BCP to copy data back into a SQL Server and it is pretty fast. (However BULK INSERT tends to be a bit faster for importing data.) So BCP is fast, fairly easy, and comes in two flavors.