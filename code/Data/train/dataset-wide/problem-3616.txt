I am certain that the Check Point knowledgebase has this answer, although for the life of me I can't remember what to do. Two general things to try: 

It worked. I got a text based set of install menus. I actually prefer seeing all the output when the system boots, so I also successfully used: 

Depending on your size of organization, you can establish a set of "Key Users" in the user population. These users have the responsibility of aggregating and prioritizing change requests for a particular application or function. They represent The Users, a key IT stakeholder, in business discussions about allocating IT resources. Most likely, Management and The IT Organization are already represented in these discussions, but The Users are not directly represented. In the process of aggregating userbase input into the IT business decisions, these individuals become seen as local experts and go-to people in their parts of the organization. They are explicitly aggregators of issue reports and serve both to focus problem reports into IT as well as to disseminate information from IT. The benefit for the organization is that a key stakeholder gets an explicit voice in IT related business decisions. The benefit for the IT group is that they have fewer points of contact that need to be actively managed. The benefit for management is that the organization is communicating more effectively. 

In the USA, many or perhaps most of the Internet Service Providers are not commonly offering IPv6 connectivity. IPv6 access is available for purchase, but to my knowledge, the service carries a premium. In 2006, I was told that IPv6 connectivity was being widely used in some markets - I believe that China was named as an IPv6 market. Most commercial network gear, e.g., not USA consumer products, supports IPv6 routing and dynamic routing protocols. Microsoft OS products all support IPv6, and most Linux and FreeBSD distributions have supported IPv6 for over ten years. The IETF working group does have the experience of IPv4 to guide their engineering efforts, so it may be that IPv6 will introduce little, if any, performance degradation. The biggest complaint I have heard about IPv6 is that it, at least in the past, required that all hosts be directly reachable by all other hosts, i.e., have a public Internet address. Some security engineers thought this was a bad idea. 

I am expanding my rescue usb flash disk. The USB's partition is bootable, and I have installed GRUB to the MBR of the jump drive. It works pretty well - I can boot Freedos and run some utilities,and I can boot PING. I would like to add the Ubuntu 9.10 LiveCD to this rescue usb drive. I have a working jumpdrive, so I can pretty easily copy over the files. I could hunt down the needed entries needed for GRUB, but the Ubuntu LiveUSB uses a fairly complicated syslinux configuration. Besides, I would like to keep as much of the current LiveCD/LiveUSB look-n-feel without porting the syslinux config over to GRUB. At the suggestion of ~quack I tried a entry of: 

This does not seem to be a zsh issue. This is an issue with the specific implementation of the command stty on your system. To investigate a new command, remember: 

If you can get an pcap of valid OpenVPN Client to OpenVPN server interaction, you could model the initial set of packets with something like netcat, as suggested by TiZon. Basically, you want enough of a valid first packet to get the server to respond with at least an error message, so it doesn't have to be perfect, just good enough. I tried going to $URL$ but I didn't see an OpenVPN example there. Perhaps, if someone else is claiming the service is up, you could get that other person to grab a pcap of the transaction. 

Use wireshark $URL$ and a host outside the firewall to see what ports are used by this particular FTP+SSL configuration. Based on the traffic you see, create firewall rules to allow that kind of traffic from the IP address of FTP server. 

Larger, easier to handle screwdrivers, torx wrenches, wire cutters. The small packs are nice in an emergency, and they go with you, but trying to use those little guys for hours on end can begin to hurt your hands. 

I am using a rack mounted device that has 10 Ethernet ports built into it. For 8 of the ports, I would like to bind them to a single vwmare-server-2.0 virtual network interface, so that I can, in effect, use the ports as a hub for the external devices and internal virtual machines. How can I bridge a single vmware vmnet# interface to several physical interfaces? 

You may need to go to your vendor. Dell has a hardware diagnostic cd for dell hardware. As for a more generally applicable suite of applications, I'm not sure. 

Set up NTP so all three boxes are synchronized to a common source. Then use the command to run your script (here's a tutorial) 

Some VPN clients are capable of enforcing such a policy. The Check Point VPN client added this feature around 2000/2001. 

Divide the problem space in half. Rule out one half of the problem space. Repeat with remaining problem space. 

The case study of Largo, Florida, may prove informative. They migrated a significant number of non-technical users to a Linux based thin-client network design and realized significant cost savings as well as increased productivity (due to reduced workstation downtime and improved user data backup) as a result. Slashdot profiled the city several years ago. Since that article, it seems that the city migrated to a Citrix solution. 

When diagnosing network connectivity issues, I start local and then move global. First, I try to ping the local ip address on the interface of the host I am using. This almost always works. Then I ping the default gateway. Then I ping the DNS server. "Pings" are not limited to ICMP Echo/Request traffic. I will try to establish a TCP connection to ports I expect to be able to access. Telnet clients are good for this. At some point, I examine the network traffic with wireshark or tcpdump. Also, if I can, I will try to use an alternate host to see if it works. 

In summary, step one, the traffic monitoring (Nagios seems to be a standard tool) helps you figure out, in general, what is going on to stop the immediate pain. Steps 2 - 5 help prevent the problem in the future. 

Use this as an opportunity to test your backup/restore plan. Use your regular backups and restore to a testing machine, either in an isolated network or on a virtual machine surrounded by a virtual network. Be sure to time how long it takes you to restore a working system - you will need that information for calculating downtime in the event of a complete failure. Also, be sure to note how much and what kind of data was entered since the most recent backup. 

Nikb's suggestion is sound. One additional area of consideration: How much RAM (or CPU cores, or disk systems) can an instance of your application / application architexture access? I saw a situation where an application on a quadcore server with 14GB of RAM ran very slowly even though CPU usage was less than 25% and RAM usage was low. It turned out that application engine could only access 1.5GB of RAM. The application instance was maxed out, even though the machine was only lightly affected. 

This seems to me to be a layer 2 issue. You want to prevent some (all?) broadcasts from going from one user to another. I think most ISPs have a point to point link with their customers. Didn't cable companies use PPPoE to simulate the point-to-point connection? I think managed switches can be set up to filter traffic. Finally, perhaps you could configure DHCP relay if your switch supports it. 

First, try to figure out what happened on the date in question. Did your system install an update? Did you change the configuration of some other security related application, such as an Anti-Virus or Anti-Spyware application? Did something change your network configuration? Did you install a VPN or SSL-VPN client which may have affected your systems internal routing tables? If you can figure out what changed, try and see if you can undo the change. Just be warned, undoing may not be possible for a variety of reasons. For the case that raised this question, it a change to the anti-spyware application coincided with the break in FTP service, but it was not possible to restore functionality by restoring the anti-spyware application configuration. One solution is, on the Windows Firewall Exceptions tab, create an exception for Port 21 TCP and then scope the exception for "My network (subnet) only. Essentially, one is duplicating the functionality from the "Advanced Tab", but the Exceptions tab allows for the limit of scope. This exception allows one to delete the "FileZilla" application exception and still permit access to the ftp server. The File Transfer Program exception is a red herring - that entry allows the ftp client on the server to access ftp servers on other hosts. The next time one tries to use the ftp client, Windows Firewall will pop-up a dialog box to create the exception, assuming a default configuration of Windows Firewall. 

The builtin filesharing has always worked for me. Install the vmware-tools and configure vmware to enable filesharing. Granted, this does open a security hole from the virtual machine to the physical machine, so be careful. The FileZilla server is pretty easy to install. Install, and then under settings, configure a user. 

A common technique from the past was to restore a backup from the Production server on to a machine in a lab environment. From there, the configuration can be altered to meet the needs of the new networking environment. 

I have subscribed to the "Sherlock Holmes" method of troubleshooting, aka Binary Search Troubleshooting Method: 

I would suggest running wireshark or tcpdump, filtered first to the IP address, and for later captures the MAC address of client system. 

I use network sniffers to troubleshoot these kinds of problems. The firewall is a likely culprit, so analyzing the network traffic on the outside and inside of the firewall can reveal what specifically is going wrong with the FTP connection. One thing to check on the firewall policy is that the non-standard port is actually configured to handle FTP traffic. It seems that the policy has been configured to enable a TCP connection to the non-standard port. Has the policy been configured to allow specifically FTP traffic? If you are using a very low-end firewall that can only perform NAT and basic packet inspection (i.e., not a TCP state aware firewall), then you will need to configure the FTP server to only allow passive mode, configure the FTP server to only allow DATA connections on some small range of ports (10 or 20?), and then configure the firewall to allow inbound connections to the FTP server for those 10-20 ports. 

I generally prefer file synchronization to normal backups for user files, because even when I was at the office without my laptop, I could still look at my files, for example, from someone else's machine or from a "burnbox" computer. Very convenient. Look at rsync For your consideration, there are two areas of backup you need to plan for: 

Recovering user data, for example, if they delete a file by accident or have a hardware failure. Time Machine or rsync can help you there. Restoring a system from a failed or damaged state into a usable state. This is mostly about getting the OS and any needed applications reinstalled. Damage can be anything from hardware failure to malware infection. I prefer disk/partition images for this. 

The scenario describes seems to be an intended limitation in functionality designed to prevent end users from inappropriately opening holes in the firewall. You may need to 1) Adjust you network config, for example by moving the server to the DMZ 2) Get a different ADSL modem or router 3) Get a firmware update. 

Yubikey? I haven't tried to install it in conjunction with a RADIUS server, but the token is much easier on end user. There are some instructions here: $URL$ 

Off the wall idea - is the all of the stored information needed or even useful? How much is the information actually worth? It seems obviously ridiculous to spend more in upkeep and management than the data is worth. Is the data in the database appropriate for storage in a database? For example, does keeping compressed multi-gigabyte core files in the support organization's database really provide any actual benefit? Is there a lot of duplicated data in the database? For example, are a thousand people keeping ten copies each of a weekly 10MB newsletter? Does some of the data have an "expiration date" after which it does not provide any value? Returning to the support organization example, for various reasons there is virtually no benefit in keeping around customer core files more than a few months after a fix has been delivered. Another thought - is keeping that much data opening the company to liabilities. Some data one must, by law, keep. Some data, however, should be "shredded" because of the risks posed if it is accidentally, or maliciously, released to inappropriate parties. 

Having investigated a number of Content Management Systems for several different projects, I must say that it depends. The biggest challenge I experienced was matching the functionality provided by the hosting provider with the goals of the client. Basically, the client wanted a fancy new site but didn't want to pay significantly more for the needed hosted functionality. That said, I have had good success using Drupal. I found it easy to administer, the designers were able to use the modules to deliver the functionality desired, most hosting plans included the PHP and SQL db functionality needed, and the end customer liked the results. Unfortunately, I have also been hit hard by XSS vulnerabilities 

This article from Microsoft ( $URL$ ) discusses configuring a proxyserver for the system. For other browsers, such as firefox, you may need to configure the proxy settings independently. 

Profile the network traffic. Try out some actual monitoring tools: $URL$ You're looking for Top Type of traffic (likely HTTP, but who knows), Top Talkers (should be your servers, but who knows), and potentially Malformed Traffic (large amount of TCP retransmissions, malformed packets, high rates of very small packets. Probably won't see, but who knows) At the same time, work with your management to develop a network resource usage policy. In general, business terms, what business needs does the computer network exist to meet, and what are appropriate uses of the resource. This thing is costing money, so there has to be a business justification for its very existence. Your company has policies for handling the "petty cash" drawer, and I would bet your network infrastructure costs a lot more that. The key thing to focus on is not catching people doing bad things but rather watching for potential malicious activity that is degrading network functionality (i.e., the employees' ability to get their work done). Southern Fried Security Podcast and PaulDotCom Security Weekly cover information about creating appropriate security policies. @John_Rabotnik idea for a proxy server was great. Implement a proxy server for web traffic. Compared to traditional firewalls, proxy servers give you much better visibility into what is going on as well as more granular control over what traffic to allow (for example, real web sites) and what traffic to block (URLs made up of [20 random characters].com) Let people know - the network is having a problem. You are monitoring the network traffic. Give them a mechanism to register network slowdowns, and capture enough meta-data about the report so that in aggregate, you might be able to analyze network performance. Communicate with your coworkers. They want you to do a good job so that they can do a good job. You are on the same team. As a general rule, block everything, and then allow what should be allowed. Your monitoring from step one should let you know what needs to be allowed, as filtered through your network usage/security policy. Your policy should also include a mechanism by which a manager can request new kinds of access be granted.