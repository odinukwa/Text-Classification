If you didn't explicitly set it, then objects created by you will be created under the default schema for your user. In SSMS, look in the database -> Security -> Users -> -> Properties -> General -> Default schema 

I would suggest an table which stores the current appointments for each doctor. We can add some constraints on this table which limit the appointment start times to even ten-minute times (e.g. 9.00, 9.10, 9.20) plus add some other common sense checks like after and doctor can't have two appointments starting at the same time. Assume that you'd also like doctors to only work between 9am and 5pm, because everyone needs some work-life balance. 

There are some other conditions too, which you can read about in that link, but those are the obvious changes that would need to occur. 

Any queries that read the table or with isolation level will proceed despite your . More info about lock modes can be found here: $URL$ Personally I'd say that if you have queries reading from the table then they should fully anticipate that they will get bad results so blocking those reads wouldn't be a priority. If your those queries need to return data that is actually correct then they should not use . $URL$ 

As Joe suggested, you should use the function to compare values to the previous row. It could be implemented like this: 

In SQL server you can only add a single user per statement, so your script is about as good as it can get (aside from adding Billy and Bobby to the role). $URL$ 

Consider having a row in the table for each attempt that the user makes at the test. This means, then, that you would have to make the composite primary key of the table to be and (which can be an column). I would maybe think that the table should be a many-to-many relationship between and rather than between and , assuming that each time a user attempts the test they may get different questions. Either way the primary key on this table needs to include more than the single foreign key, it should include both foreign keys. 

MySQL 5.6 includes functionality called Online DDL which allows ALTER operations to proceed on tables without applying a LOCK to the table (technically referred to as allowing Concurrent DML). $URL$ From what I can see, the mechanics behind this are that a copy of the table is made, on which the ALTER is made, and the data is copied between the source and destination tables. At the end of the process, the tables are renamed. Any DML operations (UPDATE/INSERT) are cached during the ALTER process and then applied to the new table once the ALTER process is complete. Is this correct? For instance, if my ALTER takes 24 hours, is it the case that I won't see the results of DML operations for 24 hours, or is there some internal mechanism that cached DML operations are updated on the source table and then updated in the new table too? I understand that the Percona pt-online-schema-change tool $URL$ does something like this, but this tool can take up to 4 times as long to perform the table change as a native MySQL ALTER operation. 

We ran an update with extra parameters and it completed. In the end, we didn't alter the chunk-size but instead altered the default settings for the copy-rows process, to increase the number of retries and the interval between retries. We also increased the innodb_lock_wait_timeout from the default 1 second to 5 seconds 

I am currently setting up percona-online-schema-change to do a ALTER on a table with 300m+ rows. I've verified that percona-online-schema-change can make the required ALTER. The source table receives updates/inserts in large batches at erratic intervals. During testing, with defaults values for percona-online-schema-change, the update fails due to the process not being able to obtain a lock on the new table. My theory is that due to the erratic nature of the updates, percona-online-schema-change is not able to obtain sufficient accuracy for its chunk-size calculations, and is trying to do too much on the new table. Therefore, I was going to set some hard values to try and obtain some more consistent behaviour. Specifically, I was going to set a hard --check-size value and disable over-sized chunk checking. I was also going to increase the number of retries for copy-rows from the default of 10 to 30, and increase the wait between tries from 0.25 to 10.0. Finally, I was going to increase the innodb_lock_wait_timeout value from 1 to 5. I know that these measures will slow down the overall operations considerably, but its more important to me that it completes in the first pass that it completes quickly. Does this seem like a reasonable and safe approach to the update? 

Note, that you should put whichever column is likely to narrow the search best first. I assume that your three columns are equally selective so just picked one. The other columns are included so that it can be an index-only read. 

Your table is extremely wide, and every time that you run a query it needs to read the entire table to find the rows needed. There are two ways that you could reduce the amount that must be read, firstly by reducing the number of rows that the DB must read (e.g. by clustering the table by ) but you suggest that most of the time, most of the rows will match the date criteria, so that probably won't get you very far. The other possibility to reduce the size of the data is to reduce the width of the rows. I don't know a whole lot about baseball so I don't understand what those fields are storing, but I can guess that almost all of the fields in your table don't need to store numbers as high as 2 billion. I think I counted 88 fields for about 352 bytes per row. Changing them to or could save 180-260 bytes per row. Along similar lines I think there are 28 fields devoted to ids of umpires, managers and pitchers. Perhaps if this data is infrequently queried it could go into another table with the same primary key; only join the tables when necessary. Maybe some other fields too. If you could do both of these then scanning the table would probably be far quicker which can make up for be fact that your queries seem very difficult to index for. 

Maybe I'm overthinking things but I don't really like this because it's currently doing 12 table scans to get the result. The optimiser is also currently choosing to sort the data and do merge joins to achieve the but I'm not worried at this stage about it's choice of doing that. Is there another way that I could do this which doesn't result in additional table scans every time the hierarchy gets one level deeper?