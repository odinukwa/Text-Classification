As a partial answer: Allow for a 10 minute break in the middle of a long presentation. It is rare for someone to follow for two hours straight, and many topics can not be done thoroughly in less time. Also, the break time can be just as productive as the presentation. Towards the parenthetical remarks about success: It is good to have a clearly defined goal. If everyone is on board with it, then set policy that helps everyone acheive the goal. In your example, I would limit/prohibit any side discussion that is not helping to educate in the topic being presented. This implies that questions about applications to other fields, or alternative definitions or problems that take one outside the scope of the presentation ( even if it is covered in some other presentation ), and their responses, should be brief or be cut short. (I would not apply this prohibition if the goal were to stimulate new research or creative problem solving.) One opportunity I missed out on was when I was an undergraduate sitting in a small graduate PDE class looking at a paper by Leray on Navier-Stokes. The students took turns translating and presenting sections of the paper. If I had been smarter, I would have met outside the seminar with the graduate students to go over sections of the paper and help improve my and their presentations. If I had done that, I would likely remember the mathematical impact of that paper, rather than forget most of the technical details needed to push through the proofs. Perhaps something similar would be effective for your "Classics" seminar: team presentations. Gerhard "Ask Me About System Design" Paseman, 2010.02.20 

I submit for your consideration Euclid's fifth postulate. Given the amount of effort taken by people to prove it from his four other postulates, I consider it canny or lucky that Euclid chose to keep it as an axiom. Of course, it took unusual thinking and some discovery to realize that Euclid's fifth was indeed independent of the other postulates. Gerhard "Ask Me About System Design" Paseman, 2010.06.10 

$$ \sum_{m=0}^k \binom{z+1}{m} = \sum_{m=0}^k [ \binom{z}{m-1} + \binom{z}{m} ] = \binom{z}{k} + 2 \sum_{m=0}^{k-1}\binom{z}{m},$$ when $z \geq k$. So the sum can be rewritten as $$ \sum_{z=k}^{n-1} \frac{z}{2 + \binom{z-1}{k}/\sum_{m=0}^{k-1}\binom{z-1}{m}}. $$ Let's call this summand $a_z$ . Note that $a_k = k/2, a_{k+1} = (k+1)/(2+1/(2^k - 1))$, and for small values of $j$, $a_{k+j} = (k+j)/(2 + \binom{k+j-1}{j-1}/(2^{k+j-1}-\sum_{m=0}^{j-1} \binom{k+j-1}{m}))$ . Now as z increases, the denominator of $a_z$ eventually tends monotonically to $(z+k)/k$, which means $a_z$ tends to $k$ from below. Thus the entire sum has an upper bound (for $n \gg k$) of $(n-k)k$. Further, for sufficiently large integers $l$, $a_{lk}$ is slightly larger than $lk/(l+1)$, so I imagine the actual sum differs from the upper bound by a $O(n\log{k}) $ amount. Gerhard "Ask Me About System Design" Paseman, 2013.05.08 

To support Mark's and Jules' observation that max(Ci)/M is a good starting point, I mention the following. Suppose an initial value r was chosen in an attempt to find s. If the Ci were all less than (M+1)r, the total error would be at most n*r, since one could find positive integers ui with (ui-1)r <= Ci <= ui*r, and one of ui, ui-1 would be in the range 1 to M. So smaller r means smaller worst case error. Also, if one has a trial r and computes ui based on r, then looking at sum ui*ei, where ei is the sign of the error ui*r - Ci, one has an indication of which direction to tweak r to reduce the amount of sum abs(ui*r - Ci), while keeping the same values ui. Gerhard "Ask Me About System Design" Paseman, 2010.06.11 

I decided to share this awk code to compute s(n), primarily because I like that it uses only addition and the distributive law, and not factoring, to compute s(n). It also uses a bit of string processing and hash-table look up, but is a nice example of the use of associative arrays. I also like it because it uses $O(\pi(n)\log(n))$ bytes of memory, essentially one entry per prime number less than n. Apologies to sleepless in beantown: I prefer obfuscated awk and nice algorithms to one-liners in Perl, so do not accept his challenge made in a comment on his answer. 

This is probably more of an example than a counterexample. Consider the following binary operation table defined on a three element set with zero: 

Surely the lattice of lattice varieties is partly known. J.B. Nation is a (in some circles) well-known researcher in lattice theory, and can probably tell you much of that structure. In particular, he knows of an equation which is satisfied by all finite modular lattices, but not by all modular lattices. Such an equation can be used to build a proper subvariety of modular lattices which also properly contains all distributive lattices. I recommend a web search on "lattice of lattice varieties". Gerhard "Not Variety Of Lattice Varieties" Paseman, 2013.05.16 

I define a kind of template or macro to be used in a slightly odd fashion, not exactly typed second order logic. Let Ab(t,A,B) be the macro that expands (when appropriate inputs are given) to the logical expression (t(A,xbar) = t(A,ybar)) implies (t(B,xbar) = t(B,ybar)) . Similarly for SAb(t,B,C,D), using (t(C,xbar)=t(D,ybar)) implies (t(B,xbar)=t(B,ybar)). Now for a given algebra AA, ((for any xbar and ybar which are tuples of appropriate length built from (the underlying set of) AA, for any term t from (the set of term operations of) AA, [For any a,b from AA Ab(t,a,b) holds] )) iff AA is abelian. Replacing the "last line" in the above with [For any b,c,d from AA Sab(t,b,c,d) holds] )) iff AA is strongly abelian gets the definition of a strongly abelian algebra as well. There are variants of the above defintions where binary terms t(a,x) instead of larger arity terms t(a,xbar) are used, and for two congruences $\alpha \leq \beta$ of AA one can define a generalization ($\beta$ is abelian or strongly abelian over $\alpha$) using $\alpha$-related in place of = and asking for certain of a,b,c,d and the bars to be $\beta$-related. Also, once the defintion is understood for an algebra AA, it can be extended to apply to classes of algebras. A web search for strongly abelian universal algebra leads to various papers in the literature, with Kiss, McKenzie, and Valeriote among the authors. Kiss and Valeriote in a paper on Abelian Algebras and the Hamiltonian Property mention some of the literature and note that matrix powers of unary algebras provide basic examples of strongly Abelian algebras. In another paper on strongly abelian varieties these same two authors mention the result that strongly abelian algebras have finite essential arity, and that something similar holds for locally finite strongly abelian varieties. I know of no nice way of expressing essential arity in terms of identities. The not so nice way involves picking an integer n and then a certain set of formulas which are tantamount to saying t(abar,xbar)=t(abar,ybar), except one needs to single out the inessential variables where they live rather than conveniently grouping them together into xbar or ybar. I have no specific example of an algebra that is not strongly abelian, but is of bounded essential arity. Here is an idea on how to build one though a finite such algebra. Take a universe of size n at least 4. (Smaller might work, but I want enough room for success.) Order the set as a chain, with 0 as the least. Create an operation of desired arity and call it b and make sure it is not strongly abelian by ensuring that b(a,xbar) is different from b(a,ybar) for at least one valuation of a, xbar,and ybar, while making it agree for c,xbar and d,ybar. Compatible with that condition, let b have the value v satisfy that it is smaller than any of the values of its arguments, unless one of them is 0 in which case 0=b(a,ybar). Now any term which has a depth of n many b's will evaluate to 0, and be constant. If you take sufficient care, you can show that this algebra has essential arity at most w^n for w the essential arity of b. Gerhard "Ask Me About System Design" Paseman, 2013.04.24 

If the idea is to have a toy model to "start getting your hands dirty" and gain some experience in model creation and analysis, then I can see why you would start with this particular setup. If the goal is to end up with a more refined model that gives an acceptable representation of what happens with real networks, then I am puzzled by your initial choice. As a prelude to such a more refined model, I would analyze the case of one user issuing different requests over a span of time that was less than a day (one to eight hours, depending on the kind of user). I would have the requests overlap, so that a user could request one or more files while one is already downloading. To make things simple, leave latency and other timing issues aside and assume the router deals with passing packets to you using a FIFO queue mechanism, with whatever parameters you like for the queue. Then you can try various distrbutions to guess when the last packet of each file arrives. The benefit of this model is that it is readily adaptable to many users: just change the distribution of requests I(y). You can also add other features to see how wait time is impacted (Hint: once you reach over 75% continuous capacity, not much additional impact will be felt). I do not presume to know the literature to give you good recommendations; as a start though, I recommend books on queueing theory and network/communication protocol design. I recall Tenenbaum as an author on engineering texts for design of computer hardware and networks; his bibliography might be of some use. Gerhard "Anyone Remember Gopher And WAIS?" Paseman, 2013.05.02