Do you want to perform the update once or on a regular basis? If the update on million rows is done once, then the best solution is to create a temp column "processed" on #testing2 table of bit (int,tinyint) which will serve as your null filter. The index on bit or int columns works a lot more optimal than on varchar. Also, having 2 type of values on index definition (0 for null 1 for not null) will be very fast. Keep your indexes and add the second index and you will have the following Plan if you change the filtering options in the query on "processed" column as it is marked in the picture below with the plan: 

It matters only related to disk space and character length. Of course search on char data types and indexes on these type of data will act slower than integer but this is another discussion. Varchar data type is a "variable" data type so if you set up a limit of varchar (500) than this is the maximum character length for that field. Minimum length can be between 0 and 500. On the other hand the disk space claimed will be different for 10, 30 or 500 character fields. I did sometimes a test for data type varchar (800) and for null values I had 17 bytes used, and for each character inserted it added one more byte. For example a 400 character string had 417 bytes used on the disk. 

Most of the forum and example online always suggest to have both and set to ON whenever someone is asking snapshot, row versioning or similar question. I guess the word SNAPSHOT in both setting get a little confusing. I thought that, in order for database engine to use row versioning instead of locks for READ_COMMITTED default behavior, the database is set to ON regardless of what setting. The setting is set to ON only to allow snapshot isolation when starting a transaction (e.g. SET TRANSACTION ISOLATION LEVEL SNAPSHOT) regardless of setting. The only reason to have these two settings set to ON is when it needs to have READ COMMITTED row versioning AND snapshot isolation. My question is, is my understanding incorrect in some way? And that these two setting have to be always set to ON together (especially for READ COMMITTED row versioning)? 

Technically you can assign any existing/new filegroup to any new partition scheme (for other table). However, you need to understand the existing partition function and scheme design, and more importantly the purpose of the existing partition. It could be partitioned for improved scalability and performance reason by having filegroup on different disks. Or it could be partitioned to archive older data on slower and cheaper disk. Or partitioned on a same/different disk for quicker subset access like during data loading process. So, the idea is to find out if the purpose of the new table partition inline with existing partition to determine if existing or new filegroup should be used. 

You can prevent creating a procedure with invalid objects before executing the Create Procedure statement like that: You will see a red line under the invalid object which underlines the error when you go with the mouse over the red line. I am not aware of other method, because the SQL will parse the procedure before executing it considering only the validity of the SQL syntax. 

first of all, the replication variables have effect on the slave only when activated on a replicated "slave" server. you need to understand that the filtering rules on master differ from the ones on slave. on master you can choose only to log a whole db or not. on slave you have more options. here is described $URL$ and here: $URL$ I think, you want to skip replicating a set of tables with a given pattern on slave. So, the variables must be configured on the slave. Change the configuration file on the salve and add the db_name instead of % for db part. 

I remember when working with clusters that the port change from dynamic to a static one doesn't take effect if you don't disable the checkpointing of the quorum. Here are more details: How to Change the Dynamic Port of the SQL Server Named Instance to an Static Port in a SQL Server 2005 Cluster Also, applications managers should be notified about the port change because there are places where this can be used. As well, the port of SQL Server Instance is used in Firewall settings in many types of network configurations. 

In this test, query 2 is waiting for query 1 to commit, dm_tran_locks DMV shows that exclusive lock on TABLE1 incurred by query 1. 

I have no issues with the same code on other SQL Server 2012 or SQL Server 2008 R2 instances. Does anyone know the reason behind and has fixed this problem? 

return ServerB under name and network_name column with id 0 So question is, am I missing some steps over changing servername or is this a bug? Version - SQL Server 2012 SP1 

From this article, I know to that slash (/) is used instead of a hyphen (-) when a net start is used with startup option, 

Run Query 1, and run query 2. DMV shows query 1 incur exclusive lock, but query 2 returns details with 'Original' without query 1 commit the transaction. It appears that READ_COMMITTED row versioning is in place. Adding on query 1 and query 2, and run query 1 or query 2 returns error - Snapshot isolation transaction failed accessing database 'TEST' because snapshot isolation is not allowed in this database. Use ALTER DATABASE to allow snapshot isolation. Third test, rollback previous transaction. Set READ_COMMITTED_SNAPSHOT OFF and ALLOW_SNAPSHOT_ISOLATION ON. 

performs best to be unique, narrow, static and ever-increasing by itself. So in this case, the inclusion of DeletedDate actually result the clustered key becomes non-unique non-static (presuming the DeletedDate value could be changed). The in the non-clustered index is useful to cover the query without having to perform a key lookup to the table. However, as the INCLUDE columns only stored in the index leaf level, it does not help in searching for the values of the query predicate (in this case, NULL) A of DeletedDate allows a more effective search on the range of records that satisfy the predicate (NULL). A filtered non-clustered index could further narrow down the subset (only NULL records) and provide a better performance as well as storage for the non-clustered index. With your description showing that the query returns all columns from the table with a single predicate, You could create a single filtered non-clustered index key for DeletedDate with . Test it and examine the execution plan. 

Like Shawn said, the code will not execute by itself unless some stored procedure which seems vbalid has an exec of another malicious code. This is the reason of checking the code inside each one of them before putting it multi user mode. 

Following Gaius post: You can create an .SQL script which does what you need with use db in front of the script -> create a SQL Agent job of Operating system type which calls the script: sqlcmd -E -S SERVERNAME -i"c:\YOURSCRIPT.sql" -o"C:\YOURSCRIPT_LOG.log" Add new step and use msdb.dbo.sp_send_dbmail procedure to send email. This feature can be customized to display inside the mail a specific query from SQL tables to confirm the execution of the script... for example dbcc showcontig of your rebuild indexes. 

But it can be easily hacked if you change the with So instead you should really use the parameters as the sp_executesql is supposed to be used. 

In order to alter any column, first you have to drop the index or any other constraints that contain that column, and after create the index/constraint again. But this task must be done by a DBA because any drop on index or constraint will affect on one hand the data being queried - while the index is dropped, and on other hand put blockage on whole table for the time that the index is re-created. You need to make sure the users will be aware of this little or big (depends on the table size) maintenance. Good luck! However the Index create can be done with Online option which is less blocking. 

Backup B (almost same size as backup A, at the same network location) 1st restore take about 1.8 min. I deleted it and try again 2nd restore take about 1.5 min. Ok.. The service account is enabled for instant file initialization. My question is why the restore duration varied on different restore attempt on the same backup file to the same machine? Is it purely on network throughput (perhaps someone was doing something on the network share and stuff), or something else like cache or something in SQL internal? 

The script show the log file was initially set at 1MB, change the size to 10MB, and shrink it back down to 1MB. Data file size remains at its initial size, 5MB. 

Backup A 1st restore take about 5 min. Ok, after restore complete, I delete the database 2nd restore take about 1.8 min. Hm.. Let's delete the database and try again. 3rd restore take about 1.5 min. Hm.. 

How about starting with this article, 'The Curse and Blessings of Dynamic SQL' by Erland Sommarskog? 

I was doing some test on server name change and encounter some errors. Here is the original setup: Server name - ServerA, SQL Server default instance - ServerA Changes: Server name - ServerB Before I change the SQL Server default instance 'servername' 

Run query 1, and then query 2. DMV shows exclusive lock incurred by query 1. Query 2 appears to be waiting for query 1 to complete. Turning ALLOW_SNAPSHOT_ISOLATION ON doesn't appear to enable READ COMMITTED row versioning. Adding to both query 1 and query 2. Run query 1 and then query 2. While DMV shows query 1 incur exclusive lock, query 2 return details with 'Original'. Snapshot isolation appears to be in place. Observation from the test shows that itself enable/disable the READ COMMITTED row versioning regardless of setting, and vice versa. 

I think you are misusing the explicit_defaults_for_timestamp. In your case this variable should be set to 0. Enabling it you are in fact restricting the insert of Null values into timestamp data type columns. $URL$ 

I have figured this on my own and I wrote on my blog. For those interested in the solution visit this posts: RangeS-S, RangeS-U, RangeX-X 

Make sure no one but one sysadmin has access to the restored database. Put the db in single user mode after the restore is completed. Check the code inside all stored procedures and functions and triggers inside this database. Perform a dbcc checkdb to make sure there are no integrity issues. Check the users which used to have access to the database and remove all of them. Start allowing access, very restricted to specific objects checked by you. 

I came to a dead point in a deadlock analyze. According to msdn: RangeX-X are Exclusive range, exclusive resource lock; used when updating a key in a range. RangeI-N are Insert range, null resource lock; used to test ranges before inserting a new key into an index. So I understand that if I have an Index on 2 key columns - and I insert a new key I would have RangeI-N lock but if I update an existing key from the index I would have RangeX-X. But my question is more or less complicated. Say I have the index IX_keys_included on column A, B and included column C. In Serializable isolation mode I insert a new value for the included column C. Will there be RangeI-N or RangeX-X locks for the index IX_keys_included? Actually , will there be any locks given the fact that I insert a new column for an included column in the index? 

No. Since the update does not update data in neither col 2 nor the clustered key (col 1), the index _idx_TableA with only col 2 does not get updated. @professionalAmateur, click on the execution plan button on the top. 

Version - SQL Server 2012 SP1 CU3. I have been using sys.dm_os_volume_stats for a while and it was working. Recently I have not been able to return any result from this DMF on one SQL Server 2012 instance. e.g. 

@elijah, SMO does have function to shrink file. The PowerShell script below shows the log.shrink method is used to shrink ONLY the log file. The shrink with default or truncateonly option work for me with full recovery model. 

You could look into changing the database to contained database. Contained database user are authenticated by the database, not at instance level through login. It makes moving database to different instance simpler. If not, you could backup the login information using sp_help_revlogin scripts provided at this Microsoft support KB. And execute the output script on the new instance. 

@matt you might want to adjust the tablix or row KeepTogether property. This affects how it tries to have all rows/table on the same page. 

@cicik, if you are looping through the databases at your local server like you mentioned, assuming you already have the list of remote databases stored somewhere in a local table, you could loop through the databases (e.g. cursor) and execute, SELECT * FROM [YourLinkedServer].[YourRemoteDatabase].[dbo].[view]; This should put the remote database context at the database you specified, and hence it works for FILEPROPERTY function and sys.database_files dmv that return values only for the current database. Also, you might want to consider extracting the list of databases on the remote server and run the script on each execution at the remote server (client side) so you won't have to create the view on each database, and won't miss any new databases created without your knowledge. UPDATE If you want to use script to dynamically extract the data from the linked server without adding the view on every database,