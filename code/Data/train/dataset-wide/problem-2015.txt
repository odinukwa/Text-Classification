As you've identified, storing the price on the order makes the technical implementation easier. There are a number of business reasons why this may be beneficial though. In addition to web transactions, many businesses support sales through other channels, e.g.: 

Note that just because an object is listed as a dependency, doesn't necessarily mean it'll be invalidated when you modify the base object. This becomes more likely in 11g with its finer-grained dependencies. So you'll need to extend this to check the field to check whether whether you have actually invalidated the dependencies, with checks as to whether it was already invalid. How far you need to extend depends upon why you need to know what the "invalidator" was. A word of warning - if you manage to (permanently) invalidate the (e.g. by dropping the table), you won't be able to run any DDL in the schema again! A suitably privileged other user will have to drop/recreate the trigger for you. 

The hybrid approach is often used, with many of the main DB vendors having a complimentary in-memory database to synchronise with their traditional database: 

With fewer different values in the leading column compresses better. So there's marginally less work to read this index. But only slightly. And both are already a good chunk smaller than the original (25% size decrease). And you can go further and compress the whole index! 

When there is a unique key in a table (not ) and procedure is running, on duplicate key error the whole process will be halted. I want to resume on error and call the procedure again. The procedure: 

In my recent project which is about social networking for an Asian country, I'm in doubt whether I use the below SQL statement for counting records: 

I want to count correct col upon ct_id(course) col. I queried a lot but gained no success. How to query such a thing? for example in SELECT I should have two columns: -correctNo -incorrectNo for each course(ct_id). Note: The last column(correct) is a derived column: IF(selected_answer=answer,1,0) as correct 

When operations are slow, and a user that has many records take so long it has domino effect on all the other operations after a few seconds. When I explain the query on the large collection I can see the result that it has used an index: 

How filtering fields affect performance? Is the performance related to the size of data that is transmitted over network? or the size of data that will be hold in memory? How exactly this performance is improved? What is this performance that is been mentioned in documentation? I have slow MongoDB queries. Is returning a subset affect my slow query (I have compound index on the field)? 

The explain plan is just a prediction of the join methods that will be used when executing a query. This can make it hard to infer which step is the most time consuming, as a different plan may be followed when the statement is executed. To get actual stats about how long each step takes you'll need to run an sql trace of the statement and review the trace file - manually or using a tool such as tkprof. This will show you how many rows each step processed and how long it took. That said, looking at the listed at the end of each line will give an indication of how many rows are to be processed. Steps processing more rows are likely to take longer to execute as there's more work to do. So in your example line which is expected to process 102,068 rows is likely to be the most expensive as the other steps are predicting one row. This is only true if these cardinality estimates are accurate however; you'll need to verify that these cardinalities match the actual rows returned. The easiest way to do this is via an sql trace as stated above. 

Times are in microseconds. You can combine these to get average time/execution stats and so on. For example, this gives you the average time/execution for a given sql statement: 

My current storage engine is and its compression level is as default, snappy. I've come across MongoDB documentation and it's been mentioned that using zlib compress better but needs more CPU. I want to know will store more data in memory compared to as it compress the data? I have a server with 16 CPU cores. As RAM is more expensive I'd rather to save on memory in case it keeps more data. Is this correct? Can I blindly switch to zlib to cache more data and improve read performance? NOTE: Our server is read intensive. 

It's good to note that I have done the same exact procedure on another server and it went all ok and MongoDB works as expected. Now my config is as below: 

I changed hostname from to . Moreover SSL keys are generated for . Mongo is now up and running with . 

I have DB with about 30M records in a collection with about 100GB collection size (total documents and indexes). I have a compound index that filters data based on user_id and some other fields like: , , etc. With I see slow queries of about 10s, 20s or even 40 seconds! I ran the exact same query and result is fetched less than 500ms (though it may get cached on second try). When I get that ongoing stat, I see the following lock status: 

Mostly you'll learn by writing queries, looking at the (expected) explain plans and comparing these to the actual execution plans (either via tracing the query or using the SQL monitor). Then re-write the query, add/remove indexes, etc. and see how it affects the plans and execution times 

As this solution implements the constraints in the SQL layer, it overcomes some of the concurrency issues discussed in the procedural solution however. UPDATE As pointed out by Vincent, the size of the MV can be reduced by only including the rows with stage_id = 1646. It may be possible to re-write the query to consume no rows, but I can't think how to do that right now: 

You can then include the as a reference in the table (with no reference to ). You'll need to ensure that you can't change the parts and tunings for a configuration or implement some form of versioning on this table to ensure you can reconstruct the exact details used to set a given time in the past. 

If you're not already familiar with reading and understanding execution plans I'd spend some time learning these: your bound to run into performance issues at some point so knowing how to diagnose the problem correctly will become more important as it's harder to add new indexes or make schema changes when your row counts are larger. 

But in the query I get the result I want, the part is not . Is there someone who could explain this and shed some light on the subject? 

is just 4,688! It is not much compared to the total data of the collection which is 30M documents. When Mongo gets slow when it has domino effect, CPU usage and Memory is not high. Mongo just uses 40% of the memory. Disk partition is if that helps. Another example of a very slow query in full details: 

I want to insert about 14000 records in a junction table, but the problem arise when there is a duplicate key for unique(iq_id,q_id)? what do do? 

Why is this happening? P.S.: I should note that Horde_groupware database have innoDB tables, when everything is messed up and I I get the error says bad information in .frm file. 

NOTE: My mongoDB is a single primary node in which replication is enabled (Use to capture changes on DB) Now when I restart it gives error below: 

Now I do not have access to primary node and it cannot show my dbs. When I disable SSL, everything works and node gets back to primary state. Why is this happening, while I have done the exact same thing on a node in another datacenter? 

Now users in the name of who has been added by uid=60 should not have be shown! The result of this query is empty. I can't figure that out, what I'm doing wrong? 

If you just want an overview of your system in a time period (including "heaviest" SQL statements), the AWR report itself gives this. You can find an intro to this on oracle-base. 

You can use the package to "re-organise" a table like this. It provides functionality for you to create a temporary table with modified contents of an existing table as you describe. It also allows you to copy over the grants, triggers, indexes etc to the new table, then switch the tables over so the temporary table becomes the live table. See this article for a worked demo. 

If you've no need to store the date component, then an is the most practical approach. The biggest advantage comes if you need to do any calculations finding how long has elapsed between two times as you don't need to do any extra processing out of the box. For example: 

To find if there's changes, have a read of Kerry Osborne's unstable plans article and look for the "awr_plan_change" script. If you are switching between plans, you can force it to keep the good plan using SQL profiles. Again, Kerry Osborne has a detailed article on this. Note that it's possible that the SQL you're interested in isn't stored in the AWR, as only the worst performing statements are kept. We can force details of it to be kept by "coloring" it however, as discussed by Dion Cho. 

Why it reports that ? Is there something in between that I have missed? Could someone shed some light on this? 

I want to get one post(e.g p_id=8) and all of its comments. how to do that? How to get count(*) from comment table while your getting your post record? 

I have a unique compound key like fr(fromid,toid) in the table, when I run the query with explain I get the following result: 

When I insert a record I need to return the inserted id by . The problem is that the table is partitioned and in partitioned table I can't use . I run multiple queries at once so I'm in dire need of . Is there a way to accomplish this? 

When we index a timestamp field like timetime. Can we use it for or is it just usable for OR ? Does mySQL use this index when doing vise verse ordering? 

As option separates table files instead of putting all data and indexes of DBs into one ibdata file, is using this option improve speed of alter table? I have a table of 40M rows and when I alter a specific table it takes about 5 to 6 hours. Does this solution help? Is there other ways around to improve alter table speed on heavy tables? 

The psuedo-column ORA_ROWSCN is assigned at commit time and can be used to identify the order of commits, with the following caveats: 

This appears to be a straightforward connect by where you're getting the next person based on the prior manager. The only difference being you want to get the top-level (root) person listed for each row it applies to. This can easily be found using the function (docs), which will return you the value of the column listed at the root node in each hierarchy. This gives you a query like: 

Whereas you can have multiple unique keys in a table and the referenced columns may allow null. By declaring a primary key, you are saying "this is the candidate/surrogate key that should be referenced by foreign keys". This is well understood by database developers who are unlikely to apply a foreign key to a unique constraint when a primary key is available. Unique constraints can also be used to enforce rules such as "a customer can only have one default contact number". A primary key can't be used for this purpose, as customers may have multiple non-default numbers, so the constraint can't uniquely identify all rows in the table. 

Oracle database licenses are quoted per processor, not per machine. They don't care whether you have two machines with two cores each or four machines with one core each. Both equate to 4 licenses you need to purchase. For clustered installations, you need to purchase the appropriate database licenses and the RAC licenses.