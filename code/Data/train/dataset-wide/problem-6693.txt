I would definitely start with Gardiner: $URL$ That's what I used back in the day and it was concise and clear. But there are a number of caveats. 

Many of your examples were addressed directly and very powerfully by Michel Fauconnier in Mental Spaces which he later developed with Mark Turner into the conceptual integration or blending theory. I highly recommend their The Way We Think. George Lakoff has enriched mental spaces with his notion of idealized cognitive models or later frames which also address many of your questions. I recommend the first few chapters of Women, Fire and Dangerous Things to understand the depth of the challenge to the truth-conditional approach to meaning which your question assumes. A more complete elaboration of these issues can be found in Leonard Talmy's cognitive semantics. Look in Part 2 of Volume 2 at semantic interaction for treatment of the issues you raise. With a richer conception of meaning than referential truth, you will find that your problems simply disappear but in that will open a far more complex and interesting world of meaning making in language than Frege, Russell and the like could grasp with their riddles. 

What I think would be a more interesting question is why this happens sometimes and not other times. The two languages that underwent historical change but almost no splintering are Greek and Persian. They were both the official languages of vast empires and lingua francas long after the political unity ceased. Yet, they never gave rise to new major languages (despite having ample dialects) in the same way that Latin, Arabic, and many other 'proto' languages did and English is in the process of maybe doing. I think it would be possible to trace the cultural and political forces in play that show all the nexus points where these developments were diverted (the presence of strong canons is probably not a strong factor) but they still pose an interesting counter example. 

So for example, if the purpose of your determination was whether you can take part in one particular class of phonetic experiments, then just the fact that you're multilingual would be a problem. But if you want to be a teacher or a translator, then things are much less clear. You can achieve native-like competence that would allow you to work as one or the other at any age. You can also lose that competence. That's why nothing about your story can let us determine whether you should or should not label yourself as a native speaker. Have you continued using English regularly for a variety of communicative purposes? Are you confident in literacy-intensive environments? Would you classify yourself as being a speaker of a particular variety of English? Do you speak with a recognizably non-native English accent? Do you have intuitions about English syntax and morphology that converge with other similarly educated speakers? You obviously have a reason to ask. If it's only a question about what you can put on your teaching resume, than I don't think you have to worry. There's no magic behind native speaker teachers. If it's a question that stems form a lack of confidence about your English facility then you have to make a judgement based on the situation. And say things like: "For the purposes of X, I am a native speaker of English. But if you ask me to do Y, I'd recommend someone else." 

There is no way to define 'solvability' of a sentence with cloze blanks by the number of gaps. What makes a gap solvable is the ability of the speaker to determine what is missing from context. And there is no algorithmic way to define that with absolute precision (in fact, even humans are not all that good at it). Nouns happen to be pretty much the worst possible part of speech to pick at random because they are a very open largely context independent class. So even a simple sentence with a single blank like: 

PS: For those who read Czech, I wrote at some lengths about the Prague School and Cognitive Linguistics in the afterword to my translation of Lakoff's Women, Fire, and Dangerous Things. Full text here: $URL$ My conclusion: There are many important connections in the idea of functional motivation of linguistic structures but when pressed Prague linguistics returned to formal analyses and never developed a cognitively realistic semantics. 

are both not acceptable. This means that the idea of the case function-form pairing itself has not been lost in English. Something just happens in the coordinate constructions with 'me'. This is similar to the loss of 'whom' in English where 'Whom did your see.' is outside most native speaker intuitions whereas 'Her I saw.' is not. English does not have a nominal case morphology so you can dismiss these examples as easy to file away surface exceptions. But you can find examples even in languages where the morphological case is unstable in even in languages where it is unavoidable. My favorite example is the Czech verb frame 'teach something to someone' which in modern Czech has both the direct and indirect object assigned the accusative case. That's because the direct agentive structure makes sense when either of the objects is on its own. 

Clauses defined broadly as subject and predicate. They can be found in any language and are much more easily and reliably identified. You can usefully study relationships between clauses in speech without recourse to the notion of a sentence and arrive at very similar conclusions. Perhaps, here the term sentence could be reserved for groups of clauses that are linked through hypotaxis (subordination) but even here, these relationships occur in many non-sentence-like contexts. Like: A: It's going to be a great party. B. Yeah! If Johnny decides to show up. Utterances or texts which are units of expression that can be studied with respect to communicative intent without regard to length or internal structure. Thus, I can treat this entire response as a single utterance/text just as much as the word 'Boo' shouted into somebody's face constitutes an utterance. Looking at the types of utterances, their internal structure and relationships (such as co-reference, turn-taking, repair, etc.) can reveal a lot of useful things about language as a whole. 

But interestingly, the two languages meet when these verbs are nominalized. English has 'knowledge' and 'ability' and Czech has two nouns that match perfectly (or rather the two nouns derived from znát and vědět are almost perfect synonyms and the noun for ability is derived from a completely different verb). There are also many other verbs related to knowledge - 'recognize', 'realize', 'distinguish'. While English has completely separate roots for these, in Czech, many of these are related to one of the verbs for 'to know' - 'poznat', 'uvědomit si', 'rozeznat'. How did this happen? Or why is Czech different from a more related language like Russian? There is no sensible way to answer this. However, internalizing these differences can give you a better insight into how language works. 

Your question is confused. You confuse a grammar (an entity) with its expression (a language). So by very definition, the grammar of any language (formal or informal) is expressed in a metalanguage. However, the difference between "formal" and natural languages is that a grammar of a natural language is always expressed in a metalanguage which is a part of natural language in the broadest possible sense (point made strongly by Gadamer in his Hermeneutics). In this, the Wikipedia quote is not very accurate because metalanguage can very easily be applied to itself. However, a grammar of a formal language is more likely to be expressed by a different order language. Even a simple A -> B contains a symbol not of that language. However, this all depends on a metaphor of thinking about formal languages as language, in the first place. It's very useful but needn't be taken to extremes. 

This is a great question. Not that it matters all that much, but it's always good to periodically revisit the directions one's discipline has taken. First, the problem is that there's linguistics and then there's study of aspects of language for a particular purpose. Lexicography, pedagogic grammar, philosophy of language - they all have a long tradition in the West. You could almost say that applied linguistics came first. One of the earliest significant works of linguistic theory was the Port Royal grammar of 1660 but it can in no way be seen as even a pre-cursor of modern linguistics. Its importance was retrospectively recognised by Chomsky but it had almost no direct influence on the development of the discipline. The true starting point of linguistics as a separate discipline is generally identified in the work of William Jones (more than 100 years after the Port Royal grammar). Jones recognized commonalities between several Indo-European languages and thus started what is still recognizable as modern philology. From him, we can trace the developments of the following century almost in a direct line. It would be another 100 years before we could point to something that looks like truly modern linguistics. By this I mean, an empirical research paradigm aimed at discovering the principles behind the workings of individual languages, their building blocks and language as a general phenomenon. Some names that stand out along the way are Wilhelm von Humboldt and Hermann Paul but it's not until the work of people like Otto Jespersen, Ferdinand de Saussure, Jan Baudouin de Courtenay and Vilém Mathesius at the start of the 20th century that we get output that we can read and still find linguistic affinities with (note: philologists can go all the way to Jones). de Saussure is by far the most famous but mostly through the efforts of his students. The competence/performance dichotomy can be directly traced to him (Chomsky's inventing history by pointing to the Port Royal grammar as his true antecedent. It was really the structuralists.) And, of course, it is only slightly later that Edward Sapir and Leonard Bloomfield contributed their significant syntheses that echo in the work of linguists to this day. We should also not neglect the developments in affiliated disciplines which have been developing along side (if often slightly behind) general linguistics. Phonetics and phonology, psycholinguistics, and even philosophy of language each have their own interesting histories and intertwining but separate interests from those of linguistics. Then there are those of subdisciplines like sociolinguistics, contact linguistics, discourse analysis, corpus linguistics, etc. which each also have trajectories that are worth pursuing most of them not really starting until the 1950s. In many people's mind, linguistics is identified with generative linguistics but that is only one of the many subdisciplines of the field whose importance was artificially inflated at least in part due to US defense funding of AI research (see Frederick Newmeyer's histories on this). Sadly Pāṇini's is always only mentioned as a footnote. Yet, his influence on all the Sanskrit scholars must have been significant. When you compare his meticulous treatment of Sanskrit grammar (including phonology) from at least 400 BCE with the meager output of European grammarians since the days of Plato, you cannot be in awe. Arguably the work of Indian grammarians provided models of best empirical practice for European students of Indo European languages but it is hard to estimate exactly what impact it's had on linguistics as we know it today. But it is without question the greatest work of empirical and theoretical linguistic inquiry prior to about the mid 1800s. Finally, let's talk about the question of "scientific" study of language. Chomsky and his followers often cover up their embarrassing ignorance of the vast field of linguistics by dismissing anybody not in their formalist tradition as somehow not scientific enough. Whatever you think about Chomsky's own theory (and I think it is an impressive achievement if not really that much about the empirical phenomenon most people would describe as language), this is just pure and unadulterated nonsense. It's a rhetorical rather than an empirical device that is unfortunately all too common in academic discourse. But it is no less disreputable by its ubiquity. Post Script: Many alternative perspectives could be offered on this subject. I have certainly focused on the work with which I am most familiar and to which I feel a great affinity. However, I suspect that for all its biases, that mine is fairly mainstream perspective on the history of linguistics. I would accept quibbles and corrections on almost every particular but the overall trajectory would probably remain the same. I wrote this from memory (relying on Wikipedia to check the spellings of names) influenced as much by my reading of the source materials as histories of linguistics I read and classes in the history of linguistics I took about 20 years ago. I spend a lot of time trying to find connections between old and new understandings of language but mostly in a rather unsystematic manner, so I took this opportunity to summarize some of my mental notes. 

I think you're confusing the issue a little bit. You need to differentiate between the concept of phonetic alphabet and transparent orthography. The introduction of the IPA or international phonetic alphabet was the one more or less successful attempt at making communication among linguists and language educator more consistent across languages. But most of its uses focus on key phonemic (rather than phonetic) features of the individual language and are subject to convention. So the pronunciations in a dictionary will not reflect many phonetic features like the nazalisation of vowels, etc. These conventions often compete with language change and dialectal variation. Take for instance the 'a' in English 'bat' Its pronunciation varies greatly across English dialects but in Britain has mostly moved from /æ/ to /a/. However, in most neutral contexts /æ/ is still used for consistency's sake. A true phonetic transcription would also record assimilations leading to spelling such as 'books' and 'budz'. To make things even more complicated, many sounds that are recorded by the same IPA symbol across languages do not actually sound exactly the same - this is particularly true for vowels - leading to more confusion and the necessity to learn local conventions for each language when reading transcriptions for the purposes of cross-linguistic comparison. As a result an adoption of a fully phonetic alphabet that would apply to all languages or even just one language was not ever contemplated because there would be no advantage to it. However, transparent orthography is a much more sensible concept. All it indicates that the way words are written makes it easy for native speakers who learned to associate individual sounds with letters (or groups of letters) to reliably pronounce any string as it was intended. No language is completely transparent in the sense, that it is necessary for readers to learn some conventions about how to pronounce certain letters in context, but there are languages that are almost completely transparent (Italian, Slovene) and those that are almost opaque (Mandarin) or very opaque (English) or somewhat opaque (French). Some othographies are only opaque in one direction (Greek - easy to read, more difficult to write, Arabic - easier to write, more difficult to read). Many languages have successfully reformed their orthographies to be more transparent but English (which is what I think prompted your question) has not. There were some attempts to do this in the US in the 19th century which gave us things like color/colour but they only tinker around the edges. There are several reform movements active in English orthography but they have no prospect of being successful in the short-to-medium term. Any possible advantages of transparent orhographies are purely marginal. Opaque orthographies have not prevented cultural, economic or geopolitical success of cultures for which they are typical (see US and China). They have certain obvious disadvantages for learning and use but the transactional cost is so small overall, that the possible long-term gain does not outweigh the huge short-term cost of a switch for a large and powerful culture like the Anglophone world or China. Smaller cultures have often undergone such reform but it was always linked to larger issues of cultural and political identity - never purely because of some sort of rationally judged advantage of a more transparent system. 

This last point has to deal with two important objections: i. What about the obviously different cognitive natures of the conventions of grammar when compared to the conventions of culture? It's much more difficult (and quite different) to 'learn a language' than it is to 'learn a culture'. Our ability to behave in conventional linguistic ways is embedded so much 'deeper' in us than our ability to behave in culturally conventional ways. Just compare acquiring the phonological distinctions of a language with learning to politely refuse an offer of food. This is a good point to which I only have a partially satisfactory answer in that the conventions of culture are much deeper and more complex than people imagine. But I have to admit that they still seem qualitatively different from the conventions of language. I've addressed some of this in another post. ii. Isn't talking about an inventory of conventional ways of expression just renaming what we've always talked about as grammar? Yes and no. In claiming that grammar is just an artifact of the grammar book, I'm not suggesting that the grammar book is a deluded enterprise. For millenia, grammar book writers have been describing real conventions and have done a great job in putting together taxonomies of these conventions. So when rethinking the nature of language without 'grammar', we don't need to start over and jettison all those old rules. They're pretty handy for what they are. But we need to rethink their place in our understanding of language. We must not confuse all those taxonomies of linguistic conventions with language for which we then need to find cognitive and neurological correlates. These taxonomies that were developed for the convenience of the reader and writer (and often influenced as much by the convention of the genre as anything else). The structure of language does NOT look like the table of contents of a grammar book. There's probably not even such a thing as a single structure. But that is often what we're lead to believe. So in conclusion, even though this question may seem a bit naive, it is actually a profound puzzle to which linguistics does not have a complete answer. The fact, that we can be so dismissive of the question only indicates how much we need to rethink what we've been taking as givens about language for at least a hundred years. There are many linguists and other cognitive scientists who are doing that. But perhaps few who would want to be as radical about it as this. But I suspect that in a few generations, this will not seem as bold, as it may now.