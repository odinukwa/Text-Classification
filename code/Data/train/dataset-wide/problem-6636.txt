Your interpretation is more or less correct, as long as you are speaking of the initiation of those articulatory gestures and not their completion (the events are roughly simultaneous). Sproat & Fujimura's study using x-ray microbeam tech found that dorsal retraction and lowering begins earlier in dark l compared to light l. The article has a number of informative details about how dark l is actually produced. 

You are assuming that "eat" and "edible", for example, share a common synchronic morpheme, which is a hallmark of the SPE analysis of English. There are innumerable problems with that analysis, which are a result of having pretty low standards for synchronic relatedness. An alternative, which has worked out pretty well for phonology, is simply to say that these words, along with pairs like "father" and "paternal", are semantically related (there is some connection in meaning) and historically related (they do derive fromt he same Indo-European root), but within the confines of a grammar, they are not related. There are a number of devices employed in the SPE analysis to account for irregular phonological alternations. For example (see p. 201) the vowel alternation of sit ~ sat is subsumed under general vowel shift (which otherwise only affects tense vowels) because some verbs have the diacritic [+F] assigned in the past tense, and they expand the rule to include the context "[___,+F]" (i.e. "when a vowel is marked [+F]"). See chapter 4 in general for segmental analysis. The indices of SPE are very helpful for understanding this: there are word and affix indices, so if you want to know the SPE treatment of -ion, it's in the index and you can see the 9 citations of that affix. The distinction between phonologically-neutral affixes (-ness, -able) and those that cause changes (-ity, -ous) is the foundation on which level-ordered phonology and morphology (a.k.a. "Lexical Phonology") is built. It has something to do with generality and productiveness – these changes are not productive or, it is often argued, only marginally part of synchronic English phonlogy, and marginal affixes have marginal phonological processes, because those processes were phonological rules many hundreds of years ago, and the results of those sound changes along with the conditioning affixation was long ago stuffed and mounted in the museum of sound change that is the lexicon of English. 

The bracketing is correct. The Compound stress rule will apply to give the 1-2 contour to "bike-ride"; then the Nuclear stress rule assigns 1 stress to "bike", causing across the board reductions of other stressed, so you get a surface 2-1-3 profile. This is analogous to SPE's derivation 16a, for "black board-eraser". 

Are there any publications which list words of English that one might reasonably expect a child to know? I assume that "father" would be on the list, and "allophone; metallurgy" would not be. As for age of this ideal child, between 6 and 12 is in the relevant range. My need for such a list relates to the question of how English phonology is acquired, especially whether the phonological generalizations that describe a plausible child lexicon are radically different from those describing the entire set of English words (the answer is "Clearly", this is about demonstrating that). My preference would be for a list that is based in actual observation, but a policy list ("children in grade 5 should know these words...") might work. 

English would be "language 114" given this list and Danish would be "language 112". Then you'd need a really big property list, with at least 32,000 properties. Alas there is no such list out there, so you'd need to do a lot of creating. A good starting point would be to try to specify all of the elements of a sentence, then ask how pairs (or triples) of words might be ordered. You'd need more than just subject, object and verb: what about "location", "instrument", "beneficiary" and various other functional relations? Subjects (objects, and so on) aren't just single words, they are entire phrases centered around a noun, embellished with... adjectives? articles? numerals? possessives? A verb in a language could just be a verb, but in many languages you have to say whether it is past, present or future tense (and maybe past and present are the same; maybe present and future are the same). When you decide that you want a language with verbs indicating past, present and future tense, then you have to make a decision about how that is indicated, for example you could mark present with one prefix and future with a different prefix; or they could both be suffixes; or one could be a prefix and the other a suffix. Suppose you decide that the present is marked with a prefix /kan/: then you have to decide if there are any rules of pronunciation where /kan-plaf/ is changed to [kamplaf] or [kanblaf] or [kamblaf]... that list is enormous. Your question essentially reduces to the field-worker's training problem: what kind of questions do you need to ask when you're attempting to describe any random language. There are a number of guides out there, which tend to spend time on issues that are not relevant to your concern (ethics, how to record and organize data, how to evaluate informant responses) but will also include some typological overview (so that you aren't freaked out when you learn that in some languages, the verb can actually precede the subject and even stranger, the object can precede the subject). Knowing what the dimensions of variation between known human languages is a first step. A starter list of readings would include Bowern Linguistic Fieldwork: A Practical Guide, Crowley Field Linguistics: A Beginner's Guide, Vaux Linguistic Field Methods, Whaley Introduction to Typology: The Unity and Diversity of Language, Vellupillai An Introduction to Linguistic Typology, Comrie Language Universals and Linguistic Typology: Syntax and Morphology, Payne Describing Morphosyntax: A Guide for Field Linguists and Exploring Language Structure: A Student's Guide, Merrifield Laboratory Manual for Morphology and Syntax, Bickford Morphology and Syntax: Tools for Analyzing the World's Languages. Maybe if you want just one book, get Comrie. 

The question is not just hard to answer, it is impossible, since it's based on two related, undefinable standards: "the pronunciation" and "reasonable scholarly assurance". Let's take "the pronunciation" of English winter. It is pronounced [wɪ̃ɾ̃ɹ̩], at least in my dialect. However, I do know that it is also pronounced as [wɪntɹ̩], [wɪntɜ], and [wɪnɜ] depending on dialect and individual. There isn't a unique pronunciation, in fact, I vacillate between [wɪ̃ɾ̃ɹ̩] and [wɪntɹ̩]. "Pronunciation" is a vague abstraction which covers classes of acoustic events, and even one individual uttering a word a dozen times will produce a dozen different outputs. The differences may be grammatically insignificant and very hard to perceive, but they exist. The question presupposes that a word has a unique pronunciation for all speakers, which is not the case. So the question need to be sharpened to specify what you mean by "pronunciation", in particular, the level of detail that you require. In typical linguistic usage, "pronunciation" admits of a lot of imprecision, yet you see people here rejecting candidate words because of the scent of imprecision. More severely problematic is the standard "reasonable scholarly assurance". What does that even mean? Is there "reasonable scholarly assurance" that winter is pronounced ˈwɪnˌtʰɹ̩ or that the Classical Arabic word is [ʔaʃʃita:ʔ]? What is "reasonable scholarly assurance" opposed to? -- no scholarly reason at all? What about "very likely, but not absolutely guaranteed". If you don't accept my representation of the pronunciation of the recorded sample of "winter", how do you show that mine is incorrect and that something else is correct? Suppose you uncover a wax recording of English "winter" made 150 years ago, what would be "scholarly assurance" that the resulting representation is "the pronunciation"? There is a difference between "absolute infallible certainty" and "reasonable scholarly assurance", yet you see people being confused between the two. I contend that proto-Indo-European and proto-Germanic reconstructions are "reasonable scholarly assurances", though in instances somewhat less certain than assumed Classical Arabic pronunciations. The (Vedic) Sanskrit word ɦimjáh (himyáḥ) "winter" is probably the oldest attested word for a season passing a high standard of certainty. There is not a shred of evidence to reject the well-established word and phonetic interpretation, so it passes the sniff test for "reasonable scholarly assurance" (though you may need to know more about the tradition of preserving Sanskrit to be assured, in case you are not familiar with Sanskrit). 

There has been a general desire to equate unmarkedness with structural simplicity, so that [i], [a] and [u] might have only one feature and [ø] would have more features. If order for this to work out, one either needs underspecification so that some values are not underlyingly present but are filled in by later rule, or are never filled in (i.e. privative features). These are assumptions shared (at least in part) by most theories, ranging from PSM, Dresher's contrast theory, Dependency Phonology and Government Phonology, UFT, and Selkirkian theory, non-exhaustively. Assuming a version of UFT, the relevant vowel place features are [coronal], [labial], [dorsal], and either [open] or [closed] for vowel height (the former being the official Clementsian approach). Accordingly, a front unrounded vowel is formally simpler than a front rounded vowel, and a high vowel is formally simpler than a mid vowel. Unfortunately the reduction of markedness to formal simplicity doesn't progress much further in this theory, since [ɯ] would be as simple as [u] but by usual diagnostics and factual assumptions, [ɯ] is more marked than [u]. I conclude that the attempt to reduce markedness to formal simplicity is a failure, that although one can emulate that effect to some extent, attempts to fit feature theory to markedness complicates phonological theory, and ultimately fails. Instead, attention has been put on what "markedness" is, and asking what really explains those facts. The basic explanatory idea that explains away the grammatical concept "markedness" is "difficulty of distinguishing". The reason why [ɯ] is less frequent as a phoneme than [u] is that [u] employs two means of signalling low F2, and [ɯ] employs only one. So [u] and [ɯ] are too similar (hard to tell apart), and [u] more clearly / unambiguously represents the percept "low F2". The prediction then is that to the extent that a surface distinction between X and Y is difficult to maintain, that distinction will tend to be eliminated (and various phonetic and grammatical considerations will dictate what the outcome is). I also think one should be careful about claims that are based on questionable writing practices. Very often, some language will have a vowel transcribed as "u" which is somewhere between [o] and [ʊ], but for the sake of simplifying an orthography, "u" is used. The distinctions [ɨ / ɯ] and [ʉ / y] are prime examples. If you have a contrast between the members of that opposition, then you might reasonably maintain that a language (like Norwegian) has both [ʉ] and [y]; otherwise, I would be skeptical unless there is acoustic evidence based on formant values of IPA vowels as produced by certified speakers, compared to the vowels of a given language. [Edit] I am usually skeptical about the notion of articulatory markedness, which would only make sense w.r.t. articulatorily-difficult gestures (e.g. intricate timing of click movement). It can't be more complicated to raise and back the tongue without protruding the lips than it can be to both raise and back the tongue and protrude the lips. The test case would be a pair of sounds, where it is physically difficult to produce the sounds differently, but when they are correctly produced, they are extremely easy to distinguish, perceptually. The problem is that we can test for ease of perceiving differences, but we have no way to quantify how hard it is to produce [q] (as opposed to [k]) or [θ] (as opposed to [s]). If you speak Czech, [θ] might be really difficult to produce, but that crazy "Czech r" which the language is famous for and I swear after a zillion tries I can't get right, that is no problem (for a Czech speaker). I'm basically willing to put all of my money on the program of reducing segmental markedness to problems of perception, and grant that cases where similar-dounding sounds can end up even more similar-sounding because the articulatory timing distinctions needed to make the phonetic distinction are difficult. 

One problem is that retroflex consonants are a disparate group, so that one might be comparing apples and oranges if you try to apply the lessons of Tamil to Polish. This dissertation by Haman gives the best overview of acoustics and perception of retroflex consonants). In the case of the unaspirated stop [ʈ], because the stop itself is just silence, ability to perceive that stop as distinct from [t] or [t̪] has to come from formant transitions to or from another vowel. Fricatives, however, have distinctive spectra, so it is possible to perceive the distinction between [ʃ] and [ʂ] just within the fricative. Since Polish retroflex consonants all have a fricative element, it is possible to hear the difference independent of formant transition. However, as is generally true of consonants, the best cues to place of articulation come from the influence of the consonant on a neighboring vowel. At the same time, the articulation of retroflex consonants is changed as a function of the surrounding vowel. 

I haven't encountered a special linguistic term for the denominator in fractions, and I'd be surprised at any linguistic work saying that "third, fourth, fifth" as used in fractions is an ordinal number -- it's the same (in form -- or pronunciation) as an ordinal number, but what a number "is" also involves its meaning. In Swahili, there are special lexical items (like "half") for some fractions, from 1/2 to 1/6 (whereafter a general construction is used "__ part of N"). The lexicalized denominators (nusu, theluthi, robo, humusi, sudusu) are unrelated to the terms for cardinal numbers 2-6 (mbiri, tatu, nne, tano, sita). Ordinal numbers in Swahili are a regular grammatical transformation of cardinal numbers, except "first" is rendered as "beginning". 

I can't pretend to really understand this article, but the authors have an algorithm for encoding speech at 800 bps (as they say, using split vector quantization). They have tested it using male and female voices and seem satisfied that the output is comparable to standard 2400 bps LPC-10 encoding, though the authors are electrical engineers, not psychologists, so there isn't a body of perceptual studies to go with the method. On the other hand, this study does have a perceptual study to go with the technology, and an algorithmic tweak (based on a theory of auditory perception) results in improves perceptual quality for 2400 bps LPC coding. Chapter 2 of the book Ultra Low Bit-Rate Speech Coding seems to be out there (here). It appears (and I must emphasize "appears") that one can reach 120 bps coding. It might be worth posting a related question to Signal Processing SE, to see especially to what extent these ridiculously low rates yield intelligible speech (esp. to get a technically-qualified evaluation of the state of the art). This thesis suggests 20 bits/sec, based on Shannon's estimate of 1 bit per character entropy in English text, 20 phonemes/second speech rate, and about 1 character/phoneme. 

Surprisingly, Indo-European seems to be an example. Silvia Luraghi has an article "The origin of the Proto-Indo-European gender system: Typological considerations" (Folia Linguistica 45/2 (2011), 435–464) which discusses this, and it appears that there is agreement that the M/F/N system of later languages developed from a two-gender system where masculine and feminine were not distinguished, and the system was based on an animate / inanimate. 

In careful speech, it's pronounced [gʊd pɔɪnt] in American English. In maximally casual speech it's generally pronounced [gʊ Xpɔɪnt], where "X" means that there may be some elongation of /p/. However, that could simply be an automatic timing adjustment of an intervocalic consonant at a foot-juncture: you don't get [ʊ ɪ ɛ] foot-finally and there is evidence that a following consonant gets attached to the preceding syllable, keeping those vowels out of final position. Between those speech styles there are various kinds of phonetic overlap in articulators where there is lingual raising but it is mostly acoustically masked by the following labial closure. It happens, so it depends on what you mean by "rule". If your universe of things includes rules of phonetic implementation (some people deny that they exist) then yes, this is due to a rule, specifically one of phonetic implementation.