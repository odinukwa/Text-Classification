Kroft et al document the raise of long-term unemployed in the US during the recent crises. This is an impressive figure from their paper: 

This means that in calibrating an RBC model, we will chose specifications that give us exactly this result. A simple way of doing so is having an additive separate utility function of King-Rebelo-Plosser type (Note that Prescott in his original paper claimed that only log-log preferences would reproduce this independence, which is incorrect). Temporary Shock revisited To conclude, we calibrate the model such that an income and substitution effect on leisure cancel out exactly. The income effect of a permanent shock is bigger than the income effect of a temporary shock. Hence, if it holds that permanent increases in TFP do not affect working hours, it must be the case that temporary increases in TFP do positively affect working hours, since the substitution effect is the same, but the income effect is smaller. 

Your toy example does not really happen in standard economic models. For one, because we assume identical preferences. Therefore, to the extent that we similarly care about different individuals, we need similar consumption (or whatever enters their utility) to maximize welfare. That does not mean that all allocations are the same. It could be that one of the agents is especially lazy - he should optimally not work. Others will have to do extra shifts, but we will compensate them with more consumption: the utility per person over both consumption and working time will then be again relative to the pareto weights. We keep focussing at classes of utility functions $u(x)$ that are finite and strictly increasing in $x$. Necessary condition Strictly speaking, diminishing marginal utility is necessary but not sufficient to prevent the existence of such "utility monsters". Given two utility functions $u(x), v(x)$, you need that $$x_1 \geq x_2 \Leftrightarrow u'(x_1) \leq v'(x_2) \quad (1)$$ and vice-versa for $v'$ and $u'$. Diminishing marginal utility only gives you $$x_1 \geq x_2 \Leftrightarrow u'(x_1) \leq u'(x_2) \quad (2)$$ It is easy to see that when only (2) holds, given a sufficiently enough difference in the levels of $u, v$, we still find it optimal to give all of $x$ to one individual. Sufficient condition Sufficient condition for every individual having positive $x$ is that for every preference $u, v$: $$\lim_{x\to 0}\,u(x) = -\infty \quad (3)$$ such that no matter the levels of $u, v$, marginal utility just blows up. This together with diminishing marginal utility should prevent utility monsters. Alternatively, $$ u(\epsilon) - u(0) > v(x+\epsilon) - v(x) \, , \forall x>0$$ and vice-versa for $v, u$ and some $\epsilon > 0$ will ensure that you will never sacrifice all consumption of everyone else in order to give it to the other guy. 

You can see that for their long US period, is a positive number, but pretty close to zero. This is why different authors take this number as either acyclical or weakly procyclical. Price rigidity required for procyclical wages? Assume that some sort of fluctuation in TFP "drives" the business cycle. Furthermore, if markets are competitive, the wage rate should be set by the marginal product: $$ w = AF_L(K, L)$$ If $L$ stays constant over the business cycle (e.g. with inelastic labor supply), this setup will generate wages $w$ that vary with $A$, and hence any change in $A$ will result in a one-for-one change in $w$ and in $Y$: $Y$ and $w$ are perfectly correlated, while $L$ is fixed. This is pretty much the prediction of the RBC model, and it is the opposite of what we observe in the data: $L$ is volatile, while $w$ is fixed. If you'd assume that $L$ responds strongly to changes in $A$, $w$ might be pretty constant over the business cycle. This is what we observe in reality. And most of the labor-side of RBC research is to come up with ways that force $L$ to respond so much. King and Rebelo (2000) should be a key paper of any introductory reading list for macroeconomics and I strongly recommend reading it. The case of less competition If there is literally zero competition, then firms will at all times only pay the household his outside-option value, making him indifferent between working and not working. If that outside option is constant over the business cycle (for which we have to some extent theoretical and empirical arguments), wages will be acyclical. However, to the extend that there is no perfect monopsony, firms have to compete for workers and offer higher wages whenever workers are worth more. Hence, wages will be procyclical even if, to some extent, firms are not perfectly competitive on the labor side. The more competitive they are, the more pro-cyclical wages should be. Alternatives and extensions to the neoclassical model The above model was just a simple one to generate a case in which pro-cyclicality of wages can be generated without price stickiness. There is much more to the subject than an answer here can provide. Here's some more references: 

I'll do the opposite and tell you something that we do not know a lot about: The elasticity of substitution between consumption and leisure. While many Macro models provide calibrations for that, there is no - as far as I know - proper econometric exercise based on micro data that gets to this value. 

Market power required for price-discrimination? In my understanding, no. I'm pretty sure that one can show this mathematically, but I'll just give you the intuition. Price-discrimination (PD) increases profits, because it incorporates different price-elasticities by having different prices. Therefore, a firm that incorporates PD should make, c.p. more profits than a firm that does not. Perfect competition means that less profitable firms are forced out of the market. In this case, perfect competition will lead to firms that act with price-discrimination whenever they can. Think about restaurants, where there is a lot of price-discrimination going on, either via lunch offers or menu pricing. Restaurants really shouldn't have market power, should they? 

The Mortensen-Pissarides (MP) framework of search labor is the thing in analyzing (equilibrium) unemployment over the business cycle. Shimer showed that the lack of hires is the big margin that affects the volatility of labor. Since then people tried to analyze potential wage rigidity for new hires. Is it safe to conclude that - in this context - wage rigidity of old matches does not matter anymore? Is this somewhat depending on the framework (As in: old wages don't matter for hires in MP because of XYZ, but this is a strong assumption)? Is wage rigidity, conditional on the match being new, the only relevant metric onwards? 

In a recent conversation, I was suggested that national working hours aggregates, as provided by bureaus of statistics, are based on surveys on time use. Is that true? Can macro labor data, which is used for so many different puzzles and theories, be vulnerable to things such as time-varying biases in estimating time use? How are national working hour statistics computed? 

Are Wages Procyclical? Here's the entry from Table 1 in King and Rebelo (2000; Resuscitating Real Business Cycles). The variable of analysis is log-wages, detrended with HP filter. It has a std, relative std, first-order autocorrelation and correlation with output of , , , . This leads the authors to conclude that 

It surely is possible: For a long while, Gold was the basis for the value of money in many "modern" countries, leaving central banks with little room for policies. 

Since I don't know where exactly you have the problems, I 'm just providing a rough solution scheme here. Note that it's quite long ago that I did this stuff, so I may be outright wrong - don't follow without verifying that the following actually makes sense. a) Since $Y = F(L)$ is increasing in $L$, the sign of $dY/dG$ is equal to the sign $dL/dG$. Through the government's budget constraint, any change in $G$ must be reflected in an increase in $M$ and/or an increase in $PT$. Set up $\max_{C,L} U $ subject to the household's budget constraint, and then replace in the FOC $M - \bar M$ with $PG - PT$ , to get the dependence of $L$ on $G$. As soon as you have $L(G)$, you can compute $Y(G)$ as $F(L(G))$. The next step then is just to look at the derivative of $d F(L(G))/dG$. Once you have $L(G)$, computing $C(G)$ is analogous. 

I know one form of a derivative, which is $$ f'(x) = \lim_{\Delta\to 0} \frac{f(x+\Delta) - f(x)}{\Delta}$$ Let $x_{t+\Delta} = x_t + \Delta \dot x_t$. Achdou and coauthors (end of appendix A1) claim that $$ \lim_{\Delta\to 0} \frac{f(x_{t+\Delta}) - f(x_t)}{\Delta} = \lim_{\Delta\to 0} \frac{f(x_t + \Delta \dot x_t) - f(x_t)}{\Delta} = f'(x_t)\dot x_t$$ I understand the first equality, that's merely the substitution. I can't wrap my head around the second one. How exactly does one show it? 

You write "qualitative easing", but I think you refer to quantitative easing. I'll just do both. Quantitative Easing Quantitative easing corresponds to the central bank (CB) expanding its balance sheets by "buying" assets. This is typically done in secondary markets. It mainly injects liquidity into the system. To the extend that there is an additional buyer of assets now, the price of assets/investment (interest rates) decreases. However, the quantitative impact should be negligible: Through its demand, the CB increases value and liquidity in the markets it is operating. The scale of its operations should be too small however to affect the aggregate interest rate. Qualitative Easing Qualitative easing is a relatively new expression and refers to the riskiness of the stocks that the CB is investing into. In contrast to quantitative easing, which is about the magnitude of assets on the CB's balance sheets, qualitative easing is about the riskiness on the CB s balance sheets, and hence the decrease in aggregate risk (on the banks' balance sheets). Downsides Independence of a central bank When the CB holds assets, it is interested in their value. This may lead it to do policies that go against its primary directive (e.g. inflation stability). Even if it does not do so, effectiveness of a central bank comes from its capacity of controlling expectations. It is sufficient for households/firms to expect the central bank to do "bad" policies in order to decrease the effectiveness of the CB. Number of Goals I don't have sources on this, but I seem to remember that central banks with one clear goal (i.e. monetary stability) are more effective than those with a basket of goals (i.e. monetary stability, GDP growth, decrease of unemployment rate). A general criticism can be that QE are not operations that help with the important margin, monetary stability - and that the central bank should focus on that instead. Note that these are not just esoteric points. In fact, the academics and central bankers at the "Rethinking Macro Policy" conference agreed that (quoting Blanchard) 

$c$ is the households outside option, $r$ is the reservation price, hence the distribution should only give positive mass to prices between $[c, r]$. $$ B_1(u) = 2\nu(u)\frac{\psi_u}{1+\psi_u}$$ where $\nu(\sigma(u)) = \frac{s}{b} = \frac{1-u}{1+u(\psi_u - \psi_e)}$. In their calibration: $\psi_e = 0.02$, $\psi_u = 0.27$. Hence $$ B_1(u) = 2\frac{1-u}{1+0.25u}\frac{0.27}{1+0.27}$$ The Issue For example, at an unemployment rate of $0.05$, we have $B_1(0.05) = .38$. However, for $p = c + \epsilon$ (for small $\epsilon$), the denominator $p-c$ becomes very small small. This means that the product of $1-B_1(0.05)\cdot (r-c)\cdots$ becomes very large. One minus that is an very large negative number. The denominator $C$ is positive. A similar phenomenon happens with $B_2(0.05)$. They call $F(p, u)$ the distribution. I assume this means the pdf. Can a pdf have negative values? Or what am I missing here? 

One argument brought forward (it is a generalization/argument for lack of infrastructures) is the Acemoglu-Robinson hypothesis of extractive versus inclusive institutions. Long-story short: some institutions are good for (long term) growth ("inclusive"), and some are good for short-term extraction of resources ("extractive"). Depending on whether the elites / rulers of a country (colony) had long-term plans of "staying in a country" or whether they wanted to just "quickly get everything they can" out of the country, they installed these different type of institutions. See also their slides and book. 

These kinds of questions depend heavily on the market structure that you assume. Most importantly, when firms have (some degree of) market power, they usually take the demand function as given and try to solve something of the kind $$ \max_p D(p)\cdot (p-c(D(p)))$$ However here, when there is no market power, firms take the price as given and solve for the optimal quantity of production $q$: $$ \max_q q\cdot (p-c(Q))$$ Then, we get the supply of any single firm, given the price $S(p)$ from solving this problem. To clear the perfectly competitive market, we remember that there are $30$ firms and solve: $$p: D(p) - 30\cdot S(p) = 0$$ 

There is no contradiction. Call $p$ the price level and $D$ the nominal debt level. Then, the real value of debt is given by $$\frac{D}{p}$$ The debt level, divided by the current price level. When we have deflation, that means that $p$ decreases. A decrease in the denominator, $p$, means that the total value increases. Perhaps an example helps. You started a debt contract in 1960, for 100 USD. That money was worth a lot back then. Suppose you would have to pay that money back now (without interest). In nominal terms, $D$ is still 100. Assuming that we had average inflation of 5 percentage points (pulling numbers from my head just for the example), the value of the currency decreased around 11.5 times (not real number) during that interval. That is, your real debt is only around 8 USD. Had the inflation over the period been higher, your real debt would be even smaller. The other way around: if there was deflation over that period, your real burden would be higher!