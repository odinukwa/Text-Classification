It will depend on what version of PowerShell you are running, but this command can be used from at least 3.0 and up: 

Second, pull the contents of the loop into a variable and then output it all at one time to the . This will will mean it will pull all of the data into memory and then output it all to the file. It depends on how much data you are pulling out but this may add on some time for the execution of the script. 

Brent noted on it in his answer but just to expand a bit more...I'll just note around learning the BI side of things that using Azure (or other cloud providers) may get a bit expensive. You will need a minimum server size to truly work with those components and running those (or forgetting to turn them off) can drive your bill up rather quickly. [e.g I've forgot to turn one server off and it ran for a day or so, had a bill for about $40.] Now that is not to say that you should shy away from doing anything in the cloud. With Azure you can get a $25 credit each month from Visual Studio Online, same place where you download free Dev Edition of SQL Server. While it is not much to do things for days at a time, it will at least get you familiar with that side of the DBA/BI world. I actually do most of my learning with BI locally on my laptop, just have local SQL Server instance installed and then Visual Studio 2015 + SSDT. (VS 2015 is current version of this writing.). I play around with data sets I download from Internet and just load them into a database...then you can play with designing SSRS report around it. You also have the data dump from StackExchange to use as well, which was just updated a few months ago. 

Underlying SSIS task that do not require SSIS to be installed to run, nor require you to edit via BIDS or SSDT-BI. They are there more or less for easy deployment of basic database maintenance that does not require much knowledge to build. Is it something that fits any environment? I would say no, it is mostly for the one man/woman shops that just need something up and running quickly and is sufficient. Now that is not to say in large environments maintenance plans don't have a purpose, just find they may not used as often. You might consider them a stepping stone to SSIS packages. 

Environment Info: SQL Server 2008 R2, running the package through BIDS only at this time. Questions: 

The package created by the copy wizard can be modified to bring a little better performance possibly but your log growth is expected. Every bit of data changed for the table and indexes is being modified, that has to be logged no matter what recovery model you are in. I would go with the restore from backup as this will be a bit cleaner. You can pull the last backup file name from the msdb backup tables. There are a ton of examples of this online, would check MSSQLTips.com for a start. You would simply pull the last backup, verify it exist, robocopy it over (unless it exist on network share), then restore it. 

It will depend on the type of data being access and your company's security requirements/policy. In general you do not want users sharing an account, especially if that account has more than read permissions on your database. I would verify with your security folks or legal group if there are any specific requirements for managing access to data, or with auditing. If you fall under any industry standards like HIPPA or PCI they have controls that require being able to map access to a particular user. With regards to how you allow users to run reports that is usually driven by policy where I have worked. Do you experience any performance issues now with how they are pulling reports and "peak" at data? As a DBA, I'm a control freak to a certain degree, I like to know who and what is making calls to my data. I have been in one environment where reports were pointed to a database snapshot of the production database, so their was less of a performance concern. Another environment it was not until their report caused a performance issue in production that we would tell them if you want that information run it after business hours. 

Encrypting files through SSIS is possible, but there is no real easy native solution available with SSIS. There are a wealth of third party productions out there than can assist with this. Time is money, so it all depends on how much you want to spend on trying to come up with a customized solution or just go buy something that can help you do it quicker. There is a good write up on this blog post that walks thorugh doing it with some custom .NET code and a ScriptTask. He also list out some of the third party products that offer a solution as well. 

Depending on the number of databases you can use the above function against a list of each database name populated in a variable and output it all at the same time, if dealing with one server: 

As it states in the message you have to restore the log backups up to the point in time of the filegroup backup. In order to bring the database online it needs to play the log back to the same point in time in order for the database to be consistent. However restoring just the primary file group is a special situation. I believe in order to restore the primary filegroup you pretty much have to do a full restore, not just a filegroup. (I'm not positive on that but believe the below text points to that requirement.) Understanding How Restore and Recovery of Backups Work in SQL Server 

This should allow your login to be to that user in the database and allow you to access the database. If you want to a login from production to your development box you can query to get the SID, or just use procedure from KB918992. 

I would suggest automating generation of the restore script. There are a few versions of this out there but Paul Brewer has a T-SQL and PowerShell version available. I have seen some setups as well that just include generating a restore script when the backup is accomplished. Which, just a note that a new open source backup solution was released by Sean and Jen McCown called Minion Backup. It includes the functionality of generating your restore scripts and migrating backups to development. 

With (1) you should be good giving them and . On (2) I would probably create a role and then grant that role execute permissions on all the stored procedures, in place of granting it to each individual user. Then just add those users to the role. If you have all the stored procedures under a schema name you should be able to grant execute to that schema. That will save time and not have to worry about new procedures being added to the database. 

As the error indicates the login does not have permissions on the schema. This would require you to specifically grant to the login, or I believe you could also just for the database. Update Sorry just caught that last sentence. I have not worked with that software but seems it might be using a different login to execute the query if you can execute the same query fine in SSMS. On a side note, since you are in SQL 2005, it would be a better practice to start using the new views for looking at catalog information. 

Getting specific to SQL Server would require there be any cmdlets within SQLPS that actually use the ByValue or ByPropertyName binding. I only know of a few like those for the Backup or Restore, but don't really use them. I think the ones around Azure SQL might use them as well (e.g. ). So in the end this is really nothing to do with SQL Server itself, just the way PowerShell works...but still worth learning about I think. The best way for you to see how they work is just use to look at the binding metadata as PowerShell does it. A simple example would be piping something to a cmdlet like or . I am by no means an internal's guy when it comes to PowerShell. Up front from what I can find, is not actually used or maybe just has a lower precedence than . If you look at use of this command on my local laptop: