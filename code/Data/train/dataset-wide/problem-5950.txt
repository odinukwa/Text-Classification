I don't think that statement 3 is particularly coherent definition of God, since in all religions that I'm aware of, God(s) interact with the world and thus it is false that he/she/it "shares no properties with the natural world". Even Deists tend to base their belief on feelings of awe -- again an interaction with the divine. Maybe someone out there really does believe in a transcendent that shares no properties with the natural world, but that such a person exists seems non-obvious. 

Though far from ironclad, this is the outline of way to reason towards attributing moral agency to others. The application amongst humans is pretty straightforward, and done on a day to day basis -- we generally assume moral responsibility as a default, but regularly relieve people of their moral responsibility in the case of clear mental defect. The nub of applying this kind of approach to more esoteric problems involves drilling down into which aspects of us (or more solipsistically, me) are the features on which we are making the equivalence claim. But how you proceed from here will depend on how you answer other questions. Maybe only humans have souls, or consciousness or self-awareness (of the right sort) or whatever, and thus only humans are, or can be moral agents. Or maybe, you take a functionalist view of mind so that other sorts of things, like say dolphins that seem to torture and kill other animals for fun can be held morally culpable. You'd do so if you concluded they both dolphins and humans exhibit the key functional characteristics necessary to support moral agency. 

Such a thing a ethics exists within the sphere of international actions. If this seems like too much structure, my question is really about when is one nation justified in taking offense at, and thus taking some sort of response to, another nation's actions. (Note this can go the other way too: one nation takes what should be considered benign/standard actions and uses them as a (false) pretext for their retaliatory actions). I'm talking about elections, and assuming that they are to some reasonable degree free and fair. These require a fair bit of information exchange, which is why I think foreign communications/propaganda are a particularly salient class of actions to consider. I'm mostly thinking about state-level actions done by recognized government actors for political purposes even though in some cases the line between public (governmental) and private actors and actions might be blurred (the United Fruit Company provides a historical example beyond the situation of today) 

Probability of each outcome is what matters In act-utilitarianism the probability of different outcomes is taken into account such that the probability of the murder is taken into account and compared for the case where: 

I'm afraid not. You are correct in your assumption that an immovable object must be able to provide an irresistible force. Consider Newton's third law: For every action, there is an equal and opposite reaction So an immovable object must be able to provide enough force to counter the impact of any other force such that it results in an elastic collision. If the object can provide any force required in retaliation then it must also be an irresistible (or insurmountable) force. 

A large gravitational force is not the same as irresistible. Equally, having no objects outside to HAVE to resist your force doesn't mean it is irresistible either. 

Ethics is always a difficult one. However as far as utilitarianism goes then, under the assumption the product makes some job easier, by travelling in time Bob is providing this ease to more humans. He is, therefore, providing a greater good which supposedly surpasses that just of Alice. (Utilitarianism ignores the fact Bob does this for his own selfish means). We are, however, stuck in not knowing the effect of this playing with the past we are doing. Initially we could consider it as copying an idea from a hermit inventor and spreading what he makes to those who would never meet him. With the inclusion of time-travel this has been complicated in several different ways. 1) What effect does this have on Alice? Perhaps her first success sets her off on a journey of discovery which ends up saving millions of lives...or perhaps her first was a fluke and her next invention goes terribly wrong and explodes, demolishing the building she was working in. The point is we cannot predict this and so some extra element of risk is taken on by Bob. 2)Time-travel is rather a sticky subject, perhaps Bob would create a paradox which tears apart the universe (not very high up on the utilitarian to-do list). 3) If Bob is in a position to travel in time and he chooses to use this opportunity merely for personal gain he has chosen not to save many many others. He could warn Japan to evacuate Hiroshima and Nagasaki, he could warn the US of 9/11, he could do the clich√© and shoot Hitler. He could take vaccines back in time and stop the Black Death before it started. The point is that with a time-machine the utilitarian greatest good for the greatest number includes all the many people of the past. Alice is no longer a consideration, only Bob's actions are. I think perhaps the addition of time-travel detracts from your morality of stealing ideas. 

Can a thought without a corresponding action be morally wrong? More fully, under which approaches to morality do thoughts, in and of themselves, carry moral significance? In particular I'm looking for approaches that yield affirmative answers to the initial question. In in some (naive) utilitarian approaches, I can see arguments in the negative: since there are no (significant -- maybe there is some very minor self-harm) negative outcomes from just the thought itself, they're not really bad. This is where my intuition lies. The only positive case I can think of is under theism: the thought can be an affront to God. But I have only vague notions on this so any elaboration on the idea of "thought sins" under specific religious dogma would be appreciated. 

The intuitive concept of particle is alive and well in experimental particle physics (and to a lesser extent particle phenomenology) . Check out the listings in the Particle Data Group Handbook tables upon tables of numbers that describe the discrete features of the countable objects that are studied at particle colliders. The operators at LHC know (approximately in practice, exactly in principle) the number of protons that are in their beam packets. When they make event reconstructions (below) they "fill in" the particles' trajectories. The point here is that for doing some aspects of modern high energy physics, the basic notion of "particles bouncing off one another" is still there, only modified to use QM in calculating (describing) what is entailed in "bouncing off". 

Radical skepticism is the term you are looking for. Descartes Meditation is an early work that starts from this position. 

Put antother way Proposition 1: for anything that exists in this world I can find/construct, even if just by assertion, a rigid designator that applies to it. Proposition 2: like proposition 1 but over all possible worlds. What are the status of these propositions? I think that in the context of reading Kripke, a definition of "anything" in this would be "any object/entity that we can discriminate/delineate via natural language". This is based on the position that if we can talk about it in a sensible way, then we could apply a proper name to it, and thus it would have a rigid designator. 

In terms of Bayesian hypothesis testing, Occam's razor is incorporated in the prior probabilities of the hypotheses. Often one can interpret differences in the prior probabilities as being "entropic". E.g. imagine two models: , , based on some parameter . If we take the maximum entropy (constant) distribution as the prior distribution for , then the a priori bias towards is due to the fact that there are "more states", i.e. higher entropy, for that model. This straightforward use of Bayesian hypothesis testing does not have the feature of balancing energetic against entropic considerations; or equivalently there isn't the analog of (variable) temperature. There is research on the statistical mechanics of Bayeisan networks, e.g. variational approaches for estimating the free energy in physical systems can be applied as approximate propagation algorithms (Yeddida is a relevant author). In the end, the thermodynamics is all done at , so that energy <=> log-likelihood. When you get to machine learning problems, you can start to see things that look like temperatures in the models; and thus you get more interesting analogies. I've seen papers (don't have reference handy) where during the initial phases of learning Bayesian networks, they used (effectively) a high temperature to prevent over fitting the conditional relationships while the structure was still uncertain, and then lowered the "temperature" until , and they were finding the maximum likelihood model for the data. There is what I consider a related application in reinforcement learning where a temperature-like parameter can be used to go between exploration (entropy driven) and exploitation (energy driven). In terms of the kind of hypothesis testing alluded to in the OP, I haven't seen research that has real thermodynamics (absence of evidence and all...), and I don't see what feature of those kinds of problems map onto the idea of a temperature. However, in various places in machine learning I have seen what look like relevant ideas; this is due to the fact that that these types of problems need to balance model fit (energy analog) against generalization (entropy analog). None of them exactly match the criteria set in the question, but hopefully provide some indications of where related ideas have popped up. 

The problem with your example of finding the cure for cancer is that it assumes a binary potential where either a human contributes to the cure or does nothing. This ignores a human's potential (and perhaps even guarantee) to cause harm during their life. To solve this you would need to calculate each person's net potential for harm based on the situation they were born into. An impossible feat and this discrimination would cause further suffering. Negative utilitarianism argues from the view that without humans you can have no cancer or suffering (of humans) and the potential happiness of humans who would live is not a factor. 

The equation you're looking for is: Force on an object = mass of the object * acceleration of the object The mass in question is therefore the mass of all the matter in the universe and not zero. 

This is a rather tricky question to answer, there is always a different view point. The 'present' is what you can experience Present is best described by experience (though this is also slightly tricky). You cannot experience the past or the future, only the present. Of course light, sound and other experiences take time to reach you so any event takes time to reach you. So once you experience it the event is in the past...right? General relativity agrees that you experience something which originates from an event in the past. However what I would argue you experience is the light of that distant star hitting your eyes. The moment it hits your eyes is the present, that is your experience. The idea of the star, your inferred information about that star etc are all secondary to the actual photons. The line of the present is thin*, yes, but it is where we live so we cannot discount it's existence or you wouldn't be able to distinguish past from future. *The thinness perhaps defined by your reaction time (like the shutter speed of a camera). 

This is, in fact, the case. All of the mass in the universe is inside the universe (by definition) however it doesn't necessarily mean it is immovable. (Though we can ask the question "move in respect to what?" ) 

The outcome is the same but the probability is not the same for each case so they are not judged as equally bad. Though of course you can also argue that the purposeful murder brings more pleasure since you fulfilled your intended action and, as such, would bring more happiness to you especially considering the action would haunt someone who did it by accident. It also depends on why you wished that person dead....and so on. In the end it is difficult to judge any action in general terms since, to determine the utility, you must know specifics of who will be happy and who will be hurt. 

There is a strict sense in which you can say that no computational process produces a (truly) new algorithm. I'd usually think of the process of one algorithm generating another as involving: 

Lying is generally considered unethical. Falsely entering into a contract is a form of lying. The Terms of Service for the Stack Exchange sites indicate that accepting the terms indicates that the subscriber is "an individual". The rest of the contract makes clear that the intention is that the subscriber is a (human) person. A bot would be unable to live up to this aspect of the terms of service. 

I believe that you've hit on what is typically referred to as the "systems reply", which is, in short, that the room system does understand Chinese. This seems plausibly true in terms of a functional definition of understanding, but getting to the point where one can conceive that the room system has a subjective experience of understanding is a much bigger conceptual chasm to jump. (though as far as I can tell, it cannot be ruled out, especially since I don't know to a high degree of certainty which of you dear readers have subjective experiences and which of you are zombies). 

Frequently, it's both. The bare judgment "race X has feature Y" is an assessment regarding the state of the world. For example put in a morally neutral term for Y like "has good fashion sense" or "gets fewer cavities". However, in many cases of race-generalization the feature has moral considerations; in those cases the racist is making a moral judgment about a set of people; this is a moral judgment in the same way as an individualized moral judgment, albeit generalized too far. A sense that I get from your question is that it also relates to the idea that a 2nd party can make a moral judgment about a 1st party's racism, and that this judgment of the racist's beliefs/behaviour is moral in nature. Under various moral theories you can see how this is the case even if the racial judgment is not overtly moral in nature -- basically it boils down to the idea that overgeneralizing in this way is indicative of a failure of judgment and thus poor decision making. Which in turn reflects on the racist's virtue (virtue ethics), or likely "goodness" of decisions (utilitarianism)... 

You've phrased the question as though actions with intent are either moral or immoral. A more nuanced view is that some actions with intent are more moral than others. In your first case where there is an incidental and unintentional moral benefit to what is otherwise an immoral act, is just that: the person acted immorally but there was a benefit elsewhere. The benefit doesn't erase the immoral action, which was undertaken with full malicious intent by the perpetrator. In your updated case, the fact that the perpetrator is able to manage his/her urges, and only indulges in them in, lets say, sanctioned conditions, is better than the former case (with or without incidental benefits) but is still far from optimal, e.g. in most moral theories it is better to stop the bomber without having to kill him/her.