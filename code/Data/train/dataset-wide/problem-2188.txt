When a detail table contains denormalized data, should denormalized columns be included in foreign key relationships between the master table and detail table? Here's more details: We have a master/detail pair of fact tables: an table with about 1M rows and an table with about 20M rows. To improve reporting performance for date-range queries we've partially denormalized by adding to the and creating a covering index on with the other columns INCLUDEd. There's already a foreign key relationship between the column in both tables. But SQL Server is unaware that the in both tables is the same if the is the same. Should I help SQL Server to know about the relationship? If so, how? Finally, will adding denormalized columns to foreign keys improve cardinality estimates when joining the master/detail pages by telling SQL Server that cardinality shouldn't be reduced when filtering both tables by the same a date range? If not, then what's the benefit of maintaining this foreign key relationship that includes the denormalized column? We're running SQL Server 2014 an are soon upgrading to SQL 2017, if that matters to the answer. 

I have two sql boxes, one primary and one mirror. The mirror stopped synchronising last night for about 12 hours and so now the primary is transferring about 25GB of logs that built up. However it's taking a long time. What happens if the primary falls over (for whatever reason) during this time? Presumably the mirror will take over as primary, but I don't know how much of the transaction logs have been transferred. 

Back in the days of yesteryear, it was considered a big no-no to do or because of the performance hit. Is this still the case in later versions of SQL Server (I'm using 2012, but I guess the question would apply to 2008 - 2014)? Edit: Since people seem to be slating me slightly here, I'm looking at this from a benchmark/academical point of view, not whether it's the "right" thing to do (which of course it's not) 

Locally attached disk that's split into two partitions E: and F: Software RAID 1 set E: composed of 2 locally attached disks (yes I know software RAID is bad-- adding this case to help me understand SQL Server's definition of "volume", not to design a production setup!) Hardware RAID 1 set E: composed of 2 locally-attached disks SAN disk E: on who knows/who cares how many disks. 1 SQL Server filegroup spread across two locally attached disks E: and F: 

So we're wondering if there's a lower-cost solution that would store as smallint but expose the colunms as ints to readers. Like this: 

But it doesn't say how SQL Server determines what is a "large table" and "small table" for purposes of this optimization. Are these criteria documented anywhere? Is it a simple threshold (e.g. "small table" must be under 10,000 rows), a percentage (e.g. "small table" must be <5% of rows in the "large table"), or some more complicated function? Also, is there a trace flag or query hint that forces use of this optimization for a particular join? Finally, does this optimization have a name that I can use for further Googling? I'm asking because I want this "use the cardinality of the large table" cardinality estimation behavior in a join of master/detail tables, but my "small table" (master) is 1M rows and my "big table" (detail) is 22M rows. So I'm trying to learn more about this optimization to see if I can adjust my queries to force use of it. 

I have some statements that I am running. Not all of them work (they are the result of running SQL Data Compare) and I want to group them in some transactions and roll back the statements if something goes wrong. Is this possible, or is it only data that can be rolled back? 

After all that one of the servers didn't have the SQL Server service running under the same user. I'm an idiot. 

I'm using SQL Server 2008R2. Last night, the host machine on which my mirror database was running basically went belly up. Thankfully the principle database was on a different host server and so was fine. However... The principle is working but says suspended. When i try to resume the mirroring, I get an error in the SQL error log giving an error number of 9004. A quick Google of this error number come back with this article. (tl;dr: transaction log is damaged) So, does this mean that the transaction log shipping between the principle and the mirror has somehow got screwed up? How do I fix this? Is it as simple as doing a full backup on the principle and a full transaction log, then restoring them both on the mirror database with norecovery switch on and then set up the mirroring again? Or will I need to do something more drastic? 

You need to distinguish between your system (Windows) user called "postgres" and the database user with the same name. Find your pg_hba.conf file - this controls access to the PostgreSQL server. You will need to edit it as a user with Administrator rights. Look for lines that mention user "postgres" and temporarily set the mode to "trust". Restart postgresql and then you should be able to connect without a password. Reset the password, then restore the file to its original settings and restart PostgreSQL. The manual has details on authentication methods and the pg_hba.conf file. 

You haven't provided any way of telling what the problem really is. One thing to check though are whether you have any long running transactions. They might be idle but still able to see old versions of the view rows. That would mean that each refresh would effectively add another copy of you view's materialized rows. See what is in the system view and pay close attention to the timestamps. 

A multi-billion-row fact table in our database has 10 measures stored as columns. The value ranges for some of these columns won't ever be above the +/-32K range of a . To save I/O, we're investigating whether it's practical to store these columns as instead of . But we're concerned about what problems might crop up from doing this, including: 

What are the pros and cons of this approach? What can go wrong? Is there a better way to reduce storage & I/O without causing problems with overflow and requiring existing readers to be rewritten? 

Our SQL 2014 server has a blazing-fast tempdb drive (2800 IOPS) but a much slower data drive (500 IOPS). Our application runs a few long-running reporting queries that are I/O-intensive and we'd like to avoid them starving our server for I/O capacity when they run. Ideally we'd be able to limit these queries to 50% of available I/O capacity. Unfortunately SQL Server Resource Pool's IOPS throttling is not percentage-baed nor volume-specific. If I limit to 250 IOPS, then it will unnecessarily slow down performance of queries that make heavy demands on tempdb. Slowing down these long-running queries if the server is busy is OK, but slowing them down by 10x+ if they need lots of tempdb access is not OK. So we're looking for workarounds that will defend other queries from these lower-priority, long-running queries, but without unnecessarily hurting performance of these long-running queries if they happen to use lots of tempdb. It's not practical to change the queries themselves to reduce tempDB usage-- these queries are generated by a custom reporting feature that may sometimes generate really complex query plans that spill results to tempdb. So far the best idea I have is to remove IOPS throttling and instead use the "Importance" of a workload group to defend the rest of the server's I/O capacity from these queries. Is this a good solution to the problem I'm trying to solve? What are the pros and cons of using Importance? Or is there a better way to achieve our goals? 

I have a an SQL server that normally listens on a private IP address (10.x.x.x). However, I would like to be able to connect to the database over the public (31.x.x.x). I tried to enable this tonight, but when I restarted the SQL Server service, the server wasn't listening on the internal IP address any more. I was using the configuration manager to let the SQL Server know the IP addresses it should be listening on. I am running SQL Server 2008R Standard. 

I have two sql boxes, one primary and one mirror. The mirror stopped synchronising last night for about 12 hours and so now the primary is transferring about 25GB of logs that built up. It's now synchronising again but I can't connect to the primary db server to check the status and to see how much there is left to transfer. Is it possible to do this through the mirror/witness? 

I assume the answers to #3 and #4 are "1 volume" and #5 is "2 volumes" but it's #1 and #2 that I'm most curious about. The specific reason I'm asking is wondering if it's possible to increase the Resource Governor's IOPS limit for locally-attached SSD tempdb while having a lower limit for our SAN data storage. So I'm wondering if splitting a single physical disk into multiple partitions might be a way to do this, by putting separate tempdb files on each partition so the total tempdb If #1 above makes SQL Server treat one physical disks as multiple volumes for throttling purposes, this may be an option. I'm assuming that this won't work-- that SQL Server is smart enough to know that 2 partitions is one "volume". But was worth asking. 

Will tempdb I/O from a single query be split across multiple tempdb files? (assuming that tempdb is configured to use multiple files, of course!) For non-tempdb databases, MDSN seems to say that yes, newly-added data will be spread across multiple files in a filegroup: