You have yourself quite an interesting project. I've never directly seen anyone try to implement something that large, at least on SQL Server. The more I read your post, the more questions I come up with... Worst case scenario infrastructure-wise (which is actually the best case scenario, business-wise), you need 10K databases times 2k users. That's 20,000,000 users. You aren't going to be successful in trying to manage 20 M SQL Server logins. IMO. Just the sheer number of them, dealing with moving them from server to server, watching out for ID collisions and mismatched IDs, plus I'm not sure how SQL Server would behave with 20 M rows in sys.server_principals. Additionally, your web app is probably going to want to connect as a single, or very low number of, users. IIS can't pool connections unless their DSN strings are identical. One of the attributes of a DSN string is the user name. Different users means no pooling. You will need to roll your own user credential scheme. It will have to be able figure out what tenant a user belongs to and then your web code will need to select the proper database. That user metadata is critical, it going to need to be stored somewhere, it's going to need to be clustered or mirrored, it's going to need to be fast and it's going to need to be well-protected (from a security perspective. IOW, encrypt it.). Assuming that SQL is even a good idea here, I would keep this database away from the instances that server tenants. This helps from a security standpoint and from a load standpoint, though I would guess that once a users is validated and the web app is steered to the correct database on another instance, there will not be any more querying of this user metadata related to that user. Quick question: should two different users, who belong to two different tenants, be allowed to have the same user name? Another quick question: If I tell you that I work for FuBar, Inc., how do you know that? Is FuBar going to give you a list of users and you give them back a list of user names, or are they going to self-provision? You are going to need to go multi-instance. If even a fraction of those users decide to hit the application at once, a single instance will melt. It won't have enough worker threads to run all of those requests at once. If only 1000 users hit your instance at the same time, it will probably run out of worker threads and request will start to stack up and wait. I've seen this happen; the proximate symptom is that new connections will not be able to log into the instance because there are no available worker threads to service them. If this is very short-lived behavior, you app might survive. If not, or your app is fussy, users will get errors. Even if you will not have many tenants to start, you should start thinking about the future and automation because when you see that your server is bogged down and there are 10 new tenants to bring online, it's pretty much too late and your service (and your clients, and your soon-to-be-ex-clients) will suffer until you write your way out of the problem. You are going to need a way to move databases around, from overloaded servers to lightly loaded (or new) servers. Whether or not you can get a window of downtime will depend on your SLA. Are you providing a specific application, like SalesForce, or are these databases just containers for whatever your tenants want to put in? How big are the databases? If they aren't very big, you could just restore from a backup file that provides a template. (This isn't much different than what the model database does, but I haven't seen anyone really use model in a good way since my days with SQL 6.5.) Once the template has been restored to the new database name, you could then customize the new database as necessary for a particular tenant. You can't do the customization before you have the tenant, obviously. If the database is big, you might follow the same basic procedure except you do the restore ahead of time, before any new tenant needs the space. You might keep a couple of these databases around, maybe one per instance. If you keep too many around, this will force you to maybe buy more hardware and/or storage than you need, plus you will have make allowances in your backup/reindex/checkdb maintenance window (or change your code to avoid databases that are 'deployed', but not 'in use'.). If this is your own app, how are you going to handle updates to the schemas? How are you going to keep versions of the database straight with versions of the code, if you are using a single URL that gets to your web app? How do you detect and destroy databases that aren't in use anymore? Do you wait until your A/R group says that someone hasn't paid their bill for three months? If tenants are managing permissions, that implies that they have some understanding of the inner workings of the app, or that your app has a very simple role structure. Using something like Blogger as a rough example, users can (read posts), (read posts and make comments), (... and create posts), (... and edit other's posts), (... and can reset other users passwords), or (... and whatever). Having a role for each of those different sets of rights and assigning a user to one role or another shouldn't be too hard, but you don't want your app running 'GRANT' statements. Watch out for roles that have a hierarchy and depend on inheritance, it can get confusing. If you are promoting or demoting a user, I'd say pull them out of all of the associated roles and then add them back to the one role that they need. Oh, and since you probably can't use SQL's native credential scheme, you are going to have roll your own code here. I think that I've only scratched the surface here, and this post is already too long. What you really need is a book, or at least a whitepaper from someone who has done this. Most of those guys won't be talking, if they view it as a competitive advantage. 

Because when you're creating a full backup you're creating a backup of the data (as it is physically and as it is materialized from the log). Restoring it somewhere else restores it as data, not log. 

Identify threads that are not unkillable (e.g. a thread that is rolling back a transaction is unkillable). Find the thread with the lowest deadlock priority. Chose the one that is cheapest to roll back, i.e. the one that has done the least work so far. 

I'm using Red Gate SQL Compare to create a release script based on differences between SVN and a database. This results in a script containing a bunch of table- and procedure-changes and it works fine. However, one thing puzzles me, it's using transaction isolation level serializable. I know what it does to dml-statements, but I'm not sure what it means for ddl. Can someone enlighten me, perhaps with an example? 

Both statements would be evaluated to , thereby we have found out the individual values. Note: In the real world, we would iterate through all values from 1, 2, 4, 8, 16 and so on to the maximum value of an integer (or whatever datatype the parameter was set to). 

The original log file might be truncated so the space can be reused, but it's size might stay the same. 

In SQL Server there is a separate thread that periodically (default 5 seconds, lower interval if a deadlock has just been detected) checks a list of waits for any cycles. I.e. it identifies the resource a thread is waiting for, then it finds the owner of that resource and recursively finds which resource that thread is in turn waiting for, thereby identifying threads that are waiting for each others resources. If a deadlock is found then a victim is chosen to be killed using this algorithm: 

I've verified that the server is set for remote connections inside the instance - properties - connections. I've verified that TCP\IP and Named Pipes are enabled under Configuration Manager - sql server network configuration - protocols for MSSQLSERVER I've verified that the browser service is running and that the sql server service is running. I've verified that the 1433 port is opened in the windows firewall. 

When using RESTORE FILELISTONLY to restore a database using code the logical_name for the files is coming back as 423. However when I do a manual restore of the database the logical file name appears to be 3497. I'm running SQL Server 2008 R2. Has anyone run into anything like this before? How do I repair this? 

The short answer would be when you have no set based way to get the information or have exhausted all other options and rejected them based on their performance being worse. The reason for avoiding scalar sub queries is that they force SQL Server to perform the scalar query once per every row that is returned. There is almost always ways to get the information back using a set operation that will perform and scale better. 

I have a sql server user that our application is connecting as. The user has been granted db_datareader and db_datawriter as well as Grant Execute on each stored procedure that it needs to use. These are the permissions I see when I veiw the usermapping on the Login and when I view the user under the database. Given these permissions I don't think the user should be able to alter a procedure; however, when my developer logs in as that user they are able to update the stored procedures. Am I wrong about what these permissions allow? Is there someplace I can look to find out about permissions that may not being showing up in the UI? Or do I need to explicitly deny alter on the stored procedures for that user? 

SQL Server's replication feature is intended to replicate data, not objects like tables. When configuring replication you may find that the slave table may not be a perfect copy of the master table; constraints and indexes may not be recreated on the slave side. If you need the same table at the slave side, I believe that you should just run the script to create a table and forget about replication. 

Assuming that your indexes are up to date, dropping from 100% cpu to something less, while having timeout problems, implies that the app is now either waiting for locks or for disk. The IO latch waits implicate the disk. I would use perfmon to look at the disks to see if there seems to be an unusually high amount of reads, look at the query plans for exact queries that are running slow and re-think what I did about the indexing. Another thought is, if I've made a change and things seem worse afterwards, the first thing I would do is undo that change. IOW, put the old indexes back. 

Note that running drivers in-process could affect the stability of the instance, if those drivers are buggy. After the drivers are installed, you will need to configure a valid linked server. You also need to be sure that SQL Server has permission to read (and maybe write) the files. 

There are third party products, such as SQL Lightspeed, that provide backup compression for Standard editions of SQL Server. There is a good chance that the purchase and installation of such a product will allow you to back up locally. The size of the nacho file depend on how much data is actually stored in the database and it's compress ability. 

The others are combinations of the documented flags, e.g. 24 is 16 and 8. This is a method for simulating optional parameters and is used in e.g. C iirc, the numbers and so on are structured like that because they correspond to binary values that combined in any unique way create a unique number. The function that accepts them then use bitmasking to extract each individual number. E.g. if you send the value 6 to a function, then we know that this is a combination of 4 and 2, to find this out using a bit mask we would do: 

Sometimes when inserting data in a table with many columns it could be useful to know which columns must be specified if the insert-statement shouldn't fail. I wrote this query to find out which columns are not nullable, identity, computed, timestamp and have no default value. 

and after that the call from the web server also completed in time just fine. I would suspect that the same plan should be used since I was using the exact same parameters from both connections, but I'm not sure. My question is: Can there be different plans or buffers for different connections? Or could it have been some other side effect caused by me running the above dbcc-commands? 

We had an issue in our dev environment where a procedure call timed out from the web server after 30 seconds. I traced the query and ran it manually (same params and all) from SSMS and it executed in about 2 seconds. I then ran 

Make sure to keep transactions as short as possible. E.g. don't show a login form after starting a transaction and wait for user input, instead gather all the info you need and then run the transaction. Use the lowest possible isolation level, e.g. don't set serializable when you just want to temporarily show some values to the user. Please note that setting correct isolation level is a science in itself and out of scope in this answer. If you are the victim of a deadlock, i.e. you get error# 1205, then re-run your transaction transparently to your user. Since the other, competing, transaction has now hopefully acquired the resources it was waiting for and finished, it is unlikey that you will encounter the same deadlock again.