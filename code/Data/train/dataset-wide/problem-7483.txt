There's also a different way of writing down the $H$-space structure, that I like for its algebro-geometric flavor. (I'll talk about $\mathbb{C}P^\infty$ here, and $\mathbb{R}P^\infty$ should be analogous.) Regarding $\mathbb{C}P^\infty$ as a classifying space for complex line bundles, we know that this $H$-space structure is supposed to implement "tensor product of line bundles". In a (not very explicit) sense this tells us the homotopy class of $\mathbb{C}P^\infty \times \mathbb{C}P^\infty \to \mathbb{C}P^\infty$: It represents the line bundle $\mathcal{O}(1,1) = p_1^* \mathcal{O}(1) \otimes p_2^* \mathcal{O}(1)$. We can use this description to write down a much more explicit (and classical) explicit representative. First, let's recall what the analogous picture looks like for finite projective spaces. The line bundle $\mathcal{O}(1,1)$ determines (upon picking generating sections) the Segre map $\mathbb{C}P^n \times \mathbb{C}P^m \to \mathbb{C}P^{nm+n+m}$ which takes (in homogeneous coordinates) $([X_0:\ldots:X_n] , [Y_0:\ldots:Y_m]) \mapsto[X_0 Y_0: \ldots : X_i Y_j: \ldots: X_n Y_m]$ where I'm choosing to be vague on the precise ordering of the coordinates. (In the end this won't matter up to homotopy, as the maps will become homotopic upon composing with $\mathbb{C}P^{nm+n+m} \hookrightarrow \mathbb{C}P^\infty$.) The analogous formula with infinitely many homogeneous coordinate makes just as much sense, one just has to a good ordering of pairs of non-negative integers. Such an infinite Segre map gives another realization of the $H$-space structure. 

If $X$ is smooth and proper, GAGA does in fact suffice (despite the observation that $d$ is not $\mathcal{O}_X$-linear: One obtains a comparison map of hypercohomology spectral sequences; it is an isomorphism on the $E_2$ page by GAGA, and thus on the $E_\infty$ page. It is to prove the general case (i.e., $X$ smooth but not necessarily proper) that one needs to do additional work. 

So basically I am asking, is it always best to evaluate the gradient at the point $x^n$ itself, or there can be transformations $h$ which might depend on the structure of the function $f$, that may help in faster convergence of the method? Can anybody refer me to any result in the literature that might have a link with this question? Any help is appreciated. Thanks in advance. 

You can try applying gradient ascent method and run it for a few iterations to get an initial guess for $\mathbf{t}$. As the function is strongly concave, the gradient ascent is sure to converge to its unique global maxima, and so running it for a few iterations will produce a good approximation for the true global maxima. Actually, by analyzing the gradient ascent steps, you can estimate how many iterations you need to run it for the approximation to meet a prespecified approximation error required for the Newton-Raphson/Halley method to succeed. See this for such a discussion. 

Let us consider a function $f\in C^2$, and convex such that the Hessian $H\le LI$. We consider following minimization problem: $$\min_{x\in \mathbb{R}^n }f(x)$$ Let us consider the following iterative process for estimating the minimizer, $$x^{k+1}=x^k-A \nabla f(x^k)$$ where the matrix $A$ can be singular. It is well known, that if $A=\mu I$, a suitable Lyapunov function to use for checking convergence, is the function $f$ itself, as in that setting we have $$f(x^{k+1})\le f(x^k)-(\mu-\mu^2L/2)\|\nabla f(x^k)\|_2^2 $$ which shows descent property for $0<\mu <2/L$. My question is, 

Just an idea. I think the best way to start is to expand on the structure of the matrices $D, C$. For example, $D$ can be readily seen to be expressible in the form $$\alpha M_1\otimes I_2=\alpha\begin{bmatrix} 0 & \mathbf{0}^T\\ \mathbf{0} & J \end{bmatrix}\otimes I_2$$ where $J=diag(1,2,,\cdots)$. I think the structure of matrix $A$ can be expressed as $\beta M_2\otimes B$ where $$M_2=\begin{bmatrix}0 & 1 & 0 & 0 & \cdots\\ 1 & 0 & \sqrt{2} & 0 & \cdots\\ 0 & \sqrt{2} & 0 & \sqrt{3} & \cdots\\ 0& 0 & \sqrt{3} & 0 & \cdots\\ \vdots & \vdots & \vdots & \vdots & \ddots\end{bmatrix},\\B=\begin{bmatrix} 0 & 1\\ -1 & 0\end{bmatrix}$$ Now the problem becomes finding $(\epsilon\beta M_2\otimes B+\alpha M_1\otimes I_2 )^{-1}.$ 

Restricting to Aff is certainly enough, but Aff isn't small (there are e.g., polynomial algebras on arbitrary sets). If your DM stack is finitely presented over $k$ (which is probably good to include in the definition, to avoid these issues), then it is determined by it's restriction to finitely-presented affines (which is essentially small). Without some finiteness hypothesis, no set of finitely-presented algebras can suffice (even for affine schemes, nevermind DM stacks). (And I suppose no small category of test objects can suffice: Take Spec of a field generated by a set of cardinality larger than that of global sections of any of your test schemes.) The set you give is insufficient even for smooth varieties over an alg. closed field: you will have a morphism whenever you have an (arbitrary) map on $k$-points. 

Now begins the speculative (and probably wrong) part of this answer: I have nothing too certain to say about writing this as a functor category, but it doesn't seem too unreasonable (to me, right now, at least) based on the above simplicial subdivision construction that we might be able to construct a reasonable candidate: some sort of mix of the cyclic category and the orbit categories for the cyclic groups. Purely combinatorially, this seems to get tricky. But, I think we can realize this geometrically: Let $(S^1)_r$ be the circle equipped with a $\mathbb{Z}$ action given by the rotation by $2\pi/r$. We could try to define $Hom'([m-1]_r, [m'-1]_{r'})$ along the lines of "(htpy classes of) degree $r'/r$, increasing $\mathbb{Z}$-equivariant maps $S^1 \to S^1$ sending the $mr$-torsion points to the $m' r'$-torsion points". This should correspond to taking all the $r$-cyclic categories and sticking them together, and in particular is bigger than what we want. But, the $\mathbb{Z}$-action on the circles should induce one on the $Hom'$-sets and the composition should respect it. Taking the quotient, we seem to get something that looks like a reasonable candidate. For each fixed $r$, we should be getting a copy of the cyclic category. And, e.g. $Hom([m-1]_r, [mr-1]_1)$ should contain $Hom_{orbit}(Z/mr, Z/r)$. (Disclaimer: It's late and I haven't checked any of this too carefully!) 

Any reference to literature and existing techniques will be highly appreciated. Thanks in advance. Edit: Another observation that I have made is that, if we have some $x^\star$ such that $\nabla f(x^\star)=0$, we can write the following $${x}^{n+1}-{x}^\star=({I-AG}({x}^\star,\ {x}^n))({x}^n-{x}^\star)$$ where $$G({x}^\star,\ {x}^n)=\int_0^1 \nabla^2f(x^\star+\tau(x^n-x^\star))d\tau$$ Then, analyzing convergence of the sequence $\{x^n\}$ is equivalent to finding suitable conditions on the minimum and maximum eigenvalues of $AG({x}^\star,\ {x}^n)$. Does the function $\|x-x^\star\|_2$ then qualify as a Lyapunov function? Even if it is true, I can not find an analog of $f(\cdot)$ which acts a Lyapunov function for the general case. I have read a few sections of the paper that @dohmatob referred in the comments, but I think I cannot find a Lyapunov function for this problem using the techniques introduced in that paper. The matrix $A$ is creating the problem. 

Is there a method to find a tight upper bound on the given integral? Note that the integral is upper bounded by $\sqrt{\pi/2}$, and thus converges. I first thought about applying Laplace's method. However, the function $-x^2/2-a(1-e^{-x})$ is decreasing and achieves maximum at $x=0$ which is an endpoint of the domain of integration. As a result, I don't think using Laplace method is a good idea to find an upper bound. Numerical evaluation indicates an asymptotic of $\sim \frac{1}{a}$, but I am not sure how to proceed to say anything about an upper bound in terms of $a$. Any ideas? Thanks in advance. 

The integral converges as it is easily seen to be upper bounded by $\sqrt{\pi/2}$. However, Laplace's method does not seem to work out as the maxima of the function $S(x) = -a\sqrt{1-e^{-x}}-x^2/2$ is located at the end point $0$. This question enquires about a similar problem, however, with the major difference that the $S(x)$ function there is given by $-a(1-e^{-x})-x^2/2$. The second answer to that problem suggests using a modified form of Laplace's method as given by V. Zorich, Mathematical Analysis II Chap. XIX, Par. 2.4, Theorem 1, to tackle the issue of maxima at an endpoint. However, for the problem at hand, this method breaks down as the function $S(x)$ is not differentiable at $0$. So, Laplace's method cannot be applied here. I tried using the transformation $1-e^{-x}\to x^2$ to obtain the integral $$\int_0^1 \frac{\exp(-a x-(\ln(1-x^2))^2/2)}{1-x^2}2xdx$$ From this, intuitively, it seems to me that, at least for large $a$, the integrand is concentrated highly around $0$, and there it seems to be approximated ``well'' by $2xe^{-ax}$, which produces a $\sim\frac{1}{a^2}$ trend. However, all this is very intuitive and I don't know how to transform this intuition into rigorous statements. Also, this intuition seems to serve well for getting asymptotics, but my true intention is to obtain tight upper bounds. As Laplace's method seems not to be a suitable choice, I do not have much idea about how to proceed to say anything about an upper bound. Please help. 

It sounds like you may want Exercise 9.7 in Hartshorne's "Residues and Duality". I paraphrase the statement: 

We can think of the splitting principle as a condition on a "cohomology theory" (of some sort) $E^*$, coming about when working with Chern classes for instance, and then ask: When does $E^*$ satisfy this condition? First, let's make the condition more precise and reformulate it: Condition 1: Given $X$ and a vector bundle $V$ on $X$, there exists $f: X' \to X$ such that $f^* V'$ has a filtration with subquotients line bundles, and $f^*: E^*(X) \to E^*(X')$ is injective. But there is a universal choice for $X'$, namely the flag variety of $V$: $p: Fl(V) \to X$. Any $f: X' \to X$ with $f^* V'$ filtered with line bundle subquotients will factor through $p$, and so we're really just asking if $p^*: E^*(X) \to E^*(Fl(V))$ is injective. Condition 1': For all $X$ and $V$, $p^*: E^*(X) \to E^*(Fl(V))$ is injective. At this point there are two ways this answer can go, depending on ones tastes: 

You can somewhat lift the algebraic closedness assumption: You have to allow an auxillary ring (actually, division algebra) to act equivariantly on both representation and tensor over it. Such a decomposition should hold whenever one of the groups has semi-simple representation category (the division rings in 1 are endomorphisms of simples). Then, the decomposition can be made canonical precisely up to choosing representative simple objects. If $V$ is a $G \times H$-rep, and $\rho$ are representative simples for $G$, then the natural map $$ \bigoplus_{\rho} \rho \otimes_{D_\rho} Hom_G(\rho, V) \to V $$ with $D_\rho = End_G(\rho)$ will be an isomorphism of $G \times H$-modules. (Conversely, given applying such a decomposition to $k[G]$ viewed as $G \times G$-module one would have to recover a representative set of simples.) For symmetric groups (in char. $0$), the endomorphism rings of simples are just the field (i.e., the simples remain irreducible over the alg. closure), so in particular you get such decompositions. Moreover, there are explicit representative simples that one can write down (the Specht modules). I don't know of the combinatorial theory to say if this gives any sort of satisfactory answer to your question 3. 

One observation: $$A = I+L,$$ where $L$ is a lower triangular matrix with $0$ in the diagonals. This matrix $L$ can be seen to satisfy $L^n=0$, and $L^j\ne 0,\ 1\le j\le n-1$. Thus, one can write $$A^{-1}=(I+L)^{-1}=\sum_{j=0}^{n-1}(-1)^jL^j$$ 

Let $\{x_n\}_{n\ge 0}$ be a sequence of reals such that $x_{n+1}=g(x_n)$, where $g:\mathbb{R}\to \mathbb{R}$ is a continuous function such that $0$ is a fixed point of $g$. My question is the following: 

Lets say, $A\in \mathbb{R}^{m\times n}$, and $D\in \mathbb{R}^{m\times m}$, where $D$ is a diagonal matrix with positive diagonal elements, and all the elements are $\le 1$. For simplicity, assume that $A^TA$ is positive definite. It is easy to see that $$\lambda_{\max}(A^TDA)\le \lambda_{\max}(A^TA)\cdot \max_{i}D_{ii},\\\lambda_{\min}(A^TDA)\ge \lambda_{\min}(A^TA)\cdot \min_{i}D_{ii}$$ However, are these bounds generally tight? For example, if $D$ is such that one of its elements is $1$, and all others are equal to some small number $\epsilon$, then does the lower bound on the least eigenvalue produce a sever underestimation of the lowest eigenvalue of $A^TDA$. Specifically, 

Let $f:\mathbb{R}^n\to\mathbb{R}$ be a given function and let us consider the unconstrained problem, $$\min_{x\in\mathbb{R}^n}f(x)$$ The standard iterative method for this is the gradient descent technique, where one iteratively generates the following sequence of points, $$x^{n+1}=x^n-\mu_ng(x^n)$$ where $\mu_n,n\ge 0$ is a sequence of positive stepsizes, and $g(x)=\nabla f(x)$ is the gradient operator. My question is the following: 

Let $\{X_n\}$ be an ergodic sequence of random variables, $X_n:(\Omega,\mathcal{F})\to (S,\mathcal{S})$ where the target set $S$ is a matrix ring. My question is, 

I have a sequence $\{x_n\}_{n\ge 0}$ with $x_0>0$, controlled by the difference inequality: $$x_{n+1}\le ax_n^2+b$$ where, $a,b>0$. Had $b$ been $0$ and $a<1$, I would find $x_n\to 0$ as $n\to \infty$. However, the presence of $b$ makes finding closed form next to impossible, except maybe for some specialized values of $b$. But I am not interested in closed forms, I am interested only in the necessary conditions on $a, b$ for the convergence or divergence of the sequence, or at least finding an upper bound for $\lim_{n\to \infty}x_n$, if the limit exists. It seems that if $a,b<1$, the sequence becomes bounded, and an upper bound is possible (although not sure if a closed form of the upper bound exists), and that if $a>1$, the sequence might diverge to infinity, at least the right hand side of the inequality seems to do so; but what about the following cases: 1) $a<1,b>1$, 2) $a=1,b>0$ Please direct me to references. I think probably the literature of nonlinear dynamics would be helpful in answering questions like this, but it would be really helpful to get pointers for specific topics in that field that might help answering this question. Thanks in advance. 

The rule of thumb is this: Your DM (or Artin) stack will be a sheaf in the fppf/fpqc topology if the condition imposed on its diagonal is fppf/fpqc local on the target ("satisfies descent"). In other words, in condition 2 you asked that the diagonal be a relative scheme/relative algebraic space perhaps with some extra properties. If there if fppf descent for morphisms of this type (e.g., "relative algebraic space", "relative monomorphism of schemes"), you'll have something satisfying fppf descent. If there is fpqc descent for morphisms of this type (e.g., "relative quasi-affine scheme"), then you'll have something satisfying fpqc descent. See for instance LMB (=Laumon, Moret-Bailly. Champs algebriques), Corollary 10.7. Alternatively: earlier this year I wrote up some notes (PDF link) that included an Appendix collecting in one place the equivalences of some standard definitions of stacks, including statements of the type above. 

$Fl(V)$ is a very geometric object over $X$, so we might as well ask that we actually have a formula for $E^*(Fl(V))$ in terms of $E^*(X)$. If $E^*$ is "reasonable" (i.e., has Chern classes giving rise to a "projective bundle formula") then iteratively applying the projective bundle formula will give such a thing, and in fact show that $E^*(X)$ is a direct summand of $E^*(Fl(V))$. (My favorite:) There's a nice way of strengthening Condition 1' that also holds in all reasonable cases, and that looks rather natural. You can ask that $Fl(V) \to X$ behave like a "covering", i.e. that (Condition 2:) $$ E^*(X) \to E^*(Fl(V)) \to E^*\left(Fl(V) \times_X Fl(V)\right) $$ is an equalizer diagram. (So not only is pullback injective, but you can identify its image...) (In fact, in reasonable cases it'll be a split equalizer diagram, related to the direct summand thing above.)