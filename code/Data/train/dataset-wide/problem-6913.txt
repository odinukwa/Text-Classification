First, it appears there are historical reasons: it was the initiative of the producers to standardize the way their goods were transported, already from the factory. And it involved transportation by rail initially, rather than ship. See interesting information here. Second, I see an economic rationale: overseas commerce is rather volatile, especially at individual-shipping-company level. In such a situation, it would be difficult for each individual shipping company to optimize the number of containers owned. But if there are companies that lease out containers to all the shipping industry, fluctuations dampen and overall capacity is more easily optimized. Another approach can be found here, where on the subject the authors write 

Let's see what inferiority of one good in the two-good case implies. Look up Silberberg's "The Structure of Economics" (still one of the best undergraduate microeconomics textbooks ever written), ch. 10 for more details. Utility maximization is described by (stars denote optimal levels) $$U_A(A^*,B^*) - \lambda^*p_A \equiv 0$$ $$U_B(A^*,B^*) - \lambda^*p_B \equiv 0$$ $$y- p_AA^* - p_BB^* \equiv 0$$ and note the use of the identity symbol instead of simple equality -these relations always hold at the optimum. Then we can differentiate both sides and maintain the identity. Do that and solve the $3 \times 3$ system of equations to determine the various derivatives, and you will find that if good $A$ is inferior, $\frac {\partial A^*}{\partial y} <0$, then we must have that $$p_AU^*_{BB}> p_BU^*_{AB}$$ If we are willing to accept $U_{BB} >0$, then the cross-partial $U_{AB}$ can be zero, and we can have a utility function as the one mentioned in @BKay 's answer. But if we want to maintain $U_{BB} <0$, then it must be the case that $U_{AB}$, the cross-partial derivative of the utility function must also be strictly negative (and so not-zero). This in turn implies preferences that are not separable, additively or multiplicatively. Perhaps you can consider something like $$U(A,B) =\ln\left[aA^k + bB^h\right]$$ and all four parameters positive. For example, for values, $a=5, k=0.4, b=0.2, h=0.8$ the indifference map is 

The approach is not correct, because $\mu^{BGP}$ is a result of underlying structural parameters. So to say "when $\mu^{BGP}$ increases..." immediately begs the question why it increases, which underlying parameter(s) has changed to cause such an increase... assume it was $\epsilon$ that decreased. But in this case $g_k$ is not affected at all, so you see that you cannot claim what you claim about $\mu^{BGP}$ affecting positively $g_k$. What is meaningful to do is to perform comparative statics for the exogenous parameters, and record whether $\mu^{BGP}$ and $g_k$ move in the same or the opposite direction (or don't move) in each case. It may not be a causal link, but co-movement is also a useful result, and testable. 

Both in Economics and in Accounting, there is the following fundamental principal: we have to subtract revenues generated in a given time period from costs incurred in the same time period (because this is what makes basic sense). Now, "cost" is the value of productive resources absorbed into production, in the given time period. The total value of an "Investment" is not absorbed in the usual time period where firms record their profits (month, quarter, year). When we buy a machine, we use it for many years. So only a part of "Investment" is absorbed in a period (essentially what constitutes an "investment" and what an "(operating) expense" is based on this criterion exactly -whether it is fully absorbed in production in one time period or not). The total value of Investment done in say one year is an outflow in the Cash-Flow statement, but it is not a Cost in the Profit and Loss statement. And, as you mentioned, that part of the Investment that was absorbed in production during the period, is reflected in the calculated Depreciation, which is chraged as a cost to the Profit and Loss statement. 

It is my impression that the subject has seen its fair share of scholarly research already. Check the references in the wikipedia article for WWW-II (the article itself is not very good), and maybe the book The Economics of World War II: Six Great Powers in International Comparison. 

It is my impression that the correct expression for "Dixit-Stiglitz" preferences is $$U=\left(\int_{0}^{1}c(\omega)^{\rho}d\omega\right)^{\frac{1}{\rho}}$$ which then can be seen as a continuous incarnation (in [0,1]) of, say $$\left (\sum_{i=1}^na_i\omega_i^{\rho}\right)^{\frac{1}{\rho}}$$ with $c(\omega_i) = a_i^{1/\rho}\omega_i$. In other words, a definite (Riemann) integral is indeed conceived as a sum of infinitesimally small rectangles, but it can also be seen as the continuous incarnation of a sum. A formal link between an integral and a sum is provided by the Euler-MacLaurin formula. 

"Normal" in this context means "trend" (usually a simple linear trend, but in principle could be a non-linear trend by using for example the Hodrick-Prescott filter device). Decompose output $Y_t = T_t+Y_{c,t}$, where $T_t$ is the trend component and $Y_{c,t}$ is the cyclical component (note that the cyclical component can be positive or negative). Then $$OMH_t = \frac {Y_t}{L_t} = \frac {T_t+Y_{c,t}}{L_t}$$ while $$NOMH_t = \frac {T_t}{L_t}$$ Then the ratio of actual to normal is $$\frac{OMH_t}{NOMH_t} = 1+ \frac{Y_{c,t}}{T_t}$$ This gives a "mark-up" (or "mark-down" if $Y_{c,t}<0$), on "normal costs" and it is in this sense that it "represents the cyclical component of costs". Namely, this ratio is the gross mark-up or mark-down we need in order to go from "normal" costs to actual costs. 

The concept of "marginal utility" (and therefore of decreasing such) has meaning only in the context of cardinal utility. Assume we have an ordinal utility index $u()$, on a single good, and three quantities of this good, $q_1<q_2<q_3$, with $q_2-q_1 = q_3-q_2$. Preferences are well behaved and satisfy the benchmark regularity conditions, so $$u(q_1)< u(q_2) < u(q_3)$$ This is ordinal utility. Only the ranking is meaningful, not the distances. So the distances $u(q_2) - u(q_1)$ and $u(q_3) - u(q_2)$ have no behavioral/economic interpretation. If they don't, neither do the ratios $$\frac {u(q_2) - u(q_1)}{q_2-q_1},\;\; \frac {u(q_3) - u(q_2)}{q_3-q_2}$$ But the limits of these ratios as the denominator goes to zero would be the definition of the derivative of the function $u()$. So the derivative is devoid of economic/behavioral interpretation, and so comparing two instances of the derivative function would not produce any meaningful content. Of course this does not mean that the derivatives of $u()$ do not exist as mathematical concepts. They can exist, if $u()$ satisfies the conditions needed for differentiability. So one can ask the purely mathematical question "under which condition the function representing ordinal utility has strictly negative second derivative" (or negative definite Hessian for the multivariate case), trying not to interpret it as "decreasing marginal utility" with economic/behavioral content, but as just a mathematical property that may play some role in the model he examines. In such a case, we know that: 1) If preferences are convex, the utility index is a quasi-concave function 2) If preferences are strictly convex, the utility index is strictly quasi-concave But quasi-concavity is a different kind of property than concavity: quasi-concavity is an "ordinal" property in the sense that it is preserved under an increasing transformation of the function. On the other hand, concavity is a "cardinal" property, in the sense that it won't necessarily be preserved under an increasing transformation. Consider what this implies: assume that we find a characterization of preferences such that they can be represented by a utility index which is concave as a function. Then we can find and implement some increasing transformation of this utility index, that will eliminate the concavity property. 

The model is $$y_t = \beta +u_t,\;\; u_t\sim N(0, \sigma^2),\; t=1,...,n$$ and the sample is independent. The estimator is $$\hat \beta = \frac1n\sum_{t=1}^n y_t = \frac1n\sum_{t=1}^n (\beta +u_t) = \beta + \frac1n\sum_{t=1}^n u_t$$ If $\beta = \beta_1$ (where $\beta_1$ is some value different than the $\beta_0$ we set as the null hypothesis), then the authors set $\frac1n\sum_{t=1}^n u_t \equiv \hat \gamma$ and so they write that under the alternative we have $$\hat \beta = \beta_1 +\hat \gamma$$ Since the $u'$ are i.i.d normal their sum/average is also normal. Then the distribution of the estimator is $$\hat \beta \sim N(\beta, \sigma^2/n)$$ Assume we set as our null hypothesis that $H_0:\beta = \beta_0$. and the alternative $H_1:\beta \neq \beta_0$. Then we form the statistic (which is a function of the estimator, not the estimator itself) $$z = \frac{\hat \beta - \beta_0}{\sigma/ \sqrt{n}}$$ The distribution of this statistic is (before specifying any hypothesis to test) $$z \sim N\left(\frac{\beta - \beta_0}{\sigma /\sqrt{n}},1\right)$$ Assume that we pose as our null hypothesis that $H_0: \beta = \beta_0$. Then if the null hypothesis is true we get $$z|_{H_0} \sim N\left(0,1\right)$$ If the alternative is true then we substitute for $\hat \beta$ to get $$z|_{H_1} = \frac{\beta_1 +\hat \gamma - \beta_0}{\sigma /\sqrt{n}} = \frac{\beta_1 - \beta_0}{\sigma \sqrt{n}}+\frac{\hat \gamma }{\sigma /\sqrt{n}}$$ The first term is a constant, the second term is a standard normal r.v. (remember what $\hat \gamma$ stands for). So the distribution of the statistic under the alternative is $$z|_{H_1} \sim N\left(\frac{\beta_1 - \beta_0}{\sigma /\sqrt{n}},1\right)$$ and the authors write $$\lambda \equiv \frac{\beta_1 - \beta_0}{\sigma /\sqrt{n}}$$ 

Under the stated assumptions, we have that (using $N_t = Y_t$ and $L_t = L$) $$u_t - u_{t-1} = \frac {L-Y_t}{L} - \frac {L-Y_{t-1}}{L} = \frac {Y_{t-1} - Y_t}{L}$$ The "growth rate of output" is defined as $$g_{yt} \equiv \frac{Y_t-Y_{t-1}}{Y_{t-1}}$$ So we can manipulate the first equation as $$u_t - u_{t-1} = -\frac {Y_t-Y_{t-1}}{L} \frac{Y_{t-1}}{Y_{t-1}} = -g_{yt}\cdot (1-u_{t-1})$$ $$\implies u_t - u_{t-1} = -g_{yt} + g_{yt}\cdot u_{t-1}$$ So we do have an approximation. Such are fine for theoretical expositions (educational or not), but whether they should be acceptable in real-world studies depends on the actual magnitudes of the two rates involved. For example with a growth rate of $3\%$ and a previous-period unemployment rate of $5\%$ we would get an approximation error of $0.0015$. The true drop in unemployment would be over-estimated ($3$ percentage points instead of $2.85$ percentage points), which appears acceptable. 

Now really, it provides only applications. I do not mean empirical-econometric applications, but presentation of dozens of theoretical models from very different sub-fields and their workings under the Recursive Approach. Mind you, the models themselves are summarily presented, with the implicit understanding that the reader already is familiar with, or will go and study the original papers first. Moreover, a lot of theoretical/mathematical underlying knowledge is taken for granted. 

Why? Such an assertion doesn't follow from anywhere. The subtleties lie elsewhere. The problem with the Lagrangian considered by the OP $$\mathcal{L} = u(c_1) + \mathbb{E}[u(c_2)] + \lambda(c_2 - f(c_1,Z))$$ is that it also is a random variable since now $Z$ appears outside the expected value (The direct substitution approach followed by maximization with respect to $c_1$ only, does not create any such issues). Now, do we / can we maximize a random variable? Well, no, because the essential characteristic of a random variable is that it is a function whose value cannot be set by command and control. But one could say "ok, let's pretend that this Lagrangian is not a random variable, and just write down the conditions for maximization, even though we know that we can't force the solution". But this won't work: if one attempts to do it one will eventually obtain $$u'(c_1) + \mathbb{E}\left[u'(f(c_1,Z))\right] \cdot \frac{\partial f(c_1,Z)}{\partial c_1} = 0 $$ which is not the same as the condition obtained through direct substitution, because here the partial derivative is outside the expected value. (for those who may think "hey, then how do we apply maximization procedures in the maximum likelihood approach" the answer is that there, nothing is random anymore when we get to apply the maximization steps).