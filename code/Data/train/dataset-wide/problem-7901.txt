I think Probability and Measure by Patrick Billingsley is what you're looking for. It contains all of the topics you're asking about (except maybe spectral theory), and I feel it does a good job introducing the world of probability to the reader without assuming you already understand it, which Durrett's book tends to do. 

It's well known that information about the spectrum of $P$ can give information about convergence of the chain to $\mu$ (namely via the spectral gap and Markov mixing times), but what about the structure of $\mu$ itself? For example, if I know that $P$ can be written as a small $\epsilon$ perturbation of another operator, i.e., $P = P(\epsilon) = P_0 + \epsilon P_1$ with invariant measure $\mu(\epsilon)$, and I know that for all $\epsilon$ small enough, I know something about the spectrum of $P(\epsilon)$ (like perhaps that its essential spectrum is bounded away from $1$), can I determine that, for example, the function $\epsilon \mapsto \int g \; \mathrm d \mu(\epsilon)$ is analytic near $0$, for all bounded measurable $g$? 

I've been reading Kato's book on Perturbation Theory, but there is no mention of measures. I also realize that you may need quite a bit more knowledge of $P$ to say anything conclusive, perhaps that it is self-adjoint on some Hilbert Space, etc... Just mention whatever assumptions are necessary when answering. I would also appreciate any references that may be helpful. This is a vague, open-ended question, so perhaps it should be a community wiki. Please comment if you think it should be and I can change it. 

The Computation Below is incorrect, but is left up as a learning tool. Please read the comments for details. Let $X_i$ be i.i.d. uniform r.v.'s on $[-1,1]$. If I compute $P_n$ for $n=3$, I don't get your value. Am I doing something wrong in my computation? I get the following: \begin{align} P_3 & = \mathbb{P}(X_1 > 0, \; X_2+X_1 > 0, \; X_3+X_2+X_1 > 0 ) \newline & = \frac{1}{8}\int_{-1}^1 \int_{-1}^1 \int_{-1}^1\mathbf{1}{\{x>0,\;y>-x,\;z>-(x+y)\}} dz\;dy\;dx \newline &= \frac{1}{8}\int_{0}^1 \int_{-x}^1 \int_{-(x+y)}^1 dz\;dy\;dx \newline &= \frac{1}{8}\int_{0}^1 \int_{-x}^1 (1+x+y) dy\;dx \newline & = \frac{1}{8}\int_{0}^1 (\frac{3}{2}+2x+\frac{x^2}{2}) dx \newline & = \frac{1}{3}. \end{align} This disagrees slightly with your formula, which gives $P_3 = \frac{5}{16}$. Let me know if I have misunderstood the problem or made an error, and I will remove this immediately. Otherwise, it seems like a possible answer to your question is to generalize this computation by an inductive argument. 

Notes: $\bullet$ One can consider A or B since both conditions are equivalent. $\bullet$ $p_0$ and $p_1$ are densities of $P_0$ and $P_1$ and the same goes to $q_0$ and $q_1$ with $Q_0$ and $Q_1$. What I know: From Huber's paper (pages 260-261) Theorem 6.1 I know that if the distance is the $f$-divergence, i.e. $D_f$, then A and B are correct. Additionally, if A and B are correct, then $Q_0$ and $Q_1$ minimize $D_f$ (iff condition). Huber considers $$Q_{jt}=(1-t)Q_{0t}+t Q_{1t}\\q_{jt}=(1-t)q_{0t}+t q_{1t}$$ and finds the first and second derivatives of $D_f(Q_{0t},Q_{1t})$. He then shows that the second derivative is $\geq 0$ (convex) and hence $(Q_{00},Q_{10})$ minimizes $D_f$ if and only if the first derivative evaluated at $t=0$ is $\geq 0$ for all $(Q_{01},Q_{11})\in(\mathscr P _0\times\mathscr P _1)$. He shows that this is really the case, hence the claim is true. I think that this result can be strenghtened, i.e. if $(Q_0,Q_1)$ maximizes $D_u$ for all $u\in[0,1]$, then it should satisfy A or equivalently B. I dont know how to proceed. Addendum: It seems that the question eventually boils down to finding $(Q_0,Q_1)$ which maximizes $D_u$ for all $u\in[0,1]$ and fails to minimize $D_f$ for at least one $f$. This will be a counterexample to the claim (of course if there exists such a pair). 

Here $\max_{\theta\in(0,0.5)}\max_{C_\theta\in\mathcal{C}_\theta}$ corresponds to all such convex curves. One can just put a single $\max$. 

Here are some remarks: $1.$ It is known that the inequalities above hold for example if $$\mathscr P_0=\{P_0|\,|P_0(A)-F_0(A)|\leq\epsilon_0\,\,\forall A\in\mathscr A\}$$ $$\mathscr P_1=\{P_1|\,|P_1(A)-F_1(A)|\leq\epsilon_1\,\,\forall A\in\mathscr A\}$$ where $F_0$ and $F_1$ are some predefined reference measures. However, in this case all $P_0$ and $P_1$ are not necessarily absolutely continuous w.r.t. a common measure. $2.$ In the paper $\Omega$ is called as an infinite set. I wonder if there is something different when $\Omega$ is countable or uncountable. I am more interested in the case when it is uncountable, and for simplicity it can be chosen $\mathbb{R}$ or any interval of it. $3.$ If necessary one can consider the following sets: $$\mathscr P_0=\{P_0|D(P_0,F_0)\leq\epsilon_0\}\quad and\quad \mathscr P_1=\{P_1|D(P_1,F_1)\leq \epsilon_1\}$$ where $$D(P,F)=\int_\Omega p\log(p/f)\mathrm d\mu$$ and here every $P$ is absolutely continuous w.r.t. $F$, and $\epsilon_0$ and $\epsilon_1$ are some positive numbers such that $\mathscr P_0\cap \mathscr P_1=\emptyset$. 

The case of irreducible, cocommutative Hopf algebras, over a field with $char(k)> 0$, is discussed in Sweedler's textbook on Hopf algebras, Ch.$XIII$, sect. $13.2$. (See prop. $13.2.2$, $13.2.3$). For the case of cocommutative Hopf algebras, over an algebraically closed field $k$, with $char(k)\geq 3$ you can have a look at: Modules of solvable infinitesimal groups and the structure of representation-finite cocommutative Hopf algebras, R.Farnsteiner, D.Voigt, Math. Proc. Cambridge Philos. Soc., v.127, p.441-459, 1999 and the references therein (among them, there is also an interesting older paper by the same authors discussing the case of cocommutative hopf algebras of finite representation type, over an algebraically closed field of $char(k)>0$). 

The answer is yes, if we are talking about finite dimensional, Hopf algebras over a field: $\bullet$ $H$ being cosemisimple (as a coalgebra) is equivalent to the dual hopf algebra $H^*$ being semisimple (as an algebra). But a semisimple Hopf algebra is also separable (see Ex.5.2.12). Now, $H^*$ being separable is equivalent (see th.6.1.2) to $H^*\otimes (H^*)^{op}$ being semisimple thus any (right) $H^*\otimes (H^*)^{op}$-module is semisimple. Since an $H^*\otimes (H^*)^{op}$-module is essentially the same thing as an $H^*$-bimodule (in the sense that the corresponding categories are isomorphic), we conclude that any $H^*$-bimodule is semisimple. This means that the category ${}_{H^*}\mathcal{M}_{H^*}$ of $H^*$-bimodules is semisimple. Now, to answer your question, recall (see th.2.3.3) that the category $Rat({}_{H^*}\mathcal{M}_{H^*})$ of rational $H^*$-bimodules is isomorphic to the category ${}^{H}\mathcal{M}^H$ of $H$-bicomodules and that for a finite dimensional Hopf algebra $H$, all $H^*$-modules are rational, i.e. $$ {}^{H}\mathcal{M}^H\cong Rat({}_{H^*}\mathcal{M}_{H^*})={}_{H^*}\mathcal{M}_{H^*} $$ thus, the category of $H$-bicomodules ${}^{H}\mathcal{M}^H$ is isomorphic to the (semisimple) category ${}_{H^*}\mathcal{M}_{H^*}$ of $H^*$-bimodules. (In general $Rat({}_{H^*}\mathcal{M}_{H^*})$ is a full subcategory of ${}_{H^*}\mathcal{M}_{H^*}$). $\bullet$ For the converse, the above argument can easily run all the way back, recalling that any separable algebra is -by definition- semisimple. Thus, the above shows that, for a finite dimensional hopf algebra $H$: 

$\mu$ can be the Lebesgue measure, although I believe the same holds for the counting measure and the discrete sets. The set $\Omega$ can be $\mathbb{R}$ or an interval of real numbers. I am especially interested in the last ''not'' case, and for this case if necessary $g_U$ and $f_U$ can be assumed to be integrable over $\Omega$. I had previously asked this question at math.stackexchange but with no answers. Addendum: For $f_U=\infty$ and $g_U=\infty$ I know that $$\frac{g}{f}(y)=\begin{cases}c_1\quad\mbox{if}\quad g/f<c_1\\ h(y)\quad\mbox{if}\quad c_1\leq g/f \leq c_2\\ c_2\quad\mbox{if}\quad g/f>c_2\\\end{cases}$$ where $c_1$ and $c_2$ are some constants and $h$ is a function of the bounding functions for example $g_L/f_L$. I think the same is true for the case $f_L=0$ and $g_L=0$. I have some work on the solution of the problem with KKT multipliers, which may help solving this problem too. 

There are two sets defined: $$\mathcal{S}_0^*=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1^k(U_k=u_k)\leq \prod_{k=1}^n P_0^k(U_k=u_k)\right\}$$ $$\mathcal{S}_1^*=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1^k(U_k=u_k)> \prod_{k=1}^n P_0^k(U_k=u_k)\right\}$$ and the corresponding objective function with $(\mathbf{p},\mathbf{q})=((p_1,q_1),(p_2,q_2),\ldots,(p_n,q_n))$ is: $$R^*(n,(\mathbf{p},\mathbf{q}))=\frac{1}{2}\left(\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0^*}\prod_{k=1}^n P_1^k(U_k=u_k)+\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1^*}\prod_{k=1}^n P_0^k(U_k=u_k)\right)$$ The success probability of the random variable $U_1$ for case $1$, i.e. $(p,1-q)$ and the success probabilities of the random variables $U_1,\ldots,U_n$, i.e. $(p_1,1-q_1),\ldots,(p_n,1-q_n)$ for case $2$ are defined on a continuous convex curve $C_\theta$, which passes through all the points $(p,q)$ as well as $(0,1), (\theta,\theta), (1,0)$. An example of $C_\theta$ is given below with a blue curve and $\theta=0.3$. The set of all such curves are denoted by $\mathcal{C}_\theta$ and any element of $\mathcal{C}_\theta$, i.e. any continuous convex curve $C_{\theta=0.3}$ must be in the orange area given in the figure, due to convexity. Notice that $C_\theta$ is a convex curve of the points $(p,q)$, whereas the success probabilities are $p$ and $1-q$, each with $1/2$ probability as defined above. 

Now, my question is whether there is some criterion (necessary, sufficient or both) regarding to when an algebra is admissible (or non-admissible) in the sense of the above definition. In other words: 

See also this, which is a more modern work, in the form of a book, discussing (non-commutative) geometrical ideas related to supersymmetry although from a more -imo- phenomenological viewpoint (emphasizing the supersymmetric standard model, supersymmetry breaking, elementary particle interactions , etc). Edit: After giving some further thinking on your question and since you are asking for some method 

$\bullet$ Using the above proposition (for $q=-1$), we can conclude that the fermionic algebra or the algebra of anticommutation relations, generated as an algebra by the fermion creation-annihilation operators $f_i^+, f_i^-$ for $i=1,2,...$ modulo the following relations $$ \{f_i^-,f_j^+\}=\delta_{ij}, \ \ \{f_i^-,f_j^-\}=0, \ \ \{f_i^+,f_j^+\}=0 $$ (where $\{x,y\}=xy+yx$) for all $i,j=1,2,...$ is another example of non-admissible algebra i.e. of an algebra which does not admit a Hopf algebra structure. This might be of particular interest to mathematical physicists since this algebra describes the spin-$\frac{1}{2}$ particles. (This last example is due to me and is not contained in the paper cited above. So if there is any disagreement ... put the blame on me!) 

According to nLab, such an action is called a Hopf action and your data specify a left $B$-module algebra. Such a structure is also referred to in the literature as an algebra in the category (of left $B$-modules). Note also, that, if instead of the bialgebra $B$ we consider a hopf algebra $H$ acting on $A$ and satisfying your condition supplemented by: $$ h\triangleright 1_A=\varepsilon(h)1_A $$ where $\varepsilon:C\rightarrow k$ is the counity map of $H$, then under such an action, $A$ is called a $H$-module algebra or a Hopf-module algebra or an algebra in the category (of left $H$-modules). (You can also check section 3.1 of this article, for a review on this and other similar kinds of actions and coactions on algebras and coalgebras). Edit: If we drop the demand for the bilinear map $\triangleright: H \times A \to A$ to be an action, i.e., if we drop $(h_1h_2) \triangleright a = h_1\triangleright(h_2 \triangleright a)$ and simply require $h \triangleright(ac) = (h_{1} \triangleright a)(h_{2}\triangleright c)$ and $h\triangleright 1_A=\varepsilon(h)1_A$, then we say that the bilinear map $\triangleright$ is a measuring or that $H$ measures $A$. If, furthermore: $1_H\triangleright a=a$ for all $a\in A$ then we are speaking about a weak action of $H$ on $A$ (see def.1.1, p.674 of the linked article). The same terminology is used for the bialgebra case as well. 

There are two sets defined: $$\mathcal{S}_0=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1(U_k=u_k)\leq \prod_{k=1}^n P_0(U_k=u_k)\right\}$$ $$\mathcal{S}_1=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1(U_k=u_k)> \prod_{k=1}^n P_0(U_k=u_k)\right\}$$ and the corresponding objective function: $$R(n,(p,q))=\frac{1}{2}\left(\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0} \prod_{k=1}^n P_1(U_k=u_k)+\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1} \prod_{k=1}^n P_0(U_k=u_k)\right)$$ $2.$ The random variables $U_1,U_2,\ldots,U_n$ are not necessarily identically distributed; 

Let $\mathscr P _0$ and $\mathscr P _1$ be two non-overlapping sets of probability distributions defined on $(\Omega,\mathcal{A})$. Consider the distance defined as $$D_u(P_0,P_1)=\int_\Omega \left(\frac{p_1}{p_0}\right)^u p_0 \mathrm{d}\mu<\infty.$$ Two distributions are chosen from each set $Q_0\in\mathscr P _0$ and $Q_1\in\mathscr P _1$ such that $$D_u(Q_0,Q_1)\geq D_u(P_0,P_1)\quad \forall (P_0,P_1)\in \mathscr P _0\times \mathscr P _1,\forall u\in[0,1]$$ 

My Own Work: For case $1$, I am able to simplify the problem considerably as follows: Assume we are given a certain $\{u_1,\ldots,u_n\}$. It will have $r$ times $1$s and $n-r$ times $0$s. The condition to assign it to either $\mathcal{S}_0$ or $\mathcal{S}_1$ is $$\prod_{k=1}^n \frac{P_1(U_k=u_k)}{P_0(U_k=u_k)}\lessgtr 1\Longrightarrow \sum_{k=1}^n \log\frac{P_1(U_k=u_k)}{P_0(U_k=u_k)}\lessgtr 0$$ Since $U_k$ are identically distributed above given condition can be written as $$r\log\frac{P_1(U_k=1)}{P_0(U_k=1)}+(n-r)\log\frac{P_1(U_k=0)}{P_0(U_k=0)}\lessgtr 0$$ which can be rewritten as $$n\log\frac{P_1(U_k=0)}{P_0(U_k=0)}+r\left(\log\frac{P_1(U_k=1)}{P_0(U_k=1)}-\log\frac{P_1(U_k=0)}{P_0(U_k=0)}\right)\lessgtr 0$$ Hence, we have $$r\lessgtr \frac{n\log\frac{P_1(U_k=0)}{P_0(U_k=0)}}{\log\frac{P_1(U_k=0)}{P_0(U_k=0)}-\log\frac{P_1(U_k=1)}{P_0(U_k=1)}}\Longrightarrow r\lessgtr t=\frac{n\log\frac{q}{1-p}}{\log\frac{pq}{(1-p)(1-q)}}$$ Here we have $r=\sum_{k=1}^n U_k\sim \operatorname{Binomial}(n)$. Hence, $$\mathcal{S}_0=\{\{u_1,\ldots,u_n\}:r\leq t\}\\ \mathcal{S}_1=\{\{u_1,\ldots,u_n\}:r> t\}$$ and as a result of above $$\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0}\prod_{k=1}^n P_1(U_k=u_k)=P[r\leq t\mid U_1\sim\operatorname{Bernoulli}(1-q)]=\sum_{k=0}^t\binom{n}{k}(1-q)^k q^{n-k}\\ \sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1}\prod_{k=1}^n P_0(U_k=u_k)=P[r>t\mid U_1\sim\operatorname{Bernoulli}(p)]=1-\sum_{k=0}^t\binom{n}{k}p^k (1-p)^{n-k}$$ Consequently, $$R(n,(p,q))=\frac{1}{2}+\frac{1}{2} \sum_{k=0}^t\binom{n}{k} \left[(1-q)^k q^{n-k}+p^k(1-p)^{n-k}\right]$$ where $$t=\frac{n\log\frac{q}{1-p}}{\log\frac{pq}{(1-p)(1-q)}}$$ as found above. For case $2$, the things are getting complicated because each $U_k$ is Bernoulli with different success probabilities. What I have is the following: $$\{u_1,\ldots,u_k=0,\ldots,u_n\}\in\mathcal{S}_1^*\Longrightarrow \{u_1,\ldots,u_k=1,\ldots,u_n\}\in\mathcal{S}_1^*$$ Similarly, $$\{u_1,\ldots,u_k=1,\ldots,u_n\}\in\mathcal{S}_0^*\Longrightarrow \{u_1,\ldots,u_k=0,\ldots,u_n\}\in\mathcal{S}_0^*$$ This is because of the convexity of $C_\theta$, i.e. $$\frac{P_1(U_k=1)}{P_0(U_k=1)}=\frac{1-q}{p}\geq \frac{P_1(U_k=0)}{P_0(U_k=0)}=\frac{q}{1-p}$$ as $$(1-p)(1-q)\geq pq\Longrightarrow 1-p-q\geq 0$$ is true due to convexity of $C_\theta$. This says that one can populate the sets $\mathcal{S}_0^*$ and $\mathcal{S}_1^*$ with (much) less than $2^n$ computations. I can also write $R^*(n,(\mathbf{p},\mathbf{q}))$ in terms of $p_k$s and $q_k$s as follows: $$R^*(n,(\mathbf{p},\mathbf{q}))=\frac{1}{2}\left(\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0^*}\prod_{u_k=0}q_k \prod_{u_k=1}(1-q_k)+\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1^*}\prod_{u_k=0}(1-p_k)\prod_{u_k=1}p_k \right)$$ Accordingly,