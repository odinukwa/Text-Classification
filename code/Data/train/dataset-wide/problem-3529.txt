There are many variations on how to accomplish this. I prefer to have one application dedicated to deciding on which server handles what kind of content, while the back end servers simply serve the files they've been requested. To that end, I employ the Varnish reverse proxy on the front end, listening on port 80. Behind that, I have Apache (port 8880) and nginx (port 8881), both configured for the same domains and pointing to the same directory structure. In my Varnish config file, I have something like this: 

Bacula. It's open source, and cross-platform. Excellent for a home/small office scenario. Can back up to tape or online disk storage. 

In the interest of avoiding auto-parking of your queries, I recommend that you do a simple nslookup of www.name.tld and then go to your respected registrar when you're ready to purchase it and verify that it is free. Can't help much in the way of tools, though. I usually look them up by hand, and if I'm looking for a prefix or substring, I'll whip up a little shell script to search the name space for me. 

It depends somewhat on your environment, intended user base, and client platform. First thing that comes to my mind is using an openvpn setup of some sort. Takes a bit of configuring, but it works well once set up properly. 

I don't believe that there is an official bacula mechanism for doing what you want. This is what I did this for a customer with a small office (5 PCs, a server, and 2 servers on the internet). First, I ran the nightly backup jobs, which backed up to the server's disk. Next, I ran a script that would restore to another location on disk (we have plenty of space) and then tar and feather to tape. The reason I did this, as opposed to sweimann's good suggestion of simply backing up the backup files to tape, was that I wanted maximum portability of the final tapes. You can pop a tar'ed tape into any machine (even windows, w/ the right software) and restore the files. The online backups are the true primary, and they go back for six months. The tapes, which were always limited to the most recent full restore, are mainly for CYA disaster recovery in case the office burned down (they were meant to be taken off site). No, it doesn't scale terribly well, and it takes much longer than a direct backup, but it works for some scenarios. 

I assume you mean binaries? If the number of executables is very large, this command may fail due to the command line for "file" and "echo" being too long. A quick example: 

Someone already mentioned "rsync". If you can mount the 2nd file system on the 1st machine, you can try running "diff -r /localfs /remotefs" and see the differences. You could also try something like tripwire or AIDE to snapshot one tree and compare it to the other. Depending on the size of the data set in question, you might consider using git or some other efficient version control program to take periodic "snapshots" (automatic, unattended adds and commits) for tracking changes. You can even sync specific changes from one machine to the other using this method if you set it up correctly. For deduplication, the "fdupes" program works well. 

It's not pretty, but it'll work as a last resort: Install "photorec" which may be included in the "testdisk" package on your distribution (both can be found here). Type and follow the prompts (choose "None -- non partitioned media" and "other" for the filesytem option). It should be able to recover many of the files, though you'll have to identify and rename them all by hand. At least it beats losing everything. (For reference, I tar'ed up my /etc dir and tried this. Even on an un-damaged tar file, it only got 225 of 337 files, and even some of those were pretty munged up.) An even more brute force approach would be to run to recover text data from the file, which would then need to be sorted through and put back into separate files by hand. BTW... a google on "recover corrupt tar file" yielded several tools specific to your problem. 

I believe that true Bourne shell (/bin/sh on many commercial Unix variants) does not support the assignment and export in a single statement. This is how I remember it from my days on Solaris. I don't know if the new Solaris versions use bash by default now. Regardless, your latter case is more portable so I'd stick with that. 

I have a power/watt meter handy, so I thought I'd measure the family PC in various modes. It's an old 1.8 GHz AMD Sempron eMachine. This is just the box, and not the monitor or any other peripherals. 

If hiding the script's source code is the goal, you may want to look at shc. Note, I've never actually used this until today. It works, at least for my simple one line script /bin/sh test script. Otherwise, you can try using gpg to encrypt the script and give each user the password. Basically, you take your finished shell script, then encrypt it to armored ASCII format: 

For a specific inbox, this is easy enough to do with procmail. Here's a link to get you started. The problem is if you want to do this to all incoming emails. That, too, could be done, but it's a little more involved due to trying to determine the correct public key for each recipient. If you want to encrypt with the same key for all messages, then it gets easier. However, as someone else said, if you want to do wholesale encryption, I'd resort to partition-level encryption, which is an entirely different (and much more involved) discussion . More specific requirements and your desired goal would be helpful to provide you with an answer. 

Enter your IP address in the "Multi-RBL Check" box here: $URL$ If you get any hits, that may well be your problem. 

Like all device that don't use a hard, physical switch (TVs, DVRs, etc.), you will get some power draw on the order of 1-to-5 watts per device. The costs of these "ghost loads" over a year depends entirely on how many you have and what your kW-hr billing rate is. Being a power miser, I use a power strip on the family PC, attach everything to it, and then flip the main switch off when not in use. EDIT: I did the math for this machine while "off". My rate is 8-cents per kWh, so: 

You may want to take a look at Bacula. Run the server on the Windows machine, and run the client piece on the Linux server. Not the most simple package to set up, but it's a full-featured backup system, and it will do incremental backups, compression, and even encryption. You can tune it to the amount of space you have and then have it expire older file sets. 

If you're comfortable with taking out the battery, look into buying a small desulphator. I've read of people restoring "dead" batteries with these. It would be a decent investment, too, since you can use it to extend the life of any lead-acid battery you own (car, boat, motorcycle, etc.). These aren't snake oil, either. They're widely known in solar, and other home-power circles. Of course, the UPS could be damaged, as could the battery itself, which the desulphator wouldn't help. You say the UPS doesn't respond at all. Every UPS I've had at least functioned as a decent power filter even with a dead battery. Maybe yours is toast? 

I've been an admin for 20 years (15 years professionally), mostly Unix with a dash of Windows as required. From the beginning, I've tended to play the paranoid admin, mostly because it's practical and instructive, not because I believe hackers from the other side of the globe are targeting my servers. ;-) Security really is a de facto sysadmin requirement, one which can be practiced daily. You don't specify whether you want to wear the official badge of "Security Specialist" and do things such as pen testing, PCI compliance auditing, incident response (forensics, etc.) or you just want to be an admin with some heavy security creds to help widen your career options and defend high profile systems under your charge. Of the few peers I know in the "official" category, the CISSP cert was the first they tackled and they went on to land decent jobs because of it (of course, they had 10+ years of hand-on experience, like yourself, to back it up). There are tons of materials online, in addition to official training materials and courses, to asses your grasp of the material. While the concepts can be learned and applied on any platform, I personally recommend Unix, since you get such low-level access to everything, with the added benefit of being able to access that information easily via remote shell: watching live tcpdump sessions, syslog entries, web server logs, snort dumps, dumping live system memory, to a million other open source tools for peeking and poking at the innards of a running system. Due to Unix being an ideal platform for learning this kind of stuff, it easily follows that a great way to learn is by throwing yourself to the proverbial wolves. Get yourself an entry-level Linux or FreeBSD VPS, a true virtualized VPS (such as Xen) with all the "hardware" and admin access you'll need to simulate the real deal in a live, exposed internet environment. Set yourself up with a live, working system. Get a live SMTP server running, and watch the spam bots and scan for malware. Set up a web server and watch the script kiddies try SQL injection attacks in your web and DB logs. Watch your ssh logs for brute force attacks. Set up a common blog engine and have fun fighting off spam bots and attacks. Learn how to deploy various virtualization technologies to partition services from each other. Learn first-hand if ACLs, MAC, and system-level auditing are worth the extra work and hassle over standard system permissions. Subscribe to the security lists of the OS and software platform you choose. When you get an advisory in your inbox, read up on the attack until you understand how it works. Patch the affected systems, of course. Check your logs for any signs that such an attack was attempted and if one succeeded. Find a security blog or list that is to your liking and keep up with it daily or weekly (whichever applies), picking up the jargon and reading up on what you don't understand. Use tools to attack and audit your own systems, trying to break your own stuff. This gives you perspective from both sides of the attack. Keep up with cutting edge of the "black hat" mindset by reading papers and presentations from well-established conferences like DEFCON. The archives from the past ten years alone is a treasure trove of information, much still valid. Granted, I have no certifications, nor do I bill for "security specialist" services. I just make it part of my daily routine to keep up with this stuff to make myself a better admin. Whether or nor the certs are desired or required for your goals is better left to someone who has them. However, I believe that a heavy hands-on approach is the best way to learn this stuff, and I hope some of my suggestions provide some food for thought. 

For really critical and sensitive changes, I will typically have a text file with the actual commands I'll use with #comments explaining what's going on. That way, I can cut-n-paste them into a terminal quickly. 

The program BCWipe has tons of good features. It's not free, but has a demo version that should handle wiping raw devices. You could also investigate the built-in "cipher /w" option. It's part of XP, so I assume that it'll be in Windows Vista/7. It'll wipe files or free space, and not an entire device, so it's probably not what you need. And for my final, somewhat top-heavy, solution, you could install VMWare, VirtualBox, or some other Vm software and boot any small Linux LiveCD and do the classic UNIX device nuke maneuver: 

It's possible, depending on the print server, to create a custom filter to do this. This could be done in a standard UNIX lpd setup. The filter could scan the submitted file, look for hints that it's a PowerPoint document, and then pipe it through the likes of enscript or mpage and then pass it onto the printer. This, however, this is a nasty kludge that I'd never want to take on. I agree with others, in that per-page accounting an billing is the only reasonable solution to this problem. If people bear the true costs of printing, then they'll think twice before submitting such jobs. If you search "print server" and "by document type" you find some interesting products. For example, this product will police the printing of color documents by document type. So I assume that there are obscure off-the-shelf solutions, or you could contract such a vendor to provide the solution you desire. 

If I'm reading the numbers correctly (difficult w/o proper formatting), I'd say your system isn't running as well as it could. Could be a memory leak for an application or just having application requirements that are too heavy for your system's hardware. Run a top and sort by memory usage to see what's taking up so much RAM. If it's possible, I'd restart the service that's taking up all the memory and watch it closely over time to see how quickly it reverts to chewing up so much memory. Using a lot of swap isn't always a problem, so long as is doesn't thrash the disks. Use of swap can be a healthy use of resources or detrimental, depending on how the machine is performing. What's the load on the machine? 

No, it's just cosmetic. If you're not careful, many programs (postfix I think, apache, etc.) will use whatever "hostname" reports, which can cause problems. Otherwise, there's nothing magical about the hostname. 

(That, or simply boot the LiveCD directly.) Just remember that a simple "free space" wipe (several suggested programs here, including two of my own) may leave some recoverable data lying around. Google for a PDF titled "One Big File Is Not Enough" by Garfinkel and Malan for a really interesting 2006 paper on the topic. 

The fastest method would be to offline the VM and do a loopback mount of the disk image under linux and copy the files. That way you're avoiding the overhead of both the network and the VM execution. 

If you replace "echo" with "rm" in the above example, those files will be deleted. The "grep" command should prevent libraries from being deleted (you can test on /lib). (Note, I only have access to FreeBSD right now, so the output of linux's "file" command may differ, thus changing what you need to do.) This should give you an easy template for how to do it. You'll just need to be sure to know what to look for in the output given by "file" and grep accordingly. Be careful. You could totally bork your system if you run this as root on the wrong directory. 

As said by someone else, never run a web server under the root account. Fortunately, most web servers will drop privs to a non-root account immediately after binding to port 80. Another way to handle this is to run the web server program on a port over 1024 and use a front-end (such as varnish or nginx) to listen to port 80 and forward to the back-end web server.