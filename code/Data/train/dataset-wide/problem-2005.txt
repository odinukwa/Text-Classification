Yes! You can make a new table which has foreign keys to both and . But be careful either in your model or in your code that you don't associate patient-records from a different patient with the patient associated with the treatment... using natural keys is one way to solve this: 

In general, for many DBMSes to solve the function using an index, a descending order index on is needed: this index has the property that the MAX value of x is the first value in the index. PostgreSQL supports a "backwards index scan" which, as its name suggests, scans the index in reverse order, so that the MAX value is the first value it finds in an index in normal (ascending) order. Then to find the MAX(x) for a key value, eg will require a composite index: 

You'll have to check the execution plans to see if the optimizer is smart enough to push the where clause into the subquery for each table. 

If you can compact the SQL making these types of adjustments the SQL becomes a lot more readable, will fit on one page, and you'll be much more likely to see how to fix the functional parts. As for the functional parts, one red-flag that jumps out is that you are calling DISTINCT in scenarios where you are using a GROUP BY clause. Calling DISTINCT and GROUP BY in the same select is redundant, since GROUP BY will already ensure the results are distinct. I'm not sure if removing the DISTINCTs will speed up the query, but since they are redundant it is worth a shot. Also, the generic advice of "run your script in SSMS, turn on the actual execution plan and add any suggested missing indexes" is usually helpful in these situations, you could be missing an index, and if it is an important one you could get significant performance gains without much analysis. 

I think the answer to your question is "Yes". The Oracle optimizer understands reference partitioning. See the Oracle documentation at $URL$ Also see $URL$ 

No. Duplicate entries are basically ignored. The documentation of describes how the rules work pretty well: 

Oracle does not support this concept of partial indexes. This syntax is valid for PostgreSQL versions from 8.0 and higher but not for Oracle. Oracle does however support indexes on partitions. If you have a license for the partitioning option, you can create a partition for the values of isdefault = 'Y' and index only that partition. I'm not sure which versions of Oracle support this; it could possibly be only 12c and higher. 

Extend or adjust for data types as needed. Remove for each data type if you wish to allow null values. The only complexity is the constraint to enforce that the appropriate column is used. If values are mandatory, then the check constraint can be written as 

SSDs are great in general, but they are much better suited to read heavily applications, while tempdb is a mix of reads / writes and the nature of tempdb generates a consistent churn which is not good for an SSD. But it sounds like you've already gotten your tempdb off the SSD, so that step is already done. I've not tried swapping out a local SSD for a SAN drive, but I've been involved in a migration which did that process in reverse (swapped a SAN drive for a local SSD) We failed over to a virtualized server during the process (the main server was a physical server) but it was very straightforward. We might have been able to swap in the SSD and done the entire process in a 30 minute window and skipped the fail over altogether had we known. But the policy at my current work is that any downtime longer than 30 minutes needs to be a failover. So TLDR, it was very easy, but I wasn't the person responsible for doing the physical drive swap, only for failing over SQL Server, then doing the work to utilize the new SSD, which was reassigned the drive letter that we used for data originally. 

-- and I got to use the operator! Note that you have to add to the right as the overlaps operator considers time periods to be open on the right (which is fairly logical because a date is often considered to be a timestamp with time component of midnight). 

What I would do is maintain a SQL script for the indexes that you've added to the "virgin" database. Store this script in a version control system, as you'll be adding indexes to fix performance problems. I would employ a naming strategy to make it easy to identify which indexes are "yours" and not part of the database as delivered. I'm not sure what features MySQL has, but in Oracle for example you can store these indexes in a different tablespace making it very easy to identify them as "added". When your vendor supplies an upgrade, first remove your added indexes, then apply your vendor's upgrade and afterwards execute your script to add your indexes. Of course, you should also have a QA environment where you can test all of this before applying it to your production system. You'll need to do this to discover if any indexes will conflict with indexes that the vendor may create in a future version. 

No, you must create a table variable, insert the records into that table variable, then pass the table variable to the procedure. The way that people used to do what you are describing is with pipe delimited text strings. For example, passing '1|Tyler|Smith|2|Jack|Blade|3|Someother|Guy' and then having the stored procedure unpack it. But that methodology has completely fallen out of favor compared to using table valued parameters. 

Then you can check the value saved in the table against the value for the person changing the record. And no, this does not break database normalization. As a practice it is quite common to include metadata like AddTime, AddUserID, LastChangeTime, LastChangeUserID. In fact in some environments I have worked in, it was a requirement that all tables have these columns for audit-ability. 

You shouldn't have issues altering the columns to a lower size. You can only alter column types one at a time with ALTER TABLE statements anyway. While the total data file size will grow intermittently, it can't grow bigger than the original with corrected column types. So here are the steps I would take: 1) Ensure that there is a primary key, or at the very least a clustered index on the table. 2) Run the ALTER TABLE ALTER COLUMN statements one at a time 3) If the database runs out of space, run a DBCC SHRINKFILE on the appropriate file. 4) Continue with steps 2 and 3 until all columns are their new appropriate types 5) Rebuild all indexes on the database to remove the fragmentation caused by the previous steps. Very easy. I've also gone the route of creating a new table with correct definitions, copying the entirety of the existing table to the new one, dropping the original table and renaming the new one to the original. However to do this you'd need more space at least intermittently. You could try putting one of the SQL Server data files on a network drive in the interim, but I think just altering the existing columns as outlined above will be easier. 

So the second and any subsequent duplicates will never be considered. So duplicate entires can not be the cause of your error. 

relies on your session's NLS settings to convert between datatypes. You should use with an explicit number format model, something like: 

Test! Be sure to test this in your Test / Q&A environment! Generate representative transaction loads, but also much higher loads to see how it will perform in the expected and unexpected cases. Even at high transaction levels, you will have to weigh up the performance associated with updating and storing the running totals vs how often they are queried. 

You're interpreting the semantics of the delete statement incorrectly. When a clause is used, it doesn't mean that records will also be deleted from those tables. Instead, those tables are purely used to join to in order to determine which rows need to be deleted from . You basically have three choices: 

I would stay away from LOOP joins, or specifying any specific joins. The SQL Server query optimizer is very good at picking the best join method to use, and it will pick LOOP on its own if that is the right one. Also, if there is a missing non-clustered index that could improve the performance of the query, SQL Profiler will find it and suggest it to you. One thing that I noticed missing from your query is the use of common table expressions. I haven't had a pivot yet where I haven't needed to use common table expressions to get the data ready to pivot. I would rewrite your query using common table expressions (particularly the correlated subquery) and see if that improves performance. If it doesn't another thing you could consider doing is changing the order of columns in the clustered index on the tblRespostaINT to better support that query. Finally I believe that what you are doing in the inner select can be accomplished with a windowed function, and if it can that should be faster. 

Once we have these two values we can determine which of the two is closer. So, something like, untested: 

It makes a difference... SQL*Plus can use variables, but they're a SQL*Plus feature and you can just grab that block of code and pass it to an Oracle backend to execute (like you could with a Transact SQL block for example). You can choose between using bind variables in SQL*Plus as follows: $URL$ Or use SQL*Plus's own variables with DEFINE and COLUMN commands: $URL$ If you really need the code to be executable in other environments, you will probably have to go down the PL/SQL route. 

You don't have a relational table but you have junk (OK, this is a contentious opinion, but a relational model needs keys on all tables). queries on this table (in the absence of any other indexes) will become slower as more data is inserted into the table. 

? But beware that this will retain the first row for every distinct combination -- which might not be the right thing without an clause this. But since you don't order in your question I presume you either don't care or haven't thought that far yet. 

I can only speak definitively regarding SQL Server, and it appears this is not consistent across all database implementations. But in SQL Server, functions may not produce side effects. This is a hard and fast rule I've tried to circumvent a number of times with no success. If you are thinking about functions in the general sense, there are SQL modules that allow side effects (for example stored procedures) but user-defined functions do not. There is a saying "Clever Solutions Don't Scale" which is especially true with Microsoft products. I've seen a number of clever workarounds in early versions of SQL Server that became obsolete in later versions, because MS added them as true features. The ones that never became features, honestly, never became features because they fundamentally broke some aspect of T-SQL development. Side effects in functions is one of them. 

I'd have to run it through SSMS to know for sure, but that syntax looks off to me. While I've defined columns as primary keys inline (like you are doing) I've never attempted to define a clustered index that was not the primary key in that fashion. What you are doing simply isn't possible in the SQL Server version you are using. I think you should define the clustered index using a standard CREATE INDEX after the table has been created via the CREATE TABLE statement. 

You'll need to execute your SQL statement. Use for this. And because you're selecting a value you'll need to it, something like this (replacing with your own where condition: 

You don't have to write twice if you don't need to retrieve it; if you're only interested in the s having a then the following is perfectly valid: 

While I'm not sure if it's the easiest way, you can use an "after servererror on database" trigger to log all errors to a table. See $URL$ See also $URL$ The documentation for system event triggers is here: $URL$ 

Install the exact same version of the software on the other PC using the same installation options (same directory structure, etc). Shutdown the instance on the original PC. Copy all the files from the original PC to the same location on the other PC. Start the instance on the other PC. 

This is not a deadlock. One transaction will simply block -- waiting to acquire the lock. The other transaction will proceed. As soon as the other transaction is done -- either by or , the first transaction will proceed. A deadlock happens when a transaction has acquired a lock on object A, and attempts to acquire a lock on object B at the same time as another transaction has already acquired a lock on object B and is attempting to acquire a lock on object A. Both transactions will then block, waiting on each other. That's the definition of a deadlock: two transactions blocked waiting on a lock that the other has.