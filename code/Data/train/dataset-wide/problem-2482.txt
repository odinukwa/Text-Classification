The function can't handle values lesser than 2, and it shouldn't as it has no meaning. But instead of returning properly it ends in a long while loop. Instead of int you could use ulong because it signals the domain of the function. There is no need to return anything from the prfact() because the nums argument holds the result. You can optimize the algorithm in that the curr argument is unnecessary as shown below. 

The calls to CheckForDuplicate(...) seems to be on the same level of concern so maybe make a combined handler for that: 

There are a couple of simple errors: 1) In you replace the next line with the second part of the split instead of inserting it after the first part: 

Please notice the full qualified method name in the assembly directive and the parameter to in the same line. The penalty is that you can't display private and possible internal members. 

If their role is more advanced than that with methods and state, it IMO calls for at class hierarchy in some way like: 

I think your code is not that bad and for small segments and data sets it is all right. May be a better naming of the class Estacion would have turned you in the right analytical direction. I'm not that familiar with your language, but I think translates to or in English where a more appropriate name would have been or . The class is error prone in the following way: In the constructor you calculate but both and (and ) are public properties and may be changed in the process which will leave the object in an inconsistent state. In stead you should make the a read only Property as or . As for the algorithm it self I think Janos' analysis says it all. 

EDIT I admit that Husk has a point about especially in where the length reach as much as but tests on my computer shows only a pair of seconds in expense for n = 8. The algorithm exhausts the stack for n = 10 anyway... As an alternative you could use which seems to have the same performance as Husks version. 

Naming: is not an index, but the actual value in the valid sequence (from min to max). I would call it or or something like that. I don't like the name either, because the target is actually the sequence of numbers from to . would be better IMO. 

It's hard to comment on the thread safety because there is too little information about the environment the class is supposed to work in. Therefore my answer focus solely on the class construct. 

3) Have you considered using and let the item class implement interface to keep track of collection and item changes in a dynamically and event driven way? 

then it would not be found as a change. If you're going to use the class with lists of reference types you need to track the state of each item between calls to - which could be a hash or something like that. 

Design It's simplicity of usage is great. However, you have no way of enforcing that it will be used at all possible call sites (where appropriate).Another possibility that comes to mind would be to use the State pattern to provide access to the OpenGL functions in form of a member, with 2 states: One that records the OpenGL calls and one that simply forwards them. The call to then switches state from recording to forwarding, executing the recorded calls.Though also not foolproof, but which is harder to forget: "I have to use OpenGL calls in a lambda passed to this function." or "I have to use this member to get access to the OpenGL functions (as they aren't provided directly by default anyways)."? Example (rough outline, as I currently have no Qt install at hand): 

On the other hand, if the requirement is that the consumer has to run on its own thread, why fuse it with its own queue? If there is only one queue, you could run multiple consumers on it to share the workload if needed and/or producers by themselves don't have to decide which consumer to call! 

TL;DR: MSVC does some extra stuff and really doesn't like the call to . To elaborate: The functions in the original examples only takes 16 instruction (GCC) or 23 (+8 inside ) instructions (MSVC) respectively. GCC inlined everything it could, proved that and don't change (so they are basically constant), elided the branch and variable allocations, and just calls directly before exiting. MSVC, however, only inlines the destructor of into and the constructor of into . MSVC seems unable to inline the call to or to prove that and don't actually change values (so it has to create them on the stack and generate instructions/calls for them). But this can be improved! Changing the macro so it doesn't call suddenly allows MSVC to come to the same conclusions as GCC and remove the checks and thus the stack allocations. 

Boundary checks You never verify if is actually a valid position in the list. checks You never check if any call to returns a . Without one kind of check, your code will very likely try to dereference a if is greater than the number of elements in the list. Wrong condition sets to , and if unequal to , executes the branch, deleting a wrong node from the list (it's only the right node if ). You probably meant to write . Memory leak At the begin of the function, you allocate a new node. This node will never be deleted (it either crashes because of nullptr dereference, or another node pointer gets assigned to before gets deleted. Unnecessary nesting The check inside the loop isn't necessary: Just run the loop until ( will then be true after the loop ends). 

If were truly of type , it would be automatically converted by the compiler to , thus calling the correct overloaded operator(std::ostream&, int). However, since is of type , the compiler does not find that conversion (it stops looking after the conversion to ). Make sure to document this (sometimes surprising!) behavior appropriately! Thinking ahead... If you allow custom getters as well, you might find cases where both getter and setter only refer to another variable (other than ). It might be worth to consider making a special case for those. If I had to implement something like that, I would probably start with something like this: 

on the view (form) maybe should be an interface which HomeScreenPresenter implements. Further: Instead of the presenter reacts on events on the view the view should call "passive" methods on the presenter - but it's a matter of taste maybe. 

In the way you show that you create each Skill instance, you really have anything than the Description to distinguish them from each other. I read your Skill class as merely an advanced flag that defines a small set of properties. In the below I further anticipate, that each skill type doesn't change throughout the game. If that is correct, then I would make the Skill class as a "singleton" per Skill type (onehanded, twohanded etc.) and let a static Factory method create/return the right Skill according to a Skill Type enum: 

I think TheQuickBownFox's suggestion is rather elegant. With a little twist (the use of ) it is only necessary to call once per num: 

Writing something consecutively to the console is of very little value. OK, returns a list of primes, but you really can't use them until all primes are calculated in the domain of , and that's a lot. For instance, it is useless to write 

When testing the algorithm against other implementations it seems to work as expected and without a thorough knowledge about calendars it is hard to comment on the details of the algorithm as is. From a computational or mathematical point of view it is possible to simplify or make the formulas a little clearer by removing unnecessary parenthesis and just rely on operator precedence: 

I'm not sure if the objective was insertion sort or tail recusion? In any case the performance is rather bad for larger data sets. If insertion sort was the focus the following approach is maybe somewhat more straightforward: 

Note: If you try PrimesBySieveAndLinq.GetPrimes(int.MaxValue) it is very slow at the beginning, but speeds up after some 1000 of primes. 

An entire solution that uses a discriminated union type for factor/nonfactor and minimize the Mock derive could be as follows - not claiming it to be best practice or anything like that. 

Although your efforts show a good understanding of details of functional programming, I think you make it all too complicated. The overall task is to map a list of integers to either the integer itself or a text in respect to one or more predicates. That can be done like this: 

If you've showed the environment the function is used in, the review could be more useful, but I think your reasoning is right, except that Lazy is used to defer instantiation and you actually use the lazy-created object () the first time it is called for by returning DataCache[key].Value. Therefore Lazy is of little use in this context as its object is instantiated at the same time as the Lazy. IMO your DataCache would give more meaning if you up front filled it with all possible queries without actually using them and then call them as needed. Alternatively you could just cache the queries directly in the DataCache without Lazy. 

Correctness can be destructed while another thread is performing an operation on it, because the destructor or its caller don't have to acquire a lock to do so. Technically destroying a mutex while it is locked already is undefined behavior, but it gets worse if another thread happens to modify the now invalid object. Same reasoning for the move constructor: The object from which will be moved could still be modified concurrently by another thread, so locking would be required. Also, rule of 3/5 violation: There's a custom move constructor (and there will hopefully be a corrected destructor), so copy constructor, move assignment operator and copy assignment operator should also be provided (currently, they copy without locking neither nor the other object). Also, should be marked , otherwise a can't be accessed. Other nitpicks 

Absolutely. In it's current form, it does far too much stuff and (at least in my opinion) is not really flexible enough. Every time you try to add something (e.g. other card patterns, loading patterns from a file, change the card border, ...) the function will just get more and more bloated. Actually, I'd implement it in a class . This way, you can transfer state (e.g. card patterns) between calls in a nicely, encapsulated way. Also, I'd split the functionality into multiple member functions, so it's easier to reason about them. 

Note This could maybe be enhanced by strategic use of atomics. However, that would require more knowledge about the members and usages of than I could gather/guess from the code given. 

Design problems is a queue fused with a single consumer, so there will never be more than one consumer for the queue!. If the consumer is not required to run on its own thread, the implementation could be simplified to: 

is used in several places, both as a integral value and a pointer value. Try to use more explicit values (especially the more type-safe for pointer values), as currently every time is involved, I have to double check whether the value represented is a pointer or not. 

So, time to explore the scary depths of template metaprogramming (well, scary for me, anyways). This library basically provides 2 different lists, a list of types and a list of sizes. Both lists support: 

Contention There is only one , and every thread is permanently in contention for that same lock for every small partial operation. Thus, all the threads are permanently in contention over that lock, and at most one thread at a time is actually doing any work. A single threaded implementation should be faster - it has to do the same work, but doesn't have to fight over lock control. So, how can this be fixed? 1) Separate independent data per thread In the usage example, elements of are split between threads so that no two threads are accessing the same elements. So, accesses to elements of don't need a lock - if this convention is followed strictly. If this is possible, locks aren't needed for those parts! 2) Finer granularity locks Right now, taking the lock stops every thread from doing any work at all. Even with the improvement of option 1, only one thread at a time can access elements from . This can be improved by introducing locks at a finer granularity: