"Cartesian product" is not simple :) In your specific query, and based on the index you have, the time will highly depend on the distribution of the data. Try this query: I guess the result is not a small number. is not healthy. The index is very big. It affects both, reads and writes. 'Reads' because it has to be loaded into the memory when it is used, and 'Writes' because it has to be checked (and maybe updated) whenever a DML operation is executed. If this uniqueness is a must, I suggest adding an MD5 hash for all the fields, and have that field uniquely indexed. To get smaller result, avoiding repetition, you may add . This will avoid you having: 

The usage of "##" is to avoid confusion is cases where (articleId, Version) = ("32111", "2"), or ("3211", "12") 

So, your approach is good in my opinion, with little tweaks: First, when you create the temp table, or when you are generating the hash value, use function (You mentioned is but not sure you're using it in the query): , so you don't have to deal with 'fuzziness' [Assuming that you want the preciseness to be at day level] To get the records that exist in one table, and not the other: 

2- Create a new table, with the same structure, and preferably add the unique key in order not to fall into the same problem in the future: 

More about GRANT Creating users and granting permissions can be done in more one way. One of them is . If you instead to create a user using CREATE USER Syntax, and then run , you will notice that the user has . All that to say that you need to grant permissions after adding the user, regardless of how the user was added. 

The result would be one of the following values: , , or . More info. In the case of format, every changed row will add info to the bin log. If there are statements that affect a table of 1000000 rows, all these changes will be written to the binlog. For example, suppose you have a huge table, with 1M rows, and you run a statement like this: 

This statement will add 1M entries to the binlog when it is row based, while it adds only one entry to the binlog when it is statement based. Suggested solution: Enable the general query log around the time when this this issue usually happens, and analyze all the statements to see if there are many that does update operation on huge number of rows. 

I tried it and it is working as expected: $URL$ However, some clients deals with every one statement as a single transaction. Try to add all of your code to one transaction and execute it. I tested that in MySQL client (command line in Linux) and on SQL Fiddle (Link above) 

Create the result table, , without index, and when it is populated, add the desired index. Make sure you have an index on (name, type) in both tables. i.e. a composite index It is not a good idea to have primary key on all fields. This is especially true if you are using InnoDB engine. Instead, alter the table to add an integer auto increment field as primary key, and if needed, add a composite unique index on the fields you want to be unique. 

I think this is due to the partial index. i.e. the index has only 8 char of It estimated that the number of matching first 8 char will lead to a second read from the table to compare the full value from first table with the full value in the other table, so the optimizer decided to skip the index, and deal with the table directly. 

Get the last ID from the target table. call it Read all rows of IDs higher than from the source table insert them into the target table 

When there is a lot on insert and delete operations on a table, the table will be "fragmented", and the statistics that are used to decide the execution plan of a query will not be accurate. So YES, it is recommended to optimize the table from time to time (Not necessarily after each delete, but when a significant changes happen) hth 

Alter the table to add an MD5 field: Update this field: Add the unique index: Either create a trigger, or do it at application level, to update this new field after each insert/update operation. 

since is defined to be unique, then IS unique, so you only have to define it as , not . [This is more helpful while inserting though] Having lots of values is not recommended. It is not good for performance. If possible, add instead. However, this is highly dependent on your requirements. If there are lots of (parent_id, child_id) = (NULL, NULL), then create a new table with these two fields only, and link it to this table. 

This is (one to many) relationship. It is not affected by the fact that more than one city have the same name. Therefore, in cities table, the combination (city_name, province_id) is unique. city_name by itself is not unique. 

Is there a reason for that? Composite PK might have performance hit, specially if you are using InnoDB engine, and if you have many updates on the table (insert/update/delete) 

[In this answer, I assume using MD5 as hashing function] The answer is YES. Adding a "hash" field and querying it would run faster. Details: When indexing a field, although the average length is 20 char, each entry in the index will be saved in its full length, i.e. 255 char. Add to this that if you are using , the entry length would be 255*3 bytes (plus the PK length). When adding a hash field, make sure it has a fixed length (32 in case of MD5), and that the CHARSET is latin, i.e. 1 byte per char. In this case, the entry in the index will be 32 bytes (plus the PK length) If you want to guarantee the uniqueness of field, it is recommended to add a unique index on the hash field (as opposed to a regular index) 

Try to load file before. If this is slow, then the data in this file has higher cardinality of key fields. Try to disable the indexes (as this is MyIsam), and load the files in whichever order, then re-enable the keys. If this is faster, then the slowness is related to the updating time of index files after certain growth: 

In MySQL, or almost any other RDBMS, it is common to have an ID for the row (object), so you use it in other tables to make a link between the tables (Relations, or PK and FK if you prefer). This is part of the normalization process that is recommended for most types of DB design [there are exceptions though]. In MongoDB however, the general way or design is to denormalize. i.e. the document (row) is stored with all its related objects. To make it more clear: In MySQL you would have two tables: and . In cars you only store the ID of the type of this car. In Mongo DB, you would have one collection called s, and the document in this collection would have a sub-document that contains the type's info. (You still can have a separate collection for , but you don't store only in car's collection) HTH 

I think this is a many to many relation ship, and the suggested way for that is to have a table for the relation. i.e.: 

Try adding a specific field to order by. That field is the value of the month if it is greater than the value of current month, and add 12 if it is less (or equal), then order by that field: