Curran & Clark Parser by itself only generates CCG derivations with categories, not with semantics. Boxer works on the parses to generate semantics. OpenCCG when used with the broad-coverage grammar, MOLOKO can show semantics. But I haven't used this one yet, and can't read their output. 

I don't think it's helpful to think of algorithms as being "correct" or "incorrect". Is the grammar model used in the Stanford Parser "correct" because it can parse many sentences perfectly, or "incorrect" because it parses many sentences incorrectly? I think it's more helpful to think in terms of degree of suitability of each algorithm for a particular task. 

I agree (roughly) with the first part of the sentence; we are getting closer and closer to the upper bound of the expected per-word entropy. I disagree with the second part because there can't be an "exact way": entropy figures can be exact if were're dealing with a closed-world system, but when it comes to text, no matter how large the corpus, new text is usually bound to contain words never seen before. A sentence like "pass me the SATA cable" would never have been uttered a couple of decades ago, but are fairly common now. 

Update: The paper by Montemurro & Zanette (2011) answers your question a somewhat more directly. For English, they report an average entropy of 9.1 for shuffled text, and 5.7 for the original text. So, that shows you how much you gain by taking "grammar" (read "word order") into account. Now, I think if someone were to devise software that takes real grammar into account, including syntax, semantics, pragmatics, etc. we could conceivably achieve about 4 bits/word. As an aside, in all this, Finnish comes out an interesting language, with an entropy of 7.1 bits/word. 

After a quick scan of the paper A Common Parts-of-Speech Tagset Framework for Indian Languages, nothing stood out as particularly distinct from the POS of European languages. However, on running over a chapter from Antony's thesis on Kannada, I noticed two categories I have not seen in most European languages: echo words, and reduplication. Finnish and Hungarian seem to have these two categories of words. On second thought, English seems to have echo words too (teensy-weensy, itty-bitty, higgledy-piggledy, etc.) but I have not come across POS tag sets that recognize these as a separate category, probably because there are just a handful of such expressions in English. In contrast, in Indian languages, these POS categories are open classes. 

It might be constrained, but NLP parsers work well, just the same. A few popular ones you could play with: 

A few corrections: lojban, though a human language, is not a natural language; it is a conlang. AFAIK, there are no native speakers of lojban: that would require teaching lojban as one of the primary languages to a very young child. Lojban is syntactically unambiguous, and only mostly unambiguous semantically. If there were a lojban programming language, this should not matter because one would avoid writing semantically ambiguous forms (like metaphors). This question has come up on various forums for lojban, Prolog, Haskell, etc., The consensus on those forums seems to be that it is possible, but no one has done it yet. Some people (e.g. 1, e.g. 2) have attempted to implement such a thing, but AFAIK, for very limited domains. 

There is a paper that gives an overview of the additive, subtractive and multiplicative derivations of number-words as used in the three dialects of Yoruba. I'll quote a few things here, but the number system is far too complicated, and needs the paper I linked to to describe it. 

A long time ago, LanguageTool used to be usable with just OpenOffice. These days, they have a stand-alone version too. In contrast to NLTK, LanguageTool is made (and extended) with Java. I suppose you could use Jython. AbiWord also has a grammar checking tool that is built using Link Grammar. This was made with C. Both tools support many languages. 

The paper also compares the preferences of the older and newer generations of people when it comes to specific numbers. 

Bigrams, trigrams, N-grams are all models of language, just as grammar models used in parser are. As before, there is no point looking for a "precise" model because the real-world language is best handled with an open-world model. But my point here is that there are plenty of grammar models. And some of them could be quite usable for deriving entropy figures for text. Table 1 of Learning Accurate, Compact, and Interpretable Tree Annotation shows the different (specialized) classes of words that the algorithm has induced. NNP-14 are months, NNP-4 and NNP-5 are common terminal words of names of corporations, etc. So, an IN-5 is rather more likely to be followed by something that ends with NNP-4, and an IN-2 is more likely to be followed by NNP-14, and so on. After all, such grammar models contain the probabilities of the co-occurrences of words with these POS tags. 

Note that these screenplays are not in NLP-friendly formats. Let alone the effort of scraping PDF, the screenplay of newer movies are in a mix of Hindi and English. 

I agree with Alex Peattie: # of websites in a language does not always correlate with # of users of software in the language. Having said that, $URL$ might give you some ideas about what languages are most popular online. The data is from Google's researchers, and so, ought to be trustworthy. 

Your broader question is about semantic relations, but the example you gave was about a narrower, but still widely researched field, that of opinion mining. Since you're asking about an entire field, it would be hard for me to get into the details. I'll treat the question as a reference request, and recommend the book Opinion mining and sentiment analysis. Published in 2008, the book is somewhat dated now, but it's still a good enough starting point to explore the various techniques used. 

We created eztreesee so that rank beginners can try out sentences without having to install parsers, models, etc. on their own computers. The backend runs entirely off the Stanford Parser. And therefore, what you see there are Penn Treebank tags. The official documentation for PTB tags is this weird PostScript file; you might prefer this quickref. 

Speaking for myself, the piece did not make me uncomfortable in any way... I just found it very very amusing. 

Some particularly conservative Hindi speakers say "raam raam" (राम राम) instead of "hello", whether they are answering a telephone, or greeting someone in person. However, this phrase is becoming less and less common. 

I agree with P Elliott, of course, that FSMs are inadequate for modelling English grammar. However, given that we are talking about formal languages, "grammar" has a broader meaning; formal grammar. Regular languages are equivalent to finite state automata. And finite state machines have been employed for spelling and morphology checking, correction and analysis (see [Kartunnen, 1994], [Pirinen & Linden, 2010], etc.). So, there you have it -- the application of regular grammar in English language. 

If a system is using combinations of neighboring, overlapping, or encompassing exemplars, I don't think a CFG is powerful enough to accommodate it all. Quoting again from the same paper, 

The figure for entropy of any language will depend on the model we use for computing it. This is quite like how someone who speaks English well would see lesser entropy in English than someone who barely speaks the language. The model that Shannon used gave him a figure of 11 bits per word. Grignetti (1963) reported 9.83 bits per word. Some of the relatively modern techniques described in Chapter 6 of the classic Manning and Schütze textbook show entropy values of about 7.9 bits per word, when tested on Jane Austen's Persuasion. This example, being from a textbook that's over a decade old, is likely to have been superseded by better models. Given all this, if I had to guess, I'd estimate about 5 bits per word. 

It has been tried but people seem to have given up on it seeing that it does not help at all. I quote from the paper Europarl: A Parallel Corpus for Statistical Machine Translation 

Recognizing accents is a relatively easy task... for people. Identifying the native language of writers is an actively researched field of Computational Linguists. So active that the field, "Native Language Identification", has its own conferences now. I paste from their website: 

write the CFG rules in your own text editor, and paste them into the text window. The online demo 'loses' rules as soon as it runs them. remember that the grammar must be in Chomsky Normal Form, i.e. each non-terminal expands to at most two other symbols (in each rule). If you want more, you'll have to compose it yourself. 

I think your question was downvoted heavily because that's not how linguists approach languages and language change, as evidenced by the comments below your question. To someone who hasn't studied linguistics, I think it is a valid question, the problem is just that it's based on false assumptions. @sumelic has answered one aspect of the question, that of loanwords. I'll write about a different aspect, that of language registers. The Hindustani language has many dialects and two formal registers, shuddha hindi and zabān-i urdū-yi muʿallá. The prestige associated with these formal register is great enough to motivate news anchors to speak in a dialect that many of their viewers find hard to understand. 

See this WordNet entry for bus. The senses are already ranked according to how common they are. In addition to that, the first (optional) field, between parentheses, gives an indication of how frequently each word-sense was seen. Frequency of use is determined by the number of times a sense is tagged in the various semantic concordance texts. I don't know how this number corresponds with anything outside the WordNet corpus. However, just stripping away all the senses that don't have this number should help you achieve your goal.