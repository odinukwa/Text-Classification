But I'm getting repeated results. Is that query right? If I have two records pointing to the same record on tb_persona should the query return those two records? I've a test data if any can help me, just say me where to send and I will to solve this problem. Also it's possible to get only fields from default_tb_persona and not from both tables? 

Can I pass an array as value or I need to iterate for the values and execute the query several times? 

I've a dilemma in my application and I don't get how to solve it at DB level. The idea is that a user can be subscribed to one or more services but services are show depending on user type. This can sound tricky but it does not. So I have a form where I pick which type of user I'm registering but then from services table I need to show only the services allowed for that type of user. I made this table: 

I have 3 tables in 3 different databases which are exactly the same. I'm trying to sync from this 3 databases to a master database as shown in the picture below: 

Following Neil and user16484 suggestions also ypercube and jynus and good practices at DB modeling, I come with this model 

But I don't know how to properly do the FROM clause and/or how to order the JOINS and if will be better to use LEFT or just JOIN. Also I am not sure if I can get every media for each row or I need to separate that into two queries. Can any help me to build this queries to get the expected output? 

Which make me ask: what value I should set for parentheses when I am defining a type. (I am using BIGINT because I don't know if INT can hold as many numbers as a phone could have - perhaps I am wrong too). Which is the right way to create|design a column in MariaDB/MySQL databases? Anyway I would like to know your opinion, experience and of course I would like to get an answer Note: I am using MySQL Workbench latest edition for create the ER diagram. I am using also MariaDB 10.0.x 

To add more to what Raadee and Paul White (also confirm what eckes's comment already stated), TF 8017 is enabled by default in all SQL Server Express Edition versions since 2005. It's probably a way of throttling number of CPUs (sockets and/or cores) unsupported by SQL Server edition. Tested on: 

Yes, OS (Windows Server) and database (SQL Server) are separate entities. So the only question is compatibility between those two products. I'm guessing it was tested and should be ok, but at this moment SQL Server 2012 doesn't have Windows Server 2016 listed (and won't have, at least not till September): $URL$ Best thing You can do is download latest Technical Preview and test it with SQL Server 2012 in Your environment: $URL$ 

Team responsible for SMSS got separated and works with its' own development cycle. That means SMSS gets updated/released more often, and is independent from SQL Server releases. Licencing You've mentioned is probably another reason behind that (and a reasonable one, if You think about it), but what's the most important is that SMSS has got some much needed attention at last. 

You can find more on database storage in Oracle documentation: $URL$ $URL$ $URL$ Some basic information about and : $URL$ $URL$ $URL$ 

Another popular idea used in cases like this would be moving relation between and out of their own tables. Create third table, that will store just the relations between both existing tables and will allow for n:n relationship. Columns could look like this: with both of them being foreign keys to and . All You need would be adding unique constraint on both columns. Be aware that this (normalised) approach creates some overhead, as it will require additional in queries, so it might not be solution for Your data volume. 

I assume You are asking about database, right? Neither of those two operations is really responsible for improving performance (and shrink doesn't improve anything about database performance/speed). 

In short, for this pattern, I would not use a LOOP hint. I would simply not use this pattern. I would do one of the following, in order of priority if feasbile: 

I have to say that I don't quite agree with some of the comments that state that what you're seeing is necessarily a framework problem. The Linq to SQL framework most certainly does allow you to specify a stored procedure as the means of input for rows. That being said, I'd highly recommend you use that mechanism instead of a trigger if at all possible. You'll be able to add your locking hints inside the stored procedure and your inserts will work fine. Your spec, however seems highly suspicious to me, and IMO the problems you seem to be running into right now are good counterexamples as to why one might not want to pursue a design such as this. So my (purely rhetorical) question is for the designer. What is that problematic sequential number supposed to represent? If it's a sequence in time, then why not just use a time and use the window functions to order the rows in something like a view instead (you can point Linq to SQL to views as well)? Is the intent to have some sort of gapless sequential number? Are you signed on to all of the complexity and locking you're going to have to deal with just to maintain that number? Unfortunately, I think there is no real answer -- no magic combination of hints which will work across your frameworks and design -- which will solve your problem. IMO my answer is take a step back and look critically at your design. 

MS Analysis Services (Multidimensional)'s [almost] whole reason for being is to abstract away aggregations so you can simply query tour OLAP data without having to worry about how the aggregate value is being retrieved. In fact, in Analysis Services Multidimensional, you point and click to create your aggregation combinations. I'm not sure what database server you're running, but if you're running SQL Server 2014 you also have the ability to build a clustered columnstore index on your fact table. Doing this will, in a most peoples' cases, essentially eliminate the need for you to perform aggregations entirely. I'd definitely look into that as an option if you're on SQL Server and your data and resources are a good fit. In that similar vein, if you're not running SQL Server, there are a lot of columnar databases out there which service OLAP workloads quite well, such as Tableau ($$$). I'm not sure what the open-source space has to offer, but that's worth checking out as well. 

I need to delete each row in (fps from now on) table if has not relation (NOT IN) (fmmp from now on). Since my input will be an array of values (fps.id) as for example: 

I have a table called where I have a default record "always". Since I'm testing all the time and sometimes I need to clean that table I'm asking if there is any way to run the following query any time I run the TRUNCATE command on the table: 

I have two databases and . is a kind of master and keep on continue sync against an external sources. I need to perform a internal sync between and so I am using triggers for achieve this. Now at db I have a table and at I have a table . Every time a new record is inserted on or every time a record is updated I need to update the row at so this is how the process should run: 

I have two databases, in first database I have a table called and in the second database I have a table called . The SQL for each is below: 

I have a file with a bunch of data which I am using for load all the data into a DB table. The file lacks the ID column (because that's is an internal column managed through our software and they don't care about it). To avoid issues with the data my "solution" was to move the column to a position where doesn't interfere with the data from the file and that's right after column or just at the end. Check the following image for a graphical representation of the table: Currently the is my first column and I haven't any problem but I am trying to move to the end because the issue I am having with the data but I got the following error: 

Just like @KookieMonster noticed, You have Auto Shrink turned on. And one of disadvantages of shrink commands is fragmenting Your indexes once again: $URL$ 

Mostly because performance issues are hidden somewhere else and defragmenting indexes works as quick band-aid (that doesn't last long, tho). It could be issue with statistics not up to date, or other issues. Just like Brent Ozar and Kendra Little wrote in their articles. 

Oracle doesn't work like (or engines like or ). There is no concept of storing tables as stand alone files in filesystem. Your data is stored physically in (big) , which contain many logical objects, like or . are also grouped in another logical objects, called . While this doesn't cover whole concept of Oracle's storage system, it should show You, in short, how it is designed. It's bit more complicated with database on , which is Oracle's volume manager and file system (and a variation/implementation of ), or, in older versions, mechanism called . If You have administrative rights in database You can check this query. It will show You data files and their location on filesystem: 

It's in syntax, so You might want to check some tutorial or course, like one from CodeAcademy. Another great source helpful with learning about joins is Jeff Atwood's visual explanation of sql joins. Just like @a_horse_with_no_name and @Vladimir_Oselsky commented, You should switch from old ANSI-89 SQL syntax to current ANSI-92, which allows for easier to write (and read) queries with more flexible syntax. 

No, shrink won't break Your log shipping configuration. But You must be aware that both shrink (and rebuild/reorganize You will have to do afterwards) will make Your transaction log files grow a lot. All of those operations cause a lot of I/O load that is logged to transaction logs. This, in practice, might mean that while Your log shipping won't break, it will make restore last a lot longer, depending on Your backup/copy/restore jobs frequency of course. That might lead to secondary falling behind a bit till shrink's (and defrag's) end. Remember that shrink has serious issues, read Paul Randal's post about it, if You haven't already: $URL$ $URL$ $URL$ 

Disabling auto-updating statistics for this table in this situation should be just fine, since you only address each record by its unique clustered index. Since the selectivity of the index is perfect, and it never changes, the optimizer should never have a need to look at other query plans based off of changing statistics. 

Install the tools you need now, and no others. You can always add more features to the installation later. If you're not installing reports or PowerPivot into your existing SharePoint installation, don't install them. When most people say that they want to get "SQL Server" installed, they mean just the database engine. If that sounds like you, install the Database Engine and Management Tools, and you'll have a basic SQL Server up and running. 

It's SQL Server that's consuming all of this memory. You aren't seeing the memory it's consuming because SQL Server is probably using "locked pages" which are not part of the server's working set, which is often all you'll see in Task Manager. There's a detailed description on why this is on one of the MSDN Blogs. 

There's no way to tell SQL Server not to use the transaction log. What you can do is set the recovery model of the database to SIMPLE, which will overwrite old log entries as space is needed. You should not do this on your production server, however, because you won't be able to do certain types of restores, such as point-in-time restores. Alternatively, you can set your transaction log file to be larger -- as an unscientific rule of thumb I'd make sure that either A) your transaction log has at least about 1.5x more free space than the size of your table or B) that your transaction log can auto-grow to a drive which has at least about this amount of disk space free. You can free transaction log space by backing up the log. If you don't care about the log contents, throw the file away. A shortcut for this is . Again, don't do this on a production server unless you are absolutely sure you understand the implications. Another thing to be careful of (though it's not entirely germane to your question) is to make sure the table you're expanding has a clustered index defined on it. If it does not, the table could incur a very large amount of heap fragmentation, and potentially become needlessly large on a change like this.