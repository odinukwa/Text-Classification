No CA will give you a certificate to sign others (at least not unless you have lots of money and a solide knowledge of certificates and how to keep them secure etc, which you currently don't have). 

SNI is essentially the same as the header in HTTP. The main difference is that the header is only inside the HTTP request and thus can only be seen by the web server after the TLS handshake is already successfully finished. The SNI extension instead is send within the ClientHello, i.e. the initial message sent by the client inside the TLS handshake. The server then extracts the SNI extension from the TLS handshake to find the matching configuration (which includes the certificate) the same way as it extracts the value of the header to find the matching configuration. Based on this there should be no qualitative difference and no additional limit with SNI vs header. But there will be some quantitative difference in that more memory will be needed with SNI compared to only header: additionally to the HTTP part of the configuration also the SSL part needs to be kept in memory, i.e. specifically the certificate, certificate chain and private key. 

Same problem with Chrome too. But, if the site does not load all you might have distrusted critical CA certificates. I recommend that you retry with a fresh browser profile because this uses the default set of trusted CA. This should also resolve problems coming from the use of HPKP with keys which are no longer used by the site. 

A web server does not have any FTP server passwords, but a FTP server has. If no FTPS (i.e. FTP over SSL) is used you can do a packet capture (wireshark, tcpdump...) on port 21 (ftp) and analyze the packet capture to extract the usernames and passwords. Look out for the following sequence from the client: 

You don't need to restrict yourself to a specific cipher, but instead simply enable all ciphers which are acceptable to you and in the order you prefer them. The resulting cipher then will be negotiated between client and server depending on the supported ciphers on both sites. Don't restrict yourself unnecessary. As for the ciphers typically used at the server side you might have a look at Quantifying the quality of TLS support where I've analyzed the TLS support for SMTP from the top 1M sites according to Alexa, which are about 600000 mail server with TLS enabled. According to my tests about 33% of the servers use ECDHE ciphers and 52% DHE ciphers, so that 85% use forward secrecy. And for some more information about the ciphers used you will not find in the study here is a detailed list of ciphers negotiated when used with the DEFAULT cipher set of OpenSSL 1.0.1: 

And rightly so. Port 443 is the port for https not http and the Apache config you show correctly configures https at port 443. Thus will not work but (i.e. https instead of http) will work as will a simple because 443 is the default port for https. 

HIGH, LOW, EXPORT etc are kind of macros which include a range of ciphers. To get the actual ciphers use command, i.e. 

In my opinion the set from Hynek makes more sense, especially since the ciphers only in the set from Mozilla are usually not supported by either the browser or the servers certificate anyway. 

Yes, an MX should not point to a CNAME. In this case simply make another A record for mail.example.com: 

You can't. EV certificates can only be issued by some certificate agencies which are hard coded inside the browser. 

Since python3 and the browser use SNI they will succeed. Python 2.7.6 will instead not use SNI and get the right certificate but with the missing chain. Because of that it is not able to verify the certificate against the local root CA's. 

From searching a bit on the web it looks like that Time Warner has IPv6 enabled for their customers. Since all modern OS can do IPv6 and will use it if available this matches my problem description. 

This header containing a realm is needed so that the browser can either prompt the user to provide the credentials for this realm or use cached credentials. Sending no but instead sending a HTML page which even contains script is definitely wrong behavior of the server. Note that response code 401 must only be used if the browsers build in authentication dialog should be used and then a realm need to be provided. If one builds its own authentication system outside of HTTP (i.e. the typical logins inside HTML like it seems to be in this case) then response code 200 must be used. 

By using an absolute URL you issue a request against an HTTP proxy. To make a request against a HTTP server you need a relative URL. Also, HTTP/1.1 the use a a Host header: 

The client does not announce in the handshake which DH key strength it supports so the server cannot use a different DH key for different clients. But, the DH key is only used if DH ciphers are used. Modern clients prefer ECDHE instead and if your nginx accepts this preference then the handshake will not use the DH param at all since it will not do a DHE key exchange but an ECDHE key exchange. For configuration examples which prefer ECDHE see $URL$ As for clients without SNI: they will simply use the default host configured in nginx. Thus if you have a proper certificate there which matches all the names the non-SNI aware client might use to connect to your server then it should work. 

That does not make much sense. With UDP you receive either the full packet or nothing, unless the application explicitly calls send with a length smaller than the packet and set also to accept packet truncation. So any limits here must be done by the application. An UDP packet itself can be up to 64k and I'm sure that Java itself does not change every receive call of the application to use a small length only, otherwise lots of other Java-based applications would have problems. So please check how the reading is done inside the application. I assume that their is an explicit limit when reading. You might also run the application with strace and check how the application calls recv or recvmsg, there you might see the size the application requests. Of course it could also be, that the application never receives the packets because the get filtered outside the application. In this case you should check with tcpdump/wireshark if the packets arrive at the machine at all, e.g. don't check at the sender side but at the recipients side. 

Yes, if a server is compromised the private key can be compromised so you really have to keep the server secure. 

A fast cipher will usually not reduce the overhead of the SSL negotiation significantly. The ciphers gets mainly used after the negotiation is done and has only a small performance impact. Some part of the cipher is relevant for the handshake (the key exchange) but unless you choose a very slow key exchange (see below) the main performance impact comes from the multiple round-trips inside needed for the negotiation. These can only be reduced if you support the reuse of sessions, so that a full handshake is only needed for the first request and the next time the same client connects a less expensive session resume can be done. HTTP keep-alive can help a lot too. Of course both these optimization work only if you actually have multiple requests from the same client. There are some ciphers with very slow key exchange, which you probably don't want to use in your case. All DHE-* ciphers have a large performance impact, but have the advantage of providing forward secrecy. You get the same advantage with ECDHE ciphers without having too much of a performance impact on today's hardware, but an overhead is still there. Using ciphers like should be a good choice both in terms of performance and in terms of security. At the end the choice of cipher depends also on the clients you use. While is fast it is considered insecure and more and more clients disable it. Thus you might and with a fast server nobody can use because browsers disabled insecure ciphers.