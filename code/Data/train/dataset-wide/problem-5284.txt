Those linguistic definitions turn out to be remarkably harder, given modern scientific understanding, than they were a few hundred years ago. Modern medicine on its own has rewritten what it means for a human to be alive. In fact, it has done so much that there are now conflicting definitions such as "brain dead" "clinically dead, " or even "beating heart cadaver" appear. Cryogenics has started adding a new definition, "information theoretic death." Most philosophy builds from premises to conclusions. If two individuals linguistically disagree on the meaning of a word, no conclusion can be reached. The linguistic issues dominate. Once agreement on a word is arrived at, then the discussion of ethical systems can take place. 

This is a topic that great mathematicians like Tarski have beat on quite a bit (I highly recommend looking up his definition of truth: "P" is true if and only if P). Part of the issue is that there is an intuitive desire to stat that sentences can have a truth value associated to them, because we do so intuitively all the time. Your particular approach runs afowl of classical logic, because you have a thing whose truth value is not true nor false. The logic you recommend looks much like intuitionalist logic, which does not assume the law of the excluded middle. This permits a statement's truth value to be not true and also not false. It is a valid form of logic, just not the most popular one today. There are a lot of proofs that are much harder when you don't have the law of the excluded middle, so intuitionalist logic doesn't prove as many things. 

The effect is an artifact of the assumption that if level "N" exists, then it must unify more individuals than level "N-1" did. Without that assumption we can do things like define Level N-1 to be "family" and Level N to be "family" as well. Of course, even if we require that each level be different from the previous, this process could end at a highest level which contains all individuals if the number of individuals is finite. If the number of individuals is countably infinite, then your theory holds and retains its oddness, but it wouldn't be the least intuitive part of infinity. If you can identify a highest level, then you can phrase the construction backwards and the behavior is a bit more sane. If you have a level N, you can define a level N-1 by creating a partition around an attribute. This process continues until you reach level 0, which is an individual that cannot be divided further. 

I find the wording of Occam's razor to lead people astray. Its wording suggests that it is always good to select a hypothesis. I prefer to adulterate his words, and in doing so, believe I get closer to the concept he intended: 

I see two reasons anonymity is important on the internet. They are very different from eachother The main issue is one of novelty. The internet has dramatically changed how humans interact. Never before have our most causal words been stored for eternity, shared with the world. Our society and its social structures have not fully adapted to this new reality. Anonymity is a powerful tool in the short run to let us explore this new and dangerous realm without catching our self in the wake. The other is a simple pragmatic issue. The nature of long-distance transmission makes it difficult to positively identify an individual. The internet backbone cannot realistically deal with this, so any identifying behaviors must be layered on top of this. Requiring identification for everything on the internet would be a crippling physical blow. Over time, I expect laws will evolve which requires identification in some places, and allows identity in others. There are plenty of legal situations where this already happens. Most countries generally allow you to do anything you please within the confines of your own home, but demand social norms in public places. Police in some countries (e.g. USA) are not allowed to demand identification from just anybody, but once you are arrested for an alleged crime, you are now required to identify yourself, or an identity will be given to you by the state. 

This sentence is quite tricky to work with, tying concepts of value and ethics together. I wanted to start off with some grounding, so it seems reasonable to quote the second paragraph from Wikipedia's article on Ethics: 

I think its trivial to argue that any theory does a combination of stabilizing and destabilizing worldviews. Naturally, it stabilizes those parts of your world view which are consistent with the theory, and destabilizes those which are not. To argue that it stabilizes more or destabilizes more, we need a way to compare these effects. Finding a way to do that is not easy. Sometimes it stabilizes more, such as when you've noticed some strange coincidence in our observations and you find out that there is a theory behind it. Other times it destabilizes more, such as when you thought you knew everything and someone kicks the chair right out from underneath you (QM and relativity are notorious for that). I don't know if you can say universally that theories destabilize more than stabilize. If so, the most unstable people would be the ones who learned the most theories. This concept of stabilization must be taken globally. You are also destabilized when you cannot predict what is going to happen because you lack theories to do the prediction with. A theory that destabilizes at first may stabilize in the long run. I think avalanche breakdown in diodes may be an excellent example. To understand avalanche breakdown you need to understand QM, because the effect involves quantum tunneling. You may choose to view QM as a destabilizing theory if you please. However, once you understand it, integrated circuits are a bit less magic. So here's the question... which is more stable? Having QM under your belt and understanding the theory behind avalance breakdown? Or believing that IC's are simply magic? It turns out that you can live quite the stable life believing that ICs are magic devices powered by smoke. And that makes the decision really complicated. Was your life destabilized by learning enough QM to take the magic out of ICs? 

The only thing which can determine that value is you, but you may be able to hedge your bet by looking the valuations of Christianity in its most general sense. 

Your specific wording suggests the statement must be true, but a reworded version of the question may provide sufficient linguistic ambiguity to permit it to be false. In our exchange in the comments, you mention that this question stems from the Deflationary Theory of Truth. If I may show my ignorance and quote wikipedia: 

Much of your argument stems from the assumption that a choice between death and something else is not a choice. There are countless examples in history which suggest this is not a good assumption to hold while delving into difficult concepts like total warfare. There are plenty of examples of people who have chosen to die for their religion when faced with such decisions. The Samurai were expected to be able to make this decision with respect to "honor." There are numerous concepts such as "love" or "freedom" for which dying for is currently romanticized in modern Western cultures. Given this, a question you must ask yourself is that, in total warfare, is everything decided by fate. Do you truly have no say in what happens? Is it not possible for an individual to choose to blink? It may be that blinking at a moment may cause you to die because you miss something, but that goes back to the question of whether a choice between death and something else is a choice. The Zen Buddhists have a koan which I think of when I consider such choices: 

This is a tricky question because the human mind is so mind-numbingly interconnected that it is virtually impossible to show that any two thoughts in the mind are not related. So at that lowest level, no, it is impossible because every thought we have is tied to every other thought we have in a huge web. However, I think there may be a class of beliefs which are almost independent of empirical evidence. There are beliefs where there is little to no evidence one way or the other, but the cost of not accepting or rejecting the belief outright is high. One can see this in a highly extreme stereotype in the movies. There are situations where someone is asked a question, and they don't have enough information to give an answer. In these movie plot situations, a gun is pointed at their head (physically or metaphorically), and they are told to decide one way or another. In order to have enough confidence to give an answer like this, they may be forced to believe one way or another before responding (otherwise the disbelief may be detected and the gun fired). In fact, after the event, they may begin inventing their own world, gathering fake evidence, just to defend the belief they were forced to accept in a traumatic situation. Related, consider Stockholm Syndrome. To the best of my knowledge, in Stockholm Syndrome, the psychological capture of a victim by their captor occurs before there is empirical evidence to back it up. After this occurs, then the victim begins collecting empirical evidence to misrepresent, but as best as I can tell from my research, the belief occurs before the evidence. 

Why do we excuse the engineer who can't write well enough to document their code? Why do we excuse the scientist who is too messy to find their own data? Why do we excuse the artist who can't sell their own painting? The answer, of course, is that they're good at what they do! I agree with you that I think the ability to communicate your ideas is fundamental to what I would call being a "successful philosopher." But sometimes it can be tricky. Many of the questions that are interesting in philosophy have no natural answer. Philosopher often dig very deep before finding something "close enough" to an answer, and they spend their time communicating that. Contrast that with many other businesses where there is "an answer," and you simply have to find it. Some of the reason we excuse a philosopher for bad communication skills is simply that those who have plumbed the work for its secrets find those secrets useful. They then become a sort of priesthood, pointing to the work of the original philosopher saying, "there's some good stuff in here!" And we can trust the priesthood, or not. It's our choice. From my own life, I can say that if you went to Cort-ten-years-ago and told him what I am finding valuable in philosophy today, I'd think you were a loon. I "knew" better than to think these sorts of things are worthwhile. I simply wasn't ready to question myself that much. So in a sense, that sort of anecdote shows a reality: philosophers are not trying to communicate to everybody all the time. Sometimes the target audience is the priesthood of philosophers who can spend the time and energy to dig into your work. I tend to prefer to think as you do. I love Alan Watts' work not just because of their content, but because he was an incredibly accessible man. He wasn't interesting in preaching and having everyone miss the point. He talked to their level, whatever that level was. I value the work of philosophers like him. But I also have to admit that there is "that other thing," that seems so difficult to attain without crossing a bridge of sorts and finding onesself unable to communicate what they have seen. 

I would argue your approach to the former, starting a question with "I pursue only intuition; please do not answer with formal proofs or Truth Tables." is tricky because the syntax is a major feature used to capture the meaning. We use syntax in this way because there is no one way to arrive at an intuitive sense of the meaning of the word. I program computers; there are plenty of arguments which make intuitive sense to me which would not make intuitive sense to an artist. In person, we can tailor our discussion of the symbols to fit what we know about the person we are talking to. Lacking knowledge of the person, the best we can do is fall back on syntax. How one can arrive at a meaning is simply out of scope in many cases, because everyone does it their own way. The latter half, however, may prove more valuable in a forum such as this. There are typically many answers for why certain meanings resonated better than others. Again, stealing from virmaior's answer, if you are considering bivalence, Aristotle's 3 laws, and a set of operators defined by their truth functions, meanings such as "implication" naturally resonate and are worth capturing in a symbol such as . In fact, in this case, it resonates so strongly that propositional logic has two related symbols for this: and . The former is fully defined within propositional logic via the definition above, while the latter is considered a "meta symbol" whose meaning is not fully defined within propositional logic. The meaning of is far more subtle, and one way to capture this meaning is to look how it has been used historically, and why philosophers felt it was needed. I think the latter concept may be something which can be explored in a setting such as StackExchange, because the question inherently has a historical context to it, and StackExchange is very good at topics that are tightly tied to history. 

You are correct. The book is sloppy with its wordings Your passages 1 and 3 are important, and (almost) correct 

One easy approach would be to argue that failing to lie by omission would cause harm. One case where I think it would be very easy to justify this would be if the person in authority is incapable of properly conveying the information. If I may use the terminology of speach acts, one often comes across situations where it is not possible to cause the correct perlocutionary act while speaking the whole truth because the listener cannot fathom the desired act. We see this in child rearing, when dealing with difficult topics such as death. We see this in religion, trying to convey the beautiful certainty our particular religion offers. We see this in all sorts of aspects of life such as love (how many times have people refused to explain what being in love feels like because they know they will fail to capture it). In many cases, if you omit content, it will lead the listener to go investigate on their own and arrive at a point where their own understanding is sufficient that you can finally tell them the truth. On the other hand, if you tell them before they are ready, they may interpret that truth wrongly, and then misuse it. In short, the mere act of telling somebody a truth does not always cause them to hear a truth. In such situations, omission may lead them closer to the truth than voicing that truth. 

I am more familiar with the Eastern approaches to breath, so there will admittedly be speculation here regarding potential Greek mindsets. In many Eastern philosophies, such as those at the root of Kung Fu or Yoga, everything gets tied to the breath as part of the flow of life. In such systems, breathing in is often strongly associated with an acceptance of the world around you, so from that vantage point, this "breathing in" or "gasping" might be associated with suddenly letting a deep unresolved truth resolve itself, and inviting it in. This reading would be validated if, in later paragraphs, Si√¢n Ede chooses to describe a process of sharing that which you have resolved with the world as an exhaling activity. The Eastern approach to breath very rarely includes only one half of the process; it almost always covers both inhale and exhale. 

Supervenience of the mental onto material implies that all changes in "mental state" can be fully grounded in some physical principle (I believe it is generally what you intended when you phrased the question in the first place, this is just the precise word). If a dualist was to somehow prove that there was a mental state which did not supervene onto a physical state, they would have proven dualism. At such a point, the mental state would not become material, because it would still not behave according to physical laws (in particular, it would exhibit mental behaviors such as freewill). As for the "proof" of dualism or physicalism, the debate has raged for a very long time, and the general consensus is that nothing resembling proof can be acquired without dying (and it is generally presumed to be very difficult to bring such information back to share with the rest of us). Until we know "all of physics," the unknown is still out there. Those that do believe there is proof typically use the word "proof" in a way that permits their belief to shine through, such as a "scientific proof" or a "religious proof." In particular, the question of what does it mean to perceive something continues to divide the camps in a way that is so difficult that even the idealists, who believe there is only mind and matter is an illusion, can make a play. The challenge of empirically testing theories involving chaotic systems doesn't make the debate any simpler. Finally, there's the question of randomness in empirical studies. If you really dig into statistics, there is no way to tell if some experimental results are a random distribution or a non-random (mindful) distribution. All you can do is look at the data and say how likely or unlikely that distribution was to arise by random chance. There is (intentionally) no way in statistics to be 100% certain that a variable fits a given distribution or not.