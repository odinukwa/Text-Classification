This isn't saying that there is a memory leak in the Instant Client. It is saying that using the Instant Client involves adding a few kb of physical memory and a bit more virtual memory to your application's memory footprint. That's normally not something that would impact performance. If you are running in an environment where a few kb of space is an issue (for example, you're building an application that is supposed to run 10's of thousands of copies of itself on a single server), then you might be better off with a full client install. Of course, the full client will undoubtedly load more than a few kb of shared libraries into memory, they just won't count as part of your application's footprint. 

creates a single file "dumpfile.dmp" in the current directory that can be sent to the vendor. It would be more efficient to use the DataPump version of the export utility but that requires a bit more work because that requires that you use at least one Oracle directory object. If this is a reasonably small database and/or the time required to generate the export isn't prohibitive, it's probably easier to use the classic export utility. 

Assuming that you mean that the table is partitioned on and subpartitioned on , the query that includes the predicate will be much more efficient. Since Oracle cannot know that subcategory only exists where the category is , it would generally not be able to eliminate any partitions. Instead, it would have to search the subpartition of each partition where a of would be stored in order to fulfill the query (assuming that there is no global index that could be used). If you include both predicates, Oracle should be able to eliminate all but the partition where values are stored and then search only the subpartition where values are stored. 

The privilege allows you to create database-level triggers (server error, login, and logout triggers). It also allows you to log in regardless of errors thrown by a login trigger as a failsafe. If you inadvertently coded your login trigger to throw an error no matter who was logging in, for example, you need to allow someone to log in to fix the trigger. It appears that the trigger is behaving as expected in this case. You wouldn't in reality create a database link from one database to another using a DBA account. 

First, are you using "database" in the Oracle sense of the term? Or are you using it in the sense that other database vendors (such as SQL Server or MySQL) use the term? If you are using "database" in the Oracle sense, that would be the size of the and tablespaces at a minimum and would possibly include the size of the and tablespaces. On a small laptop system, that's probably on the order of 2 - 3 GB but it could be much bigger. If you are using "database" in the sense that other database vendors use it, that's what Oracle calls a schema. An empty schema (a schema with no objects), by definition, consumes 0 bytes of storage. That's because space is allocated to tablespaces which are independent of schemas-- a single schema can have objects in many different tablespaces, a single tablespace can have objects owned by many different schemas, and a single object (if you're using partitioning) can be spread across multiple tablespaces. 

Are you trying to test the database? Or the application? Assuming that the Oracle database is configured correctly (i.e. it is in ARCHIVELOG mode, backups are done regularly, DataGuard is in place depending on your recovery requirements, etc.), by far the most common source of problems in a failure is that the application itself has not defined its transaction boundaries correctly. The classic scenario here is a banking application that wants to make a $50 payment from account A to account B debits A by $50 in one transaction and credits B that $50 in a separate transaction. Unless you happen to be able to test what happens when system fails after the first transaction commits and before the second transaction commits, you won't see that the transaction boundaries are incorrect and the application might inadvertently lose $50. 

If you want to convert a string to a date or a timestamp, you're better off using a or a with an explicit format mask. 

According to the query plan, the optimizer expects that the statement is going to modify 1 row. It appears to expect the Cartesian join to return a single row as well. Based on the fact that the query isn't returning in a few milliseconds, that would generally imply that the statistics on one or more of the objects (tables or indexes) are substantially incorrect. When was the last time statistics were gathered on these tables and their dependent indexes? Has the actual data in the tables changed substantially since that time? 

I'm not completely sure that I understand what you mean by "every should be unique for that value of ". If you are trying to say that each has to be unique within each , then it sounds like you just want a composite index on (, ). 

Database duplication generally refers to restoring a physical backup of a database to a different server (preferably using RMAN). That is normally done periodically to refresh lower environments from production. Database replication generally refers to the process of copying a subset of data from one database to another on an ongoing basis. Replication generally implies that the data is being copied from one production database to another production database (or a test database to a test database, etc.) 

Your product table should have one row per UPC (assuming that uniquely identifies a product). It shouldn't reference prices or retailers. Your retailer table should have one row per store. It shouldn't reference products or prices. 

Since is the position of a particular block on a particular form, it should be a column in the table, not a column in the table. If is in the table, a block would need to have the same position in every form it was on. Once you move the column to , you can then create a unique constraint on the combination of on the table.