I recently setup a Git server where I work which required SSH and HTTP(S) access for public and private repositories. I first setup the entire thing manually which took some time, but then I came across Gitlab which seemed to solve all my problems - it's pretty much Github, but your own private hosted version. I'd strongly recommend using Gitlab over setting everything up manually, given that it has so many great features and is easily manageable. There's a great tutorial here: $URL$ However, if you do want to run Git through HTTP(S), you will need to setup Apache or Nginx to use the git-http-backend binary which does all the work. There are plenty of tutorials online depending on which configuration you decide to go with. 

By default, when you create an account in WHM, it will configure local mail accounts and it will set the local DNS to use the local MX records. However, if you do not wish to rely on the local DNS for the MX records, then you can force WHM to always use a remote mail exchanger. In WHM, goto: DNS Functions -> Edit DNS Zone -> (select domain name) At the bottom, you will see something called Email Routing. Set this to Remote Mail Exchanger and then ensure the DNS server is restarted once saved. 

Your research is right. You cannot redirect with a CNAME because DNS does not work this way. You would need some sort of web server to accept the request and rewrite the URL to HTTPS. Some domain registrars allow you to do a redirect on their web server if you use their name servers. DreamHost may provide this service but I have never used them so I cannot say. 

You just need Route policies in place, and they should have been created automatically for you when you set up X0 and X4. Make sure that the computers on both subnets have the firewall set as the default gateway. 

I ended up fixing this problem by switching to a different company. SecurityMetrics had no trouble with our network. (And I never did hear back from Trustwave with anything helpful.) 

I'm not exactly sure how to whitelist them. I set up a couple address objects in the WAN zone, but I'm not sure what to apply it to. I tried turning off "Stealth Mode" (due to an associated complaint of over 60,000 open ports, which is impossible) and I even went as far as to turn off IPS (I turned off protection but left on detection because I don't want to leave us totally exposed). However, the scans are still failing. It does seem a bit ridiculous to turn off security in order to test security, but apparently that's now a normally accepted thing for PCI scans. I've opened a support ticket with them, but the first guy I talked to had no idea what to do other than reading the same information I already had, and I'm waiting for a call back from their "scanning team." I'm hoping someone here has already figured this out on a Sonicwall and can help me better.ï»¿ 

Below is a sample configuration for the HAProxy frontend and backends which I just wrote up. This is not the full configuration file, so please use the /etc/haproxy/haproxy.cfg as a guide to having a complete configuration. 

Based on your location blocks, files ending with '.php' will only be passed through PHP. So if you have an actual file in the '/images' directory, it should serve it as a static file, or otherwise return a 404 error. If the file ends with '.php' and is in the '/images' directory, it will run through both location blocks in order. Should it do otherwise? 

If you change your custom cookbooks, you need to update the cookbooks on each instance. You can do this under the "Deployments" area by clicking "Run Command", selecting "Update Custom Cookbooks" from the drop-down and then pressing the "Update Custom Cookbooks" button. You can usually leave all instances checked, unless you don't want to update one for some reason. 

If it's not set before it reaches this location block, then it will set it to a blank string. You can just as easily add a string between the quotes. I do not get any errors when doing a configuration test using this. Please let me know if you're seeing otherwise. 

It's probably not running because it's unable to find the binary. Without any error log though, that's just an assumption I'm making. Try adjusting the part in your script to use its full path. I'm not sure how you installed (either via yum or manually), but you can find out its full path by running . 

This is kind of a long story, but we discovered that controller B had quit working some time ago. We bought a used replacement, and I swapped it out. But I got an error saying there was a mismatch between the controllers. At this point, I also got errors saying "Writeback Cache Forcibly Disabled" and "Insufficient Cache Backup Device Capacity" - but they were for controller A, which I had not touched. I discovered that the replacement controller had an extra board in it, and after removing the extra board and reinserting the controller, the DS3524 was happy with it. But the cache errors remained. I thought maybe it was because the battery on the replacement controller needed to be charged, so I let it go overnight. Now the battery in controller B is charged and has completed the learning cycle. But controller A is still having the cache errors. I ran some scripts I found to disable and re-enable caching, and that didn't fix it. I then tried doing a reset on the controller. That also didn't fix it. I guess I'll need to go in person (the DS3524 is at another location) and try removing the controller and reseating the SD card. But it just seems really odd to me that these errors occurred on the controller that I didn't touch. Is there anything else I can try that doesn't require physical access? I should also mention that the firmware level is 7.86.32.00. I did see some information about a firmware bug that causes this problem, but it's for a lower version. 

You can also do various regular expressions so you can avoid from having to add a new one each time a new 'perma-link' is added. 

Rather than having a location block for each redirect, you can always just add rewrite rules into an existing location block: 

The and variables are not required as they have default values, so you do not need to include either if you don't want to. Hopefully this solves your problem. 

I do not believe this is possible as the name servers stored in the domain name do not have any given priority. The client will use the name server that it is given. 

I just tested this and it looks like the GitLab API response is using pagination. According to the documentation ($URL$ the default number of results per page is set to 20 and the starting page is 1. To adjust the maximum results per page, you need to use the variable in the HTTP request line. You can change the page number by using as well, if you have more repositories than the maximum value of . You can specify a maximum value of 100. For example, you request may look like: 

Providing that you have loaded your private key on your client, then it sounds like this might be a permissions issue on the 'git' user home directory and .ssh directory. Please try changing your /home/git directory to a mask of 0711: 

I have a "Standard UCC SSL Certificate" from GoDaddy. I needed to add another SAN to it. They were able to reissue without having to create and submit a new CSR. However, I am now having trouble getting Exchange to recognize it. I went through the wizard for "Import Exchange Certificate" in the Server Configuration, and it said it was successful, but the new cert does not show up in the list. I tried doing it again, and was told that a certificate with that thumbprint already exists. So it's apparently there, but I seem to be missing a step before it will let me use it. I've searched for help, but everything I've found is about renewing the certificate or installing the initial one. 

We've started having Trustwave do monthly PCI network vulnerability scans. The last couple months we've passed, but this month it failed for "Scan Interference Detected." This is their recommendation: 

What model and services do you have? And what methods are they using? The CFS settings allow you to restrict access to HTTP proxies, and the application firewall should keep them from using a VPN. Many web sites are now using SSL, so if you want to enforce your policies through SSL you will need a DPI-SSL subscription. Or the new SSL Control feature (under Firewall Settings) may be helpful. 

This is not possible in the current OpsWorks system. Even if you could, it would require the server to have the OpsWorks agent and Chef to be installed, which is handled already in the OpsWorks AMIs provided by Amazon. A bit late, but I thought I'd give it an answer. 

Replace /home/git with whatever your home directory for the 'git' user is, if it was different in the tutorial. If it's not permissions, then please let comment and we'll see what else might be the issue. 

This seems to work without any issues for my servers with multiple network interfaces. If you then restart your network service or interfaces, you should be able to check the routes to see that this is actually working: 

You can't map a port on a public IP address to more than one private IP address, it just won't work because how is the router supposed to know which one to go to? If you're using name-based virtual hosts, you could achieve this by sitting a HAProxy instance in-front of the web instances and direct all traffic from the router to the HAProxy instance. On the HAProxy instance, you create a front-end and specify the domains and the backend to use. Then, depending on which domain is accessed via HTTP, it forwards the request to the appropriate back-end to serve the request. I do it all the time when I want to conserve server or IP resources.