Gravitational microlensing is a way of finding planets that does not care how luminous or otherwise the hosts of the exoplanets are. The way it would work is that you stare at a dense background field of stars; then when a foreground black hole passes in front of a background star, the light from the star is magnified by gravitational lensing. Typically, the lensing event takes a few weeks for the magnification to develop and subside. If the black hole has a planet, it may be possible to "see" its graviational potential, which will manifest itself as an asymmetry in the lens light curve or possibly even a little extra spike in the light curve lasting a few hours. This technique is well established and is already used to detect planets around unseen objects. It is more sensitive than the transit and Doppler methods to planets orbiting a fair distance from their parent star. The difficulty here is not finding planets around a black hole, it is proving that the planets you found were actually orbiting a stellar sized black hole. The microlensing event would only be seen once and the black hole system would likely be invisible. There is a possibility I suppose that it may undergo some sort of accretion activity after having been found by lensing, and this accretion might then be picked up by telescopes trying to identify the lensing source. I guess a good place to start would be events where a stellar-mass lens is inferred, but no star can be seen after sufficient time has elapsed that the lens and source star ought to be resolved. 

The equilibrium temperature of the Earth, $T_E$, scales roughly as $L^{1/4}$, which is proportional to $R^{1/2} T$, where $L$, $R$ and $T$ are the solar luminosity, radius and temperature. The actual approximate relationship is derived by equating the power received by the Earth, which is proportional to the solar luminosity $L$, with the power radiated by the Earth, which is proportional to $T_E^4$ for a blackbody. Hence $T_E \propto L^{1/4}$. So the answer to your question depends on by how much you increase the radius compared with the decrease in temperature. There will be second order effects that do depend on the spectrum of radiation from the Sun (and therefore its temperature) compared with the wavelength dependence of the albedo and emissivity of the Earth. So I will post a better question... 

The spectra of a red giant and a red dwarf are completely different, so there isn't really too much to say about this. For example, alkali lines are almost non-existent in red giants, but strong in red dwarfs. The theory as to why this happens is the stuff of a standard graduate/undergraduate course on stellar atmospheres, not an SE answer. The fact is that a R=50,000 spectrum with decent signal to noise ratio will quite easily give you the temperature (to 100K), surface gravity (to 0.1 dex) and metallicity (to 0.05 dex), plus a host of other elemental abundances (including Li) to precisions of about 0.1 dex. What can you do with this: You can plot the star in the log g vs Teff plane and compare it with theoretical isochrones appropriate for the star's metallicity. This is the best way to estimate the age of a solar-type (or more massive) star, even if you don't have a distance and is the most-used method. How well this works and how unambiguously depends on the star's evolutionary stage. For stars like the Sun, you get an age precision of maybe 2 Gyr. For lower mass stars, well they hardly move whilst on the main sequence in 10Gyr, so you can't estimate the age like this unless you know the object is a pre-main sequence star (see below). You can look at the Li abundance. Li abundance falls with age for solar-mass stars and below. This would work quite well for sun-like stars from ages of 0.3-2Gyr and for K-type stars from 0.1-0.5 Gyr and for M-dwarfs between 0.02-0.1 Gyr - i.e. in the range where Li starts to be depleted in the photosphere and where is is all gone. Typical precision might be a factor of two. A high Li abundance in K and M dwarfs usually indicates a pre main sequence status. Gyrochronology is not much help - that requires a rotation period. However you can use the relationship between rotation rate (measured in your spectrum as projected rotation velocity) and age. Again, the applicability varies with mass, but in the opposite way to Li. M-dwarfs maintain fast rotation for longer than G-dwarfs. Of course you have the problem of uncertain inclination angle. That brings us to activity-age relations. You can measure the levels of chromospheric magnetic activity in the spectrum. Then combine this with empirical relationships between activity and age (e.g. Mamajek & Hillenbrand 2008). This can give you the age to a factor of two for stars older than a few hundred Myr. Its poorly calibrated for stars less massive than the Sun though. But in general a more active M-dwarf is likely to be younger than a less active M dwarf. It should certainly distinguish between a 2Gyr and 8Gyr M dwarf. If you measure the line of sight velocity from your spectrum, this can give you at least a probabilistic idea of what stellar population the star belongs to. Higher velocities would tend to indicate an older star. This would work better if you had the proper motion (and preferably the distance too, roll on the Gaia results). Similarly, in a probabilistic sense, low metallicity stars are older than high metallicity stars. If you were talking about stars as old as 8Gyr, these would be quite likely to have low metallicity. In summary. If you are talking about G-dwarfs you can ages to precisions of about 20% using log g and Teff from the spectrum. For M dwarfs, unless you are fortunate enough to be looking at a young PMS object with Li, then your precision is going to be a few Gyr at best for an individual object, though combining probabilistic estimates from activity, metallicity and kinematics simultaneously might narrow this a bit. As an add-on I'll also mention radio-isotope dating. If you can measure the abundances of isotopes of U and Th with long half lives and then make some guess at their initial abundances using other r-process elements as a guide then you get an age estimate - "nucleocosmochronology". Currently, these are very inaccurate - factors of 2 differences for the same star depending on what methods you adopt. Read Soderblom (2013); Jeffries (2014). EDIT: Since I wrote this answer, there is at least one more promising method that has emerged. It turns out that the abundance of certain s-process elements (e.g. barium, yttrium) are enriched gradually during the lifetime of the Galaxy (by the winds of dying asymptotic giant branch stars). Thus a measurement of the relative fractions of these elements can give the age to precisions of a billion years or so (e.g. Tucci Maia et al. 2016). 

It really depends what you mean by "rock". At the temperatures and pressures at the cores of stars (and at which nuclear fusion reactions are possible), "rocks" as I suspect you are thinking of, do not exist. Thermonuclear reactions do not occur because the gas is "flammable", they occur because the kinetic energies of the nuclei in the gas (at these temperatures, atoms are fully ionised) are sufficient to get them close enough together for nuclear fusion to take place. "Rocks" are made of atoms of silicon and oxygen (for example) in the form if silicates. But these are dissociated at fairly low temperatures compared to the centres of stars. Oxygen and Silicon thermonuclear fusion ignition requires temperatures in excess of $10^9$ K, and these temperatures are only reached late in the lives of stars of mass $>8 M_{\odot}$. It is possible to have stars with solid cores. This is thought to be the fate of white dwarf stars as they cool. Most white dwarfs are made of a mixture of carbon and oxygen and this "crystallises" once the core of the white dwarf cools below about a few million degrees. Ordinarily, the cores of white dwarfs are inert as far as nuclear reactions are concerned, because the temperatures are too cool. However, if the white dwarf is massive enough (or mass is added to it), then the central densities climb, and for white dwarfs of mass $\sim 1.38M_{\odot}$ it is thought that the densities become high enough ($\sim 10^{13}$ kg/m$^3$) to start nuclear fusion of carbon via the zero point energy oscillations in the crystalline material (so-called pyconuclear reactions). Such reactions in degenerate matter are highly explosive and might result in the detonation of the whole star in a type Ia supernova. 

There are some other numbers to do with adaptive optics correction or observing in the infrared that I might add later, but the above are the main ones. 

For plane parallel refraction an approximation for the deviation you are talking about is $$\Delta \theta \simeq (n-1) \cot \theta,$$ where $\theta$ is the observed elevation, $\Delta \theta$ is the change in elevation from its true value due to refraction and $n$ is the refractive index averaged over airmass. According to this source from the Green bank radio telescope, they use something like this, with an added model for how $n$ varies with height, scaled by the atmospheric pressure. The largest value of $n$ quoted is 1.00031 at ground level. This is basically the same as the refractive index of air at visible wavelengths. So, to my surprise, the effects of refraction on radio telescope pointing are similar to those for optical telescopes. It simply turns out that the real part of the refractive index (that controls the phase velocity of light and hence refraction) is just as close to 1 for radiowaves as it is for visible light. Here is another source that gives some algorithms to calculate the effective (small) real refractive index for radio waves. 

Page 12 of Batygin & Brown (2016) says that a speculative formation scenario can be drawn from recent solar system formation simulations by Bromley & Kenyon and by Izidoro et al. These suggest that the core of a nascent ice giant may have been ejected very early in the solar system's history in order to explain the properties of the observed planets; the formation of Uranus and Neptune was probably accompanied by at least one other ice giant. The source of the claim you mention is actually an article by Eric Hand in Science Magazine. He points out that possibly, to explain why this planet was ejected yet still remains part of the solar system in a much wider orbit, then you need it to have been slowed down by residual gas in the protosolar disk. So I assume the 10 million year is an (uncertain) upper limit on the dispersal of the disk and I would guess the 3 million year lower limit is just how long it takes to form a 10 Earth mass ice giant core at the distance of Neptune. 

How much? Well how accurately do you need it? How do you want it quantifying? And in what wavelength range? Jupiter scatters a fraction of its incident sunlight. It also has its own luminosity (predominantly in the infrared). A quick calculation: The solar constant (flux at 1 au) is about 1370 W/m$^{2}$. Jupiter is situated about 5.2 au from the Sun (it varies by about +/- 5%) and has a radius of 70,000 km. The albedo is about 0.34. Thus it receives about $7.8\times 10^{17}$ W from the Sun and radiates about $2.6\times 10^{17}$ W back into space. Assuming this is done more-or-less isotropically into a hemmisphere, then Ganymede, at a distance of 1,070,000 km from Jupiter, receives a flux of only 0.1 W/m$^2$ multiplied by the fraction of the sunlit hemisphere that can be seen. This compares with the $\sim 50$ W/m$^2$ it receives from the Sun! This surprising result (to me) puts into perspective all the simulations you see of things in orbit around Jupiter. The planet is still pretty faint compared to the Sun. I'd be grateful if someone could double-check the sums! [The intrinsic infrared luminosity of Jupiter is less than a fifth of what it receives from the Sun, so this would increase, but not double the received power.] 

It depends entirely on the circumstances of black hole formation. The remnant may be a stable neutron star, but if it accreted enough matter later on (by fallback in a supernova, or from a binary companion), then it may collapse. This could happen at any time after formation. If the core of the collapsing star forms a proto-neutron star above the TOV limit of stability, then no force can prevent black hole formation. It will happen on the cooling timescale of the proto-neutron star at the centre. Once it is "cold" then the collapse would occur on a free fall timescale - which is about a millisecond in these circumstances. The cooling timescale of a proto neutron is governed by how long the copiously generated neutrinos can be trapped in the core, which could be as long as(!) a few seconds 

One of the absolutely fundamental differences between the LMC and SMC, and why they have become among the most studied of astronomical objects, is their metallicity. The average metallicity of interstellar gas in the LMC is about half that of the Sun, whereas the average metallicity in the SMC is only a fifth that of the Sun. Hence the clouds act as two different, relatively nearby, analogues with which to study what happens to lower metallicity (usually much more distant) galaxies in the early universe, or how metallicity affects things like star formation. 

Conrad is almost right. It is true generally that if a Galaxy is close enough to take spectra of individual stars (e.g. luminous supergiants) then it is not far enough away to be regarded as part of the "Hubble flow" and so applying Hubble's law to this star, or its host galaxy, would not yield a reliable distance in any case, but would reflect the "peculiar motion" of that galaxy. To put some numbers on this. Galaxy peculiar motions tend to be a few 100 km/s, as do the individual velocities of stars with respect to their galaxies. Taking a Hubble constant of 70 km/s per Mpc, we see that we need to be at distances of 15 Mpc before Hubble recession velocities ($v = H_0 d$) become large compared with peculiar motions. At these distances we cannot observe individual stars - they are too faint and unresolved from the bulk of the Galactic light. The exceptions are supernovae. The redshifts of individual supernovae, that briefly outshine their galaxies, can be measured right across the universe. Here you are correct that the measured redshift is a combination of cosmological redshift due to the expansion of the universe and a velocity of the star relative to the Hubble flow at that distance. There is no way to distinguish between these two unless velocity measurements could be obtained for other objects in the same galaxy. Given the rarity of supernovae, we might wait a long time for this. But does it matter? Even if we look at a "low redshift" supernova at $z=0.1$, its Hubble recession velocity is 30,000 km/s and far in excess of any peculiar velocity contribution at the level of $\sim 1$%. 

The material at the surface of a white dwarf is not degenerate. The "visible" surface is defined as where the optical depth exceeds some threshold and this will occur at a low enough density that even at a few hundred kelvin, the ratio of the Fermi energy to the thermal energy is too low for significant degeneracy. In addition, at these temperatures, the electrons attached to atomic hydrogen and helium would not be ionised and the material would mostly be in atomic and molecular states. In terms of appearance a reasonable comparison would be with that of the very cool T- and Y-dwarfs; although some of the more complex chemistry that occurs in those atmospheres would not occur in very cool white dwarfs because the trace quantities of heavier elements would probably have sunk out of the atmosphere.