If you are generating the primary key using an Oracle sequence (PS: you should), you are guaranteed to get integers so you don't have to have Oracle checking precision, etc. if you specified to specify an integer with up to seven digits. And then if you ever exceed 7 digits, not a fun day to fix that. As the links you've referenced above mention that Oracle only uses enough space to store the number, no need to worry about how much storage is being used; and therefore you don't have to concern yourself with indexing of the columns holding the primary and foreign keys. The only time I would specify a fixed length integer would be if the primary key was an intelligent key such as a USA social security number, but intelligent keys are a bad idea too. 

What appears to be happening is index statistics are not being gathered (or not gathered correctly) on 10.2.0.5; if specifying doesn't help, you may want to consider gathering the index statistics separately via . You might also need to set a higher value for estimate_percent. You may also want to ensure you have set system statistics (which tell the optimizer how fast your disks are, how much CPU you have, etc.) via the and procedures. I think that 10.2 is more sensitive to system statistics. 

You should be fine; the would be a show-stopper if you were using it Temporarily, there would be "holes" in the data files, but Oracle will fill them in without any intervention. Ensure you have automatic tablespace management enabled. You can invoke that command but no need to; let Oracle take care of it The person that stated that probably meant there was a one-time hit, or he didn't know what he was talking about, or using an old version. You tested; his environment may have had a rat's nest of views. 

I have Oracle 12c installed on two servers. On server I have an instance that hosts an RMAN recovery catalog. On server I have two instances, and . I can use the following command on server to connect to my catalog, and target of , but when I try to connect to it gives an 

As long as the developers only have that role in the development database instance, I see no problem; far better to tune now than later. Now, if your development machine has limited resources, there may be a reason to limit access. But in general, usually tuning tasks run from a few minutes to the default maximum of 30 minutes; during that time, Oracle may try parallel queries (if enabled), so that could be an issue, but you can play with the instance parameters and to minimize the impact. There are no changes between Oracle 11g and 12c that would affect your decision. 

Then find your trace file, and run tkprof on it, and the output of tkprof will show you execution statistics (logical reads, physical reads, etc). And you can look at the actual trace file if the tkprof output doesn't help. If that doesn't help, you can enable 10053 event, and enjoy wading through thousands of lines of output; but sometimes you need to do that. But first things (well easier things) first; do as others suggested; see if table statistics are up to date, optimizer settings, etc. If you are using the exact same criteria each execution, could be the optimizer is "on the border" on plans. I would ensure you have collected system statistics; see the documentation on dbms_stats.gather_system_stats pl/sql. 

Balaz Papp's comment above is correct in the explanation why connected without a connect string is working. And his comment below is correct; any user could be granted query access to the view, when connected to the regular Oracle database instance. 

The includes a variety of files, including online AND archived redo log files; take a look at the Oracle Backup and Recovery User's Guide chapter 5 to find out the best solution; disabling is not a good idea. You can try 

Also, are all the bind variables always populated with a value, or are sometimes some of them null? IE: if and are often null, you might be able to do something to your query like 

All the comments above are appropriate. If you provide the details on the data types and any not null constraints of the columns in question, then we can better determine whether your function-based approach is appropriate. I would also gather statistics on the indexes and look at the unique values, etc. You can compress your baseline index too, if say you have very few unique values for company and hire date, you can compress by two levels. In fact, you can do the following to see if index compression will help: 

To supplement Robert's answer, if after you create a new UNDO tablespace, and are unable to drop the old one, after trying those queries with no results, and the period has elapsed, you can try this: (It worked for me when querying showed unexpired status for some rows): 

Some might say depends how big the databases are; I think regardless of the size, accessing the databases directly instead of having a "local" mySQL database to query would be best, for the following reasons: 

And as others have mentioned, processes are interrelated with sessions, so if you set one, you probably need to set the other. 

But in general, I would say a larger blocksize won't help for most environments; and for those that need it, it would be a minor improvement. Derek's comment above has good things to look at; if you post the query and the explain plan, we can provide better advice. Based on the query and explain plan you provided, you may want to try a composite index; the order of the columns below is assuming there are fewer unique values for MATCH_CODE_DATE_OF_BIRTH than for MATCH_CODE_INDIVIDUAL: 

It could be that one of the default parameters for Oracle 10G is different in Oracle 11G. I would try adding the 

I agree with EdStevens' comment, but here are some other ideas: The AskTom website has a script that will list exactly how much you can shrink the existing data files. If that doesn't yield much savings, you can find the biggest indexes on that tablespace and rebuild them; that is a temporary solution and the indexes would eventually get big again, but if budget doesn't allow more disk space now, try this query: 

Query V$SESSION and include the column which contains the value you are looking for. The link $URL$ defines all the column meanings for that dynamic view; I would probably do a query like 

? If you are using the first option, is it prompting you for a password, and are you entering the password for the user? Did you create a local Windows user account as the user, not the user id used to install the software? Oracle recommends a user with administrators group install the software, and a local Windows user be defined for the sys and system accounts. 

There is a built-in substitution string APP_PAGE_ID and APP_ID, so you could do this in your page to use the current page ID and application ID to get the page title: 

If you are mostly loading the data, and seldom querying it, then no need to split the table. Any "home-grown" attempt at splitting the table is bound to cause grief; use Oracle's (extra-cost) partitioning option if your queries are usually date-ranged; but be careful; partitioning does not necessarily improve performance. It can if the queries include critera that are the same as the partitioning criteria. It does come in handy for archiving and purging old data; if your business rules are such that you don't need to retain the charge details beyond say 3 years or whatever. With some appropriate indexes, even without partitioning, you should get good results querying millions of rows. You do not mention whether your users (or the loading process) are complaining of long response time or load time. It will come down to monitoring the database using tools such as the Automatic Workload Repository to determine where your bottlenecks are, and how to resolve them. If you are looking for future issues, look at whether the charge detail data can be summarized after xx years, etc. and purge old data after it is summarized.