It is a trade-off between benefits. Large settlements allow more efficient collective action, trade, etc. It is easier to dig one water well than 10 water wells. If I want all settlements to be fenced, say for protection against local wildlife, then it is 1/3 the effort to fence one large settlement versus fencing 10 smaller settlements. (Because area enclosed goes up with the square of the fence length). A single large population allows market fluidity: In ten settlements of some distance apart (to allow for growth), you need more doctors, so simultaneous illness or emergency in multiple settlements can all get the attention they deserve. You just need fewer of each kind of specialist in a single large settlement, and this in turn allows for more types of specialist: If you have three doctors in one settlement instead of ten doctors in ten settlements, then the other seven people that could have been doctors can be researchers, teachers, or of some other profession that utilizes their intellectual capabilities, bettering the community as a whole. But there are drawbacks to a single large community. Diseases can infect everybody. A fire can devastate or kill more people. A flood or drought can wipe everybody out. Basically there are many problems with a large concentration of people in one place, including getting supplies (food and water) into the place, and waste out of it. Sewage and garbage disposal presented a large problem when small villages became larger cities, sanitation and running water became more difficult than when everybody had their own well and their own piping. Power generation and distribution can become a problem, too. In many smaller villages (historically on Earth) the solution to pollution was dilution. There was enough air, water and fields that waste and garbage wasn't a problem. But as they grew larger, the distance to disposal increased and crap piled up (literally). It is easier to grow a small village (or start another small village) than it is to grow a large settlement, because growth of a large settlement serves to make the center of it increasingly barricaded and distant from the edges, where we need to go to dispose of waste (if we aren't going to dump it on our neighbors). You have a trade-off between the complexity of a large settlement and its advantages, and the simplicity of small settlements and forgoing those advantages. 

I tend to think for warfare, large swarms of smaller ships would be better. There are many advantages to many smaller ships. For the sake of comparison; let us say we are talking about the same total tonnage of hardware; say a battleship or a fleet of 100 smaller ships that collectively weigh the same as the battleship. Note first, this can be more expensive, but the expenses add redundancies that are an advantage. For example, you now have 100 engines, and 100 armament systems. You have infinitely more options in battle: A battleship is in one place; you have 100 ships you can arrange however you want. A battleship is hard and slow to move; 100 ships can be reconfigures into a front, a spear, with a mile between them (very easy communications distance even in poor weather) they can present a 100 mile wide search front. Unlike the battleship, you can sacrifice 10% of your ships to eliminate an enemy. A single torpedo, missile or Kamikaze bomber could sink (and has sunk) a battleship, or loaded with appropriate incendiaries start an inferno deck fire that disables the ship. But a Kamikaze doesn't sink a fleet, at best it sinks 1% of it: You have resilience. Finally, big ships tend to have centralized resources and supplies of food, fuel, communications support, etc. A fleet of smaller ships will tend to result in decentralization of these resources and supplies, which is a good thing: You have backups, and therefore (again) resilience. There are parallels IRL; in my field the modern supercomputer is actually a network of tens (or hundreds) of thousands of off-the-shelf Intel processors, the same ones you can put in a desktop or laptop machine. The chips themselves are typically up to 64 independent processors. If any fail to function, the controlling software can just not use that chip, we mark it as offline and get on with the day's work. The servers hosting this site are probably working exactly the same way: If a node fails, it is marked as offline and not used until a tech repairs it. A swarm of "small" ships (processors in a supercomputer) can do things impossible to do with a single large ship. I haven't forgotten we are talking about space; all of these translate, with a few substitutions, to the space scenario. There are some practical disadvantages; it is slightly more difficult to transfer personnel, move supplies and fuel around, etc. It probably requires more fuel. Storms that a big ship could ride out could sink some smaller ships (less of a problem in space; but translate perhaps to space debris). Overall, by having 100 hulls, 100 engines, etc, the interior livable space could be much tighter than if all that material was used to enclose one giant space. The small ship fleet is not for the claustrophobic! But we are still talking about, say, 400 ton ships, so it isn't just a pilot's seat and a few jump seats. Think more like Submarine quarters; the crew can still sit around a table to eat or play cards; you still have a galley to cook in. A battleship has a crew of 2700, so the small ship crew is on the order of a squad of 27 or so people. Of course you can have highly trained specialists scattered throughout the fleet, electrical and mechanical engineers for example; you don't have to provide one for every ship. I wouldn't rule out very strong network communications throughout the fleet and virtual reality or virtual presence. Small does not have to mean primitive. Although each ship should probably have a ship-runner (person in charge), the chain of command doesn't have to be self-contained; e.g. personnel can report to both their ship-runner and a fleet-wide executive that is on another ship, and take their orders from that executive. A ship-runner may only be in charge in situations where the fleet-wide executive is offline, dead, or the ship's comm systems are disabled. I think there are far more advantages to a fleet than there are to a monolithic ship, and the disadvantages of a fleet are manageable. Rationally speaking, I think I'd opt for the small ship fleet and the resilience it offers: If 50% of my big ass ship is blown to smithereens, I am probably dead. If 50% of my fleet is blown to smithereens, I can still win this thing. 

You are incorrect. There is no "terminal velocity", that is a term used with air resistance (or it can apply to fluid resistance) when the amount of acceleration (due to gravity) is matched by the resistance of the medium through which something is falling; like air. Pointed objects like arrows will have a higher terminal velocity than something flat. A skydiver in "dive" position will move faster than a skydiver in "spread-eagle" flat position; their terminal velocity depends upon the amount of surface area that must push air out of the way. However, in space, there is no resistance medium; but there is also no automatic propulsion system. Objects can be attracted by gravity, but otherwise will just continue to move at whatever speed they were moving; or continue to just sit there if there is no gravitational gradient and no initial motion. They do not automatically accelerate at all. Sand and dust, just sitting there, can be a problem for anything that is traveling at high speed and hits them: That is the equivalent of them traveling at high speed and hitting an object at rest. Since high speed does increase the energy of such a particle, you are correct in intuiting they could cause damage: But it is not because everything in space accelerates. 

First, as a professor involved in AI research, I will affirm it can be intelligent without a sense of self, and with or without a sense of consciousness. Intelligence is just the ability to extrapolate from the current time to how things will work out in the future, or from pieces of evidence how things must have happened in the past. In a human this may require memory, imagination, simulation of physics, politics, society, other humans and their emotions. The more accurate this extrapolating ability, the more intelligent: This is why we admire Sherlock Holmes (and all the fictional detectives like him): He can take the tiniest of clues and figure out a crime. This is why we admire Chess grandmasters and champion Poker players: They have to see "what will most likely happen" with greater accuracy than their opponents or they don't win (a little luck is involved with Poker players, but skill is required). Plus they generally play against other humans, so they need to understand at least how humans react in this circumstance. An AI that informed politicians can't just say "Vote Yes" or "Vote No." To be truly intelligent, it must also know to whom it is responding. Is it a die-hard free marketer that believes all government regulation is bad? Is it a die-hard communist that believes all income disparity is bad? If the AI is an advisor and has no absolute power, then in order to influence the politicians with its advice it must correctly reflect and explain what they truly care about. In some cases, this may actually be their constituents, their country as a whole, or the world as a whole. In most cases (IMO) it is their own self-interest, which includes self-enrichment (money, property, fame) which is enabled by the power of their office to regulate the rich, and that requires re-election. So to them what their constituents want only matters to the extent they need to keep their office, and making money is the whole point of holding office. To them it would not be worth the hassle (or risks of fame) if they were forced to do their job without getting rich at it. Your AI would have to figure out how to convince such people that voting Yes, voting No, or abstaining was in their best interest; meaning it would help get them re-elected or would make them (personally) more money. So ..... Can it lie? It does not have to be self-conscious to do that; it may compute that some particular lie has a better chance of influencing a politician than telling the truth. As an AI and good predictor, it may conclude that humanity would be better off if politician Abel lost his re-election campaign, and then devise a plan of disinformation to convince Abel to engage in all sorts of self-enrichment (that Abel is already keen to do) that results in scandals and Abel being booted from office or perhaps even imprisoned. Let us stipulate for the purpose of argument that the AI is totally right: Abel is a corrupt criminal, and getting him out of office (at any cost to Abel) really is, on balance, better for humanity. Let us also stipulate that the only way the AI can do this is to mislead Abel by telling him what he wants to hear. Should your AI lie? That scenario is a specific version of a bigger issue, rife with known philosophical paradoxes; many of which come down to assigning values to human lives and human morals in some circumstances. For example, most people (the 95% that feel sympathy for the plight of others) agree that imprisoning an innocent person is wrong. The American justice system, in reflection of this sentiment as stated, often errs on the side of innocence. It demands a high level of proof of guilt, thus letting those that are actually guilty --- even when cops and detectives and eyewitnesses are personally certain they are guilty --- to go free, to harm again. Even in cases of rape, murder, and fraud that bankrupts people and leaves them penniless. Of course most people could flip the sentiment and claim that the government should protect its citizens from as much crime as possible. That would probably result in a justice system that erred on the side of guilty with a much lower burden of guilt; just a "preponderance of evidence" should be enough to send somebody to jail for life. So put aside all questions of corruption. Your AI cannot be internally corrupt! If the AI accurately computes, say, a 75% probability that some individual person is guilty and will do more harm than good to others, why should it wait until that harm is realized before it neutralizes the person by incarceration? Of course it wouldn't be just one person, but the principle holds: What is more important, The 3% of people that are more than 75% likely to be harmful to others as frauds, cheats, criminals, rapists and murderers, or the 97% that are most likely to do more good than harm? I disagree that what is "best for humanity" is difficult to define; it isn't. Less misery, hardship, despair and predation by other humans. More joy, love and happiness. "Progress" can certainly help that, progress in medicine has certainly done that, progress in other technologies too, including AI. But at the bottom, what is best for humanity is rooted in maximizing the positive human emotions shared by the majority of humans, and minimizing the negative human emotions shared by the majority of humans. (Note that maximizing the positive human emotions naturally means more humans if their existence on balance increases the sum total of positive experiences and/or decreases the sum total of negative experiences.) An AI dedicated to that principle, however, must play the odds, and this is inconsistent with Democracy. It is inconsistent with always telling the truth to people you suspect or know are criminals. It is inconsistent with "innocent until proven guilty" and instead demands a standard of "reasonably sure of guilt" that will also undoubtedly punish the innocent. My Answer An AI dedicated to the betterment of mankind that is a super-intelligence (and therefore able, with a high degree of accuracy, predict the outcomes of various political policies both generally and specifically for politicians) will tilt the government and policy toward a more probabilistic approach to reducing crime with much less emphasis on Rights and Proof. Not with any malicious intent, but actually to increase the overall chance of people living happy lives without being abused, robbed, defrauded, physically assaulted or raped or murdered, poisoned by careless corporations, exploited by them, or forced to live in poverty to protect the property rights and wealth of others, and so on. This tilt would occur despite the fact that it would also be creating some misery by incarcerating a larger number of innocent people, due to its focus of erring on the side of caution: It would also be incarcerating a larger number of actually guilty criminals that, in our current system, go free to harm others again. Trying to simultaneously maximize one kind of experience while simultaneously minimizing another can be seen as trying to maximize the gap between them. This does not necessarily mean the minimized version is zeroed out, it could actually be raised versus a system that only tried to maximize or minimize a single thing. Say for example a woman is murdered and the only person we can find with both motive and opportunity is her husband. We know they fought, we know he gets rich on insurance and inheritance, we know he was (and still is) having an affair, but despite honestly diligent effort by investigators we can't find any proof at all that he did the deed. 75% (9 out of 12) jurors, after hearing the inconclusive evidence we have, vote to convict him. It is probably better for society (and his girlfriend) if we just incarcerate him on that. He might be innocent, but is most likely a danger to society. Minimizing the chance of sending an innocent to jail does not minimize crime or misery.