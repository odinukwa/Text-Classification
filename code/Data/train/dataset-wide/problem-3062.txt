The basic idea of all Project Euler problems is to find an efficient solution to the problem, by knowing enough math to simplify the computation your program must do. You can brute-force all of these with enough computing power, but there is a way to reduce the complexity such that a program that solves the problem will give you an answer in under one minute with a 5-year-old computer. The brute-force answer would be to start at 21 and try every number in order until we found our answer. That's not a great idea, as the answer is quite large, and while, with a powerful computer, you might arrive at the answer in good time, brute force simply won't work in other Project Euler problems (you'll exceed hardware/firmware limitations, or end up with an exponential algorithm taking longer than the estimated heat-death of the universe to compute). In this case, with a few smarts, we can break down the problem and work with smaller numbers, thus taking fewer steps. You want to know the smallest number divisible by every number from 1 to 20. Well, for one number to be divisible by another, all of the prime factors of the divisor must be present in the prime factorization of the dividend. For instance, 20 is divisible by 10 because 10's factors, 5 and 2, are both present in 20's factorization of 22*5. The quotient is the product of the remaining prime factors; in this case there's only one, 2. Now, the solution of the problem becomes more apparent; the smallest number divisible by every number from 1 to 20 is the smallest number that contains all the prime factors of every number from 1 to 20. So, the solution is to find the prime factorization of every number from 1 to 20 and keep a count of how many of each prime factor is necessary. Prime factorization is technically an inefficient process (you have to start with the number and divide by every known prime number until you get a whole-number quotient, then repeat from the beginning with that quotient until you're left with 1) but the numbers are so small it doesn't matter. The solution is the smallest number for which the prime factorization of every number from 1 to 20 is a subset of its own prime factorization. At the least, you'll need one of every prime number from 1 to 20. You will also discover that in order to be divisible by 16, the number must have 4 2s in its prime factorization, and to be divisible by 9 and 18, the number must have 2 3s. The smallest such number is the number that has ONLY the needed factors, so the answer is 

Personally if I were doing this myself I would set up a factory that could produce various reports based on input identifying the type of report to produce. Tigran's idea of a Dictionary of delegates is an excellent one. 

First red flag; you're using . The Count() overload that takes a predicate must iterate over all elements of the parent enumerable to determine the correct count, and you're telling it to do so for each element of the child list. You simply want to know if at least one of those elements matches; for that, the Any() overload which accepts the same parameter will be faster, because it will quit as soon as it finds the first element that matches (unfortunately the worst-case of a match not existing will still require a full scan of the enumerable). You're also parallelizing parallelization. While parallel is good, threads waiting on threads waiting on threads can easily result in an algorithm that is bound by the ThreadPool's ability to spin up new threads (the default is a 250ms wait for each new thread after a predetermined number of "readily-available" threads have been scheduled). So, after creating, let's say 10 threads for this algorithm, most of which will be waiting on sub-threads, the ThreadPool will only create four new threads per second. Let's say the TPL begind the PLinq library thinks that one thread per 100 elements of both collections is necessary. On a collection of 4500 elements, for each of which a comparison of 4500 more elements is necessary, the ThreadPool could schedule two thousand worker threads to execute. After 10 of them are created near-instantaneously, only four more per second will be created as long as at least 10 are running, meaning this algorithm will take up to 500 seconds just to schedule all the necessary threads. The more "instantaneous" threads you tell the ThreadPool to spin up before queueing them, the more cache-thrashing you'll do forcing the CPU to juggle all these threads. First step to improving this: Instead of grouping, select a list of distinct child IDs from the parent list. This process will produce strings instead of groups of larger ParentItems, which should hopefully reduce the amount of "heap-thrashing" required to generate the groups only to reduce it down to a much smaller collection. You only ever need the child IDs from the parent list anyway. 

The implementation you have basically splits your array into 3 equal sub-arrays, and each stack can only use the elements of its subarray. It would be roughly the same as setting the first index to 0, the second index to array.length/3, and the third to array.length*2/3. Instead, you're working based on modular congrence; the first stack uses indices where that have a modulo by 3 of 0, the second stack 1 and the third stack 2. "Improving" it depends on what you want to improve; space efficiency or performance? You could create a stack for which any number of unused indices could store values of any one stack, by implementing two "slide" helper methods: 

The first major problem I see is that you're not considering when incrementing and vice versa. This can cause L and R to cross, which will cause additional unnecessary swaps: 

I'm going to assume that the != in the main pivoting loop is a typo, because combined with the lack of checking that the positions don't cross, the positions would be incremented beyond the ends of the array and the whole thing would error out. and should NEVER cross. In addition, even if they don't and you miraculously end up with both leftpos and rightpos on the same element (which would have to be the pivot), you swap that element with itself unnecessarily; don't do that. Finally, in your code the "left half" call always starts at index zero; that means that the left-half call that branches from all right-half calls will scan through the entire left-hand side of the array, not just of the half it should have been given. Because the left half is sorted first, there should be no swaps, but it still has to scan through all those elements, which makes the right-hand half of the algorithm approach O(N2) complexity. Try this: 

The program to find this number will start at 2, iterate to 20, and for each of those numbers it will divide by every known prime less than that number (technically any prime less than the square root of the number but you won't save much that way) to determine their factorization. You will find the numbers that are prime as you go (they won't be divisible by any lesser prime, by definition). Any number that requires more than one of a particular prime factor should be tracked, and the maximum number of each factor remembered (I'll tell you for this problem that only 2 and 3 will require multiples). Then, simply multiply the necessary number of the necessary factors to produce the answer. If this sounds like a lot, meh; the computer can whip through it pretty quickly, and it will take far less time than counting multiples of 2520. As a point of reference, the correct answer, if you didn't peek, is 5 orders of magnitude greater than this number, requiring about 10,000 iterations * dividing by 20 numbers on each one = 200,000 steps, versus a worst-case of 20 (number of divisors) * 8 (primes less than 20) * 7 / 2 (naive worst case of every number <= 20 being divisible by every prime <=20; not possible but the result's still nowhere close) = 560 steps to find the prime factorizations of the first 20 numbers. 

So I had this idea; what if I could decorate my Enum values with an attribute that cross-references a "default" value of another Enum that should be used when a variable of the current Enum type has this value? The desired usage would look like: 

The upside is that we quit as soon as we know the answer to the question (is the tree unbalanced?), which will increase the average performance (but not the worst-case performance on a tree that is balanced or is unbalanced at its furthest extremities). The downside is that we no longer know how unbalanced the tree is; the MaxMin returned to the top level will always have a Max and Min that differ by the first detected difference greater than the threshold (probably 2), not the absolute difference in depth of leaf nodes in the tree. The one case that is difficult to determine in an n-ary tree is that of a tree that never forks. This algorithm will find a depth difference between any two or more branches, but when there is only one branch, the maximum and minimum depths of the tree are the same and there's nothing to use for comparison. Technically, it would have only one "leaf" node (usually defined as a node with no children) and so by the given definition all (one) of the leaves are the same distance from the root, however if you looked at the map of an N-ary tree that was more like a linked list (or even a V or Y shape) and went deeper than two levels, you wouldn't call it balanced. Perhaps a change in definition may be required; a node is a "leaf" node if it does not have N child nodes (where N is the order of the N-ary tree). So, in a quaternary tree (4 children per node), if any node has fewer than 4 children, then the depth of the current node is considered as a "minimum" depth and will almost certainly be the minimum depth at that level. That's another easy change: 

Well, the first thing I'd do is put the patterns in a list, and loop through matching them. The second thing would be general cleanup; use namespaces where you can, one-liner if/else clauses don't need braces, use var where the type is obvious, etc etc. This gets rid of that if-elseif structure and makes the code much more concise. As far as merging the patterns, you could match on a combination of two: 

First off, I would use something other than a LinkedList to hold the children of an n-ary tree. A LinkedList requires you to enumerate the list in order to get the reference to any child other than the leftmost, which is going to slow you down. Second, your code is basically returning false in any situation where the Node's parent has one child that itself doesn't have any children. In your example tree, that's the exact case (you have two children of the root, one of which has another child) but the tree is balanced according to the rules, so I would expect this algorithm to return a lot of false positives (and negatives). Finding the depth of an n-ary tree is a linear-bound operation; you traverse each branch of the tree recursively, finding the depth at each leaf node. You can find both the maximum and minimum depth in one traversal; at each level, ask each branch for its maximum and minimum depth, passing the depth of the current node. The base case is that of a leaf node (no children); the min and max is the depth of that leaf. You can store this result in a Tuple or in a more specialized MinMax struct, as you please. With those results returned to the calling level, scan them to find the lowest Min and the greatest Max, and return that to the caller. Here's a basic implementation: 

In your particular situation, this would drop right in, which IMO makes it better than Scroog's because his requires the user to put the conditions into IEnumerable format. The keyword is beautiful that way. This function (like many answers) assumes that the desired State will always be the concatenation of a big-endian bit array of the conditions, in order. 

I'd A/B the above algorithm against the one you already have; you should see at least some performance increase. Notice that although this may well be faster as it does exactly what you want and doesn't see if you want to do anything else, it uses more LOC to achieve the same result. This is Regex's real power; powerful string analysis with very concise code.