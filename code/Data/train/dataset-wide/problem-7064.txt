It sounds like you already had (1) and (2) and were just unsure about what exactly to do in (3). If you're working in a deterministic environment (and from the HJB it looks like you are), no discretization on your part is necessary: you can just use a canned ODE solver from Matlab or any other software package to numerically solve the differential equation above for $k(t)$. To be a little more specific: 

I'm not sure that there is any convincing answer, but most answers I've seen take the form "some other form of cyclical heterogeneity offsets the differences in Frisch elasticities across groups". For instance, the most cyclical industries have historically been construction and durable goods manufacturing, both of which predominantly employ males. (See this BLS report for the effect of industry cyclicality on the gender composition of the labor force in the most recent recession.) If industry effects are strong enough, they may overwhelm males' much lower intertemporal substitutability of labor supply and cause male employment to be more cyclical. Note that although this explanation sounds plausible at first glance, it raises many other quantitative problems for business cycle models. For instance, the explanation only works if labor supply cannot easily be reallocated across sectors; otherwise the needed reduction in manufacturing and construction employment could take place via men there switching to other sectors, and then women in those sectors exiting employment. This is reasonable enough: in the short run, labor supply probably isn't too elastic across sectors. But once we acknowledge this fact, the quantitative puzzle of low employment in business cycles becomes much more severe. For instance, if most employees in construction are stuck in construction in the short term (so that it makes sense to think about labor supply elasticities for the construction sector on its own), then the puzzle of how construction employment fell by 30% while real wages actually inched up in the recent recession becomes extremely hard to resolve in any frictionless labor model. So ultimately, yes, this is very difficult to resolve on its own; the obvious industry-oriented explanation introduces new and even more insurmountable problems. 

(Note that this answer implicitly makes reference to the specific model in Lee and Saez.) Short answer: the increased taxes on high-skilled workers exactly offset the higher real wages they obtain from a decline in the minimum wage for low-skilled workers. Longer answer: Suppose that I'm the government, and I decide to lower the minimum wage $\bar{w}$. The direct effect will make low-skilled workers currently earning the minimum wage worse off, while making high-skilled workers better off. (The fall in low-skilled wages means an increase in high-skilled wages.) Furthermore, a lower minimum wage means that fewer low-skilled workers will be rationed out of the labor market. These workers will be better off. As the government, I want to turn this into a Pareto improvement - which it currently isn't, because workers who are already earning the minimum wage are hurt by the decrease. I try the simplest possible offset: I adjust taxes so that everyone's after-tax wage rate is exactly the same as before. The combination of this tax rate change and the minimum wage decrease means that everyone's welfare is unchanged, except for some low-skilled workers who were previously unemployed and now can work. These workers are better off - hence overall we have a Pareto improvement. Great! There's only one catch: I didn't verify that this policy was feasible for the government. Maybe the proposed change in tax rates would violate the government's budget constraint. This is where some slightly more involved logic comes in, and it's useful to think about the infinitesimal case for simplicity. Before the policy change, output is produced by a mix of low- and high-skilled workers, who I'll call the "old" workers. The minimum wage increase leads to additional low-skilled workers entering the mix; I'll call them the "new" workers. These new workers produce additional output and earn income. At the margin, though, (pretax) wage equals marginal product - so when an infinitesimal number of new workers is added, they increase output by exactly as much as they draw away in earnings, and the total earnings of the old workers are unchanged. There's a redistribution in earnings among the old workers, toward the high-skilled and away from the low-skilled, but this is just a zero-sum redistribution, and the government can use taxes to reverse a zero-sum redistribution while maintaining a balanced budget - so this part is fine. All we need to worry about now is the budgetary impact of the new low-skilled workers. But this is simple: we're initially taxing low-skilled work at some rate $\tau_1$, and the net budgetary effect of more low-skilled entry will be positive (even after the infinitesimal decrease in $\tau_1$ that's part of the reform) as long as $\tau_1>0$. New workers are good for the budget as long as their tax rate is positive. So that wraps it up: we see that there is a potential Pareto improvement involving a decrease in the minimum wage and an offsetting change in tax rates, and this policy change is feasible as long as the initial tax rate on minimum wage work is positive: $\tau_1>0$. Lee and Saez emphasize this point (in conjunction with their other results) to demonstrate that in their model, the minimum wage can only be rationalized as a policy that's complementary to a labor subsidy $\tau_1\leq 0$. They really like this purported complementarity, and it's one of the most popular current arguments in favor of minimum wages. (I happen to think that the way they prove this complementarity, which relies on the assume-a-can-opener assumption of efficient labor market rationing, is incredibly silly. But that's a different part of the paper, not directly related to your question here.) 

Of course, there is one other key fact used above, which is $\partial e(p,u)/\partial p_i = h_i(p,u)$, which for $w=e(p,u)$ becomes $\partial e(p,u)/\partial p_i = x_i(p,w)$. This is best viewed, instead, as a direct consequence of the venerable envelope theorem. ($\partial v/\partial p_i$ can also be derived from the slightly more advanced version of the envelope theorem, where constraints as well as the objective are allowed to depend on a parameter. Since varying $p_i$ in the utility maximization problem changes the budget constraint rather than the objective, the envelope theorem says that its effect will depend on the Lagrange multiplier on that constraint, which is the marginal utility $\partial v/\partial w$ of wealth. This is a good intuition for why the expression for $\partial v/\partial p_i$ is more complicated than the expression for $\partial e/\partial p_i$, picking up an extra factor.) 

Given this tradeoff, the committee honed in on a long-term target of 2%. It is interesting to consider all the issues that were not present in this discussion. The most conspicuous is the usual question of redistribution. This is because, although borrowers benefit and savers lose from unanticipated inflation, fully anticipated inflation should be incorporated into nominal asset returns (except for the tax issues discussed above). Since anticipated rather than unanticipated inflation is the relevant issue for setting a long-term inflation target, this was not such an important question for this debate - though, of course, it still mattered for the transition to any new target. Another cost of inflation commonly cited by academics is the distortion from departing from the Friedman rule of 0% interest rates: since the central bank can more-or-less costlessly engage in open market operations to buy securities and create more money, a positive interest rate means that individuals hold a less-than-socially-optimal amount of money in their portfolios. This cost was presumably ignored in the discussion because, in a low inflation economy, it is of trivial magnitude - not many people make decisions about how much cash to carry in their wallets based on whether the interest rate is 0% or 4%. Indeed, the cost here may be nonexistent, since a large amount of cash is held either outside the US or in support of illegal activities - and it is optimal for the US government to tax the former and discourage the latter. 

The paper is assuming that some form of "law of large numbers" (LLN) applies for the continuum. The expected value of capital for an individual agent is $$\mathbb{E}[R\cdot 1_{R\geq R^*}]=\int_{R^*}^1 R\,dR\tag{1}$$ The LLN assumption says that when we have a mass-1 continuum of agents, their actual total will equal their expectation in (1). What justifies this assumption? Imagine that rather than having a continuum of agents, we simply had a very large number $N$, with each of these agents still having $R$ drawn from the same uniform distribution on $[0,1]$, resulting in an expected value of capital given by (1). Then as $N\rightarrow \infty$, the standard law of large numbers implies that the sample mean of capital will approach the value in (1). If, as we take this limit, we normalize each agent to have a size of $1/N$, then the sample mean will equal the total amount of capital - so in the limit, the amount of capital will always be (1). Here comes the weird and not-entirely-correct thing that economists do. We often say that having a mass-1 continuum of agents is like taking this $N\rightarrow \infty$ limit (after all, there is an infinite number of agents in the continuum...). We therefore infer that the total value of capital in the continuum will equal the expected value given in (1). Like I said, this is not entirely correct or rigorous. Judd famously complained about it, pointing out that if we take a continuum of agents each with independent random values, then the resulting function will usually not even be Lebesgue integrable, so we can't even define the aggregate value in a rigorous way. Al-Najjar recently suggested that we replace the continuum assumption with a discrete construction that actually implements the $N\rightarrow \infty$ limit. There are some other suggestions out there too. Most economists, though, just ignore these technical concerns and continue to assume that when you have a continuum with independently distributed values, you get the expected value when you integrate. I don't mind this, since it's just a convention, and they are right in spirit, since a continuum realistically is intended to model a very large number of agents. (This issue came up once with a coauthor, and after reading Judd and Al-Najjar and others for a few hours, we just threw our hands up and decided to ignore the problem! My personal, technically acceptable way to resolve it is to say that agents - rather than living on a continuum - live in the probability space itself, and define aggregation over agents as probability-weighted integration over this probability space. But this is pretty technical and obscure, and the basic point is that 95% of economists ignore these issues.) 

There was only one reason to ever think that nominal interest rates couldn't go negative, which is that the nominal return on both forms of base money (electronic reserves, and paper currency) had a floor of zero -- and investors wouldn't accept a below-zero nominal return when they could get a higher one by holding base money. But for electronic reserves, there's certainly no practical need for a floor of zero: the central bank can easily pay negative interest on reserves. (For instance, the Swiss National Bank is now paying -0.75% on most "sight deposits", which is their term for electronic bank reserves.) It's quite easy: they just tell the computer to charge a certain amount of interest on reserve accounts, just as they would normally tell it to pay interest. The potentially serious problem is paper currency, which by its very nature generally pays a nominal interest rate of zero. (If you have a \$5 bill, it will still be worth \$5 in a year - zero nominal interest.) The concern is that if we try to set nominal interest rates below zero, people will just massively switch to paper currency paying 0%, and the traditional financial system would disappear and be replaced by paper. This is where we reach a very important point, which is that return isn't all that matters in an asset. Other factors, like convenience, play a role as well; different assets are convenient in different ways, and aren't perfect substitutes for each other. This is why in normal times, people are willing to hold paper currency even though it pays less than electronic accounts (because paper is useful in some ways that electronic accounts aren't); inversely, it's why in these abnormal times, when Swiss cash pays more than Swiss deposits, investors don't flock exclusively into cash. Hence, when the SNB pays -0.75% on electronic reserves, rates in Swiss-denominated money markets (like the SARON overnight rate or LIBOR CHF) also drop to near -0.75%, and not everyone converts their electronic assets into paper. Indeed, there has been very little response on that front: if you look at SNB balance sheet column 16, banknotes in circulation, it hasn't budged very much thus far since negative rates began in December and the rates fell to 0.75% in January. Again, this is just a matter of imperfect substitutability: paper currency simply doesn't have the same kind of transactional and liquidity value that electronic accounts do. (And then there are the costs and risks of paper currency storage, as mentioned in the article you cite.) This is the feature of the world that wasn't adequately captured, before recent events, in the simple zero lower bound models used by many economists. The one quasi-mystery, to me, is why banks haven't stockpiled paper. After all, even though an electronic account is more useful than paper to an end user, this doesn't apply to a bank that's already overflowing in electronic reserves. (Indeed, conceivably banks could be large-scale intermediaries here, stockpiling paper to back electronic deposits held by their customers - effectively transforming paper into a more convenient asset.) Yet the quantity of banknotes held by Swiss banks has remained low (see the 4th column at the bottom of the 2nd page here). There are two possibilities here. One is that the logistical costs of holding cash are still higher than the 0.75% spread, so that it's still not worthwhile for banks to hoard. Another is that banks could make a profit by embarking on some large-scale cash hoarding program, but they know that the central bank would quickly get angry and create new rules (like a 0.75% tax on excess cash holdings by banks) to stop this, so they don't bother -- and indeed, maybe there has been under-the-table pressure from the central bank already! Ultimately, when the government has the authority to regulate financial institutions, it can always disallow organized cash hoarding schemes that threaten the negative interest rate policy, so none of this is a long-term threat. The real issue is cash hoarding by end users - and there, the disadvantages of cash apparently still loom large at -0.75%. 

Free trade is, on the whole, one of the few otherwise controversial policy topics on which economists have near-perfect consensus. Historically, this consensus has long been strong in the English tradition (Hume, Smith, Ricardo, Mill), albeit less strong elsewhere. Famously, 1028 American economists signed an unsuccessful petition in 1930 begging Herbert Hoover not to approve the Smoot-Hawley tariff. If the IGM Economic Experts panel is any guide, consensus remains firm today. That said, off the top of my head, various cases in which some modern economists have departed from advising free trade include: 

Capital is included in all the big estimated New Keynesian models (Smets-Wouters, Christiano-Eichenbaum-Evans), etc. But you're absolutely right that the stylized core NK model does not have capital - which is hard to defend on empirical grounds, since capital investment is a very important part of business cycle fluctuations and the response to monetary policy. Ultimately, the reason does basically boil down to the "modeling difficulties" that you mention. First, there is an obvious way in which capital makes the NK model more complicated: at an absolute minimum, it introduces at least one additional backward-looking state variable $K$. In contrast, the two core equations (the intertemporal Euler equation and New Keynesian Phillips curve) of the ordinary log-linearized NK model are completely forward-looking. Adding $K$ to the mix eliminates this nice analytical feature. Still, on its own, this is not such a compelling reason to leave $K$ out of the standard presentation of the model, since the increase in complexity would still be tolerable and possibly justified by the added realism. The additional complications that make capital much more difficult to include are the following. Capital adjustment costs are needed to avoid absurd results. Suppose that there are no capital adjustment costs, and that firms rent capital each period on competitive markets. Suppose also that there are no shocks today. The real rental cost of capital today will be approximately $r+\delta$, where $r$ is the real interest rate that was expected yesterday and $\delta$ is depreciation. For each firm $i$, we have $MC(i)=(r+\delta)/MPK(i)$: real marginal cost equals the real cost of producing another unit of output by increasing capital. Now, suppose for simplicity that firms are identical and there is no price dispersion. (Moving away from this, everything I say will still be approximately true.) Then we can eliminate the $i$ and just write $MC=(r+\delta)/MPK$. Furthermore, real marginal cost is just the inverse markup, so we can rewrite this in terms of markup $\mathcal{M}$ as $MPK=\mathcal{M}(r+\delta)$. Finally, if we assume that production is Cobb-Douglas with capital share $\alpha$ this becomes $$\frac{K}{Y}=\frac{\alpha}{\mathcal{M}}\frac{1}{r+\delta}$$ Pick some reasonable parameters: say, $\alpha=0.3$, $\mathcal{M}=1.33$, $r=0.04$, and $\delta=0.06$. Given these we have $K/Y\approx 2.26$. Suppose that we exist in this steady-state world for a while, and then the Fed pushes down the projected path of interest rates such that the expected real rate declines to $r=0.02$. Then leaving $\mathcal{M}$ constant, the value of $K/Y$ given by the expression above increases to $K/Y\approx 2.82$. The NK model implies that this value should hold in the period after the shock. This is an immense increase in $K/Y$, and the model tells us that it should happen in one period. If our period is a quarter, and $Y$ was not expected to take a sudden dive, then we'd have to invest well above the entire usual level of GDP to accomplish this. (In the continuous time limit, it becomes simply infeasible.) And although the assumption than $\mathcal{M}$ is constant is not right ($\mathcal{M}$ is determined endogenously in the NK model from the relationship of sticky prices to costs), relaxing this could easily make the puzzle more extreme: if $r=0.02$ is expected to prevail for a while, then it will imply higher-than-usual output $Y$ and lower-than-usual markups $\mathcal{M}$, which raise the implied $K$ even further. The model simply doesn't work in this form: you need some form of capital adjustment costs. And these, of course, make the model still more complicated. (By the way, the problem here isn't so much the NK model as the fact that an assumption of no adjustment costs is generally absurd: seemingly small changes in the real interest rate must be accompanied by massive swings in the capital-output ratio, which we never see in practice. The NK model simply brings this absurdity, which is found in the basic RBC model as well, into sharper relief because exogenous interest rate shocks are such an important feature of the NK environment.) Firm-specific capital is needed for strategic complementarity. Even if we fix the problem above by including capital adjustment costs of some form, we run into another awkward feature of NK models: taken alone, the Calvo price rigidity is not plausibly large enough to make the NKPC as flat as we think it is. The most popular fix is some form of strategic complementarity, where firms try not to set prices too far from the aggregate price level. And the most popular way to get strategic complementarity is to assume that firms face both a high elasticity of demand and a steeply upward-sloping marginal cost curve. That way, for instance, any firm that sets its price too far below the average price will receive a flood of demand that causes its marginal cost to spike - and this discourages the firm from setting such a low price in the first place. (Yes, this sounds a little ridiculous, but it's how the models work.) When the model excludes capital altogether, it's easy to just write a declining-returns-to-scale production function for each firm with labor as the only input. This makes each firm's marginal cost curve slope upward. But when we include both capital and labor in the model, the firm's production function probably should be much closer to constant-returns-to-scale. And this means, if the firm can rent any amount of capital from a competitive market on demand, that the firm's marginal cost curve is much closer to flat. This limits strategic complementarity. To get around this, you need to dispense with the assumption of a common rental market for capital, and start talking about firm-specific capital accumulation. But then the model becomes much more complicated, and you're reduced to opaque quantitative exercises like ACEL. Given all this, you can imagine how economists in insight-building mode often just dispense with capital altogether.