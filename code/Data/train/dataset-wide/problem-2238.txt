Values will be (45, 10), (45, 28), (46, 2), (46, 22) etc. Similarly with IncidentCategories2. Reps column becomes 

To be able to values into the shape of your target table you need a set of (parameter_name, parameter_value) tuples. I assume the design is able to distinguish one parameter value from another, either because of its position in the file (row 1 is Parameter A etc.) or due to a label in the row (ParamA=ValueA etc.). For a positional design, create a reference table with two columns - and and populate it accordingly. Create a staging table with an identity column and . Import the parameter rows from a single source file into this staging table. Values will be generated by the identity in the order that rows appear in the source file so the first parameter value will be ID=1 and the last will be ID=40. Join the staging and reference tables to get the tuple. For the embedded labels use string functions to separate the name and value on the delimiter. This will be simpler with two staging tables - one with a single column to hold the raw records from file and the second with two columns to hold and separately. I'd suggest you import each file in two steps. In step 1 bring in the 40 rows that represent your parameters. In step 2 bring in the remaining rows which are data. has and parameters which can control this. bcp utility has similar switches. Now you have a set of 40 (parameter_name, parameter_value) tuples that can be ed into your table's schema. Use this to to table, which presumably will have a surrogate primary key which you can capture (an will help). Use this value in the of the remaining data points. This is why I suggest doing it in two steps. Package the whole thing as a stored procedure. Use a Powershell script to itterate over your files and call the SP for each. You will need to use dynamic SQL for the . If you use temporary tables for the staging table you can run any number of processing streams in parallel without worrying about tidy-up. Before each file is processed the staging table. This will reset the idenity as well as remove the previous file's values. Of course the whole file could be read into a staging table and the parameters separated from sensor data afterwards. This works equally well, it just represents additional run-time work for little value in my opinion. 

A full answer to this would be huge. Off the top of my head, some of the things a RDBMS will give you are 

If you simply include the column which holds these values into an clause they will come out in the sequence you want. For example 

In brief: yes. SQL Server moves data around in 8k chunks known as "pages." This includes disk IO. The more cruft there is in a page, the lower the amount of useful information, since the page size is fixed. Therefore to process a certain number of rows will require more IO in a cruft-filled table than in a cruft-free one. This will be slower. Even if the data is permanently in memory and never updated the management overhead of the extra pages will be fractionally higher. Similar consideration should be given to choosing data types. A tinyint will use less space than int, which is less than bigint. The difference is only a few bytes but may add up to a considerable amount at data warehouse scales. Blindly defining all strings as is just plain bad. 

Will this index end up lopsided? No. The way the BTree algorithm works keeps it balanced. (One interpretation of the "B" in BTree is for "balanced".) As leaf pages fill they cause parent pages to split, with half the rows going to each new parent. Further rows cause further splits, which cascade up the tree until the root has to split, at which point a new root page is created. At every stage all paths from root to leaf have the same length and hence "balanced". Should it be reindexed regularly? Yes. Each DBMS has its own idiosynchracies but likely regular re-build of indexes will be recommended. If for no other reason, the statistics will get out of date eventually which will result in less efficient query plans. Should I use a low, normal, or high fill factor? As all writes will be on the right-most pages leaving free space anywhere else will be a waste. There will never be any writes to use up that free space. Use a fill factor of 100%. Should we try to find something else to put into the index Indexes aren't (generally speaking) created for entertainment purposes. They're there to improve query performance or support constraint enforcement. If your workload would benefit from a index then create that. If not, don't. Indexes require extra work during writes and consume space and maintenance time. Create the ones you need and no more. And no less. Keep in mind filtered indexes and included columns. Bear in mind that optimiser math may mean your indexes aren't used anyway. Will there be issues with hotspots or index page contention .. What can I do to mitigate that? Well, yes, in theory. First test and prove you'll have a problem meeting your expectied transaction-per-second count. If you can't, are you sure it's this index that's the limiting factor? If it is, in-memory tables and snapshot isolation may implement concurrency mechanisms different to the "base" system's, depending on DBMS. There may be alternative storage engines whose characteristics differ. Try those to see if the pain receeds. You can always post a follow-up question here with quantatative observations! I understand that for an ascending surrogate key.. Surrogate keys are made-up values that replace human-intelligible natural keys. They're used for reasons releated to DBMS implementation peculiarities and are not inherent to the relational model. Your date is not a surrogate key - it is business data. a reverse key index might be better. Hmmm .. OK, so instead of writing all today's rows to key value 20161021 you'd write them to .. 12016102? How does that help? Hashing won't help either: to use the hashed values for lookup each date would have to produce the same hash so there's still a hotspot. Or is all of this a total non-issue, and indices of this kind are fine and I should stop worrying about it? Probably. The vast majority of indexes on most systems are like this. By-and-large they work just fine and the DBMS is able to process significant workloads on modest hardware. Indexing is one of those things you can (and should) tweak ad nauseam after go-live. Concentrate on delivering a normalised, feature-rich, debugged system. Tune it afterwards. 

then that part could be handled by the value-storing software and the GIS eliminated from the architecture. I have not implemented such a system. I'm really just thinking out loud here. At the petabyte scale there are no off-the-shelf solutions. There are, however, many satellite data providers so your problem is tractable. Good luck. 

Context_info acts like a global variable. It's binary so will require some work to extract the password each time it is used. BOL has details and examples. 

You may have to involve , too, if the SPs are spread across multiple schemas. Take the output of the above and run it against the staging DB. You can do something similar for functions, triggers, other executables and any type of object. Once done, backup the staging DB and send that to the customer. The whole thing can be automated. 

Most often the CASE is written with the column name given and various values listed. I've inverted this so multiple columns can be checked against a single, known value. This could be achieved just as well using the longer form of CASE i.e. 

This way the network transfer will be a minimal part of the elapsed time and you can eliminate it from your enquiries. 

The program which performs log shipping runs under a specific UserID. Assuming you are using SQL Server Agent to perform log shipping on a schedule, log shipping will happen using SQL Server Agent's credentials (but see below for proxy accounts). You can find this by starting SQL Server Configuration, highlighting SQL Server Services and right-clicking on SQL Server Agent. The account listed there must have write permission on NAS device. Importantly, the source Agent and the destination device must share a source of authentication. If they are on different network domains, say, you may have to get your LAN administrators involved to establish the trust between the two devices. Proxies: it is possible to establish additional credentials which SQL Agent will use to run particular jobs. See here for information. If your site is using proxies to run log shipping then it is the proxy account which will need access to the NAS. To test, I would suggest you create a new job with a single step that copies a small text file from the DB server to the NAS. Use the same credentials for this as you do for log shipping. Run this and adjust permissions until it works. Then duplicate those permission for the actual log shipping job and re-test. 

I'd suggest a key-value DBMS, but I throw this out there for interest's sake. Instead of performing INSERT & DELETE statements, only do UPDATEs. The table structure will be something like 

One approach I've used is to separate the querying and the emailing into different job steps. The first runs your query. It writes output to a file (Job step properties -> Advanced page -> Output file) using tokens to identify each day's file. The second step sends the email with the first step's output as an attachment. The first step sets a "success" or "fail" condition to control whether the second step runs or not. 

SQL Server uses four part names. To reference an object outside the current DB qualify the name more fully or create a synonym. USE provides security context and default for the optional parts in four part names. 

The secret sauce is using as opposed to . The former retains only distinct rows whereas the latter keeps duplicates (reference). In other words the nested querys says "give me all rows and columns from EmpDtl1 and in addition those from EmpDtl2 which are not already in EmpDtl1". The count of this subquery will be equal to the count of EmpDtl1 if and only if EmpDtl2 does not contribute any rows to the result i.e. the two tables are identical. Alternatively, dump the tables in key sequence to two text files and use your comparison tool of choice. 

Then create a constraint on the table to ensure . You don't say which RDBMS you're using but many of the major ones support this. If anyone tries to lend an item to himself the DBMS will throw an error and refuse to store the data. 

I happen to have a table with 3,423 rows and 195 distinct values in . I'll call this table (person) and duplicate it to create (person2). There is a unique, clustered primary key on an integer ID column. I'm using Microsoft SQL Server 2016 (KB3194716) Developer Edition (64-bit) on Windows 10 Pro 6.3 with 32GB RAM. With the base query 

There are a couple of concepts which need to be distinguished. One is about structure and the other about schema. Structured data is one where the application knows in advance the meaning of each byte it receives. A good example is measurements from a sensor. In contrast a Twitter stream is unstructured. Schema is about how much of the structure is communicated to the DBMS as how it is asked to enforce this. It controls how much the DBMS parses the data it stores. A schema-required DBMS such as SQL Server can store unparsed data (varbinary) or optionally-parsed data (xml) and fully parsed data (columns). NoSQL DBMSs lie on a spectrum from no parsing (key-value stores) upwards. Cassandra offers reatively rich functionality in this respect. Where they differ markedly to relational stores is in the uniformity of the data. Once a table is defined only data which matches that definition may be held there. In Cassandra, however, even if columns and families are defined there is no requirement for any two rows in the same table to look anything like each other. It falls to the application designer to decide how much goes in a single row (also referred to as a document) and what is held separately, linked by pointers. In effect, how much denormalisation do you want. The advantage is you can retrieve a full set of data with a single sequential read. This is fast. One downside is that you, the application programer, are now solely responsible for all data integrity and backward compatibility concerns, for ever, for every bit of code that ever touches this data store. That can be difficult to get right. Also, you are locked into one point of view on the data. If you key your rows by order number, how do you report on the sale on one particular product, or region, or customer? 

There are three different things we need to store information about. First is the positions within the corporate organisation. Second is the reporting structure between these positions. Third is the people who currently fill each position. While it is possible to design a database that holds all this in a single table, especially with the very limited cases in the question, I believe that would not be proper to do so. Take salary as an example. Most jobs have a salary range or pay grade. An individual person, however, will have a specific number (which lies within the position's range). That number is functionally dependent on the person, not the position, and so should be normalized into a separate entity type. It is clear, too, that tax details and next-of-kin details will follow a person if she changes position and do not relate to what she happens to be doing right now. Conversely it is quite usual for a new position to enter (or leave) the org chart some time before a person fills it (or is made redundant). It is here, in the position, that the roles you list in the question are stored. Position 1729, say, could have the title "widget fixer" and role "worker." Similarly with the reporting hierarchy. The information required to define the relationship is the senior and junior position IDs - the Team Leader reports to the Manager, the Secretary is under the Director. This makes it different to either of the others and its own entity type. 

A) Scale The middle tier can be scaled easily - hence the web farm concept. Scaling out the DB tier is much more difficult. While some products can do this it is not yet trivial and mainstream. B) Cost Typically web servers are common-or-garden boxes. DB servers, however, tend to be larger, more complex and more resilient. This all translates to "expensive." A recent employer estimated a CPU tick on the DB was ten times more expensive than one on an application server. C) Reuse Logic embodied in a stored procedure cannot simply be linked into, say, a stand-alone mobile app. Changing the SP affects every application which uses that DB, whether that application is ready for the change or not. D) Reuse Logic embodied in a stored procedure is common to all applications which use the DB. Programmers cannot side-step the rules at a whim. Changing the SP affects every application which uses that DB, ensuring consistency across the enterprise. E) Tooling There are more, and more sophisticated, languages, tools and techniques available for development in the application tier than there are in the database (from comments, with thanks). F) Network traffic Typically a business function will require many reads and / or writes. Often the output of one statement will be the input to a following statement. If SQL statements are held in the application each will require a network round-trip to the server. If the SQL is held in a stored procedure there will be a single network trip, sending the parameters in and receiving only the final result back; the network cost of the intermediate results is avoided. 

The answer will vary according to your understanding of the word "equivalent". From a logical / application perspective they are the same. Both reference all columns of the table and define an ordering on all rows. Physically the image on disk and the thread of execution through the server software are likely to be different. The optimiser may treat them differently, too. 

No trigger is needed. If you SELECT from Stockitem the DBMS will take a shared lock on that table. This will prevent updates to this table since UPDATEs need an exclusive lock, which cannot be granted while the shared lock exists. So, in the application (or stored procedure or whatever) start a transaction (to ensure locks are held until we're finished) read the Stockitem row, compare requested and available quantities, respond accordingly and commit the transaction.