The advantage of this version, is that it doesn't depend on a prebuilt table, which also is it's disadvantage if you are going to test this for a large number of \$N\$'s. Another benefit is that it only calculates as many binary looking decimal as needed, which for \$N = 97\$ would make the loop go until \$i=225\$, before returning the result: "11100001". That is saving over 3800 iterations of precalculations in that particular case. It also displays a different version on how to find all the binary looking decimals, and if so wanted can be a replacement for the calculations used in with a few slight modifications. As the code is untested, I will not claim it to be faster or slower. But the memory footprint, should at least be smaller. 1 Conversion to a binary string based on a variation of this answer 

As I didn't have any available, I've commented out that segment, and use a random generator instead. This in combination a lot of print statements should show that in most cases it would continue with next sentence in a lot of the cases, and hopefully not fall through to the dictionary test or "Couldn't decide" very often. When deployed to production all of the statements should be removed. Do also note that this code is written in Python 2.7, so no parentheses are needed for the statements, and some strings needs to be declared as unicode strings. But you didn't state which python version you are using, so I made this in the older variant. Hope this helps you along the way. Alternate trigger word list tests At the cost of looping twice through the list, one could remove the inner loop, and all other code related to : 

Some notes on regular expressions Note also that your original regexp where somewhat flaky, that is when you test against which you used for and , this matches a string starting with , but it can continue with whatever, so both and would match it! In the new variant I've used to match any combination of digits, 'a-f' and period , which is a loose translation of an ip-address. I further used to name the groups, which isn't strictly neccessary, but can be useful sometimes. I've also added a near the end to pick up the address when matching the 'icmp' lines. A neat tool to test regexp's is found at $URL$ and your regexp is found here. This site also explains the regexp to some extent. I've played a little more with this regex, so try out the different version in the drop down menu near the top. Added version handling icmp info Here is the currently latest version of the regexp, where I also have added some explanation: 

You've clearly been around, and most of your code does follow the PEP8 guidelines, with some minor exceptions: 

1 Added: Thanks to my question at Mathematica SE I now know that the formula is: $$ \frac{n(n+1)(n+2)}{6} $$ 

When all this is said and done, read the documentation or do searches like "python sum" or "python mean" before reinventing the wheel. 

To answer you question on how to handle this, one needs to address why your current approach is good or not. And the main issue I have with your code is that if either fails, you'll happily continue until all three helper methods have completed. This is not good behaviour, and might trigger bigger errors which might terminate your program, removing your chance of correcting the initial errors. According to this documentation will return the number of bytes written, but you keep on 'ing the result as if you're expecting to indicate an OK write. Either way your or-ing will possibly hide the initial error code. I.e if the first fail returns and the second fails with you get an error code of . Anyway the final error code is misleading as to why your code originally failed. Two alternate ways to handle these are: 

Added: As commented upon, your teacher is not fond of lists, so here is a version of the function without the list. Note that, in my opinion, it is a little better to change the order around, mainly because I'll expect the first case to be most prominent and should break out of the -block firstly. But that is a matter of taste, and your mileage may vary. 

Now when you, as a developer or support, gets a hold of the exception you should be able to locate where this exception occured in your code base by a single search for "Failed creating user account". This message is not dynamic, but unique. The properties are dynamic, and should add details to the exception. With such an exception you can locate the offending code even though the stack trace is not available, which it wouldn't be if the exception is found is shown in a dialog window or status bar, or in some cases in a log file, or if it being copy-pasted from somewhere into an email. The unique message is also something you can train users to respond to. They can now search in a given list where they can follow some procedures, whilst for other (and unknown exceptions) they contact support. One final note on unique messages, even a unique number, would be better than nothing at all. Just don't let it be autogenerated which renders it useless when you rebuild your code. Some code bases I've seen has numbering schemes like Exxyyzzz where xx can denote module, yy file in module, and zzz could be a running number within that file. 

The advantage of this method is that you now get all of your data from one single query, and one roundtrip from your code base to the database engine. Using subqueries like this shouldn't be to expensive, and at least when comparing to making several roundtrips it is a way better solution. PS! You could consider using heredocs or other variations to get the multiline string in your query. But do try to break it over multiple lines to enhance the readability of the SQL query. See PHP string doc for various option on how to use strings in PHP. 

Not quite sure if this covers all edge cases, or if you need to assign trees to field based on some location closeness or similar. But I think it is worth a try to go from this direction, as you can't brute force all variations of fields and then test using your solver if it is playable. 

Code refactored Here is my version (written originally in Python 2.7, but I think it should run directly in Python 3): 

This will using only A create the upper right part of the matrix, and if also including B it'll create the full matrix. To avoid creating a full sparse matrix, you could opt for using a hash array, where the key is like . When printing you could sort the keys, and print 0 for all non-existent keys, and the actual values for existing entries. The dimension of the full matrix are given by the number of authors in . The time complexity of this method should be \$O(n*m*log(m))\$, with as number of articles, and as a local number of authors for a given article. The space complexity should also be optimal, as you don't store the articles or any extra information besides the global list of authors, and intermediate list of author ids for the current article. I don't think you can do a lot better than this. (The space complexity is somewhat dependent on the storage of the potential sparse matrix) Your questions Testing your code If you make this into a function, and have proper reset functions, it should be rather easy to make a function allowing for multiple input sets, and then match the output towards your expected result. Depending on test methodology you could opt for making tests which tests that the author matrix is as expected, or just the result matrix. In other words, whether you also want to test the inner parts of your code or not. The various test set should test for various aspects, like many articles with few authors, or few articles with many authors, or many articles with many authors, or many articles with few authors. Edge cases Typical edge cases are a few articles with a lot of authors, or many articles each with a unique author. The latter case would trigger the worst space consumption if not some technique for the sparse matrix issue. A million articles As this solution doesn't store the articles, the number of articles is somewhat irrelevant. The author list will increase, but that can't be helped. If implementing a good handling related to the sparse matrix (and possibly only storing the upper right part), like the hash-keying, you can't reduce the space complexity either. For exceptionally large cases of multiple authors, you could opt for using databases for the author id list and/or result matrix, but that seems like overkill in most cases.