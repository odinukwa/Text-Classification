This is explaining why we can't open the file, the OS returned error 5. This can be validated by checking the OS return code by executing on a command prompt. 

Again, depends, but generally I would agree (a counter example would be availability groups where it makes sense to use a single domain account across all instances). 

If it were me, I would. Even though driving them there doesn't "sound" like it's very far away - who knows what else will happen. When you remove the replicas the databases will go back into a restoring state so it should be fairly easy to add them back in. 

It's not really a gap, it holds everything that has changed since that time (not as transactions but as what extents were changed) and then a little log. The "gap" is the changed extents part and what "makes up for it" is the little bit of log in the differential backup to make it consistent. 

I don't understand the question. If it's specific to Ola's script then I do not know, if it's a SQL Server question then you must take at least one full backup and one log backup before adding the database to the AG, so the answer would be no it isn't true. 

If you're receiving data on that secondary but the distributed ag still shows not synchronizing or not healthy then I'd let it go for a bit to see if the DMV values change since it's obviously receiving and processing log blocks. If, however, it isn't then we'll need to investigate further which is out of scope of the answer. 

This may or may not be true depending on your restore strategy. If you have a differential to bridge the gap, that's perfectly valid and you can still use the other log backups that occur after it. If you're extremely worried about single bit flip in a log backup, mirror the log backups to another storage solution. 

I didn't bunch this with extended events, even though in reality it leverages the same framework. This is an enterprise only feature, and while it can do this and is targeted for items such as this... I feel most aren't running enterprise edition. $URL$ 

How do you have an "extra AOAG listener" just chilling out? I don't understand that part of your question. MSDTC is for distributed transactions, which aren't supported in 2012/2014 and only supported with certain restrictions (as of this moment) in 2016. Thus this is not required in a supported scenario. If you're going unsupported then this is still a moot point as the local MSDTC will be used. Unless I'm missing something (completely possible) or not understanding, this is not needed. 

Because of the way decisions made and the way it happened, it doesn't sound like there will be any way to reuse the databases unless you have local backups there that can get you to a sync point in terms of LSNs with the primary database. My guess is you don't. Let this be a lesson in making sure the technology and process is understood before deciding a path or course of action. 

That's because it isn't required. You can use whatever you'd like, but if you truly wanted to find the proper value - testing is required. 

In this case, not a whole lot. The SSMS cert is still a self signed cert. Self signing just means you're the only authority and no one else can claim you are who and or what you say you are which in this case, doesn't matter. The private and private keys will still be used to encrypt and decrypt regardless where the certificate was made, it just needs to be a valid cert. You may or may not be bound by using an PKI ( Public Key Infrastructure) you have at your company or using HSM (Hardware Security Module) to store the keys versus just generating a local certificate on your machine for use in other places. 

Importance doesn't touch IO, it deal with scheduling. High runs more often than normal which runs more often than low. It doesn't mean it won't eat up your CPU and IO, it just tries to be more fair with scheduling based on your interpretation of the workload (i.e. those marked high are more important). This most likely will not help what you're attempting to solve. 

The reason it is most likely (but not 100%, because if this is a VM it could have been over subscribed, paused for a snapshot, paused from migration, etc.) is due to the information later in the cluster log: 

The web server can't run under the account or the application context can't run under the account? For example, the web server is sitting in a DMZ? 

If you're using Windows Server 2012+ then dynamic quorum will be enabled by default. If you shut the servers down one by one in a controlled fashion then dynamic quorum will kick in and automatically readjust the votes. You won't have to do anything. Additionally if you're using Windows Server 2012R2+ dynamic witness will also be on by default, thus giving or taking away witness votes as needed. Again, because you're doing this in a controlled fashion there shouldn't need to be any extra involvement or customization for the short period of time. 

Restarting the instance didn't actually fix anything. If the databases were synchronized then the log blocks were hardened on the secondary. This doesn't mean there were redone yet, jus that they were hardened to disk. By restarting the instance you've effectively forced redo to complete to a point and thus creating the second log file. If it is taking this much time to redo log blocks, there may be an issue with how much load the secondary could handle and this is one of the ways you'll see it manifest. The last item that is indicative of this behavior is a blocked redo thread, which is most likely the cause. 

This is actually pretty common, especially in environments that use tons of 3rd party applications where the connection string information isn't known or licensing code prevents the renaming of a connection (ugh). 

If SQL Server created the listener for you, I highly doubt it is setup incorrectly... I won't rule out edge cases though. 

Once you failover to the 2016 instance, the database is now upgraded fully to 2016. This is a one way trip and will no longer be able to be hosted on anything but SQL Server 2016. Since the partner is not 2016, the data movement will be suspended (stopped). No new data will "flow" back to the older instance. 

The main reason is someone had a bad experience with them (or, conversely, no experience with them) and has completely ridden them off... forever. This is otherwise known as personal preference. Now, there are some reasons that you couldn't use them. The number one reason I can think of is that a 3rd party driver or application/tool (think filter driver, disk replication, etc.) does not support it. A quick example of this is a block level disk replication tool that did not support anything other than NTFS, with only specific cluster sizes and couldn't go above 2 TB for any specific volume. 

No, not really. The worst thing that could happen is the log file grows again, given this is a dev server that's not a problem. Worst case is the log takes up all available space and your db goes into read-only... which is easily rectifiable and downtime is fine as it's a dev server. Shrink away. 

The primary server is always where the write workload goes. If you're asking how the replica gets updated once it transitions from Primary->Secondary the answer is it finds the primary and checks in with its' LSN. There is a negotiation that goes on and depending on what happens in the interim between down and connected to the new primary it may send log blocks until the old primary is caught up or it may require log backups to be applied or a complete new synchronization. 

OS Disk fails Cluster partitioning Data disk fails Out of memory Bugcheck log disk fails Overwhelmed disk slow networking windows patching sql patching server power failure 

I'm going to be brutally honest here. Availability Groups are not a silver bullet, fix everything HA & DR. It seems that the technology is being used, but not too much is known about how it works or why it does these things on both an AG level or a WSFC level, so I completely agree it may not be setup properly... however, it is working as it is setup - I can't fault the cluster for doing what it is supposed to be doing. 

Distributed AGs don't hold databases, they hold availability groups. Availability Groups hold databases, that's where we need to go. 

See above. Many of the companies I know have an alias policy for that reason. It's also how AGs work for the listener as it is a network layer indirection. 

Then the AG wouldn't be synchronized, it'd change to synchronizing. This is more likely what I descried above, where the log blocks were hardened and weren't yet redone (blocked redo thread). Assuming the log blocks were sent, the AG is synchronized, and the redo thread was not blocked... this could be due to the transaction still being open. Until the transactions is committed (trancount = 0) your queries on the secondary won't see it as it isn't a yet committed transaction. Assuming the transaction was committed (trancount = 0), the default isolation level of is mapped to the snapshot isolation level on the secondary. This could also be due to isolation level changes and the query running. 

So I don't know if the tech person didn't understand, or what, but you can't split brain with a single node... so, I don't know why (s)he thought that, but that's incorrect. There really aren't any issues with a single node cluster other than it's a single node cluster which defeats the point of having a cluster TBH. It's a different story if it'll eventually be built out, but if you're going to wait for a disaster to do so... you're already behind the ball. Keeping just a single instance, not in a cluster would be faster to replicate as well should something happen. In the end it's up to you, unless I had a much more compelling reason I'd go with just a single server without clustering. Also, don't listen to their techs anymore ;) 

Find the OU with the CNO in it Create a computer object, name it your DNS listener name Set the security on the new VCO and give the CNO full control over it (full isn't required but the list of needs is fairly long so just give it full control) Create the listener through SSMS/TSQL/Powershell 

This is completely expected. I've talked about this before but I'll recap it for you. There could be multiples reason you are seeing this behavior. The two that most people are confused or inaccurate of how it works are how availability groups synchronize data and how queries on readable secondary replicas work. Please note the below is ONLY pertinent for SQL 2012 and 2014, and to some extent 2016. How Data Synchronization Works (brief overview) There are two types of replica synchronization, synchronous and asynchronous. The way the data synchronization happens in both is exactly the same. The way that SQL Server behaves, though, is different. When synchronous is used we wait for the data to be HARDENED on the secondary replicas. This means it only needs to be acknowledged that it was successfully written to the log, not that the log block shave been successfully redone. Asynchronous does not wait for the status message and just continues. Thus it is entirely working properly, however there seems to be a misunderstanding as to how it works. AGs ship log blocks, not transactions, thus the entire transaction may not be shipped together and may not even be redone yet. Querying Readable Secondary Replicas When you run a query on a secondary replica, the read committed (default) isolation level is silently mapped to snapshot isolation under the covers... whether or not you have SI or RCSI enabled for the database. Since snapshot works by row versioning and is consistent from the beginning of the transaction, you may not be able to see new data as it comes in per how snapshot isolation works. This is entirely working as intended. The other point to keep in mind is that just because you have the acknowledgement that the data was hardened to the log on the synchronous secondary replica does not mean that the REDO thread has redone those log blocks yet. Thus, just because you have an acknowledge on the primary doesn't mean REDO has completed on it, only that it has been hardened to the log. Additionally, if your redo thread is blocked it may take a long time (or never) to not be blocked and thus your redo queue size will grow. In the end I can totally expect the behavior you are witnessing, however it doesn't mean that it isn't synchronous. It just means it doesn't necessarily work the way it was believe or thought to. Hopefully this clears the confusion.