I like "Hodge Theory and Complex Algebraic Geometry" by Voisin. The focus is on the Kahler case, but the early explanations of Dolbeaut cohomology are for all complex manifolds. 

It's a $20 \times 20$ determinant. Take each $2 \times 4$ matrix $$L = \begin{pmatrix} s & t & u & v \\ w & x & y & z \\ \end{pmatrix}$$ and turn it into the $4 \times 20$ matrix $$M := \begin{pmatrix} s^3 & s^2 t & s^2 u & \cdots & v^3\\ 3 s^2 w & 2 swt+s^2x & 2swu+s^2 y & \cdots & 3 v^2 z \\ 3 s w^2 & 2 swx+w^2 t & 2swy + w^2 u & \cdots & 3 v z^2 \\ w^3 & w^2 x & w^2 y & \cdots & z^3 \\ \end{pmatrix}$$ If the pattern isn't so clear, the columns are indexed by the $20$ cubic monomials in $4$ variables. For each monomial, plugin $( a \ b ) \begin{pmatrix} s & t & u & v \\ w & x & y & z \\ \end{pmatrix}$ for the $4$ entries and collect coefficients of $a$ and $b$. For example, the monomial $s^2 t$ becomes $(as+bw)^2 (at+bu)$; expanding and collecting coefficients gives the column $(s^2 t,\ 2swt+s^2 x, \ 2swx + w^2 t, \ w^2 x)^T$. A cubic vanishes on the row span of $L$ if and only if that cubic is in the kernel of $M$. Stack up the $5$ $M$ matrices and take the determinant to get your answer. 

Finding the recurrence (and proving it is correct) can be done by the standard techniques for extracting the diagonal of a rational power series. Let $\beta_1$, $\beta_2$, ..., $\beta_N$ be the weights of $V$. Let $\rho$ be half the sum of the positive roots and $\Delta = \sum (-1)^{\ell(w)} e^{w(\rho)}$ be the Weyl denominator. Then $$\sum_{n=0}^{\infty} t^n \chi \left( V^{\otimes n} \right) = \frac{1}{1- \sum_{i=1}^N t e^{\beta_i}}$$ and $$\sum_{n=0}^{\infty} t^n \dim \left( V^{\otimes n} \right)^{\mathfrak{g}} = \mbox{Coefficient of}\ e^{\rho}\ \mbox{in} \ \left( \Delta \frac{1}{1- \sum_{i=1}^N t e^{\beta_i}} \right).$$ For example, if $\mathfrak{g}=\mathfrak{sl}_2$ and $V$ is the two dimensional irrep, the right hand side is $$ \mbox{Coefficient of}\ u \ \mbox{in} \left( \frac{(u-u^{-1})}{1-tu^{-1} - tu} \right)$$ which can be seen without too much trouble to be the generating function for Catalan numbers. The diagonal of a rational generating function is $D$-finite by a result of Lipshitz. The particular recurrence can be found by Sister Celine's method (see theorems 10 and 11). I found these references in Stanley, Enumerative Combinatorics Vol. II, solution to exercise 6.61. Stanley warns that there is a gap in Zeilberger's argument, but hopefully his algorithm is right. 

I am not sure about Milne's reduction, but your fix is too strong. First off all, I don't understand why you write $B/p B$ with $p = \phi^{-1}(n)$. I assume $\phi$ is the map $A \to B$. But then $\phi(n)$ is an ideal of $A$, not $B$, and $b$ is an element of $B$. I am going to assume you meant "$b$ is not a zero divisor in $B/nB$." But requiring this for every maximal ideal of $B$ implies that $b$ is a unit! We surely don't want to impose that. Let me add that, in my opinion, it is rude to use the name of a living person as a pseudonym. I do not think that Grothendieck would appreciate other people posting under his name. If you want to honor him, why not name yourself for one of his theorems or definitions, as fpqc does? (Of course, if your name is Grothendieck, I apologize.) 

It might be useful to see quickly why Jeremy's answer, although a very reasonable guess, is wrong. Consider the two partitions $(2,2)$ and $(3,1)$. In refinement order, neither one is greater than the other. Consider the subscheme of $\mathbb{P}^1 \times \mathbb{C}^2$ cut out by $$x^2=u xy + v y^2=y^3=0$$ where $(x,y)$ are the coordinates on $\mathbb{C}^2$ and $(u: v)$ are the homogenous coordinates on $\mathbb{P}^1$. Every fiber over $\mathbb{P}^1$ has length $4$, so this is a flat family, and we get a map $\mathbb{P}^1 \to \mathrm{Hilb}_4(\mathbb{C}^2)$. The image of this map is a torus invariant curve in the Hilbert scheme. Its two torus fixed points are the monomial ideals $\langle x^2, y^2 \rangle$ and $\langle x^2, xy, y^3 \rangle$, corresponding to the partitions $(2,2)$ and $(3,1)$. So, in any Bialynicki-Birula decomposition, one of these partitions must dominate the other. 

I think I have an example of a Gorenstein toric non-l.c.i. Let $D$ be the lattice $\{(i,j,k,\ell) \in \mathbb{Z}^4 : i+j+k+\ell \equiv 0 \mod 2 \}$. Let $C$ be the cone $\{ i,\ j,\ k,\ \ell \geq 0 \}$ in $D \otimes \mathbb{R}$. Our toric singularity will be $S:=k[C \cap D]$. Note that we can think of this as the subring of $k[w,x,y,z]$ consisting of even degree polynomials. So $S$ is generated by $w^2$, $wx$, ..., $z^2$ -- ten monomials in all. Normal semi-group rings are always Cohen-Macaulay, and the dualizing module is $\mathrm{Span}_k (C^{\circ} \cap D)$ where $C^{\circ}$ is the interior of $C$. In other words, the dualizing module consists of the $k$-span of the even degree monomials where each variable appears with even degree. This module is clearly generated by $wxyz$, so the dualizing module is free of rank $1$, and we see that $S$ is Gorenstein. Now, let's check that $S$ is not l.c.i. We see $S$ as a quotient of the polynomial ring in $10$ variables, coming from the generators $w^2$, $wx$ etc. Let $R$ be the polynomial ring in $10$ variables. I'll write $[w^2]$ and so forth for the generators of $R$. Let $I$ be the kernel of $S \to R$; we want to show that $I$ is NOT generated by any $6$ elements. Let $\mathcal{M}$ be the maximal ideal of $R$. Since $I$ is graded, it is enough to check that $I/\mathcal{M}I$ is NOT $6$ dimensional. Well, there are already $6$ degree $2$ elements of $I$ which are the $S_4$ symmetries of $[w^2] [x^2] = [wx]^2$. In addition, we have two more relations $[wx] [yz] = [wy][xz] = [wz][xy]$. These $8$ elements are linearly independent in $I/\mathcal{M} I$, so we see that $S$ is not an l.c.i. 

Recall that the usual definition of a triangulated category is an additive category equipped with a self equivalence called $[1]$ in which certain diagrams, of the form $X \to Y \to Z \to X[1]$ are called "exact", satisfying certain axioms. Two of these axioms are that (1) Given $X \to Y$, it can be extended to an exact triangle $X \to Y \to Z \to X[1]$ and (2) Given a commuting diagram $$\begin{matrix} X & \to & Y \\ \downarrow & & \downarrow \\ X' & \to & Y' \end{matrix}$$ and exact triangles $X \to Y \to Z \to X[1]$ and $X' \to Y' \to Z' \to X'[1]$, there is a map $Z \to Z'$ making the obvious diagram commute. And, as every source on triangulated categories points out, one of the standard problems with the theory is that there is no uniqueness statement in these axioms. So, why not make one? I envision a definition as follows: For any category $C$, let $Ar(C)$ be the category whose objects are diagrams $X \stackrel{f}{\longrightarrow} Y$ in $C$ and whose morphisms are commuting squares in $C$. Note that there are obvious functors $\mathrm{Source}$ and $\mathrm{Target}$ from $Ar(C) \to C$, and a natural transformation $\mathrm{Source} \to \mathrm{Target}$. Define a conical category to be an additive category $C$ with a self-equivalence $[1]$ and a functor $\mathrm{Cone} : Ar(C) \to C$, equipped with a natural transformations $\mathrm{Target} \to \mathrm{Cone} \to \mathrm{Source}[1]$, obeying certain axioms. I noticed one place you have to be careful. In a triangulated category, if $X \to Y \to Z \to X[1]$ is exact, then so is $Y \to Z \to X[1] \to Y[1]$ (with a certain sign flip). The most obvious analoguous thing in a conical category would be for $\mathrm{Cone}(Y \to \mathrm{Cone}(X \to Y))$ to equal $X[1]$; the right thing is to ask for a natural isomorphism instead. But everything else seems work out OK, at least in the case of the homotopy category of chain complexes. And it seems much more natural. What goes wrong if you try this? I know that this is the sort of subject where people tend to mention $\infty$-categories; please bear in mind that I don't understand those very well. Everything I've said above just used ordinary $1$-categories, as far as I can tell. 

At a lower level than Flajolet and Sedgewick, Chapter 9 of Concrete Mathematics (Graham, Knuth and Patashnik) is a good introduction to elementary methods. 

Suppose we have three $n \times n$ matrices $A$, $B$, $C$ with floating point entries. We would like to compute the polynomial $\det (xA+yB+zC)$. At least in Mathematica, and I think in all computer algebra systems, this will take $n!$ steps; Mathematica chokes around $n=15$. What's the smart way to do this? I have some ideas, but they are all complicated enough that I don't want to implement them before hearing from others. This question is on the border between MO, cstheory and Mathematica, but I suspect that this is the right place to start. ADDED IN RESPONSE TO QUESTIONS BELOW The trouble with Gaussian elimination is that you have to divide by polynomials in $(x,y,z)$, and the expressions soon get huge. Interpolation, probably at roots of unity so that the interpolation matrix will be unitary, is my best idea, but I wanted to see if there was a better one before I tried it. 

I have an $4 \times 4$ solution which I don't think is on any of the components I've found above. Haven't figured out how to embed it it in a $4^2+4$ dimensional family yet but, of course, it must do so. (The intersection of $n^2+n$ equations on a $2(n^2+n)$-dimensional variety must have every component of dimension $\geq n^2+n$.) I start with a $2\times 2$ solution which is not symmetric. Let $$I = \begin{bmatrix} i&0 \\ 0&-i \end{bmatrix} \qquad J = \begin{bmatrix} 0&1 \\ -1&0 \end{bmatrix} \qquad K = \begin{bmatrix} 0&i \\ i&0 \end{bmatrix}.$$ (This is the standard embedding of the quaternions in $\mathrm{Mat}_{2 \times 2} (\mathbb{C})$.) Then take the $4$ matrices of the form $\pm p I \pm q J \pm r K$ with an even number of $-1$'s. They clearly sum to $0$, and $$(\pm p I \pm q J \pm r K)^{-1} = \frac{1}{p^2+q^2+r^2} (\mp p I \mp q J \mp r K)$$ so their inverses do as well. Now we need some trickery to make this solution symmetric. We can replace $I$, $J$ and $K$ by $iI$, $iJ$ and $iK$ and this still works, with the benefit that $iI$, $iJ$ and $iK$ are now Hermitian. If we embed $2\times 2$ complex matrices into $4 \times 4$ real matrices in the standard way, then Hermitian becomes symmetric. In other words, use $(\pm p I' \pm q J' \pm r K')$ where $$I' = \begin{bmatrix} 1&0&0&0\\0&1&0&0\\0&0&-1&0\\0&0&0&-1 \end{bmatrix} \ J' = \begin{bmatrix} 0&0&0&1\\0&0&-1&0\\0&-1&0&0\\1&0&0&0 \end{bmatrix} \ K' = \begin{bmatrix} 0&0&1&0\\0&0&0&1\\1&0&0&0\\0&1&0&0 \end{bmatrix}$$ 

The most elegant way to interpolate would probably be $$f \exp(t \log(f^{-1} g)).$$ where $\exp$ and $\log$ are given by their power series expressions. That can probably be simplified a lot since we are just dealing with $2 \times 2$ matrices; does anyone want to give it a try? 

If you can show it for a quadrilateral, then it is true in general. Proof Sketch: by induction on the number of sides. The base case is $n=4$, which you say you have done. Let $X$ be the set of joints. By a compactness argument, the maximum is achieved; pick a particular placement which achieves the maximum. Consider a new linkage $X'$ with $n-1$ vertices. The distance between $x'_i$ and $x'_{i+1}$ is the same as that between $x_i$ and $x_{i+1}$; the distance between $x'_1$ and $x'_{n-1}$ is the same as the distance between $x_1$ and $x_{n-1}$ in the chosen optimal placement. Then the chosen placement of the $x_i$'s must also be an optimal placement of the $x'_i$'s; if not, we could keep $(x_1, x_{n-1}, x_n)$ in the same place and move the other $x_j$'s to a better placement for linkage $X'$, obtaining a better placement of $X$. By induction, we see that $\{ x_1, x_2, \ldots, x_{n-1} \}$ lie on a circle. But the same applies to any index, so $X \setminus \{ x_i \}$ lies on a circle $C_i$ for any $i$. We see that $C_i$ and $C_j$ have $n-2$ points in common so (for $n>4$) they must be the same circle. So all the points lie on a circle. 

I can't find a set of online notes which contains this statement. Kirsten Eisentr√§ger's notes are generally very good, but they get this result wrong on the first page -- Theorem 1.1 claims that, if $f_A$ is a power of an irreducible polynomial, then $A$ is simple, ignoring the possibility that $A$ is a power of a simple variety. Let $A$ be isogenous to $\bigoplus A_i^{n_i}$, where the $A_i$ are simple and mutually non-isogenous. Every abelian variety has such a decomposition. Then $f_A = \prod f_{A_i}^{n_i}$. Suppose that $f_A$ is a power of an irreducible polynomial. Then all of the $f_{A_i}$ must also be powers of that polynomial. In particular, for any $i$ and $j$, either $f_{A_i}$ divides $f_{A_j}$ or vice versa; without loss of generality, suppose $f_{A_i} | f_{A_j}$. By a result of Tate, this means that $A_i$ is isogenous to a subvariety of $A_j$. Since $A_i$ and $A_j$ are simple, this means that $A_i$ and $A_j$ are isogenous. Since we assumed that the $A_i$ were mutually nonisogenous, there must in fact be only one summand in our decomposition of $A$, and $A$ is isogenous to $A_1^{n_1}$ for some simple $A_1$ and some $n_1$. Suppose now that $A$ is isogenous to $B^{n}$ for $B$ simple. Then $f_A = f_B^n$. So our goal is to show that $f_B$ is a power of an irreducible polynomial. If not, write $f_B = gh$ where $g$ and $h$ are relatively prime of positive degree. By a result of Honda, there exist abelian varieties $C$ and $D$ with characteristic polynomials $g$ and $h$. By the result of Tate cited above, $C$ and $D$ are isogenous to subvarieties of $B$, contradicting that $B$ is simple. $\square$ 

In many circumstances, it is natural to define a torsor as a nonempty set $X$ with a $G$-action such that the map $G \times X \to X \times X$ is an isomorphism. (Here the map is given by the group action $G \times X \to X$ in one coordinate, and the projection $G \times X \to X$ onto $X$ in the other coordinate.) The advantage of this definition is that you never have to mention an element of $G$ or of $X$, so the definition works for a group object in any category. If you remove the word "nonempty" from this definition, the empty set is a torsor for every group. It definitely comes up that one can prove all parts of this definition except the nonemptyness, and has to say things like "$X$, if nonempty, is a torsor for $G$". For example, in deformation theory, the first order deformations of a complex manifold are measured by the vector space $H^1(X, T_X)$ and the second order deformations extending a given first order deformation, if nonempty, are a torsor for $H^1(X, T_X)$. As to whether it would make sense to broaden the definition in this way, I don't think it would. The empty set feels very different from a torsor, so one would just have to say "nonempty torsor" all the time and one would probably frequently forget to do so. But it is certainly a matter of taste to what extent definitions should capture the simplest concepts and to what extent they should capture the most useful ones. 

A potential strategy: What are (up to conjugation) the maximal subspaces of $n \times n$ skew-symmetric matrices on which the rank is always $\leq 4$? Then we can focus our efforts on finding large subspaces on each of these which miss the rank $2$ locus. So far, I have only been able to find four maximal subspaces. I'll describe them all as block matrices with the size and nature of the blocks indicated: $$\begin{pmatrix} 5 \times 5 & 0 \\ 0 & 0 \end{pmatrix}$$ $$\begin{pmatrix} 0 & 2 \times (n-2) \\ (n-2) \times 2 & 0 \end{pmatrix}$$ $$\begin{pmatrix} 0 & 3 \times 3,\ \mbox{skew symmetric} & 0 \\ 3 \times 3,\ \mbox{skew symmetric} & 0 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}$$ $$\begin{pmatrix} 3 \times 3 & 0 & 0 \\ 0 & 3 \times 3 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}$$ If these are the only options, then the only one which grows with $n$ is the second case so that's the one we need to concentrate on. In that case, the question is equivalent to "what is the largest linear subspace of the $2 \times (n-2)$ matrices which includes no rank $1$ submatrices?" I can show that the answer to that question is $n-3$, over $\mathbb{C}$, and is $2 \lfloor (n-2)/2 \rfloor$ over $\mathbb{R}$; I'll post the argument if anyone cares.