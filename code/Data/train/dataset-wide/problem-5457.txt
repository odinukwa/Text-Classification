Why are you postulating that medicines might "have a bad day" as it were? The whole point of statistics is to avoid being fooled by chance circumstances. (You might still be fooled by unexpected correlations.) Anyway, yes, medicine is falsifiable. It's hard, because it's difficult to adequately control conditions, and it is tempting to use statistics that are easy to calculate instead of those that better capture the underlying distributions. But people do it all the time: just look at any issue of e.g. the New England Journal of Medicine. (Whether we should always wait until medicine has passed attempted falsifications is a different issue; there is some reason to think that we might have different standards for what we think is "likely a good approximation to the truth" vs. what we think is "worth trying in such-and-so circumstances".) 

I think there's a much deeper and more fiddly issue going on here: political ideology requires actual people to implement. There might be issues with logical inconsistency in ideology, but most times you never find it because of the much greater disparity between premises for what people are like and what people empirically are actually like. Additionally, political ideologies typically don't frame themselves in an idealized form requiring the selection of e.g. a particular type of predicate logic. So although formally we might entertain the idea of a political ideology being "right", practically we don't care. We don't have the ideology in the form it needs to be to make that judgment, and if it was right in some sense (about some model) the rightness wouldn't translate to the real world because the real world isn't the model (we don't know enough about important aspects like how rational and irrational responses to various social situations are triggered). Instead, it's more productive to ask whether the ideology is a good match for actual people. If it has the word "ideology" attached, chances are good that the answer is "no", and the follow-up is to figure out just how bad and in which ways. 

In practice, you just have to read off the answer from the society you find yourself in. For example, it is generally not considered okay to alienate yourself from people on the basis of race in the U.S.; although I have not experience it firsthand the accounts I read indicate that it is okay in Israel for not-particularly-religious arabs and Jews to segregate from each other. (Religion is a particularly tricky case because people often do not behave in accord with what they profess to believe, so it often would come down to alienating someone because they have a different set of incorrect and non-predictive rationalizations for their behavior than you do...seen that way, deciding that alienation is rude sort of makes sense as a way to circumvent pointless animosity or lack-of-cooperation.) But there is also the question of when it should be socially acceptable, which leads one to ask what social acceptance is good for anyway, and what our ultimate goals or moral principles are. Here, again, it really matters what those goals or principles are. For example, one could without much difficulty argue that a utilitarian should engage, not alienate, those with other viewpoints as the best way to get the other person to act in a way that maximizes whichever good that utilitarian is trying to maximize. (Actually, the answer would always be alienate or engage depending on whichever will result in the best outcome, as defined by said utilitarian.) I'm not sure we know enough to make such decisions wisely, however. Studies of social shunning by individuals or groups are pretty few and far between; it's hard to know what the impact would be. Even principle-based systems of morality need to know the consequences of actions now and then. 

You seem to be appealing to some sort of Platonic ideal of what is really there. There are simply two equivalent ways to define things: starting from continuous functions and making step-like functions arbitrarily steep to get discreteness; or starting from discrete sets and allowing arbitrarily many states to approximate an continuum. As you note, the physical world seems to contain things that in some sense are seem more intrinsically one or the other (quantized energy levels, continuum of momentums). Thus, using one model is often more natural than the other, but that doesn't mean that it's necessarily more fundamental. It's not clear that asking which is more fundamental--once we recognize that there is an equivalence between the two models and that certain fundamental phenomena can fall into either category more naturally--is even a sensible question. "Both", "models are a property of what's in your head, not necessarily reality", "discrete, because we have better theorems there" and various other things seem equally reasonable. 

A medical ethicist recently proposed that selecting (and possibly engineering) certain traits into our children is not just morally permissible but desirable: 

You can do this if you can demonstrate factually what the purpose of morality is. Let's assume that you can. Then any of these work: (1) 

So the answer is that firstly, you should only send a ticket if it's actually more effective than sending a warning (since and are both positive), and secondly it'd better be the case that the inequality above holds. You can then, with conditions clearly mapped out, suggest which category things are likely to fall into. For example, in the traffic ticket case you might argue that a well-designed warning could be more effective than sending a ticket at correcting behavior, so that should always be the first resort. Of course you can always make things more and more complicated, but this sort of reasoning is about the bare minimum you need to make sense of utilitarian considerations. 

Individual species don't matter so terribly much, but ecosystems do; if you can identify an individual species going extinct, there are probably many more also going extinct. If you can save the ones you notice, you are almost certain to also save adequate habitat for a bunch of species you don't notice. (This is the concept of flagship species.) Biodiversity provides robustness and richness; the former is valuable if conditions change (in a different way than what was causing the extinction, obviously), while the latter provides more opportunities for us to learn, study, enjoy interactions, find drug targets, etc.. 

There are preconditions that must be met in order to get an altruism algorithm to produce benefits that reward the storage of additional complexity in living organisms (leaving aside the rather thorny question of what counts as "life"). So although on our planet these preconditions are usually met, there is no logical reason why they must be so (e.g. in a very dilute and highly agitated solution, there may be no opportunity for interaction and no benefit for complex behavior; certain types of life may not possess the capacity to be more complex, or the costs for increased complexity may be very high so that it does not develop). Furthermore, even if complexity is rewarded (and altruism or cooperation is not the only type of complexity that can be beneficial; it's also highly beneficial to encode information about the environment or expected environment if this can allow altered behavior that produces superior outcomes), it doesn't explain our existence without many other factors considered also, since you still have questions about what the upper bound of complexity is, why you should end up with social primates as the (most deeply) sentient beings on the planet, etc. etc.. 

There is far too much hand-wringing associated with this topic. It's really very simple, if you're not too afraid to do it: you use exactly the same methods as you do for any other subtle yet nonrandom effects. If your religion makes no predictions at all--not even about biases in outcomes, let alone specific events!--then the answer is easy: it's an unnecessary hypothesis, reject it. No further attention necessary. If your religion does make some predictions, then you test it. For example, if Dionysus is supposed to make grapes sweeter on Tuesdays, you go measure the sugar content and the reported sweetness of grapes on Tuesday. You then chase down any other causal factors (grapes that ripened over the weekend can't all be picked on Monday, so those from Tuesday are the ripest) and make sure they don't explain what's going on. And, if these things keep coming up in the affirmative, then your Dionysus theory seems to have explanatory power and you accept that part of it consistent with experiment and speculate that other parts of the religion may also be true (assuming that there is good reason to believe that whichever factors led to the truthfulness of the part you've tested also apply to the part that wasn't). And that's it. You can test claims about religion as well as you can test claims about anything else, which is to say if there isn't actually any claim at all about anything you can possibly sense or measure or experience, you can't test it. And if there is a claim, you can. (The experiments may be hard to set up, admittedly, but there are lots of difficult experiments.) This whole non-overlapping magisteria business is so unsupportable I'm surprised it even got off the ground. Everyone should just have said, "Hey, Steve, that's just batty--maybe you want to think this through a bit more?" Non-overlapping magisteria makes about as much sense as saying that logic and politics are entirely distinct realms of existence and have nothing to do with each other. Maybe politicians would like logic to stay away, but when they make truth-claims and try to reason, logic has something to say. If they don't make truth-claims or try to reason, everything they say is pretty empty. Likewise, if religions make statements about what will happen, science has something to say. (And religions do make such statements, and science does say things, and religions don't fare too well, and it's always tempting to shoot the messenger instead of doing some soul-searching.) 

Although the paper is well-written and contains a lot of very interesting results, I think one of its premises is flawed, and hence it is not clear that Tononi's critera need to be amended. First, Tegmark argues that a hypothetical state of matter with the minimum necessary conditions for self-awareness, hereafter called Perceptronium, must not merely have information and integration; it must also have autonomy, which arises from dynamics (future state is not the same as past state, though it is computed from it) and independence (insensitivity to external input). But this is not to my mind very solid. It's an interesting idea to explore, and Tegmark does a good job of it in my opinion, but there is another equally plausible scenario: the system doesn't have much independence (it is highly sensitive to input) but it is oblivious to this fact. Without a strong case for independence, utility falters also. One of the central claims in the paper is that there is an apparent conflict between the information and integration claims: you need a large number of states, but this information be integrated in a way that is not easily separable into independent components. The reason that this appears problematic is that our information is stored in our brains, and the best-known example of a neural network with fully integrated states is a Hopfield network, where it turns out that if you have neurons you can maintain about distinct attractors--states around which the whole network converges. Although a Hopfield network brilliantly delivers on the idea of integration (for each state is a global attractor; any neuron that starts going off on its own independent way will be pulled back into the state it's supposed to be in), and it has a measure of biological plausibility, the whole brain is certainly not actually a Hopfield network. The most obvious discrepancy is that in a Hopfield network every neuron provides input to every other, whereas in our brain one neuron provides input to only about 10,000 neurons out of >10,000,000 (a ten thousanth of one percent, or less). But let's suppose that a Hopfield network is a good model. If so, we get on the order of 10 billion possible attractors, which we can encode in about 34 bits (Tegmark's math is a little different). So, consciousness only can flicker between 10 billion different states? That seems wrong somehow--surely there is more variety of experience than this, and so surely the criteria have to be altered somehow. Tegmark acknowledges that there may be a way to encode more state, but doesn't really address the issue. But there is a very easy way to encode more state. In computer science there is the idea of a hash code, which is a deterministic function that scrambles the input values so much that even a small change in input results in a very different output. Let's imagine a case where instead of one monolithic Hopfield network, we have a million Hopfield networks of size 10,000 (each holding about 10 bits of information). Now let's suppose that we compute from these networks a hash code that combines their input such that we disable half of each network's states. So we get 9 bits of information per Hopfield network, each of the million Hopfield networks depends rather strongly on all the others (because of this hash-code based selection function), and yet at any instant we have 9*1,000,000 bits of state represented. The state might be dynamic, as the hash-coding makes invalid certain states, which then causes a new hash code to be computed, which makes other state invalid, etc., and this dynamism may cause the network to settle into one of fewer than 2^(9M) states, but it seems exceedingly unlikely that it would go all the way down from 9M to 34 or whatever. And if we look at the architecture of the cortex, we find that the architecture supports this kind of model better than a universal Hopfield network. So I don't think Tegmark provides a very compelling case that there's even a problem to solve. Without a problem to solve, the dynamics principle isn't needed. That said, it seems intuitively that Tegmark is right that dynamics are needed, or at least that we in practice are dynamic and therefore our kind of Perceptronium is one which contains dynamics, so I'm not arguing that the paper is wrong, either. Just that there is plenty of wiggle room if you want to stick to Tononi's original criteria. 

Viewing these sorts of questions mathematically / computationally as decision theory sheds light on the subject more efficiently, I believe, than traditional philosophical approaches. Suppose you have a universe, and you have an indicator function f that you can call on subsets of the universe and it will return true or false (guaranteed--no halting problem). Every such indicator function defines a category, and if there exists any subset of the universe for which f returns true, then you can say there is f-existence. Now, if you have some f that describes "heapness", then presumably if you pass in the (information about the) cup of sand on my desk, it will return "true". On one grain it would return "false". As you add and reorient sand from one grain to a cupful, it will presumably go from false to true at some point (maybe many points--it might oscillate back and forth for a while!). We certainly have f-heapness, then. The question is then whether the category of f-heap is useful or agrees with human intuition. Here we take a detour through machine learning: in many, many cases we can create classifiers that agree with humans on obvious cases 100% of the time, and where people disagree will tend to come up with a value (if we parameterize from 0 = surely-not-an-X to 1 = surely-an-X) equal to the probability that people will think an object is in that class. Some such classifiers have nice properties of stability as values increase, so you could be sure that two cups of sand would also be a heap. Thus, we could create (in principle) a well-behaved f that would give us an objective definition of f-heapness. Now, though, we wonder what happens at the decision boundary? How does sand go from being a not-a-heap to a heap when no single grain ought to make the difference? If you look at a classifier of heaps, what would happen is that--if we use one that is not forced to return false or true but can return a value from 0 to 1--at some point, as we start adding more sand, the value creeps upwards from 0 until it reaches 1. (Orientation of grains and context of the question might also be important.) If we force it to make a false-true choice, it could either pick at random, taking the value as a probability, or it could have a sharp threshold. But even if we take a sharp threshold, all we learn is that this is the point where there is maximal disagreement about whether this is a heap of sand or not: 50% of people would take each perspective. Even if we all agree that f-heapness is true heapness, we would be maximally unhappy (on average) about its agreement with our intuitions at that point. So, in conclusion, logical inferences "one x is not a y, and if n x's is not a y, (n+1) x's is so similar that it could also not be a y" are not really capturing the essence of what needs to happen to classify things. If you take an approach that does usefully classify things (e.g. machine learning), you find that the paradox cannot be constructed.