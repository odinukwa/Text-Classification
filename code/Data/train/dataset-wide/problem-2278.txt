It depends. They all speak different dialects of SQL. Not having any details to work with I would recommend Postgresql though. 

Which is what we would expect-- the $PGDATA/base/83637 directory is gone, there should be nothing to vacuum. Are you sure there isn't something else eating up your disk space? One of your other databases? log files? Something that you could try would be to: 

For Postgresql on MS Windows, the Postgresql wiki has an "Automated Backup on Windows" article that may be of use. There is also "Instant PostgreSQL Backup and Restore How-to" available from Packt that provides a good overview of the various options available using the tools that come with Postgresql. There is nothing in the booklet that can't be found on the web but they do a nice job of pulling that all together in one document and the price is very reasonable. 

While you can't "copy" them you can use the database meta-data to dynamically generate the DDL to create them. 

$URL$ Let's say our database "concurrent insert" parameter is set to "Auto" (1). And we have a MyISAM table with a gap. When we insert new rows and fill those gaps, does the table "immediately" get ready to accept "concurrent inserts" for future insert queries? Or do we need to run "OPTIMIZE" before the table knows there are no gaps? 

We're using Percona's pt-table-checksum for MySQL replication integrity check. Is it possible to pass some or all of the options using some sort of configuration file? For instance, can we write the list of tables we want checked in a file? I understand the manual does not talk about such a file: $URL$ 

If we query for "WHERE receiver_id=5", partition pruning will not kick in, right? It will need to search all partitions. But if we were to search for "WHERE sender_id=5", then we would immediately know the result is in p0. So for a table where two columns could potentially be individually important, partitioning might not be the best solution since now we lost the benefit of a full-table index for the secondary column(receiver_id, for this case) in the columns parameter. Is that right? 

Since most of the information schema is in the form of views against pg_catalog you can get a big chunk of it using : 

Once you've learned Postgresql then pick another RDBMS and learn their SQL dialect-- compare and contrast. Rinse and repeat. 

In psql you can to make psql show you the queries used to generate the output of the commands. I've found these queries to be very useful as a starting point when digging metadata out of databases. 

If your problem is file fragmentation then no, there isn't. In Postgres each table gets it's own file, or set of files if it uses TOAST, in the file system. This differs from, say, Oracle (or apparently MS-SQL) where you create pre-sized tablespace files to drop your tables into-- although even there you could have file system fragmentation issues if the tablespace files get extended or the file system is badly fragmented to start with. As to your second question... I have no idea how to would cleanly deal with the file system fragmentation as MS-Windows is the only OS where I've experienced fragmentation issues and I don't run MS-Windows any more than absolutely need be these days. Perhaps placing the database files on their own disk(s) could mitigate that to some extent. 

It is certainly possible that joining complex large views can cause additional overhead but the best way is to test. Create queries on the base tables that yield the same results as using the complex views. Then run a series of queries against the base tables and compare the same series against the complex views. I've seen one database structure where all application access was through views and speed of response issues started appearing with the reporting tool as the size of the database grew. As a_horse_with_no_name points out there are built in tools to help you analyze what is going on under the hood. With large applications it is hard to point to one thing and definitively say "this caused the problem". Test in development, change one thing, test again and repeat. This approach keeps cautious managers happier than just saying "the problem is this". 

We use Lucene.Net to search for contacts on our Oracle database. It's a bit of an investment in coding to set up the indexer, index and searched fields but once it's done you take advantage of the many search options. Users report the usual problems: enter too wide a search term and get back too many results but it is really good for names where the name entered is, for example: "Robertson" but your search is "Bert" 

With mysql 5.7 you can consider moving your undo logs to separate files: $URL$ This will also give you the option to truncate those logs when you like: $URL$ In my.cnf this setting should do the trick: 

pt-online-schema-change by Percona shows remaining time estimate. By default it prints out the remaining time estimate and progress percentage every 30 seconds. It also has additional functions compared to just running the ALTER command by itself. $URL$ 

Is there a way to separate memory allocation for databases in mysql on the same server? Say for database_1, we would like to allocate 20GB RAM, and for database_2, 10GB RAM. Is there any way this is possible. 

When starting mysql on Centos servers with a custom data directory you might get an error similar to the following: 

If your chmod and chown settings are already correct you might need to look into selinux. This page explains how to configure selinux for mysql: $URL$ 

By Oracle 11 everything you can do with a CTXCAT index you can do more with a CONTENT index. Why not use that? Edit: The op asks if CONTENT indexes must be synced. Yes, this is correct. I find the time and load factor to do this is entirely acceptable for smaller records sets. Your mileage may vary based on record size. Further reference: 

My opinion is that unless there are performance or other application defects that are only seen on the production stack then access should not be granted on production. A view of the v$ views you mention on development should be fine. Security procedures vary wildly from company to company and factors that are external to Oracle security are more often than not the deciding factors. In short, only give access when you are satisfied that there is a real documented need that is supported by management. In answer to the OP's question of what can you do with v$session I didn't know this till I looked for it but here is an example of what you can do with v$session. That's the thing about security, it's not what you can imagine people can do with information that is dangerous. It's what you don't know about. To a point, less access to interesting system information is more security. 

Before setting up a replication within MySQL, the data first needs to be transferred to the slave. Since MyISAM tables can be copied as files. Would it be okay to copy MyISAM files, .myd .myi .frm, to the slave? Would this be enough to start the replication? 

Unfortunately, this setting appears to be available only on fresh mysql installs. You cannot just restart your server with this setting, you will get an error if you do. But, since you will also probably want to shrink your ibdata1 file, you will need a full dump anyways. 

Just updated to MySQL 5.5 and this issue is resolved. Tried it with files as large as 10GB and MySQL is not affected at all. 

Even though we are using "innodb_file_per_table = 1" from day one. The ibdata file still continues to grow. Total size of our database is around 800GB and the ibdata file is currently around 50GB. All our tables are innodb. How can it grow when we use innodb_file_per_table? Could it be blobs, mediumtext or these kinds of columns? Is there a way to shrink this considering we already use innodb_file_per_table? 

This yields the same subset as using COMPOSE or CONVERT with AL32UTF8. Two strings that are identical as far as upper/lower case, white space, invisible characters but have a different length. Security restrictions mean I cannot post real data. 

I assume you are using the 64 bit version of Oracle to install. I assume you have Windows 7 x64 - Professional, Enterprise, or Ultimate edition In the case of unusual errors it would not be impossible for something silly like the language pack in use on Windows to be the cause. Try starting with a new Windows install that has not been customized or has had any other Oracle products installed or removed Check the install logs. With all the versions and folder name changes I am not 100% sure where but you should look in the c:/Windows/oracle or in the install folder if located on hard disk 

I get hundreds of results that are identical for case, white space and hidden characters. I verified the identical nature using two text tools. The only difference I can find is the length of the description due to the difference in character set. I tried using COMPOSE, CONVERT and CAST to only detect real changes in the description but this gives the same results as using only an equivalence operator. How can I compare two strings in SQL that use different character sets and find the ones that have different text? Edit: Raj asks if I have used CAST as in