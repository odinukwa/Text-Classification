One model for the hyperbolic plane is Minkowski space. This is 2-space, say, with the caveat that the dot product is negative on the last coordinate, i.e. $(x_1,y_1)\cdot(x_2,y_2)=x_1x_2 -y_1y_2$. If we want to define the length of a vector to be the square root of a vector dotted with itself in this inner product, we need to require that $x^2\ge y^2$. One can then check that the point $(0,1)$ is distance $1$ from the curve $(t,1-\sqrt{t^2-1})$, and likewise for generic points so that the distribution of points equidistant from a fixed point is a hyperbola. Using calculus, one can compute the length of the arc along the "unit" hyperbola starting at $(0,1)$. This is how the Greeks defined angles, if I am not mistaken, though they did so without calculus. One can then define the hyperbolic cosine as the inverse function of the length of the above curve, and so forth. I think this is probably how it would begin. Defining the hyperbolic trig functions as combinations of exponentials would likely come later. Perhaps there is a way to come up with the length of the hyperbolic arc without calculus? 

There is a nice relationship between convolution of probability measures and random walks which is very clear on finite groups. For a particularly concrete example, suppose you are shuffling a deck of cards. You can model this as picking elements of $S_{52}$ according to some probability measure $P$ on $S_{52}$. This generates a Markov chain with transition matrix whose $(s,t)$-th entry is given by $P(ts^{-1})$ --- if I am permitted to abuse notation somewhat, the element $ts^{-1}$ is the shuffle that takes the deck from ordering $s$ to ordering $t$. If one wants to know the transition matrix for two shuffles, this corresponds to the square of the original matrix. One can then check that this new matrix corresponds to constructing the transition matrix generated by $P*P$, that is the matrix whose $(s,t)$-th entry is given by $(P*P)(ts^{-1})$, and in fact $k$ shuffles corresponds to the the $k$-fold convolution of $P$ with itself. Convolving two different probability measures then corresponds to shuffling your deck according to one technique and then a different technique. 

A more concrete way to fix the OP's idea (which is similar to Stefan's but avoids Cantor-Bernstein) is to simply delete $\mathbb{Q}$ from $A$ to produce a new set $B$. Split $B$ into a countable family of sets $B_k$ where $B_k$ consists of all the elements of $B$ with $k$ leading zeros. There is now an obvious bijection between any $B_k$ and any set of the form $[n,n+1)-\mathbb{Q}$ by simply viewing elements of $B_k$ as sequences in binary instead of decimal and ignore the leading zeros and first 1. There is no need to worry about the OP's original concern since $B$ only consists of irrationals. Since there are countably many $B_k$'s and countably many $[n,n+1)-\mathbb{Q}$, pick your favorite way to match them up. The remainder, $A\cap\mathbb{Q}$, is obviously countably infinite, so biject it with $\mathbb{Q}$. 

Consider the problem of shuffling a deck of cards using some shuffling technique. One may wonder, "If I use this or that shuffling technique, will I shuffle the deck?" The problem can be transferred to a question of whether a particular finite Markov process converges to the uniform distribution or not. Omitting some details, a classical theorem says that, yes, the process will converge (to the uniform distribution) as long as your technique is reasonable. Not only that, the convergence will eventually be exponential. This seems like a useful theorem, but it is actually rather deficient. The problem is that your practical side may wonder how many shuffles are required to get the deck reasonably randomized, and this theorem doesn't help. So you might say that the original analysis was soft because the result does not help in solving this quantitative problem. It tells you that shuffling is a good idea, but it doesn't give you any clue whether a given technique could shuffle the deck in your lifetime or not. A hard analysis would tell you, for example, "If one defines reasonably randomized by measure blahblah, then $2\log_2(52)$ riffle shuffles are sufficient to randomize the deck." 

If P holds for f and g, then P holds for f+g. If P holds for a bounded, convergent sequence $f_n$ then P holds for $\lim f_n$. P holds for characteristic functions of measurable sets. 

I'm assuming that non-mathematical subjects, like physics, don't count --- there the heat, wave, Schr√∂dinger, KdV, water wave equation, Navier-Stokes, Helmholtz, ..., equations are all fairly important objects. In fact most of the PDE I could name would be related to physics in some way. I would say that most PDE are in this direction. In some sense, the entire field of complex analysis comes down to genuinely understanding solutions to one PDE; complex analysis, I think you'd agree, is a pretty big field, with plenty of applications of its own. A number of tools have been produced by PDE which are of universal appeal in analysis. For example, the Fourier transform, which has a broad range of applications in analysis, not to mention generalizations, e.g. the Gelfand map, was developed as a tool to solve the wave equation. Another is the convolution (which I'm assuming is also from PDE) and along with it a variety of dense functions, nice partitions of unity, and so on, along with notions of convergence which are also very useful in a variety of contexts. Things like the Poisson kernel and the Hilbert transform have become prototypical examples in integral operators. PDE in general are rather hard, and so any particular PDE is likely to be rather narrow in scope. So many of the things of greatest interest to come out of it are tools to solve problems rather than necessarily specific solutions. 

The important thing about Euler is he saw an approximately correct path to the correct solution. These kinds of people have always been the most regaled in mathematics. Whether or not he could write proofs to the degree we expect today is probably immaterial: he improved the human understanding of mathematics more than anyone in his generation and probably more than all but at most a handful of mathematicians in all of time. If he could continue to generate almost correct proofs/heuristics to produce correct answers to problems, he would no doubt find himself with employment as well as plenty of coauthors eager to check his pencil marks. Mathematical rigor is important because intuition is too frequently wrong. But I think seeing the big ideas is still considered more valuable than getting the proof completely right. Riemann's proof of the Riemann mapping theorem was flawed because it made use of the Dirichlet principle to a greater extent than is actually possible (as shown later by Weierstrass). Even if Riemann turned out to be somewhat wrong and the theorem did not hold to the generality he believed because the intuition of the day was that the Dirichlet principle was universally sound, would we not call it the Riemann mapping theorem (and instead name it after whoever gave the first complete proof)? In a similar-but-somewhat-different light, should Thurston's geometrization conjecture-now-theorem have a different name? 

Suppose that $f$ is in $L^2(\mathbb{R})$ and consider the set of integer translates of this function, $V=\{f(x-k):k\in\mathbb{Z}\}$. This set is linearly independent: taking the Fourier transform of the finite sum $\sum a_k f(x-k)$ one gets $p(e^{2\pi i\xi})\widehat{f}(\xi)$ for some polynomial $p$. If the sum is zero and $f$ is nonzero, then $p$ must be zero on some set of positive measure; this is an infinite set, implying that $p$ must be the zero polynomial and so each $a_k$ must be zero. I find this to be an especially nice application of the Fourier transform. My question is this: does there exist a proof of this fact which does not use the Fourier transform? The $L^2$ condition could be modified, but obviously one needs some kind of integrability condition to disallow the constant functions. One can prove this using a variant of the Fourier transform, so I should say that I'm really looking for a proof where you don't integrate against complex exponentials. As for why $V$ would be an interesting thing for mathematicians to look at: the closure of the span of $V$ (in $L^2$) is one of the fundamental objects in wavelet theory --- a principal shift-invariant space. 

Non-closable operators are not necessarily nasty. Consider the usual Hilbert space $L^2([0,1],dx)$ and the dense subspace $\mathcal{D}=\mathcal{C}[0,1]$. Define $T$ on $\mathcal{D}$ by $T(f)=f(0)$, i.e. the constant function on $[0,1]$ with value $f(0)$. This is a densely defined operator, but it is easy to see that its adjoint is not densely defined. It is not hard to show that in the class of densely defined operators, a closed linear extension exists if and only if the adjoint is densely defined. 

While I'm not saying anything new, I feel the responses thus far either miss the point or are not very complete. Generalized functions (aka distributions) are defined as linear functionals on some class of functions, typically referred to as test functions. To begin with, one usually wants any locally integrable function to be a generalized function. If $f$ is any locally integrable function then the generalized function corresponding to $f$ is just the linear functional $\int f\phi$ when $\phi$ is a test function. So the obvious first choice for the space of test functions is the space of compactly supported functions since integrating a locally integrable function against a smooth compactly supported always makes sense. Then one can define the derivative of a generalized function, say T, to be the functional T' which satisfies $T'(\phi)=-T(d\phi/dx)$ whenever $\phi$ is a smooth, compactly supported function. If T can be represented by a smooth function, then this is just the integration by parts formula, which makes sense since $\phi$ is compactly supported. So the function $e^{e^{e^x}}$ is a perfectly reasonable generalized function in this case. As said a number of times above, one would also like to define the Fourier transform of a generalized function via the formula $\hat{T}(\phi)=T(\hat{\phi})$. The problem with the space of compactly supported functions is that the Fourier transform of a nonzero compactly supported function is never compactly supported. So $T(\hat{\phi})$ might not make sense if T is allowed to be any locally integrable function. In particular, suppose that $\phi$ was some smooth function of compact support whose Fourier transform goes to zero slower than something like $e^{-x^{10}}$. The function $e^{x^{11}}$ is locally integrable and hence a linear functional on the space of compactly supported smooth functions, but it is easy to see that $\int e^{x^{11}}\hat{\phi}$ isn't going to be a finite number. The Schwarz space is nice because the Fourier transform of a Schwarz function is a Schwarz function. So given any linear functional T on the Schwarz space (such a T is called a tempered distribution), one can define the Fourier transform $\hat{T}$ of $T$ via the formula $\hat{T}(\phi)=T(\hat{\phi})$ when $\phi$ is a Schwarz function. This formula will always make sense when T is a tempered distribution.