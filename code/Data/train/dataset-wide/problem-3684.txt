Doing this on Apache level is not only impractical but introduces a huge overhead. If you need to rate limit per IP the best way is to do it within OS's firewall (IPFW in FreeBSD for example). Not only will it be more flexible in the long run, since it's running on system level the filtering is done at lighting fast speeds. In regards to actual API implementation of this, you should be handling this within your API application not Apache. Record requests to a fast medium like Memcache and have cron retrieve and store the data within database for processing. When user XYZ reached threshold, simply impose it within the handshake or next request. 

Procedure will not create temporary tables unless you tell it to create them. Perhaps it's best to split up that 'business logic' in a few different iterations. Also make sure you are using proper indexes. 

Why not run that same command again with the proper permissions then apply your desired permissions to sites/default? 

I wouldn't recommend it. There ARE still methods of cracking these quite easily. I personally recommend a Truecrypt volume that contains a Keepass database. It servers me well and is extremely portable. And I'm using it in an environment with thousands of passwords. EDIT: And Keepass is already well laid out for password management. With a nice GUI(i.e., easy to see what password is which type) and built-in password generators...can't go wrong. 

I'm assuming you mean block access for incoming users and not for YOUR users connecting to the site(external)? If the former: Look into apache mod_rewrite. That'll do what you want. If you're not using apache, you may need to consult with someone else. If the latter: Look into setting up a proxy server. It'll depend on what OS you're using. 

Do you have open_dir restrictions in effect? If this is on user account I would set APC's tmp directory relative to users home directory with proper permissions. 

@Tom O'Connor this is not a bandwidth/pps type of DDoS. Sounds to me like a simple service denial. Keep alive will make it worse, you problem here is that Apache can't process requests as fast as it should and spawns a lot of workers that are unable to keep up with requests. As this grows the chances of recovery are pretty much at zero if attack continues. You can obviously increase MaxClients directive but from what you described it will just make you go down a minute or two later. I'm not sure what stack you are running but the goal for you is to simply improve Apaches response to a single request (are you running PHP? Is it connecting to MySQL? Are you not caching?) page that loads within 0.010 seconds will respond 100 times better to a service denial .vs page that looks up tons of stuff in MySQL and finishes in 2 seconds per request. If somebody makes 100 requests, your server has to work for 200 seconds but since it does it all at once that 2 seconds/request is now 40 seconds/request * 100. More requests, more load. Another way to address this is to identify top xyz connections and simply block them, but this will be a little bit more tricky and requires a bit more knowledge to properly attempt. 

I just spun up two EC2 instances and got an elastic IP for each. I can't seem to get it to connect. Here is the bulk of my config: 

We have a bunch of existing servers in EC2. Future servers are created with Cloudformation with Cloudwatch integration. However, I need to setup Cloudwatch for servers that weren't created with Cloudformation. I have been asked to create a Cloudwatch Cloudformation stack. Is it possible to just create alarms in Cloudformation? If so, how do I specify which servers to monitor? Thanks! 

I've setup IPSEC tunnels between 3 management VPCs in 3 distinct AWS regions. Each of those regions has additional VPCs (dev/prod) that are peered to the management VPCs. It's setup in a hub/spoke like this: 

The requests per second is magical number and makes no sense. I can serve 10,000 requests per second with web server running hello world and 0.0001 requests per second with some bloatware behind the same web server. What kind of server do you need? You need a person that knows what they are doing not a new server. But to answer your question 'a big kind'. 

Your IRCD is only as stable as your uplink/server. Most of the mature IRCD daemons will do just fine. 

Oh boy, somebody is using fancy words. Your question has more buzz words then the actual context, and yes it depends what you are serving. High traffic for a file hosting site might be measured in outgoing bandwidth, high traffic for a regular web site might be measured in requests/p seconds, etc. 

I think you're going about this the wrong way. Look into tuning Apache first of all. Then, research Linux memory management. You WANT your server using the ram, otherwise why do you have it? 

I have approximately 30 users on a box. Those users are in overlapping groups(about 6-10 groups). I need them to be able to land in a specific folder based on their group assignment when they FTP in. I.e., group1 -> /tmp/site1 group2 -> /tmp/site2 Is this at all possible with VSFTP on a SuSE box? Using SFTP isn't an option unfortunately. Thanks! EDIT: And in the event of a user being in several groups, just dumping them to the highest-level folder necessary to view the various folders they have access to. 

It's disabled by default because a lot of LAMP configurations are on shared environments that might or might not like their users loading random extensions without administrators permissions. You seem to know what you are doing, so there should be no security risk unless you will be using the same configuration on a server web server where you have untrustworthy users. 

Okay I have not used this for a while, but appliance we used pulled logs from our web servers via transfer and we had to tell it to parse them manually. This was back in the day so things might have changed. If you are still using that process I would advise checking the parsing scripts/permission/data of logs. Your next best bet is logging into the shell of the appliance and looking at /var/log/ for any clues. 

I've seen this done before. It's only as insecure as your network/destination servers make it. Only you know that. Are you transmitting these over a secure network? If so, you SHOULD be fine. But we can't possibly guarantee that. Why not write a simple ssh script to distribute them? That's what I would recommend. Or write a script to download the cert from a central server and distribute the script via puppet. Just an idea. EDIT: Since there is some confusion. I'm NOT saying Puppet/SSH are anymore secure. But if you're worried about unauthorized access, ensure everything is secure. This is most easily done with a custom SSH script YOU distribute. 

We're presently using googledocs(word processing, spreadsheet and email) for everything. We'd like to move away from remotely hosted software. We have a large internal infrastructure to support this sort of thing so technical resources won't be an issue. Are there any decent alternatives to googledocs that meet the following requirements? Open-source Local Hosted option Collaboration/multi-user support Word processing Spreadsheet support Privacy features Importing/Exporting Minimal installation footprint I've been leaning towards a collaboration suite, however, I was also wondering if there is some way to collaborate within Openoffice? Thanks!