The light from the Sun spreads, at least initially, in an isotropic fashion into the universe. As it gets further from the Sun, some of that light will interact with the interstellar medium (ISM) and therefore some of the energy emitted by the Sun will be used to excite atoms and molecules or even ionise some atoms. This will be the fate of almost all the light which is emitted from the Sun towards the plane of our Galaxy, which contains sufficient molecular gas and dust to block starlight travelling through it for any distance. We know this happens because we can "see" dark clouds in the Milky Way, that can be penetrated by longer wavelength radiation to reveal all the billions of Sun-like stars that lie behind them. Roughly speaking, about half the visible light from the Sun will be absorbed every 1000 light years when travelling in the Galactic plane, so it is essentially all absorbed within a few thousand light years. But most of the Sun's light is not travelling in the direction of the Galactic plane, and interstellar and intergalactic space has a very low density of gas and dust. The equivalent extinction number for the intergalactic medium is that light travels many billions of light years with almost no chance of being absorbed (see Zu et al. 2010). This means that most of the light from the Sun will travel to cosmological distances (billions of light years) over the course of the next billions of years. Indeed light emitted from the Sun shortly after its birth has already travelled 4.5 billion light years. We know this has happened and will happen, because we can observe galaxies (the light from which is nothing more than the summation of light from many stars like the Sun) that are 4.5 billion (and more) light years away. As the Sunlight travels towards cosmological distances, its wavelength is "stretched" by the expansion of the universe, becoming redder and redder. We know this happens because distant galaxies have redshifted spectra. If the universe keeps expanding, then its density will continue to decrease and there is little to stop the radiation from the Sun travelling on forever, with a wavelength that scales as the scale factor, $a$, of the universe. If we follow a co-moving and co-expanding cube containing the Sun's radiation as the universe expands. The total radiative energy inside that cube diminishes as $a^{-1}$ - that is, the energy content of the universe in the form of radiation from stars (and other sources) becomes energetically less important as the universe expands and appears to be being superseded by the energy contained in the vacuum itself (a.k.a. dark energy). In conclusion, most of the energy emitted by the Sun is not "used" for anything; it propagates into space, becoming more and more dilute. 

The spectral type of an object is almost entirely determined by the temperature of its photosphere. ie Saying something is type M3.5 is just a measure of its surface temperature. An M3.5 brown dwarf is at a very similar temperature to an M3.5 star. Brown dwarfs begin their lives as hot balls of gas and gradually cool with time. They start off as M-type objects and then cool to become L-type and then eventually, T-type objects. Stars on the other hand, start their lives by cooling, but stabilise their temperatures at a mass-dependent value that corresponds to an M-type classification or even as cool as type L2 for the very lowest mass stars. Given that stars and brown dwarfs appear to have been born throughout Galactic history, then a range of masses are possible for any spectral type. In order to use the surface temperature (or spectral type) as an indicator of whether something is a brown dwarf, then you also need to know its age. In general that is not known, unless the object can be demonstrated to be part of a cluster of stars (of known age). In order to be a brown dwarf the mass should be below about $0.075 M_{\odot}$, but unless the object is in a binary system, this cannot be measured. So what people do is trust the models(!) and compare the luminosity (or estimated temperature) with model evolutionary tracks at a given age. This gives an age- and model-dependent mass that can be used to claim that an object is a brown dwarf. The fact that brown dwarfs can have an M-type spectral class when they are young, but then move through L- to T-dwarfs as they get older is not a problem, it is a confirmation that the evolutionary models are (roughly) correct! A plot might help (from the work of Burrows et al. (1997)). This shows the evolution of temperature with time. Each track represents a brown dwarf of a given mass (the tracks from top to bottom) are masses of 0.2 - 0.08$M_{\odot}$, representing stars (in blue). Then come masses of 0.07 to 0.015$M_{\odot}$ representing brown dwarfs (in green, that will never ignite hydrogen in their cores to any extent). Then below this are the "planetary mass objects" from 14 Jupiter masses to 0.3 Jupiter masses at the very bottom (in blue - the definition here is that these objects don't even ignite deuterium in their cores). The horizontal lines mark the boundaries of the spectral type classifications (M at the top, L in the middle, T at the bottom). Notice how the stars level-off in temperature (when they begin H-burning) but can have M-type or even L-type classifications, but the brown dwarfs cool throughout their lives. Even objects as low as $0.01M_{\odot}$ begin their lives as "M-type" objects. To answer some of your more specific queries: The warmest spectral type below which you can almost guarantee that an object is a brown dwarf is $\sim$ L2. Any object warmer than this could be a star. A brown dwarf is always cooler than a low-mass star of the same age. Current observational estimates are that there is about 1 brown dwarf for every 4 low-mass stars - the weighted mean ratio for stars between 0.08 and 1 solar mass to brown dwarfs from observations of several clusters is $4.3 \pm 1.9$, according to Andersen et al. (2008); the commonly adopted/assumed "Chabrier Initial Mass Function" has 4.9 stars for every brown dwarf. The spectrum of a brown dwarf and an M-dwarf star at the same temperature are very difficult to distinguish. The M-dwarf is only slightly bigger and this might lead to small gravity-dependent differences. Lithium is not burned in brown dwarfs less than $0.06 M_{\odot}$, whereas it is burned completely in low-mass stars on a mass-dependent timescale. Thus an older (than say 1 billion years) M-dwarf star would certainly have burned its lithum, whereas a younger M-type brown dwarf of lower mass would still have lithium. 

The elements you mention (actually are all "islands of nuclear stability". Whilst their binding energy per nucleon is not as high as that of iron, it is a little higher than elements immediately around them in the periodic table. Nuclear fusion reactions are exothermic (up to iron), but they require energy to initiate them (in a similar way that you need to get coal hot before it will burn). This ignition becomes more difficult the higher the atomic number (the number of protons) in a nucleus. This means that it is not a given that nuclear reactions will simply proceed to the element with the highest binding energy. If there is a way to prevent the material in a star reaching this ignition temperature, then the reactions will not proceed through to iron. This means that even in a very massive star, it is only the core where burning runs to completion in the iron peak elements. The majority of the star is still hydrogen and helium, with the layers outside the core containing the remnants of burning of lighter elements. It is these products that get blown into space in a supernova and mostly consists of products of helium burning (C/O) and alpha capture onto these elements (Ne, Mg, Si, S, Ar, Ca), with each being slightly harder to produce than the last and accounting basically for their relative abundances. Most iron would end up trapped in the neutron star or black hole remnant. Iron is mostly produced following thermonuclear detonation of C/O white dwarfs - a completely different production path. Hence the abundance of Fe is decoupled from things like Oxygen, and more dependent on the rate of type Ia supernovae, governed by binary interactions and statistics than the details of nuclear physics. Carbon and Nitrogen are mainly produced in lower mass stars that never become supernovae and where conditions never reach ignition temperatures to produce heavier elements by fusion. There are many more of these than stars which end as supernovae. 

Here are some thoughts adapted to an answer I placed on Phyiscs SE to a similar question some time ago. In order to observe the past we need to detect light from the Earth, reflected back to us from somewhere distant in space. The average albedo of the Earth is about 0.3 (i.e. it reflects 30 percent of the light incident upon it). The amount of incident radiation from the Sun at any moment is the solar constant ($F \sim 1.3 \times 10^3$ Wm$^{-2}$) integrated over a hemisphere. Thus the total reflected light from the Earth is about $L=5\times 10^{16}$ W. If this light from the Earth has the same spectrum as sunlight and it gets reflected from something which is positioned optimally - i.e. it sees the full illuminated hemisphere. then, roughly speaking, the incident flux on a reflecting body will be $L/2\pi d^2$ (because it is scattered roughly into a hemisphere of the sky). Now we have to explore some divergent scenarios. 

On the basis of the recent detection of GW170817 and a host of other observational evidence, it seems that a neutron star merger is one way to get mass out from a neutron star - maybe some tenths of a solar mass. There is also evidence that the ejected material from the collision is neutron-rich, at least initially, and then produces neutron-rich nuclei through the r-process. It is impossible to have small lumps of stable neutron star matter. A high density is required to prevent the neutrons decaying (see $URL$ ) The (theoretical) minimum mass for a stable neutron star is of order 0.1-0.2 solar masses, though none have been seen in nature.