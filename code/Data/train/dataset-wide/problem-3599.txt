I use apache2-mpm-itk to run one of my web applications with a custom uid and gid, both for audit and security reasons. This works nicely, however I am not sure what kind of privileges this user would need. I am currently doing something like but that creates pretty much a regular user. So first a newbie question: how do I add a 'system' user that can that cannot be used for e.g. ssh'in or does not appear in the login screen on Ubuntu Desktop? And what kind of privileges would I need to give this user? E.g. I assume that www-data can write to the Apache log files, so it needs something more than a regular user. But at the same time I don't want to give it more privileges than necessary to run my webapp, definitely not root. 

I have been using cloudflare CDN on my website (wordpress) for about 4 months, mostly because I was hoping the CDN would make things faster, and make the load on my cheap server somewhat smaller, especially because of the many static .js and .css and .png files that wordpress contains. However so far my experiences have been less than optimal. I am not actively monitoring, but I have noticed downtime on cloudflare CDN servers, cloudflare Nameservers, and even the cloudflare website itself. However, the cloudflare status page rarely shows downtime, usually everything is green OK. Now I am wondering if I am doing something wrong here, or if more people have this experience? Assuming that Cloudflare knows what they are doing, I assume that downtime is caused by DDOS attacks on sites that use Cloudflare as well. Is it a good idea in the first place to use a public CDN for a small site like mine that is not at high risk of ddos attacks? 

How can I modify the policy to allow httpd to read ? Background: My web application connects to a mysql database. Instead of hardcoding the credentials in the code, they are stored in a mysql group in : 

My application relies heavily on AppArmor for security. I use Ubuntu to host it myself, but I have gotten requests from others that want to host in on a Fedora or RHEL machine. Now I am aware that Redhat prefers people to use SElinux instead of AppArmor. However, I have looked into it, and I think it is going to be very hard to translate my AppArmor profiles to SElinux policies. Furthermore, I don't think it is very linux-like to force software on the user. Ubuntu for example supports both SElinux and AppArmor and leaves it up to the user. I don't see why Redhat wouldn't do the same. Anyway, most of the stuff on google about rhel + apparmor is pretty outdated. How is the support of AppArmor in the latest versions of RHEL / Fedora? Is it possible to get it working without manually building a kernel? Are there any packages available? 

I have been using this module for years and it has always been stable. I have recently been installing some backports from PPA repositories, I suspect that one of these packages has broken my system... 

I have a service daemon that creates a lot of temp files. Recently my server died, because a malicious user managed to flood /tmp and fill up the disk. I have taken some measures to actively clean up the temp dir, but additionally I would like to constrain the max size of this applications temp dir. Is there any way I can create dir, say, that will never be larger than e.g. 10G? I know I can set disk limits by-user, but I just want to limit this tmpdir; the application should always be able to write elsewhere. I am running Ubuntu Linux 12.04. edit: All of this should eventually be wrapped up in an installable Ubuntu package though. So I don't think I want to rely on modifying the partitions, unless I can somehow simulate it. 

Everything works fine, except Apache2 some how overrides the response header for html files. If I request the content type is , whereas for I get to see the correct page, but the content type is set to making the browser display unrendered html code. It only seems to happen for html files. Why is this happening? 

Recently I grabbed some preconfigured CentOS 5.0 instance from EC2 and installed our stuff on it from rightscale. The instance was a little old, but I assumed that it would be up to date after a single . However, for reasons beyond my understanding, CentOS is not updated beyond 5.0: 

I created a simple web application that consists of a dir with html, css, js. No server code. For reasons complicated to explain, my administrator insists on turning it into a .war file, so that it can easily be deployed on tomcat. Again, the application does not contain any Java code at all. I didn't create it with ant or eclipse. I tried creating an archive: 

Which does indeed create a war file. However, when I deploy this war on tomcat, it doesn't seem to work. Tomcat automatically extracts the war file to a directory but it does not become available through the web server. I guess it has something to do with that my war does not contain directories and . Is there an easy way I can turn a static client side web application into a deployable war file? 

In the web application I can simply refer to when connecting to the DB. This works fine in Debian/Ubuntu, but in Fedora it fails. The problem is that SElinux prevents from reading and therefore cannot resolve . When I disable SElinux, the problem disappears. 

One recurring problem with CORS is that the spec prescribes request headers get stripped from the preflight request (HTTP OPTIONS). However if the server requires authentication, this means the preflight request will fail (because the header does not get included) and it will not be able to receive the required header. The only way out seems to be to configure the server to not enforce authentication for HTTP OPTIONS requests. Is there any way in apache 2.4 that i can make conditional on the http method? 

Edit: the problem is related to the loading of an apache2 module. I found some traces of the problem in: 

How do I turn this into a script that enables this policy on my CentOS servers? I already enabled booleans for and but that didn't help. 

I.e. the name of the upstream backend should have been replaced in the response header. Is this a bug in nginx or am I doing something wrong? 

Turns out that installing the package will automatically create some self-signed certificates, with an out of the box configuration file 

I need an Ubuntu 11.10 server, but for reasons beyond my understanding, our host only preinstalls Ubuntu 10.04 LTS. So I decided to get a 10.04 server and use Ubuntu's do-release-upgrade to get to 11.10. I expected that do-release-upgrade would get me straight from 10.04 to 11.10. However, after running do-release-upgrade, I ended up with Ubuntu 10.10. So after reboot I had to run it again, and then it upgraded to 11.04, so then I had to run it a third time to get to 11.10. Is this expected behavior or am I doing something wrong here? 

Note the url has a slash in the end to note that it is a directory. Also note that Apache is "smart" by using the incoming request hostname in the redirect header. All fine so far. However, when using nginx with load balancing, this 301 is sent to the client without translating the nginx upstream name back to an actual server/domain. So the client will receive the following: 

Turns out nginx has quite a lot of proxy redirect options to deal with this sort of problems. I ended up using something like this: 

So I ended up moving over from mod_jk to mod_proxy_ajp, which I should have done a long time ago. It is much easier to configure and works out of the box. When using mod_proxy_ajp, adding a tomcat site is as easy as putting a file insite my /sites-enabled/ containing: 

I am creating installation packages for our software, which has a dependency on . Unfortunately, this package is not present in Debian squeeze, which ships only with the package . The upcoming release of Debian 7 (Wheezy) ships with both Tomcat 6 and 7. Does that mean I can take the source package from Wheezy, rebuild it for Squeeze and put it in our custom repository along with builds of our own software? Or will this be likely to result in conflicts on Squeeze systems somehow? There are instructions on several places how to backport tomcat, however what worries me is that Tomcat 7 is not part of the official Debian 6 backports project. I don't want to mess up the systems of any of our users. For example if they try to install our software on a system that already has installed, which I think conflicts with . In that case it should resolve this gracefully in the same manner as would happen on Wheezy or Ubuntu. 

I would like allow to executed files from a user's home directory. From I understand that this implies the following policy: 

Now my service is computationally very heavy, and I would like nginx to limit the number of active parallel (simultaneous) requests to a single upstream backend to e.g. 10. I looked into limit_req module, however this module only seems to focus on incoming requests per minute. I specifically would like to limit the number of active backend connections; i.e. take into account if requests have already returned or not. Is this possible at all? In Varnish this can be done using e.g. 

One of my server daemons uses a lot of space in /tmp. Because I don't want to reboot my machine when the server runs out of disk space, I need to run a CRON script that removes old temporary files. What would be good a way to recursively remove all files and directories under /tmp which are more than 1 hour old from user , say ? Of course it should not resolve symlinks and start removing files elsewhere on the system. I am using Ubuntu 12.04 and will run this cronjob as root. 

I am running into an odd problem for which I am not sure if it is a configuration issue or a bug in nginx. My setup is a nginx reverse proxy which has Apache2 backend servers. The load balancer is pretty basic similar to the example from the wiki, e.g. simplified: 

I would like to set a limit on the number of requests a single client IP address can make to my server, based on their IP address. Nginx has a directive. However, this directive will actually limit the average hits. If I set my limit to , it will actually allow only one hit every 10 seconds. Instead, what I need is an actual limit per minute. E.g. the client should be able to fire 3 or 4 requests quickly after each other or even simultaneously, but not more than e.g. 10 per minute. Is there a way to do this? 

My webserver hosts several subdomains (vhosts) of a website, say sub1.example.com and sub2.example.com. The only difference between these vhosts is the documentroot. Everything else is shared across vhosts. Now I would like to do the same for HTTPS, but of course ssl + virtualhost is tricky. The good thing is that my SSL certificate is valid for my complete domain. Hence I don't need to specify per-vhosts certificate. The only thing that I want to specify per vhost is the document root. The FAQ says: 

I am hosting a REST service which is sending appropriate cache-control headers. I use Varnish as a caching server in front of my webserver. However, a limitation of varnish is that it doesn't support caching HTTP POST and HTTP PUT. Is there any alternate caching server that will be able to cache these requests? I understand that caching POST is a bit tricky because you cannot just cache based on the url as a key like for GET; it needs to actually inspect the request body. In case of requests, there should probably be a limit on the size of the request body for it to be cached (so that big file uploads, etc won't be cached). Nevertheless I really want to be able to cache short HTTP POST, or at least the ones. 

Ah I solved the problem. Taking a close look at revealed that the version was hardcoded in the repository url, e.g: baseurl=$URL$ So I replaced "5.0" with "5" in every line and then did a followed by a which updated me straight to CentOS 5.7. 

I am running an Ubuntu server on EC2 ebs, and my application needs a lot of temporary disk space, allocated in /tmp. However, on ec2 the root drive which also contains /tmp is pretty small, around 10GB. All of the remaining disk space is mounted under /mnt. As a result, my application returns 'out of disk space' errors, because /tmp seems to be full. What is the best way to solve this problem? One thing I can think of is create /mnt/tmp and make a symbolic link 

I purchased some (cheap) HTTPS certificates from StartSSL. They work great on all browsers I tested. According to this post they are even recognized in IE 7 and 8, which good enough for me. However, one user complained that his WordPress RSS client/aggregator stopped working. His website runs the latest version of Wordpress and simplepie, but it does not recognize the StartCom CA: 

However I am a bit reluctant to mess around with something that is used by so many linux programs and tools. I am not sure if every program will correctly resolve the symbolic link, and not sure what it would to to performance. 

This problem only appears on Fedora, not on Ubuntu or Mac running the same version of curl. I suppose it must be related to nns then: 

I am using Ubuntu 11.10 which ships with Apache 2.2.20 and openssl 1.0.0e so I think I should be good. However, I can't get it to work. I already have default and default-ssl sites enabled. If I add a virtualhost like I would do for HTTP: 

I am using nginx as a reverse proxy to my backend(s). Configuration is pretty basic, e.g. the core is just: 

I still get a 404. Any ideas why the JkMount does no work outside of the even though other Locations do work like this? 

I would like to remove any packages that I installed from third party repositories. I recently installed some backports from PPA repositories, and I suspect one of them broke my Ubuntu 12.04 server system. Is there any way I can list all currently installed packages in that are not available in the repositories, or, which version is higher than the one available in the current repositories? 

Our web service software consists of a (precompiled) war file and a mysql database. We provide installation packages to deploy it on Ubuntu or Fedora, in a standard Tomcat7/MySQL/Apache2 setup using . Now some people have asked if they can try our software on windows. It should be possible, but I have no experience with deploying on an OS without a package manager. Is there a way to provide a self contained installer for windows that will include Java, Tomcat7 and MySQL? Something like wamp but then with tomcat and java instead of php? 

Or alternatively, is there a good way to add this script as a hook to the /etc/init.d/apache2 or /etc/init.d/httpd script? I am looking for a solution that works on both CentOS and Ubuntu. If I am totally approaching this the wrong way, any other suggestions are welcome as well. 

In Debian/Ubuntu, my packages depend on the package, to provide some self-signed https certificates, if nothing else. From the Debian page: 

So the problem appeared to be that the back-end server was not setting any headers for some files due to misconfiguration. When this happens, inserts a based on local configuration like in Apache. I have not been able to figure out what caused the different behavior between OSX and Ubuntu, but after we fixed the back-end server to always send a response header, the problem disappeared. 

On two Windows 7 machines in my LAN I have Symantec Endpoint Protection 12.1 installed. Things work fine, but several time per day I get to see the following warning: 

There are often backports available from more recent Ubuntu releases for the latest server release. For example, this repository seems to have some. Although they don't seen to be updated very recently. $URL$ You can of course also just grab the .deb for Quantal and install it on Precise using , but that is usually not recommended, due to potential changes in dependencies. Although I think in this case you'll be fine. 

Ah I think this can be done with the setting as decribed at the bottom of the limit_req wiki: If delaying excess requests within a burst is not necessary, you should use the option nodelay: