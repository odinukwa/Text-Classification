There is no need to rebuild an index if the data hasn't changed, particularly if the entire table is populated in a single batch. If the table has been populated over time, however, you may see some fragmentation. 

SQL injection relies on readily accessible data. You can restrict this by only allowing data access through a controlled form, like views or stored procedures. This would mean not giving the user any access to the raw tables, but rather just access to stored procedures or similar. For what it's worth, I wrote a series of blog posts that cover different aspects of permissions, execution context and ownership chaining. See (1), (2), (3) and (4). But first and foremost, you need to protect your app against SQL injection. This cannot be overstated, and other security measures can only augment it, never replace it. 

Short answer: No, you cannot partition by a column in another table, because that would cause problems whenever you change something in the "other" table - rows would magically have to move between partitions. Summarizing some of the views in the comments from Aaron, Kenneth and me on your question: 

When you have a specific date format in your varchar column, you can tell what that format is in order to get a correct conversion, independently of language or regional settings. In your case, 

If every process accesses all the objects in the same order, the example above doesn't result in a deadlock: 

Or you could build a primitive encrypted scalar function that uses a paramter as a primitive "password" of sorts - this would give you an additional layer of security. Using ownership chaining, you can allow your stored procedure to retrieve the value, while still denying "regular users" permissions to view the table/view/function's contents. Or, why not put the password in your scalar encryption function in the first place - just remember to add to it. Important: 

In this design, the notion of a "report" is implicitly what you decide it to be: a day, a week, a month, etc. You can add various elements depending on your needs, for instance: 

Look for a process where the "command" is something like "SELECT", "EXECUTE", "INSERT", etc, and where you have a high number in the CPUTime or DiskIO column. The SPID number for that process can be used with the KILL command to terminate that specific process: 

I have intentionally not pivoted the results because the PIVOT syntax needs you to hard-code the names of the columns, which in this case are dynamically defined in the UnitType table. While we're at it, you may wish to consider a few revisions to your schema: 

In the first query, we loop over a table, row-by-row. In the second query, the entire calculation is done with the entire table as a single "set", i.e. set-based. Obviously, most cursor-based patterns will be a lot more complex, particularly if they launch a stored procedure for each row in the cursor. There isn't a single trick for how to convert a cursor-based solution to set-based one. 

As for possible solutions, try changing the to - but note that this may lead to results that you don't expect, because returns NULL instead of a conversion error. If I were to guess, your problem is that there is a which won't convert to decimal, but that value is not found if you filter on . So if SQL Server filters the material first, your query succeeds, otherwise the conversion fails. For more details, here's a blog post I wrote on the subject. The performance problem that I mentioned is caused by the in the clause, which prevents SQL Server from using an index properly. The won't solve this, it only takes away the actual conversion error. For more on this, check out "sargable" expressions. 

Please note that (and similar manual workarounds) will produce silent errors in those cases where you actually do have an invalid date in your varchar column. Based on this question, I also wrote a blog post that may be of interest. 

You can build a view that "unpivots" the table and divides and by 12 for each row. The syntax would look something like this: 

They're two slightly different animals that can be used for the same purposes, as in your example. CROSS APPLY is your only option for "joining" table value functions and "expanding" xml documents, though. Some queries, particularly parallel queries, can exhibit vastly improved performance using CROSS APPLY, provided you have the requisite processor threads and indexing strategy. Microsoft MVP Itzik Ben-Gan elaborates a couple of great examples in this talk 

.. returns more than one row. If you limit it to return only the first row, or rephrase the WHERE clause to return only a single row, you should be ok. 

If you query this view, the server will still need to check that there is a matching record in for each row in and vice-versa. The more tables you add, the more joins. You would work around this by using , but I doubt that this will have the desired effect once you add a lot of tables. In my opinion, designing your queries to use just the needed tables (as they do currently) will be your best bet. If you need to improve query performance, I would look at creating specifically targeted indexes on frequently used tables/columns. Basically, it comes down to performance vs simplicity of use, but the performance penalty for your view solution may be severe. Building a stored procedure to actually update the view is a fairly simple matter. 

Logins (on the server) do not automatically correspond to users (in the database). For instance, "sa" does not have its own user in the database, but uses "dbo". This might get you started. 

Things you can do to reduce (you can never eliminate) deadlocks in a multi-user relational database: Reduce the duration of transactions. Any deadlock is the result of two processes competing for the same resources, so the fewer simultaneous processes are running, the less risk of a deadlock there is. This does not just apply to hour-long ETL jobs, it could very well apply to millisecond OLTP transactions if there are many enough. Performance tune your long-running queries or tweak your server infrastructure for better throughput. Also, I've seen examples where a process can hold a transaction unneccessarily. For instance, in some ETL processes, there's often just a single process loading data and you may not need transactional integrity at all - if it fails, just truncate everything and reload it again. Remember that a regular SQL statement, even without a / also implicitly creates a transaction (which commits as soon as the statement completes). Try placing locks in the same order. The textbook example on how to create a lock is: 

.. given that the table or common table expression contains 39 unique records with . You could add different filters for your purposes, like how many odd numbers: 

For the record, renaming database objects or columns may lead to problems with existing views and other database objects. Find a time when the database load is at a minimum (ideally a service window) and perform the entire change within a transaction - it'll block those two tables for everyone else while it's running, but this prevents anybody else from changing any data while you're performing the schema changes. 

Another issue with shrinking tempdb is that SQL Server keeps a lot of stuff in there, preventing it from shrinking. You'll find that even if you try to shrink tempdb, it just won't, unless you pretty much restart the server (either literally or by clearing all sorts of buffers, both of which can have a dramatic impact on your production environment). In summary 

The construct lets SQL Server "loop" over each row in and for each query in that table, it runs an aggregate query on the much larger table, for which we'll optimize with the following index: 

Solution Store dates as dates, store numbers as numbers. It's really that simple. Speak to your software people about this. After all, they type all their variables in C#, why so lax in the database? Short-term You could replace with if you're on SQL Server 2012 or newer. This returns a value instead of an error whenever the input won't convert. On older SQL Server versions, there's no easy way to do this - you could try a lot of conditions inside the , making sure that invalid dates are passed to as values. 

A #temp table works just like any other table in the sense that you can write SELECT, UPDATE, INSERT and DELETE statements on it. Examples: 

This is a rolling average, which is a windowed function in SQL Server 2012 and newer. You could solve it like this: 

You could try adding a non-clustered index with the same definition, that would make the index considerably smaller in size (particularly if your table is wide). You won't eliminate the scan, but it would be much faster. Clustered indexes contain the entire table's data, whereas non-clustered ones only store the specific columns that you define (as well as a pointer to the clustering key or row ID, but in your case those would already be in the index key). 

The common table expression returns a list of TicketID, CreatedMoment and MAX(StatusChangeMoment). Using this result set, we can apply a second aggregate in the SELECT statement below. What I'm doing there is a conditional SUM(). I'm adding 1 for each row that meets the criteria and 0 for each row that doesn't. NB: I've simplified your criteria a bit, I hope it's equivalent to what you're looking for. 

counts the number of unique values of a column or expression. returns different values depending on the outcome of different conditions. In this case, if the number of unique values is equal to or greater than three, we want to return 'PASS' otherwise 'FAIL'. Obviously, if you want to return a bit value (i.e. boolean), you could change the strings 'PASS' and 'FAIL' in the code into 1 and 0 respectively and cast them to bit using . 

You can set a SQL Server-style wildcard in the variable to determine which characters are "legal". In my example, I'm allowing lower-case and upper-case A-Z as well as numeric values and space. 

There's no replication or out-of-the-box solution to your problem, particularly since you don't have that type of access to the remote machine. As other commenters here have noted, you're probably going to have to build your own ETL logic, perhaps in one of the following ways: 

Important This is not a trivial operation, and it carries quite a few risks that you should be aware of. 

You can use DMVs to see who's logged in at the time, but this won't get you any historic information. 

As a matter of best practices, transactions should be kept as short as possible and never wait for user interaction; every time you perform some type of data or schema modification within a transaction, this places locks on the objects or rows that have been touched/modified, which keeps other users' queries waiting. This is turn can create chain effects that can bring your database server to a standstill. In the scenario you're describing, I would instead recommend you to make a copy of the data to separate "what-if" tables where you can make your modifications and review the results. Once happy with the results, use a transaction to merge the data of this table back into the original table(s). 

If you want to return the running total at any given time (including the first few dates as well), you would change the to something like: 

... rather be... ? I'm thinking that for all the records of any given partition, the should be the same, and you probably want to loop through each "segment", i.e. the , right? You may already get the correct results by accident with the current query because the records may be correctly ordered from the Index Seek on , but I would probably want to make sure by specifying the correct ordering. As a bonus, this might improve performance. 

Creating a SQL Server Agent job (owned by a sysadmin member) will do the trick, although I realize that this is not a very pretty solution. The user can start the job (ansynchronously) using msdb.dbo.sp_start_job. Running an Agent job synchronously, however, requires a few more lines of code if this is a requirement. Also, obviously, you need to have the SQL Server Agent service up and running. 

I wrote a script a while ago, that does just this. I've posted it on my blog, $URL$ Remember that a Windows user can be a member of a Windows group, and in SQL Server, you can't see those memberships, so you'll have to look at Windows users and Windows groups separately, if you're doing a security audit, for instance. If you want to read up on SQL Server security, there's also a series of posts on the blog on that subject. 

The goal is to make the table's primary key column, , an column that will start at 21 for the next record that gets inserted. For this example, the column represents all of the table's other columns. Before you do anything, please read the warnings at the bottom of this post. First off, in case something goes wrong: