now I wrap the script into a procedure that I create called that receives as parameter and check if is running: 

Looking for missing indexes on my distributor databases, I surprisingly find too many. the tables that need to be indexed are generally 3: 

the solution that I thought involves creating a calculated column on table2 called the_level which will give you the exact level depending on the ActivityCode. the second step is on your query, where you join table1 and table2, there you need to identify whether or not the item is entered within the timeframe set by the query, and change the the_level accordingly. the_level has been created as PERSISTED, you did not mention indexes and performance but you said billion hours, so you will need some indexes there. this is an example of calculated column with case and not null. I have, in the past, create calculated columns that reference a different table, but in my environment the performance was not the best. Somehow it slowed writes, but I had an index on the calculated column. I am not stating this is a rule, but I would monitor it before doing it on a busy updated table. 

Now I want to delete a specific record from my order table. this is the record I want to delete: Using the following script I generate a script to view all the records that I need to get rid of , before I can delete my in the order table. 

I would like a script that would tell me at any point what transactions I have currently open on this and\or on the involved tables for example in the script above ,. I have been looking at the following interesting links: GUIDE FOR READING SQL SERVER TRANSACTION LOGS Questions About SQL Server Transaction Log You Were Too Shy To Ask so far I have: 

there are things that could be changed, but that is all up to you, plus if you want to get rid of the nulls and replace them by zeros please have a look at the following question and the answers at: How to replace (null) values with 0 output in PIVOT And that will also give you some ideas as how to produce all this in a dynamic way. 

The script generated should be run in the publisher (server and database) this is the picture from the replication monitor: note on the right hand side before there were 2 lines, one with a red error cross on it, that was the subscription we wanted to delete and now it has been deleted: 

I need to delete duplicate rows from a large table. what is the best way to achieve that? currently I use this algorithm: 

If I have to grant select on how can I find out what select permissions I need to grant in any other databases that because of the synonyms also need to be granted? 

...which would suggest a performance degradation if your server cannot offer the resources it requires. Just to clarify, the article above describes local transactions being promoted when remote systems are introduced, but I have seen this become the case for transactions on the same server when using cross database queries. As Thomas Stringer points out in his comment, there will be extra overhead in authentication although I think as this will be SID-driven there will be minimal overhead there unless you have to use separate credentials to access the other database. If there were difference in database settings which caused additional overhead in the join that could impact larger than the previous suggestions - for example database collation. Database collation could manifest as a functional difference, not just a performance difference. I think Aaron has the strongest argument for performance with the optimizer not having the advantage of using relationships for cross-database queries whereas self-contained within a database you could use relationships to your advantage. 

Will that perform a single or multi-threaded write? Our Server support company advised us that if we were to multi-thread our backups we would get better disk write performance. They advised us that SQL Backup is multi-threaded, but i think the above statement would be single-threaded. If we specify multiple DISKS to write to, then it will stripe the backup across multiple devices and therefore be multi-threaded by my understanding. If I perform the following T-SQL while the backup is processing, i can see that the SPID performing the backup has multiple processes, but only one seems to write to disk, the others i presume are reading and doing other things. It's just the write threads i'm referring the question to. 

which is fine as long as the user is to be mapped to a SQL login. If the login is to be mapped out to a windows login, SQL throws an error when we try to remap: 

So the question is, what is the best way to set this up in the project so that the developers can create a user which can be named whatever they like within the database users and then how would we re-map this to a windows auth login? thanks in advance! 

Sorry, I don't have enough reputation to comment on the question, but from my experience if the client application raises a transaction for a query which uses cross database joins then it would promote the transaction to distributed and have the overhead of a DTC transaction. The DTC overhead in this case could be viewed as a negative to performance. Generally the difference would be negligible although Microsoft describe DTC transactions as such: 

Which is great, as I can create a user which is mapped up to no login, and then when it's deployed I can run: 

I am working on a SSRS report to display the logins permissions in a set of databases on a specific server. the server, the logins and the databases are all parameters. NULL shows them all (logins and DBS) server - only 1 - must be specified. The Procedure accepts MULTIPLE parameters, which I pass to the procedure as a COMMA DELIMITED STRING. and here is where I think I am making a mistake. the report must list the permissions only for the list of logins supplied, and list of DBs supplied. Here is a picture of the report, collecting the parameters to be passed to the stored procedure: 

what if I wanted as well the number of , , and ? How would I go around that? I think the exact calculation would need to have a reference to the number of seconds happened. Let's say for instance I am interested in how long a sql server job has been running for. I have jobs within the replication category that have been running for years, and the starting point is known. When not specified we could assume the number of seconds are up to . 

and this one I would like to see all the operations for those tables that I am touching but this script returns nothing. so, the question is: 

We had great results on our queries that were using this indexed view, however, I had to drop it because of too many deadlocks caused by the replication. replication was deadlocking with itself when inserting into table dbo.tblBOrderPayment or table dbo.tblBOrder how could I have avoid this rollback? whilst I cannot re-create the view and put it in place in a busy production system, I have add below information from the log that shows me 2 replication procedures deadlocking each other. 

Basically every user would be free to navigate through the information, however, when a user presses the button, no one else can edit that record, although they can read it without a problem. let's say presses the button on id=4 . something like this: 

And when I have done the tests, and found out the user can run the script then I come back to be me: 

The writes are too high, it is a production system, I would like to have more support regarding this. Would I really benefit by creating these indexes? Is there any documentation that supports creating indexes on the distributor databases of transactional replication? Why Microsoft has not supplied these databases with the proper indexes on them? 

that is all fine, it worked great, it did what I wanted, however, when I look at the structure of the table I get the following: 

and that worked for me too. to run ssms press the and on the ssms icon and that will give the options below: