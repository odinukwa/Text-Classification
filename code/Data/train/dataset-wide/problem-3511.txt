This is a common feature of workflow software. You could roll your own, I suppose, but you'd have to account for what to do if the second host is not available, what to do if the initial cronjob is still running when the next interval comes up, etc. It isn't too long before you need flow control, branching, and other logic when one part of a multi-part/multi-host process fails. The most obvious answer off the cuff (and likely full of it's own faults) would be to append the launch of the second stage via an ssh command appended to the original cronjob. You'd likely have to setup ssh keys between the hosts to allow automated login from the initial host. A second option is to somehow pass status info between the hosts, such as a flag file on the original host picked up by the second periodically, etc. I generally find this less desirable as it involved polling, but it doesn't seem that you need an enterprise-level solution here, else this would be posted on serverfault instead. 

You might consider an EXPECT script that allows you to set your path and environment (like EXRC variables) upon pressing a certain keystroke. Shouldn't be too long before someone posts a similar script. When your server farms number more than a few dozen (think thousands) then having something easily setup your enviroment on a 'virgin' box is a real lifesaver Often when I login to a box, it creates my homedir for the first time! 

The process is waiting on some system resource, perhaps NFS? that is not allowing it to 'let go'. Would love to hear some solutions to this beyond mine... reboot the box, or let the process sit around. I wonder if you might be able to 'STOP' the process via kill -STOP {PID} to prevent it from consuming more cpu. Restarting it should be possible even with another stopped process sitting around, but it may require a list port or shared memory segment that is still in use by the other process. 

Always in the middle... both vertically and horizontally... Avoid the top floor, ceilings leak Avoid the bottom floor/basement, too easy of access (broken window) and/or flooding. presumably they have a freight elevator, so loading servers wont be an issue you might try not-too-far from the external A/C units, which may reside on the ground, on the top, or sometimes in the middle if the building is tall enough. Farther distance to the A/C unit will cost you in some way, but I suspect it's negligible. 

When I ran a group of admins the issue of who was oncall was always a concern. At the time I felt that it was all part of the job of being an admin, but alas, in hindsight, while I loved the job, it didn't mean they did. Not paying a stipend or some other acknowledgement of required ad-hoc hours (either onsite or remote) is just a slap-in-the-face. Regardless of budgeting, folks asked to carry pagers and response in a timely manner should be given credit either as pay, flex hours, paid cell plans, etc. Where I work now it's a flat-fee stipend. You can spend it on your cell plan, internet connection, laptop, etc... if you do your job well, 'in theory' you wont get calls that could be avoided, just 'acts of god' ;-) Luckily all the stuff I do now does not require on-site presence (whew). 

This works pretty well for me on linux only: -a = All sockets -n = doNt resolve IPs -p = show Pids/Programs associated with each 

Sounds like you have a policy problem... ... and presumably this was caused by a real problem vs you being paranoid. Aren't you limiting the effectiveness of the computing platform by doing this? many scripts can be run by simply invoking the interpreter directly: sh ./mybashscript.sh perl ./myperldaemon.pl So presumably you are trying to prevent a native executable from running? Best to deal with this problem via policy and policing/auditing... at best, you'll only keep the less experienced users at bay anyway. 

Sometimes it's hard to decide if you want to REPLACE the timestamps in your 'filter', or if you want to preface the line with the timestamp found. I flip back-n-forth on that one. 

This may be old school, but my stage and production apache configs were identical, except that some of the parameters within each were different (ie: IP address). I used a perl wrapper around a start script to 'fill in the blanks' in a methodical way. The actual base config 'templates' were indeed identical. This insured that the stage apache was functionally identical to the production apache. Sharing this production config with development enabled them to test as close to production as they wanted (and they wanted to, because they would not get into production unless their stuff 'worked' in stage. We supplied help occasionally (for mod_rewrite rules for example) and left the rest up to the developer. Difference between this particular setup and other places I've worked are: developement staff were encouraged to dabble in areas not directly related to the programs they worked on... ie: they may work on back-end applications (thus wouldn't need to know much about apache) yet they configured their own apache servers (in fact built their own desktops, but that's a line crossed IMnsHO). operations actually drove a lot of the infrastructure, rather than just running it. 

The ssh process likely daemonized, disconnecting from the shell. You can think of it like this (but possibly not in this order): The 'ssh' process you launched actually closed all it's file descriptors (stdin,stdout,stderr), disconnected from the tty, and forked another child ssh. then called exit on itself. This orphaned the child process and the original ssh process actually 'completes'. As a result, the original ssh process is complete and the orphaned process is no longer a 'child' of your shell. Use 'ps -ef' or 'ps -fu$USER' instead to see the ssh process you are interested in. 

smashing magazine (online) has several articles on favicons: $URL$ While not required, favicons will often be requested by modern browsers to add some eye-candy to the location bar, table labels, and bookmarks. Once you are inspired by the examples from the link above, you can try using an online generator to build your own: $URL$ (look down the page for favicon generators). I found the following one pretty nice: $URL$ 

bugfix response for chrome has been slower than other browsers, thus they prevent it from running at work via windows policy controls. Now that updates have started, wonder if they'll ever reverse that policy :-( 

I have many processes on a box listening on several ports. I am trying to map ports to pids. The problem is that lsof is not telling me what ports belong to which process. Given an apache listening on port 80, I can see it listening via netstat: 

RLimitCPU doesn't always help because not all portions of the apache code have checks for it. MaxRequestsPerChild may not help either, as I've seen this with relatively 'fresh' children. In my case, I suspect it's something to do with the module we're using (mod_perl) and perhaps something with a broken socket connection. We only seem to see this problem with browsers connecting, not from wget or curl (which we use heavily for 'data delivery'). 

I have used keynote and gomez networks in the past, but first I must ask, how do you know it's not really a problem with your site? In a prior job we once had complaints that our service was not available, and found that only thru packet tracing that the user was connecting and getting zero bytes returned. Nothing shows up in the access logs and no other trace of what went wrong was left on our systems. It turned out to be some kind of bug in the apache module that caused it to panic instead of emit an error of any kind. 

I tend to use 'du -k myfile' to get kbytes and visually drop the last three digits, but I'm just looking for approximate size. Turns out that du often (always?) has -m option for MB. Keep in mind that how large the file likely differs slightly from the amount of diskspace used, as the disk allocation occurs in blocks, not bytes. If you are looking for 'fat' files because of low diskspace, that would be a more enlightening question, as the solutions would be more varied. 

for more flexible control, use the foxyproxy plugin which supports whitelists and other advanced pattern matching to decide when and which proxies to use. 

I'm monitoring the TCP stack on a server hoping to generically infer problems with application on the box. My first inclination is to measure the number of sockets in all reported states (LISTEN,ESTABLISHED,FIN_WAIT2,TIME_WAIT, etc) and detect some anomalies. A teammate suggests that 'lsof' would be a better tool to see what state the TCP stacks are in. Any preferences or experience tips from the serverfault crowd? 

What you are trying to scale here is your IO. Using a caching proxy like squid or varnish is a way to populate the cache to increase spindles without replicating low/none-used files in your archive. CDN devices do this for you too. Are these files media? CDN devices can do streaming for you as well. Do users get file download failures and re-attempt a download often? A high retry rate will greatly increase your IO needs. Do you have any control over how the files are fetched? a download manager can fetch each file in separate chunks, thereby splitting the request over several apaches over time (though they could also download in parallel, saturating your internet pipe). As an 'experience' reference, I've only ever been in environments that place all that data onto a NAS (netapp in particular) and use apaches with NFS to deliver the files (though there were many smaller files, not 1GB ones). We also used a CDN as a caching proxy to stream video. 

Notice the *:65535 in the NAME column. Does anyone know why lsof is not reporting the port in use? I am running as root. I am using a mix of lsof and os versions: lsof v4.77 on Solaris10 sparc lsof v4.72 on Redhat4.2 etc I know that linux solutions can use "netstat -p", so I guess I'm only looking for why solaris isn't working, but I find lsof is frequently silent and not showing me expected data. 

I have a 3rd party application that exposes in-memory data structures via a JDBC api over RMI. How can I get data out of that database api without having to run squirrel or similar GUI application? Perl solution preferred, but java solution acceptable. 

ratchet and webbing (kinda like a tie-down for a truck) to 'lift' servers high enough to place on shelves or racks. cart/wheeled chair for a 'crash-cart' console when you don't have a Cyclades port for each server. indoor/outdoor thermometer to check for periodic hot-spots suspected in the datacenter. garage-style retractable power-cord located semi-centrally for when you need power but dont want to hunt under the floor for a receptacle/plug. 

FROM epoch: dump epoch timestamps to readable forms from stdin (first example) or as args (second example), and dont forget the power of strftime! 

TO epoch: If you have Date::Manip module installed for perl (which usually ISNT there by default), you can use: 

root user: Dont be root unless you have to. If the vendor says it needs to run as root, tell them you are the customer and that you want to run it as non-root. How many software packages off the shelf wants root 'just because it's easier'? habit: never ever use '*' with remove without looking at it three times It is best to build the habit of using ls -l TargetPattern, then use 'rm !$'. The biggest threat is not being where you think you are. I almost type 'hostname' as often as 'ls'! crutches: a standard prompt helps alot, as do aliases like "alias rm='rm -i'", but I often do not have full control over the machines I'm on so I use an expect wrapper script merely to set your path, prompt, and aliases with '-i' find issues: using a full path helps, but in cases where that is not possible, cd into a more safe location and ALSO use '&&' to insure that the 'cd' succeeds before you do your find, remove, tar, untar, etc: example: use of '&&' can prevent a tar file from being extracted on top of itself in this case (though in this case 'rsync' would be better) removes: never remove recursively if you can help it, especially in a script. find and remove with -type f and -name 'pattern' I still live in fear of feeding 'nothing' to xargs... tar and untar to move stuff around (use rsync instead)