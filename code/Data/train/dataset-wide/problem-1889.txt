Firstly you need to define what you mean by average daily temperature. What algorithm are you going to use to compute it (from several time spaced measurements), why and what do you think the result represents. Obviously if you use a measurement which is going to be affected by very local anthropogenic activity (e.g. cars) you can expect timing of activities to affect individual measurements. a measuring station which is going to be thus affected is not really going to be useful for anything wider or general though. When siting weather stations we try to find locations which are not subject to such local disturbances. Ideally well away from any heat producing apparatus (e.g. fixed a/c units), heated buildings, variable traffic and so on. Locations in open non urban areas are preferable for understanding changes in the weather and climate. Past locations thus chosen however have often been compromised by the expansion of nearby housing or industry as populations and cities have expanded. This is why you sometimes see references to Urban Heat Islands (UHI) in discussions of climate measurement. 

The answer, as already given is "By radiation" . But comments and other answer detail have added extra information which can be useful in understanding this in the wider context, particularly relating to arguments about the Earth's energy budget and climate change. Here is some other extra stuff to consider; We pose the same question for the planet Venus. Venus has an atmosphere composed of nearly pure CO2 (actually about 96.5%). The surface temperature on Venus is around 750K at the equator when facing the sun and drops by a mere 5K overnight. Night and day are about 120 earth days in length so that's a pretty slow axial rotation and a very slow loss of surface heat during the planet's night. Now the atmosphere on Venus is very thick, masses of that lovely CO2, and surface pressure is about 90 times that of earth (9.2MPa). If you calculate the optical attenuation of the atmosphere to incoming solar radiation then the solar flux at the surface is a mere 4W per sq m. It is therefore self evident with such a low solar flux at the surface that the direct radiation from the sun cannot warm the surface to 750K. It is therefore interesting to draw some energy flux diagrams (similar to the Trenberth ones for the earth) and calculate the source of all that extra heat warming the surface. Don't forget to draw 2 diagrams, one for night and one for day to check you understand how a pure CO2 atmosphere works. This is a great exercise for school students (and anybody else) interested in the greenhouse effect of CO2. 

I think there's stuff out there that you can use. The key is searching for "intermediate complexity models", or "toy atmosphere models" (where toy is science jargon for simple model that does things qualitatively right, but not useful for precise quantitative prediction). In terms of your question "are there any other simulators that consistently do the same thing", I'm not sure what you mean.. do you mean a simulator that will produce the same solution/climate state for the same set of initial conditions? For the question "what would be the required specifications for running CESM on a fairly constant basis?", that's going to depend on how complex of a simulation you set up. For example, if you are running a detailed simulation (high spatial and/or temporal resolution) and you want to have data for multiple variables at every grid cell through time, then you'll need supercomputing resources. If you were able to get access to one, you usually have to reserve time a few months in advance, run your simulation, and hope it worked out. With cluster computing, you would have to reduce the simulation size (compared to the supercomputer example), but you'd get more freedom to run multiple simulations. Here is a small list of less computationally intensive and self-contained models 1) The first is called fast climate and is written in matlab code. I know nothing about it, but it looks to be self-contained. 2) Also i just came across a GCM that claims to be user friendly called EDgcm. I know nothing about it, but it looks worth digging into. 3) There's an intriguing app called "GCM" I've honestly never messed around with it, but it could be worth a shot. 4) Here's an energy balance climate model that can be executed in excel. If you're determined to use a GCM, I think you could probably get CESM working using either the aquaplanet or dry dynamical core setups. Depending on how simple you set it up, you could likely run it just on a standard PC. Another option you have is MITgcm, which can also be configured to run very bare-bones simulations that don't require much computational power. If you have a university near you, I would consider seeing if there are any climate modeling groups, and reach out to see if there are any grad students willing to lend a little support. I know grad students who spend usually the first semester of their phd trying to get a simulation going. 

Climate science is a very exciting field to get into right now. Not only do we have a change at the top in politics in the USA and the prospect of big changes there but the new theory of a Gravity Induced Temperature Gradient is coming to the fore. I recommend researching and reading about both sides of the debate, not just the CAGW theory stuff. If the ideas of Loschmidt (c.1870's) and the recent experimental results of R.Graeff (2007) take precedence over the current consensus theories then there will be great opportunities opening up in this field for those with the right knowledge and background. Good luck with it ! 

OK, you have all seen the question before BUT let's make the challenge a little different this time. I want you to use the Feynman Method ! You have to explain it simply, preferably just text but one or 2 sketch diagrams may be OK (no complicated graphs with multiple colours please) and I am going to limit you to 150 words. Who fancies the challenge - make sure you don't leave out any of the important physics. This question is asking for an explanation of how something works. I am not interested in experimental evidence or proof, just a clear explanation that you can chat about to someone to help explain the physics behind it in as simple terms as possible. Like if your mate down the pub says, OK, tell me how this works then. 

An interesting point to consider comes from your assumption of a constant rate of precipitation. Many earth system processes have non-trivial stochastic variations (or high-dimensional chaos). Consider the conservation of mass for ice: change in ice thickness = (precipitation, mass in) - (pick your favorite process, mass out). Both (mass in) and (mass out) processes have significant random variability, both in space and time. When you integrate a random variable, a special thing happens: you get a random walk. The figure below shows 5 random walks 

A few thoughts - assuming like you say that the aqua-planet is the same size as earth, and all else being equal, it's best to consider the two major classes of waves: Nondispersive wind generated waves: Here the maximum wave height is a function of wind speed (fetch). However, wind speed in general is not a function of wave height (unless you invoke concepts like walker-circulation from ENSO, but you wouldn't get ENSO since there are no continental boundaries). The relationship between maximum wave height and fetch in deep water, can be described empirically as $$H_{max}=0.332 \sqrt{F}$$ So this all goes to say that it is impossible to determine $H_{max}$ without knowing something about the atmosphere dynamics. If you assume that the atmospheric dynamics are the same as on earth (in terms of storm statistics), then this little empirical relation would suggest wind generated wave heights should be no different for an equally sized aqua-planet. Larger planets do however have larger storms... Eddies and planetary waves Eddies are waves, yet quite different from wind generated waves. While they certainly don't reach the impressive heights of wind-generated waves, their wavelengths are several orders of magnitude larger. And so technically, you could say they are in fact "bigger" waves. Along with eddies, there planetary waves such as Rossby waves and Kelvin waves. All these eddies and planetary waves are limited in size by the Rossby radius of deformation. Which is to say, their maximum size is effectively limited by the curvature of the earth. Upshot To get larger wind generated waves, you need larger storms. Therefore you need a larger planet (e.g. great red spot on jupiter). To get larger planetary waves and eddies, you also need a larger planet! The reason that larger planets have both larger storms and larger planetary wave/eddies is the same ~ which is that the rossby radius is bigger for a bigger planet. You might be interested to check out something called the "aqua planet experiment" project. These are general circulation models run on a hypothetical planet with no continent, they're all just atmosphere and ocean. The image below is cloud cover output from an aqua-planet simulation. 

I am restricting my consideration, for the moment, to just the troposphere. With regard to our understanding of the GHE do we predict that an increased GHE due to increasing CO2/Water/Methane will change the lapse rate postively or negatively? The dry adiabatic lapse rate (DALR), according to my old text books, is easily calculated from the formulae -g/c, where g is the acceleration due to gravity and c is the specific heat capacity of the atmosphere. This is supposed to be a general formulae that can be applied to any planet with a solid surface and a gaseous atmosphere. So for earth is the formulae now -g/c + (GHE) or is it -g/c - (GHE)? The bracketed GHE term being specific to the atmosphere of a specific composition, e.g. including 400ppm CO2. The second part of the question then , after we have the correct sign, is how is the term (GHE) expected to vary as the CO2 (or other greenhouse gas) concentrations vary? 

Irrespective of how accepted any scientific theory happens to be it is often worth thinking about a possible experiment to test said theory and perhaps falsify it. If we wanted to do this for the theory of the greenhouse gas effect what areas are worth considering when designing such an experiment? Would we limit ourselves to just the radiative heat response of CO2 or are their wider issues we need to incorporate into the experiment? This question is NOT a duplicate as it relates to the generality of greenhouse gases and specifically asks IF we should limit ourselves to the radiative response of CO2. Therefore one possible avenue of answering could be to consider the wider issues of water vapour and methane and such an answer would be clearly outside the scope of the CO2 specific question which has been postulated as the duplicate. 

Here's some alternatives to matlab: 1) nasa has online calculator that spits out raw text tables of insolation. 2) If you are savvy with fortran then you may like this fortran code. 3) The fortran code above also has an online interface 4) If you like python, then you may like this python code, climlab, where there are detailed instructions here on how to use the code and visualize the information. 5) If you like R, then try palinsol which has a little documentation here. 

Convergent cross mapping (CCM) is a recently developed tool to answer the question you've asked. It's based on tools developed in nonlinear time series analysis and dynamical systems theory. It allows you to: 1) determine if a causal relationship between two variables is present 2) establish the direction of causality 3) do so even in the presence of noise. As for an interesting application, check out the paper Causal feedbacks in climate change [van Nes et al., 2015], where CCM is applied to co2 and temperature based on the Vostok data sets. EDIT: Below I've added a more detailed explanation of CCM to show the original poster that this technique does indeed answer their question, as well as to show it has a rigorous mathematical underpinning. The general idea of convergent cross mapping is based on phase space reconstruction [F. Takens, 1981],[H. Abarbanel, 1996]. Numbers 1 through 5 explain the idea behind phase space reconstruction, which is needed to understand CCM. Numbers 6 through 8 very briefly explain CCM. References are listed at the bottom for more depth. 1) A physical system that is described by a set of equations (e.g. conservation of mass, momentum, etc) has a phase space. The solution to the system of equations is a trajectory through (or subset of) the phase space. 2) An attractor is a subset of the phase space that the trajectories/solutions evolve toward. 3) If you know the governing attractor, then you have all solutions of the system for all time. 4) Taken’s theorem says that one can reconstruct the attractor of the system based on a single observable. For example, if temperature, pressure, and velocity are the three variables of the system, then you only need measurements from one of these variables to reconstruct the attractor of the system. State space reconstruction 5) The reconstructed attractor is not exactly the “true” attractor, but it has a direct 1:1 mapping to the true attractor. Taken's theorem 6) If two observables belong to the same system, then they each have a reconstructed attractor with a direct mapping to the true attractor. The reconstructed attractors also have a direct mapping to one another. Convergent cross mapping 7) It is then possible to make predictions on one observable, based on the reconstructed attractor of the other observable, if they are in fact from the same attractor (causally related). Time series and attractors 8) Last, a series of tests/predictions with the data help to establish the direction of, strength of, and linearity of the interactions between the two variables. This is detailed in the papers [sugihara et al., 2012] and [van Nes et al., 2015]. To answer your question "given two observed variables, how do you tell if an third variable is simply forcing the two observed variables, making them appear correlated? First, the process of phase space reconstruction would yield an estimate of the "embedding dimension", which is an estimate of the dimension of the phase space (how many variables there). In the CCM framework, a one-way forcing relationship between the two known variables (v1 and v2) would be attractor for v1 can make skillful predictions of v2, but attractor v2 can not make skillful predictions for v1. Contingent upon the situation where you have an idea of what the third variable is (v3), I think what you could do is the following, take the reconstructed attractor of v3 and make predictions on both v1 and v2, and show that v3 has more predictive power on v1 (compared to v2), and that v3 has more predictive power on v2 (compared to v1). I'm not sure about this though. Also, if the forcing (v3) is thought to be linear, you could simply remove/detrend v3 from v1 and v2, as is done when you remove seasonality from temperature data. Note: There is MATLAB code available to mess around with this technique. I believe you can find similar codes in R as well. References Sugihara et al., 2012, Detecting causality in complex ecosystems. van Nes et al., 2015, Causal feedbacks in climate change. Abarbanel, Analysis of Observed Chaotic Data,1996, Springer publishing. Takens, 1981, Detecting strange attractors in turbulence 

I think this is it: ppm(mol)=ppm(v) x (density CO2/density total atmos) x (Molecular Weight total atmos/Molecular Weight CO2)​​​​. Try some number crunching to see if that makes sense. 

Another way of looking at the question: I take 2 standard wheelbarrows of bits of plants from my garden. One wheelbarrow is used to fill the compost bin. The other goes to the bonfire. How much ash do I have from burning compared to how much compost from the compost bin. Any gardener will know that the amount of compost will be much larger than the amount of ash. So now we need to compare the chemical composition of the ash with that of the compost. For Wood Ash there are quite a few research articles. Most of the carbon atoms in wood ash will be in the form of Calcium Carbonate (CaCO3). Percentage in the ash will vary widely but lets take a median of 50%. Ash as a percentage of original organic matter will be maybe of the order of 8% by weight. For an original 100kg of organic matter (yes, its a big wheelbarrow) we would have 8kg of ash. In this ash would be 4kg of CaCO3. The carbon atoms would be 12% of that (relative atomic masses). So 100kg of organic bits gives you 0.48kg of carbon atoms left in the ash when you burn it. There seems to be fewer research articles on composting but we found the following approximations. For 100kg of organic matter there will be about 9% organic carbon (9kg). Composting will turn 100kg of raw material into around 30kg of compost in which the organic carbon content will be around 25% ( 7.5kg of carbon). These figures are median "pot luck selection" as results can vary widely. We can however see that composting retains massively more (7.5kg) of carbon atoms compared with burning (0.48kg). The difference will be in the atmosphere.