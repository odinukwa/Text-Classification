sudo yum update --assumeno this will list all available updates without actually updating anything, assuming you have a yum version that support the switch. otherwise just be careful because by default it will ask you before it does the update and you can still say no. 

while your laptop is unlikely to have an interface that will do vlan tagging, some servers can. vlan tagging just adding some info to the ethernet frame so presumably you can do it with programming, but usually it is done by switches to send them through trunk lines where frames from all vlans are mixed together. yes, generally switches, but other devices can do tagging as well. the end devices should only see untagged ethernet frames, as the vlan tags are stripped before handing it to the end devices, unless the end devices have interfaces that does the stripping. 

There should be no expected issues just from running this command (otherwise the man page will be full of dire warnings for sure). If things die, then it would not be due to atime update turned off, but some hardware problems you have. that remount simply turns off the atime updates and will thus have LESS disk usage and therefore make things less likely to die. Have done this sort of things many times and never see any issue. If you are still concerned and if you can later restart the system or mysql, you certainly can do so. 

A certificate will only be valid for the exact host name it was created for. That's the "CN" (Common Name) part of the "Subject" field in the certificate. The typical way to deal with what you're doing is to buy a certificate with another field called "Subject Alternate Names" where you can list other hosts name that should be considered valid by the client. These certificates typically are a bit more expensive (although not always). 

You can't do this directly: the credential manager isn't actually designed to serve private/public key pairs but to hold passwords. You can, however design a workaround: get keepass 2 and the "Keeagent" extension. remove pageant since it will conflict with keeagent. Once both are installed and running, create a database and link it to your current user account. After that, create a new entry in keepass, add the private key file as attachment to that entry, use the private key password as the entry's password and set your 'nix username as user name. Then go to the KeeAgent tab of the entry and enable it to be used in agent. You're done. When you now connect to a system that uses agents (putty, mostly), you will be prompted for the keepass database password which is actually only the user account: just press enter and you'll be in. Be careful to keep a safe backup of the key, however, since you WILL lose access to it if you have to delete your windows user account. Of course, it would be much safer (and quite a bit more flexible) to secure the keepass database with a password and a key file. If you're willing to do so, you'll be able to use it for all your passwords needs (personally, after going down that road years ago, I can't imagine going back). 

Port can be reassigned to confuse people etc., so if you have a plain http server actually listening on port 443 then your http url should work, and vice versa. However the restriction you are facing may be more than just the port. Since ssl and http are different protocols, it is trivial to figure out that the packets are not http and your connection is thus blocked. It is probably easier to do ssl tunneling if your http traffic in this case. 

Not by default but there is a patch. Not sure if your distro has a patched package already but it may worth searching before you patch it yourself. $URL$ 

It could be the hardware I used, but periodically the openfiler would freeze up and required a reboot, about a year ago. However, judging by your needs it is probably adequate. However, the new version could have improved. RH is simpler to set up for me as I am more familiar with it and can have full control easily. Openfiler has another problem that I did not quite figure out in file permissions and ownership, if you go the nfs route. If you use iscsci however, it is just a block device so it is easier. So I would say RH if you are a linux guy, and openfiler if you are not, because the latter provides a web gui to configure things very quickly and easily, for those who has no prior linux/bsd experience. 

You can mix SAN certificates and wildcard certificates, thus creat a single certificate that can be used for all your domains. However, IMNSHO, The best way to handle this is really to assign a single IP per 2dn level domain and the use a SAN or wildcard cert for sub-domains and different hostnames. The main reason for this is that if you try to put all your certs through the same IP, you will run into trouble when you want to add or remove a domain or host because any change to the published list will require you to request a new certificate. This can end up being pretty expensive, quite a bit more than getting a handfull of IP addresses. Also, public CAs typically limit the number of SAN that you can include in a si^ngle cert. 

This really sounds like a storage issue. What kind of storage are you using for the pagefile ? Otherwise, the best tool I know to diagnose that kind of issues is procmon from sysinternals (MS now). It has the ability to perform long sessions as well but you'll have to have a way to identify the exact time frame when you experience the issue, in particular if you're going for a full system monitor. If it's not a page file issue, then it most likely will allow you to find the culprit. 

snapshots over time makes performance problematic, so what you are doing is definitely a good thing. it is generally advisable not to use snapshot for any long-term solution of anything (more than a week or 2). however, ovf creation of vms with snapshots did not cause problem for me when i tried it and i have not found any documents that says snapshots would adversely affect the ovf export. still, i have been like you, clean up snapshots whenever i can and whenever i need to use vm for something such as export. 

Yes, it is possible. You can edit the settings of the vm and add a new disk or an existing disk. However, your guest OS need to do a rescan to discover the disk and use it. So it is a multiple-step process. 

Virtualbox and vmplayer will do also. Xen is another. For guest os you might want to start either with jeos or one of the minimal ubuntu isos. You can easily find all these. 

it would depend on who manages the dns servers. if your providers do, the presumably they would have to have separate zone files (it is better to organize things) etc. to manage your subdomains. if they delegate your domain to your own dns servers and you create your subdomains arbitrarily, then they should have no more overhead at all and have no reason to charge you more. not a legal expert so if they still charge you in that case, i am not sure if would be legal for them to do so. 

Don't use shortcuts. Instead, use hard links or volume mount points Edit: The simplest way is to create a mount point for the root folder, but it has a couple of limitations: you MUST redirect a whole folder and you MUST use a complete volume as destination. To do that, create an empty NTFS folder where you will redirect your volume and then open disk manager, select your volume, right-click on it and select "change drive letter and path", remove the drive letter mapping (if any and if you want to) and add the path of the EMPTY NTFS folder. Hard links are a bit more delicate to use and, upon reflexion, it won't help you on since all links must point to the same volume (which isn't your case). 

The first two DNS servers are part of the global Internet infrastructure and is handeled by IANA. Since the same organisation also supervise IP address assignment (as well as the global DNS root), it is the authority that can (and will) delegate the management of the others servers in the query path to the respective owners of the IP block. An interesting thing to note is that your computer is not going to perform all these queries itself. Typically, it will place the request to your local DNS server which will perform the query for you completely and just supply you with the answer (it will perform a recursive DNS query). After that, it gets fuzzy because it all depends on how each server in the query path is configured. So, what it means is: 

default install of SQL server express only allow local connections with no network enabled. So tcp/ip is disabled by default, and you have to enable it manually. Named pipes should be enabled by default though. 

Only is expecting a shell command to be executed, not a condition like what you have, which should be a case instead. 

You would have to change the adapter to bridged so that the vm can get an ip in your LAN. Or you need to have a way to forward the icmp traffic in your LAN to the vm. 

It certainly could, but automatic reboot is usually associated with hardware/software issues such as overheating or kernel bugs. So it is possible that heavy load causes overheat which leads to reboot. In any event, you should investigate the log or kernel dumps to find the exact cause. 

It would depend how your mail is set up. Generally pacificseatheat.org sounds better, as in most cases, organizations use this format regardless of the actual server sending the mail. However, using the longer one will work as well. You can always change that later in /etc/mailname. 

Actually, you can setup the new DCs with new names, transfer the roles, decommission the old DCs and then rename the new ones. However, as mentioned by other comment, it's neither necessary nor desirable to do so if these machines are only DCs. Is there any other requirements you forgot to mention ? 

It's hard to know for sure but, in my opinion, you have a MTU path issue here. Do a path MTU discovery and reduce the MTU of your gateway (or server NIC) accordingly. If it solves your problem, then you have your proof that some node in the path isn't handling MTU correctly (either dropping the ICMP code 4 packets or simply not sending it back). 

You can change the source address of any IP packet. That is called "spoofing". However, for TCP, that will not help you: TCP isn't a datagram-oriented protocol but a stream-oriented one. In practive, that means it's designed to simulate a stream-oriented communication channel using discreet packets transparently. In order to do that, it will reassemble packets together to form a stream but also has built-in mechanism to recoved from packet loss and incorrect packet arrival. It also has mechanism to handle connection status management. All that means that, before you can send "a request" to a TCP server, you much first establish and that will require the client and server to perform a 3-way handshake: a negotiation between the client and server that ensure that both end of the connection agrees to open the connection. During that negotiation, both client and server will pick a random sequence number that must be used by the other peer. This number is then increased by one for every packet sent by one end of the connection and used to check whether packets have been dropped or received in the incorrect order. Since you will not get the answer to your first SYN packet (the first packet in a TCP handshake) and since you need to know the sequence number that is contained in that reply before you can finalize the connection and start sending data, you can't just spoof the source IP in a TCP connection. Now, if you're somewhere on the path between the server and the spoofed client, you could stiff out the answer (even if it's not meant to you), finalize the connection by using the proper sequence number and send your request (you can even read the answer. This, however, will require you to be connected to one of the segment between the client and the spoofed server AND be able to read these packets as they go through the segment. This condition is quite difficult to achieve in practice outside of the final segment and even then you need to use either special hardware (a switch that can use a port as a "monitoring" port) or use ARP poisoning on the switch. It's not really trivial and will still allow you only to spoof a machine on your local segment. 

although it would be nice, but there is no unapply available. you would have to write an undo recipe yourself, depending on what you did exactly (installed package? then purge it, added user? then disable it, etc.) the replaced files should be stored in the clientbucket (/var/lib/puppet/clientbucket usually but it depends on your version and your setting) 

You can specify user shell when you realize the user, which should be created as a virtual resource. See or search for puppet best practices to get details. 

can you at least download the zone files? that is actually all you need. if you cannot even do that, you would have to use dig or another client to go through all the records that you know to find information, that would be painful. 

yeah, in ovf export (File -> Export). not sure if it is a good idea to save to usb directly, as usb i/o is often iffy. it is better to save to your hd and then copy. done many many times with no issues. 

can you use acl? you can man setfacl for details. this allows you to specify what each user or group can access which directory/file with what permission. if that command does not exist you can install it from repos. unless your file system is really old, acl should be supported. you might have to specify acl support in mounting however in some cases. 

It could be something else but this smells like an issue with the TCP checksum. This function is often offloaded to the hardware card and you will not even be able to see the problem in Wireshark from the same machine (although it is usually obvious when you see it from a firewall dump). If the checksum in the TCP packet is incorrect, it will cause the packet to be re-requested. If, after some time, one of the connected parties cannot build a properly validated window, it will consider the connection as broken and send a RSET packet. So, have you tried disabling TCP offloading on your NIC ? 

In such a case, the system will prompt for a password every time the private key for your user's certificate is accessed which includes every time an encrypted message is opened. 

Short answer: you can't. Long answer: describe the problem you're really trying to solve because "cmd commands" is awfully vague and limitative. Literally, it would be interpreted as "only commands built into the cmd.exe command-line interpretor" but your mention of the ping command already tells us that is not what you mean. So: do you want to do it by batch ? Then you can download nmap and tell it to attempt an OS detection. You can also attempt to connect to the machine's SMB share and see if the administrative shares are there. Also, if it's a single computer, why not just walk over there and check ? 

If you can afford, I suggest that you use Oracle Goldengate which targets this sort of things. I used to be an Oracle DB admin a long long time ago and otherwise have no relation to them. Or else you can just export sql, massage the data, and import yourself (you can stream the delta changes to make it look like synching). The effort in that case depends how much your schemas differ and how much nonstandard SQL you are using. 

You can study livecd layout of your distro, but likely you need /var instead of /var/log and in some distros there are files in /etc that must be writable. /home as well unless you put home dir elsewhere. 

Almost certainly a hardware problem. Get a diagnostic tool from your vendor and run it through. If you cannot get something like this, at least run something like memtest provided by most Linux versions, which you can run by getting a lived. If it is in warranty, it is best to ask for vendor service. 

You can recursively setfacl the folder to give you write etc. while still have http own the tree. Acl should be supported in arch linux. Man getfacl and man setfacl for details.