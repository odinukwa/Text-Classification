On a SQL 2008R2 box we recently had a number of jobs fails for various reasons that were mostly memory related, including one stating the page file was full. The Windows 2008R2 VM had 16GB of RAM and a dedicated disk for a 6GB page file. For now we moved the page file back to the C: drive and increased its size to 8GB. The long term effects of that are yet to be seen. Our Server Admin, this morning, increased that "swap" drive to 25GB as was recommended by the GUI. What struck me as odd is that the admin also changed virtual memory to be mananged automatically across all drives. This strikes me as waste of space but I don't really understand how Windows automatically manages the page file. Here is a snapshot of the current virtual memory settings to help with the description. 

These are Windows Server 2008r2 machines in an AD environment. Some time in the past a DHCP server failed. An admin replaced it using a different host but the same address. Later on, when troubleshooting a rogue DHCP server, it was discovered that both the old and current servers were list as authorized DHCP servers. Once discovered it was de-authorized. That de-authorized our current productions DHCP server for a short period until that was reversed. Those two, with the same ip address, were listed for almost 6 months. Would that have been causing any issues or merely just an harmless entry in the authorized list? 

I am trying to automate a weekly process where I download a CSV copy from the Client page on the Meraki website. Before I lose you I am aware there is an API and it works very well. However there in a device attribute that is not exposed in the API. I got this from their support: 

I have a Windows Server 2008 R2 SP1 machine that is isolated in a DMZ. Historically it has not had issues but everything works before it breaks. The port 8530 is open on the firewall appliance and I can telnet from the client to the server which proves the site is ready and open. This machine is not attached to the domain so WSUS server is set in the registry. So under HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\WindowsUpdate I have 

I need the Owner as that is how I associate devices to people in our organization. Using PowerShell I am able to successfully log into the website and get a 200 response for a basic page. My problem comes when I try to use the same session to get to the data I want I keep getting a page with just breadcrumbs and no actual data. 

Worst case is I am hoping to get the of the Client List table so I can work with its contents. I can't seem to get any useful data no matter what page. I am not sure if the issue is working with Meraki or my PowerShell approach. I have only done simple scraping up until this point. I realize this is a very specific request but I am curious to know if someone that has Meraki is able or has been able to get this information using these means. 

The windowsupdate.log corroborates this. I would like to try and include only what is required to try and keep the post length down. The client reaches out to the server and see that it has X available updates. However it fails to download those. The log shows entries like this: 

With regards to the SendRequestUsingProxy failed, that should fail. The server does not have access to Microsoft websites so it will be blocked from being able to go there. What I can't figure out is why it isnt getting the updates from the WSUS server directly. We do not use a proxy nor is one configured. On the WSUS Server side of things I see that it get a download failed status for each of the updates. So in short the communication is there but the client is trying to download the updates from externally. It is a 2k16 server and reading the logs with has not proven useful. This is the only external server I have to the network so I do not have any comparison systems to know exactly where the system is. In an attempt to testing connectivity to the server I try to browse to $URL$ which is met with page cannot be displayed on the client server. (That link works fine on the internal network) Why is my Windows Update client not honoring the WSUS path for updates and instead attempting to go externally for Microsoft? 

Is there any way to allow the upload (put) to the default directory, without having to chdir first? I do have flexibility to move these directories around. For example, the sftp chroot dir doesn't have to be in the users home directory, I can even change around the users home dir (but the user must still be able to use authorized_keys to login). Please note: I do understand many SFTP clients, and the command line SFTP client allows for defining a relative path at login. That is out of scope for this question, I'm desiring this config be done server-side and the client simply just needs to login. 

EDIT: I've also tried setting the host definition to a standard ping check, and put in a service definition for the command. The host shows UP but the service is DOWN. Same exact error "Socket timeout after 10 seconds" -- The command line use is nearly instant with the UDP OK return. I am stumped. EDIT 2: I tweaked the debug settings, and have turned logging verbosity up to the highest value (2). It doesn't really tell me much, but it looks like Nagios is interpreting the command as expected... 

Since you mentioned Python, I assume you're using BOTO. Check the boto documentation, you can even filter on certain tags: $URL$ Noted on that doc page: 

I'm following this guide: $URL$ Which works great! My backups are saving to s3 without issue. However I would love to put an S3 lifecycle policy to purge backups older than 14 days. This is easy enough to do, but there are some other items in the bucket related to gitlab backups (restore instructions, some configs, etc) that I never want to purge. These can be easily skipped by only including backup files via S3 lifecycle policy "prefix" rule which only purge files with a certain prefix in their filename. Great, however the prefix of the gitlab backups using have a dynamically changing prefix. My question: Is there a way to change the file name format of the backups created with ? Or is there a way to alter the Lifecycle policy to take a suffix rather than a prefix? Any suggested alternatives to meet the goal is appreciated as well. 

This is commonly done with instance An EC2 instance can have any number of tags in key:value format. They are often used liberally to help further processes identify instance roles/types/environments/etc Some examples: 

Issue: In passing a variable declared within Jenkinsfile to a command that ssh's and executes on a remote host, the variable contents are not retained on the remote host. Built-In Jenkins variables retain fine both locally and on the remote host. The variable I've defined works fine locally but doesn't translate on the remote host Although this issue references docker, this is actually 100% Jenkins Pipeline based as it could apply to any example with or without docker Background: I'm trying to dynamically create an image name based on the current build tag, and put that name into a variable. Then I pass that variable into step which remotes over to a docker host and runs a build step with the name defined. Snippet of applicable parts of Jenkinsfile... 

This question is about KMS activation over a different network from the one my workstation is connected to. The software I am dealing with is Windows 7 Enterprise and MS Office 2013 Scenario I manage a LAN for my company. We have our own internal AD and DNS/DHCP servers. However, Our physical workstations are owned and managed by a Corporation and they handle licensing of the workstation software and manage the KMS servers. I can physically plug the workstations into the CORP LAN and Domain and force activation, but this is basically the worst case scenario. My Goal: My Goal is to create a constant static route to kms.corp.com through a router that is connected to both networks. I want the workstations to then have DNS records that automatically point to the proper host for KMS activation. I drew a diagram of the scenario I would like to have: 

Sometime ago I enabled the active directory recycle bin using PowerShell. At the time I knew this to be working because of the following results: 

I made a script that takes data from an HR database and populates correlating attributes in AD e.g department, title, manager, location. Since people change titles, departements and/or locations on occasion it is important to keep AD up to date since we have processes that depend on the validity of this information e.g. location based dynamic distribution groups. To try and keep the process fast I just run for each users regardless if something changed or not. I had worried that I would be changing the modified timestamps needlessly since it is faster to just make these changes constantly then it is to verify that no changes would be needed. To my surprise it seems that AD is doing something like this for me as most of the user modified timestamps are not matching subsequent script execution times. This seems like a positive for me as AD is doing this for me under the hood. FYI I have 2 DC's that I could be talking to for this and I have checked both times to ensure that I am not just drawing a horrible conclusion. I cannot find an authoritative source that explains this and I am not sure if this is PowerShell doing the job for me or something Active Directory is doing. 

I have seen this message in relation to configuration issues with PostFix. My environment is Exchange 2010 with a Barracuda Spam & Virus appliance as our mail gateway. When I looked up the MX record with nslookup I got 

From what I can tell from looking at other tutorials about setting this up that should be showing up. For me though, there should be other objects there that have been deleted over the past couple of weeks. I am running my tests as a Enterprise Admin user. Searching for "recycling bin" and "active directory" leads me to other users that have similar issues but most of them are addressed by either actually enabling the feature or being at a lower forest level. In my case both are correct. Not sure what I am doing wrong here or assuming. A fact that is quite possibly related is that I cannot see this "Deleted Objects" container from ldp.exe either as per this guide I was using for comparison. The last step to see the container being: 

When I run that command I get some of the user accounts and groups that I was removing while testing. According to this page on TechNet forums though that means the the bin is enabled. 

I have a Windows 2008R2 print server hosting about 40 printers. For the longest time we had a Point and Print GPO that allowed the users to install the drivers for these printers without administrative interaction. Administrative Templates\Printers\Point and Print Restrictions: Disabled. This is still in place. Recently though that is no longer working. Take the "sales" printer for example. People have been connected to it for years now and in the last few days, when someone tries to print, their computers (All Windows 7) have been asking to install a print driver. Even new users that have not been attached to that printer before are being asked for admin rights to install the printer. This has affected about half of the printers on this printer server. So some printers users are able to install just fine. So when someone has the issue I hop on their machine and provide my rights so the print driver will install. I am sure I know of the catalyst that caused this but I have no idea how it directly relates. For inventory purposes, I updated the host names of the printers. To clarify I went on the web interface off all the printers and in each of their network IPv4 configurations I updated the host name from its generic Ricoh to be the same as the DNS record I made for the printer. So each printer has a share name, port name on the printer server which are both the same as the physical printers host name e.g. "sales". No changes have been made to the print server hosting these printers. I don't understand how that change would cause this. In the case of the "sales" users it is preventing them from printing. We have to allow the driver to update before they can print. That is how we knew there was an issue and was able to tie it to my inventory update. These users are not all in the same OU in AD and both have the same policies applied anyway. I am testing different GPOs as when you look up network printer driver GPOs there are more things people change than just the one I mentioned above. Any ideas why what I did is causing this issue? Perhaps I am chasing the wrong tail and something else is wrong?