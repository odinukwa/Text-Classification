I went and checked a really old kernel (EL4) and that restriction above was still there. This means its fundamentally not feasible to do what you want to do on your architecture (x86). Given you provided the name of the NAS I did some googling and discovered this: $URL$ Which implies it uses a PPC CPU. 

To check if modules without parameters in /sys show up as having parameters in modinfo but I couldnt find any. I am no expert, but the difference here is that modinfo reads the module file itself for the parameters by looking in the .modinfo elf headers, whereas sys is reading these from its runtime variant. It may be possible to have parameters you can modify at runtime which dont appear as a modinfo parameter value, but since the module format should be pretty fixed I dont imagine its possible for you to pass a option parameter to a module when loading without there being a .modinfo structure for it linked in. I am curious, does your module suggest there are parameters passable with modinfo when you check it that way but there are none in /sys for it? Certainly on my system I was unable to find any examples of this using the command provided above. 

OK, so lets go through each bit. Active memory is regions of memory that get thrown to the top of the LRU stack (basically get called a lot). Inactive memory is stuff thats not being used a lot and is a swap nomination should memory need to be swapped. Free is genuinely free memory About 40Mb. What gives? The clue is in these lines: 

Firstly, dont use fakeraid if your optiong for linux. Its useless. It does not balance reads/writes at all, opting to read from ONE of the disks. Dont do it. With regards to the hardware/software raid, my experience has been that software (mdadm) RAID is really very good where the number of disks does not exceed six. Theres a few reasons for this. 

So, to start this creates a 512M file that is the basis of our virtual block device which we will punch a 'hole' in. No hole exists yet though. If you were to you'd get a perfectly valid filesystem. So, lets use dmsetup which, using this block device -- will create a new device which has some holes in it. Here is an example first 

The fourth column (which counts reads) indicates only 1 read occurred, despite the fact you requested 1 byte reads. This is expected behaviour since this device (a SATA 2 disk) has to at a minimum return its sector size. The kernel simply is caching the entire sector. The biggest factor at play in these size requests is the overhead of issuing a system call for a read or write. In fact, issuing the call for < 512 is inefficient. Very large reads require less system calls at the cost of more memory being used to do it. 4096 is typically a 'safe' number for reading because: 

So, yes it does do this, but not for the reason you think it does. Certainly not because of the post labelling policy. This occurs because a specific named rule exists which provides this behaviour. 

I think theres something odd going on in that policy of yours. If you check the audit logs, it says whilst the SELinux source context is correctly labelled as the target context is labelled as . This is despite what your policy says, that it should be . This means whats in the kernel and whats in policy dont match. The port is still 6379 though. You may want to check what you have configured for your as well as your . As far as I understand, port policy bindings can only have one label per port/protocol, so I suspect whats in your policy store does not reflect whats in your server presently. You may want to try doing a to rebuild and reload your policy to try to fix the synchronization problem. If no luck, search whats in the port listings for and update the question. 

Since you only had 8G of free extents after you vgextended you only get a lv representing that absolute size of 8G. You must inform LVM you intend to make an additional space size. From the manpage of ; 

Given you are actually making a spool file. This gives other macros the ability to work with that spool if they have 'manage spool' privileges in their policy. 

Lets presume you have the following shell script you use as root to update some web application and you have set the being used in 

It will support that many rules, but you really wouldn't want to traverse a chain of 4500 rules. As @Zoredache pointed out you could also binary split the chains. If you did it perfectly you could drop the number of chain traversals to 13. The simplest way to do this is with ipsets. I am using EL6 which provides support for this. Obviously I dont know all the chinese netblocks so I'm just filling this with garbage.. 

Also, note su, on its own wont help you in this regard since you'll su into the staff_t type which wont do everything you want. To fix this, edit sudoers and add your user to it such as this: 

The port remains bound because the calling application that is binding to the port does not use the socket option . You should fix the calling application to do this. In C: 

Probably. Reading the manual, trying this stuff out for yourself is the first step. Whether or not it will get you into where you want to be is a different question. A business which has services handling thousands of requests a second for example is very difficult to replicate in a training environment off of your own back. But, yes - demonstrating your knowledge and knowing you can put it into practice should at least get your foot a bit into the door. 

The parameter refers to traditional UNIX IPC shared memory. You can see the memory allocations of that using the command . The temporary filesystem is a totally different subsystem used as a ram based, swap backed filesystem. is used as the basis to perform posix shared memory in fact, which is a different type of shared memory system to unix IPC. 

The kernel makes the following procedure to determine what the file type of a newly created file will be. 

Following on from the link in you're reference, it suggests you are trying to mount with NFS as the root, in which case you should never end up landing in this function at all. In which case: 

Is enough to tell me that this is not your problem and its the responsibility of the sysadmin to provide an adequate explanation. I dont want to sound to rude here but; 

I got this to work in hfsc. I assume "X" in your example is 100mbit, but that could be anything of course.. The trick here is to create a tree class like so: 

The first value if you cat that gives you precisely what you are after it would appear. For the record I couldn't get the output to match it even with some amount of fudging but I gather if thats what the kernel says its more authoritative than the list you get from anyway. 

With KVM/libvirt you can run on the host VM, it exports the UUID generated in libvirt from the XML description. It should be unique to each VM. 

This call is the cause of your issue because cron doesn't run a TTY. It is doing this because the tty name is in the banner message I.E 

If any binares have been changed you can run "rpm -aV". This will md5 match the rpm installed agianst the binary on disk (and also check permissions and such like). Its not exactly tripwire - and wont detect if any user uploaded content has changed obviously but if somethings been changed in /usr or /bin etc it should show up. It will flag up a bunch of false positives for stuff in /etc which has been deliberately changed, so you'll need to review its output after you've ran it. 

Consicely, I do not believe it affects the strength of your encryption. I've checked the source code and as long as I'm interpreting what I read right, you dont necessarily have to worry about this. This code belongs to the module 'stdrng'. At least on Fedora 23 this is built into the kernel rather than exported as a kernel module. When stdrng is initialized for the first time the following calls occur. In crypto/drbg.c initialization starts here. 

From the looks of things the kernel parameter libata.force=3.0G should work.. With regards to data loss, probably not -- but frankly since the SATA vendors clearly haven't honored the SATA spec correctly (or its buggy or whatever) then who knows. 

The best way to fix this is to avoid using standard unix permissions as enforcement. The following four commands should resolve this: 

So I've ran last in a debugger which hopefully will give you at least some answers to your question. My feeling is the root cause is deeper though. Why does last -i show 0.0.0.0 for pts line entries The best way to explain this is what happens when you dont pass -i. The reason for this is in this code section of 

When you run with setuid binaries you only change your effective UID not your real one. So the call will always fail. You should remove the . Its redundant in this case since you use fopen to open the file anyway, its also racey to perform an access check like that and then perform a read on it. 

You can pipe the coredump directly into a program that saves the trace to a temporary location, fetches the backtrace, then puts it somewhere the user can access it, then removes the core. See for an example of how to do something with a coredump pattern. Essentially you can set the kernel control value pipe the coredump into a specific program. This way you have total control over the logic of when to save a coredump and when not to. Be aware that the program you pipe into runs as root! would be an example of doing this. Alternatively, already provides functionality to do this. If you set the coredump pattern like so 

You can dump the set for later reinitialization (after a reboot for example) with which prints to stdout. You can also of course add more IPs to the set at any time using . Also you might actually want your IP blacklist to auto-expire. can do this too by specifying a default timeout for any IP added to the set at creation time with: The default permits up to 65535 hosts in one set, but check the man page, you can create bigger sets using the parameter. 

There is a explicit named rule. There is a explicit non-named rule. Inherit the same context as the parent directory. Apply the . 

You could try using chrt to change the scheduling policy of the tar program to SCHED_BATCH. As per the man page sched_setscheduler(2) 

This should return: [ 0, some_context_stuff ] You need to run this using the same context as sshd. The "initrc_devpts_t" is definitely right. The other context is the referenced user context which you provided. Your code in sshd currently gets -1 which spits out that error. That would happen if the user context (argument 1) being generated in sshd is incorrect. If you can get this python to not produce -1 in the first element of the tuple, then the only thing I can suggest would be to modify the sshd source to add an extra debug line in that function to find out what really the context turns out to be and turn on debug3 logging in sshds config. In 

When you setup a fileset in bacula, it will literally read the pathspec line-by-line and back up like this. It wont create two threads to read the different file paths in the agent. As @SpacemanSpiff said, if you wanted to to do this, the way forward would be to setup different jobs, one for each filespec you wanted to backup. 

Also note that child pids can have independent limits. This program also sets a random limit on invocation of each child. 

Decrypt the keyfile and use that. This offers permanent graceful restart of your services which is what you are after. But the key then is unprotected, unencrypted stored on your server. Give the service provider passwords to your certificates. This might be ideal however you need a level of guarantee that the service will be restarted with the prompt on startup. Be that they may have their monitoring software alert them for this event, however you should under these circumstances be open to situations where the key password is not entered, either at all or with a delay in the password being entered by the service provider. It will be down to you to negotiate what terms the service provider will offer you this service and what guarantees of success they will provide. If they will provide it at all. Some http servers, such as apache provide a option called SSLPassPhraseDialog which will attempt to decrypt the key using an written program that will pass the correct passphrase out when executed. This can be a shell script or program of your choosing that will do this. This has the benefit of giving you option no 1, but acts merely as a weak barrier to all the problems described in option number 1. In addition, depending how careful you are (or not) supplying a password executable this might provide a mechanism for someone to get something other than the passphrase executable to run at startup. Have your hosting provider require they supply with with a scheduled downtime (if its them doing the rebooting) so you can be ready to insert the password in yourself, or alter the updates to run at time where you can be ready to enter the pass phrase to the certificate promptly. 

By adding the entry as a local context, it is fetched from this file instead: which appears after the file that matches at the moment. So, in order to fix this (which is basically a little bit of a hack) you can add the entry as a local override. Alternatively I tried adding this as a HOME_DIR override (like I originally suggested, but using audio_home_t to test the principal) doing the following: 

In your case, the value in here is zeroes. This is why when you run nothing appears for you. In the case of then dns_lookup is invoked. This will pass the entry (p->ut_addr_v6) to be resolved via DNS. In your case this value also contains zeroes. Most of is window dressing and heusteric. Basically what matters is the function . This is a library call which in this case will try its best to resolve the binary value stored in the . When this entry contains zeroes (such as in your case) you are actually resolving this to as is what happens with your output. Is there anything (other than malicious activity) that would reasonably explain the behavior? Well, its probably a bug or oversight. Its unlikely to be malicious since it seems stupid to leave any trace as an attacker rather than omitting a source address. The focus of answers so far have been looking at the wrong place. just reads or . However is doing its best with the data it has. Your root cause lies somewhere in the manner to which is being written to! While a few applications directly write into I guess the source of your problems lie in the way is handling the session management. Other than bash history timestamping, are there other things I can do to track the issue down? is not typically writable and is not meant to be. is written by applications designed to log you in and setup your session. In your case that is . Why sshd is not handling your user properly is very strange as it should be properly copying in the hostname you came in from. This is where debugging efforts should probably be focused. Start with adding debug output of sshd to your logs and see if anything anomalous comes up. If you want to work around the problem (or, maybe even possibly discover more about the issue) you can use to manage by adding it to the session entry to /etc/pam.d/sshd. As a matter of fact it wont hurt to check if it is already there -- because contains a option which would definitely explain your behaviour you are experiencing. Finally, you could not use last at all. does the same job via the audit subsystem. Might be worth a try to see if that has managed to at least write the correct address. If it hasn't then your problem must be with sshd as sshd is passing the DNS names around different subsystems like utmp or audit.