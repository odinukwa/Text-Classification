I used the column to find indexes/partitions that have ghost records. I found a few that have one such record. I then tried to queue them up for processing by scanning all pages: 

This sometimes causes the query to abort. How can I disable IO timeouts on my dev box? Also, in case the query does not abort I sometimes get dump files. How can I disable those? 

Tonight our server decided to install SQL Server 2014 CU7 for SP1. The update appears in the Windows Update history and I found downtime in our logs for it. The previous patch level was CU3. I was surprised by this because I am not used to Windows or SQL Server automatically installing CU's. I also never witnessed a service pack being installed automatically. What has changed? According to sqlserverbuilds.blogspot.de the CU7 was released today. It was installed right away after release. I am not categorically against this but I would like to understand what policies are driving the automatic update choices. Strangely, tonight was not a "patch Tuesday". No other updates were processed. The server did not reboot, either. I'm certain this was not a manual action. So far my plan was to keep the server at the CU3 level and upgrade to 2016 eventually. The new CU being installed caused unnecessary downtime and introduced risk which I normally was not willing to take. In particular I do not wish to install patches at a random time. So what's the automatic update policy and should I do something about that? 

does not order groups randomly. is totally random. comes to mind but I need a new random order each time. How can this be done? 

What you're describing sounds like Microsoft Sql Server HADR commonly called AlwaysOn Availability Groups. It combines traditional Windows clustering with Database Mirroring to allow you a primary replica and up to 4 (Sql Server 2012) secondary replicas. Once set up correctly it will provide you with a single IP address and port # to use as a connection target for your application. Setting up the mirroring portion is a bit more than just deploying your database to the connection though there are ways to automate that also. The replicas may be geographically separated as well as residing on different sub-nets and domains. 

It sounds somewhat like you are hitting a bit of memory pressure. Dirty Buffers are pages in memory that have changes in them that have not been written to disk yet. Normally they would stay in memory until one of the various processes clean them up and/or write them to disk. There are some things you will want to look into but with only the information you have given here I have to ask, "How much ram have you reserved for the OS?" You are running a 64 bit version of Windows. Now, the 32 bit kernel will use 4 GB of RAM. However, we have found (the hard way) that 64 bit versions of the OS will use 8 - 12 GB of RAM dependent on how busy the Sql server is. By that I mean are you doing a lot of reading and writing to disk. If the OS can't allocate the memory it needs for this (because the Sql server has sucked it all up) you will see the CPU being consumed for the additional context switching it needs to do to write those buffers out to disk. When you added the additional RAM to the server did you reset the Sql Server Max Memory setting? When you only had 16 GB of RAM your Sql server was constrained as much as the windows kernel was and so you were likely using virtual disk (hard drive space) quite a bit. When you added more RAM you opened things up a bit and now the Sql server is attempting to keep more of the database(s) in memory. 

I'd like to keep rows with the same together in the result set. The groups themselves should be ordered randomly, though. is supposed to be the secondary sort criterion. Like this: 

The documentation calls out the behavior of and so I understand that part. But the operator seems to behave under a more complex set of rules. 

On my dev box I sometimes need to run very IO intensive queries such as index builds and . This can put so much load on the disk that it hardly can process anything else. This causes enormous lagging in other programs. For that reason I sometimes need to suspend using Process Explorer. If I do that for longer periods of time I sometimes get IO timeouts such as 

Row versioning maintains 14 bytes of internal versioning information for each row. But is this really a cost per row or is this a cost that also applies to each index on the table? It seems the 14 bytes must be added to all index records as well so that index-only scans (and other index-only accesses) can see the versioning information and perceive a point in time snapshot of the data. All information that I could find on the web only talks about a per row overhead of 14 bytes, though. 

I'm not sure how to conclusively test which way it is. I'm not sure the Live Query Statistics output can be trusted in this way. Does anyone know how this works? 

Pull first probe row. Complete the operation if no row available (short-circuit). Match all probe rows against it. 

Both multiple files in a filegroup and multiple filegroups are tools for managing your database. The first lets you manage your IO and both will let you manage your backups. It is possible to backup a single file of a database as well as a single filegroup. Be sure to backup the tail of the transaction log when you do if you are planning on restoring it somewhere. Database files allow your multi-core CPUs to have multiple read/write streams to the database without hitting higher disk queuing values. It may help to think of the filegroup as a logical division and the file as a physical division. If you have multiple filegroups you will automatically have multiple files as a file can only belong to one filegroup. It can also help (if you have enough cores on your server) to have multiple files in your filegroups. That can give you the best of both worlds. You assign database objects to a filegroup not a file. You can put the files on different physical disk arrays. When I first started doing database work it was common knowledge that you put your data and log on separate disks. If you had the budget you would put your non-clustered indexes on another disk. It's tough to get that these days with SAN technology everywhere. However, SAN is a management tool not a performance tool. As you pointed out having different filegroups will allow you to isolate high traffic tables from each other and from lower traffic tables. It will also allow you a limited additional protection from a corrupted database potentially limiting data corruption to a smaller part of the data.