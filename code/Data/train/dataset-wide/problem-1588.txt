Do we really know that the origin of gold was in a supernova explosion? Up until the last few years that was the generally accepted explanation, but things have changed somewhat due to two developments. About half the elements heavier than iron (including gold) must be produced via rapid neutron capture - the so-called r-process. This is not under debate, but the main astrophysical site of the r-process is. (1) It has become increasingly difficult to produce sufficient amounts of some of the heavier elements (including gold) in theoretical models of supernova explosions. The problem is that the exploding material is just not neutron-rich enough, unless very specific conditions to do with rapid rotation, magnetic fields and low overall metallicity are met. (2) The simultaneous observation in gravitational waves (by aLIGO) and electromagnetic waves of GW170817 has revealed that merging neutron stars are possibly a more effective site of the r-process element production. The GW detection clearly provides evidence that this event was produced by merging neutron stars and detailed infrared spectroscopy of the light from the accompanying "kilonova" explosion has revealed the presence of heavy lanthanide elements in the ejecta, which are a product of the r-process. Now there are three difficulties in leaping to the conclusion that all the gold is produced by merging neutron stars. It may well be the dominant source but firstly, it remains to be established what the rate of these events are in the local universe, let along what the rate of these events were before the Sun and solar system were born. There are order of magnitude uncertainties here, although one can make another inference/extrapolation to assume that merging neutron stars are responsible for short gamma ray bursts and use the rate of these to estimate the cosmic history of these events. Secondly it is uncertain just how much r-process element production takes place. This cannot be very well established from the observations of just one event and there are considerable theoretical uncertainties. Thirdly there are observations of very old halo stars that have atmospheres that are enriched with r-process elements. It seems unlikely (though perhaps not impossible) that there would have been time before the birth of these stars to produce this enrichment via the merging of neutron stars, which is a process that can take hundreds of millions or even billions of years. This there may be some proportion of r-process elements that are produced in certain kinds of core-collapse supernovae. Some more details and references can be found in my answer to this closely related question. But maybe your question is broader than this. How can we be sure that heavy elements are produced inside stars? The answer to that lies in a long process of theory, followed by observation, followed by refinements to theory, but the basic answer is that we can see examples of very old stars born right at the beginning of our galaxy. These stars contain almost no heavy elements at all (less than a ten thousandth of what we infer to be in the Sun). We can also measure the presence of (some) heavy elements in the ejecta from high mass stars and we can also see fresh heavy elements appearing at the surfaces of giant stars that has been mixed up from the centres of the stars where they are being produced in nuclear reactions. 

On any practical timescale (ie say 100 billion years) a galaxy could not be said to have died. The majority of stars that ever formed in that galaxy will still be "alive", but they are dim, low-mass stars. However, some of those low-mass stars will have reached the ends of their main sequence lives and, if more massive than $\sim 0.4M_{\odot}$ will go through a luminous, and highly visible, red giant phase. Mitch Goshorn's definition of a cessation of star formation is a reasonable one, although on that definition many of the gas-poor elliptical galaxies we see today would be "dead". The reason the galaxies "run out" of gas is that is might be blown out by the winds and supernovae caused by higher mass stars; it might be stripped from them in tidal interactions with other galaxies, or in the long-run it is because the star formation process mostly produces stars that have the mass of the Sun or lower. Such stars, once formed, can lock away material and prevent it being recycled. The lifetimes of these stars increase rapidly with decreasing stellar mass - reaching a 100 billion years for a 0.4 solar mass star, which is still a bit more massive than the average star. So the mass spectrum of stars in a galaxy should tilt towards lower masses as it gets older. Even when these low mass stars reach the ends of their lives they only return about half their gas to the galaxy, the rest is trapped in a cooling white dwarf. So new star formation will gradually expire as the amount of returned gas gets lower and lower. Note, that the fact that the gas has been inside a star previously does not prevent new star formation. The returned gas consists mostly of unused hydrogen. It is just a question of how much of that gas there is, and whether it can be concentrated sufficiently to form new stars. 

This is too vague a term to be useful, because the Sun does not have any kind of sphere of influence (beyond the solar system) and nor do the nearby stars affect us with their light or gravity (Oort cloud perturbations aside). Instead it would normally refer to the stellar population in the solar vicinity. This is a mixed bag of objects, with few shared characteristics. For example, objects within 10 light years of the Sun now, will not be so in 10 million years time. A working definition might be those stars that are close enough to the Sun that we think the census is complete to that distance. Unfortunately, incompleteness remains in all such samples at low masses. The RECONS programme has been working hard for the last decade to complete the census of objects that are within 10pc (33 light years). The definition of a distant star is even more vague - all stars are distant. The Sun is 5 orders of magnitude closer than the next stars, so if anything, that is the only natural division between local and distant! 

Ricky, it is very rapid. The core collapse and initial neutrino burst takes seconds to tens of seconds. We don't normally think about neutrino interactions, but so many are released that even this might be a problem for a nearby habitable planet. It then takes a few hours for the shockwave from the core collapse and bounce to make it out to the surface, accompanied by an intense flash of UV light that would likely sterilise anything in its planetary system. The outer layers of the exploding star are hurled out at around 1000 km/s, so could travel an astronomical unit in a day or so. The supernova continues to become more luminous for about a week thereafter, increasing in luminosity, from what must have already been enormous (thousands of solar luminosities) by another factor of $10^5$. The equilibrium temperature of any planet scales roughly as $L^{1/4}$, so temperatures would rise by more than an order of magnitude in a week. 

A protostar star in the class I phase is surrounded by an optically thick envelope, but accretes matter through a disk. The protostar will have some magnetic field - it may be mostly primordial at this stage, or could be generated by an internal dynamo. Material accreting from the disk is thought to be locked onto the field lines - a well known phenomenon caused by charged particles spiralling around the field lines. What happens net to drive the jets/outflows is still very actively debated. A very popular baseline model is the so-called Uchida-Shibata theory. Imagine an initially radial magnetic field. As the material locks onto the field is must pull it into a spiral pattern because the accreting material has angular momentum and cannot fall directly into the stellar surface. This action will twist the field lines into a helical shape, with an axis similar to that of the accretion disk of the protostar. The helix will be tighter closer to the stellar rotation pole and will open out further away from the protostar. This gradient in magnetic field strength provides a pressure gradient (because a force acts on charged particles perpendicularly to the field lines) which accelerates material along the polar axis. At the same time, the accelerating material is collimated by punching its way out through the envelope - once some material has opened up a path, it becomes easier for subsequent material to follow. Below is an extract from a paper (unfortunately behind a paywall) by Matsumoto (2009) which has a picture that gives you the general idea. 

For tidally locked binary stars, the two points in question are known as the substellar points. For a tidally locked exoplanet, the point closest to the star would also be known as the substellar point. If the star was also tidally locked to the planet, then there would be a subplanetary point. For a moon locked to a planet, the point on the moon would also be the subplanetary point. If the planet was mutually locked, it would have a sublunar point (not sure whether this latter only applies to the Earth's moon). 

Let's assume a uniform laminar emitter, oriented so that it is at right angles to the line to the detector. The specific intensity emitted by a blackbody, at the surface of the blackbody is $$B_{\nu} = \frac{2h\nu^3}{c^2} \frac{1}{\exp[h\nu/kT] -1}\ {\rm Wm}^{-2}{\rm Hz}^{-1},$$ where $B_{\nu}$ is the "Planck function". Using the definition of specific intensity we can calculate the flux (energy per unit time, per unit frequency) received at the detector as $$ f_{\nu} \simeq B_{\nu}A \Delta \Omega \cos \theta\,$$ where $\Delta \Omega$ is the solid angle of the detector subtended at the laminar surface, $A$ is the area of the laminar surface, and $\theta$ is the angle with respect to a normal from the surface and here, $\cos \theta \simeq 1$. The solid angle subtended by the detector at the surface is $\Delta \Omega = a/D^2$, where $a$ is the area of the detector, but we can also define a solid angle $\Delta \omega = A/D^2$, which is the solid angle of the surface at the detector. Thus $$\frac{f_{\nu}}{\Delta \omega} = B_{\nu}a $$ If the detector has a fixed sensitivity per unit solid angle then unless the object is unresolved, then it does not matter how far away it is; the value of $D$ does not come into it. It is a well-known fact in astronomy that resolved, extended objects have an observed surface brightness (flux density per unit solid angle) that is independent of distance. 

I don't have detailed model calculations to hand and I'm not sure they have been done for the small range of metallicity you mention. However, in handwaving terms. (1) Mass and composition are the most important variables. Helium abundance, in addition to metallicity, could play a role. Rotation could be a very second order effect. Very rapid rotation would lower core temperatures at the same mass. (2) This is the general assumption if a star plus brown dwarf companion formed in a binary or even if they formed in the same cluster. (3) Higher metallicity leads to larger opacity in the atmosphere and a larger brown dwarf. This is a small effect in fully convective objects like these, except perhaps when they are very young and have superadiabatic surface layer. However, as this is when D burning takes place, possibly there is a small effect - higher metallicity leads to a larger radius for the same mass and a lower core temperature. So the mass threshold would be slightly higher. (4) The duration of D burning is mass dependent. The graph below, from Burrows et al. (1997), shows luminosity vs time for brown dwarfs, planets and stars. The D burning phase is the early plateau in luminosity, lasting a few million years for things which are almost stars to maybe one or two hundred million years at 13 Jupiter masses. When D fusion ceases, then cooling recommences and the luminosity will continue to fade (forever), unless the object is massive enough to ignite hydrogen (i.e. a star: the difference between the green lines and blue lines on the plot). 

You cannot just add the ideal gas pressure and degeneracy pressure together for any particular species of particle. (Well, you can, but it would be a poor approximation to the actual pressure). The trouble is that they are not separate things. In general, the electrons in the gas are partially degenerate and always obey Fermi-Dirac statistics. Unfortunately there is no analytic solution that gives you pressure as a function of temperature and density; you either have to use Tables of pre-calculated results, or there are analytic approximations that work well in certain regimes. Clayton's Principles of Stellar Evolution and Nucleosynthesis contains some of these tables and also some better analytical approximations that work in certain regimes of (partial) degeneracy. What is true is that you are fine to add to this pressure, the radiation pressure due to photons and the ideal gas pressure of the ions. EDIT: I realise that what may be troubling you is not the above, but why you can add the separate pressures together at all (eqn 122 in the notes you reference). The reason can be traced to eqn 31 in the same notes. This says that the pressure is an integral over the number density and momenta of individual particles. Note that this equation does not contain anything that identifies the particles (i.e. their mass). The integral can therefore be treated as a summation over all particles, which in turn can be separated into three integrals that cover the ions, electrons and photons separately. 

In a normal stellar atmosphere, where the temperature decreases with height, I think this is not possible. H-alpha absorption arises from the $n=2$ to $n=3$ transition; H-beta from the $n=2$ to $n=4$ transition. Thus both transitions are governed by the number of atoms in the $n=2$ lower level. The absorption coefficient for a transition can be written in proportionality terms by (in local thermodynamic equilibrium) $$ \alpha_\nu \propto \nu B_{lu} n_l ( 1 - \exp[-h\nu/kT]),$$ where $B_{ul}$ is the Einstein absorption coefficient, $n_l$ is the population of the lower energy level and $\nu$ is the photon frequency corresponding to the transition. Using the well-known relationship between the Einstein emission and absorption coefficients and values for the emission coefficients found here, we can estimate a ratio of absorption coefficients for a given temperature. $$ \frac{\alpha_{H\beta}}{\alpha_{H\alpha}} = \frac{g_{n=4}}{g_{n=3}} \frac{A_{H\beta}}{A_{H\alpha}} \left(\frac{\nu_{\alpha}}{\nu_{\beta}}\right)^2 \left(\frac{1 - \exp(-h\nu_{\beta}/kT)}{1-\exp(-h\nu_{\alpha}/kT}\right),$$ where $A_X$ are the Einstein emission coefficients and the statistical weights $g_n$ are given by $2n^2$. Thus if we take $\nu_\alpha = 4.57\times10^{14}$ Hs, $\nu_{\beta}=6.17\times10^{14}$ Hz, $A_{H\alpha} \simeq 10^{8}$ s$^{-1}$, $A_{H\beta} \simeq 3\times 10^{7}$ s$^{-1}$, then $$\frac{\alpha_{H\beta}}{\alpha_{H\alpha}} = 0.29 \left( \frac{1 - \exp(-29642/T)}{1 - \exp(-21956/T)}\right)$$ So when $T$ (in Kelvin) is small, the ratio is about 0.3. When $T$ becomes large then the ratio increases, but above $T \sim 12,000$ K, all the Hydrogen is ionised. Thus in thermodynamic equilibrium is it very difficult to manufacture a circumstance where the optical depth in the H$\beta$ line is larger than that in the H$\alpha$ line. [I'd be grateful if someone can tell me whether the ratio I calculated above is correct, since I could not find it in any reference and need to work it out from scratch]. 

This is an intriguing proposition, but I would ask how your hypothesis explains that the universe appears to be flat? That is with $\Omega_M + \Omega_\lambda = 1$. The evidence for this comes from measurements of the cosmic microwave background, yet if we sum up all the matter (including dark matter), we only arrive at $\Omega_M \sim 0.3$. I do not think that your proposed arrangement both allows an accelerating expansion of the visible part of the universe (which note, can only be gravitationally influenced by "outside regions" that are within causal contact with it) and for it to be geometrically flat. 

I think there is a missing piece of information. The BAT is a coded mask telescope. The imaging is done by photons passing through a mask and falling onto an array of 32768 detectors. $URL$ The "mask-weighted" light curve is produced after a complex ray tracing exercise using an estimate of the position of the source. Looking at some of the software specifications (eg $URL$ ) it seems that the count rate is divided by the number of active detectors. So in this case it looks like the detection rate was more like 200,000 counts/s in the peak 2ms of the event. There would be 400 detected counts, assuming there are no other corrections for sensitivity or vignetting. However, I think there probably are - the mask must take out about 50% of these counts and there are gaps between the detectors. A background rate has also been subtracted. So overall I guess that the first point in the burst is due to around 100 detected photons, and so on. 

I'll add to Wayfaring Stranger's comments. In fact most of the time you would be able to see fewer stars in the night sky of Mars, than in a good dark night sky on Earth, because of dust obscuration. Even in favourable conditions, the optical depth of the Martian atmosphere is usally somewhere between 0.5 and 1 per airmass. (Petrova et al. 2012; Lemmon et al. 2014) and is nearly wavelength independent. This corresponds to a reduction in flux to between 37% and 60% of its value above the atmosphere. This compares to a typical optical (V-band) extinction of around 0.1 magnitude at a good site on Earth, which allows 90% of the flux through. There are times when it can be much worse than this on Mars. This means that the limiting magnitude on Mars would be somewhere between 0.44 and 0.96 magnitudes brighter than it is on a dark site on Earth and that all the stars would be fainter by these values. As a rough guide to the effect this has I took the Hipparcos catalogue and made a frequency histogram of V-magnitude of the stars. If we define a Hipparcos magnitude of 6 as the limit you can see from a good dark site on Earth, then you would see 4559 stars (that's over the whole of the sky in both hemispheres). If that limit was reduced to 5.56 or 5.04, then this number decreases to 2745 or 1560 respectively. Thus there would be a reduction in the number of stars you could see by typically between a factor of 1.66 and 2.92.