No. The query optimizer should see through this. There's no general rule that subqueries are materialized in query execution. 

The typical design here is to have a Transaction table and an Account table The Account table has the current balance, as of the last transaction that has "posted". A modern "online" system will post transactions immediately, but older systems would capture transactions online, but run "batch" to post the transactions later. PK on the transaction table would normally be (AccountId,TransactionId), which balances the cost of insert and the cost of pulling the transactions for a single account. I don't think there's a rule controlling whether you also store the running balance on the transaction table. Normalization does not require you to recalculate the account balance or the running balances. Balance is a proper attribute of Account, regardless of whether it can be derived from rows in another table. And you don't necessarily have to perform these calculations as you insert transactions. 

It's pretty important that you don't grant the user the ability to alter a schema owned by another user, especially one owned by dbo. If you do ownership chains will allow the user to skip permissions checks on the schema owner's objects in all schemas. 

Since you have not applied any CUs for SP2 you can get the fix without taking all the CU fixes by applying SP2 GDR KB4057120 12.0.5214.6 or you can apply the latest Cumulative Update and the GDR hotfix at once with SP2 CU10 KB4057117 12.0.5571.0 

Remove the unnecessary GROUP BY columns and create clustered index on the remaining ones. What subset of those GROUP BY columns are necessary to uniquely identify a row in #OUTPUT? Keep those, and replace the others. Something like: 

docs.microsoft.com is all on GitHub, so you can propose doc changes. I've submitted a pull request to update the language. You can follow it here: $URL$ 

This may have been an existing problem with the database you backed up. Do you need to preserve the existing Service Broker conversations? Messages? Broker Identifier? If you don't need the Service Broker messages, or to preserve any of its conversations or place in a distributed broker topology, you might end all the conversations WITH CLEANUP, or ALTER DATABASE ... SET NEW_BROKER. If you do need to fix this while preserving the other conversations and messages, you would probably need to open a support case, as there are really only a handful of people who can advise you on this. 

The meta-answer to this is to use SSMS to publish the article and examine the replication scripts. The answer is that the @source_owner parameter to sp_addarticle has the target schema. This dates from before schema/owner separation in SQL 2005, where an object owner and a schema were the same thing. And the script you list uses does not publish dbo.Product, it publishes Production.Product 

Apart from the licensing question, there's a question about the technical limitations of Web/Standard edition. Web edition is limited to "Lesser of 4 sockets or 16 cores", but for a Virtual Machine the sockets and cores are reported by the hypervisor, and may differ from the underlying hardware. That appears to be the case here, as the hypervisor is presenting the VM with 8 VCPUs and claiming that they are on 8 separate sockets. SQL Server on startup looks at the reported sockets and cores when and will limit the schedulers accordingly. Here that's limiting the CPU use to half of what it should be. To fix this, the hoster should reconfigure the hypervisor to accurately report the socket and core count of the hypervisor host. 

Would normally use the clustered index. But scanning the whole clustered index to count the rows, or to fetch a page of rows from deep in the sort order is expensive. So it's a separate non-clustered index on the same key is sometimes created to support these queries. 

The CPU_USED row should be accurate. And so your workload is only managing to use 7sec of CPU time per second. This is like only running on 7 cores. In the mean time you have 1000sec of page latch time, meaning that you have, on average 1000 sessions waiting to latch a page, and only 7 performing useful work. You are also running very old software on a very large server. The throughput on this server is pretty high, suggesting that the operation it supports is important. Also consider your username. So you should consider, strongly, engaging Microsoft Support or a partner with expertise in this sort of thing. That said, the next step is to identify the source of your latch waits. Generally these are either on a hot page of a table in your database, or a system page in TempDb. The sys.dm_exec_requests.wait_resource should tell you which. If tempdb, then Recommendations to reduce allocation contention in SQL Server tempdb. If a table in your database, then post the DDL (including indexes) for the table and a the details of the queries that are blocking. There is a whitepaper on diagnosing and resolving latch contention on SQL 2008 here: Diagnosing and Resolving Latch Contention on SQL Server 

I favor computing and materializing granular object access entitlements so they are simple and cheap for the database to enforce at runtime. And to simplify the relational model. 

With lock hints, a reader can take an U or X lock, so in that case yes. But otherwise no. Even in SERIALIZABLE isolation level, readers don't take locks that would block other readers. 

Yes. In fact a if the primary discovers a corrupt page, it will repair the corruption by getting an uncorrupted copy of the page from a secondary. This is called Automatic Page Repair 

Linux Support Roadmap for SQL Server Machine Learning Services: "Machine learning using R or Python in-database is not currently supported in SQL Server on Linux. Look for announcements in a later release. However, on Linux you can perform native scoring using the T-SQL PREDICT function. Native scoring lets you score from a pretrained model very fast, without calling or even requiring an R runtime. This means you can use SQL Server on Linux to generate predictions very fast, to serve client applications." The stand-alone Machine Learning Server is already supported on Linux. 

Putting aside the dubious utility of single user mode here, you need to be connected to the database when you put it in single user mode to prevent another session from connecting. EG 

An alias is a client-side concept. To be effective you must configure the alias on every client computer connecting to SQL Server. The alternative is to create a Hostname Alias to enable that name to be resolved by all clients on the network. There are four easy steps to creating a Hostname Alias, but they are poorly documented and often misunderstood. All you have to do is: 1) Create a DNS Record pointing to the IP address of the target server 2) Configure SQL Server to listen on port 1433 on that IP address 3) Add SPNs to enable the SQL Server Service account to use Kerberos Authentication (optional if you don't use Kerberos). 4) Add BackConnectionHostNames entries to enable NTLM Authentication $URL$ 

In Azure we have a dedicated service for landing telemetry streams like this: Event Hubs. It's whole purpose in life is to capture the events and store them for long enough for your stream processors, which can be running in Spark, or Azure Functions, or Azure Stream Analytics, to reliably process them. Your stream processors would do things like archive the event streams, perform any realtime analysis you need, and load (perhaps sampled or summarized) data into a DMBS. You can also load a DBMS by batch processing against the archive of the raw telemetry, using the so-called "lambda architecture" see, eg $URL$ 

IMO this problem lacks sufficient generality to build standards or reusable solutions. It's really just a problem you solve with data modeling, Something along the lines of: 

This is spelled out in some detail here: $URL$ But the basic answer is that stored procedures have cached and reused query plans, and permissions checks are not necessary when objects owned by the stored procedure owner are accessed from a stored procedure. If valid ownership chains exist between the procedure and the object to be accessed, then only EXECUTE permission is checked before the stored procedure is started. Other permissions checks are skipped. 

@sp_BlitzErik is correct. AGs need planning and care. AGs don't have the particular issues you struggle with in Transactional Replication, but they have others. If you are comfortable with Windows Server Failover Clusters (WSFC), then you have conquered the biggest hurdle to deploying and running a SQL Server Availability Group. AGs use WSFC for the quorum model, and for a floating IP that moves between the nodes. If you are willing to not have automatic failover, you can use the new "Clusterless" AGs in SQL Server 2017. See Read-scale availability groups. They are called "Read-scale" AGs to emphasize that they are not intended for HA, but they do support synchronous (zero data loss) replicas, and manual failover. 

Because it's it might be GBs of data, so the query tools like SQLCMD and SSMS truncate the results for display. You can dump to a file with bcp.exe, setting the "prefix-length" to 0. 

Packet-flow must, of course, work in both directions. If the Distributor can connect to both the Publisher and Subscriber, you can use Push Subscriptions. With a Push Subscription all the processes run on the Distributor, and the Subscriber would never need to connect to the Publisher or Distributor. 

Every time you create a group or an order you would generate the AccessControl entries using (potentially complex) business logic and configuration data, but storing, auditing and enforcing the security is simple. 

Yes. Once you have the subscriptions created and initialized then you insert your data into the tables created on the subscribers. The next time the subscriptions synchronize, the data will be sent to the publisher. 

PAGEIOLATCH_SH means waits for reading database pages. When you create a database snapshot, none of the database pages are in the cache, and you will have to fetch them from disk. 

Other than that: What is the point of the TenantIsolationID? You should include TenantID in every clustered index that contains tenant data. It should be the leading column, unless you're using it for partitioning, in which case it can be a trailing column. You must plan for splitting your single database into smaller databases as a scale plan. But split can be a one-way operation. Whether TenantID is an INT or UNIQUEIDENTIFIER only matters for index size. All your secondary indexes will be bigger if you use UNIQUEIDENTIFIER. But that's not a huge cost. Fragmentation and page splitting won't be a big deal here. See Good Page Splits and Sequential GUID Key Generation for details on the performance implications of having multiple insert points in a table. 

First for an AG you need to Force Protocol Encryption on each instance participating in the AG. With an FCI there is only one instance. Also with an AG clients can connect to the AG Listener, and they can connect to the instances directly. So you have to configure the certificates accordingly. See $URL$ Note, that this configuration is not required to have protocol encryption. It's required to authenticate the SQL Server to the clients. If you merely want protocol encryption, clients may always request it, or you can force it on the server with a self-signed certificate. 

Pro: partition operations can be used to operate on single tenants. Eg SWITCH, TRUNCATE, REBUILD. Con: Maintence, and possibly cost of partition splitting to add new tenants. But see: 

SSMS is only a development and management tool. To install a SQL Server instance you need do download and install SQL Server. EG SQL Server Developer Edition. See: $URL$ Or provision a SQL Server database in the cloud with Azure SQL Database. $URL$ 

The main thing to remember here is that all the queries in a batch are parsed and compiled before any of them starts to execute. Permissions are checked during execution. And it's even possible to reference a table in a query in such a way that permissions on that table are never actually checked. If you watch an XE session like this: 

The secondary doesn't apply the logical operation. The log record indicates exactly what modifications are made to the database pages. 

A Database Snapshot is a NTFS copy-on-write snapshot of the target database. There is only one copy of the unchanged pages shared between the database and its snapshot. If you use a backup/restore instead of a Database Snapshot, then you could put it on a different disk. 

You've found it. It is expired, and out-of compliance. It will stop working soon. There should be no need to wipe and reinstall SQL Server. You can perform a quick, in-place Edition Upgrade. The supported Edition Upgrade paths are listed here, and in this case are: From: SQL Server 2016 (13.x) Evaluation Enterprise** To: SQL Server 2016 (13.x) Enterprise (Server+CAL or Core License) SQL Server 2016 (13.x) Standard SQL Server 2016 (13.x) Developer SQL Server 2016 (13.x) Web Of course any time you are installing software on your server, you should ensure that you have a current set of backups. 

even without the extra included columns only a few logical IOs. 3 or 4 to traverse down the IX_User_UserId_Email to find the (TenantID,UserId) associated with the email, and 3 or 4 to traverse the clustered index to the leaf page containing all the user data. 

You don't have to run the query in the context of each database to read its catalog. You can use a three-part name like . This should also allow you to include the 80 compat databases. So this simplifies to: 

If the views were created by the Database Tuning Advisor, then they are probably indexed views. The Tuning Advisor "recommends how you can improve query processing performance by modifying database structures such as indexes, indexed views, and partitioning. " An indexed view can be used even if you don't explicitly query it because of Indexed View Query Rewrite: "The view does not have to be referenced in the query for the optimizer to consider that view for a substitution. " -docs So you should include them in a (periodic) evaluation of your indexing strategy, looking for both missing and unused indexes. You can find lots of DMV queries for both, based on sys.dm_db_index_usage_stats sys.dm_db_missing_index_group_stats 

Do that. This isn't really a hard case. (rule_id, index) uniquely identifies a row, so that should be your clustered PK, unless there's some compelling reason to use a different design. and because 

Always print out your dynamic SQL before executing it so you can check for syntax errors. Should be something like: 

A queue is a table. SEND is an insert. RECEIVE is a delete. There is some functionality to process queues from inside SQL Server, but you can always just use external clients. See eg $URL$ for a sample of how to process a queue from an external application. 

You'll need to hard-code the view on each server. But you can write a stored procedure to recreate the view on-demand. 

There is ZERO additional documentation available to MSDN subscribers. All published documentation and support material is published to the web. The best sources for SQL Server Internals are the work of Kalen Delaney $URL$ and Paul Randal $URL$ In particular Paul has some posts on reading the SQL Log. eg $URL$ 

Database Mail does not send the mail through SMTP when you call sp_send_dbmail. The message is put on a service broker queue, and sent by a background process. So if the transaction in which the trigger executes is rolled back, then the message will be rolled back from the queue, and the mail will never be sent. So that's one reason why your mail may not be sent. Otherwise you can check the log table: 

Every SEND and RECEIVE is in the context of a "conversation" which is a potentially long-lived session between two "services". conversations can live forever and be reused, and are the source of much confusion for getting started with service broker. 

if @@trancount <> 0 then that SELECT will run with the current transaction isolation level. Which (unless it already happens to be SNAPSHOT) will require S locks for reading data. If you want all SELECTS using the default READ COMMITTED isolation level to use row versioning instead of S locks, set READ COMMITTED SNAPSHOT on the database.