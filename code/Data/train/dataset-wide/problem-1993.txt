Note that this example assumes your database is called . You'll have to substitute in the logical name for the file and database. The recovery model change in the above example must be preceded and followed by appropriate backups. Make sure your backups are well-tested, and stored in a secure off-site location as well. While this particular command shouldn't cause any corruption, backups are critical. 

Max Server Memory should be no higher than 445 GB. This gives Windows enough memory to manage itself. Yes, it really does need that much. Max Degree of Parallelism should be set to the number of logical cores in one NUMA node, up to a maximum of 8. If it's running an OLTP environment, make sure Optimize for Ad Hoc Workloads is enabled. Ensure power saving on the OS is set to High Performance. TempDB should be on your fastest storage, followed by transaction logs. There's a whole lot more you can look at, but these are the main items. As some comments have noted, SQL Server 2008 R2 is out of mainstream support. Extended support ends in July 2019 (not this year), so there's just about enough time to start a project to move to a newer version. I would be looking firmly at SQL Server 2017 or later. 

This is a full database import which also includes tablespace definitions. Find out from the original Solaris database what are all the target tablespace names and sizes. Pre-create the tablespaces manually using the datafile paths of your choice () before performing the import. You will see which you can ignore. 

In some versions of Oracle, you cannot put space between the beginning-of-line and the WALLET_LOCATION keyword, and you must put space between the beginning-of-line and the definition of a wallet. Your snippet indicates that you failed at one of these things. I think they removed this silly requirement starting from some Oracle version, but better safe than sorry. Good: 

Beware the OS authentication is done by client machine, not by the database server. I think that within reasonable pessimism to Oracle tools you could expect that it boils down to this: database server receives a TCP connection from whichever IP address that might pass the network path, and that connection just claims "I promise I've done OS authentication and, trust me, this connection is made on behalf of OPS$JohnSmithCEO." Neither the database client or database server is given OS password for additional OS authentication! And you can't even trust that this connection came from a valid Oracle software. It can be man-in-the-middle in reality. It's worse than telnet. Whatever "Windows-specific" checks are done by Oracle, they seem contrived and you can't really trust they are complete and secure. 

Use from the command line. For example, to dump all your databases to a file, use the switch and a result file for output: 

This means that a percentage of space is left on each 8KB data page, to try reducing fragmentation. See link below about data pages. 

You're running out of RAM on the server. One of the comments refers to lowering your Max Server Memory setting. For an instance with only 4GB RAM (which is not really enough to run SQL Server), your Max Server Memory should be set to 2048MB at most, possibly even lower. 

Neither one of these statements is correct in their current form. If you want to insert values from the output of a statement, you need to remove the keyword. Once that's done, we are left with: 

As Shanky implied in the comments, your Max Server Memory value is probably not changed from the default. SQL is designed to use everything at its disposal. I've written a script, based off work from Jonathan Kehayias of SQLskills, that will let you know the value you should use. Jon advises reserving the following amount of RAM for the operating system: 

So, Dev could simply back up unused blocks, and there are plenty of reasons why Prod would have no symmetry here. Another possible reason. RMAN will do Level 0 (i.e. full) if it believes you erroneously requested Level 1. Manual says: 

Because for some strange reason, although regexp_like looks like a function on a first glance, it is not a function in reality, but is a condition (an integral part of SQL syntax). But, more confusingly, it is a function if used in PL/SQL: 

No. The operations go to DR site in exactly the same sequential order as they were performed on the primary site. You can only re-order operations at the level of your application code, the database engine cannot do this for you. The database engine cannot reorder changes to data because it needs to ensure that DR database is bit-to-bit identical to the state of primary database at a very recent point-in-time. Your impression that COMMIT transfers all the data is incorrect. Data is transferred all the time throughout the transaction, and COMMIT is quite a small transfer itself that ends it. The only special thing about the COMMIT is that it is the only transfer for which the primary database has to wait until the COMMIT mark is transferred to DR, written there to disk, and acknowledged back to primary. Anyway, I would strongly suggest to change SYNC to ASYNC in your archive_log_dest_x, which would obviously solve your problem (problem of waiting for COMMIT), at a cost of potential loss of the unsynchronized data in a real DR situation. If your software controls real-time multimillion FOREX transactions, and you have a high-bandwidth high-available network connection, then SYNC is the way to go. If not, you should accept this small risk, as most other users do. 

I do not discuss typical maintenance tasks you should also be running on your instance, which includes , index and statistic maintenance, etc. Those are outside of the scope of this answer. 

Stop restarting SQL Server. Just stop doing that. If you have a particular problem with a particular process that is being killed and rolling back, let it finish. Then, find out what's causing that to happen. Use Extended Events, or a server-side trace, heck, even a third party tool like sp_WhoIsActive. Do something. SQL Server has a transaction log. All modifications go in there. When these are committed, the changes go into the data file. When they are killed, they have to roll back. That's just how it works. The rollback has to finish for the database to remain transactionally consistent. Restarting SQL Server does not make this go faster. In fact, you could easily make things worse, possibly even taking the database offline or corrupting it. Admittedly, this answer does not address your question, but there's not enough space in the comments to explain why you need to figure out what's causing the killed processes. It could be a DBA, it could be an automated process. Bottom line: let SQL Server finish what it's doing. If it takes a long time to do something, killing the task will take just as long, if not longer, as that to roll back. 

Forget copying binaries around. I would integrate remote RMAN executions into your normal backup application/solution (i.e. filesystem backup to fire a script) for the sake of consistency. Especially scheduling of backups would be easier. Re: "In this case parameterfile.par must exist in SRV_PROD_1". Not a problem if you dig into RMAN's own CREATE SCRIPT syntax - you can store the run{ } script inside your catalog. Re: "rman.log will be created also on SRV_PROD_1". Not a problem if you take a look at catalog schema contests - surely there is one large table per each catalog that holds the logs anyway (I can't remember if it's ROUT or RMAN_OUT?). 

Verify here you have had only a single control file and not two or three of them. If you have had more than one control file, then this instruction is not for you. 

The 11.2 manual says you should also check plain LOG_ARCHIVE_DEST. Although it comes from Standard Edition, it is still valid in case none of numbered Enterprise Edition destinations LOG_ARCHIVE_DEST_n are defined. I haven't tested that personally. In case all are unset, the another 11.2 manual says 

To summarise, if you just want to return a value for a client application, you probably don't need an output parameter. However, if you want to pass values around in T-SQL between stored procedures, they can be very useful. For what it's worth, I hardly use output parameters. 

In the example in the article, they use a Person as the generalized entity, and derive Faculty, Staff and Student from the Person. There are also rules that govern a generalized entity: 

This is probably one of the silliest processes I've ever heard of, but anyway. After you've updated your data to remove the offensive words, run a checkpoint, and then restarted SQL Server (just to make sure it's all committed), the only way to guarantee that the underlying disk has been overwritten, and the "offensive" data expunged, is to perform a shrink of the database file. This effectively takes the data pages from the end of the file and inserts them into open spaces at the beginning of the file to fill up "empty" space. It's an horrific process for SQL Server, as it causes massive fragmentation, but it will "solve" your problem. Once you've moved your data to the new server, you must run a full index rebuild to defragment the tables, but, hey, it should work. 

Based on your additional comments, I'd say you could keep the query as is, but create your table as an "index-organized table". This is basically a way to tell Oracle: dear, keep this ORDER BY forever for this data, my queries will use this order a lot. 

You issue from the same operating system where the server (database) is installed. In this case there is no difference between AUTHENTICATION=CLIENT and AUTHENTICATION=SERVER or SERVER_ENCRYPT (by the way, I believe the manuals strongly recommend against CLIENT setting). If you are logged in as an OS user , the DB2 will not doubt that OS authenticated you as such. I am not aware of any trick that would cause DB2 to re-authenticate OS user just to "double check". But a very easy way to accomplish what you require is to be a different OS user while issuing . If you are logged in as and try to , you will be asked for a password, because DB2 can see that OS have not authenticated you as . Hence DB2 asks for a password and performs OS authentication on your behalf. In a default database setup, all users (including ) would be able to connect as themselves, so, as you apparently have security in mind, you'd probably want to first. 

Why not have always populated? When anyone who creates a shopping bag, they get an auto assigned, but there's another field on the account table with a boolean flag set true by default. You could keep things like customer details in a separate table with a 1:1 map to the account table, for customers who do create accounts. Your challenge then is that you might have repeat customers with different s, but might be worth it depending on how likely customers will create accounts. 

I'd stick with . An takes up 4 bytes of space. On 500,000 rows, that works out to 2 million bytes, or just under 2MB per day of overhead (which is practically nothing these days). Double that for , which is 8 bytes in size, so 500,000 rows will need under 4MB of space. We generally recommend if you're going to add (or change) over 2 billion rows over the lifetime of the table or index. While supports a range of 4 billion values from roughly -2 billion to 2 billion, people generally start their value at 0 or 1, cutting out half of the available range. In a case where you're doing 500,000 a day, a might be better for you. It's a good trade-off between storage and data churn in the medium to long term. 

This needs to be char-by-char identical every time. I see it ends with a slash here, which is non-standard, and I suspect that you've added it by mistake. Adding a slash will result in "ORACLE not available" in so-called bequeath sqlplus, I've just experimented on my system. Try again with one consistent setting like: 

XE does not support Streams, as officially documented. SE and SE1 support Streams but without redo capture. EE and PE (Personal Edition) fully support Streams, and also support Logical Standby. 

TL;DR to be able to restore data+undo without overwriting the entire current undo. When you perform RMAN online backup, some transactions are in the middle of processing, they did change something but not yet performed their COMMIT. Transactional model of work ("atomicity") requires those transactions to be rolled back (completely undone) on recovery. Oracle engine does this automatically. Oracle engine does not allow onlining tablespace if this cannot be done, to protect user from seeing possibly inconsistent data. So do transactions save somehow the "consistent" data before they change it? Yes, in the undo tablespace (or a "rollback segment"). This is why, recovering a tablespace, you cannot "forget" it had undo, you need to recover undo from the same point in time (same SCN). To sum up, Oracle engine needs to have undo data synchronized with from the same point in time as the main data. Undo is, usually, put into a single tablespace despite it is being a mix of pieces of "consistent" information from many different data tablespaces. This is why you TSPITR to an auxiliary instance - to be able to restore data+undo without overwriting the entire current undo. Once auxiliary is online, the auxiliary's data has been made consistent and auxiliary's undo is no longer needed.