Syllables do not have to conform to the notion of "sonority sequencing", which is only a rough approximation of crosslinguistic syllable structure tendencies. It is generally recognized (following decades of argument by Ohala, also see Wright 1996) that abstract "sonority" is not particularly relevant in explaining segment sequence tendencies, and what matters is whether a sequence is easily parsed. The desirability of onset [tr] relative to [rt] or [tw, nw] relative to [wt, wn] can be explained by the fact that the former lead to acoustic landmarks which make it easy to identify the segments (in an onset), but the latter are not (for instance, [w] has very weak acoustic cues, no striking release, low amplitude, but a strong influence on vowel formants, so [w] "should" be next to a vowel). Sequences of the form [sC] where C is not a fricative are fairly easy to parse acoustically – though not trivial, so they might be allowed, or they might be disallowed. Sequences of the form [ʃC] would also be good, but having a constrast between [sC] and [ʃC] is not so good (because it is harder to distinguish the difference between [s] and [ʃ] before a consonant). Since [ks] is not a possible onset in English, there is only one possible syllabification of /VksV/: [Vk.sV]. The maximum onset principle does some work for you, in explaining why /VstV/ syllabifies as [V.stV] in English, not [Vs.tV]. 

You have apparently taken a position on the phonetic value of the sound sometimes written as [z], that it is voiced. The alternative is that it is an "emphatic", though that raises questions as to the phonetic nature of emphatics (i..e were they pharyngealized or were they ejective?). Given the assumption that the phoneme is phonetically realized as z, the rule would apply just to the "voiced alveolar fricative". It is plausible that the other potential voiced fricatives <ꜣ,ꜥ> were not fricatives but rather were approximants, i.e. sonorants, in which case one could just express the change as devoicing of fricatives (regardless of place of articulation). If you instead adopt the emphatic interpretation of "z", you would essentially have the same rule, replacing "voiced" with "emphatic" and "voiceless" with "non-emphatic". 

As you know from your previous questions on the topic, the waveform of a breathy voiced vowel is more sinusoidal in shape compared to that of a modal voiced vowel, which is more triangular. The power spectrum of the two types of vowels differ in that more of the energy is distributed over low frequencies in breathy vowels, compared to modal vowels. As I recall, Rothenberg has characterized this as saying that the main difference is in the amplitude of the fundamental, where a breathy vowel has a very prominent fundamental and higher harmonics are much lower in amplitude, and flatter, but modal vowels have a more consistent gentle roll-off across frequencies. By comparing the ratio of two harmonics, one can quantify this relation. An obvious technique for quantifying spectral tilt would be regression through amplitude of harmonic peaks. However, this does not work the way you'd expect, since a breathy spectrum is a high fundamental and everything else flatter, and regression essentially treats the fundamental as an outlier. Comparison of H1 and H2 has been the most useful metric since it involves the fundamental, and minimizes the effect of formant differences. There is evidence that planned linguistically-contrastive phonation differences can involve a lengthening of the vocal tract via lowering of the larynx, which leads to formant shifts, which gives you a bit of an unsolvable problem. However, for pathological breathiness, this is probably not relevant. Harmonic to noise ratio calculation is based on the observation that with breathiness, there is a lot of non-harmonic noise in the signal (though this is not always true with linguistic breathiness), where the amplitude of the spectrum at a multiple of the fundamental doesn't stand out so distinctly. I don't know whether research on the topic has concluded that there are any reliable acoustic differences between pathological breathiness and phonemic breathiness. (Likewise, creakiness). [EDIT] Harmonic-to-harmonic measures are supposed to quantify the concept of "spectral tilt", which relates to the overall waveform shape which in turn relates to the glottal volume velocity function which relates to how the vocal folds open and close. The ideal (in the sense of an ideal frictionless surface) is that there would be some amplitude at the fundamental, and other amplitudes at multiples of the fundamental, and nothing in between. However, that ain't the reality of it, so besides the energy contributed by the vibration of the vocal folds, there is "noise", which is low-level random energy (which may have a particular frequency distribution). So then you need a way of discerning the "noisiness" of the signal. As applied to phonation types, as far as I know the added noise should be evenly distributed across frequencies, so you don't expect concentrations of high-frequency noise as you get with [s]; nor should the noise be just low-frequency. So there is a difference in expectation, compared to measures of spectral tilt. I'm not sure what you mean by "harmonic-to-harmonic ratio" versus "harmonic-to-frequency ratio". 

There is already such an alphabet: the International Phonetic Alphabet. I assume that you mean that you've created an alternative to the IPA (perhaps one omitting nonexistent sounds, depending on what language this is supposed to be for). Competing alphabets (and abjads / abugidas -- a technical distinction in kinds of non-pictographic writing systems) reflect social distinctions of some kind. The indigenous Vai syllabary is used I suppose you would say to be "traditional", but Latin-based letters are used to be "modern". The most frequent divide, as far as I know, is religion (e.g. Sindhi which is written in Devanagari, Gurmukhi and Arabic which roughly aligns with the Hindu / Sikh / Muslim division). There are also nationality-based distinctions such as Latin alphabet for Kurdish in Turkey vs. Arabic script in Iran and Iraq, or traditional Mongolian script in Inner Mongolia (China) vs. Cyrillic in Mongolia (or, Latin on the interwebs). Likewise, Canadian Aboriginal syllabics are used along with Latin script, depending on language and dialect, province (Nunavut vs. Labrador Inuktitut). 

There is a tradition of transcription which treats all syllabic sonorants in English as deriving from schwa plus sonorant sequences, and that is a phonologically credible analysis. The tradition that transcribes syllabic [n] as [ən] is basically using a broader transcription than the one transcribing this as [n̩]. There is actually phonological evidence that schwa is deleted, coming from an allophonic rule where /t/ becomes [ʔ] immediately before [n, n̩], but not before schwa. You see this in "lighten" ([laɪʔn̩]) but not "light a newspaper" ([laɪɾənuʊs....]). This shows that /t/ and the nasal are phonologically adjacent, thus there is no phonetic schwa after the glottal stop (thus supporting the syllabic sonorant transcription). The problem is that for something like "seven", there is a brief period between the release of the fricative and moment when lingual closure is made, where there is something that could be called a schwa-like vowel. There is no definitive test that will tell you whether that moment is "a vowel" or simply a transition between a consonant and a syllabic nasal. The same goes for "problem". In the case of the initial syllable of "employee", I would say that is unambiguously [ɛm...], not even schwa, but that is no doubt a dialect difference, and I can imagine there are dialects with reduction. If you switch to "employ", I can pronounce that [ɛmploɪ] or [m̩ploɪ] (not particularly natural, but not totally out), so reduction to syllabic nasal is optional for me, word initially. In that context, you don't get *[əmploɪ]. This argues that the apparent schwa-like vowel is just a release feature, and there actually is a rule reducing schwa plus sonorant to a syllabic sonorant (obligatory for me; reduction to schwa is what is optional, for me). It is very hard for students to believe instructors telling them that there is a schwa, or that there isn't a schwa, when the student's own pronunciation is different. I am a fairly aggressive reducer, and I found that some students had syllabic sonorants (thus agreed with me) and others had schwa plus sonorant (thus disagreed with me, and some of their colleagues). 

As I understand it, the essential character which you're seeking is that it is regressive suffix-to-root assimilation (not progressive, and not bidirectional), and the trigger has to actually be there. This would leave out cases that are regressive root-to-prefix as well (various dialects of Arabic have such rounding "harmonies"). Various Romance languages ("Italian" and "Spanish") have suffix-triggered raisings, a.k.a. metaphony, which I believe are triggered only by suffixes. Jose Hualde has written on that (not that I can remember a specific paper). There is also regressive non-iterative harmony in North Saami which is triggered by a suffix and applies within the root, though only to epenthetic vowels connected with "Q3" (both rounding and fronting). It seems that Bengali and Assamese have such a non-iterative suffix-triggered harmony, but more digging into word structure is necessary (one putative example might exemplify harmony within a suffix – or, a different analysis of the suffix). For illustrative purposes, metaphony might be the best bet. There may be other examples, depending on whether you want to broaden the characterization. For example Icelandic u-umlaut is arguable iterative. Likewise Uyghur umlaut. 

"Prosody" derives from προσῳδία, which you can read all about here: it roughly is about pitch being superimposed on speech. In Latin this was extended to poetic meter. Modern linguistics (in the past 70 or so years) has extended that to pretty much all properties. It is not a technical concept, so it doesn't have a precise definition and it doesn't sharply contrast with something else. The essence of "prosody" is that it is any sound property of speech that can extend over more than one segment. It is equivalent to "autosegment". "Segment" on the other hand is a much crisper concept, although there are unresolved debates of the "one segment or two?" variety, e.g. "Is [tʃ] one segment or two?" – typically the answer is deferred to some higher-level a priori postulate such as objecting that [tʃ] can't be a cluster because in the language there aren't any other obstruent clusters in the onset. In feature-geometric theories the answer is trivial: something is suprasegmental iff the thing dominates the root node (mother node of all segments). The London School of Phonology spearheaded by Firth would give a "prosodic" analysis to whatever it could, and that use of "prosodic" is very similar to "autosegmental". In essence, vowel harmony would be said to be prosodic just in case it marks off a continuous span of speech. By inspecting the history of vowel harmony in autosegmental phonology, one can clearly see that "suprasegmental" vs. "segmental" is completely orthogonal to dominance relations. In early work on harmony, harmonizing features such as front/back, round, ATR (see Clements' Vowel harmony in nonlinear generative phonology) were "suprasegmentalized" just in case they spread. However since the first hierarchical model of features in Clements 1985, it was understood that sub-segmental features can spread just as well as suprasegmental features. It is completely controversial where tone features in Chinese (any Chinese language) attach -- subsegmentally or suprasegmentally. Sticking to the most-Greek understanding of "prosody", only tone would be prosody, so everything else would be non-prosodic: therefore, Chinese at least has been analyzed as a segmental prosodic feature, and Turkish (in Clements' analysis) has been analyzed as suprasegmental non-prosody. However: that is all just a terminology game. There is no utility to the term "prosody", which has largely been coopted by people working on intonation. The distinction "subsegmental" vs "suprasegmental" refers to something well-defined which is not easily detectable (that is, we know what the terms mean, and we don't have tools that easily identify whether is it one versus the other), for which reason "suprasegmental" is a fairly useless term as well. 

The words utrom, morgen, mañana don't all derive from the same word in Proto-Indo-european, so that is why they are pronounced differently. As to why "morning" and "tomorrow" are sufficiently similar in semantics that they can be the same word, this is a reasonably common fact across languages (very common in Bantu, for example). If you're going to develop terminology for time, the most fundamental division is the boundary delineated by the long sleep, which terminates in the morning, which comes tomorrow. (The most basic distinctions of time are "present", "future" and "past", and "tomorrow" is a much higher order concept, which depends on also developing a discrete 24-ish hour concept "day"). The fact that "tomorrow" is an important concept for human existence does not mean that the word for "tomorrow" is doomed to remain unchanged. Swallow (the bird) is likewise often the same root as the verb (e.g. in Logoori), which is explained by the distinctive feeding habit of swallows. There are similar semantic intersections between "right" (direction) and "right" (correct), or "male" ("right hand" is the "male hand"), explained by the predominance of right-handedness in humans and thus various ostensively positive descriptions of right (direction). Under language contact, polysemy of a word or phrase can be borrowed by a language. An example is French marché aux puces which we translate as "flea market" (that's how it literally would translate into English), even though nobody sells fleas there. Such borrowings of translations are known as "calques". Lakoff & Johnson have a book, Metaphors We Live By, which discusses the importance of metaphor in language and human life. 

The only way to determine the exact nature of your "l" is with a broader corpus of recordings plus some actual physiological data. L is subject to various degrees of backing and vocalization not only in English but many other languages. In my dialect of American English, /l/ is retracted, but still lingually articulated, when preceded by a back vowel in the same syllable (however: /l/ is completely deletes after /ɑ/ in certain clusters, e.g. wa(l)k, pa(l)m where some people retain l). In some dialects, this backing also applies in milk, help, fill. It resembles [w] to the point that you might well transcribe milk as [mɪwk]. It is thus possible that in your dialect, l has simply become w in some coda context. The question then is whether "code" and "cold" are pronounced the same or different for you. You also want to look at l intervocalically, as in "folly, allow", because stress affects syllabification which may affect how l is realized. There is no right or wrong in terms of pronunciation, just confusing or not, from a TESOL student's perspective. There is no one standard, rather there are many standards, for example US standard, RP for British Standard, Kenyan Standard, etc. If you're aiming for RP, sorry, you missed the mark, but it sounds pretty okay for SA standard. The downward formant transition at the end of the vowel in in your "golf" clearly indicates that there is a w-like element for "l". In the case of "also", there is nothing at all. I suspect that the vowel quality is different, and different from "ostrich", "awesome" or "arse" (if you have the arse/ass distinction). 

In some Bantu languages, it is used to indicate "almost doing", which isn't a subjunctive use I know of in Romance (maybe I just don't know about it). Part of the problem is that there isn't an independent, robust test for whether something is a "subjunctive". I suspect that if a verb form were used for realis moods, it would not be labeled "subjunctive". So while I think it is an interesting question what variation there is, there is a presupposition that we independently know what a "subjunctive" is, as distinct from an optative, conditional, potential, or even "future". Perhaps the key is to identify predicates that always call for a "subjunctive" in their complements, and then understand the semantic properties that distinguish that from more general irrealis. 

The underlying issue is what / how much information is necessary to characterize a segment. The background assumption is the Jakobsonian one that every segment is defined in terms of a collection of features = descriptive attributes and values associated with the attributes (we usually say the values are "plus and minus" but in fact values have included plus, minus, "plus-and-minus", 0, u, m as well as numbers). The answer to the "what structure" question then depends on the nature of the feature set. If there is a fixed set of features for language where all and only those features are present in all languages, then a segment can be defined as a set of pairs "value, attribute", without ordering of those features. They can also be defined as an ordered list of values (without attributes), where the attribute can be projected from the order -- e.g. the first attribute is interpreted as "consonantal", the second as "sonorant" and so on. Thus [æ] could be simply (-,+,+,-,+....), and the interpretation "the third segment is [+low]" would derive from a modular arithmetic computation based on the 57th value in an utterance being "+". A third possibility would be to have ordered value-attribute pairs i.e. (-cons,+son,+syl...): however, having both ordering and attributes is redundant – one can be supplied on the basis of the other, so there is no reason to use both (therefore, one doesn't -- it's either ordering, or value-attribute pairs). This is a classical application of Occam's Razor, that there is no evidence (given the assumption that features are value, attribute pairs) that the grammatical faculty also includes ordering of features. The foregoing assumes, however, that the set of features is universally fixed. That assumption was made in SPE phonology for the set of phonetic features, but it was not assumed for the remaining features. There is an unlimited supply of arbitrary non-phonetic features, including both letter diacritics (X,Y,D...) and rule features ([rule37]). This thwarts the modular arithmetic computation, because the 57th value might be a phonetic feature of the second segment in one language, or a rule feature of the first segment in another language, and so on. Given the conclusion that the set of features is not universally predetermined, then the ordering of the features also cannot be predetermined, so there can be no universal scheme for interpreting a list of bare values. In that case: the only alternative is that features are value, attribute pairs (and furthermore ordering of features would be redundant, since the attribute part provides the necessary information that allows a feature matrix to be physically interpreted).