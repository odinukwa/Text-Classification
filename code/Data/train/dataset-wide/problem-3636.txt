Check your Vagrant file and make sure you have setup the modules and manifests directory. Then search the puppet forge for the modules, for example apt, nodejs, ruby etc. Download them and install (unzip in the modules dir). Make sure to change the modules directory names. For example from to Take a look at the modules overview and usage guide it will tell you how to use a module. Also check out this tutorial about using vagrant with puppet. If you want to learn more about puppet take a look at the learning VM and the docs. 

I wouldn't worry at all about the small swap usage. It happens that kernel drops some data from memory to swap. You can control the behavior of kernel with the swappiness option: 

If you connect using an IP address then your certificate must contain a matching IP SAN to pass validation with Go 1.3 and higher. This is not (yet?) mentioned in any README file or documentation but there are some issues ( #226 , #221 ) open at the project github repo. To permit IP address as the server name, the SSL cert must include IP address as a field. To solve that you can use following procedure for the creation of the SSL cert and key: 

I need to know if current date/time is reliable in a small embedded Linux system Busybox-based. I have Busybox ntpd running, but apparently there is no program to to query status. On a more conventional Linux installation I would use ntpdc, ntpq, ntpstat or even timedatectl, but none of these is available on this Busybox/Buildroot system. What else can I do? 

... but I see no way to save the change and, unsurprisingly, if I reload the page order returns back to the original one. Also in "Issues" page, when sorting according to Priority, Redmine shows "Background" at top. What am I missing? What should I check? 

I am trying to convert an old infrastructure featuring several webservers into a virtualized environment with a single public IP address. All servers are relatively low-traffic, so performance isn't an issue. I currently have nginx installed directly on my firewall/bastion-host reverse proxying to a few servers (three, at the moment). I have everything working with plain HTTP. My current HTTP configuration is (simplified): 

Look at twill, which gives you a command-line interface. It doesn't support Javascript, but it does support cookies and forms. The Mozilla project has a more complex offering, XULrunner, which is supposed to support the whole XUL runtime, but I don't know how well this works in practice. My gut feeling is that the semantics of javascript are hard to model satisfactorily with a browser-in-the-middle. 

Machine: Xen-3.0 image running stable Debian Linux 2.6.18, pretty vanilla. My VPS provider asks me to deal with some trouble my image is causing, namely handling IP addresses it is not supposed to: 

I've used PointHQ: they don't really support being a secondary in their free version. Namecheap I didn't manage to use. ClouDNS don't let you set SOA records in their free version. So for this task I recommend Xname, which is painless if you are happy with BIND configuration. 

The good news is that running a web server shouldn't look much different, in my experience, after the move, except that you can change some things faster because you don't have to ask anyone. If you enjoy looking after boxes, you will find a VPS much more fun than a shared host. Try running an experimental VPS for a few weeks before you commit anything to it. 

You won't receive any mails, because the SMTP servers send mails between each other on port 25. ISP's block the outgoing port 25 to stop spam and this is now almost "an industry standard". Port 465 and 587 should be used for client - server connections only, so 99% of SMTP servers which will send mails to your server will do it on port 25. As you have changed the port from 25 to 465, they won't be able to connect to your server. Most probably you would need some kind of relay host which will forward mails to your server on different port. 

No, they won't be securely removed. If you overwrite the disk with zero's then it still will be possible to recover the data. If you do it with random data (/dev/random) once, it still will be possible to recover the data. You need to overwrite the disk with dd at least couple of times. Which will take exactly the same time (or even longer) as with . So to sum up, it always takes a lot of time, there is no quick method. 

My own experience is that very little comment spam is intelligent, in the sense of getting around filters, in the way that email spam is. 

DNS: you probably have to set up and maintain a nameserver; Mail: likewise a mailserver; Securing ports and syslogging: clients on shared host systems have to worry about only a small fraction of these issues. Ssh daemon and user authentication. 

seems like a common place to put clojure's binaries and libraries — why I don't know, it seems a natural for — so creating a subdirectory under this for these bash scripts seems fine. The general point is that it makes more sense to organise scripts by function, not have all bash scripts in the same place. 

If you do want a host physically resident in the EU, use a European VPS. Go for something based in Amsterdam: that's where the AMS-IX International Peer Connection point, which means it is pretty central for most of the European internet, has as good connectivity to the US as you'll get, and has a huge number of VPS providers. Cf. The ICANN IP hubs map. 

This control is used to define how aggressive the kernel will swap memory pages. Higher values will increase agressiveness, lower values decrease the amount of swap. A value of 0 instructs the kernel not to initiate swap until the amount of free and file-backed pages is less than the high water mark in a zone. The default value is 60. You will find more information about the virtual memory subsystem in the kernel documentation 

If you connect via host names, you can remove the IP SAN's, otherwise add your logstash server IP address. 

You can try to use to check which processes are using the network connections. List al network connections: List all the TCP or UDP connections: Processes listening on a particular port: List network files which are being used by a process: List the network files opened by the processes starting with ssh: 

As it is mentioned in the Apache Common Misconfigurations wiki site "...Because of the nature of SSL, host information isn't used when establishing an SSL connection. Apache will always use the certificate of the default virtual host, which is the first defined virtual host for name-based virtual hosts. While this doesn't mean that you won't ever be able to access the second virtual host, it does mean your users will always get a certificate mismatch warning when trying to access some.domain2.com..." And from the apache docs: "...The reason is that the SSL protocol is a separate layer which encapsulates the HTTP protocol. So the SSL session is a separate transaction, that takes place before the HTTP session has begun. The server receives an SSL request on IP address X and port Y (usually 443). Since the SSL request did not contain any Host: field, the server had no way to decide which SSL virtual host to use. Usually, it just used the first one it found which matched the port and IP address specified. ..." It is possible to have multiple SSL certs with one IP address with SNI (Server Name Indication) but only in the most recent versions of Apache and OpenSSL (with Apache v2.2.12 and OpenSSL v0.9.8j). In short: If you want to use different SSL certs for virtual hosts then you need to provide a different IP address for each of them or use SNI. 

I currently have an office network revolving essentially around three services, all hosted under Linux: 

In a brand new installation of Redmine 3.4.2.stable (under Centos7/Passenger/NginX, if that matters; I can give the whole setup, if requested) I added two more "Issue priorities": "Postponed" and "Background" which should be lower than "Low". This is how I see the page after creation ("Postponed" and "Background" have highest priority): 

It turns out this is a known bug fixed in v:3.4.3. The bug report is here and the fix is a trivial one-liner: 

but this does not work for lack of ssl_... stanzas. The "real" (proxyed) servers already have their certificates, but nginx seems to need "local" certificates (which I wouldn't like to provide). What is "best practice" in this usage case? NOTE: as said I need to deploy the reverse-proxy on my firewall (IPFire) so I'm rather limited in my choices; nginx and haproxy are supprted, sniproxy isn't. 

All these are actually self hosted into a set of managed VMs. Clients may be local, over the internet (for web services) or remoted via VPN. Clients are either windows (the majority) or linux. Currently all services have independent user/pass management and this, coupled with need to periodically change passwords is rapidly leading to a nightmare as user base enlarges (currently I have ~100 users). Is there some way to manage a Single Sign-on for these (few, bit different) services? Optimal would be to have a single Authentication Server where each user can manage a dingle password for all services and Administrator can assign (rather specific, at least as Samba and Git are concerned) privileges to users. Does such system exist? I am aware some very big services (e.g.: google) can provide authentication for other (mainly web) services, but I'm unsure if this can be scaled down to a single (relatively) Small Office. 

If you're familiar adminning desktop systems with the same OS as your VPS, then you'll be spared the biggest shock that comes with VPS. VPS is to shared hosting what (remote) adminning a system is to having a user account on that system. Things you have to worry about with VPS that you didn't have to with shared hosting: 

If you approve all messages that appear on your website, then don't use a captcha. There are comment filtering services out there that can analyse comments in a manner similar to mail spam filters (all links to the client API page, organised from simplest API to most complex): 

If the alternative is irregularly applying updates, you don't actively follow security updates, and you are running a vanilla stable Lenny, then auto-updating probably increase the security of your machine, since you will update known security holes faster. 

You can tell iptables to only allow each IP address to connect once to port 110. There are a couple of disadvantages to that: 

This will create a kind of wildcard certificate accepting any hostname and the IP addresses mentioned it that file. Of course this is just a simple example and you wil need to adjust the settings to your needs. 

You would get much better results using puppet modules for apt, nodejs etc. take a look at puppet forge. Using exec is a bad idea, as you really need to take care of the idempotency on your own. For example instead of lines: 

You can use testdisk to recover your partition table. TestDisk is a free and open source data recovery utility it can: 

You don't need to update all of them. Basically you need to know your HW, for example if you don't use a FC storage, you have no need to update the qlogic driver (most probably it's not even installed on your system). The other way around if you have an Intel network card then you need to upgrade the e1000 driver. You can check your HW from OS level with also will give you the information about modules loaded(used) by kernel. This should give you some hints about any drivers to update before hot fix installation. 

Your emails are going to get marked as spam regardless of how you send them, if the recipients mark them as spam: GMail, &c, will learn from this, and mark other emails from you as spam. Make sure that people want to receive the emails you send. What perfomance you need will depend pretty heavily on what the social networking software will need, as well as on the number of users and what they do. As a rule, say, PHP sites are less resource intensive than Ruby sites. 

You can tell which screen processes are linked to s by looking at the output of : if a screen client process is connected to the screen interaction processes, then they will share tty devices. So for instance, with: 

The trouble was, as I didn't find out for myself, the ethernet interface was bound to the forbidden IP addresses, although only the gateway address was shown using //. would have listed the missing addresses; using on the two misconfigured ip addresses fixed the problem. I should really learn more about Linux networking — this seems like it should have been obvious to me to check. I found Daniel Weiss' Proxy ARP with Linux useful.