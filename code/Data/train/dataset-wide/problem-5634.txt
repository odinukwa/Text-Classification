The style of the "New Philosopher" proposed by Nietzsche makes the meaning and relevance of the "scholarly" ambiguous. So, although there is a lot of scholarly work about Nietzsche, he himself would probably dismiss most of it. Kaufmann, the most popular translator of many of Neitzsche's works, indicates in his introduction to his translation of BGE that he purposely tries to keep Nietzsche's reasoning from sounding like philosophical argument, to forestall the illusion that the intention was to join in the scholarly tradition. Since the core of the issue is a call to action, what may be more profitable is to look into movements that have acted upon this call. I would point you at new religious movements like Crowley's or Witchcraft, and other attempts to contrive a postmodern lifestyle. 

I think what you may be after is modeled by the class of all ordinals, which does exist, in traditional mathematical logic, but cannot be a set. One definition of the ordinals is the model called L. L starts by defining the integers in terms of sets: 0 = {} 1 = {0} 2 = {0, 1} 3 = {0, 1, 2} ... omega is the union of 0, 1, 2, 3, etc. Then every whole number is in omega, and omega is infinite. But omega is only 'infinite' it is not 'infinity'. We can easily create the object which is "omega union {omega}", and that object is larger than omega itself, the first 'transfinite successor'. We can continue building from there, and get the class Omega (capital) of all 'ordinals'. Each of these is an infinity that is also limited. So there is no inherent conflict to that concept. You can prove that every potential model of inclusion is represented somewhere in that class. And any real network of sets is equivalent to a combination of ordinals. So 'L' is a model of all of set theory. The problem is that if Omega is a set, we could cram it into another set. That would allow us to construct "{Omega} union Omega". Unfortunately that would be another model of inclusion, not isomorphic to any of the ordinals. The way modern set theory avoids this is by declaring Omega not to be a set, only a 'proper' class of sets. But that is not losing anything from most naive notions of infinity, because one of the standard properties of a realized infinity is already that it would not fit inside anything else. You can do a very similar and only sightly more confusing construction with pairs of sets, if you want a broader sense of 'all the numbers', which includes models of everything most folks can consider a number. The result is the class of J. H. Conway's 'Hyperreals' as elaborated in Donald Knuth's "Surreal Numbers". 

Starting from Aristotle, science is a branch of philosophy, the branch that attempts to explain actual outcomes. 'The Physics' is as much a physics as any later one, but it failed. So it evolved and was replaced by things that worked better. It is common now to think of things like Alchemy as simply never having been science. But Newton took Alchemy quite seriously, and it did have actual productive discoveries. For us to retroactively redefine 'science' by looking backward from a place where we have centuries more experience with what does and does not work is intellectually dishonest. All of this stuff was science. But nothing has changed the fact that science relates to logic, uses learning, has human processes, etc. It needs an ontology, an epistemology, and to some degree even a prescriptive politics (with a moral obligation to honest disputation, a proper structure for peer review institutions, etc.). So all science takes place within a broader philosophy, and can only ever be part of a philosophical system. 

The value of formal instruction in critical thinking surely depends entirely on what type of person you already are. So having said one shouldn't generalize too much, I am simply going to do so, and let you filter my advice through your experience. Don't take my negativity as anything too much of an absolute. As someone who has studied and taught math between career changes in computing, I am highly skeptical that introductory-level courses generally meet the needs of people with real-life experience unless that instruction directly addresses problems met in real life from some remedial angle. I buy the Montessorian model of a skill as abstracted from applications in which one finds value. This applies to everyone, but especially to those not of 'school-age', including very young children, resistant teens, and full adults who have already done real work in the world. Unless there is some real barrier to getting on with the things you want to accomplish, the skills you need to accomplish them will arise naturally from use, and should not be programmatically taught. Formal modes of thinking critically are closely related to language, and most things linguistic are surely best learned by example, and not by rule. But the best examples are not available in a general course in logic, or in a course intended to inculcate habits of logic. They are found in real solutions to real problems. If you have trouble following philosophical arguments in general, or you are tripped up by the subtlety of the logic involved, it might pay to study Logic per se. But it might pay more to study formal linguistics, or to focus on the grammar of some very subtle language, or to study a highly self-critical mathematical discipline -- either math itself, or a modern science. Or, alas, it might pay most to simply trudge through the philosophy and discuss it until it coalesces into something you can 'get'. Introductory courses intended to teach a specific kind of abstract still, instead of a real applied discipline, strike me as an unwarranted form of remedial education. They are assuming you need 'a grounding' in something that is usually discovered naturally at the point of application. 

The Categorical Imperative requires maxims to be categorical, not hypothetical or contingent, and universal, not specific to you. Your example is neither, it is contingent upon a trait of yours -- what you like. So a lot of things can't be formulated as maxims, and a lot of maxims don't end up having a clear moral status. Kant sees this apparent weakness as a strength. It leaves more room open for autonomous behavior or cultural fine-tuning of mores to the environment. Even if the framing did not have this problem, your example does not work out. You cannot generalize 'Always buy a given thing because you like it.' Nor 'Never sell a given thing because you like it.' There are moral circumstances where the price required would deprive you of the resources to do something morally required or the resources freed by selling it would enable you to do something morally required. You would not buy it, and you may have to sell it if the alternative is not fulfilling an obligation like feeding your children. You need to have some underlying principle like 'Meet your established economic obligations with what resources are available." And that contradicts both halves of your example independently. 

I would claim that infinity and repetition have nothing to do with one another at a basic level, but that every human representation of infinity can only be made via repetition. We have only finitely many symbols to work with, and via them we can define only countably many things with clarity. We can define the integers, and give each of them a unique representation, and go from there to the rationals, and from there to all algebraic roots of rational polynomials, and from there to all closed integral forms with bounds among those algebraically identified numbers, etc. etc. etc. After a countably infinite number of layers, we could eventually have a representation for every real number, in theory. But if we really want to write down an allusion to infinity, it has to be in terms of a finite set from among this tower of more-and-more complex representations, and beyond that, if it is really going to be deterministic, it has to be captured in some finite algorithm that would write out the rest of the representations to which we are alluding. Therefore loops or recursion, and therefore repetition. When we think that the points of the real line all 'look the same', it is because the vast majority of them cannot have names that would focus our attention on them in a way that would make a difference. But this is an illusion forced on us by language. We cannot capture the detail in any meaningful way with finitely many symbols. (From a constructivist point of view, that means most of those points do not exist, and everything is finite with a single countable iteration represented as a process. Infinite constructs are useful for projecting concepts onto, but if you cannot construct the results, you have no object. From that point of view, all infinities are countable, and inaccessible. So in that frame of mind (which I manage at my best), your observation is correct. Still, being correct in reality and being correct in principle are not the same, this association is false in principle.) 

Insisting upon a human interpretation to put a result in context does not reduce the value of the result itself. Interpretations of statistics are still interpretations, after all. The objection is to considering the result as a meaningful object on its own, outside a context that integrates all of the social constructions that might have pushed the results in one direction or another. The supposedly objective method is not irrelevant, it is just no more relevant than other sources of ideas. All sources can be integrated into an overall explanation. Clinical psychologists of a basically interpretivist bent often use scored instruments, especially projective media, to derive a starting point for searching out ideas about their clients that might be hidden by the client during communication. They just do not accept that the instrument necessarily 'says' something definite. After all, the fact that someone gives answers similar to those given by people with a given way of interpreting the world can mean you are one of those people. But it can also mean that you have been trained or influenced to consider those kinds of answers more acceptable, or that you have an unusual way of taking the world it that produces the same patterns for some other reason. Various kinds of ascetics, for instance can come across as depressive if you look at how many hours a night one sleeps, what one's food intake is, how one moves through space, and other objective measures, and even opinions about the world and other people. An instrument will score them that way. But you can just look at them and see that a different frame of reference is in order. That does not mean that an objective measure might never tell you someone is depressed when they pretend otherwise in public because they have been trained to not get others down. It means that you have to actually think, despite the fact you have a supposedly objective instrument. 

The simplest way out, which is left out of @PaulRoss's broad answer, is Quine's approach to the notion of wellfoundedness: invoking 'weak stratification'. You take the opposite of Zorn's Lemma as an axiom, which forces you to implicitly order all self-references and you choose a rule for addressing them in sequence. 

Nietzsche is talking about how morality appears to arise in a succession of changes, each base upon the previous one. He wants to emphasize the organic nature of the succession of moral theories. He argues that slave morality is formed in reaction to master morality. It can only be understood as a 'second generation' rebelling against its origins. And ultimately he does not seek an underlying reason for the original master morality. So he is not going all the way back to origins, or even trying to. To that extent, to use such a label would be misleading. 

The Lowenheim-Skolem theorem says that any axiomatization that has any infinite model at all has at least one model of every potential cardinality. So if your notion of a possible world includes the integers, and can be described in a finite language, there are more than any given infinite number of possible universes obeying its rules. Because if all of them were of a given cardinality or below, we could haul out the mechanism of Lowenheim-Skolem, and make one of the next cardinality up. To see this another way, what could differ? Obviously, the value of some untracked constant could be different, and could take any value in a range of real numbers, so there are at least uncountably many variations. Now think about functions from the reals onto the reals. There are 2^c of those, and clearly some irrelevant one of those could vary any way it wants, and we would not notice. How about transformations of those functions into one-another -- there are 2^2^c of those...