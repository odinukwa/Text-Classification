However, I've heard that it is good practice to modify the bins such that the observed counts are above a threshold (typically 5, sometimes less) as a large number of bin counts below such a threshold can result in a bad fit (assuming minimized chi-squared). If the observed bin count is less than this threshold, then the bin is merged with the next bin. Assuming a distribution (such as a Gaussian) with a central peak, the next bin would be the next-right bin (i to i+1 bin) when the bins are left of the central peak while the next bin would be the next-left bin (i to i-1 bin) when the bins are to the right of the central peak. I've created an algorithm that I believe works and covers all-edge cases (assuming a single central peak). I was wondering how it could be improved in terms of speed/efficiency. I also feel like I am duplicating code using similar approaches in two while-loops; can this be averted? 

Since I could not get to compute a derivative successfully, I wrote a script to compute it manually. Running the script below will output a plot of two functions and over the interval . 

First, in Java 7, I'd abstract the parser component and let it just parse and do nothing else. Don't use the class to the parser as this is not a necessary thing here. You can easily use just an you could obtain from anywhere, not necessarily an input stream from assets (this simplifies unit testing very much). 

The method names are self-descriptive, and you can implement the methods in any way you want (the simplest case: just log the s and s). Note that I intentionally pass raw values as this doesn't let the interface bloat, and you can implement/delegate a value parser in a handler implementation. Third, then, as one of many other implementations, we can define a simple aggregating handler. 

This implementation renders the formatted strings and table borders as whole strings. Probably rendering the each cell separately might be a better or more optimal idea, but I don't know how it would affect the character then. I don't know how fine is the implemented spliterator, especially its method. I'm not 100% sure if dynamic expanding the array is a good idea. For example, I would assume that the collector must only expect well-formed data, and detecting jagged rows is basically out of the scope of the collector (simply speaking, it probably should always expect for n x m matrices). Well, a collector that produces a stream. 

as you are creating as a copy of ; any change to changes as well. Also, you can iterate through your unique values in instead of checking if each element is in . Sorry I could not be of more help. Let me know if anything is unclear. 

Can the speed and/or accuracy of this algorithm be improved? Is it proper to use centered differences at interior points and one-sided differences at the boundaries? 

The for-loop has the advantage that you can later change the number of output plots without changing too much of your original code (if say, you wanted to output 2 or 4 plots, or perhaps change the to something entirely different). This may or may not be relevant, but it may be nice to associate each individual plot with a specific legend label, marker style, and/or color, all of which can be handled using . This can be especially useful for overlayed plots (especially considering that your data consists of and over the same x-interval. 

Since the sum changes with each iteration through your dictionary values of , I don't know of a way to pre-compute the right-side sums before-hand. This would be a little easier if you could use external modules. That said, you might see a slight speed-up if you use more comprehensions. For example, you can get via . Also, use (not ) to check zero equality and use (not ) to check . Also, you are iterating through all possible combinations. In the case of using and , you know that and . So you know that your lower bound is cut-off at 76. Those are 76 permutations that you do not need to use. You can include a statement at the lower bound since you are incrementing downward (), but I would instead use this lower bound as the starting point and iterate by incrementing upward as you may find some other way to restrict the sums in the upper limit. Lastly, the use of globals and non-descriptive variable names makes it hard to edit the code. For example, instead of , instead of , etc. Also, you can pass into . I personally prefer iterating over lists instead of dictionaries since you can use zip. I don't have a full solution for your problem, but it may help as a start. 

Other code simplifications Now that we provide the full snake to the function, it doesn't really need to and values. 

More advices If you are learning Python, it would be a good idea to give Python 3 a try. There is not much to change in your case but the earlier you take the good habits, the easier it will be. Regarding your length comparison, you could try to use izip_longest to handle scenarios with strings of different lengths. 

Good comments have been posted already but no one explicitly pointed out that the doesnt bring anything in the function and : 

You could use the Boolean type for the variable. You could also get rid of it altogether by using break (this is a bit personal, some like it with a variable, I'd rather see avoid it). 

Not a proper code review as per se but I was wondering if things couldn't be done in a more straight-forward way like this : 

Now, because of the way American cities are named, it might be worth counting the cities keeping their state into account (because Portand and Portland are not quite the same city). Thus, it might be worth storing information about city and state as a tuple. This is how I'd do it : 

I am trying to write an algorithm that can find user-specified nearest neighbors. By user-specified, I mean that the user can specify whether it's a general nearest neighbor, a forward-nearest neighbor, or a backward-nearest neighbor. The idea for this code was inspired from this SO post. While it isn't ideal to search the entire array (perhaps use searchsorted as an alternative), I want to find all occurrences of the user-specified nearest value in the given data array. While there are other techniques that can be used to achieve the same goal (such as using the cumulative sum of differences of argsorted values), I feel the code below is easier to read/understand and is likely quicker since it performs less operations that require traversing the entire data array. That said, I would like to know if there are better approaches (in terms of speed) to achieve the same output, as this code will be applied to a dataset of at least ~70,000 data points. More than the value itself, I am concerned with the indices at which the values occur. 

About six years ago I implemented a simple tabular pretty-print Java class that mostly simulated MySQL CLI query result tables. I don't really like it because it was implemented in a, I believe, pretty dirty manner being not extensible, and I would assume that the same thing might be implemented using Java 8 Stream API that might be a bit more generic. The following implementation is just a matter of reimplementation interest and I'm trying to learn writing some s. The source code: 

Note that decorator (basically a result can be immutable, as it's just a snapshot of a given XML) "protects" the aggregation result directly from elsewhere, however in this case it can be modified using the handler methods, but you can make rework it to build a result object with defence-copied maps so no handler interaction could change the result maps. I'd recommend the class and the strategy method to be fully extracted elsewhere. Note that the implementation above can be tested, in principle, using unit tests during builds (except of that would throw a stub exception, but this could be worked around using the Strategy pattern). 

This can be rewritten using a helper function (or alternatively a for-loop), which may look something like this: 

I have written a script that I believe works and covers all edge-cases. I am curious about ways to improve upon speed. While the given example below covers a multi-dimensional array of , my actual use case will be (where depends upon the number of data parameters being searched). Given individual arrays of data points, the goal is to combine them into a multi-dimensional array and find the columns in which all conditions are satisfied. If the same column of each row satisfies a given condition, the index that corresponds to that column is output; otherwise, an error is raised. I have included a small named because it has many other functions relevant in my main code, though I've only included the parts relevant to the goal in this question. 

One alternative method I have yet to explore is using set intersection/unions to find the same indices, though I'm not sure if that would necessarily improve performance. I posted a similar example some time ago, though I later realized the code had bugs and could have been improved upon as an example.