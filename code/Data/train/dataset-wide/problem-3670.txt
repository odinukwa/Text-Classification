"OpenSSH 7.0 and greater similarly disable the ssh-dss (DSA) public key algorithm. It too is weak and we recommend against its use." Source : $URL$ 

It seems that everything is okay but your telnet command tries to open port 23 and not 10000. ( "connecting to ...could not open connection to the host, on port 23: connect failed") try 

We have to find a string, let's say "foobar" among several webapps. However, some webapps contain zipped files, eg log4j.jar. Therefore, grep -IR "foobar" /pathto/tomcatroot/ won't work, because of compressed files. unzip -c /pathto/tomcatroot/libdir/log4j.jar |grep foobar can solve the problem, but only for one file Is there a way to achieve this for a whole directory? 

apt-get install libssl-dev You must install openssl development files in order to compile php $URL$ Also, once you compile php successfully, don't forget to change apache module to the right directory for php.so There must be a line looking line LoadModule php7_module modules/libphp7.so in apache configuration 

The problem here is the is part of the modern and Centos 6.4 doesn't seem to have it. I've tried building my own but as soon as it gets installed and in my local I can't run any other programs, because all of the existing executables on the system try to link against it and they fail. I want to use the new compiler because it has dramatically better handling of C++ STL code, and because the optimizer in GCC 4.8 makes my code run in 1/2 the time as the GCC 4.4.7 compiler that comes with Centos. Any suggestions on how to do this? 

RAID5 and RAID6 can detect and usually correct bit corruption if you verify parity of the entire drive. This is called "scrubbing" or "parity checking" and typically takes 24-48 hours on most production RAID system. During that time performance may be significantly degraded. (Some systems allow the operator to prioritize scrubbing over read/write access or below it.) RAID6 has a higher chance of correcting it, because it can correct it if you have two drive failures, whereas RAID5 can only handle 1 drive failure, and drive failures are more likely when you are scrubbing because of the increased activity. 

Your issue was solved by rebooting the box. Therefore, we won't be able to determine the root cause of the problem. 

If i understand your question correctly, you want to receive all mail for test@example.com, but none of the mails sent to joe@example.com 

According to your firewall logs, you've forgotten to allow dns queries. Allow port 53 udp and your setup will work better ! 

Depending on your monitoring system, you should be able to write a script that parses the output of ifconfig and displays an alert if the number of errors of dropped packets is too high. 

"" Is there a way I can block traffic from china?"" Yes, for example, you can use iptables to blacklist these ip ranges : $URL$ (i just googled china ip block list) 

You can try to type mount -a just after system has booted. If this workaround works, you can set up a script whose content is just something like "sleep 60 && mount -a" and make it executed at boot time (via cron, systemctl or any other mean). It's really dirty, the good solution would be to investigate why some fs does not mount correctly. 

We are using Lustre in a cluster with approximately 200TB of storage, 12 Object Storage Targets (that connect to a DDN storage system using QDR Infiniband), and roughly 160 quad and 8-core compute notes. Most of the users of this system have no problems at all, but my tasks are I/O intensive. When I run an array job that has 250-500 processes that are simultaneously pounding the file system typically between 10 and 20 of my processes will fail. The log files indicate that the load on the OSTs are going over 2 and that the Lustre client is returning either bad data or failed function calls. Currently the only way we have of resolving my problem is to run fewer simultaneous jobs. This is unsatisfactory, because there is no way to know in advance if my workload will be CPU-heavy or I/O heavy. Besides, just turning down the load isn't the way to run a supercomptuer: we would like it to run slower when running under load, not produce incorrect answers. I'd like to know how to configure Lustre so that clients block when the load on the OSTs goes too high, rather than having the clients get bad data. How do I configure Lustre to make the clients block? 

Which user is running mysqld? It is likely that the user who runs mysqld, usually mysql, does not have write access on /home/site/public_html/mysql-logs/ Therefore, it cannot write log files. Try creating a group for both /home/site/public_html owner and mysql, then chmod 0775 directories and chown directories 

sender_canonical_maps (default: empty) Optional address mapping lookup tables for envelope and header sender addresses. The table format and lookups are documented in canonical(5). Example: you want to rewrite the SENDER address "user@ugly.domain" to "user@pretty.domain", while still being able to send mail to the RECIPIENT address "user@ugly.domain". Note: $sender_canonical_maps is processed before $canonical_maps. Example: sender_canonical_maps = hash:/etc/postfix/sender_canonical Source : $URL$ 

You can use tcptraceroute to guess where port 443 is blocked. tcptraceroute 52.56.123.246 443 should give you the ip adress of the firewall which is blocking requests to port 443 

It looks like a dns issue. To figure out, try in cmd.exe : ping myhostname It should not work. Then you have two option : 

I need to use a port that is already defined becasue I am upstream from a middlebox that only allows connections on ports allocated to specified services, and port 22 is blocked. 

We would like to have a network backup system with a user that can read any file on our servers but not write any file. Is there any way to do this under Linux (and specifically Fedora)? We would rather not have a remote that can erase any file... 

It appears that Hive, Impala, Pig, and others all provide SQL or SQL-like access to data stored on Hadoop clusters. They all seem to have support for HDFS, S3, and other forms. So why are there so many different ways for accessing Hadoop information by SQL, how are they different, and how does their performance compare? Do we have so many different versions because all of the projects were started at the same time for more or less the same reason? If so, is there an advantage to knowing more than one of them? I have found several articles that attempt to explain the differences (e.g. 10 ways to query hadoop with SQL and Selecting the right SQL on Hadoop, but mostly they just list features. 

"Only option left after reading quite a number of forum is to make the ssh key same on all the server. " I think there is another option : you can use ssh certificates. 

What is the best way to set up automatic backup for switch FC configuration? I have searched the web and came across $URL$ , but it looks like it runs on windows and hasn't been updated since 2009. We have multiple brocade switch FC and would like to be able to save their configurations. The firmware version is often different. Even sometimes we have other vendors. Any help appreciated, thanks in advance. 

" node uberlamp3 advertised itself as uberlamp.14.by instead." What happens if you type "hostname" on the client? Have you checked munin client configuration? 

I have no idea how it works with storcli64, but I just had to replace a raid1 failed drive (slot 0) with megacli64 : 

What are the network protocols that can be used to measure the system clock (time) of a remote server? So far I have: 

So this makes no sense to me. This means that I have 64 CPUs with 16 cores each, or a total of 1024 cores. However, online documentation for the Intel E5-2686 v6 claims that it has 36 cores and hyperthreading, for 72 virtual cores. What's going on? How many cores are there? 

RHEL 7 includes in the official repository. However, this module does not install mod_php. I have tried all of the approaches for installing rh-php70-* and none of them installs and registers the appropriate php module. How does one do it? 

Is there a way to read the certificate in a consumer credit card EMV chip and use this to authenticate to an Apache web server using the client-side certificate support in SSL/TLS? 

"what happens when server #1 is under such heavy load that he cannot handle the requests properly" Usually clients get "connection timed out" error message. Don't worry about the load balancer : with modern hardware, they can handle thousands of requests, especially if you use cache for static content. Most of the time, the bottlenecks are : 

"I think there should be one if there's a local one in ~/.inputrc" According to documentation, this assumption is wrong. You can have ~/.inputrc for some users without the file /etc/inputrc. Even worse, /etc/inputrc is read only if there is no .inputrc for a user. Therefore, if you want to use global settings for all bash users, you have to : 

027 is better from security perspective. Even better is to use 077 for root. With 027 mask, when root creates a file, it can be read and executed by users who belongs to the group of the file. With 077 mask, only root can read write and execute files. Why is 077 better?