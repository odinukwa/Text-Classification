However, this mechanism is for "tweaking" the existing rewrite-rule framework but not for making completely arbitrary changes to rewrite-rules. Rules using LOCAL_NET_CONFIG will always be inserted at the same place: halfway through ruleset 0. And they can't be wildly different from what was there before such that delivery no longer approximately matches the assumptions made by the existing "parse" functionality. New rulesets (subroutines) from LOCAL_RULESETS will either be called only by your inserted rules, or be called directly by the sendmail program itself depending on specific (and possibly obscure) subroutine names and sendmail.mc FEATURE specifications. And extensions to existing rulesets (subroutines) from LOCAL_RULESET can add new functionality, but probably cannot change existing functionality, as a match and "return" by an existing earlier rule will terminate execution of that ruleset before your additional rules are even reached. Nevertheless, this may be adequate for what you want. If you do this, use the test mechanism to make sure it's behaving the way you intend. Remember, your "style" should be to compose your new rules in such a way that they fit seamlessly into the existing ruleset framework (rather than making arbitrary changes with little consideration for the existing framework); it's rather like adding a new feature to existing code that was structured by somebody else. The distributed rewrite rules are very good at handling not only mainline behavior but also edge cases (MX for individual hosts? Masquerade exceptions? UUCP connectivity? aliases? etc.?); hopefully your added rules will be similarly comprehensive. 

Is it possible to use .htaccess file to rewrite this: $URL$ into this: $URL$ If it is then how? EDIT: The goal is to avoid any updates to .htaccess when a new customer is added EDIT2: I see that my question has been too vague. I have a CMS that is being used for lots of different customers (www.foo.com, www.bar.com, www.somerandomname.com). Each customer has it's unique files (design, uploads) in a directory /data/www/cms/customers/XXXXXX/ where XXXXXX is the name of the customer. Apache config file has default DocumentRoot set on /data/www/cms/. What I'm trying to do is to have the URL $URL$ to return a file from the given customers data directory, i.e. /data/www/cms/app/customers/foo.com/assets/css/bg.png. 

The "Why?" part: According to $URL$ the maximum LUN size for VMFS is 2TB. According to $URL$ what ESX will do if the LUN size is bigger is it makes a 740GB partition and the rest is inaccessible. The "How to fix it" part: Present a smaller than 2TB LUN to ESX. That can be done by creating partitions on the disk (/dev/sdb1 /dev/sdb2) and exporting those partitions as separate LUNs to ESX. 

I have several boxen running Debian Wheezy. They appear to have the highly annoying problem that pressing ctrl+c in a shell that has been su:d to root will kill su, not whatever is running in the root shell. This makes working with e.g. ping or tcpdump near impossible. As I understand it, the issue is debated at length in Debian bug #628843 but there seems to be no consensus. 

For development work: If your shell is bash (echo $SHELL -> /bin/bash) you may want to add a JAVA_HOME entry in /home/<user>/.bashrc. However, note that if you only work with one Java version, you should install the package and no explicit JAVA_HOME setting should be necessary for most scenarios. Also, it is sometimes convenient to do something like this: 

I tried to add 0:01:0 as a spare to the set using but this seems to have forced the volume into readonly mode and blocked any attempts to use afacli on the controller. After waiting 10 mins or so, I rebooted. State remained as above. So no my question is: how do I convince the missing drive to re-join the mirror set? If it helps: 

We're trying to access Foxpro file-based database files via MSSQL Server's linked server feature, using Foxpro ODBC driver. For this, we've 

I have googled high and low but there doesn't seem to be any example doing raid10 with megaraid (only the syntax). Can anyone explain what is wrong? 

I have a 2.7 TB virtual disk (LSI MegaRAID controller with ten 600GB SAS drives configured in RAID10) under Linux. I am sharing this disk to a remote ESX host via ISCSI. Unfortunately ESX will only make a 740GB VMFS partition if you present it LUN greater than 2TB. I could make a 6 disk RAID10 (which would be smaller than 2TB) but I really don't want to lose spindles (IOPS). Is there a way to split this big RAID10 virtual disk up (for ESX) in Linux? 

I have a Strongswan IKEv2 server and I can connect to it from Windows 10 using built in VPN client but I cannot ping the subnet behind the vpn server. It only works when I manually add a route to the subnet with route add 192.168.12.0 mask 255.255.255.0 10.100.0.1. Basically the same issue that strongSwan server with Windows 7 clients doesn't route traffic. Is it possible to automate it from the server side (i.e. I don't have to create a bat file on every client desktop to add the route)? 

The usual mantra: "what's changed?" By any chance have you fiddled with /etc/resolv.conf, maybe trying to tighten down the timeout a little? (Or is it possible the machine that's running BIND is quite a bit more heavily loaded and significantly slower than it was at first?) Only a network trace (wireshark?) would tell for sure, but it looks to me like the first request for isn't being returned fast enough, so the resolver is timing out and then trying (append "domain" to whatever ...even if it's a silly duplication). That second query probably never did work right even before (possibly because of the missing trailing dot mentioned in another response), but it didn't matter during your initial testing because the query never got sent anyway. Now the query is being sent, and tickling the incorrect behavior that was latent in your BIND configuration all along. Try turning the in /etc/resolv.conf way up and see if it stops happening. (Or turn the option way down so it tickles the problem all the time, then fix the root problem, then turn the option back up to a reasonable value.) 

For unclear reasons, this problem no longer persists. It may be that it disappeared on Ubuntu upgrade. 

However, it is not obvious to me from above data that there is much wrong with your setup. While you can probably wring a bit more performance out of the box, you may simply be approaching the limit. Update: Clarification re: comment below. A typical network-oriented TCP server consists of a daemon that has a listening socket and a number of open connections to clients. Each of these sockets has a process waiting on it (one process may wait on numerous sockets). Those processes will be in sleeping state and will be woken up by the OS when some data arrives. If it is efficient (say static web server) you may never catch it running, as it takes only some 100 microseconds to wake up, serve some data and go back to sleep. Update 2: A modern OS allocates free memory to new disk buffers until it runs out of memory and then reuses the least used buffers. Thus, memory will always be full. Furthermore, there are several ways in which two processes may report the same page of memory as part of its size. The upshot of this is that a) a modern OS is always out of memory, and b) it is difficult to tell exactly how memory is used. The best simple indication is to strive for buffer and cached numbers as a large fraction of physical memory. On this box more than 30% of memory is in cached disk data. 

This will mount the CD disc, make it the only package repository and install a kernel that has support for more than just a handful of vmware/xen/etc virtual devices. After you get it working, restore /etc/apt/sources.list file from the backup and update aptitude again. 

When you close VMware Player, you have two options: 1. Shutting down. 2. Suspending. Shutting down the virtual machine is like powering off your computer. Suspending actually creates a snapshot of your VMâ€™s state at that point, which you can restart from it later on. VMware Player only supports one snapshot per VM. However, if you need multiple snapshots, you can consider buying VMware workstation version. Source: $URL$ But you could make a copy of the virtual machine folder while it's suspended and name the folder MyVM_Snapshot1 etc. 

But the last step never completes (Executing is displayed forever). When Management Studio is forcibly closed and restarted the new linked server is there but only contains Catalogues subitem. If we try to expand it, Management Studio goes into loop yet again. 

The most basic bit is that you need the rpmbuild tool and you need to write a spec file. I have the following script that takes a spec file and a tree as it would look installed on the target machine: 

I'm somewhat confused. Do you want to give users the ability to publish web from their home directory? If so, see the userdir module: $URL$ If you want to just have a VirtualHost pointing to /home/X/public_html, you write a VirtualHost section. Having said that, the error message "Permission denied: access to / denied" seems fishy. Unless I am much mistaken, / here is a directory and not a location, meaning your web server can't read the filesystem root dir, which would indicate something more basic is broken/misconfigured. 

I can now answer one of the questions: it appears that the CERC has no problem with a 1TB SATA disk as long as it is in "Volume" mode. I added a 1TB SATA disk to the controller and initialized it as a "Volume". The disk appears to be fully readable/writable and when moved to a different controller in a different machine, could still be read. UPDATE: In the end, I gave up on the STARDOM JBOD chassis. I never managed to got the CERC to recognize the disks through ESATA connections.