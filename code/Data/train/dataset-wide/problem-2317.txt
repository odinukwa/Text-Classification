I'd go with Justin Cave's suggestion and use a separate sequence, but if you really must use the same sequence, then you can change the INCREMENT value to two and then simply add one in your insert like this: 

I don't know if you can do something like that in MYSQL, so here is a version using a GROUP BY and self join that works in MySQL 5.5.28. 

If even an does not remove the session after a bit, then the only thing you are left with is to kill it on the OS level. In Linux you can get the command to kill the process as follows: 

By doing the following I can determine if an index will benefit from compression and how many columns should be included in the compression: 

The same issue seems to be effecting the following, which returns the wrong data even though the number of rows is the same. 

If the case option isn't flexible enough you could bulk collect the data in your procedure and manipulate the array(s). 

=== Update 4/9/2013 === I have an anonymous block that can successfully validate XML based on a schema. This again points to a permissions issue that allows this to work when roles are enabled, but not when they are not available. == Update 4/12/2013 === Every bit of evidence I get seems to indicate that there is something wrong with my ACL or perhaps more likely with the way I set the ACL. Taking the XDBADMIN role away from the user causes the anonymous block to fail with an access denied error even though I have granted all the permissions the role gives according to dba_tab_privs. My setACL follows this form: 

A subsequent call from the same session re-initializes the package state and therefore runs successfully. The interesting thing is that this behavior seems to change when the caller catches the exception and does a raise_application_error. As expected the raise_application_error is included in the stack, but contrary to expectations, the ORA-04068 is not. Here is an example error stack: 

You will need to evaluate this option with respect to your existing infrastructure and requirements. 

It looks like you're trying to find rows that have one more, one less, or the same number of rows as the the highest number of rows for a particular yilid where the id is equal to the balik. Since there will never be more rows than the max, can be eliminated. The other two conditions can be combined to . It looks like kas_cari may just be a view of cari, but I can't tell from your information what it may be filtering/transforming. If they are functionally equivalent you could eliminate the double querying of cari. Even if they aren't it would be useful to merge the view into this query to see if any redundancy there can be eliminated. Finally, the explain plan isn't as useful as it might be due to the parallelism kicking in. Turn off the parallelism and you will see a much simpler version of the explain plan that will be easier to tune. After that turn the parallelism back on. 

I agree with ik_zelf(+1). You will probably want to do an Exchange Partition, therefore you should read the SQL Language Reference information about it and the rules it must follow. The purpose of DBMS_Redefinition is to make changes online rather than offline. You should read the Administrator's Guide to gain an understanding of the situations in which it can help you. 

The diversity of a column's data is known as selectivity. Selectivity is useful to know when determining whether an index will be useful, but it is not the only thing that determines the speed benefit. Other factors include how fast the storage the index is on compared to the table, how much of the table/index is already cached, how large the index is in comparison to the table, and several other things. Knowing the data-type of the column does not necessarily help us determine how selective an index on the column will be. Even a column constrained to two values might use those values for only a few rows and have the remainder NULL. On the other hand, a column that could have many distinct values could have the same value in every single row. Even with your column where all the rows would have unique values, if you are searching for rows with an >= 10, the index probably wouldn't be useful even though it is highly selective. You can't use selectivity alone to determine whether an index will be useful or not because even if it returns 100% of the rows, if the index includes all the data necessary for the query it will be faster than using the table. On the other hand, for a small table it may be faster to query the whole thing even if the row being sought only represents 1% of the total. Determining what indexes should be created is less about looking at the table structure than it is looking at the important queries and what data they need to retrieve. 

You were getting close. WIDTH_BUCKET does not allow a subset of the ranges to be specified, you must specify the start and end of the whole range and how many buckets you want that range divided into. In the case of 1-100, 101-200, 201-300, 301-400, & 401-500 your start and end are 1 and 500 and this should be divided into five buckets. This can be done as follows: 

With a logical drive striped across an unknown number of disks in a SAN disk group containing 30 drives, I'm not sure what this means. I ran the procedure for values of 1 and 30, yet the IOPS, MBPS and latency were nearly identical. Is this number just for informational purposes or is it actually used? If the the later, then what should it be set to when there is not a one to one relationship between logical disks and physical disks? Block to run procedure: 

To say that an instance mounts a database means nothing more than the control files have been opened and read. At this point in memory it has knowledge of the database, but the datafiles, redo logs, and therefore the database are not open. The Oracle Concepts guide has already been cited, but here is the 11.2 version of the same. $URL$ 

Another option with it's own limiations would be to use the negative of the sequence value for the second entry. 

ANSI is a private non-profit organization that creates voluntary standards. As such it doesn’t actually regulate anything. Often it is to a company’s benefit to follow recognized standards, which is why many database companies follow the ANSI standard for SQL. Of course as each company seeks to differentiate their products, they will develop additional functionality beyond the standards. From w3schools: 

These nodes are connecting to different instances of the same database. I have done a on both and verified that the sqlplus settings are identical. 

As Niall said this data is not available. It would be crude, but you could roll your own solution for this. By periodically querying sys.file$ for size changes, you could determine that one or more auto-extend events have occurred and then based on the size change and the auto-increment value determine how many extensions have taken place since the last check. The more often you check the more closely you could identify the times. Inserting the results of every second (processed separately) would give you about as clear a picture as possible. While this does technically answer your question, the advantage of having this information would not be worth the overhead. I highly recommend NOT doing this. 

Then you can connect as using the password and it will behave as though you are logged in as for schema references, permissions, and almost everything except database links. 

There seems to be an issue with the mixture of ANSI and Oracle style joins as well as the referencing of a cross joined table in a subquery. Converting/removing these constructs, you should end up with something like this: 

The documentation also includes notes on four places that comments should not be used, but these do not include any further differences. 

If the database is Oracle you might want to consider an indexed virtual column that will limit the data to entries needed for particular queries. For example, you could have a virtual column called AcceptedFromUserId that uses the function DECODE(StatusId,1,FromUserId,NULL). The index would only contain AcceptedUserIds and would therefore be smaller than an index for all UserIds. If you regularly clean out rejected requests an indexed virtual column on PendingToUserId might be more useful. An alternative if you had partitioning would be to partition the table on the StatusId. If you don't need multiple friend requests between the same users at the same time you could abandon the FriendId using FromUserId, ToUserId, and StatusId as your primary key. In this case you should also consider making the table an Index Organized Table. 

In a perfect world all tuning would be done in the design phase proactively and nothing would be reactive, but the world isn't perfect. You will find that test data sometimes isn't representative, test cases will have been missed, loads will be unexpectedly different, and there will be bugs that cause performance issues. These situations may require some reactive tuning, but that doesn't mean reactive tuning is preferred. The goal should always be to catch these up front. Your planning for retroactive tuning is very pragmatic. When you are testing you should document expected timings and throughput and at times should actually build in analysis that lets you know when production processes are not meeting design specifications. In this way you may be able to identify in advance what code needs to be tuned. You can then determine not just what the problem is, but why you didn't catch it in the design/test phase. 

You might want to verify this in SQLPlus. If it still doesn't disconnect you then verify your assumptions by running the following after the trigger finishes: 

You could do both a union of the tables and an intersection of the tables and then get the difference between these. 

SQL Fiddle's showing Option #2 and Option #3 with the additional requirements Joel Brown added and how various questions would have to be answered in SQL. - $URL$ - $URL$ 

See the Oracle Database Backup and Recovery Users Guide. In particular the section titled Recovering After Losing of All Members of an Online Redo Log Group. Your course of action will depend on other factors you can read about in the documentation. 

For your broader question you can look into tracing, auditing, flashback database, flashback query, logminer, materialized views, streams, and change data capture (not necessarily in that order). We would need more details about what data you need and why before directing you any more specifically. 

If you need the timing for one execution you could set timing on in an SQL*Plus session, turn on tracing with TIMED_STATISTICS, or add a comment to the SQL so that there is only one execution in v$sql. 

It seems that something went wrong on the step. Since this is a new install, I would follow the instructions for uninstalling and then the instructions for installing. 

To count the rows deleted just after the delete statement use SQL%ROWCOUNT. Here is a demonstration. 

A distributed query cannot be optimized in the same way that local tables can, however, distributed queries are optimized and caches, therefore the vendor is incorrect. You can see this by running something like the following. You will see the execution count rise with each run. 

The short answer is No. Unfortunately the way to do this in Oracle does require the "big stack of boring queries". The articles you linked to are some of the best information available on the subject. The datafile does indeed become fragmented, so that even if free space exists below the highest segment, Oracle will not automatically consolidate it when a is done. To "defragment" the tablespace you need to move these segments to the start of the datafile rather than at the end. For tables this is an offline process meaning the table will be unavailable while the move is taking place. Indexes can be moved either offline or with Enterprise Edition they can be moved online. Since you have an outage window I recommend you follow these steps. A. Shrink datafiles with free space beyond the high water mark. This can be done as follows (the query is similar to Frosty Z's procedure): 

Yes, you can do an online backup without archivelogs if you have a storage system or backup tool that can take a point in time snapshot that will be consistent across all your datafiles and control files. It sounds like you have such a tool and are using it for cold backups. If that is the case, your documentation will still need to change. If there is a problem with the system and you need to restore the backup, starting up the restored database will be as though the power plug was pulled a the time of the backup, therefore crash recovery will likely be necessary. Given that your documentation needs to change anyway and that you want zero downtime backups, you should probably re-think your resistance to archive logs. It limits your recovery possibilities greatly. For example, if due to a disk error you get a bad block in one of your datafiles, and you have older consistent copies of that datafile, you can recover it using archive logs. With snapshots taken online you would probably end up doing lengthy extracts from multiple snapshots of the system (assuming you can tolerate the downtime) and would probably still loose data.