There are few simple solutions. Snapshot and restore Take a Snapshot of the old RDS and create your new RDS from this snapshot. $URL$ $URL$ Mydumper Use mydumper to backup and myloader for restore the db. This will be faster than native mysqldump. $URL$ Tune the parameter For Faster restore use this parameter. MySQL any way to import a huge (32 GB) sql dump faster? 

The table has 10000000+ records, so I want to update first 1000 rows, once it is done then update 1001 to 2000 rows. I need to make it as a loop. 

You have to create the index in the below manner. This query should check this first. This covers the 

Im using mariadb 10.0.29, I need to change the innodb file size to 256M, default size 50M, My config file is located on 

You already know the answer for this, it is FAST. Here are very basic and known benefits of using SSD. 

Instead of using single transaction and flush logs, you can pass This will help you to get the correct table contents and . Once the dump is completed you can see the bin log file name and its position inside the dump. 

Restore collections with different pattern There is no straight forward method for this, but we can create a loop to restore this. create a file with list of collections that we need to restore 

Then created a new ndf in the same file group called sec_file.ndf and created few tables, inserted 1000 rows on each table. After an hour took a file backup of my MDF and NDF files 

While installing this cluster, its asking to remove the existing How can I install this without remove anything? 

So you can create a view to converting your datetime column to the specific date format. Create table with Datetime 

PG_pool1 and PG_pool2 should send all the connections to Master Server, If master server goes down then the PG_pool1 will promote Slave as master and the pg_pool1 and PG_pool2 will send the traffic to Slave(now its promoted as master) Is this possible one? Or any other things to archive this(I tried Pacemaker, DRBD, But this could be a simplest one) 

As dezso said, its easy to change the data directory. But I did a small PoC for this. So its possible to use existing physical files to another Postgresql. Limitations 

Try to use SSIS which is the simplest method. Or generate the script for each table in a separate .sql file. then create a batch file to execute one by one. 

Now my question is I have enabled the trigger after restored the DB. It took 4 hrs. During the Restore, there were so many rows get inserted/Updated/Deleted. So is my target DB how will get these changes? Because I have enabled the trigger after the restoration. 

As of now no issues, tested the replicatin. Then I did a restart both nodes. After that mysql won't start. Server1: 

Here somehow my sec_file get deleted so I want to restore latest NDF backup, how can I restore it? Output of restore headeronly 

Then I need to replicate the . Just take the binlog position and set replication on Master1 will work, but the problem is binog file and position is changing frequently. So before execute the Change master command on Master1 the binog get changed. How can I achive this without downtime. 

Im doing a replication with 2 PostgreSQL servers with Bucardo. I just want to know I have started to restore the data only to Server 2. Once it is done then I started the Bucardo triggers. Initial Restore: 

I'm trying to import a CSV file using import export wizard, Im facing an issue. The first row is also added as a column name. Import wizard: 

Grant full access to SQL server service account for M:\UserLogs. (Which must be a domain account) Manually restore the database. While adding the db to AG choose JOIN ONLY. 

I also had the same problem while implementing mirroring to one of my customers, the problem was SQL server Service account for both instances must be a domain name. Once again confirm that. And try this too. 

You can restore any database to anywhere. It'll help to refresh you Dev database. It'll help in PITR. You can restore any specific table from the specific database backup. Finally it will save a lot of time while restoring a single Database. 

Then, repeat the same step. Its my main production table and its always busy, so no locks, blocks, and deadlock. Can anyone suggest a better approach for this? I have already asked a question about archival, but here I don't want to partition the table. 

How the unique column has 3 duplicate values (value 2)? While restoring why its throwing this error? 

As per my knowledge, PostgreSQL will create a Primary key on column and Unique index on . So the EID is the unique column. --On both servers 

Im using Master slave streaming replication in PostgreSQL. Repmgr is configured to handle automatic failover. Once the failover happened I need to make the old master as a slave without doing any basebackup, also the new primary requires some changes to act as a master like need to enable wal logs and other things. How can I avoid these changes in Secondary during replicate to Old master and sync Old master without basebackup? 

If you have any Log backups job please disable it. Avoid taking log backups while enabled log shipping, or just manually run the logshipping backup job for log backups. 

As of my knowledge, DRBD is for Block level replication (works on storage) and Pacemaker is Service Cluster (works on service level). So which is the best one to use and best HA solution? And another question is Im using AWS EC2, So how can I manage VIP (I mean if the primary goes down how the PGpool will switch the VIP to Standby also its and in AWS)? 

And I found this error on master. found a zombie dump thread with the same UUID. Master is killing the zombie dump thread(7). On Master: 

IOPS limit is based on your volume size if you are using General Purpose SSD. And IOPS are your reserved IOPS. 

Keep your databases in SSD will handle more IO in very less time. Keep temp DB in SSD, obviously, it will improve the performance of the queries. We can use SSD as buffer pool (extension). 

Im using the below query and it took min . I tried to create single column and multi-column index. But still I can't able to increase it's performance. Row count on each table: 

Is there any upgrade command to do this? OR Do I need to install 5.7 and move the data files to 5.7's directory? OR Take dump from 5.6 and restore it on 5.7(DB size 1.5TB) 

Also disabled the wal log, cleaned the wal log files and reset the xlog. I just wanted to why happened and how to fix them? 

Im using Postgresql 9.2. I have disabled auto vacuum in Conf, but still, I can see this auto vacuum is running on few tables every day. Can anyone help me find out why its running? 

Then I tried to query the actual database, and I can see the primary key value twice. And tried to reindex and got this error. 

Hope, this has been fixed, in future I recommend to use mydumper for take backup of huge database and restore it. Native mysqldump and restore is pretty slow, because it's single threaded, but in mydumper you can use upto 62 threads. $URL$ 

I have MySQL Database and its 1.5TB. I need to setup replication for this. I have 32core CPU and 250GB Memory. The problem restore is taking more time even I used mydumper/myloader to backup and restore. Here is my MySQL Setting: 

Im using mysql 5.5 All of my Tables are in innoDB. Today my app went down and the mysql was unable to start. Here are my findings. 

Im using AWS MySQL Aurora, the current wait_timeout is 28800 which is the default one, and interactive_waitimeout is also 28800. But I can see more than the default value. 

Its not a good practice for production, but for reporting and staging you can use, Even though SAN and iSCSI are related to a network share. As Dan Guzman(who commented below question) Microsoft recommends keeping the files on SAN, Local Server or iSCSI. 

I need to setup a Highly available Postgresql Cluster. So I found Pacemaker and DRBD with pgpool load balancer are the best options. So the architecture will look like this. 

Im using MySQL replication with binlog_format=mixed on both master and slave, but still Im getting error 1032. 

I have already asked here about PostgreSQL HA setup, and I got many possible answers. But the bad thing is, even though if I use any third party software like PGpool, Repmgr, pacemaker are asking for timeout(If master is unavailable for n seconds). So there will be a minimal downtime. Is there any best option to overcome this and setup a zero downtime cluster?