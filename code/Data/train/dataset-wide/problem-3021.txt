This avoids duplicates for corners and edges and I could cache the results for even faster computation (though this is not very expensive to compute). Here is the full example as a gist: $URL$ 

First of all, this is not a Python issue. Rather this is an issue of the implementation itself. I agree with @Gareth Rees . You should always provide a minimally working example of the code. This is true for StackOverflow and especially true for CodeReview. In that respect all we can review is the little snipped you provide under the assumption that the functions you don't provide do certain things. The first thing that you can cut is the block. It enters iff the last element of is in and what it does is it adds the last element of to ; in other words: nothing. As a side effect you do pop the last element and since you do it in both cases it is better allocated above the . It looks like will return an iterable thing containing all rank 1 neighbors of . If you don't do any caching, is incredibly slow! Loosely speaking that's for the naive implementation and for the one given in the blog post you mention. To make matters worse, you do this for every friend of a word exactly one (since you prune duplicates). So what your running is which in the very crudest worst case can be (!); although this case is only relevant for theoretical considerations. Here is a list of things you can do: 

Instead of two types, and , which were holding outputs of the different insert operations, let's have just one, parametrized by . 

Then comes for free from Foldable defined as . This allows you to use other monoids for which is less costly compared to of lists, for example or . 

(I suggest you to try out hlint, which gives many interesting hints how to improve a Haskell program, including the two above.) 

Similarly tihs second function reads a string until it hits , consumes the optionally and returns the string. Then it's easy to construct a instance. The parser library already has a handy function that converts a parser into a function: 

This returns a pure result, but works somewhat more efficiently than (which is otherwise the way to go in Haskell), at the expense of being more imperative. 

The data structure doesn't help in determining if the required invariant holds or not. holds information that is outside of its scope - its level in the tree. This means, among other things, that a node can't be shared between multiple trees, which is something you want when manipulating trees (a new tree is just a slightly modified version of an old one, sharing most of its nodes). Your design focuses only on creating a tree from a list. If you don't need any other operations, that's fine, but if you want to do more operations later, like insertion/deletion, merging trees, etc., it's going to be problematic. In particular, for some operations, when the total height of the tree must change, you'll have to update the whole tree. Most likely, it'd be possible to find an arbitrarily long sequence of operations, each taking O(n) time (where n is the number of nodes in the tree). 

There's a few issues I can see with this code. The repositories are are sitting in the presentation layer. A big no no. It's software 101, don't place data access in your presentation layer, and not good to put business logic in the presentation layer. Those _roleRepository calls need to go in to a RoleRetrievalService or something similar. That centralise the business logic. Your repositories should to sit behind a service layer which handles all of the calls to the DAL. Your service layer should contain business logic and data transformation which turns those DAL entities in to models which can be consumed by the presentation layer. The query which does the left join should go in the service layer with the parameters required to execute the query being passed in as parameters. You could also pass in the sort options as well. That last statement which builds the results () is exactly what should be produced by your service layer. It all hinges one how much time you have and how much technical debt you want to incur :) 

next we can get rid of and instead sample a array of uniformly distributed random numbers and threshold them. This also allows us to vectorize the for loop and stay in numpy even longer (0.786 seconds): 

I now average over 1000 runs for a single game for both versions (compared to 1000 different games in my original post), using the method posted in Update 4. The other answer is very efficient in cutting down on the constant overhead and rightfully points out that the number of calls (even if it's doing no modifications to the board) adds unnecessary bloat. Juvian also pointed out that I do have access to the current move, which inspired this idea. For each adjacent field I use floodfill to find the group of enemy stones (if any). While doing so, if I encounter a border which is free the group has liberties and I stop the floodfill. If I manage to fill the entire region without encountering an empty border field the group has no liberties and is thus captured. Another advantage of this approach is that it conforms with the rules (See this Boardgames SE question), because I remove all stones in the opposite color first. Here are the two relevant functions: 

If you're going for a junior role, don't worry. All they want to see is that you can solve the problem. Forget about the code entirely. Code quality will come with time and several code reviews. Another factor which determines implementation is the use case it serves. How many times will this code be called? Once per day? Several thousand times a second? Those sort of questions will also determine how much effort and (more importantly) how much money you invest in that code. I had a quick shot at this problem. 

You haven't indicated that these processes are somehow related. I'm guessing that updating SQL Server and Access can be done separately. The only commonality I see here is that they are both run at a set interval. My advice would be to avoid async and threading entirely. Split this functionality so they run in their own processes and avoid having to deal with any async/threading complexity and Heisenbugs. Your code will also be much much shorter. Process 1 

Now my biology is lacking at best. I assume you are not respecting that because you can make the assumption to only ever deal with nucleic acids in DNA? Hence you ignore checking for that? You seem to further seem to be able to reduce this to only which I guess also has some very sound biological reason that I don't know. This will probably blow up if you aren't careful what files you put in. 

At this point we could start to tackle the loop, but it would start to get very micro optimized. One might consider aggregating the results in prob and then using and outside the loop, but that only nets like so it's more of a personal preference thing. 

will concatenate all sequences, despite them potentially belonging to different DNA Sequences. Here is Wikipedia (the best source to cite xD): 

If you don't need any user state, you can just ignore it. And the argument allows you to run your parser in another monad - is a monad transformer. If you don't need it, just use , which is exactly how is defined. You might be interested in using , a conceptually simpler parser bundled with GHC. Also this blog post about writing your own, simple parser, might be useful for you. 

Not only they are very useful on their own, but they can be also used to easily implement other operations such as insert, delete, merge and intersect. The general idea is that when we want to do an operation on two trees, we can split the first one using the root key of the second one and perform the operation recursively on the subtrees of the second one. Splitting two s is more-or-less straightforward, but merging them is more problematic, as for merging we need to start from the bottom, not from the top, and we don't have information about heights of trees. We could traverse both trees until the bottom layer, unwinding the visited nodes into a heterogeneous lists that holds trees of increasing heights and then merge them backwards up. Or we could embrace this idea into the data structure itself and implement finger trees based on . 

A) This is not very useful output. I would rather return how many lines have been inserted into the database or nothing at all. The fact that something happened somewhere while this code executed is implied by the user calling the script. B) If the script fails, you should inform the user; however, it is probably better to not catch any exception and then continue. The block will be executed regardless of what happens in . You can omit the entire block. A setup is thought of as a "cleanup block" which will make sure that the finally is executed before any exception is escalated. 

Unfortunately, you didn't specify your Python version. In 3.X there is the flag. Hence you could refactor this to You could also get access to (there are implementations for 2.7 on pip). This would allow dealing with paths in a pythonic way, because you can now do things like: rather then and have python deal with the os specific bits (symlinks, correct slashes, ...) 

I would avoid those throw statements entirely. You should only be catching exceptions if you plan on doing something with those exceptions. What you're doing is just logging and redefining the exception message. IMO a better approach is to put your logging code in the global exception handler. You'll have the complete stack trace, Exception type and if you're deploying PDB's with your production code (I hope you do), then you'll have line numbers as well. Search the web for these global handlers: in Global.asax 

Update Following up from one of the comments from @nikita-brizhak, if you're developing a library then you might want to log those exceptions depending on your use case. Another case I can think of is wrapper type classes. For example if your application connects to a MySQL and an Oracle database, you might have an abstraction over those connection methods which catch and and rethrow a generic with generic information.