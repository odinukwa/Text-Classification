Possible modifications Again, this may not be perfect for your needs. You mentioned that you are considering using it as a trigger, so then you'd need to a trigger instead and set up your triggers. You may not like that I ignored 'extraneous' fields, and you may want to make columns or flag an error in these cases? Maybe I made the wrong assumption about access to predicates? This certainly isn't a perfect functional implementation, but it will probably be enough for you to modify it to meet your needs. Best of luck! 

This should fix you right up! Here is a SQL Fiddle for you to play around with if you like. Edit: And as suggested... The Oracle version: Schema and Data 

Try out this SQL Fiddle, and see if it gives the results you're looking for. I did the best I could with the description I had. 

When you run , it only returns to you the plan that the database has created for running the query. When you remove , then you are asking the database to actually perform the query. What this implies is that, essentially, your query is going to take 3 minutes. Okay, so it's more complicated than that... Try By running a query with at the beginning, you are asking the database to actually perform the full query, but rather than returning results to you, return a report about the actual query performance. Here's a few things to keep in mind: the first time you run a query with , it probably has to access data and indexes which are on disk, and so this will slow down the query for sure. However, try running the same query a second time, and most often all of the data you need to perform the query is residing in the shared buffers. Then, you'll see the query typically finishes much more quickly on subsequent runs. In the end, is a good tool for hunting down how long your query is actually going to take, and if your query plan is any good. I personally don't really use prepared statements, so maybe someone else will chime in about these. I know that it may have an effect on the quality of the query planner results. 

I know that I should test my SQL script as well, but I don't have time to make a test before leaving for work! I can make a SQL Fiddle later, but for now it seems that no one has quite answered the question correctly. 

to set my back to being sufficiently large for my query. EXPLAIN & Recheck Condition So, running my query with only as 

Just trying to brainstorm a couple ideas which might jump-start the query planner into using a better query plan. I can't be sure if they'll work any better because I don't have your data. 

which lets you first order to put entries at the top, and then sort by a secondary column, where in this case I've chosen as the secondary column. You can use this SQL FIDDLE to confirm it works for your needs. 

Masi, The PostgreSQL B-Tree index is very strongly based on the implementation by Lehman and Yao, which includes a lot of work oriented around multi-version concurrency control, but there's still great info in this paper. Of course, PostgreSQL doesn't make a 100% accurate replica of the method in the paper, and to find the differences, there will be almost no way to do it other than to (1) find someone who understands the PostgreSQL B-Tree, and has the time to go through the intricate explanation, or (2) dig through the source code yourself. Another possibility is for you to visit Bruce Momjian's excellent reference website, where he discusses PostgreSQL internals in more detail. In this case, however, based on the nature of your questions, I feel like you may have a fundamental misunderstanding about how B-Tree indexes work. In this case, I think a little Google searching, or maybe reading through a portion of a textbook like Fundamentals of Database Systems by Elmasri & Navathe would do you some good. 

Kyle, I'm afraid that, due to the storage structure of PostgreSQL, a simple copying of indexes is not possible. PostgreSQL uses a storage method called heap-structured tables, where the insertion of data into the table has no guaranteed ordering or position. If PostgreSQL were to copy a complete table from one table record to another, including all versions of every row, information about deleted rows, etc., then I imagine that one would be able to copy indexes as well. In the case of how PostgreSQL currently operates, copying over data from one table to another replica table only guarantees logical consistency: that is, every row present in the original table will be present in the new table, and the data will be a row by match at the row level only. In terms of physical consistency, there is no guarantee that the way in which the data is physical stored on disk in the original table will match the physical storage of the data in the replica table. The reason this is important is because an index is essentially a method for mapping some information about the data in the table to the CTID of the row which contains the data. Long story short: other than some creative hacking, I believe the conventional wisdom is that you must rebuild these indexes on the new table. Sorry I don't have better news. 

You're permitted to do multiple joins in a single statement. You can do this even when joining on the same table by using aliases. I created a very quick example which is similar to your case. I just used ids instead of a time, and I stored the whole name as a single field; you'll just have to concatenate the first and last name on your own ;) Schema and Data 

Long story short, for only, as expected the query plan indicates that a Recheck condition is possible, but we can't know if will actually be computed. EXPLAIN ANALYZE & Recheck Condition When we include in the query, the results tel us more about what we need to know. Low 

Using these tables, you can establish a foreign key relationship from to , allowing a many-to-one relationship between and . Subsequently, you can obtain all invoices pertaining to a particular client via 

where you add your measurement values as (key,value) pairs of (timestamp, measurement). Use case: This is an implementation probably better left to someone who is more comfortable with PostgreSQL, and only if you are sure about your access patterns needing to be bulk access patterns. Conclusions? Wow, this got much longer than I expected, sorry. :) Essentially, there are a number of options, but you'll probably get the biggest bang for your buck by using the second or third, as they fit the more general case. P.S.: Your initial question implied that you will be bulk loading your data after it has all been collected. If you are streaming the data in to your PostgreSQL instance, you will need to do some further work to handle both your data ingestion and query workload, but we'll leave that for another time. ;) 

My recommendation would be for you to check out Cassandra. In my experience, and based on the requirements you seem to have, it seems like it would be a good fit. It appears to me that you have essentially have these requirements: (1) Key-value storage (look up data based on ID) (2) No need for or sub- (3) Need for a SQL-like language (4) Horizontal scalability As for (1), Cassandra will hash the primary key (in your case you can use a single column as your primary key) and which node in the cluster stores the data, and the Cassandra internals ensure look up by primary key is very fast. Cassandra doesn't support as in (2), but you appear to be OK with that. Since you are essentially doing key-value lookups, I'm actually not sure why you need an SQL language. Perhaps you are wanting to not only look up data by primary key, but also perform aggregations and use ? In this case, Cassandra won't work, but if you only need a simple SQL language to retrieve data as per (3), Cassandra has CQL, which may serve your needs. Horizontal scalability is built right in to Cassandra, covering (4). All that being said, Cassandra isn't a silver bullet. It's a great technology for certain applications, and it sounds like it fits you needs. Still, if you want aggregations, or if you want to query for multiple primary keys in a single query (e.g. ), then you may run in to some issues. Now, if you can let go of constraint (4), I'd 100% be promoting PostgreSQL. You stated the database will be getting hit with thousands of requests per hour, but that's nothing for a database like PostgreSQL, if your queries are only individual primary key lookups, and you aren't trying to transfer large amounts of data in those queries. Best of luck! If you provide more info based on my feedback, perhaps the community here can help you narrow down your selection even more.