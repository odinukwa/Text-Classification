Projectivity is a concept that applies to both dependency grammars (DGs) and constituency grammars (CGs). The extent to which it is applicable to both approaches to syntax is discussed and illustrated with numerous trees here: $URL$ The term itself, i.e. projectivity, was introduced in the 1960s in early explorations of word order in DG formalisms. The term is generally NOT used in CGs, although the concept is perfectly valid for both formalisms (DGs and CGs). More common terms that the projectivity concept underlies are long distance dependency, discontinuous constituent, and discontinuity. These terms all point to same thing, i.e. crossing lines in the syntax tree (that are then overcome in various ways, e.g. by movement or feature passing). Thus to answer part of the question (What is the role of projectivity in DG?) as directly as possible, the role of projectivity is to identify discontinuities (i.e. crossing lines in the syntax tree). When the crossing lines obtain, one is dealing with a discontinuity. There are numerous types of discontinuities, those associated with wh-fronting, topicalization, scrambling, and extraposition. Wikipedia has articles on each of these types of discontinuities, and each article has both DG trees and CG trees for illustration. Addressing the other part of the question (What is the connection between DG and CG?) is a difficult task. I disagree with Jlawler's comment in this area (assuming I understand it). There are big differences between DGs and CGs. DGs are much, much simpler than CGs at the most basic level. They often posit about half the amount of syntactic structure as (measured in terms of the number of nodes and edges in the syntax trees). You can teach a kid to diagram simple sentences using dependency, but the same cannot be said for constituency. The extra structure that CGs posit allow one to do things that one cannot do with dependency. For instance, c-command is not possible in dependency-based structures. DGs are, however, likely to challenge all the extra structure that CGs necessitate. They argue that it does not increase our understanding of syntax; the simple DG structures get the job done with much less effort. Finally, the point about whether dependency and constituency are notational variants of the same thing is mostly decided in DG circles. Most DG people reject the notion that they are notational variants of the same thing. This means that, as stated, you can do things with dependency-based structures that you cannot do with constituency-based structures, and vice versa. 

These examples illustrate that studied syntax is behaving as a coherent unit, i.e. as a complete subtree. This verifies that has is the root, since only if has is the root, does studied syntax qualify as a complete subtree. To state the point from another perspective, sentences (a-e) would all be bad if studied were the root over has. The point about which verb is the root of the sentence is going to be delt with in detail at an upcoming Depling conference in Uppsala Sweden: $URL$ My coauthors and I will be presenting a number of papers, all of which demonstrate that, among other things, the finite verb is the root of the sentence. Whether the computational people (who want to take a non-finite verb as the root) will listen, though, remains to be seen. In any case, I will happily share these papers with anyone who wants more information. Contact me at tjo3ya@yahoo.com. 

The main clause predicate is the two-word combination is hungry, so hungry is a predicative adjective. The attributive adjective happy is also a predicate, but it is not part of the main clause predicate, but rather it is a secondary predicate (or embedded predicate). Both adjectives are assigning a property to the boy, so both are definitely predicates in the relevant sense. From a technical standpoint, the distinction between attributive and predicative adjectives can be examined from the point of view of structural syntax. An attributive adjective is a dependent of the noun to which it assigns a property, whereas a predicative adjective is not a dependent of the noun to which it assigns a property. Many languages encode the distinction in terms of word order, as pointed out in the question. In Germanic languages (English, German, Dutch, Icelandic, Swedish, Norwegian, Danish, etc.) attributive adjectives precede the noun to which they assign a property, whereas predicative adjectives usually follow the noun to which they assign a property. While predicative adjectives are often introduced by the auxiliary verb be, they also appear without be, e.g. 

If the finite verb has is the root here, then studied syntax is a complete subtree underneath has. If, in contrast, studied is the root, then studied syntax is not a complete subtree. Diagnostics for constituents settle the matter, since they clearly reveal that studied syntax is a complete subtree: 

Prescriptive grammar has resulted in a situation that allows both forms in this case. The importance of the finite verb is also quite clear in answer fragments, as illustrated with the example in the question and shown here further: 

The square brackets mark the small clauses. They are called "small clauses" because they are indeed small -- they contain just a subject and a predicate -- and because they lack a verb (no verb appears inside the square brackets). They are not generally considered to involve ellipsis because there is no way to acknowledge elided material, e.g. 

There is a rather simple answer to this question. This answer is that conversion from dependency to constituency is not really possible, at least not in the way imagined. Dependency structures are usually flatter than the corresponding constituency structures. What this means is that when one translates from dependency to constituency, the result is a rather flat constituency structure. Most constituency grammars assume more layered structures than dependency-based systems can acknowledge. If a constituency structure is entirely headed, then it can always be easily translated to the corresponding dependency structure, and the resulting dependency structure will be considered valid by most people who do dependency parsing. The opposite does not hold, however. If a dependency structure is translated to the corresponding constituency structure, the resulting constituency structure will be flatter than most people who do constituency parsing want to assume. To restate the point in other terms, it is possible to automatically translate layered constituency structures to rather flat dependency structures, but translating rather flat dependency structures to layered constituency structures is impossible. Finally, I cannot confidently answer the last part of the question, since I am not working in NLP. However, my limited exposure to trends in NLP suggests that the answer is yes, i.e. dependency parsing is indeed the "state of the art" in the field of NLP. From what I understand, dependency parsing is now preferred in many NLP circles because it is in general simpler and faster.