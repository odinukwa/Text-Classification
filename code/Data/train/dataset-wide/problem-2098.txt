In your first option, will there be a one-to-one between and ? If so then it adds no value. Instead, it will complicate your SQL and may make for a less efficient query plan. The should be omitted and your second suggestion is the way to proceed. 

The DBMS can be left to deal with fault detection and recovery (when configured correctly). It would be most unusual to manually implement such behaviour in the application. Indeed, that is what a software stack is for - to remove from applications those features which are common and frequently required, much as an OS looks after memory management and thread scheduling, say. That said, you could add another column to each table containing the character strings. It will hold a hash of the string. Retrieve the hash along with the string, recalculate and throw an error if the two hashes differ. Given this is a university thesis the professor may be trying to convey a learning point, aside from any practicality about implementation. Your long-term benefit may be to investigate possible implementations of his suggestion, rather than listing reasons that he's an idiot. Just sayin'. 

Common practice when modelling compound values like your SourceURI is to split them into the component parts, which have a column each. These are given the types basic to the DBMS i.e. string and UUID. On retrieval the parts are passed to the application as-is, or concatenated with appropriate punctuation. You may choose to create a calculated column or similar to ensure consistentcy across the application. Postgres has the composite type construct which may simplify implementation. I have not used it so cannot comment on the implications. 

Since existing records are static they can be converted to the new schema any time after the migration process is signed off. Specifically, as soon as the new DB is available you can start loading it with old data. Start with the oldest and work your way forward one day at a time. Assuming it takes less than twenty four hours to convert one day's data, you can run this process non-stop until you have caught up with today's data. Thereafter you can continue with a more-or-less parallel run, with two options. Option "A" is to load each new day's data into the old DB, then run the convertion process to get it into the new DB. Option "B" is to run the old ETL to load today's source file into the old DB and run the new ETL to load the same soruce file into the new DB. Option "B" has the advantage of testing your new ETL process, too. Then on cut-over day you will have at most one day's worth of data to move. As for the convertion process itself I would suggest you read all the data for one business transaction from the old tables (i.e. a referentially in tact set), convert it, then write it entirely to a set of staging tables in the new schema. Note not to the live tables themselves but to a parallel set of staging tables. Then you can write lots of reconciliation checkes against these staging tables. If there's a problem you can bulk-delete the staging tables without affecting the new DB's data, fix the convertion and re-run. Once the reconciliation completes successfully you can bulk-copy from the staging to the live tables. The staging tables can have additional columns to help with reconciliation that you would not want in the live tables (source surrogate ID, for example). Retain the staging tables for a few months for any post-mortem on your go-live problems. I know it sounds like all this staging and reconciliation is eating precious time during a cut-over. And it is, but is nothing compared to the time it will take to post-fix two hundred million rows in an active production system. 

Large, general-purpose search queries often produce an inefficient plan. This can be ameliorated by adding to the query to invoke parameter embedding. Now your problem is that the general-purpose query is recompiled with every execution, which is itself an overhead. Avoid this by pulling out the common sets of parameters and have specific queries for each: 

If you really want id_Products in Options for performance reasons (you do not need it there to ensure data integrity; indeed it confuses that argument) you can do so. Make the foreign key in Options a multi-column key. Make it point to the corresponding columns in Variants. Even if Variants.Id is unique across all products there is no risk to the data by including is_Product in the primary key. Usually this is a waste of time. id_Products will not be meaningful to a human so all queries will join to Products to get the semantic value. You can join Products to Options without having to have an explicit FK constraint in place. 

Do you really need to copy the data? By defining a view the data can stay where it is but the benefits of a single, unified point of access can be achieved. The view can be defined using either UNION or UNION ALL syntax as appropriate. 

I think the complexity of performing this task is small and does not change with size. The same SQL that will find a missing recipient from 100 users will also find a single missing recipient from 100M users. The time to do so, however, is likely to be linear in the number of posts and the number of users. I can see two ways to organize the data. One is a "positive" list, the other a "negative". For the positive route the system actively stores what has happened. As a post is dispatched a list is kept of the recipients. For one million users this will be one million separate data items. Each item is marked as "unread". As a user sees a post the corresponding data item is marked "read". This will result in two million writes per post. It is likely most reading would happen within a short interval of the post being sent and tail off rapidly thereafter, so there will be spikes in write activity. The system will have to account for new users joining after the post was sent and users leaving before or after seeing the post. In the "negative" route you store only read events - as a user sees a post the corresponding event is stored. When you need to check who has read the post the system compares this "read" list to the master list of users and returns the intersection or complement as appropriate. Should a new user join after the post was sent they will show as "not read" even if that post was never sent to them. (This assumes a push model, like email, rather than a pull model, like browsing a website.) How to store this? In a relational model you would have one row per user/post. For 1M users and 100k posts, and integer IDs for each, this works out at 150GB, probably about 500GB once indexes and overhead are considered. This is not big by modern standards. Data compression or a columnstore would help here, but IIRC PostgreSQL does not provide either out of the box. It would be possible to reduce the amount of storage at the cost of additional writes. Rather than store the individual user/ post receipts store a post/ userid range i.e.