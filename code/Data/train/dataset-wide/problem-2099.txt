and others. It depends highly on what your code is doing. Duplicate the database to another server and create different configurations of indexes and run your code against it. ( again here!) Multi-column indexes are great because they help the planner check multiple filtering/sorting criteria in one pass of the index. Third, to address your last points, I'm not sure that pagination or moving to a non-relational model will help. The fact that you're using foreign keys actually points to a strongly relational set of data. You can, of course, try MongoDB for free and see what the performance looks like. 

You don't need to only on a schema. You can get more specific and specifically to a table, like this: 

Attempting to do an insert will not invoke , even if the trigger is defined as , because the rule is modifying the command itself; after the command is re-evaluated, it's no longer doing anything, and the trigger no longer applies. Obviously your rule does not need to be defined with , and in that case, your rule and your trigger could both apply. With specific regard to your code, it's hard to say without knowing the definition of , but it doesn't look like it since your trigger and rule are defined for different operations on different tables. 

I don't believe you can use pgloader in this way if any of your HTML contains character sequences that would be interpreted in the context of CSV. For example, any backslash followed by digits is perfectly valid (and not-special) HTML, but would be transformed to something else by pgloader while importing. I can't even estimate how pgloader will handle the double-quote and single-quote symbols scattered around in your documents. You would also need a byte that doesn't exist anywhere in your content to serve as a row delimiter; you could use some low-value control byte for this purpose assuming your HTML is valid and doesn't contain any of these special bytes. This naturally depends on your chosen character encoding, and is rendered worse if you have multiple encodings among your documents. Assuming everything is UTF-8, the NUL byte should not appear in any UTF-8 stream. Assuming you aren't concerned about special CSV character sequences, you could attempt to do this import using NUL as your separator value: 

I posted this in stackoverflow but realised it probably belongs here instead. Imagine the following table: 

I have been struggling trying to find a way to properly and reliably maintain a balance for accounts in an order/payment system. Currently, the process consists of three tables. Below example is very simplified but gets the point across. Table one contains the account ID, name, total paid, total orders, balance etc. Table two contains payments, date, the account ID (FK) and amount. Table three contains orders, date, the account ID (FK) and amount. Calculating the balance is of course as easy as 

All good - the problem is - how do I maintain this over let's assume years of activity and many, many transactions? Currently the accounts table is updated every single time a transaction is made, meaning the above three statements run every time. There is of course an index on the ID column in all three tables, and everything is fast and neat for now, but I work with less than 100,000 transactions, meaning everything fits into the RAM and basically all I throw at the database gets executed immediately. Over time this will change, of course - so how do I handle this? Is it better to mark the accounts as "dirty" and process balancing in a batch job later, or can I keep updating the balance real-time even when I reach the point where the orders and payments tables will not fit into the RAM? Do I need to segment the orders and payments and "lock" everything before a given time, such as before last calculation? (I would rather not). I ask because then I could do a SUM WHERE DATE > [last lock timestamp] and update with UPDATE BALANCE = [balance] + [value for last lock] and prevent the problem from outgrowing resources. I don't need a running balance for each transaction. That's all I can find results for when I try to research this problem. I just care about the total amount paid, the total amount ordered and the balance of accounts. Why don't I calculate on the fly when I need the balance, you ask? Because one must be able to query the system and ask "Which of my (let's say) 50.000 accounts have a balance below 0?" - something we cannot do easily if we do not store the balance with every account. 

right-size the datatype of each field -- does this offer performance boost? most fields are text or boolean flags. break table into at least 2 others, e.g., visit info and page info. Keep it in one table and partition it. From what I've read, I think partitioning on the most frequently queried field is the optimal choice. 

Question Which optimizations should I employ to make this DB manageable? I've read a lot of posts about optimizing big dbs, but I'm pretty new so it's difficult to determine which techniques are suitable for my setup. DB Info I loaded about 90 mil rows from flatfile csvs into a mysql database (aws RDS, if that matters). There are 60 columns and each row is a click on a website. Datatypes: All datatypes are text because of limitations with the mysqlimport utility (specifically, its treatment of null vs. 0 values in numerical fields, which i could be wrong about) Each column falls into one of these categories: 

Visit info: time, referrer, etc. Many pages log millions of visits (front page, for ex.), some log only a few. User info: city/state, etc. Users log anywhere from 1 to thousands of visits, with the distribution skewing right. Page info: url, content flags, etc. 

Anyway, now I'm trying to make a Dockerized version work, but that feels like a layer of complexity I don't want. These symptoms are all over support forums for MySQL, and almost every post has a different accepted answer. So my question is, of course, how do I fix this? But deeper than that, what the heck is going on? 

IO - The database only has 4 users, so will not have a lot of i/o, mostly just queries to populate dataframes in our python data analysis environment. indexes - none besides the default generated index. Candidates for indexing are user ID and page view timestamp (potentially a multi column index?) Options I welcome any other ideas, but this is what I've come up w/ with some searching. Interested to hear which steps are worthwhile and which aren't.