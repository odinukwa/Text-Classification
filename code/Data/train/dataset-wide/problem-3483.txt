They probably suggest that, because 50 MB memory per request is not the common thing. You should probably check, if you can get rid of any modules you're using. Also check if you can move any mod_ (like mod_php) to fcgid. After you did that, you probably have only thread-safe modules left and can safely switch to worker MPM, which will likely resolve all your performance problems. 

You are not required to provide an MX record for your domain. If there's no MX record present, an MTA is supposed to use your domain's A record to deliver mail to. This will however cause a significant number of spam filters to flag mail originating from your domain as possible spam. So even if mail isn't your "main task", you probably still want to keep the spam filters happy. To accomplish that, your zone has to look like this: 

This is probably caused by your DHCP daemon that issues dynamic DNS updates. When shutting down the Linux box it probably releases its DHCP lease, causing the DHCP server to delete the DNS record for it. Your windows client then receives a negative DNS response while the Linux box is rebooting. When the Linux box returns, it acquires a new DHCP lease, the DHCP daemon adds a new DNS record, but the Windows client still has its negative response cached. Possible solutions would be lowering the default/negative cache TTL of your DNS, raising the TTL of the dynamic DNS resources created by the DHCP daemon, or ditching DHCP in favor of static IPs and DNS records. 

In order to route multicast traffic, you need a userspace daemon like (recommended) or . Since you'll also be needing some kind of interface representing the destination network in the routing table, you could create tunnel interfaces for your SSH connection using the . This will create an interface on both your SSH hosts. 

(The ICMP packets are reaching host A and there are no iptables rules in place that block ICMP.) Possible reasons why this happens, that I can think of: 

No idea what your sysadmin is trying to achieve with a loop mounted file for (and then using a journaling filesystem for it). There's plenty of space on your root partition , so I guess if you just you'll be fine until he's back. 

The process seems to be stuck in a single long-running syscall. Since doesn't show the syscall that was running, when it attached the process, you get no output. You could maybe get more info using a debugger like and running a stack trace (gdb command: ). 

You might consider using `date +%w' as part of your tarfile, so you have a tar file for each of the last 7 days and dont have to worry about purging old copies. 

Not saying I know specifically how tomcat performs shutdowns... I would expect a pidfile associated with the process, or a control port that tells the application to shutdown. Barring those, however, it's common for scripts to 'hunt-and-kill' by looking at 'ps -ef' output (or similar). In these cases, it's easy for the kill scripts to be too aggressive and kill off all matching pids (or just the parents of those pids). Cant tell you how many times I've been editing a script in 'vi' only to be killed off by an aggressive 'stop' command someplace. 

I would recommend against using 'tcsh' as a shell. It tends to make you think that writing shell scripts in tcsh is ok. It's not. The real attraction seems to be the 'up-arrow' command line ease-of-use, but with bash you get that anyway. Also, coding scripts is much easier in 'sh' and it's derivatives (like bash and ksh)rather than csh and tcsh. I've also found that sh is on ever flavor of unix, and bash is easily obtainable as a first choice add-on. I'd warn against using the features of ksh and bash (like variable arrays and hashs) unless you can guarantee it's existance throughout the enterprise. 

7.5% for a single child doesn't sound too abnormal, but it all depends on what the child is supposed to do... I run systems with apache and mod_perl and those children get huge. Watch your apache children memory footprints over time to see if they stabilize. If not, use MaxRequestsPerChild to control how often they are restarted. Use MaxClients to limit how many concurrent children you have (to avoid swap or out of virtual memory issues). In my experience, it's generally a memory bottleneck on the webservers. 

looking for spikes is an interesting problem in general. is your data noisy? Try using the TREND modifier like this 

ACK! several folks have suggested the wonderful date +%Y%m%d_%H%M%S style solution but nobody has mentioned the major caveat of '%' in crontabs... '%' it is equivalent to '\n' so your cronjob will likely fire and fail mystereously! You'll more likely want to simply escape it with backslash like this (and I also like to get some kind of inventory or other output to check that it ran). 

In the original DNS specification (RFC 1034/1035) there were two steps of cache invalidation that needed to take place, before a zone update was globally visible. Additionally to the already mentioned TTL expiry of caching resolvers around the world, you first needed to wait for (all) your secondary name server(s) to refresh the zone data from the primary's zone. Only after DNS NOTIFY (RFC 1996) was specified in the year of 1996, there was a standard way of promptly notifying all authoritative name servers about zone changes. So maybe the original phrase of "change propagation" was more appropriate at the time, since it was a two-step process. 

In a network where one of the authoritative nameservers sits on the border of the internal network, I use views and the directive: : 

If your setup is not working as intended after that, the output of will probably be helpful for further debugging. 

Your CPU needs to support either VMX (Intel) or SVM (AMD) - Check The kvm kernel module needs to have set - check The virtual guest CPU has to export the VMX flag - check for a stanza like 

into the alphabetically last subdirectory of . Try anywhere to help visualize this. Now, judging by the other comments you made, it appears you already shuffled stuff around further. Since the you ran is unlikely to really destroy any data, you should be able to move stuff back in place. If you have no statically linked rescue shell like available, you'll have to boot into a rescue system. From there you should be able to locate , , , etc. If you manage to put these back into your system should be bootable again. Think before taking further steps, it seems your recovery attempts so far only worsened the situation. 

My starting point for issues like this is usually . This will quickly give you an idea where the largest amounts of time are spent. See $URL$ for some nice examples on how to use it. 

Since you already located most of the time spent in the kernel, I'd suggest enabling CONFIG_LATENCYTOP and running, well, to see more. Can be done with , too, but is way more convenient. 

In this case eth3, which doesn't get you to the desired destination. The gateway you're asking to forward your packets (192.168.1.254) truthfully responds, that it has no path to the destination network: 

I always run the above to make sure it works, then remove the 'n' flag that once I'm happy with the results. The key features of the above combinations: 

shame that so many monitors are 'widescreen' these days... I wish the 4:3 form factors were more common (and cheaper at resolutions of 1600x1200 or higher)... not everyone wants to watch a movie on their PC. I have a 1200x1600 (portrait mode) 20" and while I enjoy (and depend) upon the vertical size, it still feels a wee bit cramped horizontally. 

wiki's tend to have revision control built-in, and many are file-based (vs stored in a database), so rsync should work perfectly fine. I know folks who do this for 'TWiki' for replicating their installations to mulitple servers. Perhaps you only have 'ftp' access to your wiki files? you might consider 'wget' to pull from ftp (rather than the http interface) with the recursive (-r) and timestamping (-N) flags set so that it only transfers file that are 'newer' (which isn't exactly a diff). Once you have a 'copy' of what is out on the ftp server, you'd mark the update time somehow (often with just a 'touch' of a specific marker file). You would then edit normally via your local installation of the same wiki, then use 'find $dir --newer touchmarkerfile' to identify the updates for ftp and transfer them over via a script around an ftp delivery tool. I have used such a solution before (though I had the advantage of sucking the changes back to the main server via 'wget', so just used the recursive timestamping approach again. In hindsight, if I had 'ssh' access (I didn't), I would have simply used 'rsync -globtru[n]cv' to simply pull (or push) the files in each direction. 

We run into this when editing CGIs... the #! interpreter line gets Ctrl-M on it somehow, rendering the executable not found. It looks like a perl error but is really the 'she-bang' interpreter line having 'nearly' invisible characters at the end. In our case, we found this after the file was written. try using dos2unix command to copy to another name and try hitting that. If it works, you've found your root cause. Sorry to say that I have no real workaround except to recognise the problem when I see it. --edit-- Our error message is usually: scriptname: file not found NOT the 'file busy' mentioned in the question. 

I know this is a rather complex approach, but in my opinion going with LDAP based netgroups, nss_ldapd and possibly augmented by sudo-ldap is the most integrated solution. Once LDAP is in place you get Kerberos SSO virtually for free as well. 

This has nothing to do with DNS. The host name usually is only set once when the network comes up. Someone probably changed it by accident, just change it back. 

What is the best way to fix this, without disabling PMTU discovery globally or lowering the interface MTU? Maybe clear the DF bit somehow like FreeBSD does with ipsec.dfbit=0? 

The backups (via Bacula) of one of my servers (“A”) connected via IPSec (Strongswan on Debian testing) to a storage daemon (“B”) don't finish 95% of the times they run. What apparently happens, is: 

This is a bit of a shot in the dark, but: You're using which will only limit the heap portion of your memory. Since your process grew to ~ 50 GB memory usage, this could have been caused by a huge stack. So if your application spawns a lot of threads, heavily relies on recursion or stuff like that, this will be your cue. If that's the case, look into things like GC logging, , , , to analyze the situation and into , JVM parameters and ThreadPool sizes to remedy the problem. (Not entirely sure about the parameters, Java 5 was the last version I had to do with professionally.) 

Your nameservers are both on the same AS, same subnet even. I'd try getting additional slave servers somewhere else first, via zoneedit for instance. 

Add a Content-Type header such as or use a mail client that guesses the Content-Type (GNU mailutils for example probably do that). 

Bacula opens a TCP connection to the storage daemon's VPN IP. (A → B) Since the kernel setting is set by default, the IP Don't Fragment bit is set in the plaintext packet. When routing the packet into the IPSec tunnel, the DF bit of the payload is copied to the IP header of the ESP packet. After some time (often around 20 mins) and up to several gigabyte of data sent, a packet slightly larger than ESP packets before is sent. (A → B) As the storage daemon interface has a lower MTU than the one of the sending host, a router along the way sends an ICMP type 3, code 4 (Fragmentation Needed and Don't Fragment was Set) error to the host. (some router → A) Connection stalls, for some reason host A floods ~100 empty duplicate ACKs to B (within ~20 ms).