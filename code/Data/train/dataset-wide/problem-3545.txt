The second link in an aggregate pair clearly not going to be adding any kind of performance or processing advantage while unplugged. Beyond the obvious (plugging in both interfaces) the only advantage I could see is the ability to plug in the second NIC and then unplug the first one without the overall logical interface coming down. The more likely answer is that the upstream network infrastructure either didn't have sufficient port capacity or there was some sort of mismatch in capabilities between said switch (or switches?) and the host. Having both ports plugged in might have broken something and unplugging one fixed the issue, at least temporarily. 

The spec for option type 6 has variable length and can support more than two entries. The length field is 8 bits and represents the number of bytes. 256 / 4 = 64 IP's. Clearly this is well beyond the number that the client must recognize, but specifying 3 entries is likely supported by many clients. It certainly won't hurt anything. orst-case the client will ignore the third. Whether it makes sense or not is a function of your own infrastructure and availability. A WAN site might have two redundant servers on site and a third remote, for example. There are a number of mechanisms to improve availability of DNS infrastructure, many of which don't require additional IP's to be configured on the client side (i.e. anycast, load balancers). 

The quickest solution would just to use a packet sniffer - tcpdump or wireshark (the latter having a graphical interface). If there's some kind of constant traffic source it will tend to stick out. You'll also be able to dig into the particulars of the payload once you do discover the source of the unknown traffic. 

What are you trying to accomplish? Is it minimizing (smoothing out) the spikes in demand created by a bunch of servers simultaneously booting or something that can push emergency adaptation of poweroff? If it's the former then look for power sequencers - there are in-rack units designed to power up groups of outlets with programmable delays - say a few minutes between each of your groups to allow each to settle before booting. This used to be pretty common with some vendors in the larger side of midrange. 

Yes - this is valid, and actually pretty common. The router will treat these interfaces (and associated routes) as being part of a common routing/forwarding table which will be kept separate from any others (unless you manually cross-import). 

You could also add this to be run automatically by creating a script called that would look something like this: 

Have you been through keymap, loadkeys, etc - $URL$ . It's the same idea but for the kernel and the keyboard driver. You should be able to take your present layout and just add a couple of lines. 

Without question you should use LVM - you can put the entire 45T (or some usable subset) into a volume group and then parcel out individual logical volumes as file systems, raw devices for VM's, set up snap shots / mirroring, etc. I have a 12x2TB and 8x500G arrays (the former software RAID the latter hardware) and have been extremely happy with the ability to grow and shrink volumes (assuming the correct filesystems, of course), create mirrors for backup/test, etc. My advice would be to set up your boot / OS drives (RAID-1 generally) and then put the rest into a common group (RAID-6 or RAID-10 depending on requirements) and overlay LVM onto this latter group. 

What are you trying to accomplish? You can simply create additional interfaces in kvm and tie them to tagged VLAN's on the host's NIC. 

It's actually fairly common practice to keep channel group numbers consistent along a chain, but it's not necessary. The PC number is only significant to the switch - so you could have different port channel numbers on either side of a given connection (i.e. po1 on switch A connects to po2 on switch B), while these switches use otherwise overlapping numbers for other connections. Consider the case of twenty access switches connecting to a pair of aggregation switches. For convenience's sake the access switches are identically configured such that po1 corresponds to aggregation switch A while po2 connects to aggregation switch B. On aggregation A a decision is made to number the port channels 101-120 (i.e. 100 + switch number). The same setup is used on the B switch. The aggregation switches have a crossover channel that is numbered po2 on both sides. Each also has a po1 that connects to a VPC trunk on a Nexus core switch (or MLAG on another vendor). Complete overlap of numbering in a fairly typical enterprise configuration. 

The comparison to 14.4k modems isn't apt. In the case of an access server with a bunch of modems attached, each client has a dedicated connection and its own share of bandwidth. In the case of wireless all clients are sharing the same radio frequency space. As the client density on a given AP increases, the likelihood of contention for this available resource also increases. The 40-some-odd client is a guideline and can vary based on client activity (among other things). This issue is independent of what actually tended to be an issue with access servers, which is oversubscription of uplink bandwidth. Several hundred 14.4 users being directed out of a single T1 meant an often untenable degree of resource contention but that contention wasn't between the client and the access server but rather between the access server and the rest of the world (vs the wireless case where contention occurs between client and access point). 

1.) Jumbos -might- help if you're seeing a lot of processor load for interrupt traffic but if TCP is operating correctly it should be able to ramp well past 2G on a 10G link. I've seen plenty of 10GE links running above 90% without jumbos enabled. 2.) If you do use jumbos, enable the same size on every NIC and every switchport in the VLAN and/or broadcast domain. PMTU works when packets cross routers and mixing MTU values within the same network will lead to nothing but misery. 3.) I'm not particularly familiar with the Procurve gear but TCP traffic can be tricky at high speeds if there are any questions about buffer availability. I've seen other testing where this has manifested (without apparent TCP drops) as a huge cut in performance that ended up being fixed by actually reducing buffer sizes. 4.) Make sure that the actual TCP settings (1323, SACK, etc) are all configured consistently. The operating systems in question should be fine out of the box but I don't know much about the storage node. It might be worth digging into - either in terms of settings on the device or via a protocol trace (wireshark or tcpdump) to observe window sizing and any retransmissions going on. 5.) Try eliminating as many variables as you can - even getting down to a crossover cable between one of your storage nodes and a single workstation - to further isolate the issue. Don't be afraid to disable some of the offloads you mentioned as well, as they've been known to cause issues from time to time. 

Either connect the netA and netB devices with a physical cable or the netB device needs to be set to be a client of netA (either directly in client mode or via an extend-a-network mode or however TP-Link specifies it). 

Do you have a sense of the size and frequency of the flooded packets - or a particular VLAN, for that matter? One common phenomenon is unicast flooding due to the mismatch of CAM and ARP timers. If CAM ages out but the corresponding ARP entry is still there then the switch will flood these frames. I've seen circumstances where this has resulted in literally gigabits of traffic showing up places it wasn't supposed to. There have also been circumstances where this correlated with CEF also losing entries - which then manifests as CPU issues on many platforms. As far as pulling the address count via SNMP - check out this page: $URL$ . It's slightly painful, but the mechanism is to pull the list of VLAN's and then to pull a list of CAM table addresses per VLAN and count accordingly. On the plus side it will give you a clue about where to look if there is actually a sudden proliferation of addresses somewhere. You could also simply call "sh mac address-table count" either via ssh or a periodic EEM script that would then transmit the result back via e-mail, syslog, trap, etc.. This is dependent on the hardware platform and code rev, though.