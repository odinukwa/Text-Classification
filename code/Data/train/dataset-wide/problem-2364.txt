What I've had to do to fix the issue is delete every instance of 'MyReportAcct' on the server, then recreate it, then add to each database. This doesn't happen all the time, but often enough that it is irritating. Further, we shouldn't have to do this at all. I think that we should change this account to an Windows Account instead of a local SQL Login in every instance. I have a feeling that this might fix the issue, but I need to prove why or provide a reasonable answer. I think part of the problem is the mirroring in some way, but I don't know how to prove that either. Security is a weakness of mine and I'm employing steps to change this, so any insight you can provide would be greatly appreciated. Please let me know if you need more information. 

It runs in 64ms. (Note that I also raised the limit from 200 to 100000). Why is it trying to allocate 1GB ram to process this query with limit 200 and isn't for the subquery form? 

Versions: OS Debian, Master: MySQL 5.5.40-0, Production Slave: Mysql 5.5.40-0, Local Slave: 5.5.37-0 I have a MySQL Server with 2 slaves, one on the cloud and the other on a local machine. The local one gets desynchronized with the master for reasons I can't find. From time to time a specific command fails showing that the data on tables are not equal. This command is: 

In other words, if I have only a daily tournament on schedule and run it to process the next 7 days, it needs to create 7 rows with the correct start_time on each. Consider that in a real world it will have a lot of recurrent tournaments daily, weekly, monthly... And it needs to populate everything for the next X days (7 in the example). I was wondering the best way to solve this and want thoughts/ideas about. 

I have a single instance database server with several databases. Some do not require log shipping and some do require log shipping. Is there a way to edit Ola's LOG backup job to ignore the databases I have configured for log shipping and backup all the remaining log files? The only way I can think to do this without turning up a new server or create a secondary instance is not using Ola's script and create my own custom maintenance plans. 

Yes, you can do what you're asking. You should have your database in full recovery mode and then be performing log backups (via jobs/maintenance plans). $URL$ 

I seriously suggest building another system with SQL Server Express and configure it as your witness server. Then set your operating mode to "High safety with automatic failover (synchronous)". This solution will alleviate you of this problem. You will incur a cost for the desktop windows license, but as you may already know, SQL Server Express is of no cost monetarily. 

Postgres 9.3 Debian 7 I'm working on a mobile game project (poker like) where an admin user can register a recurrent tournament. As it's recurrent, I modeled it crontab like as follows. 

Pg 9.3 on debian 7. I have a very huge table in rows, about 178 million (will turn 250 million in a week and then grow 500 thousand a day), but not that huge in size, wich is 20GB, it's a small varchar, some timestamps and integers. I need to prevent a group of 5 columns to be inserted repeated, so I created an unique index on them. The problem is that the index is 34GB now. My questions are: How can an unique index of 5 columns in a table of 9 be bigger than the full amount of data itself? Will this index ever be used, cause I guess it's cheaper for the DBMS to scan the entire table instead. And is there any way to turn this index smaller or maybe have another solution to implement an unique constraint? 

I forgot to update this post with what ended up working for me. I don't know why, but running SSMS as Administrator allowed the backup to complete as you'd expect. 

Problem started May 3rd. We are no longer able to import SSIS packages that have been modified by Visual Studio 2017.X into our SQL Integration Services (SQL version 2017 14.0.3023). SSMS Integration Services importation component (the GUI) keeps crashing! For any NEW packages or any Packages we modify with in Visual Studio 2017 x. We are able to import historical packages and Visual Studio 2012 packages. Here is the error message from SSMS 17.5 

Another option to consider if you don't want to impact the connection between the log shipping servers during production hours is to use Distributed File System (DFS). You will still need a log shipping secondary in Korea, but you will disable the copy jobs for all the databases on the log shipping secondary. All you have is the backup job running on the log shipping primary and the restore job on log shipping secondary in Korea. DFS provides you a lot of flexibility to throttle bandwidth during production hours and then increase during off-hours. Essentially, DFS would copy all your .trn files directly from the log shipping primary to the log shipping secondary in Korea. 

None of its columns are mentioned in the query, so the question is: Why it's being used and how it's being helpful? The second question is, why it's not using this index I created: 

And I need to create a job (procedure) that will populate a queue of the next tournaments in the history table as follows. 

In the comparisons each field needs to be equals to the generated timestamp field OR NULL that means every. The resultset was: 

If I remove the array_agg from the query, it runs smoothly in 1ms. It used to work fine and started to fail on last days. Nothing was changed in the config file or on OS config. Another funny fact. If I write the query as: 

Situation: We recently had a consultant company build new Lync 2013 Servers in a mirror setup. There are two security principles set, SA (disabled), and a AD group that is only associated with the Public server role. 

You're correct about how the VLFs grow in size. Check out the following video from Jes Borland for some more tidbits. How SQL Server Works: Log File (Video) 

When I posted this question, I had all, but “NetFx3-ServerCore-WOW64” enabled. Further, my problem was an assumption on my part. I assumed that these features would be installed along with SQL setup, this wasn’t the case from the CORE installation, you need to make sure these features are installed/enabled BEFORE the SQL 2012 installation begins. When I performed the GUI-based installations, I didn’t have worry about enabling the aforementioned features. Moral of the story…DON’T ASSUME ANYTHING! ALWAYS check prerequisites if you’re having installation problems for any software/hardware. 

My solution was to make a full join between the schedule and every minute timestamp (as it's the smallest division) from now until the days limit and comparing each timestamp field with each schedule as follows (In the example I'm calling the query to process 5 days (1440 minutes * 5)): 

On the local slave it gives me the following error: 'BIGINT UNSIGNED value is out of range' because the column is unsigned and the value would become negative after the subtraction, but on the Master and Cloud Slave the value is bigger, allowing it to be subtracted without turning negative. I've tried to drop and recreate the local slave several times but it always end up desynchronizing and giving me an error like this. Any thoughts about where to look for stuff? Maybe the difference of the minor version on the local slave? 

Is there a command at the CLI or in PowerShell that will list the components installed for SQL? I'm looking for something like the Feature List you can get from running the discovery report from Tools in the SQL Server Installation Center program option. I'm running SQL Server 2012 on the CORE version of Windows Server 2008 R2 Enterprise. I've searched around the Net, but haven't found anything useful. 

Are there any configurations that I can change to stop this? It only appears to be coming from two servers. 

I believe that I have most of the work done for whoever wants to tackle this question. I have the following query built: 

Postgresql 9.3 - Debian 7 54gb RAM 8 cores (On google compute engine) I have a huge table called search_token (~50M rows) and I'm trying to execute a very simple query that is: 

After some time i figured out how to do this. Lets put as sample data 2 recurrent tournaments: One daily at 16:20 and another every sunday at 14:00 

That would make much more sense and it's even smaller in size. The odd index is 41mb long and the second one is 30mb long. I'm trying hard to understand how indexes work. 

(Debian 7, Postgres 9.3, dedicated machine with huge cache) I have one big table called process_data (14gb) and another tiny lookup table called process_location. I'm doing a query between these two and in the explain query Postgres is using an odd index not at all related to the query stuff like this: Query: