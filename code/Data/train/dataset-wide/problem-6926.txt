The actual rate for the most recent year(2016) we have data is 88%. This comes from the Bank for International Settlements. 

You should do none of the above. This is an invalid decision-making process. In the best of all possible worlds, create a series of do loops and go through the set of all possible combinations of variables. Calculate the AIC or the BIC. If you know nothing about either, just pick one as they usually give the same result. The model with the lowest AIC or BIC is the model that is probably the closest to the true model in nature. Statistical significance does not matter at all. If the best model is also not significant, then it is not significant. It could just mean the other models have a spurious significance. Likewise, if the model chosen is significant under the F-test, but has non-significant variables, then you cannot change them. If you do not know how to write do loops or for-next loops, then find a package with step-wise regression. It will not cover all the models, but it will cover many. Use the same criterion either the AIC or the BIC. Because you are asserting the null is true, you cannot have more than one null. It means nothing if you add or subtract variables because you are changing the null every time and you cannot do that. The AIC or BIC are non-Frequentist methods of estimating the true model and so the solution of using either bypasses the question of significance until after the model is chosen. EDIT I thought I would provide an edit to cover the statements in the comment. For starters, I agree with the comment. I thought I should ground the above statement better as to the logic behind it. The various information criteria, the AIC, the BIC, the DIC and so forth, can either be grounded in information theory or in Bayesian theory. From an information theoretic perspective, if you have outside information about which models should be included or excluded either from theory or experience then that information needs to be incorporated. Since there is no direct method to merge them together, you should use judgment in which models to look at. From a Bayesian perspective, the various information criteria are stylized point approximations of the Bayesian posterior under somewhat restrictive assumptions. In many respects, they are not good proxies because the smaller the posterior density, the less likely it is to be true, whereas the reverse is true in the criterion. It is best to think of them as both being rankings and they would provide the same ordering of rankings. Now there are two Bayesian issues present. First comes from the fact that you can construct Bayesian theory from Cox's axioms. Cox's axioms are built around Aristotelian logic. You would use Bayesian methods to assess logical statements. If some statements do not need assessing or could be excluded by logic, then they should be excluded from consideration. The second comes from the nature of the prior density. If you have prior knowledge that some cases cannot be true, then you should give them zero prior weight. This would exclude them from consideration at all. Nonetheless, the combinatoric method should be considered because it maps to a Bayesian argument and the information criterion are non-Frequentist constructions and so you shouldn't be using Frequentist criteria in making this type of decision. Bayesian hypotheses are combinatoric. If you are testing the model $$y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+\varepsilon$$ and you have no logical reason to exclude any case, then you are going to have to test eight hypotheses, removing one, two or three variables at a time. This is the practical equivalent to the Frequentist hypothesis that $\beta_i=0$ which cannot be tested in Bayesian methods because it is a sharp null hypothesis and of measure zero. Removing a variable and assuming its coefficient is zero is the same thing. Because Bayesian methods lack a null hypothesis, which is both a strength and a shortcoming, no one hypothesis gets special treatment or special weight, absent material external information. You test the posterior probability of each possible model as part of your process. Now, this is where a legitimate objection does come in. Information criteria are stylized approximations of the actual posterior. In some cases, they are perfect approximations but they can be poor approximations in other cases. In the perfect approximation case, you should be using the information criterion that is appropriate for your problem and your worry is not about the criterion, but rather the representativeness of your data. In the poor approximation case, close differences could, in fact, be reversed if the posterior was actually computed. As you multiply your combinations, the greater the probability of a single pairwise mistake will happen. Nonetheless, since you are not doing model averaging, but model selection it will probably not be the highest two pairs. Using tools like the information criterion does highlight a problem in data-based measures over a true null hypothesis based methods, the combinations show how little independent data that you may have. If you are not model hunting, but instead testing the one true model, then you will lose a few degrees of freedom and unless you have collinearity problems, then you are fine if your sample is of reasonable size. In my example above, it is quite like dividing your sample by eight. It isn't a subtractive process as one would see with degrees of freedom, it is more like a process of division. Add into that the internal correlations and there may not be much in the way of independent information in your set. Still, it didn't seem to me like you felt you had a mental model of how the relationships should be, so I would still recommend either the combinatoric or the step-wise solution. 

It depends upon how and why the inequality exists. Equality can also be bad for growth. In the old Soviet system, most people were approximately equal and this was done by the use of police power. It did permit rapid industrialization but then stagnated. Not all innovation was good either. Look at the Trabant. An amazing vehicle. Watch $URL$ The US used to be very flexible. That was good for growth. If you were born poor, then you could become rich. If you were born rich, then you could become poor. Some income inequality is due to changes in technology and required education. Some are due to legal changes that transfer resources from ordinary people to the rich. It is now very difficult to become poor if you are rich and to become rich if you are poor. That damages the incentives to build a business to grow the system. Inequality is neither good nor bad, by itself. It is a consequence of something else. It is smoke; it is not fire. There are several papers by Peter Temin that are very good on the topic, but he has quite a few and I don't know exactly what you are searching for or have already read. 

My paper takes these four and other papers, such as a paper by Koopman and one by Jaynes, to construct the distributions if the true parameters are unknown. It observes that the above White paper has a Bayesian interpretation and permits a Bayesian solution even though no non-Bayesian solution exists. Do note that $\log(R)$ has a finite mean and variance, but no covariance structure. The distribution is the hyperbolic secant distribution. This is also by a well-known result in statistics. It cannot really be a hyperbolic secant distribution because of the side cases such as bankruptcy, mergers and dividends. The existential cases are additive, but the log implies multiplicative errors. You can find an article on the hyperbolic secant distribution at 

So, if a distinction is made, as continuous double auctions are usually just called double auctions, then the difference has to do with frequency. It is easier to have an example. The New York Stock Exchange is an example of a continuous double auction. Within its trading hours, you can bid in either direction as much or as often as you want and so can everyone else. The best example of an ordinary double auction was the Arizona Stock Exchange which closed in 2001. It only permitted two bids per day, at the open of US trading and at the close. It sought to solve an institutional liquidity problem. Certain types of institutions are at risk if they cannot clear their orders at or as near to as possible a particular moment in time. Mutual funds, for example, usually only have one valuation per day and if there need to be a lot of transactions from redemption or purchase activity, then the least risk is to have the trade near that time. Most are at the open or close. The Arizona exchange found the single price that cleared the largest number of trades. You paid a fee to participate. There was no bid-ask spread. So possibly hundreds of customers would place their limit orders at the same time as buyers or sellers. What they did in practice was create single supply curve and a single demand curve and the price paid/received was the intersection of the two curves. 

Although I disagree with von Mises, he was reacting to what he saw as a very real problem, which was the use of relatively primitive statistical methods to describe behavior. In fact, Leonard Jimmie Savage's work on probability and statistics should have been sufficient to meet his objections, although the computational speed would not have yet existed for that to have been worthwhile. The challenge the Austrians have is that they are really arguing that no regularities exist in human behavior. They do not see it that way, but probability and statistics are about the discovery of regularities. Their position is that humans are too unique to be subject to statistical measurement. All of economics could be solved axiomatically, in his understanding. I would agree with him, as long as you got every axiom correct and the set was complete and properly partitioned for all possible cases. Of course, the only way you could know that would be empirically and that would defeat his argument. Although other branches of economics use axiomatic assertions, they then check to see if the predicted behavior really happens that way. If so the axioms are at least not disconfirmed. Most assumptions are also things nobody would disagree with. For example, there is a proposition that humans prefer some things to other things. If you also assume that for any pairing of goods, we will call them x and y, then either y is not dispreferred to x or x is not dispreferred to y, or neither or dispreferred to each other. If you add transitivity where for all triples x,y,z then if x is not dispreferred to y and y is not dispreferred to z then z is not dispreferred to x. If those conditions hold, then it can be shown by theorem that it must be true that a utility function exists such that if the utility of x is not less than the utility of y then y is not dispreferred to x. If those assumptions do not hold, then something else is true, but if they do hold then the entire mathematical theory of functions opens up to economics. With the addition of some other mild assumptions, then calculus becomes available. These do not need empirical checking because it is the only possible outcome, by force of math. To go much past this point, however, becomes tenuous. For example, must some of these functions be concave or are there circumstances where they could be convex. If they can be convex, do real-world examples of this convexity exist? Do real-world examples of the concave case exist? Are there forced implications from either case or the existence of both cases? These questions can only be approached empirically. 

The assumption of normality is not a required assumption for OLS. Further, the assumption is that the errors are normal, not the residuals. Nonetheless, there can be a material concern when normality is violated because it depends upon why it was violated. It only needs "fixed" if there is a reason that it was violated. There are a few specific cases where it would be a problem and in those cases, you may need to switch to Bayesian methods, as mentioned above. Let us imagine you were estimating the prices of automobiles. There is no reason at all to believe the errors would be anything but normal. On the other hand, if you were doing work on growth rates or financial assets then the errors couldn't possibly be normally distributed. To back up a little bit, there are three primary classes of estimators, the Frequentist, the Likelihoodist and the Bayesian. Frequentist and Likelihoodist methods are null hypothesis methods. Bayesian methods are not. You were most likely taught the Frequentist method. It seeks the minimum variance unbiased estimator. In the general case, the method of ordinary least squares is the minimum variance unbiased estimator. Under assumptions violations, it may be inconsistent or inefficient. In that case, robust methods are advised such as Theil's regression or quantile regression. The main alternative in economics is Fisher's Likelihoodist method. The assumption of normality is required to use ordinary least squares in regression with this school of thought. You need to know the likelihood function to find the maximum likelihood function. If it does not hold, then OLS is the wrong estimator. If you wanted to use the method of maximum likelihood and normality did not hold, then you would need to determine the likelihood function and take the first and second derivative to determine which method was actually correct. The Frequentist estimator is never a more accurate estimator than the MLE, but it can be worse. BEGINNING OF EDIT A good example of where ordinary least squares would not be the maximum likelihood formula would be in the logarithmic approximation of the time series $X_{t+1}=RX_t+\epsilon_{t+1},R>1$. In the log form as $x_{t+1}=rx_t+\varepsilon_{t+1}$ the likelihood function is $$\frac{1}{2\gamma}\text{sech}\left[\frac{\pi}{2}\left(\frac{x_{t+1}-\hat{\beta}x_t-\hat{\alpha}}{\gamma}\right)\right].$$ Ordinary least squares would be used because the MLE is not analytic and so Fisher's method isn't really viable. Ordinary least squares is usable for any linear relationship where all the variables have a defined second moment. An example where this would not be the case are returns on stocks which cannot have a variance, or a mean for that matter as they are the ratio of $$\frac{p_{t+1}q_{t+1}}{p_tq_t}-1,$$ where $p$ are prices and $q$ are shares. Assuming the firm did not go bankrupt or was merged out and ignoring liquidity, the likelihood function would be $$\left[\frac{\pi}{2}+\tan^{-1}\left(\frac{\mu}{\sigma}\right)\right]^{-1}\frac{\sigma}{\sigma^2+(r_t-\mu)^2}.$$ If you perform the integration to find the expectation you will find it does not exist. This can be derived from the fact that equity securities are sold in a double auction and that there are many potential buyers and sellers. You can also verify this against the disaggregated trade data in CRSP. If you control for liquidity, this will be your likelihood function. END OF EDIT The primary circumstance one would use either method is when there is no actual prior information about the true values for the coefficients in regression. The methods minimize the maximum loss that could happen by using the estimators in the real world. To use an analogy, using either of these two methods would minimize the risk of burning a house completely down to the ground from having a group of teenagers come over for a party. Bayesian methods, conversely, minimize the average damage expected to the house from inviting a bunch of teenagers over for a party. The Bayesian method doesn't minimize the specific risk of total loss but does minimize the risk of loss from all sources. Bayesian methods will require you to look at the prior research on the topic and capture this prior knowledge in a probability distribution usually called "the prior." You would use both the information in your data and the knowledge in the literature to estimate the location of the parameters. This tends to make Bayesian methods significantly more accurate than non-Bayesian methods, simply because there is more information included in the solution. It also makes them difficult to use. You should get supervision at first or do a lot of reading. Bayesian methods operate in the parameter space and not the sample space as Frequentist and Likelihoodist methods do. Data is not random, parameters are. There is no null hypothesis. You cannot falsify a null because there isn't one. There can be infinitely many hypotheses and the set of hypotheses must be mutually exclusive and exhaustive of all cases. Because you would be operating in the parameter space many problems that exist in non-Bayesian methods are not problems in the parameter space. To give an example, autocorrelation is a serious issue for non-Bayesian methods and not an issue in Bayesian methods. It doesn't make the estimate inefficient and it doesn't interfere with hypothesis testing. Of course, as you have seen there are a different set of problems in Bayesian methods such as constructing the prior. An important place where Bayesian methods shine is in prediction. Frequentist predictions depend upon two things. First, a pivotal value has to exist. Second, the maximum likelihood estimator needs to be representative of the true value. If both are true, then the Frequentist prediction can be as good as the Bayesian prediction, but if not will be far from reality. The Bayesian estimator does not require either of these. The Bayesian estimate is the distribution $$\Pr(\tilde{x}|\mathbf{X}).$$ Notice that there are no parameter estimates in there or parameters. It is not $\Pr(\tilde{x}|\beta;\mathbf{X})$. If you have never used a Bayesian method before, begin with William Bolstad's Introduction to Bayesian Statistics. You will need to know calculus through integration. This is because the estimate of the population mean, which is $\bar{x}$ in Frequentist methods is not the estimator of the population mean in Bayesian methods. For example, the estimate of the mean of an exponentially distributed variable is: $$\Pr(\lambda|x_1\dots{x}_n)=\frac{\prod_{i=1}^n{\lambda{e}^{-\lambda{x_i}}}\Pr(\lambda)}{\int_0^\infty\prod_{i=1}^n{{\lambda{e}^{-\lambda{x_i}}}\Pr(\lambda)}{\mathrm{d}\lambda}},\forall\lambda\in\mathbb{R}^+$$. The $\Pr(\lambda),\forall\lambda\in\mathbb{R}^+$ is determined by your research about prior information on the location of $\lambda$. As you can tell, this is not plug-n-play like you get for ordinary least squares. If I were you, until my skills in other methods developed, I would use quantile regression. It is plug-n-play. If you want to provide a little more information about your problem, it might also be possible to tell you if the absence of normality is an issue.