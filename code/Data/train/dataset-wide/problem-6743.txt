I'm writing some software that will need to do a bit of phonetic analysis. Are there any programs, libraries, or software packages that I can extend to my program which are commonly used for real-time phonetic analysis? I'm aware of software like Praat and WaveSurfer, but to my knowledge I don't think I can send/receive data from them in real time. Asides from those, other NLP tool-kits seem to output on the lexical/phrasal level, which is not the level of analysis I'm looking for. 

Parallel to these levels are disciplines like: psycholinguistics, cognitive linguistics, neurolinguistics, linguistic anthropology, computational linguistics, and speech-language pathology. (This list is by no means comprehensive!). These disciplines will use linguistic theory to answer specific questions. For example, a neurolinguist may use popular theories in phonetics and phonology as a framework to answer questions about phonological processes and their neurological realization in the brain. Now, to complicate things further--every single one of these levels of analysis have competing theories WITHIN them, and listing them all would be far beyond me. So to answer your question: tons! 

Lambda expressions are evaluated "hierarchically"--we resolve functions in the daughter node before we resolve functions in the mother node. In a given constituency, a sister node may define a variable for a lambda expression. But if there are three sister nodes, there will be ambiguity as to which order the variables get assigned. Does this mean that syntax trees must be binary branching in order for lambda calculus to work? 

But, as it turns out, that assumption is wrong. What defines a transitive complementizer is precisely the fact that it assigns accusative case to nominals, and not the other way around. 

How did the two languages develop similar structures (have-aux + verb-past participle), that mark different grammatical tense-aspects? Did one language borrow the construction from another and changed its meaning over time? Is it just pure chance? Or some other explanation? 

So, for example, think is usually pronounced [fĩk], then is pronounced [den] and with, [wif]. Since Brazilian Portuguese does not have dental fricatives, this kind of sound change is much to be expected. What is interesting about it is the regularity of the pattern. Among other "likely" candidates, such as [s] or [t], which are also sounds found in BP, [f] is — almost always — chosen to replace [θ]. Similarly with [ð], that could as well be replaced with [z] or [v]. Even those who have never heard other people speak that way will make this choice. So, my question is: can this sound change be predicted from Brazilian Portuguese phonology? What else could explain its regularity? In general, if a phonologist is presented with two languages, A and B, can they predict what sound changes are most likely to occur if the native speakers of A start speaking B as a second language? 

In English and other Germanic languages, noun compounds are formed simply by “appending” the nouns in a certain order. For example, phrases like this are very common: 

I've seen diacritics corresponding to tones (in tonal languages), but asides from that I haven't come across a system for transcribing prosody in my studies. Is there a popular convention people use? 

In english, taboo language may be realized as swear words--though I could see some languages not having "swear words" in the english sense, while maintaining "vulgar expressions". All cultures have taboos. However, do all cultures have linguistic taboos? 

I often see a noun such as "Jimmy" in my textbook represented as [DP_Jimmy]. But there is no more detail provided as to its internal structure. This means either the D is null such as: 

I hear that only complements and specifiers can contain the argument for a verb. But there are certain structures with ditransitive verbs I believe you can represent as an adjunct. Here's an example: "Bill gave Jessica a gift" 

I think that this really boils down to level of analysis. (Formal) semantics asks: are there atomic units of meaning, and if so, how do they combine to form linguistic meaning? Of course, this is not a full explanation of linguistic meaning, as not all meaning in a sentence can be explained by reducing it to its atomic units. There are other studies of meaning, such as pragmatics that aim to address dimensions of meaning that is non-compositional. So you can consider "semantics" to be a disciplinary term within linguistics, and "meaning" to refer to the broad phenomena of meaningfulness. 

Is this a scientifically valid assertion? If so, how do you measure the cost (and thus the economy) of the pronunciation of a certain sound in a language? 

As I understand from the principles and parameters theory, all parameters are binary. In particular, the head directionality parameter can be set to either "head-first" or "head-last". The setting of one value or the other determines, among other things, which kind of adposition the language will have. Head-first generates prepositions (e.g. English). Head-last generates postpositions (e.g. Japanese). But Finnish is an example of language that has both prepositions and postpositions. What am I missing? 

Since these datasets are is licensed under a Creative Commons Attribution 3.0 Unported License, you are free to share, remix and make commercial use of them, as long as you obey the conditions specified in the license. 

This means that, in some environments both [w] and [l] can be found, as in the [a.'kli.vi] versus [a'.kwi.fe.ru] example. So, evidently, I'm failing to understand what "complementary distribution" really is. Any help to clarify the concept would be much appreciated! 

The comparative method is used to reconstruct unattested languages from the attested ones. By comparing different sounds for the same words in various sister languages, it is possible to infer some phonological rules. The internal reconstruction method, on the other hand, compares variant forms within a single language to discover how they evolved from a past regular form. Would it be possible to combine these methods to "look forward", so to speak? For example, comparing several dialects of a language with how they were fifty years ago and then extrapolate this data to predict how these dialects will change in the near future? Of course, some degree of uncertainty would have to be allowed. 

For future people who may need this, I ended up writing a webscraper and compiling everything from www.minpairs.talktalk.net. You can view/download the corpus here: $URL$ 

I've seen [+constricted glottis] described as encapsulating ejectives and implosives, but the feature matrix according to Hayes (2009) (that I pulled from his personal website over here: $URL$ only lists the glottal stop as being [+c.g.]. Is there disagreement here? Or an error? 

I suppose, like many things in linguistics, this may vary depending on your framework--but which one is more acceptable? 

Say you're analyzing a completely "new" unstudied language, and you come across a set of sounds [g],[k] that are in complementary distribution and that you suspect are allophones of the same phoneme. What do you then call this phoneme? Do you just ask the native speakers what sound they think it is? Or look at elsewhere conditions? Why call it /g/ over /k/, or not something entirely different? 

I don't quite understand why having an operation that takes two elements and combines them into a set is particularly useful in describing language. Perhaps it helps in describing how it could be that these strings are generated, but why is this any better than, say, phrase structure rules? 

In previous syntax courses, I could have sworn that adjuncts were drawn as being daughter of X' and sister to X', so there are two X' levels Eg: 

Phonetics - studies the physical speech signal. Sound is interpreted in the brain, which leads us to: Phonology - studies the how speech signals are perceived, as well as mental categorization of sounds. These conceptual sound categories may combine to form morphemes, which leads us to: Morphology - the study of the smallest units of meaning (e.g. "book"+"-s"="books"). Morphemes may combine to form words, and those words combine leading us to... Syntax - the study of "sentence level" structure--how words combine to form phrases, and those phrases combine to form sentences. Of course, words and phrases just so happen to have meaning, this leads us to... Semantics - the study of meaning, both at the lexical and phrasal level. But of course one word may have slightly different meanings in different contexts, which brings us to: Pragmatics - the study of meaning in context. 

Khoisan languages have click consonants, a feature that has not developed independently in other languages. Bantu languages, for example, have borrowed them from Khoisan. So, this was probably some sort of an "early invention" that was not very "fit" and got abandoned along the way. If you look at a map depicting where Khoisan languages are spoken, you will see a major area over parts of Namibia (eastern part, mostly), Botswana and South Africa and two small pockets (one in central Tanzania and one in the western coast of Namibia). These small pockets suggest that Khoisan languages once dominated most parts of Africa and then got replaced by Bantu languages (which, in fact, sorround those pockets). Genetic and archeological evidence suggests that Homo Sapiens originated in that part of the world. 

Any language has a formal variety, primarily (although not exclusively) used in writing, and one or more informal varieties, used in everyday speech. Yet, for some languages, like Norwegian and Arabic, these two varieties lie so far apart that they are considered two distinct dialects. According to Wikipedia, this situation is called diglossia. This is not the case (as far as I know) in most English speaking countries, for example. So, where do linguists draw the line? What are the features that must be taken into account to consider a certain society diglossic? 

In Brazil, the Portuguese dialects spoken in rural areas preserve, despite their own innovations, several features of the language that were common in the 16th century. This phenomenon is particularly evident in the so called “caipira dialect”. These conservative features include phonological, lexical and semantic aspects, and are considered archaisms in the modern standard dialect. On the other hand, a number of innovations in Brazilian Portuguese originated in the more urbanized areas (southeast region, especially São Paulo and Rio de Janeiro) and then spread to the rest of the country. On another geographic scale, Brazilian Portuguese as a whole is considered rather conservative, when compared to European Portuguese. Historically, until the 19th century, Brazil remained as basically an agrarian economy, while Portugal was the urbanized colonial center. Evidently, this is not a scientific observation (so it may be strongly biased), but it seems that urbanization and the rate of language change are somehow correlated. Is that a fact? 

I've been using the two terms interchangeably. One of my assignments is asking me to identify cases of both suppletion and irregular inflection. I've been going over course notes/google to no avail--they seem like the same thing to me. The only bit of difference is that one seems to have a more synchronic focus, and the other a more diachronic one...what's up with that? 

Part of the difficulty surrounding donkey sentences, to my understanding, is about how hard they are to translate to FOL in a matter that is consistent with other translations to FOL in english. Take "every man who owns a donkey beats it". The knee-jerk translation would look something like this: 

This is problematic because y is free in the consequent. But now say that we extend the scope of the existential quantifier so it reads as follows: ∀x(MAN(x) -> ∃y[ ([DONKEY(y) ∧ OWNS(y,x)] ∧ BEATS(y,x)) ∧ DONKEY(y) ]) 

I can see how SL can be a decent metalanguage for doing cross-linguistic semantic work, but I feel like it's severely limited by the fact that you can't translate any kind of non-declarative sentence. This is means that only tiny fraction of any one given language can ever be logically represented. How is symbolic logic useful in linguistics? 

Categories like "Noun" and "Adjective" can be said to be lexical categories. But what are categories like "subject" and "object" called? 

This may be the wrong place to be asking this, but I'm not entirely sure what kind of advantage using a language like Prolog gives you over an Object-Oriented language like Python. Also, is there a reason why Prolog is popular in both linguistics AND ai? I assume the two fields are connected. 

The words this and that are demonstrative pronouns. The sentences "I would be more tired" and "I actually ended up being" have the same syntactic function as "this" and "that" would have. But, being full clauses (instead of simple pronouns), they are classified as subordinate or dependent clauses. In your examples, they function as direct objects of the verb "think". Note that in your second example, the word "that" is also being used to introduce one of the subordinate clauses. But, in that case, it is a subordinating conjunction. 

Among the descendants of the Latin word sic ("thus, so, or just like that"), only the Portuguese word sim ends with a nasal consonant. Actually, in modern Portuguese, it ends with a nasal vowel, [sĩ], which must have developed from [sim] by assimilation. Compare this with its "sister" words, taken from the Wiktionary, which have all dropped the final [k]: 

As far as I know, language families originate from a process of divergent, tree-like evolution. All the languages within a family or subfamily can be traced back to the same proto-language, which was spoken several centuries ago. At some point, the various regions in which that proto-language was spoken started developing their own particular innovations. This process went on and on, to the point that, after some time, they were not mutually intelligible anymore and could thus be considered as “different languages”. But is there any case of the opposite trajectory, i.e., languages that started out very different from each other, then started to converge because of language contact and ended up very similar, much like actual language families? Now, I am aware of the existence of language areas (Sprachbunds), in which languages share several features by virtue of the high level of contact among them. But even in Sprachbunds, languages maintain their familial “identity”, so to speak. In the Balkans, for example, we can talk about Slavic languages, Romance languages, etc. There is no such thing as a “Balkanic familiy”.