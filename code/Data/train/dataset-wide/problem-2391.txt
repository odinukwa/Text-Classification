You create clustered index on only one column. Technically, you can do it on multiple, but the main goal to have it as short as possible. If you created clustered index on you DO NOT NEED any other indexes on that column. I can guess that your performance improved because before there was no Clustered index and indexes were so bad that SQL decided to do full table scan instead. Suggestion: read a book about Indexes, their differences and how they work. 

Your provides 2 sets of the data: - DB Size + Unallocated space - These numbers include BOTH: Data and log file; - Total statistics of RESERVED space for all objects within the database; I bet your 36 GB of free space are in the Log file. For real numbers use following query: 

There are 2 options: 1. Create clustered index by account_id. Delete all other indexes. 2. Add new column and create clustered index on that column. Create another non-clustered index on ONLY one column . The second option is more preferable because it is much faster. You join only by 4 bytes. While your current queries join by 40 bytes. That means your current operation is 10 times more expensive. Also, ask yourself couple of questions: - Do you really need have Account_ID as Unicode? - Can you convert Account_ID to INT or BIGINT? Hopefully, you've got my point. ADDITION: 

I want to see the size of a spacial index. I am usually using one of the following code blocks to get the indexes sizes of a given table, but none of them is working for a spatial index. 

I have on virtual machine and want to install SQL Server Integration Services. On the drive is mount. I am reading a book where it is said that: 

execution plan - slow query: $URL$ execution plan - fast query: $URL$ The queries are executed on the same hardware. 

I want to install SQL Server 2014 and editions on two separated servers with same hardware in order to test the performances differences running internal set of procedures. The DBA gave me two s with the following name: 

and I am wondering which is the best value of rows to be deleted in a batch delete statement? I have seen different variants and the only thing I have found as recommendation was from here: 

I want to query spatial data for nearest neighbor. I am using this article and the following query works perfectly: 

You did not get the point of SQL programming. You can have 1000 copies on multiple machines of a script to create/alter an object, but until one of those scripts is executed against a server, that object on that server will be unchanged. Yes, you can have multiple tabs/windows with the same procedure, but for SQL Server all of those are different/competing sessions and whoever is last - wins.On another hand: SSMS. You can have multiple copies of a script of the same object in different tabs, which is very handy - you can try different scenarios just by switching tabs. It is not job of SSMS to track your changes and keep records which script is a "Master Copy". That is YOUR responsibility. However, if you are in a project development, you and your boss can't completely rely only on your discipline. Project manager have to choose which way to do a source control. Here is a not full list of things, which can be done: - Implement Change Data Capture on the server; - Restrict permissions on Dev/Test and allow "playing" only on Dev or personal machine; - Use change tracking/deployment/source control tools, which will keep records of any changes. And do not be surprised with SQL. All developers are experiencing that issue with source control. See it here: $URL$ 

The result was the same. So, it is not true that each row is occupying at least 8 KB, but the size is actually increased. I know that: 

Add column to the table called with default constraint and Rename the current column to Add new column called Add new computed column with the original column name which will return the if flag is , otherwise the By default a query will return encrypted data - in order to decrypt it, additional logic will be implemented 

I have found this Why is GETUTCDATE earlier than SYSDATETIMEOFFSET? discussion about differences between old and new date time functions. Then build the following query: 

I am testing how one of our stored procedure is working on vs editions. I have created two virtual machines and install a () and restore a database on each instance. Then I have generated some test data (large volume but the same on each database) and started testing if there is an execution time difference. I have been told that there should be better performance on the edition when a large volume of data is used because there is NUMA Aware Large Page Memory and Buffer Array Allocation. So far, there is no such difference and the execution time is almost the same (a or second difference). I cannot say I understand completely what is and how it works, but I guess the hardware is on as the following query returns me and : 

Runs successful! There were NO permission changes but I guess powershell needs a [temporary with PS-Drive] local path to run. Again ZERO permission changes and if I were to replace the "P:\" with the "\\network share\Results" it would fail. This is what I needed. The network share name will not be changing but a drive letter on the server can change so this is maintainable for me and reusable in my other Jobs. 

I have a SQL Job step running powershell that cannot find a network drive in one of its steps For brevity, I am calling this server Production, the network share is D:\ on Production and the Job is running on Production. The powershell Job step with issues in running under the SQL Server Agent Service Account. The first Job step creates an input file on a network share. The second Job step queues an email based on information in the first steps output file. This job is run on server we will call Production. The network share is based the server were the Job is run specifically what we will call here the D:\ drive. When writing the Job step if I use SELECT-STRING .. -Path "\\network share.." my job fails it cannot find the Path, but if I use SELECT-STRING .. -Path "D:.. it runs successfully! I kept the Job tokens in the code below in case the option for creating a script comes up. I have a solution here, but there is a possiblity I will need to use this script on a different server that does not a have a D:\ and I want to use the network share name for maintainability. First Job step creates file(run as CmdExec): 

Might happen you still have some opened transactions, which hold you from shrinking individual files. See who hols them, close them. Reboot server if necessary. Shrink individual files while nobody accessing the database. switch it temporarily to Single User if necessary: 

Finally found an answer: You can create database user, which won't have a SQL Login and won't be able to authenticate, but will have permissions and can be specified in clause for stored procedures and functions. The simple way to create such a user is just to use clause during user creation: 

In your sample case SQL Server made very good choice. Here is what it was actually doing: - In the first set of data it extracts 10 rows from table based on the column. Then it extracts 10 corresponding IDs from table using kind of operation for each ID. - In the second scenario, when there are 100 matching rows in table SQL decided not to do operation, but use instead, which is much cheaper from the I/O perspective. Yes, you can use hints for your queries ($URL$ 

I think I have found how to fix my issue, but I am not going to accept this as answer as I am not able to explain what is causing the problem and guarantee this will work anytime. It's fix found after a lot of testing and I will be glad if someone can bring more light here. 

I have a primary database in recovery mode which is part of group. Is there a way to minimally log insert operation under recovery model? I have a process that is executed each day and insert few millions of records in a table. While the operations continued the transaction log file size is increased dramatically ( from 1 GB to 40 GB). As I have read I can used some variations of which are not fully logging the operation but I am concern about the effect of switching the recovery model? 

The history table columns will be and I am going to save a lot of space versus ordinary implementation which is logging all data (this is due to my test and my bussness cases). As I the supports I am wondering are their any pros/cons/differences between using it and trigger-based logging? I have check few artciles (here and here) and cannot see what more can give me. 

Can see it only on my SQL2014 on Win8.1 VM. Error persists even when run query locally as local admin 

Yes you can. Use SQL $URL$ By doing that you'll have a full control behind the curtains, but please fully document it and be consistent. It is a nightmare to troubleshoot databases with synonyms. As an alternative you can use dynamic SQL, it might be more transparent. 

The problem occurs ONLY when I switch default port to another value. When return its value back to 1433 everything goes back to normal. During the execution could capture following waits: 

However, there are plenty of objects in the database, which were created with credentials of that account. I've tried to run one of these procedures and it was executed successfully. When I've tried to recreate that scenario in my test database it returned me an error: 

I'm not a replication guru, but record may be marked as deleted only in case when an update is generating Page Split. For the case of replication you can test it by looking at DBCC PAGE and look through transaction log.