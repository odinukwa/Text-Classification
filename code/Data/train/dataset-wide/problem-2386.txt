Come clean ASAP. The sooner you do this the better. If I'm reading the timeline correctly, when you restored the backup over your production database you broke the backup chain. This means you wont be able to get the data for 06:30PM to 10:00PM into your current database that holds data for 01AM to 06:30PM + new data since the . I would suggest you the database with 06:30PM to 10:00PM data and figure out a T-SQL code based way to move that data into your live database. You may have problems with columns as new rows in both databases have probably taken the same IDs. IMO the quicker you get other people involved to build a plan to fix this the better. Good Luck 

Dynamically creating multiple triggers for individual rows sounds quite nasty. This is not likely to scale well. After reading your update I would suggest you implement 1 trigger that fires for all CRUD operations on the table. This trigger checks to see if the modified rows of are in the table. If they are send a notification, if not don't send notification. 

A separate AD service account for each service on each instance is the most secure model and if you have some good password management software it shouldn't be a problem. The work involved in creating all the service accounts and entering them into your password software will be quite time consuming depending on how big your environment is. Some people sacrifice some security by using the same service account for all the services on a SQL instance. The problem with this is if the account gets locked out then all the services including the SQL service account will be locked out. This problem becomes even worse if you use the same AD account across SQL instances as one instance can be affected by another. Also obviously if an account is compromised, the more services it is used for then the more services that are compromised. 

You could have separate tables for product type and product catalogue to get to 3NF but that's a decision for you to make depending on the amount of different values those columns could have and the way in which this data will be accessed (OLTP only or heavy reporting used here too?). 

From reading your question I think you are building the SQL code in your application and then sending it to the server as one long script with multiple statements. If this is the case you can separate your statements with GO to create batches. The following is valid SQL: 

If you do need to persist the columns then use this statement to create a VIEW UPDATE You would need to upgrade to SQL 2012 to do this though. 

Trace flag 5004 is useful for starting/stopping TDE. Might be worth giving it a go in case it helps. DBCC TRACEON(5004) GO DBCC TRACEOFF(5004) GO 

The most comprehensive way in my view would be to encrypt/decrypt the database with TDE. This will ensure that each and every page will change in memory and will be flushed to disk. I've tried this with success on 'legacy' dbs that were originally created in SQL2000, after I discovered that several pages didn't have actual checksums on them (0x...200) if you look at the header with dbcc page. If you were to try this, I would recommend testing it on a restored version of the live db, just in case you have undetected corruption that could be caught and stall the encryption process. There are flags to deal with it, but better play it safe. Obviously you'll want to backup the certificate used by the encryption, so you are covered for any eventuality during the time the db is encrypted. If anyone has a better idea for writing checksums on all pages, I'd love to hear it :-) 

SSDs tend to have a natural 4K sector size as far as I know, but would it be beneficial to format Windows cluster size at 64K? This is good practice for spinning rust, but is it relevant on SSDs? 

Performance advantages of CCIs are not only space-related: batch execution mode is also there to speed things up (in supported operators). Batch sizes can vary from 64 to 900 rows, so it would be reasonable to expect that using smaller datatypes would lead to 'fuller' batches, closer to the 900 max figure. $URL$ Being economical with datatypes is a good habit anyway - why consider bigint if tinyint (or bit) would do the job? 

There is enough info for you to estimate roughly how many pages/extents were lost (deallocated by the REPAIR_ALLOW_DATA_LOSS option). What good is that though? Without backups there is no natively-supported way to recover the data. What logs are you referring to? Transaction logs or Errorlogs? TLog entries need to be interpreted (not for the faint-hearted) and then applied to a consistent database file (which you haven't got). Errorlogs are useless for data retrieval anyway. 

It would probably had been slightly better if you had a version of the database before running checkdb with REPAIR_ALLOW_DATA_LOSS. I feel for you - can't be obsessive enough when it comes to backups... 

Long shot perhaps, but worth checking file \Program Files\Microsoft SQL Server\100\DTS\Binn\MsDtsSrvr.ini or the equivalent on your setup. You may have to manually edit it with the instance name. Otherwise SSIS connections might be looking for an msdb of a default SQL instance that doesn't exist. 

You would need to build a custom backup script which uses dynamic SQL to build the name of the .bak file. The maintenance plan could execute this with the 'Execute T-SQL Statement Task' item. The T-SQL would look something like this 

This looks like a lot of work but I have done it in the past and it is quite simple. You basically create a HTTP endpoint in IIS that your excel clients connect to. The endpoint allows basic authentication while IIS authenticates with the cube on a predefined account. Full instructions here 

I don't think you need a sub query here. Does this give you what you need? If so it should be more efficient. 

If I understand you question correctly you could use a schema compare tool to generate a script that would be used to insert the extra SPs and triggers into the new datawarehouse DB. Redgate have a tool like this in their DBA bundle and I think data and schema compare are now functionalities of visual studio. 

Any GUID for an identity column will be an issue due to the size of it. This leaves less space in each page for actual data which means your data density is lowered and you are likely to see more page splits and fragmentation. The problem when you use non sequential GUIDs on an id column with a clustered index is fragmentation. If you had 100 rows in the table with batches of 20 with sequential GUID values as the id and then wanted to add a random id value the engine may need to find space in the middle of your table to insert that record instead of just adding it to the end. Depending on your fill factor level this could cause page splits which cause index fragmentation. Lower the fill factor of the index can help with fragmentation but not as much as always sequential values. This way each new record can be added to the newest page or a new page can be added at the end without the need to reorganize existing data. Check here for more info. 

A table can contain a maximum of 8,060 bytes per row but and columns can be stored off row to provide more space. More details here 

When it comes to SQL Server index maintenance you cant beat this maintenance script. $URL$ I wouldn't use the profiler to decide anything like this. If you want to write your own simple maintenance plan you need to look at the index DMVs. 

In 2014 you get the new cardinality estimator which could see your queries get better plans and therefore execute more efficiently. A good reason to upgrade from 2008R2 is that it is on extended support meaning it is unlikely to receive anymore updates and only SQL Server 2012 SP2 + versions are able to operate with TLS 1.2 see here 

I'd be very careful using this flag on a VM, as their memory has an extra level of abstraction. Had more than enough trouble with it on physical servers with lots of RAM dedicated to SQL. Example: with 3 2008R2 instances co-hosted when restarting one of them it took forever to come back because it could not find contiguous memory segments anymore. The performance benefits were neither here not there (lets say 'statistically insignificant overall). I treat it as a 'TPC special'. Also consider that 834 doesn't play nice with columnstores either. 

Shrinking log files for user databases might be OK if internal fragmentation is becoming an issue (search on Virtual Log Files for detailed info). Best to avoid it by setting up appropriate autogrowth sizes, avoiding growth in percentages as well. The defaults are inappropriate for any realistic data load. Shrinking tempdb log files may not be a good idea at all, I have even seen data corruption that I believe resulted from a job that was regularly shrinking the tempdb log file. Best to pre-size it according to the workload. 

Yes, but only from SQL2012 onwards, if I remember correctly from Bob Ward's 2013 PASS session (gave me a headache!) 

Unfortunately, as is the case with other counters, the definition of 'reads' is not identical across the board. If a plan has calls to UDFs, statistics IO may under-report them (or hide them completely) while profiler still displays them. Regarding the PLE counter, if the server has more than one (physical) NUMA nodes, it is important to use the ones under 'SQL Server: Buffer Node'. The PLE figure under 'SQL Server: Buffer Mgr' is an average of the Node PLEs, and can be hiding horrors sometimes (I've seen PLE of 400 on one node and 7000 on the other, with SQL supposedly using both nodes). 

Just came across this 2-year old link: $URL$ It implies that 64K NTFS cluster size is still recommended for SSDs To improve this answer it would be ideal to hear from real-life experience with latest generation SSDs (FusionIO or SATA-controlled). Maybe 256K is even better for columnstores on SSDs! 

If performance is important, Option1 has the clear edge. Every time a query goes across a linked server, performance takes a hit as what you expect to behave like set-based operations become serialised by OLEDB. They go row-by-row. It helps if you follow best practices for linked server queries (OPENROWSET for example) and ensure all remote processing is done on the other side, but the results will still come across serialised. (Look for Conor Cunningham's presentation on Distributed Queries). If all your remote tables are small lookup tables this may not be a serious issue, but if millions of rows have to come across linked servers then performance will suffer. There used to be an extra layer of problems with invisible statistics (for data readers) over linked servers, but as you're running SQL 2014 this will not affect you. If you can afford the Dev cost of eliminating the linked servers, Just Do It!