You should use Query Store to measure query performance for Azure databases. Note: Query store is not supported (Thanks @wBob !) The only reliable way is to use below query that I got from the portal 

Do not copy to root directory as @AaronBertrand mentioned. As a side note, why are you using SQL Server to do filesystem tasks? This should be done by PowerShell e.g. 

is at the transaction level or at the session level while is a query hint. You mentioned that you fully understand dirty reads, so using at the transaction level is what I would recommend. if you want dirty reads on some tables only, then NOLOCK hint will help you. SQL server 2005 and up allows you to use SNAPSHOT isolation where readers don't block writers and it uses row versioning with some penalty on tempdb. essentially SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED and NOLOCK are same thing but the use depends on the context of what you are using. 

Best is to test it as Netezza does not support varbinary. So it has to be converted to a supported varchar(16000) or VARCHAR(64000), VARCHAR(n), NVARCHAR(16000), NVARCHAR(n). Test your migration plan before hand. 

If you go to server properties, in the database settings, you will find the default database data, log and backup locations : 

Above answer covers this question. A failover to 2016 will break replica sync status. Once you upgrade to higher version, the boot page gets upgraded which prevents tlog restoration of higher version to lower version. 

SQL server does not trace revoke or deletes by default. In order to trace those activities, you need to run a server side trace (or XEvents as Aaron pointed out). Below trace will help you to find command (Text), start date, end date, login, database (I have filtered it for ), and many other fields. Adjust the filters as per your need and environment. Also, specify how big the trace file would be through . 

Thats not possible. The user that you are using is having default schema of . Just change the default schema of the user to so any new objects will be having a default schema of . 

updatable subscription is deprecated. If you still want to implement it, you have to use T-SQL. You wont be able to do it using GUI. See Updatable Subscriptions for Transactional Replication and Deprecated Features in SQL Server Replication 

For now just run it on the Primary database server. When you do a failover, you can run it on the mirrored server. e.g when you deploy it on mirrored server, you (refer to: $URL$ 

Why not install another instance of SQL Server e.g. a named instance called or something similar that suits your needs. BTW, there is no Windows 2007 version. 

Gentle reminder to read - Bad Habits : Using MDF/LDF Files as "Backups" If you are fully aware and have a situation that you cannot avoid, you can use dbatool's - Mount-DbaDatabase 

Yes it has to do it again .. scan and fix the errors with potential data loss. Be cautious, use it as a last resort - repair & allow data loss ! Refer : Corruption .. Last resorts that people try first… from Paul Randal. Remember : SQL Server 2000 is out of support .. so microsoft is not going to help unless you upgrade to a supported version ! 

It allows you to leverage automation and has a lot of flexibility in-terms of different parameters e.g. blocksize, compression, etc that you might end up writing when deploying the software to many machines. See the comparison chart : $URL$ 

Based on my understanding, the master server (MSX) cannot be a target server (TSX) that receives jobs from another master server i.e You cannot list one MSX server into another MSX, you have to defect it and make sure it is not enlisted with any other master server. Its a simple two-level hierarchy: a server is a master server, a target server, or neither. Cannot be both at the same time. FROM BOL : 

For the sake of completeness, I would add using method : I will use the same fiddle written by Leigh Riffel. 

e.g. The PS below code will move all the databases (overwrite if the same name databases exists since it uses parameter) from Source sqlserver\instance to Destination sqlcluster. It will exclude databases. It will migrate all the logins except , will migrate all the sql agent jobs, will export the sp_configure options from source to destination server. 

The sum of operator costs is more than 100% in execution plan is a known bug and is closed as by design ! Also, AaronBertrand filed a similar bug - SSMS : Execution plan sometimes exceeds 100% If you want to understand how plan costing works .. Paul White explains it at his best here. From the query processor team - What’s this cost? General guidelines for better performance - 

No it wont change/incement the values of timestamp column. altering a collation is specific to that column only. From BOL : 

IMHO, rather than just relying on SSMS to generate change scripts for databases, you should look into some sort of deployment framework that can be tweaked as per your need and can be automated. Refer to : Deploying Database Changes with PowerShell Read up on : 

You are missing creating the base result set - which is the highest level in the hierarchy. This is identified by . So if you might have a CEO that does not report to anyone, he will have it set to NULL. 

EDIT: Below is an excerpt from msdn /Conf[igFile] filespec (Optional). Specifies a configuration file to extract values from. Using this option, you can set a run-time configuration that differs from the configuration that was specified at design time for the package. You can store different configuration settings in an XML configuration file and then load the settings before package execution by using the /ConfigFile option. You can use the /ConfigFile option to load additional configurations at run time that you did not specify at design time. However, you cannot use the /ConfigFile option to replace configured values that you also specified at design time. To understand how package configurations are applied, see SSIS Package Configurations and Behavior Changes to Integration Services Features in SQL Server 2008 R2. You have asked on How to override SSIS 2008 package config file path? What you are talking about is SET switch -- Overrides the configuration of a variable, property, container, log provider, Foreach enumerator, or connection within a package. I have been dealing with SSIS dev's and I use /CONFIG when deploying it to PROD .. provided the paths are same in the package. EDIT: 2 Agree with OP that the behaviour has changed in 2008 and up : In SQL Server 2008 Integration Services, events occur in the following order: 

We create the jobs (and sync them as well) across all the instances that are part of AlwaysON AG. There is an additional logic that you should put in the job to check if the instance is primary and then only run the job. A pseudo code would be like below : 

why isn't it possible to access the data directly from the table discarding the B-tree? (most likely by scanning the table row by row) wouldn't that be more appropriate than inaccessible data at all? To answer your question, Indexing basics comes more handy -- An index is made up of a set of pages (index nodes) that are organized in a B-tree structure. This structure is hierarchical in nature, with the root node at the top of the hierarchy and the leaf nodes at the bottom. For more details refer here. Also, as many people have described, Clustered Indexes == Original tables which are physically ordered with one or more keys or columns. So, when a clustered Index is disabled, its data rows cannot be accessed. You wont be able to Insert any data (for Non Clustered Index the Insert will succeed -- but that is not entirely related to this post -- as here the discussion is of Clustered Index) as well or neither Reorganize operation will work. Below will explain you in detail : we will use Adventureworks database to see the effect of disabling the CLUSTERED Index. 

You can use 3 part naming . Below is an example to get you started. You can modify as per your needs : 

To enable verbose logging you must add two parameter values to the Run Agent step of the replication agent job for which you wish to review more detailed logging: 

Download PSCP (an SCP client, i.e. command-line secure file copy) Make sure to convert the files first on Linux server to windows format and then download to Windows using PSCP - to remove the from the files since they are generated on Linux server. Now you can even use your SP or just use SSIS to bulk insert those into SQL Server. 

Note: For completeness of this answer, even though Database Mirroring is announced as Deprecated, it is still supported in SQL Server 2012. You have to be careful, as the database snapshot name will be different from the original database name, so if your app relies on the database name, it will need modification. Also, as snapshot works on "COPY ON WRITE" method, it can be significant overhead if there are multiple snapshots. Also, it will randomizes IO during query processing. 

This can be found by reading the boot page of the database. Note: DBCC PAGE is undocumented. I have not seen any side effects of using it, but since it is undocumented, use it with caution. 

Yes AG support multiple subnets as described here. Also make sure that your data provider supports MultiSubnetFailover .. .NET Framework 4 supports it. To answer your question ... IF you use .NET framework 4 or 3.5 then the provider will support it as described here. Also, a good reference to SQL Server Multi-Subnet Clustering is well documented. With legacy client libraries or third party data providers, you cannot use the MultiSubnetFailover parameter in your connection string. To help ensure that your client application works optimally with multi-subnet FCI in SQL Server 2012, try to adjust the connection timeout in the client connection string by 21 seconds for each additional IP address. This ensures that the client’s reconnection attempt does not timeout before it is able to cycle through all IP addresses in your multi-subnet FCI. 

Note: If you are going to use any formula or online calculator to calculate SQL Server memory configuration then best is to read Beaware of Wow… An online calculator to misconfigure your SQL Server memory ! - by Jonathan Kehayias first. Some good references : How much memory does my SQL Server actually need? - by Jonathan Kehayias 

Third party software are : ApexSQL Log. It has free trial as well. Toad for SQL Server - It has Log Reader management which Rolls back transactions in the transaction log without the need to restore from a backup Native to SQL Server are below methods : you can restore your Database from a backup, and then RESTORE Transaction LOGS to a point in time with the STOPAT = '6/10/2013 12:30AM' argument. Read this excellent post from Paul Randall - Using fn_dblog, fn_dump_dblog, and restoring with STOPBEFOREMARK to an LSN 

In the import wizard, you can delete the rows first and if you have identity fields, then you can enable identity insert on as below 

If table or view data access mode is chosen then your triggers should fire normally because the insert is done row-by-row. 

Unfortunately, does not have any parameters for specific database or objects. Best is to rebuild the index. 

Why are you reinventing the wheel .. as there is a cost to reinvent it ! You are also using deprecated stuff e.g. .. instead use . You should use - 

You dont have to manually cleanup the old/static snapshot files, the distribution cleanup agent will purge the old folders for you. A bit of under the hood story of what and how snapshot files gets generated : Basically, the snapshot agent creates snapshots for the subscribers to initialize. The snapshot folder will contain various types of files like - 

The function breaks an input set down into sized groups. To determine how many rows belong in each group, SQL Server must first determine the total number of rows in the input set. If the NTILE function includes a clause, SQL Server must compute the number of rows in each partition separately. Once we know the number of rows in each partition, we can write the NTILE function as 

This is a big security risk, since your "SA" password is a known password in your organization. "SA" is the most elevated privilege account that if misused will cause a disaster and/or data breach by internal people. I agree with @AaronBertrand, that you should not give "SA" password out to people you do not trust. IMHO, "SA" should be disabled (or atleast change its password on frequent basis), since there are other ways wherein you can give people access following principle of least privilege to get their work done. SQL Server does not have LAPS (like windows server now has), so you have to design and implement a process that changes "SA" password on frequent basis. Below are some things that you can do to limit use (see reference cited below for more information article by - K. Brian Kelley): 

I dont see a need for 1. You can use below script to move logins from one server to another. It uses SQLCMD and xp_cmdshell. You can also look for a PowerShell option if you dont want to use xp_cmdshell. 

So, you have to drop the constraint, partition the table and the recreate the costraint. Better to do as a transaction, so on success it can be committed or on failure it can br rolled back. Step by step tutorial can be found at : $URL$ And $URL$ 

Note: By default, the generated scripts will have => the subscription will need initializing as a brand new subscription. Instead change it to => does not generate replication procs at subscriber. Obviously, logins and users with appropriate permissions should be created before hand. References : 

Given above points, SQL Server does not have an easy way of detecting what users/applications are using "SA" to connect. I have implemented a light weight server side trace that can be used to find out applications/users/host name that uses "SA" to connect. Script can be found here and below is the sample output : 

It depends on what you asking your DBAs to trace (I am only talking about server side trace). We have a 24x7 trace running which captures minimal stuff which helps us in troubleshooting specific scenarios e.g. User error messages, logins ,applications which can be useful when we want to move the database to a different server or when we want to decommission a server, etc Being a DBA, I would not run a trace that will capture execution plans, statement level executions, etc. These events can bring down your server. Instead you can use sp_whoisactive - log it to a table or use Glenn's DMV queries using tsql agent job on a regular schedule. This will help baselining your server and spot anomalies easily. In his blog post Measuring “Observer Overhead” of SQL Trace vs. Extended Events - Jonathan Kehayias concludes 

No tempdb physical size does not matter (you should properly (equally size) you tempdb files (not more than 8) with TF 1118 and 1117 enabled). 

If you really care to know which databases are in use, you have to use dmv's (log that data to a table as they get flushed out when server is restarted) or profiler (server side trace) and just filter out the logins and program names connecting to those databases. once you have that info for your complete business cycle, you will be able to figure out what databases needs to be moved to new server. also, backup/restore should be a preferred method unless you have a reason of not doing it. note: we do not know your environment, nor your business so we CANNOT answer what databases you need to move. reportserver* are eelayed to reporting server that might be installed on the current server.