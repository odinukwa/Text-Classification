The reason that localhost is acceptable would be because it is using a default instance of SQL Server. The default instance, by name, would be MSSQLServer, however you do not need to provide that. Previously the express was a named instance, which requires you to pass along the instance name (or port) in order to connect. The database name is MyDatabase. The assignment of MyDatabase_Data is for the data file inside the database. In this example a data and log file are designated and named. You can name these however you like (although you cannot have duplicate names within a database). You can also rename them after the database has been created should you so wish using an ALTER DATABASE statement. Filegrowth is dependant upon data being written to the database. Log files will continue to grow if the database is in full or bulk-logged recovery unless you take log backups. However if in simple recovery then the log will grow as large as is required to support open transactions that happen, or to prevent a loss of data with CDC or replication (if a log continues to grow you can check for reasons using the log_reuse_desc column of sys.databases). For the data file, that will grow as you insert data, as an when needed. It is generally recommended to not use percentage based filegrowths as this can lead to unexpectedly large file growths, not to mention performance problems. 

When restoring SQL Server has to undo uncommitted transactions (the relevant data pages get written to the undo file that you specify). The option does not care about the state of the log, or performing any potential rollbacks, that would happen if the database were recovered. Because the two leave the DB in different states you would have to reapply the final transaction log in order to be in a good state. There's an excellent write-up on this at $URL$ As for your monitoring software, I would recommend putting in a temporary exclusion for certain databases, or even instances while getting the AG setup so that you do not get those spurious alerts. 

There is a bit of a hack workaround for this, it's not ideal, but it might help in your situation. Create a new database, and within the database create a set of synonyms that point to your main database. Add this new database to an entirely new Availability Group, and configurute the read-routing target to be whatever replicas you want. Now change your read-intent connection string for those queries that are more latency sensitive to use that new AG listener to connect. That will hit the database, and the synonym will hit the correct database. The challenges for this will come with ensuring that this AG is in the same place, and the relevant sync nodes are used for the read-routing (this is something you could script out, and check configurations for frequently). Also, security might become a bit more difficult to handle (but that is all going to depend on how things are configured for your servers). 

That's likely a memory problem, or possible database corruption somehwere in your system. Run on all of your databases, and ensure that you do not have any hardware problems. As for tempdb getting recreated, well yes, in a sense that is true, all objects are cleared, but the files themselves are not recreated (except in the circumstance that you manually delete the files from the OS). I Would recommend reading this post on the zeroing out of log files in tempdb. 

Which will give you the relevant information (note the bold, italicised SP1 from the example results) 

I would recommend using sp_whoisactive by Adam Machanic. It's highly powerful, can give you all the information you need, and can run frequently, loading its results to a table. 

You will need to license your secondary if you allow read intent routing. That is something that gets configured at the AG level. But ALWAYS check with a licensing representative to ensure that you are compliant. You should not need additional bandwidth for AG traffic if you are already using mirroring. Packets are compressed prior to sending between machines (when running in asynchronous commit mode). 

I have attempted the same thing using SSMS 16.5.3. It is possible that there is a bug in the version you are using (latest is at $URL$ ). Consider creating the script using TSQL rather than the GUI to see if you get the same result. 

You have to have a read-routing list, even with only two instances, if you want to offload reads to a secondary replica. The following script is an example of setting this up (you would change the AG name to whatever your AG is, and the replicas to your servernames (using the fully qualified domain name, and adjusting the port if required). 

In this instance if you had the same table in multiple schemas that you would get incorrect results. You could fix that by also joining to the sys.schemas table with the schema of the table name. *Updated SQL to be dynamic as requested in updated question 

I turned your PS into a function, just changing the calls, ran it through the PS ISE, and it worked just fine for me in testing, outputting the data to the location. I would ensure that you have the ability to write to the root of the C: drive. Typically this can require admin permissions. It's possible you are swallowing the error somewhere. 

The call needs to be dynamic for the databases, as does the object_id. This is basic (does not check for offline databases and the like), but gives you an idea as to how it would function, looping through the databases in sys.databases. 

Enabling the remote admin connection allows you to access the DAC from another machine on your network, otherwise you would only be able to access it from the local machine where SQL Server is running (as the DAC would only listen on the 127.0.0.1 loopback address). This could be a problem if, for example, there is a high CPU condition that prevents you from being able to access Windows and launch a sqlcmd/PS/SSMS session. With the remote DAC enabled you can access it from a remote machine using the admin:server/instance logic. This can be highly useful in bad scenarios, and save a lot of time. It's something I have setup by default on all of my instances. Further reading for this at $URL$