ʩ - velopharyngeal fricative (snoring); ʬ - bilabial percussive (lip smacking); ʭ - bidental percussive (teeth gnashing). 

If you want to learn more of the theory, you need to research/look for specific topics (e.g. those referenced in the above programs). For example: 

Nasals You may be able to approximate a nasal sound with closed nostrils, but if you pinch your nose shut and try to pronounce a nasal sound you will find you cannot do it. You can feel the air building up behind where you have pinched it shut. Also, the nasal cavity acts as a secondary resonator. It is involved in producing nasal vowels from e.g. French and Portuguese. Speech Synthesis Speech synthesis works on the acoustic signal. The formant synthesis approach is based on Linear Predictive Coding (LPC) techniques from speech signal compression studies. These use a source-filter model. You can divide the signal into voiced (where the vocal chords are vibrating, producing an impulse train) and unvoiced (where the source is a chaotic airflow, usually modelled by a random number generator). This forms the source. Some speech synthesizer voices use a signal derived from the original audio recording as the source -- for example in Residue-Excited LPC in the diphone voices from festival. In LPC analysis, LPC coefficients are produced that work to reconstruct the acoustic signal. From these coefficients, the formants (peaks) of the acoustic signal can be derived. Using either the LPC coefficients or the formants produces the same effect -- to filter the source signal, reconstructing the original. The first two formants (F1 and F2) indicate the vowel on the IPA vowel chart being produced, and are used this way in acoustic speech analysis. The other formants add richness and additional harmonics to the phoneme. There are several techniques for producing the speech signal using the LPC/formant data: 

According to the Wikipedia definition of a metalanguage, a metalanguage is a language used to make statements about statements in another language. That is, it is a language designed to describe the constructs of another language. Wikipedia then defines a metasyntax as the syntax or grammar of a given metalanguage. Therefore, Backus-Naur Form (BNF) is a metalanguage which defines a metasyntax (i.e. a metagrammar) that you use to write statements in BNF to describe the constructs in another language. 

In English phonology (esp. American English), these are all different ways of transcribing/representing the same thing -- a rhoticized schwa sound -- when using a broad (phonemic) transcription. The transcription is similar to other syllabic consonants, where you may see them transcribed with either a schwa before them, or with a syllabic diacritic. Some transcriptions (e.g. the OED) even leave out the syllabic diacritic, although for LETTER (Wells' lexical set), they use to denote it being rhotic in some accents. I have found used in the MBROLA text-to-speech voice (as ) for [NURSE] and [LETTER] (as American English does not to distinguish the two). 

Using the formal syllable identification rules, you have the following (with an example pronunciation): 

You can work out how the features map to the IPA consonant and vowel tables, then apply that to the phonemes from English not present in German. You may need to define the feature set differently, as phoneme features are typically defined in a minimal way to express the phones in a given language, and may not accurately model the IPA. For example, the document defines as a mix of consonant and vowel terminology (high fricative) instead of consonant terminology (palatal fricative) -- this is because it uses coronal for (alveolar) and palatal for (post-alveolar). How you define the set of features depends on what information you want to preserve, and what information you want to ignore. It also depends on how many languages you want to support using the model. If you want a feature set that covers the majority of IPA phonemes, you may want to look at Appendix A (feature abbreviations) of Evan Kirshenbaum's ASCII-IPA at $URL$ I am using an extension of this in my Cainteoir Engine application to support all IPA phonemes: 

There are also phrases like "that's rubbish", "that's pants", etc. that can indicate that something is not very good. Likewise, phrases like "that's mint", "that's the bomb", "that's cool", "that's wicked" can indicate something is very good. Also, with "wicked" it can also mean the musical based in the Wizard of Oz universe, among its other definitions. Likewise, there is a German band called Die Toten Hosen ("the dead trousers"), which Google translate leaves in German. Interestingly, while "that's pants" translates to "das ist Hosen" in German, it translates back to "that is pants" in English, mainly because it preferentially uses pants over trousers (e.g. translating "Hosen" to English), due probably to its US English slant (pants mean underwear in England). Software translators also do a poor job when encountering poor grammar (e.g. not capitalizing the correct words in German, leaving out punctuation, etc.). I would suspect that Hebrew would be difficult because from what I understand it tends to leave out the vowels. 

Use the LPC coefficients and residue (remaining signal) -- this is used in concatenative synthesizers like Festival and Flite. Use the formants, storing them for each segment of sound -- this is used by e.g. the Klatt speech synthesizers. Derive the formants from target F1 & F2 formants for a given vowel -- this is used by e.g. the rsynth speech synthesizer. Model the vocal tract as a cylindrical tube, with the width of each section derived from the F1 & F2 formants -- this is used by e.g. Gnuspeech. Use Hidden Markov Models (HMM) to predict the speech parameters (pitch, duration, etc.) including either the LPC coefficients or the derived formant data -- this is used by the HTS and clustergen based voices from Festival and Flite. 

eSpeak -- This is a formant synthesis-based speech synthesizer along the lines of the Klatt synthesizer that supports a large number of languages. I also have a version at $URL$ that makes it easier to build the program and associated voice data on POSIX systems (Linux, Mac, BSD, etc.). klatt -- This is an implementation of Denis Klatt's formant synthesizer. This code is a modernized version of klatt304. This does not produce speech from phonemes, but uses formant parameters as the input. Cainteoir Text-to-Speech -- This is my text-to-speech program. It currently interfaces to other speech synthesizers (eSpeak, MBROLA, svox), and provides linguistics and text-to-speech tools (a phoneme converter, a pronunciation dictionary manager, etc.). Festival -- This is a text-to-speech program that supports different synthesis types (concatenative synthesis, Hidden Markov synthesis (HTS and clustergen)). It also includes facilities for building voices via FestVox, along with several example voice databases. Flite -- This is a light-weight C implementation of festival, designed for constrained systems. MBROLA -- This is a program that synthesizes audio from phoneme, length and pitch contour information, with a lot of voices available in several different languages. This is not open source, but is free for non-commercial, non-military applications only. 

a list of IPA phonemes for English at $URL$ the IPA consonant and vowel charts at $URL$ the equivalent X-SAMPA at $URL$ 

NOTE: /ç/ is a common allophone of /x/ in other languages (e.g. German). Thus, what I suspect is happening is that in some dialects the tongue tip does not raise enough to the hard palate, but the body of the tongue is shifted toward either the hard palate or velum to compensate (i.e. differentiate it from a [ʃ] sound). This may be because the latter are easier to pronounce. Source: Wikipedia 

It depends on what you are trying to achieve. If you are trying to assess the structural complexity of any given sentence, then metrics like you suggested are useful, as is measuring the complexity of the syntax tree or using something like the Flesch-Kincaid metrics. If you are trying to assess how difficult a sentence is to comprehend (e.g. for a specific school grade/age), there are various factors involved. One factor is whether a sentence can have multiple valid syntax trees due to the words belonging to multiple part of speech classifications, or have different meanings within the same part of speech classification. Consider: 

she took the lead in the dance; he took the lead in the box to the dog's new owner; they took the lead in the box to be analysed. 

Have a look into visemes. These are like phonemes, but relate to what the face is doing (esp. the mouth) for each phoneme. More than one phoneme can map to a viseme (e.g. voiced and voiceless consonants have the same viseme as the distinction is due to the vibration of the vocal chords, not the shape of the mouth). As far as I can tell, although there was earlier research into confusion of similar phonemes, the visemes were first created by early Disney animators. They defined 12/13 facial shapes for the 45 or so English phonemes. They are also used in text-to-speech applications. Other classifications can have a different number of visemes,, for example the Microsoft Speech Application Programming Interface (SAPI) defines 21 visemes. There does not appear to be any clear consensus on the phoneme to viseme classification. I'm not sure what it is like in the lip reading field (a Gamasutra article mentioned there were 18 lip reading visemes, but I was not able to find a reference to this in my brief search, although visemes were mentioned in connection to lip reading in other results). There is the problem of context, where the shape of the mouth is influenced by the previous and next phoneme. In addition to this, the simplified viseme model does not account for the shape of the cheeks and jaws, nor the position of the tongue, which can all be visual clues. The ability to lip read will take the movement of the facial features into account and make inferences based on context (e.g. a t/d shape after could be or ). This is similar to how speech recognition software works, but that operates on the acoustic (audio) signal.