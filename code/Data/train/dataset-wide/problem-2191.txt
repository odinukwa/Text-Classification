Where are these files likely to have have come/why weren't they eventually expired and deleted like the rest of the archivelogs? Why does crosscheck archivelog all not detect them and add them back? is it because there is a 4 month gap between them and the next most recent files? Is it safe to just delete these files off disk? 

I've enabled tracing on the reports server and can see requests on the server coming from the external clients (i think), but I only get one line per report run attempt. (But consistently get this same line so I think the rwclient is at least seeing the rwserver): 

(The java processes above are all for ) I assume the 0.0.0.0/127.0.0.0.1 multiple ports things is why the server can see the initial probe, but the client then fails to connect properly. I suspect there's got to be an option somewhere that determines what interface the server tries to bind to but I can't find any option in the report server .conf to enable this. Questions: 

For experimenting it's ideal, in my experience. Of course, it's unsupported, but as you won't have a support contract anyway that's a bit of a moot point. As David said, PL/SQL is integral to the database, not a separate component, so it is available in the VM image. You get some tools too, including SQL Developer, but you might have less friction running that natively and connecting it to the DB in the VM. 

... where is the database service name registered with the listener. You can also create your own in a different directory and set the environment variable to that directory path. But is is the SID and not the service name then you'd need to modify what you'd proposed. For example, create containing: 

It uses a range scan because is the first field in the composite index. It's cheaper (according to its stats) to get all matching index records for that location, and it possibly then filters the index results on the date range before retrieving table data. A skip scan would be considered if you were querying on and , or just on . 

As mentioned, this is the same oracle home, same server, same disks etc - the two databases should be performing at least roughly the same I think. Any know what could be causing this? Or where I should be looking to see what could be causing the slow down? Thanks 

I'm having issues with Oracle 12.1 rman - specifically the MAXPIECESIZE parameter not being honored by my Level 0 backups. This value is set in rman as 

(Weekly I run a level 0, but the script is other wise unchanged) The backup list looks as I'd expect (had to purge the backups on the 5th due to space constraints), with the tagged level 1s and 0s interspersed with the Full autobackups. 

I've got two databases running on the same 12.1.0.1.0 oracle home on the same machine, on the same disks. Each database has multiple schemas of approximately the same size. On database A, the datapump exports take on average of about 2 minutes. On database B, the exports take on average, about 20 minutes. These exports get run twice a day, and the performance has been consistent for at least as long as I've been paying attention. I've also tried different times of day incase it was related to system load, but that seems to make no difference. The configuration for the databases are largely the same, the only differences I can see are file locations, and the pga limit/target, both of which are big enough for the respective databases, and the log_buffer (which I don't think should impact exports, but maybe?) I've added METRICS=y and LOGTIME=all to the backup scripts - that revealed some interesting timings: Database A (fast) 

You should also consider using the data pump equivalent, , if you're on 10g or higher, which puts the dump file on the server; and you can create the dump using the API calls rather than with the command line tool if you need to. Getting the dump file off the server might then be an issue - not sure if you're using for a reason, of course. 

The let you mount and open the database so that it is accessible to users. Clause Use the clause to mount the database. Do not use this clause when the database is already mounted. You can specify to mount a physical standby database. The keywords are optional, because Oracle Database determines automatically whether the database to be mounted is a primary or standby database. As soon as this statement executes, the standby instance can receive redo data from the primary instance. 

Then from within the actual exports, for Database B, the times reported by the LOGTIME parameter all give a time of a few seconds per table, but the datapump reported times are all 0 seconds: 

I'm running a 12.1 SE database on Oracle Linux 7. I am getting nightly errors/warnings from my rman scripts about missing datafiles when trying to recover. I have been running rman to a local drive using the following script: 

Is it actually even possible to connect to the reports server from external clients? and if so; Where abouts do I need to configure the reports server so it listens for external connections? 

My understanding from the documentation is that this is normal on the first run (when there is no datafile image copy), and normal on the 2nd run (when there is a datafile copy, but no incrementals), but on the 3rd and subsequent run there should always be a datafile copy and an incremental to apply to it. This has now been failing for the last 6 nights. From $URL$ 

DazzaL has highlighted the nub of the problem but to give a bit more background, on the assumption you're fairly new to Oracle... The syntax shows that your command is trying to create a table called in the schema . Schema and user are essentially interchangeable in Oracle, so the error saying 'user does not exist' is referring to the schema/user. Unless you've already created that in your database, that won't exist by default. You can either (a) create the user, (b) update the script to replace with a (non-built-in; see below) user that does exist, or (c) update the script to remove the schema reference which will cause objects to be created as the user you're logged in as. But who you are logged in as is something to check - it isn't clear from the question, so apologies if you know this and it isn't relevant to you. It's important not to create your own objects under Oracle's built-in schemas, especially and as you could do a lot of damage, and you should not be logged in to a built-in schema routinely - only for specific activities that can't be done otherwise (like shutdown). The only exceptions to that are maybe the sample and schemas, and even then it'll be better in the long term to use your own account for anything except queries against those. If this is a fresh install and you're logging in as because that's all you had available, please create a new user, assign it the relevant privileges, and do all further work under that. 

I have tried explicitly setting the CHANNEL DEVICE as part of the backup script (immediately before the BACKUP INCREMENTAL line) - this made no difference. Also maybe related, the location does not seem to match what I am specifying above - vs - this seems to match up with db_recovery_file_dest set in the database: 

I have an Oracle Reports server 10g (10.1.0.4.2) that for the last few years has been running okay with the rwclient.sh client application residing on the same host. Due to some performance issues caused by long running/heavy reports we are looking at trying to separate the reports server from the application server. From memory rwserver.sh should be able to accept connections from a client on the same LAN, but not on the rwservers localhost (but I'm going back many years since I saw that sort of setup and I was just a developer, not an admin, so I could be misremembering it). When I try and submit a report across the network I get: 

The connection string you showed at the top of the question is commented out, so you're actually connecting with . That isn't supplying a service name; but your does have , so this should be the equivalent of . (I've tested this with 11gR2 EE and the parentheses around the in the don't seem to matter). But that doesn't quite seem to work as expected. When you changed that and set the user name explicitly: 

But you then need to work out how to manage your archives, and set the recovery area size to a sensible value for your backup and retention needs. This may be as simple as scheduling a job to remove obsolete backups, e.g. from the Enterprise Manager console's Availability tab, under Manage Current Backup; but you need to determine the best course for you (and not blindly follow advice from some random guy on the Internet who knows nothing about your requirements, and not that much about EM/RMAN either). 

(in hindsight, symlinks may have been a better option but at the time I preferred to make the location of the backups on a seperate drive explicit/obvious). This channel change didn't seem to have any effect. Initially I suspected because I have a 7 day retention period set and the old image were still valid, however after 7 days datafile images were still being created in the old location. As diskspace was becoming increasingly tight, I changed the backup script to use a different TAG to try and force a new set of backup images to be create on the new disk. This seemed to work - the new set of backup images were created okay on the new disk that night, but subsequent recovery operations seem to be failing. 

but I can't see any options in there that would override the MAXPEICESIZE To be clear - I'm NOT concerned about the location just the file sizes. Is there another setting somewhere that I am missing? Do I need to remove the db_recovery_file_dest parameter maybe? Thanks Backup summary of the latest backup: 

12c's Multitenant model gives you a single container database that manages the resources at operating system level and shares them with the pluggable databases as needed, potentially significantly reducing the total resources the O/S server needs, and hiding/simplifying the management of the plugged-in dtabases. It's somewhat analogous to running virtual machines on a physical server - the physical server needs fewer actual CPUs and less memory than are allocated in total to all the VMs, and the VM management layer controls the balancing and contention of the resources depending on the actual needs and loads of all the VMs. The VMs themselves aren't really aware of that happening, they just use what they need; similarly pluggable databases aren't really aware that the container is managing their resources rather than the operating system. 

You can use the same functions to get your table and view scripts if you prefer - I believe SQL Developer's export uses them under the hood anyway - and to get role definitions and grants. 

I have two 12.1 databases on the same server that I've set up with rman and have been running okay for the last year or so (with numerous restores to a seperate test server - so at least restore/recovery wise rman is set up okay.). These databases are both around 100G in size, with the recovery area (originally) sized at around 200G. (Currently bumped up to 300+G to give a little more lee-way time whilst I work out what's going wrong). Just recently one of the databases has started filling up the db_recovery_area with backup pieces - normally this has been fairly static increasing over the day and then dropping back to zero when I run the rman level 1 backup over night (or level 0 over the weekend). However, over the last couple of weeks, the recovery space has trended upwards eventually getting to the point where I need to delete all backups/archivelogs out of rman to avoid a production database freezing due to lack of space. There's been no configuration change or upgrades on this server for some time and I can't work why the behaviour has changed - but only for one of the databases. Looking at V$RECOVERY_AREA_USAGE the only thing consuming a substantial amount of space is the backup pieces.