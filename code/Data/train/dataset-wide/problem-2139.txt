In my opinion it is not related to or Windows issues. Per pg_basebackup docs, the main data directory will be placed in the target directory, but all other tablespaces will be placed in the same absolute path as they have on the server. 

Update: Konrad corrected my misunderstanding of his question. The goal was to count queries, not transactions. How to count queries? Method 1 Use pg_stat_statements contrib. Method 2 Enable full logging of queries for a representative period of time. To enable full logging, for PostgreSQL 9.0 - 9.3, change following settings in 

I'm not yet sure if this is doable in pure SQL (probably - yes), but here is the brute force solution using cursors. This is just a (working) draft, with some twiddling and dynamic SQL you could add more parameterers, like function name. 

Indeed, you'd hope the optimizer would realize that only name and AnotherColumn are necessary to run the query, but I think that's the actual question here: what does the MySQL optimizer do when shown this statement? That'll depend on which storage engine is used for Table01, and which indexes are available on it. Ideally, this query has an index over (id, AnotherColumn, Name) so that the query is naturally satisfied from the index and won't need to seek back to the table for additional columns or perform a sort operation to satisfy the ORDER BY -- assuming the ORDER BY is honored. 

Sounds like the database really does exist, but is just not mounted; does it show up when you carefully refresh the object explorer in SSMS, or when you look at sys.databases? 

If you have users who are updating/insert records then I am sure they are also deleting records most likely with no or a very limited audit trail. If this was my database I would 

Create a SQL Server Account and use those credentials to connect to the SQL Server <-- this might be the easiest Trust the domain in which your GIS Programme is running <--I would be loath to do this 

Using the above example a new table, #tmpTable will be created with the data from coulmnA, columnB and columnC from tableA. The data types in #tmpTable will match the datatypes of tableA. You can of course create a true physical table and have multiple tables in your select statement. 

The issue is that you are running your GIS programme from a domain that is not trusted by the domain in which SQL Server resides. That are numerous methods you can use to correct this. You should discuss with your network admin/sql server admin as to the best approach. 

SQL Server does have 3-way set operations; the CONCATENATION operator accepts n inputs. Given, for example, ten tables: 

It seems like MySQL allows ORDER BY in a subselect where it shouldn't matter, like in this case. The outer SELECT doesn't have an ORDER BY, so the results of the query aren't guaranteed to be ordered. What three SELECT statements are you expecting to emulate, Jakobud? I only see one: 

If it appears in either place, you'll want to make sure you've backed up the files safely, then DROP the database. Once it's dropped, you can ATTACH the files you've backed up after moving them where you want. The SQL Server error log will tell you what SQL Server is trying to do about the database when you start the server instance. Showing the log entries for the database that appear at server start up would probably be helpful. 

Start at the YQL Console In the top left hand corner under Datatables ensure you tick the Show Community Tables In the search box type Finance. This should display all of the Finance tables (approx 31) In the "Your YQL Statement" window type your YQL statment. In order to get the average daily volume for Yahoo you could use the following YQL: 

By opening SQL Enterprise Manager, locating the database in which the SP resides and viewing the SP under Programmability. Use sp_helptext 'MySP' which will return SQL Servers stored version of the SP 

to the offending SP to force just that SP to use the prior CE. The SP should now perform in Compatibility Level 120 as it did with Compatibility Level 110. <-- I prefer this approach as your Database/System gains all of the other advantages of SQL 2014 while the "fix" only impacts the offending SP. Irrespective of the approach I would then profile the execution plan/read/writes etc of the offending SP on a test system in SQL Server 2014 Compatibility Level 120 and rewrite the SP to cater for the new CE. Once tested and verified I would then apply to production. Microsoft have a good article on the Cardinality Estimator changes in SQL 2014 

You can eliminate the overhead by frequently running OPTIMIZE TABLE on the table in question. I don't think you've got enough overhead to worry about; it's right around 1%, and shouldn't be giving you much trouble. Are you finding that a particular query is running slowly, or that you can't execute something as fast as you'd like to? If you can show the query that you're having trouble with, then you should post that query and describe the symptoms (as well as the table itself and the pattern of inserts you're doing). I note that you're using MyISAM. Have you considered using InnoDB instead? InnoDB has several advantages over MyISAM, and you might find it more appropriate for your application. 

We'll see a query plan that gets the rows matching (with predicate push down in the TABLE SCAN operator) then concatenates all the results into the operator. The reason that you don't get a plan merges then sorts is because it would be very slow, and the sort isn't necessary to implement the operation. In your BOO, FOO, and KOO tables, you've declared a primary key. When the CLUSTERED INDEX SCAN accessor enumerates the rows, they are produced in the order of the underlying clustered index -- guaranteed. Concatenating two sets then sorting the result is much slower than using the MERGE JOIN operator, and the MJ operator can be used very readily since both sets are sorted and indexed. 

I have recently seen this exact same problem with an upgrade from SQL Server 2008 R2 to SQL Server 2014 where a very small number of Stored Procedures (SPs) (approx 5 out of 3,000 ) performed worse in SQL 2014 Compatibility Level 120 than in SQL 2014 Compatibility Level 110 or in SQL 2008. In SQL 2014 the Cardinality Estimator (CE) has changed which may impact how a SP performs in SQL 2014 Compatibility Level 120. There are a few possible solutions 

SQL SERVER SCOPE_IDENTITY/IDENT_CURRENT/@@IDENTITY. While SCOPE_IDENTITY/IDENT_CURRENT/@@IDENTITY perform similar functionality the values returned from each differs depending on the current scope such as a stored procedure, trigger, batch each. The documentation should be read and understood before determining which one to utilise. CREATE TABLE Test( TestID INT IDENTITY(1,1), TestName VARCHAR(20) NOT NULL ) INSERT TEST VALUES('Test1','Test2','Test3'); SELECT @@IDENTITY,SCOPE_IDENTIY(); In this example both @@IDENTITY,SCOPE_IDENTIY() will return 3 as they are in the same scope. Depending on scope though they may return different values. SQL SERVER Output allows you to access all of the rows returned from an INSERT/DELETE/UPDATE or Merge statement. You may need to use a variable to access the identity. The links provide more comprehensive information.