You could do a lot of stuff but integer and text do not sort the same. Text would break a search on week > 6. In text 10, 11, 12 are not > 6. I would use 0 for no date Not that size is the big of a deal but you would use tinyint for week and smallint for year one varchar is the size of a smallint 

Your research is wrong. Index on GUID will fragment if it clustered or non-clustered. It just takes a little longer to defragment a clustered index. If you have a PK GUID then what purpose does an int ID serve? 

I don't think so. You pick one way or the other to reduce fragmentation. If you compare query plans with one index sort both ways the cost is split 50 50. 

StudentTeams is a bad name as in plural. With that design a student can be on 0 or 1 team. If multiple students are on the same team (TeamName) repeats. Two student could be on the same team but someone could enter TeamName incorrectly so you would not know. I would either 

If you to exclude a character from possibly being in both a movie and a show then go with two character tables 

That is a sufficient composite PK And I would do it in that order for quick counting NoteID select count(*) from UserNotes where NoteID = X would be very very fast So what 1000 x 1000 rows. You have 1000 x 1000 unique relationships. It is just two Int. One trick is have a UserID Public ID 0 for all Just a preference but I don't use plural in a table name Tables are used to store many rows 

you really need a composite PK of bigint? you have ID that is not an Identity it appears you have an index on a bit it is not clear to me what you are trying to accomplish I've been noticing that some queries are slow to respond I hope your queries are not inserting 500000 rows in a table Now responding to the indexes posted Those indexes are just crazy That is a level of indexing I would expect on static table An index has overhead to add, delete, and update and you are adding 500,000 rows I think you are lucky to be getting 500,000 in 14 seconds Add Disable / Enable to all but the PK Do not disable the PK And try a composite PK of Int, Int clustered 

This is not going to be the shortest but I think it will be the most efficient. Pretty sure not exists is going to be the most efficient way to eliminate col2 with a value. 

A transaction like Kenneth Fisher answered And look at optimizing Why are you updating after the insert? Why not just insert a hard coded TargetAmountSetTable.isModified of 1 in the insert? Another option is to output the whole lot of joins once to a #processTable 

I don't think the is producing the sort The is producing the sort See the order by in the query plan This might be faster It breaks if the purpose is multiple accounts with same name 

The where is killing the left joins in the exists so you can drop that This may give you better performance 

It is typically better to perform the write operation locally Here you are both reading from and writing to the linked server My recommendation is if you are going to create #localTempTable then do so on LinkedServer. Have a session on the LinkedServer and link to local to populate #localTempTable but #localTempTable is now on LinkedServer. Flop the link. 

Is it really up to you to dictate the first? You throw in least privilege to get the app to work without proof the app will never will need to update the value. Who is responsible for data integrity? With SQL constraints can you guarantee data integrity? No you cannot as there are often business rules that go beyond what a database can do. VendorID should never change but what if two vendors merge. Never say never. If the app team contaminates the data and they said they needed that authority then it is on them. If the app teams works for you then you can dictate. The appropriate question is should the app ever update the data. 

You are totally trashing the index on NewTable.C2 Just drop / disable the index and then build / rebuild it after the insert is complete And you need to break this up as you are putting a massive load on the transaction log If you want to optimize the select then just run the select alone 

Would benefit from indexes on: 1) results.person_id 2) people.boss_id Index on date may help but start without 

You confuse what with index means. Use of an index in no way directly drives the order of the results. A table does not have an inherent order. Even a table with a clustered index may not return results in the order of clustered index. Now a sort will use an index but the index alone does not force the sort. In your case the index made the sort efficient and I suspect if you look at the query plan the sort used the index. 

You should not always set auto growth to unrestricted. If something is wrong you don't want to fill up the disk. And you want a heads up that something is not going as you expected. Pick a big number that you don't think you would need if things are going as expected. It takes time to create and grow. If you start at 20GB less total time is taken compare to start at 10GB and grow by 10GB once. Pick a starting number that will fit your normal needs. You can start small and see how big it quickly grows to in normal operation. A fixed number of 10GB would be 9 growths to get to 100GB. If you double (100%) would be 4 growths to get to 160GB. I usually pick like 10-40%. You get some fragmentation when you grow but that is not much of a factor unless you are growing a lot (like 20+). If you have more physical drives then it is typically best to put it on another drive to level out traffic. TempDB has little to no need for redundancy. 

Rather than just use [T3].[NetworkNode] Odering by twice and only taking 25 So if [T2].[SiteID] is not unique this will be non-deterministic 

All you need is bar code to file I would just store bar code and file path in XML. When you load the application you read the XML into a dictionary. Use a dictionary lookup get the file name and then just read the file from disk. You you can use any database that will store a binary (they all do). This is just a simple table. It is kind of a pain to read and write binary. It is is easier to read files from disk and makes backup and restore easier. P.S. I write document management software for a living 

As far as theoretical why do a look ahead for EACH row? Just read the rows and hold the 0 in a buffer If you get to a 1 then update buffer with 1 information and write out the buffer If you get to a new item then discard the buffer This is an easy single pass operation This would be all of 12 lines of C# with CLR 

Are you not getting rapid fragmentation of that clustered index? That would negatively impact insert and select. Consider a different index You are loading data daily - I assume for the day or prior day PK cellX, cellY, timeStamp That is maximum fragmentation Consider PK timeStamp, cellX, cellY And load the data sorted by that order Even if you have to load it into a staging table to sort That is minimum fragmentation If you really need a cellX, cellY index for query performance then put that in another index on another partition with a fill factor < 1. And perform index maintenance. If this is done off hours it might be faster to disable the index, insert, and then rebuild the index (in this case can use a fill factor or 1). 

Delete does not always use the same indexes as a select but optimize the select will typically help try this 

Job.PersonID does not have a unique constraint I think this is what you mean to have But is is still messed up in only one person can have a job If that is the case then just combine Job and Employee 

15K in 5 minutes 50 / second - that is reasonable The likely culprit is page splits and index fragmentation Give this a try On the insert sort by the order of the PK on the insert Put a fill factor of like 50 on all the indexes Try an insert If this work then tune the fill factor and add in index maintenance Maybe even tune the order of PK to put those with a more common insert sort early - like if one FK is a customerID and you load by customerID then have it early If it does not work then go back to your prior fill factors You might also try tablock holdlock Some times it is faster just to take a tablock get your business done and get out If you have lock contention you can create a test table or test when usage is low 

I am going to accept that you need regex. Regex does stuff you cannot do with Like and Full Text. Breaking it up in 3 tables will help. Or you could just have a column of word count and include that in the where. Where word count will not be as fast as separate tables but compared to regex it is super fast. Use Like when you don't need regex If you need regex use Like to limit the number of regex you need to evaluate if you can I might be worth split it up the ngrams into separate words. But then you would need to reassemble the ngram in the search (or view). Like 'hum%' can use an index but like '%hum%' cannot use an index. You could do stuff like find words with a small Levenstine distance. For ngrams I would not use Full Text. I would break it up myself. Full Text has overhead and mixing regex with Full Text could get messy. 

If an AcctNo had Code both 1 and 2 then this would be non-deterministic If you need one (actually 2) to win then 

That update is probably not the problem. There are probably some selects that are holding a read lock and it takes a while for it to get an update lock. Look at the selects that are used the most. Does the application hold on for example process DataReader that leaves the select active? 

Not sure about mysql but in mssql like '%val' is a full scan Multiple keywords in a varchar(200) column is probably not the best design. Spit the keyswords into separate words into a separate table with a FK back to company.ID. So each word would be a row. Then you can search on keyword.value = 'xxx' and use an index. 

if you have indexes on all the Reffd and Date then this should be smoking fast those loops are killing it hopefully this will get rid of some of those loops this is like one of the knife commercials - but wait there is more 

This has nothing to do with the union Learn to break down a problem and format a query so you can debug 

I am not positive on if this would reduce page splits but I would put date last if that has the most changes. Have a fill factor of less than 100%. Even 90% or 80% will slow down fragmentation significantly. Have a maintenance plan to reorganize / rebuild the index. A non-clustered index will use more room but it would make for more efficient maintenance. You can place a unique constraint on a non-clustered index. 

Pretty simple. Create the index and look at the query plan for an is null search. I tested in MS-SQL and the answer is yes search on is null used the index. Search on is not null also used the index. 

And why a iden PK in events_tags? Drop that. Just have a composite PK of tag_ID, event_id in that order. This might be faster but I doubt it 

Yes even on the same disk you should experience performance improvements with multiple tempdb. A rule of thumb is number of cores. Recommendations to reduce allocation contention in SQL Server tempdb database 

Not the stated question but for speed I would try materializing this referencing Entity.EntityAddress a in the where makes it an inner join are you sure c.EntityId is not good enough for ds.Value, dn.Value? For grins see if it runs fast(er) 

If you have no option to disable FK or take the database offline you could delete in batches with a wait (sleep) the keep the cpu down. It will take longer but that might not be a problem. 

Please post the query to populate #localTempTable If you use that table for other stuff then flopping stuff around may not be and option 

Are you sure you even need the left join to get a good count? Are you using values from #rLog in the distinct? And I would go with , distinct not + this line is different 

What database? Does your database support row_number(). And you changed the query. In the first max(scanTime), max(scanId) are not necessarily on the same row. Those are two independent max. The scanTime of the row with the max(scanID) is different and what you got with the second. It is faster as scanID is indexed (PK) and scanTime is not. Since scanID is the PK you don't need: 

I think the Notes table is missing a userID FK column (and put an index on it) You need to identify the owner of the Note The UserNotes table does not need an ID just: 

That top being kind of removed from the order by makes is hard for the query optimizer It does not need to just do a lousy 10 lookups as it need top 10 WHERE [Extent2].[id] IS NULL With more statistics the query optimizer may get smarter I know you are using an ORM but give this a try 

this sorts on PlanCode - if you have duplicates then use a row_number() if you need a particular order then you need to have that order in the table 

If so I am not seeing the value here That is a messy join to avoid an ass_users_events table And you lose declarative referential integrity How is the first causing the ORM to do unnecessary work? Dedicated tables are performant and sustainable 

Indexes are going to slow down insert, update, and delete As the indexes get fragmented it gets even slower Few thing you can do 

Still give them access to the View for those that need current data and are willing to pay the price. 

This is not 3rd normal form. If two or more users share a computer then you would have duplicate entries for the computer. If there is no sharing then you can just use the computername alone. Should have separate tables for user and computer. Then a 3rd junction table. 

Loading a file on each request is not an option. That is around a TB of data. Many conventional data bases (e.g. MSSQL will handle that). Clustered index on min_latitude, max_latitude, min_longitude, max_longitude may be the way to go. Since you don't have any relationships a document db may be the way to go. Consider a Cloud solution that can quickly scale. In don't know much about big data. Hopefully you will get more answers. 

Yes it will definitely go faster without indexes. The indexes will be fragmented and need to be rebuilt anyway. Disable the non-clustered indexes and then rebuild them. Leave the cluster index live. Also disable (nocheck) the FK. FK checking is a lot of processing. Then turn it back on (check) after the large insert. Insert in the order of the PK. This assumes the PK is the clustered index: 

Typically user would have the fk user: ID (pk) Name StatusID (fk referencing status.ID) status: ID (pk) Name A user has a status - not a status has a user 

But you have an index on TweetID. Is that index not used by the joins? Even a non-clustered index will fragment with insert and update. As that index fragments inserts take longer. How are you managing fill factor and index maintenance on this non-clustered index? An unused PK does nothing. TweetID as PK will save some space. What % of rows get added daily. If only 1% rows get added daily you could use a fill factor of 90% and defrag daily or weekly. Start with like 80% or 90% and see how fast it fragments. If the TweetID are out of order but in the same range then the problem you have there is fill factor gets hammered in that range and unused in the rest of the range. But a defag is faster as it is only cleaning up that part of the range. In that case you may be better off with 100% fill factor and a regular index reorganize. There is no cut and dry answer. You need to manage and monitor the index for your data and your workload.