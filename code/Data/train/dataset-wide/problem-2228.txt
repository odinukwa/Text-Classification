Trying to remove the duplicate rows delete myTable where pkcol = 1; Yields: ORA-01502: index 'MYTABLE.PK_MT' or partition of such index is in usable state. I'm using Oracle.DataAccess.Client.OracleBulkCopy to fill the table. As far as I understand documentation from Oracle PRIMARY KEY constraints had to be checked. Obviously they are not checked, as I found by doing the same bulkcopy two times in succession which ended in duplicates in all row. Now I'm only using it after deleting all rows and I'm using a table with a similar primary key as source. As result I expect no problems. But embedded deep inside my MS Build scripts, I end up with just 2 duplicates out of 2210 rows. I guess that ignoring the primary key in the first place is a clear bug. No Bulkcopy should be allowed to ignore primary key constraints. Edit: Meanwhile I found, that the 2 conflicting rows where normally inserted by some script before bulkcopy was called. The problem reduces to my known problem, that bulkcopy doesn't check primary keys here. 

Now, as you talk of replace in all tables, that's something not possible in single query. Though you can create a routine to do so! For the sample you can refer a procedure which is finding a string in all tables of all databases. 

Before doing any changes and attempts, take complete backup of your databases. If you have physical backup of your database: 

Here is an old instructions for setting up master-master replication for windows but concept is same. Considering you already have a 3306 instance by default you can follow below steps to setup as well (if above doesn't help) 

Here I will show some results I got while using Vincent Malgrat's approach. First I learned, that when using more than 1 such top 1 sub query based on different tables or order, I have to use the RANK() function and not the ROW_NUMBER() function. Second when using rank() there is the problem of ties. SQL Server's top 1 arbitrarily selects one of the rows with rank = 1 while using rank() can return more than 1 row. I think it is bad design to use SQL Server top 1 in these cases. To make the design correct some unique constraints (e.g. unique indexes ) have to be found, which prevent this ambiguity. Here is a SQL Server example if you want to try it by yourself. Comparing the execution plans of the last two select statements below shows that Vincent Malgrat's approach is better than the top 1 solution. 

You can check if the return value for the command, if 0 then OK else failure. Put may put it in shell script as follows: 

So there's tuner's logic, if the (Created_tmp_disk_tables/Created_tmp_tables)*100 is more than 25(%) then increase your tmp-table size (upto max 256M). Again, mysqltuner is for reference, if you don't see performance issues you don't need to blindly follow the suggestions. Also note that tmp table has nothing to do with InnoDB vs MyISAM (if you mean that in your last line). You might want to read about internal temporary tables. 

If you're using MyISAM, convert to InnoDB and you might see some drop in this counts. There is no other way to reduce this unless you stop querying them. Increase the table_open_cache and table_definition_cache & observe the stats...Raise this as much as you can! May be you need to increase system file handlers too and that is a way to go. 

Don't think because it looks boring for a human, that it is bad for a computer (especially when running Oracle). But don't add further parameters just to force me show a solution using dynamic sql, instead avoid insane designs requiring such solutions. 

The most critical resource is RAM. Each running Oracle instance allocates some RAM for its own, when just started and not under load. We are running a 10g with 10 and 11g with 8 instances, but these are development servers. After restart of the OS some of the Oracle services don't start automatically and must be started manually: Oradim -startup -sid xxx. We are just beginning to use Automatic Memory Management, but the situation keeps different from SQL Server, where you can add databases as much as the disk space gives. I your case, with more instances on one machine the SGA for each instance becomes smaller, less precompiled sql can be cached and the machine has to do more sql compilation, which reduces performance. Adding RAM might help in your situation. 

mysql will write rollback to binary logs provided the transaction has mixture of engines (myisam and innodb tables). As the myisam can not be rollbacked. I guess you'd like this piece of documentation... 

MySQL Installer provides an easy to use, wizard-based installation experience for all your MySQL software needs. Included in the product are the latest versions of: 

You can generate these SQLs from information_schema database and source it to mysql to quick action. 

Considering all the slaves were in-sync with master while you issued reset-master, it is safe to issue on slaves with newly generated first bin-log. 

I installed the 32-bit ODAC (11.2.0.2.1) from here. The Oracle Universal Installer version contains Oracle SQL*Plus 11.2.0.2.0. But I'm missing the sqlplus/admin/ folder in the installation, where I usually put the glogin.sql file. Edit: This install is on a new Windows 7 - 64 bit Notebook, which is my first 64-bit Windows system. The initial idea is not to install any Oracle server there, but I guess I'll change my mind about this during the first 2 month. I find sqlplus.exe in the folder D:\app\berndk\product\11.2.0\client_1. BTW This install has no sqlplus subfolder and the ORACLE_HOME environment varaiable is not set. As I do not need ODBC that install generally works quite well. 

Now, if you need slave's changes to be reflected on master, you need your master to be replicating from your slave as well. That's known as master-master replication. You can find plenty of posts around that. By default, all the databases are replicated. If you want to replicate only few of the objects (DB / tables) there are replication filters for that. 

are you sure you're doing what you want? In first query you specified localhost while in second you specified %... so for % you will have to connect to server remotely or using ip. Check below: 

OK its late and I'm a bit lazy, but filling in the remaining 12 cases is straight forward. Not using dynamic SQL has some advantages: 

This is no independent answer, but an added explanation to René Nyffenegger's code using bind variables. SaUce asked why this code is immune to sql injection. Here I change René's code to not execute the dynamic statement, but to display it: 

I would like to get an error when assigning a longer varchar to a variable and no silent truncation. Are there any settings? This truncates silently: 

I'm just testing my PowerShellScript to do bulkcopy of multiple tables between sql-server and oracle Databases. When the destination is Oracle, than I can monitor the progress by by executing select count(*) from Mytable the single batchsize chunkss individual committed. Using a sql-server destination, there seems to be a single transaction. Which was rolledback due to network error at first trial. The second is still in progress. Will the use of internal transaction commit the individual chunks? The use of bulkcopy seems a bit of an all or nothing, if it fails you can restart from the beginning. 

Oprn. is "slow" because you do ensure the data durability & consistency. Oprn. is "Fast" because you did less work (disk operations). Even for the "lies" mysqld goes an extra step. 

(Consider backing up binary-logs if you want point in time restores.) For backups you can use traditional mysqldump or mydumper/loader. If your data size is large, it'd be better to go /w physical-backups, follow settingup xtrabackup for mysql with Holland framework. 

This will let the slave catch-up with latest available changes from master. If your binary logs were not yet shipped (via replication) to slave, then you might want to ship them manually and play it on slave. (Provided they're available - as you say you have co-ordinates) 

I think it is equally important to look at the explicit added denormalisations, either added aggregate values or some fields from a master table copied to a detail copy. The argument mostly being some performance argument. If you do so enforce, that those fields are updated by triggers and it's up to the database to keep them consistent. 

My feeling is, that this transformation could have been done by an optimizer. It seems to be straight forward, but how can I be sure if he does? Comment on Igor's answer: (comparison fixed thanks to Matts comment) This inspires me to the following: