Since the semicolons need to be part of the trigger definition you need to change your delimiter first. From the prompt first run 

I feel my answer may lack what you are looking for but with out more context here are a couple options: 

Note Your actual value may differ. Then from your shell shut down mysql, back it up and copy it over. Something like: 

I see innodb_file_format is dynamic so I can just switch over with out a bounce. Are there any implications of doing this I should be aware of. All I can tell is that means new tables or subsequently altered will be created with that format. Is this all correct? I was hoping to have to not go through and convert all my tables. Is is kosher to have antelope and barracude tables coexisting in the same tablespace? Even if it works are there any gotcha's to look out for? 

See $URL$ for various tunables related to the amount of memory allocated to myisam check. Most notable is the myisam_sort_buffer_size param. The more memory you can throw at this the better. If you can give this one or more GB this will help speed things along. mysqlcheck is designed to be running while mysqld is actually running. myisamcheck will work directly against the .MY[ID] files on the file system. This requires the mysqld first be shut down. If you can take down the entire mysqld temporarily that will help free up additional memory you can feed to myisamcheck. Assuming this is a dedicated mysql server with out any other services running I'd give myisamcheck 4 gigs of ram for the myisam_sort_buffer_size. Running it with --force --recover will start rebuilding the table right away instead of just reporting it finds a problem with the broken table. If you want more screen action as to the progress it's making you can run with -v, -vv or -vvv to ramp up the verbosity of the progress. 

The problem though is by step 4, there might have been writes on M2 which had since propagated to S2 but where blocked from getting to S1 b/c of M1's read lock. Another idea: 

You definitely should disable old_passowrd. You'd need to regenerate them all to start using the new hash. (You probably could setup a script to brute force crack them all for you ;) Is you team the DBA team or developers. If DBA then yeah you are the guys that should have root access and any root password changes should be coordinated by the team and have the password stored in a shared password manager (we use PassPack). In general you want to minimize the amount of privileges each account has to what is necessary (with in reason). We only have one super "all privileges on ." account and it is configured to only allow connections from "localhost" meaning you need to have a systems level account to begin with. Consider if general users need RW access. Especially with phpmyadmin, that account credentials should be given database.* specific access enumerated. This way you don't have to worry about someone updating the mysql.users table via phpmyadmin (or other client). To prevent passwords from ending up in the .mysql_history file we have a user management script that prompts the administrator for username, host and password (while not echoing to the terminal). This does an initial generic "grant usage on . to user@host identified by 'assdf'". After this is done other grant management can be accomplished without specifying the password since the user already exists. As an aside make user to run w/ 

Stop slaving on M2 and S1. Start slaving on M2/S1 until some common position on M1. Change master on s1 to M2 (s1 and s2 are now slaving directly from M2) stop slaving s1, s2, start until common position on M2 

Are these slow tables under any kind of access from other applications? If the tables are being placed under an explicit read lock that could be blocking phpmyadmin. There could be an implicit lock by a large insert going on as well. If it's taking that long you should be easily able to run a show processlist from a straight command prompt to see if you PMA user is sitting locked, sending data, nor maybe nothing at all on the database side. 

It appears to be a change in the optimizer. I haven't tracked down the exact setting that caused this yet but the explain on the 5.5 version was showing it using the Primary Key. On 5.6 It was using the secondary char1(8) index. Adding force index (primary) got it back to it's 3-5 minute count time. 

This question came back on my radar after a recent comment was posted. This issue at the time turned out to be this reporting table that was meant to be maintained by triggers was on a slave only. The insert on duplicate key statements were getting flagged as not safe for statement based replication and getting pushed through in RBR (the stream was in mixed mode). RBR events do not fire triggers. 

Can also give you insight to what is locked at the moment and who is holding them. Although the output can be a bit cryptic at first glance it can give some pointers. 

I was going through compressing a bunch of archival myisam tables using myisampack It was making decent progress until it got to the most recent tables. The overall database is a set of archives for a single production table. Each archive version represents 1 quarter of a year going back several years. Nothing has drastically changed in the data being stored or the average row length. The individual archives have been growing with time as there simply are more on more rows each quarter over the previous one. Up until the most recent 4 quarters the rate of compressing/rebuilding the index (which is only a single int PK) was around 14-16k rows/seconds. The last ones were crawling along only doing about 1k rows/second. Even right from the start; it wasn't like it started of as fast as the others and then suddenly slowed down after getting so far into it. There wasn't any spike in general system load or i/o contention from other processes when it started slowing down. Has anyone experienced this? Is there something myisampack where it's overall rate of progress it should be expected to tank when compressing a table over a certain size? Edit: Since these are myisam tables I should point out they are all in the same datadir and all stored on the same physical array. 

If you're running MyISAM tables then at least make sure your key_buffer_size can accommodate the index_length (or size of your .MYI files). 

The simplest option is to just back up everything in the filesystem and carry it over. To find out where your data is, look at the datadir variable with in mysql 

You'll have to specify a character set of UTF8 on the table schema. See $URL$ Depending on your needs you can specify table defaults which then apply to all unspecified text columns (char/varchar/text) or you can specify on a per column level. You'll also need to have your applications to specify a UTF-8 character encoding. The specifics on how to do this will depend on the language you are using. 

Don't forget to specify the characterset you're using client side too. From the mysql shell you would run set names utf8; 

Use the mysqldump command. mysqldump -u user -pPassword --no-create-db --no-create-info --where="primary_key=N" 

I've had to solve this specific problem and realized there's not way to say "all tables except these". What I did to work around this was create an internal database, restricted_tables. It had a couple of simple table schema: 

This is just a hunch, but make sure your collation settings are the same between the dbs. I know you said they're synced, but check. If you can't find anything there run each select into their own outfile 

Promoting a slave would probably be my preferred route. As you pointing out any selects on MyISAM tables would require table level locks. There is one tool that might be able to help, pt-table-sync. It's primary purpose is to find gaps and differences in existing master slave relationships. A nice thing about it is it does this in nice "chunks". Think of it kind of like antilock breaks. The chunk size is configurable but you could, for example go through doing 1000 rows at a time, minimizing lock times and letting things flow through. I haven't used it to fully repopulate a slave from scratch though although I'd give that a look. Once you have a full copy, do another run to catch new stuff, updates that have come in. Do a flush table with read locks, do a final table sync. run show master status to get the binary log position to start slaving from, unlock tables. Oh, if you don't have binary logging you'll at least need a master bounce and cnf change unfortunately. Another approach that might be much simpler depending your write downtime tolerance. You're all myisam, you gave the size in rows by what's the disk footprint in MB or GB? Figure out how long it would take to transfer that size between your machines (hopefully there both in the same local network). You could do flush table with readlock, again still show master status, then just rsync the .MY* files over to your new DB's datadir. One final alternative, depending on how you're setup: Can you do an LVM or other kind of filesystem snapshot? This would be the best way to minimize downtime. You flush tables with readlock, show master status, start snapshot, then unlock the tables to allow full read write activity to flow through. The difference here is you'll copy the snapshot you started. You'll just need to feel confident the write activity won't exceed the snapshot size you allocate before the copy finishes. What ever method, before promotion I would verify the character set conversions went as desired. They can be a real pain to reverse. I would also recommend upgrading to innodb if possible to make it possible to use xtrabackup in a non blocking fashion in the future. 

If you go the route I suggested in your previous post I'm going to say < 10 minutes. But again, soooo much that isn't given will affect the wall time. Is this a dedicated MySQL server or is other stuff going on? Specifically things that will be affecting the disk i/o? How fast are the disks? Are they SSD? Is it write through or write back cache? In short there is no "explain" for operations like this that will give you a time value. Don't forget to disable/enable keys during the modifications. Report back and let us know the results. 

I think the bigger problem here is your default database context was database1. Thats's why your slave tried to execute the update on database2 since it was specified in database2.table format. Basically it's not safe to user db.table syntax with wildcards or you find yourself in the situation you did. If you're wanting to use the wildcard do or ignores it's generally safer to always specify your default db using "use" and execute the query in that context. 

That is both S1 and M2 slave from M1; both S2 and M1 slave from M2. I'm wanting to get it to a point where S1 and S2 are stopped and should be in exact sync. As I try thinking this out I keep running into gotchas that won't quite work out. Initial niave approach: Flush tables with read lock on both masters, ensure the slaves are all caught up to their respective masters position and just stop slave. Even if that's scripted to happen one after another it's just a race condition where say a write got to M2 and subsequently S2, but had been been blocked from getting to S1 b/c of M1's read lock (which made it in before). My second plan was: 

Federated tables are going to be inherently slow, especially when doing joins. It's basically copying down an entire copy of the table for each join. Federated tables should simply be avoided. 

After upgrading to Percona 5.6.20 from 5.5.34 I noticed counts on larger tables take a massively long time to execute. The table is about 27G and 300M rows. It is not compressed and was innodb before the upgrade as well. Before a count(*) on that table would take maybe 3 minutes. Now, it's consistently taking over a day to run. The last one I actually let spin until it finished took 36 hours. The DB is on the same hardware as before the upgrade. Buffer pool size remained the same (20G). One config change was increasing the number of buffer pools from 1 to 8. Has anyone ran into this before? Are there any new 5.6 cnf settings that might be related to this? Table schema in question 

The JDBC API lists a includeThreadNamesAsStatementComment parameter that seems to indicate it will prefix all queries with the Java thread ID. I thought this would be useful in tying back DB activity to application logs. I added it to my resource URLs and it seemed to only be working part of the time. Curiously it was only the applications heartbeat thread that periodically pings the database as part of a health check. None of the "real" queries were getting tagged. Why isn't this working for all queries? 

Snapshot are a common practice for backups. See $URL$ For this to work safely though you'll want to ensure innodb_flush_log_at_trx_commit is set to 1. That being said, the general practice with backups is test restoring them. You want to be sure your backup method is working functionally and you want to be sure you know the steps to restore them. If you're having to restore a backup it likely means your in a situation you're loosing money while it's down. You should be able to restore a backup while being groggy after getting a 3am wake up call. If you have a dev environment you might setup a weekly script to restore the backup into your dev environment. 

In addition to what Rolando said, switching to the per file setup may allow you regain some disk space through periodic maintenance. Optimize table doesn't do much for you with everything in a single ibdata file. However in a per file setup you can often regain some disk space space by doing this. Optimize table will effectively rebuild the entire table, eliminating unused space left over from fragmentation during deletes. You can see which tables will benefit from this by running show table status. Alternatively run 

I'm trying to figure out the difference between some warning messages related to data truncation. Consider the following table: 

The problem I forsee is the server crashing and you loose 500 gigs of data. I'll only use in memory engines for temporary things that aren't really production critical or queues that can be ultimately rebuilt in the event of a crash. It would be more prudent to go with innodb and make sure to set your key buffers accordingly and leave the rest for disk cache. It doesn't sound like money is the problem for you so invest in an SSD raid solution for your disks. 

I've been able to find some stored procedures that do this for other brands of SQL than I use. I realize all the required information is in information_schema and I could use that to build the create statement myself. It just seems like this is a problem that's been solved in other flavors and there's likely already a stored proc out there to do what I want w/o reinventing it myself for mysql. Why I'm trying to set some dynamic schema introspection as part of documentation. The idea is I'd like to have the doc pages illustrate a current representation of the schema in the various environment it may exist in (dev, qa, prod). The "obvious" thing to do is just query the dbs and "show create table blah". However doing that requires select access on the table. I don't really care to create an account with global read access to every table in every DB. I'm looking to either have a restricted script on a cron that runs/collects the output of show create table; or have a mysql event that constructs the output I want in an event. Either way this output would ultimately be persisted to a documentation db w/ just the schemas stored so the documentation app can pull this information without its db account having select access to the actual tables. I'm tempted to lean toward an internal event that populates this just so there's not an account out there with wide read access. Why the why I'm explaining my ultimate goal in case someone can point out a different implementation path or solution that matches my requirements (or convince me my requirements are off to begin with). 

I ran into apparmor headaches getting multiple instances going on ubuntu myself. Your configs look legit but I'm not super familiar w/ apparmor to spot a subtle problem if one lied there. In my case I was just running multiple copies of the same binaries w/ different cnfs Your initial error cited 

You can tweak this value in your cnf to be a bit lower. Changes to this will require a server restart. In a perfect world, though, you'd want your buffer pool size to be large enough to hold all your innodb tables in memory for fast performance. Of course this isn't always feasible.