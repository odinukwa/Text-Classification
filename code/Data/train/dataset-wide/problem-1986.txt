Replace calls with logging to a table, as the output of the former will not appear anywhere in case of a scheduled job. 

This is what Oracle Database Vault was made for. $URL$ Note that, it is an extra cost option for Enterprise Edition. 

There are built-in, non-documented simple list types, so you do not need to define your own types at all circumstances, but relying on non-documented features is generally bad practice, as they may change in newer versions without warning. If there is a reasonable limit for the number of arguments: 

There is a do-not-ever-do-this-in-a-live-environment hack you can use where space is limited, by restoring the log file to a compressed folder. Attempt this by compressing an existing folder and restoring to it will result in an error, so you have to cheat with a symbolic link. 

This would be the sledgehammer approach. It's likely you could eliminate the parallelism with appropriate indexing but can't advise on that unless we see an execution plan and or statistics. @deleteEntities in the deadlock trace is a little "suspicious". You're passing in a single identifer but have this temporary table in there?. Optimiser is likely to be producing an execution plan that estimates 1 for this, so if it contains a variable number of rows I'd switch to temporary table and force recompile (as above). 

At this point, the procedure becomes INVALID automatically, but you can not query errors related to it yet: 

There are other files that are not critical, but they need space, for example standby redo logs and the block change tracking file: 

Obviously, if you want, you can just stop the listener, take care of the listener log, then start the listener again. 

It is not that easy, unless you notice some trivial mistake. Cost is just an estimation. An estimation, based on statistics, that may be outdated or non-existent. Even if you have 100% up-to-date and accurate statistics, the optimizer will have a hard time with nontrivial queries, and it will make inaccurate estimates. Explain plan with bind variables makes it even worse - explain plan will simply ignore your histograms and assume that your data is evenly distributed for an equality filter, and it will estimate based on predefined rules for non-equality filters. About the efficiency and small number of rows - unless you know your data and query well, also not. The database may choose a different access path, join order or join method based on the row sources. What works on a small scale, may not work well on a large scale. If you want proof: test and evaluate. If you have a query, and you know the cost of its execution plan is 15957, you know nothing. If you know the amount of elapsed time, cpu time, disk io time, reads, gets, number of rows processed, executions, memory/tmp usage, etc. per step, those are some results that can serve as a baseline. 

Now the most important part of getting these kinds of searches right... apply the 80/20 rule. The majority of calls to your procedure are likely to comprise a relatively small number of the possible variations of parameters. You cannot create optimal indexes for all combinations of 15 parameters, so identify the most common patterns, create static stored procedures for these and index for them appropriately. Deal with the remaining combinations with dynamic SQL, following Erland's best practices. In these scenarios, you will often find the usage patterns closer to 95/5 than 80/20 so the additional work of creating static procedures is not as labour intensive as it seems at first glance. 

Split above current maximum value, drop the empty MAXVALUE partition, then convert to interval partitioning: 

This is described in the note: How to Clean up The information in EM Backup Report (Doc ID 430601.1) Edit , find the procedure , change the default value 60 as needed: 

I have drop this diskgroup from target host and then with " oracleasm deletedisk " This was a mistake. You removed the metadata describing the content. You should have just simply detached the disks, then attach to the new host, leaving the disks intact. 

If you can format the output of this into a readable form and add to your question, someone can make a more educated guess at the best use of your 18 disks. If I was doing this blind, I'd probably start off allocating: 

Licensing is always complex. Microsoft licensing is at least as complex as other vendors. Unfortunately, my experience has been that they have not yet mastered the art of training their own, or their resellers employees about license models. There is much fun to be had by locking a Microsoft licensing specialist in a room with a reseller specialist and watching them argue about the finer points of SQL Server or MSDN license arrangements. If you think SQL Server is a minefield, try MSDN. NB: The "helpful" diagram at the top of the article explaining edition compute capacity limits neatly demonstrates why folk lose their way. 

a) Use an OS that is certified for installing and running Oracle Database (Ubuntu is not amongst them). Native or virtual machine, your choice. or b) here is an unofficial guide for installing Oracle 11.2 on Ubuntu Linux $URL$ I have never tried myself, but several people succeeded installing Oracle 11.2 on Ubuntu with the help of this. 

That timeout can be configured with the SQLNET.OUTBOUND_CONNECT_TIMEOUT parameter. For example, place the below in sqlnet.ora, which results in outbound connections timing out after 10 seconds without answer: 

OLEDB waits cover a variety of states, including (but not limited too) client-side profiler traces, some DBCC commands, materialisation of DMVs, possibly some full-text functions in 2005 (IIRC, most have been split out to FT specific waits in 2008) and also linked server queries as you mentioned. I'm not aware of UDF functions inherently resulting in OLEDB waits, unless they are performing one of the actions I mentioned above. Rather than looking at the raw percentage wait time attributed to OLEDB, have you checked the average wait time per wait? It may prove to be so low as to be insignificant. 

Make sure the folder exists with the correct content. If the above still does not work, set the below before starting opatch: 

Install GoldenGate for SQL Server on the server with the SQL Server database. Install GoldenGate for Oracle on the server with the Oracle database. Here is an example of setting this up: $URL$ It is outdated, but the point is, you can see what goes where in this. 

You do not need to enumerate each object you want to export/import, you can pass a query that returns the list: