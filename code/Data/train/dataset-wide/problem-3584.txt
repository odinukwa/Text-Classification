You want to run tcpdump on the machine serving the web pages. Do not use a host address; you will be capturing traffic only to that host anyway. You want to capture to a file everything on port 80, and you want to make sure you capture all of the data, without truncating it. The command you want is: 

To do this properly, what you really want to do is collect the host public keys of the VMs as you create them and drop them into a file in format. You can then use the , pointing to that file, to ensure that you're connecting to the host you believe you should be connecting to. How you do this depends on how you're setting up the virtual machines, however, but reading it off the virtual filesystem, if possible, or even getting the host to print the contents of during configuration may do the trick. That said, this may not be worthwhile, depending on what sort of environment you're working in and who your anticipated adversaries are. Doing a simple "store on first connect" (via a scan or simply during the first "real" connection) as described in several other answers above may be considerably easier and still provide some modicum of security. However, if you do this I strongly suggest you change the user known hosts file () to a file specific for this particular test installation; this will avoid polluting your personal known hosts file with test information and make it easy to clean up the now useless public keys when you delete your VMs. 

(Note: this was posted before the question poster clarified that by "looking in [the repo]" he was not looking at the files stored in the repo, but at files outside the repo, those being the svn database files.) on the root of the repo (change the URL as appropriate) will give you all of the changes to the repo, from most recent working backwards, listing the files that were added, deleted and modified. Using this, you can figure out if someone moved or deleted the files for which you're looking. If that doesn't work, you have one of two problems: 

Barry Brown, in a comment on the question, provides a clue to a possible answer. The packaging system uses the program to start the server after the package is installed.[1] This program will run to determine the policy on starting that server. The package includes the script, which either runs with its parameters if present, and exits with that script's error code, or exits with 0 (success) otherwise. The interface is expected to offer is documented briefly in the manage, and much more extensively in . The next step, I suppose, is for me to test this. Things remaining to be answered: [1] What other parts of the system use ? [2] Does this actually work? 

Splice a UPS into power cable in order to keep the machine running continuously. Insert a pair of Ethernet bridges between the computer and the network termination point that will bridge the traffic over a wireless network of sufficient range that the host will maintain network connectivity. Open the box and use a probe on the memory bus to grab interesting stuff. Use TEMPEST devices to probe what the host is doing. Use legal means (such as a court order) to force me to disclose the data Etc. Etc. 

As others have mentioned, having a route doesn't necessarily mean you have connectivity. If that's what you're looking to test, offers the option to scan to see if a port is open: 

Ensure that your Graylog process can read the certificate files and key. (It's normally running as the "graylog" user, not root.) "Failed to initialize an accepted socket" is exactly the error that appears Graylog can't read these files. You typically want your key, certificate and other files under (I use ) and owned by the user. Make sure that the key is not readable to group or other. 

You get a message along the lines of , in which case you have the URL to your repository wrong. Find out where your repository is really located. It will have in it files and directories such as , , , and . Your URL is not for the root of your repo, but is within the repo (this will be the case if the version numbers in the log are not sequentially decreasing). Drop the last element from the end of the path in the URL and try again. 

So what are my options for setting this up? As I said before, I'd prefer to have everything (aside from perhaps a small boot partition that does not contain /etc) encrypted, so that I don't have to worry about where I'm putting files, or where they're accidentally landing. We're running Ubuntu 9.04, if it makes any difference. 

If you're running a clustered application, you've got lots of options beyond Ethernet, but you'll need to figure out what characteristics will best suit your application. Often you'll need to make a trade-off between low latency of communications and high bandwidth. In extreme situations, you may want to look at spending more money to use a smaller number of higher-powered nodes to reduce latency to memory-access rather network-access levels. And, of course, you should take a look at your application to see if there are ways of rewriting it to work better with the technologies out there. Wikipedia offers a handy list of network technologies and nominal bandwidths that you can use to start off your research. It gives nominal speed (the actual throughput you'll get will be lower) and doesn't discuss latency. Note that if you're not using the latest and greatest servers, you need first to look at what you've got available in terms of internal buses. You can certainly put a 10GigE card on a 64-bit 66 MHz PCI bus and run faster than with a GigE card, but you're not going to get anywhere near the network's nominal rate of 1 GB/sec or so because the bus can only do about 500 MB/sec. As far as whether you should "use a routing switch stop Ethernet clashes," if you're talking about using a switch instead of a hub, these days that's pretty much automatic. Hubs are darn hard to find, in fact. However, not all switches are created equal; one that might handle two hosts transferring at 100 Gbps may not handle six doing the same.