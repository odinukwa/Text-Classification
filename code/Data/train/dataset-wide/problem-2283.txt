This isn't a direct answer to your question, since what you want is trend analysis, but you should familiarize yourself with the reports in SSMS at the different node levels. They are much-improved over previous versions. There's one at the database level that shows size of individual tables. Look into the Management Data Warehouse, as well. 

I'm planning a project to upgrade internally-developed app from SQL Server 2005 to SQL Server 2012. I hope to use Service Broker as a bridge while in transition. Are there any issues to consider in a forward-compatibility scenario like this other than newer features like multicasting not being present? I can't find anything in BOL specifically about compatibility. 

I have an app to deploy in production that uses 'honor system' security. That is, all users connect to the DB using a SQL user/passwd credential and the app manages permissions itself. The latter part doesn't bother me as much as the fact that the connection object contains embedded credentials and can be copied around freely. I'm try to find some way to limit connections to a more limited set of clients. I can create firewall rules to limit by IP, of course. Is there any way to 'prequalify' SQL logins either by Machine account or domain membership? 

In SSIS(2012 specifically) there is an option in the Flat File Datasource to skip a number of lines at the beginning of a file. The preview appears correctly. However, when trying to execute the data flow, it appears that those rows are still being validated; so that if they contain invalid data(blank rows), the datasource fails. This prevents using a conditional switch to remove rows, since the data flow never starts. Is this accurate description of the normal functioning and is there any way to work around it? Sample data looks like - 

Not really. Since Excel isn't a proper client server database, you can't ask it how many rows are there before bringing them in. You could bring them into a staging area and count rows as they come in. If they don't match and you need to handle that with a business process, the destination is untouched. If they meet your validation criteria, do a second transfer to the real destination. You can use a database table for this, or a raw file connection if you'd rather spill to the filesystem. Cache connection managers are useful if your dataset is reasonably sized. 

I'm looking into Central Management Server and I'm a bit surprised there's no 'Custom Reports' node in SSMS to add things into. It seems like that's exactly where you would want to place something like that. Failing that, is there any documentation on how to create SSMS add-ins that use the CMS server list? 

This is almost exactly the example used for 'factless fact tables' in analysis services, which illustrates the point. The thing represented by the table is a 'booking', which has no natural primary key. The true key would be a composite key of location-timeslot. You need to create an artificial primary key if you don't want to do that or the DBMS does not support that. You need at least these tables- Location(room #, Capacity) Timeslot(day,starttime), module(code,name). It's not clear if the person is tied to the module or tied to the booking. It's not clear if the extension is tied to the person or the location. 

I'm trying to perform the same scenario as in the following link; Create a SSIS Script Component as a Data Source that uses a pre-existing HTTP Connection Manager to retreive a page with GET and emit rows into the Data Flow pipeline. $URL$ My target platform is SQL Server 2008 and therefore C#. The MSDN documentation gives examples of File and SQL Connection Managers but not HTTP ones. $URL$ The specific problem is that I can NOT figure out why there's no HttpClientConnection constructor in my current context. The MSDN documentation of that class does not seem to apply in the case of Script Components and translating this to something useful is apparently beyond me. $URL$ My non-working code looks like this - 

Developer Edition was created for this specific reason and can be used perpetually. It was $50 USD last time I looked. Like the Eval, it has all the Enterprise Edition features. If that's not the target for deployment, take care not to use those features. Another thought is that Amazon offers AWS instances that include SQL Server licensing built in. You can test deployment against a Standard Edition Instance and only pay the hourly usage rates. 

I'm trying to implement a writeback measure group for required quantities by month and part number. I'm puzzled by the behavior I am seeing in Excel 2013, but I don't know if SSAS or Excel is responsible. If the dimension selection in Excel is not the native granularity of the the underlying Measure Group, 'Publishing' the data distributes the data as equally as it can among the dimension members. Which is not necessarily wrong, but a bit weird and definitely unexpected. e.g. If I publish a value at the year level, it will divide it equally into quarters and months. If I don't include the part dimension it will divide the quantity equally into all part numbers. I need to prevent data from being published anywhere but at the intersection of Part and Month. My only thought so far is to control write access through the Role with an expression that includes the Measure and Dimensions, but I'm not having much luck with that due to my limited MDX skills. Is there any other mechanism to modify writeback behavior? 

I have a number of XML Schema Definitions describing business objects. It's unclear to me what the correct way to deploy these is if my intention is to make typed xml columns. The name Xml Schema Collection implies that more than one Xsd Document definition can exist in a collection, but any example I look at shows a Collection as the specific constraint on the column, not a document within it. Do I create a schema collection called 'BusinessObjects' and create individual Xsd Documents inside it? Do I make one Collection for each Document? 

I have few SQLCLR functions implementing business logic that return SqlXml data. I would also like to make the Xml Schema Definitions in the dll available in Sql Server. It's not a 'SqlUserDefinedType', as it is used indirectly by the Xml type. However, the collections node appears under 'types' in SSMS. I would prefer to keep the xsd along with the functions to simplify deployment. Is this possible? 

I'm re-designing some SSIS packages for 2012 and some parameter/variable/configuration issues are not clear to me. The result that I am trying to achieve is that the user context's temp folder(stored in the %temp% OS environment variable) is available in my package. Specifically, when I open the package in SSDT, it should expand the environment variable to the current path so that it passes validation. I can get this at run time with script task, but it's ugly working around it at design time. In the SSIS catalog, I can apply an environment variable to a parameter, so that aspect is not an issue. How can I get this to happen when a project is opened? 

After investigating a production issue, I have found a number of references to SSIS packages getting stuck in 'validation' when run with DTEXEC or through as SQL Agent job steps. Specifics of my situation aside, is there a technique to make this more robust so that in a production environment, I am notified if a job starts, but does not complete? Is it possible to have a timeout period on validation so that the execution explicitly fails? Or have a way to specify number of retries? 

is returned. Is there any way to achieve this behavior? To clarify, the behavior I would expect to see is demonstrated in this example 

I'm troubleshooting a SSRS security issue where one user is being denied access, while another user with apparently identical settings is working as expected. I'm seeing "The permissions granted to user 'mydomain\myAccount' are insufficient for performing this operation" in the browser, but curiously NOT seeing the (rsAccessDenied) at the end. Is this indicative of something that can help me troubleshoot the issue? 

How do I structure a conversation or series of conversations to touch all three systems? Can I have 'A' submit to 'O', which does a lookup into 'C' - all in the context of a single conversation? 

Do maintenance plans provide any functionality beyond their component parts? It would seem that the only value added is the convenience of grouping subplans into logical bundles, and the simplified GUI for creation. All the actual work is being done by the underlying SSIS tasks which are being scheduled as agent jobs. Is there anything more to it than that? 

I have a drill-through problem apparently related to security. Users in one role are seeing timeout failures when trying to invoke a drill-through action. They have permission on the action through that role and seem to have all necessary dimension and cell security rights. Profiler is not showing me any obvious reason this is failing. What else can I use to debug this? If I modify the user's role membership, the action works as expected, so I'm confident this is related to security somehow. 

In my specific case, this was a result of cell security. The role did not have access to all the fields returned in the drillthrough action. I missed this because there are two very similarly-named fields and my eye did not notice. It's still odd that a security denial creates a query timeout, but at least this note is here for the next person. 

I have done a bit of experimentation with non-relational datastores and found the integration tooling is usually poor or non-existant. It's probably more productive to approach this as a pull from the MarkLogic side than as a push from the SSIS side. 

That's either a problem with MySql or with the driver you're using. All SSIS knows is that the driver says 'I can't do that'. The place to start is by manually inserting as record of that kind into the destination db and looking for a more meaningful error. 

Import it as a string and do a derived column to convert it to a float. Whatever rows fail conversion will be re-directed to the error output and you can at least see what the problem is. Flat File connections fail completely when they encounter problems. It's generally more productive to bring them in as loosely as possible and adding structure inside SSIS. You almost certainly don't want to be using floats anyway. Use 'numeric' once they're inside the package. 

That's a filesystem concern, not an SSIS one. Any file that SSIS creates will be owned by the user SSIS runs as. The best solution would be to set the ACL on the target folder to be what you want; when SSIS creates the file it will get those permissions by default. If you can't do that for some operational reason, you can manipulate the file after creation with a command task. You can use SetACL or any command line tools for this. 

You are almost certainly 'doing it wrong' in one way or the other. If your application truly needs to operate that way, you probably should be using some other sort of datastore. Maybe something like Hadoop or a key-value store. Alternately, you may be doing something that is inherently a batch operation and should be done on the server side rather than remotely. Your description sounds like load-test simulation software designed to find the limits of connection pooling. 

I have a server with a working installation of the Sql Server 2012 SSIS Catalog. I need to set up an additional instance including the SSIS Package Store service as an interim step while the packages are being re-written. The Package Store is a per-server feature, not a per-instance feature. Can these two features operate side-by-side? 

You need to work your way up the networking stack to determine where the issue is. Can you ping the destination from the source? Can you telnet to the postgres port(5432 by default) from the source? Can you connect to the postgres service with a management tool(pgadmin or psql) from the source? 

I'm trying to implement a Service Broker architecture for a business process involving three systems and I want to do it in the spirit of using SB as designed. 

This query works in the simplest case, but does not allow adding attributes to the root element, which is required in my case. 

If you intend to run the SSIS jobs from the SQL Agent, then they will execute as the user the agent runs as. That's usually the same as the account the instance runs as, which has rights to everything. 

I'm trying to make a multi-statement table valued function and failing due to what I think is a limitation of functions and CTEs. The intended function defines an input parameter(@Param) and and output table(@ResultTable). It then executes a complex insert statement involving a CTE into that table variable which is (of necessity) terminated by a semicolon. I then attempt another complex update statement to that table variable involving a CTE and receive an error saying that "Must declare the scalar variable "@ResultTable". Apparently it has dropped out of scope somehow. I have used this sort of pattern in the past, so my only thought is that the CTEs seem to limit the scope in some way. Is this a known limitation? 

It's not clear what you're try to do. It might be that you want a read-only copy for reporting; in which case you should probably consider log shipping, db mirroring or the like. If you just want to be able to query data from another server, linked servers are an option. The queries would always return fresh data, but there's a performance hit to consider. SSIS is the correct tool, if you want periodic secondary copies updated batch-style. Service Broker is almost certainly far too complex for your scenario, but addresses asynchronous cases. 

In that unpleasant case, I think the best strategy is to compartmentalize it away from SSIS. Either move that out to a linked server with an altering view or use some other etl tool like a powershell script to get it out of the dancing schema into a table with a fixed one. If you have any history you should be able to guess a reasonable value for the upper size of the field. You could always make it varchar(max). I'm presuming MySql is doing a 'select into' to create the table. In any case MySql is not making any reliable assertions about the data, so you need to do that yourself before SSIS is going to be happy. 

I'm starting a few new database projects and I'm attempting to create them at Data Tier Applications. There are two items I'm not able to find documentation for. I would like to set the db owner to SA and set the initial filesize and growth rate. Even if those items are outside the scope of the app, I would expect that there would some way to specify that at publish time, either in SSDT or SSMS. I can find no documentation either way. Is this the case? 

As a newly minted DBA, you should understand that, as a matter of policy, SQL server will consume all the memory in the system to the exclusion of all other programs. If you do not want that behavior, you need to set the MAXMEM setting for the instance. SQL server is not just an app, it has low level functionality that overlaps with things you would ordinarily expect the operating system to handle. That's not a problem, it's intended behavior. If you are on 2012, look into "xVelocity memory optimized ColumnStore index". It's a way to get a taste of columnar performance without re-architecting your schema. You could also look into the SSAS Multidimensional and Tabular/PowerPivot/"xVelocity in-memory analytics engine" products,but those would be a significant effort. If you're really forward thinking read up Hekaton. I'd also suggest you start reading Brent Ozar's Accidental DBA 6 month training plan. 

There's no answer that applies in all cases. In general, however... If the lookup list is small and you can cache it(or use a cache data source), there's not much performance penalty to doing it in SSIS. If you want crossref a list of 50 location codes to names of cities, go for it. It's nice to see all the process on-screen in one place, rather than buried in sql statements. TSQL will be better-performing in most cases, since it knows the most about the data and the query optimizer is always going to be smarter than you. If all the data is in one DB, you can hide a lot of complexity in a sql query source. If the data is spread out across different systems, the middle ground is to to do an SSIS merge join from each system. Trying to do that at the RDBMS level is madness. Always do the sorting in the source query, though. SSIS Sorting is almost always a bad idea. 

What edition and licensing mode are you in? You are probably not using all the cores. See the note on this page - $URL$ "Enterprise Edition with Server + Client Access License (CAL) based licensing is limited to a maximum of 20 cores per SQL Server instance."