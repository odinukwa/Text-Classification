To make an efficient compare of all columns between 2 tables, you could use a UNION method with GROUP BY as described in Diff2Tables. Alternatively, if your profile table can have rows deleted from it without cascade delete, you could, drop indexes that are not for PK of the profile, Delete rows having the same key in both tables, and insert the rows from the other table into the profile then re-build the indexes on the profile table again. This may sound bad but in fact it is not that bad. This is a typical scenario in Data Warehousing. 

This is a typical 1 to many relationship (or parent child relationship). The parent would have the fixed infromation and the child would have the many varying information. The key of the parent must appear in the child. Example: Receipt : {ReciptID, Date} RecieptItem : {ItemID, CustomerPrice, Quantity,TotalPrice,ReceiptID, Status, Date} Item:{ItemID, ItemName, Price} Now each Recipt can have 0,1 or more Recipt Items. Each Item is references on 0,1, or more ReciptItems. The column ReceiptID in ReceiptItem is referred to as a Foreign Key. A constraint could be built to make sure that every ReciptID in ReciptItem corresponds to a ReceiptID in parent Receipt table. Example ERD: 

Do you know of an industry standard that is designed to describe the metadata contents of a relational database (or part of it, such as tables, columns, etc.)? Do you know if a tool exists that does export the metadata of any of the top 5 RDBMS tools in any format such as XML (without having to type in DDL in the syntax of particular database)? Thank you. 

I believe that I have most of the work done for whoever wants to tackle this question. I have the following query built: 

We use Quest Spotlight for database server monitoring. Depending on when SQL server decides to fail over automatically, we will get an alert stating that a database has not been backed up in over three days (the factory default) if that server has not been primary in over 72 hours. My suspicion is that SQL Server does NOT replicate this data, but I wanted to hear from the community since a quick Google search didn't provide anything. Plus, I am travelling and do not have time to test and I need to provide a fairly immediate response. 

I have a few databases in which the frontend application manages certain maintenance tasks. In this particular instance, I'm running the INDEX OPTIMIZE job every night and every so often it clashes with the application and fails. The following is the error that I receive: 

In short, put the OSes on slow storage and all the data/log & backup files on fast storage. I know that my environment is different that yours, but I think similar principles apply. We have a SAN and the storage pools range in different speeds, 7.2k, 10k, 15k. We install the OSes on the slower drive pools and have all backup files and data/log files stored on the 15k drives. There was a time when we accidentally put everything on the 15k drives and we didn't notice a performance difference. It was brought to our attention by our SAN admin after an audit that the OS was stored on the faster drives. We monitor SQL Server performance with perfmon and Dell Spotlight. For grins, we went and looked at the historical metrics that mattered, and saw nothing to note. 

When a cube is "materialized", all combinations of aggregates over all dimensions are computed and stored.. the WITH CUBE operator does exactly this.. $URL$ 

Nope no native method in 2005 (there are DMVs in 2012, dm_os_volume_stats), however this "hacky" xp_cmdshell might work.. $URL$ 

Running DBCC SHRINKFILE without the TRUNCATEONLY option on the primary will propagate the change to the secondary once the log is applied, so it will shrink to the same size.. 

You can run you query in a transaction and run sp_lock @@spid after running the query, you will know all the locks held. 

If I create a table with an identity column as primary key, and all the other columns have default values, for example 

Based on your model, it looks like its unlikely that more than one car is owned by an owner (I guess at most two, if the car owners are private individuals), and I guess a parking contract cannot exist for more than one car (these are some assumption I am making, please correct me if I am wrong).. In that case just one table with all the columns will do.. A few rules that I would like to mention are: If you see a use case where a car_owner or a car can be reused elsewhere (maybe in a duration table to determine the timings of parking), then splitting the tables to normalize makes sense, if there is only one instance of a car\car_owner throughout the data model for a unique tuple in all cases, using a single table is fine. To optimize storage you can define subtables for values that repeat frequently (like say the Car_Make). However this should bot be done for small data modes. 

I have the following remarks: 1-According to your question's text, the fk for city and country are repeated unnecessarily in the Address table. given the city id alone, you can find the country and state by a join. Repeating them represents redundant relationships. This desing may speed search by country or by state slightly though. You'd have to include 3 FKs in the Address table if the PK of State is composed of StateID+CountryID and the PK for city is CityID+StateID+CountryID. If you do this, the the FK in the address table becomes the compound key of CityID+StateID+CountryID. 2-Table names should be singular, e.g. City not Cities. 3-The Code column is not clear to me, is it unique? If it is, why use another ID? 4-The 'Address' table is probably meant to represent the address of the resort. However, this may not be quite practical. Some resorts have more than 1 distinct (sub-resorts) that may require different (or maybe the same) address. Also, there could be a management or contact address that is different from the resort address. You need to take care of this if your business rules demands it. Edit - Attempting to show that only 1 FK is a must in the Address table and the rest are redundant from modeling perspective. FK2 and FK3 could be obtained by joinng the Address table with Country, State and City tables when only FK1 is present in the Address table. 

Looks like that is a default setting in postgresql 8.5 onwards to guarantee recovery of higher precisions floats.. $URL$ 

I get an arithmetic overflow on a datetime column (error converting to smalldatetime), however the destinations schema that got created has a datetime column and not a smalldatetime column.. 

You can do this using SQL Server alerts, which can trigger a job after the WMI event occurs. Your WMI Query in this case will be 

You can simply add an "is_student" bit column to the relation "UserToFOS", and create a unique key on user_id, fos_id and is_student. 

Since one car may not have more than one parking contract, it is fine to have CAR as a part of the PARKING CONTRACT TABLE CREATE TABLE PARKINGCONT_TBL(contract_id,,car_regnumber,,owner_id [FOR_KEY TO OWNER_TBL] CREATE TABLE OWNER_TBL(owner_id,) The question of adding a customer without a contract should not arise because whatever is inserting (the application) should create a contract (not a customer), so a owner is added only when a contract is created (there is no way to enforce this, since there is no dependency between owner and contract, and creating one will result in a cyclical dependency) 

The namespace refers to a schema (select * from [schema].[tablename]). If you search the "SQL Reference guide" you will find the term used interchangeably, although, schema is used a lot more since its standard SQL. 

You're correct about how the VLFs grow in size. Check out the following video from Jes Borland for some more tidbits. How SQL Server Works: Log File (Video) 

Is there a command at the CLI or in PowerShell that will list the components installed for SQL? I'm looking for something like the Feature List you can get from running the discovery report from Tools in the SQL Server Installation Center program option. I'm running SQL Server 2012 on the CORE version of Windows Server 2008 R2 Enterprise. I've searched around the Net, but haven't found anything useful. 

Everything in the result set was correct, save monitor_server, it had the value of OddServer03! As many of you know, when configuring Log Shipping through the GUI, it is damn near impossible to "accidentally" configure this option. We never intended having a monitor server in the mix. We use Quest Spotlight to monitor the aforementioned servers which will yield alerts when jobs fail or when log shipping gets behind. One other perplexing matter, the job that is getting created is instead of . Why is the service account trying to authenticate to a server that isn't part of the configuration? 

I've had a maintenance plan in place for years that has run without fail and the associate logs backup this claim. Also, the backups are time stamped and in the place expected. I did a restore in a test environment and ran a consistency check without any errors. I'm not sure that I really see this as a true error or cause for concern considering the aforementioned evidence. So, maybe this is just an anomalous event? Either way, I'd like to poke around and a assuage my sensibilities. Has anyone ran into this situation before? If so, what did you do to investigate? 

First, I suggest you read about star schema first. Not understanding the concepts could lead to wrong results. You may use the transaction as your fact table as in the diagram below. Don't use DISTINCT, instead you want to use: SELECT ... FROM fact GROUP BY dimension data (time, transaction and bank) WHERE condition to restrict dimension data and join with fact. 

Having a table for each data type has nothing to do with normalization. Nomralization is about not repeating base information in tables. It is valid to have a key for a row and different columns each having its own data type. In fact this is the most common way. 

A user takes zero, one or more quizes A quiz contains 1 or more questions A user provides an answer for zero,1 or more specific question on a specific quiz 

If you want to do this in an on-line transaction, then you probably have the wrong database design. If you are trying to do this in batch, then select only the fields of interest to a flat file, or export to a flat file then process your data from the flat file. You could even split the output file to several files and run parallel tasks to process the data. This will at least free your database server resources during your processing. This practice is not uncommon in data warehousing applications. 

The answer here depends on a lot of things, since you are using direct disks (Local RAID) and not SAN disks which are wide striped (I assume), sequential operations are faster, so the logs (tempdb included) are benefited in E: . However there can be a situation where you don't want tempdb activity (logging activity) to interfere with the logging on your other databases, this can be the case if your tempdb activity is erratic and not related to application performance, while the logging performance of your databases is directly important to your use case. I would generally test both setups and see which does better under the load you intend to use (the benchmarks could be transaction times for SQL Server, or disk queue lengths or writes\sec on the log drive). 

and reference the first table with this, while inserting just look up the key and insert the key_id in SearchKeyword 

To answer the question, you would need to test how many rows your change will affect (an update to a column without a where clause, or adding a column will affect all rows). A reindex will not cause replication to be "rebuilt". Can you clarify what do you mean by "rebuilt" do you mean all rows are re-replicated? or do you mean you are forced to snapshot? To know what is actually being replicated, you can use $URL$