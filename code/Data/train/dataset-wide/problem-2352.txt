If you want the simplest approach, just create a Maintenance Plan within Management Studio. Expand the Management node, right click Maintenance Plans, and choose Maintenance Plan Wizard. The plan can be configured to perform backups using a separate file for each backup, and the Maintenance Cleanup task will delete the old ones after a specified number of days. It's a bit limited in terms of flexibility, but in the majority of cases, it handles the backup needs fine. 

I won't go into detail about why it's not a great idea, since you're already aware of all that. For anybody else stumbling across this answer: Try not to do this if possible! For the occasions when I need to schedule an after-hours restart (patching, configuration changes, etc), I normally just run this in a batch file scheduled via Windows Task Scheduler: 

...Then hit F5 in the VM I was browsing from, and the directory for PDF reports showed right up. I was able to browse in there, and copy some files to the directory. So I think I'm good, but I'd appreciate anybody piping up if I've done anything bone-headed. 

Using a database snapshot located on your production OLTP server will, in all likelihood, make performance worse. There are two main reasons: 

That one is an old trick I found somewhere on this site. I wouldn't use this in any production code, what with all the SELECT *, but for quick interactive querying, there shouldn't be any harm. 

The fact that the backup is 365 MB while the transaction log is almost 30 GB and the database is in FULL recovery, and yet there's never been a transaction log backup is a bit perplexing. There may be something else going on that's allowing for transaction log reuse besides a backup. If you want to double-check whether log backups are happening, you can do this from Management Studio. Right-click the database, and go to Reports, Standard Reports, Backup and Restore Events. Expand "Successful Backup Operations" and see if any log backups are happening. If not, then check whether or not the database is regularly being changed from FULL recovery to SIMPLE and back to FULL again. This will show up in the SQL Server error log as something like . With that out of the way, you can shrink the transaction log to something more sensible - probably around 25-50% of the size of the database - using . You'll want to make a transaction log backup first to make sure active log space is marked for reuse. Next, run to get the name of the transaction log file (the logical name in the column, not the physical filename), then shrink it to an appropriate size, e.g. (the size is given in MB). Note that it may not fully shrink to the target size depending on what portion of the log file is in use. If that's the case, you'll want to run , then run another log backup, and again. Then for ongoing maintenance, I would recommend leaving the database in FULL recovery, and adding a nightly transaction log backup just before the full backup. This way, you'll at least be able to recover the database in the event of a failure of whatever volume the data file is on (assuming the transaction log file is still safe), and also retain the ability to do point-in-time restores. 

It's nothing too fancy, obviously. I also have a few calculated measures that use ParallelPeriod to calculate YTD figures from the previous year, for quick side-by-side comparison without requiring the user to choose a specific slice of dates. Just pick the current year, and it will find the latest date with sales in it, then compare to that same range from the previous year. Finding the appropriate date in the previous year normally boils down to this: 

They are both 100% required. However, mssqlsystemresource is read-only, and specific to your version/patchlevel, so it's easy to restore, in theory. 

If you want to omit those specific SQL statements, you could probably put them into the TextData NOT LIKE filter for the trace. Escape the underscore with . 

Without reading the transaction log and/or log backups (which may be impossible, if msdb is set to the default simple recovery), I can't think of any good way. Going forward, if you want to log the changes, you could create some log tables, and add logging triggers to these tables in msdb. 

I've got this fairly simple reporting procedure that searches through inventory transaction data (stuff being received, moving around the warehouse, etc). The whole thing is listed below - I've only anonymized the procedure name. It's pulling data from Dynamics GP (explaining the awful table/field names), and an internal data warehouse. 

(Note that I've simplified the query a bit. The real thing has an explicit column list and an , but the semantics are the same. I get the same symptoms with either version.) Most likely, SQL Server is able to take into account when creating an execution plan for the first query, but parameterizing it is causing the whole view to be run and then filtered by person_id. So I figured I'd create a plan guide. And now I have two problems. These are the steps I'm taking, which appear to have no effect. First, run the 'good' query to get it into the plan cache... 

Since TDE relies on a certificate stored in master (which is used to encrypt the database encryption key), then this would work only work if you could restore the master database to another server in such a way that the certificate could be decrypted. This is the TDE encryption hierarchy: 

Tune your mdf files for random read/write access in 64 KB chunks. Tune your ldf files for sequential write access in 64 KB chunks. 

Unless you're referring to contained databases (which are new in SQL Server 2012), you can't use the second approach. Whenever a person authenticates to SQL Server, they are authenticating against a particular server login (or possibly multiple logins if you're creating logins for domain groups). This server login can then be mapped to one or more database users to allow access to different databases. Unless you're doing some tricks with EXECUTE AS or using application roles, there's no way for somebody to authenticate directly as a database user without a server login being mapped to it. 

This effectively turns your outer join into an inner join, as the join between and is not optional. You would likely want to write the expression like this, joining records in to records in , but only if they match a record in : 

I don't know if this will work with PDW specifically, but I've got an awful, dirty hack I use with user-defined functions, which also don't allow RAISERROR. Just attempt to cast a varchar literal containing your error message to int. 

If you're setting MAXDOP at half the cores, and other queries are still getting sidelined, you may be dealing with contention for memory or disk I/O. How much RAM is in the server, and how large is the application database? Take a look at these perfmon counters when performance takes a nosedive, and you can get some idea if the server needs more memory or a faster disk subsystem: SQLServer:Buffer Manager 

While I haven't run AlwaysOn in production on 2008 R2, I have experimented with a VM lab using 2008 R2 guests on 2012 Hyper-V. The only real issue I encountered was having to install hotfix KB2494036 to enable changing the node weight property of the cluster nodes. My test environment simulated three quorum-voting nodes in the primary facility, two of which were hosting SQL Server (with Availability Groups), and a fourth "off-site" non-voting node hosting a third replica of the Availability Group. I threw a bunch of tests at it, like ungraceful power downs, fiddling with TDE settings, manually joining nodes to the AG, etc. and I at least wasn't able to make it break down in testing. There may be some edge cases where 2008 R2 clustering falls apart, but I wasn't able to uncover any while playing around. However, if I were building a new production machine from the ground up, I'd probably use 2012 to be on the safe side, and avoid the manual hotfix patching. 

I would probably add a boolean column to the rooms table indicating "virtual rooms" (room "ABC", for example, would have this set). Then add another table called VirtualRoomMembers, with two columns, both of which are foreign keys back to your Rooms table's primary key (assuming here): 

Are you copying the mdf/ldf files with SQL Server running? That will almost never give you a usable copy, unless there are no writes to either file during the entire copy process. Either stop SQL Server or detatch the database before copying the files. 

The best out-of-the-box solutions I've found are to use a combination of the slow query log (which sucks compared to Profiler), and just running Wireshark on port 3306 (which really sucks compared to Profiler, and won't work if you're encrypting connections). There's also SHOW FULL PROCESSLIST, which is like a reduced combination of sys.dm_exec_sessions and sys.dm_exec_requests (with a little sys.dm_exec_sql_text thrown in). 

To make a long story short, I have a view called vwRelatives which uses CTE recursion to build family trees. It's meant to be queried for a single person at a time. This runs in about a quarter second: 

I've done this in SQL Server using a modified version of Dice's Coefficient. Basically, you store a precomputed table of q-grams from your lookup data set. This table should have the primary key of the source record, the resulting q-gram, and the number of times that q-gram appears when breaking up the string. Then when you want to do a lookup, break your input string into a similar list of q-grams (temporary table, subquery, etc.), and join those to your lookup table on the q-gram, grouping by the primary key from the lookup row (which, in the lookup table, is actually a foreign key rather than a primary key). For each matched q-gram, calculate the number of matches as the minimum cardinality between the input value and the lookup table row, e.g. the string 'aaaa' could have 'aa' * 3, and 'aaaaa' could have 'aa' * 4, thus you would have 3 matches. Double this number to get 6. Once you've got this doubled number of matches, divide by the total number of q-grams obtained from both the input string, and the rows in the lookup table associated with a given primary key. So 'aaaa' would have 3 q-grams, and 'aaaaa' would have 4. You would calculate the similarity as 6/7. You can then filter this similarity quotient based on your desired threshold. The nice part about this algorithm is that it's fast, since after splitting your input string, it's just a bunch of index lookups. The downside is that it doesn't put any emphasis on ordering of q-grams, so having two words out of order won't affect the match quality (assuming you're not letting q-grams cross word boundaries). Following is some sample code from SQL Server to explain the concept, but it makes use of features not found in MySQL. You'd probably need to rewrite some of this using subqueries rather than common-table expressions. 

I've got an UPDATE trigger on a table that watches for a specific column changing from one specific value to any other value. When this happens, it updates some related data in another table via a single UPDATE statement. The first thing the trigger does is check to see if any updated rows had the value of this column changed from the value in question. It simply joins INSERTED to DELETED and compares the value in that column. If nothing qualifies, it bails out early so the UPDATE statement doesn't run. 

Oh, damn it, not 30 seconds after I post this, I stumble across sp_altermessage (naturally, after spending a significant amount of time trying to figure out if I could do this with extended events). So if anybody else is wondering how to do this: 

It's difficult to say exactly what's causing it without seeing the code for the trigger, but I'd bet some non-zero amount that misuse of the INSERTED and DELETED virtual tables is to blame. The thing about INSERTED and DELETED is that they have NO INDEXES. If you join them directly back to the base table (or to each other), you can end up with some very awful performance. You can generally mitigate this by loading the two into temp tables and creating indexes for however you'll be joining them (usually on the primary key). 

Or you could combine the two; look for earliest matches first, and within those, prefer shorter values. 

Welcome to hell. I've found that the most reliable way to get data from SQL Server to Excel is to use the querying capability from within Excel itself. The location of this function has moved around in recent releases, but right now, it's under Data, Get Data, From Database. This usually gets the best results for various data types, long data, etc. 

I have a calendar date dimension backed by a physical table of dates (originally created on SQL Server 2000, hence the datetime instead of date): 

I can't explain why the first one isn't working (perhaps some subtlety of that particular Unicode character). This produces a rather unexpected result: 

In the equality conditions, I like to list the tables in the same order they appear in the query. That's just personal preference, and from a technical standpoint, it makes no difference if you swap the sides of the operator. Obviously, for more complex (in)equality statements, or conditional expressions, you may be forced to improvise a bit. For outer joins, the vs. designation is simply to allow telling the DBMS which 'side' of the join should have all records returned regardless of no matches existing. A full outer join includes all records from both sides. The table listed first is considered 'left', and the table listed second is considered 'right'. Potential outer join pitfall: 

If that still doesn't work, check DBCC OPENTRAN, and the log_reuse_wait column in master.sys.databases. 

You can't have nulls in a primary key column. You can, however, create a unique index/constraint on a column, and that column will then allow only a single null for the whole table (in addition to requiring unique values). 

I'm running SQL Server Management Studio 2008 (10.0.5500.0) and was attempting to edit the built-in template "Create T-SQL Trigger (New Menu)". I open the Template Explorer, right-click the template, choose Edit, modify it, then save. But when I try to use the template by right clicking "Triggers" underneath any table and selecting "New Trigger", it still uses the default template. In Process Explorer, I can see it accessing this file when I edit the template: 

The result is that you change it from a many-to-many join from "person" to "person", to a many-to-many join from "person" to "friendship". This will simplify joins and constraints, but has the side effect of allowing more than two people in a single "friendship" (though maybe the additional flexibility would be a potential advantage). 

Yes, although it's possible that some new functionality might not be supported by the older client. We have a number of programs using the 2008 Native Client to connect to 2012, and haven't observed any problems. 

After that, you'll have a query you can bind to a form or report, much like you would with a linked table or any other query. It will, of course, be read-only. You can still use all of Access' filtering and sorting functionality on the recordset, although it may not be quite as optimal, since Access has to fetch all the records and filter client side. Linked tables allow for some of this to happen server-side, I believe. But going back to the original question, I think it's reasonable to encapsulate query code in user-defined functions. I do this in cases where I need to be certain that code won't be duplicated, and slowly drift out of sync in multiple places. For example, I have a function that selects an appropriate commission calculation rule based on assorted input parameters (salesperson, item properties, customer properties, etc), and this function is the only place where that happens. Any procedure that needs to calculate commissions cross/outer-applies to that function where needed. It's a good practice for cases where you need to be certain that the logic you implement stays in one location.