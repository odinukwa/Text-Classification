Languages like Japanese traditionally used vertical writing systems where the columns proceed from right to left: 

They can be very similar to and sometimes the same as main clause or subordinate clause open interrogatives: 

(In the sentence above we might not feel that Bob has moved to the beginning of the clause from another position, exactly. It is more as if a second Bob has been deleted from the end of the smaller clause. Again, this would depend on what kind of grammar you subscribe to.) Now it is the relationship between these gaps and the words or phrases that they depend on for their interpretation which we refer to as long distance dependencies. Why do we call them long distance dependencies? Well, for one thing, these gaps are non-adjacent to their antecedents. We could suppose that we could measure the distance between the antecedent phrase and the gap itself to see how long distance this dependency was. So consider the following sentences: 

Final obstruent devoicing—where a lenis consonant is systematically and cumpulsorily replaced by its fortis counterpart word finally and before fortis consonants—is a feature of many languages, for example Catalan, German or Turkish. This is a case of one phoneme being replaced by another. Notice that this is a phonological rule and is very different from the phonetic devoicing undergone by lenis consonants, in English for example, when they occur next to unvoiced sounds or next to silence. These consonants remain lenis, even when devoiced and the devoicing can in no way be thought of as being 'compulsory'. When said in isolation the [d] in bed is likely to be devoiced, but will nonetheless remain a [d]. 

As with the other respondees here, I do not know the answer to your question, but can speculatively suggest one main one. All natural languages accommodate the fact that there is a pay-off between time and effort on the one hand, and informativity or cognitive effect on the other. So we will prefer a shorter and less energy- or time-consuming way of saying something given a choice and an identical cognitive effect. It is simply much more calory-expending to use sign language than to use an air-pressure mechanism to communicate. However, the reasons may be multi-factorial in any case. So a preponderance of factors weighing in on one side or th other may make a difference. One other factor wich may benefit an air-pressure mechanism (i.e. sound) is that a lot of spoken language occurs when there is little or no light available, which is clearly very useful. 

Basically each of these algorithms center around calculating the probability of the lowest common subsumer between two word senses $c_1$ and $c_2$, which is the lowest node in the hierarchy that is the parent of both $c_1$ and $c_2$. The probability comes from the distribution sampled from, i.e. the corpora used. Resnik is the simplest implementation of this, while Lin expands it by considering similarity as both the information content shared between two senses, and the difference. Jiang-Conrath is actually a distance function, best summarized from the chapter: 

I would leave it as an exercise to compute this for other metrics, but I am 100% certain you will get similar results for any other metric. What you could do is investigate metrics that consider similarity and not relatedness, i.e. would rank the similarity of love, romance higher than love, hate. Jurafsky and Martin don't seem to give any references to papers about this, however. Hope this helps. 

The most accessible resource that explains the difference between each of these word similarity metrics would be Dan Jurafsky and James H. Martin's ubiquitous Speech and Language Processing 2nd Edition. Specifically, pages 652-667 in chapter 20 (Computational Lexical Semantics) briefly and comprehensively cover each metric/algorithm in a way that anyone with just a basic understanding of math, language, and graphs can understand. I will do my best to summarize each metric, using Jurafsky and Martin as my primary citation (attribute all my summaries to that book), annotated with my own understanding / insight where relevant / useful. Broadly we can group the metrics based on what parameters they operate on. Roughly there are two groups: (1) metrics which use only a thesaurus (e.g. WordNet) and (2) which use a thesaurus and probabilistic information from distributions in corpora. These metrics belong to the thesaurus-based ones: 

If you are going for a computational approach to predicates, I think you may wish to consider consulting VerbNet. It deals a lot with knowing just what arguments a (verb) predicate can take and also classifies verbs in many different semantically categories. I do not think there is a Russian form of VerbNet yet, but I think it is a relevant resource to your problem. You may also wish to read about Beth Levin's work on verb classes. This could prove helpful in creating your own ontology of Russian (verb) predicates, but I suspect that such a resource may already exist or at least someone has worked on it. I know there has been work for an Arabic VerbNet. 

There is a good rule for determining whether to use a [j] (like the first sound in yes) or [w] to link two words like this. The first thing you need to know is that the choice depends on the first vowel and not the second. Therefore in terms of the Original Poster's question, the choice is determined by /i/ at the end of the word be, and not by the beginning of the word ok. Now if the first vowel is either a high front vowel, for example /i/ or a diphthong ending in a high front vowel, in other words /eɪ, aɪ, ɔɪ/, then we need to use a /j/ to link the words. If you think about it, [j] is phonetically like a high front vowel. It is the same type of sound. So this kind of makes sense. On the other hand [w] is phonetically like a high back vowel. When the first word ends in a high back vowel such as /u/, or with a diphthong that ends in a high back vowel such as /oʊ/ or /aʊ/, then we use a [w] to link the two vowels. If the vowel is not a high vowel, or doesn't end in a high vowel, then you may find linking with a glottal stop, [ʔ], in Gen Am. In non-rhotic Englishes (those which don't have /r/ in the coda of the syllable) such as British RP, you will often find /r/ used to link a non-high vowel with a following vowel. 

Not only do many languages have wh-prepositions, English itself has at least two wh-prepositions. These are the words when and where, which, although classified as adverbs in traditional grammars, are recognised as prepositions by such grammars that allow for intransitive prepositions (for example, The Cambridge Grammar of the English Language, Huddleston & Pullum, 2002). The words when and where pass all of the tests for preposition-hood in English and none of the tests for being an adverb. In relation to the sentence: 

This is often referred to as dealveolar assimilation, because the consonant is moving away from its normal alveolar position, to effectively become a different consonant. The Original poster's question: There will be no elision of the /d/ in this environemt. However we are likely to get dealveolar assimilation. Because this consonant will now be homorganic (made with the same parts of the mouth) with the following /p/, it will not be released and may be less easy to hear. There will also be some devoicing of the /d/ because of the following voiceless /p/, and so it may appear more /p/-like. 

Another GUI interface tool for annotation you can use is MAE (Multi-document Annotation Environment) written by Amber Stubbs. It requires you to supply your own .DTD file (essentially .xml), which defines the possible annotation tags and links for the annotator. It also allows the creation of non-consuming tags, which are for links that do not have any corresponding text / image in the annotated document, but are necessary to create to satisfy the annotation guidelines / specification. MAE is designed specifically for linguistics undergraduates to perform annotation of documents. It is written in Java and the source is openly available. So if you need to tweak something, it can be done. google-code link to MAE: $URL$ 

Using this (and the basic understanding of each similarity metric), the word senses of love and hate, while antonyms, are very related, since they essentially belong to the same semantic type (one's feeling for something else). Thus, it would be expected that the metrics give a higher similarity to them, than say love and romance; romance is very similar to love, but its type is not as close as say hate or dislike. This hypothesis is quickly confirmed by testing the WUP metric: 

How do these work? Path similarity computes shortest number of edges from one word sense to another word sense, assuming a hierarchical structure like WordNet (essentially a graph). In general, word senses which have a longer path distance are less similar than those with a very short path distance, e.g. man, dog versus man, tree (expectation is that man is more similar to dog than it is to tree). The path similarity can be defined as: sim$_{\text{path}}(c_1, c_2) = \text{pathlen}(c_1, c_2)$ where $c_1$, $c_2$ are word senses, and $\text{pathlen}(c_1, c_2)$ is the shortest number of edges between those two word senses in a given thesaurus like WordNet. Leacock-Chodorow Similarity, or LCH, is practically the same thing, except it uses the negative logarithm of the result of path similarity. sim$_{\text{path}}(c_1, c_2) = -log \text{pathlen}(c_1, c_2)$ The negative logarithm is in the domain of information theory. The Wu-Palmer metric (WUP) is very similar to LCH, except it weights the edges based on distance in the hierarchy. For example, jumping from inanimate to animate is a larger distance than jumping from say Felid to Canid. In some sense we can think of it as sort of edit distance, assigning type changing operations a higher cost the higher they are in the hierarchy. These metrics belong to the thesaurus- and corpus-based ones (also called the Information Content metrics): 

Are there known reasons why these writing systems developed in this way? Does it perhaps have to do with the mediums in which the languages were written or read (for example, on scrolls, tablets, clay)? Do we know? Also, are there languages which go from bottom to top, for example, or in a different format altogether? 

The Original Poster's examples don't imply anything very different from each other. However, the general question of whether or why it matters where we put the negation in a sentence is quite interesting—especially in relation to certain verbs: It is a fact known to millions of hardworking English language students all over the world that native English speakers strongly prefer negating the verbs think, believe and want, amongst others, to negating the complement clauses that they license. So, for instance, all other things being equal, we prefer: 

The 'sentence' above will seem odd if not ungrammatical to most English speakers because the verb like usually requires an Object. Such an Object is missing from the phrase above. We cannot use like with its normal meaning without a Direct Object in a straightforward clause: 

In (5) there is a five word interval between the gap and the antecedent phrase. In (6) the interval is fourteen words. However, this kind of interval is of little interest to most linguists, and has little to do with the term long distance dependency. In fact it is sentence (5) and not sentence (6) which we would say best illustrated a long distance dependency in its most important sense. In all the other examples given so far, the antecedent phrase occurs directly to the left of the main body of the clause containing the gap. so if we showed this in (6) we would get: 

In short we need to be careful about confusing word categories and functions/grammatical relations. These two things are entirely different. 

Again, how you wished to measure the 'distance' of the dependency relationship would depend on what exactly you were trying to show. 

Here we see the word who occurring directly to the left of the nucleus (the main body) of the clause containing the gap. The long string that awful nit-picking man with the bobbly hat and the stripy shoes is just the subject of the verb punch. Because it is long it has moved the gap further away from its antecedent who. But nonetheless, the word who occurs here directly behind the very clause containing the gap. We can compare that to (5):