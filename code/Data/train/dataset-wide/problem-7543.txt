There is no space cutoff because it is $d(x,y)$ which depends on the two points. If you put a constant $c$ in your integral $\int_c^\infty d\tau$ then yes you would have introduced a spurrious cutoff in position space. If you are in $\mathbb{R}^d$ you can write the free massless propagator as $$ (-\Delta)^{-1}(x,y)=\int_0^{\infty}\frac{dl}{l} l^{-(d-2)} u\left(\frac{x-y}{l}\right) $$ for some nonnegative, positive semidefinite smooth function $u$ with compact support say in the unit Euclidean ball around the origin. See for example my article "A complete renormalization group trajectory between two fixed points". Comm. Math. Phys. 276 (2007), no. 3, 727-772. This can be very useful for a rigorous renormalization group analysis (see this MO answer). Clearly, there is no cutoff above because the integral over length scales $l$ is from $0$ to $\infty$. However, because of the support property of the infinitesimal multiscale slice $u((x-y)/l)$, the previous expression is equal to $$ \int_{|x-y|}^{\infty}\frac{dl}{l} l^{-(d-2)} u\left(\frac{x-y}{l}\right)\ . $$ There is a theory for this type of compactly supported multiscale decompositions developed by Brydges, Guadagni, Mitter, Bauerschmidt, Talarczyk, and others. See the JSP article by Mitter "On a finite range decomposition of the resolvent of a fractional power of the Laplacian" (and its erratum) for a recent account. 

Edit with a few more details: What I said above shows that the two examples are the same. For the first one this follows from the base change formula in Section 5.12 of the article by Jouanolou, which is a vast nonlinear generalization of the transformation formula I wrote above. One has: $$ {\rm Res}_{dp_1,\ldots,dp_n}(P_1\circ G,\ldots,P_n\circ G)= $$ $$ {\rm Res}_{p_1,\ldots,p_n}(P_1,\ldots,P_n)^{d^{n-1}}\ \times \ {\rm Res}_{d,\ldots,d}(G_1,\ldots,G_n)^{p_1\cdots p_n} $$ where each $P_i$ is a homogenous form of degree $p_i$ in $n$ variables and $G$ is a polynomial map $\mathbb{C}^n\rightarrow\mathbb{C}^n$ with components $G_i$ that are homogeneous polynomials of the same degree $d$. The first example is given by $$ \left\{ \begin{array}{ccc} G_1(x_1,x_2) & = & x_1^2\\ G_2(x_1,x_2) & = & x_2^2 \end{array}\right. $$ Spaces of forms of the form (sounds horrible I know) $P\circ G$ should give you tons of examples of this factorization phenomenon. Another thought: this is vaguely reminiscent of the Frobenius determinant factorization. The two matrices $g$ above are involutions and correspond to the discrete group $\mathbb{Z}_2$. I am not familiar with the Galois theory of covers like the one given by $G$ but I think it may help. 

Not yet an answer but you can see $f$ as an element of the symmetric power $S^d(V^{\vee})$ for $V$ a vector space of dimension $n+1$. You can take $V=S^n(W)$ with $W$ of dimension 2. That should give something like your representation for $g$. The indices labeling the $x$ basis of $V^{\vee}$ become labels for monomials of degree $n$ in your $y$ variables. More explicitly, if $$ f(x_0,\ldots,x_n)=\sum_{i_1,\ldots i_d=0}^n f_{i_1,\ldots,i_d}x_{1_1}\ldots x_{i_d} $$ with the (physicsy) tensor $f_{i_1,\ldots,i_d}$ being symmetric, then the associated form is $$ g(y^1,\ldots,y^d)=\sum_{i_1,\ldots i_d=0}^n f_{i_1,\ldots,i_d} (y_0^1)^{n-i_1}(y_1^1)^{i_1}\ldots (y_0^d)^{n-i_d}(y_1^d)^{i_d}\ . $$ It is of course essential to now see $f$ as a symmetric multilinear form rather than a homogeneous polynomial, i.e., to work with $n$ distinct pairs of binary variables $y$. This is the idea behind the symbolic method of classical invariant theory (see my answer to this MO question for more details). 

See the book "Invariant Theory of Finite Groups" by Neusel and Smith, as well as this other book by Derksen and Kemper. 

Edit: Apparently there is such a formula due to Lascoux and Schutzenberger, see Theorem 9.6.1 page 148 in the book "Symmetric functions and combinatorial operators on polynomials" by Alain Lascoux. Another source on the web is here. It also has the required property here which is that the number of finite differences taken is the same as the degree of the multiplying Schubert polynomial. 

I have the book right in front of me and from a quick glance at the relevant section it seems that GJ chose a somewhat non-standard presentation. I think the most commonly used approach is the one explained by yuggib or suggested by user1504 which involves smearing and understanding everything in some suitable "sense of distributions". However, as you rightly noticed, GJ are really talking about statements which are pointwise in the momentum variables. Your (formal) formula (I know that sounds bad) for $a^*(k)$ which contains a delta function immediately shows that there is a problem with the interpretation as an unbounded operator on $\mathscr{F}$. GJ are well aware of that and mention on page 96 that the domain of this "operator" would have to be $\{0\}$. Then, they (adroitly!) pirouette this by saying that they do not define $a^*(k)$, for fixed $k$, as an operator but rather as a bilinear form $\mathscr{D}\times\mathscr{D}\rightarrow\mathbb{C}$. For $\theta_1,\theta_2\in \mathscr{D}$, the definition of this bilinear form $$ \langle \theta_1, a^*(k)\theta_2\rangle $$ is: compute Ã  la physicist using the "formal formula" and then declare the outcome (which has no delta function) to be the definition. Similarly, $a^*(k)a(k')$ is not the composition of two linear maps (with suitable domains) but a bilinear form defined by the same recipe. I should add that what is really happening behind the scene here is multilinear algebra (the way for instance representation theorists use it, often with the help of diagrammatic algebra for duality pairings or tensor contractions) in infinite dimension. This is what Laurent Schwartz calls Volterra composition in the ultimate reference on the subject: volume 3 and volume 4 of his book on distributions. I like to call this body of knowledge the Schwartz-Grothendieck Theory because of important input of AG needed to make it work, i.e., the concept of nuclear space. 

I don't know what you mean exactly by "compute the invariants": perhaps finding an explicit linear basis? Essentially $\wedge^q\mathfrak{p}$ is the same as the antisymmetric plethysm $\wedge^q(\wedge^2(\mathbb{R}^n))$. If I remember correctly, this kind of objects appears in relation to the Hodge conjecture for certain Abelian varieties as in work by Ken Ribet and Fumio Hazama. Namely, in some particular situations, the invariant elements in such a plethysm are the Hodge classes in the cohomology of the variety. See in particular the article "Branching rules and Hodge cycles on certain Abelian varieties" in AJM 1988. There could be some useful tools for your question in that literature. As for a linearly generating set of invariants, it is easy to construct as follows. Let me call a Wick contraction any set partition $P$ of $\{1,2,\ldots, 2q\}$ with blocks of size two. Let me denote by $P_{\ast}$ the special case $$ \{\{1,2\},\{3,4\},\ldots,\{2q-1,2q\}\}\ . $$ For indices $i,j$ in $\{1,\ldots,n\}$, let me use the Kronecker delta notation $\delta_{i,j}$ for the indicator function of the condition $i=j$. For a Wick contraction $P$ and a collection of $2q$ indices $i_1,\ldots,i_{2q}$ in $\{1,\ldots,n\}$, let me write $$ A(P)_{i_1,i_2,\ldots,i_{2q}}=\prod_{\{\alpha,\beta\}\in P} \delta_{i_{\alpha},i_{\beta}}\ . $$ One can view $\wedge^q(\wedge^2(\mathbb{R}^n))$ as the real vector space of all arrays $T=(T_{i_1,\ldots,i_{2q}})_{i_1,\ldots,i_{2q}\in\{1,\ldots,n\}}$ which change sign if one exchanges indices within a block of $P_{\ast}$ or if one rigidly exchanges two blocks. Now define the linear forms $L_P$ indexed by Wick contractions given by $$ L_P(T)=\sum_{i_1,\ldots,i_{2q}\in\{1,\ldots,n\}} A(P)_{i_1,i_2,\ldots,i_{2q}} \ T_{i_1,i_2,\ldots,i_{2q}}\ . $$ By the first fundamental theorem of 19th century invariant theory for $K=O(n)$, these form a linearly generating collection for ${\rm Hom}_K(\wedge^q\mathfrak{p},\mathbb{C})$. Of course one can restrict to $P$'s which are "transverse" to $P_{\ast}$, i.e. , such that $P\cap P_{\ast}=\varnothing$. I don't immediately see how to prune the remaining collection to get a basis, since I expect tons of linear relations. The stable case $n\ge 2q$ might be easier to analyze first. 

I always found it strange that, in the context of invariant and representation theory, averaging over the group is called the "Reynolds operator". As far as I know the work of Reynolds was in fluid mechanics. He introduced the idea of local averaging in order to distinguish slow and fast variables. As such it is a precursor of things such as the Lyapunov-Schmidt method and Wilson's renormalization group. How did this terminology end up in invariant theory? 

The system can be put in the form $Y=LX-G(X)$. Here $X$ is the column vector with the unknowns $X_1,\ldots,X_s$. $Y$ is the column vector with components $Y_i=-a_{0,\ldots,0}^{(i)}$. $L$ is the matrix describing the linear part of your system, namely, $$ L_{ij}=a_{0,\ldots,0,1,0,\ldots,0}^{(i)} $$ with the $1$ at the $j$-th position. Finally $$ G_i(X)=-\sum_{j_1,\ldots,j_s} a_{j_1,\ldots,j_s}^{(i)} X_1^{j_1}\cdots X_s^{j_s} $$ where you only keep terms of degree at least 2. Then your criterion and the Gershgorin circle theorem guarantee that the matrix $L$ is invertible so you can try to solve iteratively $$ X=L^{-1}Y+L^{-1}G(X) $$ $$ X=L^{-1}Y+L^{-1}G(L^{-1}Y+L^{-1}G(X)) $$ etc. In the fact one can write the end result as an explicit series in terms of trees. See for instance Theorem 1 in my article "The Jacobian Conjecture as a Problem of Perturbative Quantum Field Theory". It is possible to write explicit bounds which imply the convergence of this series and therefore the existence of a real solution. Such bounds essentially mean that $Y$ and $G$ are sufficiently small. The question is whether your criterion implies such bounds. In the above paper the particular case treated is that where $G$ is homogeneous. To see how the formulas look like in the nonhomogenous case you can see my other article "Feynman Diagrams in Algebraic Combinatorics". 

Basically there is a rather complicated system of algebraic equations of degree $n+1$ called the Brill-Gordan equations for the coefficients of your polynomial which tell you if it is a product of linear factors. You would need more work to eliminate the coefficients of the non-multilinear terms in order to get a system of equation for the multilinear terms only. I don't know what the effect of imposing orthogonality would be. 

By looking at Coolidge's "Algebraic Plane Curves" Ch. VII, one may guess that $p$ stands for PlÃ¼cker. You should have a look at the reference cited by Coolidge in his footnote to the first page of Ch. VII with title "PlÃ¼cker's equations and Klein's equation" where the notion of genus is presented. The footnote says "For an historical account, see Berzolari, p. 343". The citation is to: Berzolari, `Allgemeine Theorie des hÃ¶heren ebenen algebraischen Kurven', in EnzyklopÃ¤die der Math. Wissenschaften, vol. iii, Part $2^1$, Leipzig, 1906, 99. 

A standard way of going from a symplectic manifold to quantum mechanics is Fedosov quantization. Now if you want to do this in infinite dimension, that means you are interested in quantum field theory. A good place to start is the thesis by Giovanni Collini (a former student of Stefan Hollands) which develops such a Fedosov quantization in the QFT context. 

I would look up a book on the calculus of finite differences in a multivariate setting. The claim here is to show that for any multi-index $\alpha=(\alpha_1,\ldots,\alpha_n)$ of length $k$ one can express the multiple derivative at zero $$ \left(\frac{\partial}{\partial t}\right)^\alpha \ (t_1x_1+\cdots+ t_n x_n)^k $$ as a linear combination of finite difference analogous expressions which should only involve the evaluation of $(t_1x_1+\cdots+ t_n x_n)^k$ at integer points $(t_1,\ldots,t_n)$ with nonnegative coordinates adding up to $k$. This is the same as the above candidate basis considered by you and Peter. I don't know if there exists a multivariate analogue of the Newton series. If so then this would immediately imply the wanted statement. 

When listening to the beautiful lectures by Gilles Schaeffer at the SLC68, the following (perhaps crazy) question occurred to me: did anyone attempt (succeed?) to combinatorially prove modularity of elliptic curves using dessins d'enfant? Of course I am not talking about a combinatorial proof of the general result due to Wiles, Taylor, Breuil, Conrad and Diamond. If such a thing existed, everyone and their dog would have heard about it. I am interested in learning about combinatorial proofs, if any, even for very modest examples. As I do not know anything about the subject, references to the relevant literature would be appreciated. This question can be broken down into the following three: 1) Can one tell `by looking at a dessin' if the corresponding curve is defined over $\mathbb{Q}$? If this is too hard, can one construct an explicit collection of dessins which catches all elliptic curves defined over $\mathbb{Q}$? 2) Does one know explicit dessins for all modular curves? 3) Let $X\rightarrow\mathbb{P}^1$ and $Y\rightarrow\mathbb{P}^1$ be two coverings given by dessins. Is there some sufficient criteria for the existence of a cover $X\rightarrow Y$?