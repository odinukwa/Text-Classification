How do I find out what these are actually doing? Can I see a list of files being accessed by each PID, or any more info? We're on . I tried but it's not giving me much useful info about what's actually going on. Edit - Additional stuff tried based on comments/answers: Doing on each of the PIDs shows the following: 

I previously had cURL 7.22.0 on Ubuntu 12.04 Server.. but I now need to upgrade to cURL 7.30.0. I've done the following to compile this version for Ubuntu: 

I've called Dell twice to try and clarify this basic (or so I thought) question, but I've had two different answers so far. We have a Dell R710, and a Dell R720. Does DRAC come installed by default on either? Dell said it comes build into the motherboard on the R720 but the second time I rang, they said we didn't have it. Can anyone tell me if DRAC comes as standard on the R720? I'm aware we'll need a license as well, I'm just interested in the physical capability on the server. I've never used DRAC before and know next to nothing about it, so apologies if this seems blindingly simple. Thanks 

All the commands etc work as expected but we need to be able to access the OpenManage web interface, however, it isn't starting up for some reason. 

When it failed after 5 minutes or so with the error in my post, the memory immediately freed up again: 

We have a MySQL database server running on an Dell PowerEdge R720 (PERC H710 Mini RAID controller) ( (Ubuntu 12.04). We're considering upgrading the 2 x 146GB 15k SAS drives to Samsung 840 Pro SSDs. The Dell ones are just far too expensive! (nearly 2k per drive), and with us putting them in RAID 10, we can just have spare drives standing by for if any fail? Does anyone have an identical setup, and are we likely to get problems with this configuration? I've read reports online of the Dell RAID controller deciding it doesn't like the drives and marking them as offline randomly, but then other reports of people running 100+ of these in RAID and having no problems at all. Should we just avoid this entirely for a pretty critical database server? 

Below is a sample configuration for the HAProxy frontend and backends which I just wrote up. This is not the full configuration file, so please use the /etc/haproxy/haproxy.cfg as a guide to having a complete configuration. 

Providing that you have loaded your private key on your client, then it sounds like this might be a permissions issue on the 'git' user home directory and .ssh directory. Please try changing your /home/git directory to a mask of 0711: 

It's probably not running because it's unable to find the binary. Without any error log though, that's just an assumption I'm making. Try adjusting the part in your script to use its full path. I'm not sure how you installed (either via yum or manually), but you can find out its full path by running . 

I just tested this and it looks like the GitLab API response is using pagination. According to the documentation ($URL$ the default number of results per page is set to 20 and the starting page is 1. To adjust the maximum results per page, you need to use the variable in the HTTP request line. You can change the page number by using as well, if you have more repositories than the maximum value of . You can specify a maximum value of 100. For example, you request may look like: 

If it's not set before it reaches this location block, then it will set it to a blank string. You can just as easily add a string between the quotes. I do not get any errors when doing a configuration test using this. Please let me know if you're seeing otherwise. 

I'm trying to import a large 70GB+ database (all ) using . This is a development system on Windows using WAMP server. . I'm getting the following error: 

Turns out this was down to the domain being different on the NAS and the server! on the ReadyNAS came back blank, and on the Ubuntu server came back . I changed it with: and updated: to Rebooted with: and it all started working as expected (think I possibly remounted the share as well, but not 100%)! 

After doing all that I ran expecting to see the new version installed. cURL had updated to 7.30.0 as expected, but libcurl hadn't: 

I usually use MySQL Workbench to do the import but I wanted to use the parameter and I don't think does this. What's wrong with this syntax? I also tried: 

A few days ago we were getting errors when trying to access files on the large RAID 5 partition. We rebooted the server and got a message about . We've had this before, and just needed to use Dell's RAID configuration utility to on the RAID. Last time this worked, but this time, it started doing a disk check then we got this: 

We noticed one of the drive lights was not lit at all, and thought this may have failed and be the problem. We replaced the drive with a spare, and tried "F" to repair it again, but we keep just getting the same error as above. In the RAID configuration utility, all drives show as "online" and "optimal". We do have this data on another replicated server, so we're not worried about "recovering" anything, we just want to get the system back online asap. The server has 64 or 32GB memory, can't remember off the top of my head, but either way, with a 14TB RAID, I think it may still not be enough. Thanks EDIT - I checked the memory usage while fsck was running as suggested and after 2 or 3 minutes, it looked like this, using up nearly all of our servers memory: 

You can't map a port on a public IP address to more than one private IP address, it just won't work because how is the router supposed to know which one to go to? If you're using name-based virtual hosts, you could achieve this by sitting a HAProxy instance in-front of the web instances and direct all traffic from the router to the HAProxy instance. On the HAProxy instance, you create a front-end and specify the domains and the backend to use. Then, depending on which domain is accessed via HTTP, it forwards the request to the appropriate back-end to serve the request. I do it all the time when I want to conserve server or IP resources. 

This is not possible in the current OpsWorks system. Even if you could, it would require the server to have the OpsWorks agent and Chef to be installed, which is handled already in the OpsWorks AMIs provided by Amazon. A bit late, but I thought I'd give it an answer. 

By default, when you create an account in WHM, it will configure local mail accounts and it will set the local DNS to use the local MX records. However, if you do not wish to rely on the local DNS for the MX records, then you can force WHM to always use a remote mail exchanger. In WHM, goto: DNS Functions -> Edit DNS Zone -> (select domain name) At the bottom, you will see something called Email Routing. Set this to Remote Mail Exchanger and then ensure the DNS server is restarted once saved. 

Based on your location blocks, files ending with '.php' will only be passed through PHP. So if you have an actual file in the '/images' directory, it should serve it as a static file, or otherwise return a 404 error. If the file ends with '.php' and is in the '/images' directory, it will run through both location blocks in order. Should it do otherwise? 

It is my understanding that network address translation (NATing) goes away with IPv6. How do we isolate network resources to those that need them from the rest of the internet? I am specifically thinking about allowing access to internal network resources like file servers or VM hosts to remote users, such as those working from home. A similar scenario also comes up in IPv4 today. At many universities, including my own, each network device gets a publicly routable IP. I'd like to run a file server, but don't really want it publicly accessible. Ideally it too would have a public IP and VPN would not be necessary. Comments? 

How do cluster file systems avoid the myriad of possible race conditions? I'm trying to get a grip on using a cluster file system in a Master-Master architecture. I'm thinking specifically about GlusterFS, so implementation details for it are welcome, but I'm hoping for a general answer. 

I have servers that I wish to control access to. I have a firewall running pfSense between them and the public internet, but all machines have public IPs. A remote client may or may not be on the same subnet as the servers and firewall. I wish to, based on authentication, allow the remote client full access to the servers behind the firewall. I believe the best way to do this is through VPN. Note that normally when people refer to VPN connecting the same subnet, they refer to two machines having the same private IP ranges. That is very different than what I am describing. I simply want to tunnel traffic for a subnet through VPN to bypass the firewall. What is the best way to go about this? If you suggest OpenVPN, tun or tap? Thank you! 

The and variables are not required as they have default values, so you do not need to include either if you don't want to. Hopefully this solves your problem. 

The Amazon free tier is only valid for 12 months since creation of your account and it has quite strict guidelines in terms of what you can do - if you've stuck by these and still being charged, it's best to contact Amazon billing and ask them why. It really depends what instances you're running, what AMI you've used to launch the instances, how much disk space you're using, how much I/O activity you have going on the servers, how much bandwidth you're using and if you're using other Amazon services. Without being able to see the activity statement, it's hard to analyse it. Usually if you give the Amazon billing department an e-mail, they will be able to help you out quite well. If you have anything more specific about the usage though, I'm happy to help you answer it. 

If you change your custom cookbooks, you need to update the cookbooks on each instance. You can do this under the "Deployments" area by clicking "Run Command", selecting "Update Custom Cookbooks" from the drop-down and then pressing the "Update Custom Cookbooks" button. You can usually leave all instances checked, unless you don't want to update one for some reason. 

Rather than having a location block for each redirect, you can always just add rewrite rules into an existing location block: 

I do not believe this is possible as the name servers stored in the domain name do not have any given priority. The client will use the name server that it is given.