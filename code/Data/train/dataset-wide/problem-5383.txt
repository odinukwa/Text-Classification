A classic example is someone saying, "Gravity is only a theory." In this case they are committing a logical fallacy by conflating the everyday meaning of theory with the way the word is used in formal science. I'm just looking for the official name of this fallacy. 

Maybe you'd be interesting in fictionalism. There's value in reading Moby Dick, even though the characters don't exist and the story's not true. Likewise, there's value in mathematics that is not "true" about the world. And who's to say which part of math that is? Maybe it's all of it. When one first hears the fictionalist hypothesis, it can seem a bit crazy. Are we really supposed to believe that sentences like ‘3 is prime’ and ‘2 + 2 = 4’ are false? But the appeal of fictionalism starts to emerge when we realize what the alternatives are. By thinking carefully about the issues surrounding the interpretation of mathematical discourse, it can start to seem that fictionalism is actually very plausible, and indeed, that it might just be the least crazy view out there. $URL$ 

The question is based on a flawed premise, namely that thinking is an immaterial activity. In fact, thinking is a product of electrochemical activity in the brain that requires energy. In fact brain activity consumes 20% of the calories burned by the body. $URL$ This SciAm article quotes the same figure: 

In a computer system, executing programs are called processes. A process is an execution of a given program. So for example Microsoft Word is a program, a set of bits on a hard drive. When you double-click it, the operating system creates a process executing MW. The processes share the same code; but they have different data. Each process has its own private data space. The processes are separate entities accomplishing work. It's the same with minds. Assuming you could in fact upload a brain to man-made hardware -- a proposition we'll accept for now, but which is highly debatable -- at the moment of creation they'd be two different people executing the same code and having identical data. A moment later they have each had different experiences and now have different data and life experiences from that moment on. But they are definitely two distinct people. Another computing analogy is the fork() system call in any Unix-like operating system (linux, MacOS, Solaris, etc.) A fork() operation allows a running process to create a brand new process with a copy of the parent process's data. From that moment on, the child process has its own separate existence. If you happen to run Chrome, each tab runs in a separate process. The parent process, the main browser process, spawns child processes -- windows and tabs -- as needed. [Note, Not all browsers use a process-per-tab model] It seems extremely clear to me that human mind-cloning would be the same. For example suppose you had a matter transporter like on Star Trek; but instead of transporting you, it copied you. So one you stays on the ship; and the other beams down to the planet to have adventures with whoever you find down there. At the moment of cloning, one process becomes two, having the exact same data. Same memories, same states of biological processes. After that moment, the life experiences and biological states diverge. Two separate people. To answer your specific question: The clone would be a brand new being, coming into existence with the exact same state as the original. This isn't actually all that different, when you think of it, from the phenomenon of identical twins. An egg is fertilized and starts growing into a human being. The egg splits; and now you have two separate human beings. That's really the essence of it. The real question is wether you can upload your mind to a computer. Is a mind something that exists independently of the medium in which it executes? Microsoft Word on a PC is pretty much the same program as Microsoft Word on a Mac. But is that true about humans? Is the you running on your wetware the same as you running on a supercomputer in a lab? Personally I do not think so, but that's beyond the scope of this question. 

GIT only applies to axiom systems that can express the natural numbers. Given any such one-axiom system, it would be incomplete. An example of a one-axiom system that's incomplete would be the conjunction of the Godel-Bernays axioms. $URL$ In other words, NGB can be finitely axiomatized so that you could then take the logical conjunction of each of the axioms to make a single statement that encapsulates all of the axioms. It would then be incomplete. 

JSTOR is set up so that you can't download the individual pages except as images, one page at a time; You can't download the pages as a single article. However what you can do, and what I did for my own private use, was to download each page, one at a time, as an image (the only way JSTOR lets you download pages); and then put the pages together, in order, as a pdf. For a lengthy paper it's a fair amount of clerical work. I'm probably one of the few people if not the only person without an academic JSTOR membership to own a clean pdf of this paper. At the moment I'm involved in a discussion in an online forum in which I need to refer someone to parts of this paper. I can't possibly make my point by referring them to JSTOR. Nobody is going to sign up for JSTOR and go through their shelf process to see what I'm talking about. Now on the one hand, the paper is freely available to anyone who chooses to jump through the JSTOR hoops. On other other hand, there is no version of the paper accessible online without going through JSTOR; and no full pdf version available online at all. And on the third hand, the author of the paper is deceased and not likely to care at this point. Is it ethically permissible for me to upload my private pdf so that I can make it easy for a few readers to see it without jumping through the JSTOR hoops? (edit) What would the great philosophers say about this entirely hypothetical situation? 

If by bit we mean a mathematical 1 or 0, then there is no smaller unit of information. But if we are talking about implementations of bits, then a bit has smaller constituent parts. An electrical engineer can calculate the number of atoms that make up a bit in a particular semiconductor material. Read world bits are made of atoms. Same as if you used a sequence of coins showing heads or tails to represent a bitstring. Each coin is made of atoms, quarks, strings, whatever level of discourse you prefer. Another interesting fact about the implementation of bits in a digital computer is that they are not absolute. If you have a particular electronic element representing one bit, that element has at any moment some particular voltage that rapidly transitions between high and low states to represent a 1 or a 0. In theory, the transition is represented by a perfect square wave. But in practice, there are no perfect square waves. So the designers of the circuit never examine the bit during its transition state. They use the system clock to measure the bit only in the middle of the square-ish wave so as to avoid the transition period in which the state of the bit is indeterminate. In other words we know the state of the bit only because we agree to measure its voltage during the stable part of its cycle, and never during the unstable transition. During the transition interval, the state of the bit can not be determined. It's not zero and it's not one. In short, the answer to your question is that in theory, a bit is the smallest unit of information. But in practice, it takes a lot of electrical engineering to pretend that there is any such thing as a bit with an exact value. Bits are made of atoms; and their value is deterministic only by choosing to measure them during intervals of electrical stability. 

That quote says it all. The art is to find the right definitions. You mentioned topology. As you know, it took decades of struggle to find the right characterization of continuity. The realization that the notion of the open set was the essence of continuity was a huge breakthrough. It's only obvious in retrospect. You mentioned Riemann integration. That's another good example, because in higher math the Riemann integral is no longer used. There's a more general theory called Lebesgue integration which behaves better. That doesn't mean that we were wrong to "trust" Riemann. Trust really has nothing to do with it. We live in the world as it is. The struggle to find good definitions is at the core of the development of mathematics. So when you ask, "... how to make sure a definition captures the intutive reasoning correctly, taking all special cases into account? the answer is ... if we knew, we'd bottle it and give it to the undergrads. There's no magic formula for progress. But you are also asking if we can "trust" experts, knowing that in a few decades they'll be proved wrong or foolish. If you had an infected leg in the 1800's they sawed it off. Without anesthetic. Did you trust your doctor back then? What other choice did you have? It's the same in every field. Math is no different. Mathematics is a historically contingent human activity. It's never perfect but it's always getting better. Painstaking struggle, false starts, the occasional genius, lots of plain old hard work. That's how progress is made in every field. Whether you trust, and what you trust, is up to you. You drive over bridges. Sometimes the bridges fall down. Over the years we learn to make better bridges, never perfect bridges. You'd be foolish to trust all the experts all the time. But you'd be even more foolish to never leave the house for fear of a falling bridge. Someone the other day asked the difference between rationality and logic. Rationality is what lets you drive over a bridge that you know might fall down, even though you can never personally investigate every nut, bolt, and corrupt government contract. I just happened to run across a guy named Ignaz Semmelweis. He was a German doctor in the 1840's who said that obstetric deaths could be reduced if doctors would just wash their hands before delivering babies. He was rejected by the medical community, committed to an insane asylum, and beaten to death by the guards. That's human progress.