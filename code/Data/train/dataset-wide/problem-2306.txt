Another way to ensure very fast selects is to store periods as sequences of days - then we can select number of open intervals directly from an indexed view dbo.ConcurrentPeriodsByDay, which is as fast as it goes. 

In such cases I am storing version numbers, and using referential integrity to make sure version numbers begin with 1 and have no gaps. Because I use constraints, I am sure my solution works in high concurrency situations. I have described this approach in this article 

You can create a covering index on the following columns (message_type_id, hidden, message_id). Try out different order of columns in the index: that might make some difference. I would rather not use a filtered index: it is quite likely that the optimizer would fail to use it. 

We can use trusted constraints to make sure each PrescriptionId covers a range of dates without gaps and overlaps, so that one interval stores as one unbroken sequence of dates. Note: I know you are using DISTINCT in your first subquery, so you are assuming that one person can take one drug on one day from more than one prescription. I am not mkaing this assumption, for simplicity. Are you sure it is a correct assumption? If yes, we will have to change the design. Once we have this table, we can essentially materialize your first subquery as an indexed view: 

As usual with such problems, it is very easy to accomplish in Java or C++ or C#. If you really need to do it in the database, you can use an RDBMS with fast cursors, such as Oracle, write a simple cursor, and enjoy fast performance without having to write anything complex. If you need to do it in T-SQL, and you cannot change database design, Itzik Ben-Gan has written up several solutions in "MVP Deep Dives vol 1", and some new solutions using OLAP functions in his new book about window functions in SQL 2012. Alternatively, you can add another column consecutiveMarker to your table, and store precalculated values in it. We can use constraints to ensure that pre-calculated data is always valid. If anyone is interested, I can explain how. 

It has five constraints which work together to implement the business rule. Let me demonstrate how the more complex ones work. Of course, some constraints are simple and as such do not need any explanations. ** 

If all you need is to store which step of your process comes after which previous step, then all you need is the following: ProcessID | ParentProcessID | PreviousProcessID Of course, you will need a FK constraint to make sure that (ParentProcessID | PreviousProcessID) points to a valid (ParentProcessID | ProcessID) If I understood your requirements and this design is valid, then it is easy to insert/remove/move around steps in your process - you do not have to propagate any changes to your child tables, because they refer to your primary key on (ParentProcessID | ProcessID). HIH 

MSDN says that the range of REAL numbers is - 3.40E + 38 to -1.18E - 38, 0 and 1.18E - 38 to 3.40E + 38. Apparently the true lower limit is much lower. The following script populates a REAL column with 1.401298E-45: 

If you find yourself saying "nullable columns" before you have written code I generally think you need to normalize more(as always, it depends.) If you want one system to maintain, the last option seems the most relevant, but "splitting them into two tables" is not really where you would want to take it. if we want to track basic things like Players, Games, and Sport type you would simply add intersection tables between the relevant things you want to store. 

This means we could count all of Michael Jordan's games by sport(as he played multiple) with a query something like: 

This table would only store information relevant to an exam, such as the title of the exam, the exam's attributes and categories. 

Edit:Adding a little flavor text as requested. Basically what is happening is you are choosing to aggregate one of the values (in this case the counts of the salary) so that it "rolls up", and the group by generally indicates which value you want to do the rolling up by. It makes some sense to say "I want to group by the number of employees" but you are actually trying to express "I want to return the number of employees grouped by department." 

Simple answer, you grouped by your sum. Solution is simply to remove that from your group by statement, eg: $URL$ 

You are grouping by the thing you are counting, not by the department name. Change your group by to: 

Yes, SQL Server can report how long it took to do any of those actions (though you may have to run it to get some additional details such as actual row counts returned) Statistics Time 

I dont think so except for the fact that there is a 5NF, which describes a design where your joins are only on the candidate keys. Many "4NF" designs meet this criteria, but not all, and it is definitely something you can change a 4NF "into" to be be more normalized. 

This table would store the ids for both the previous tables and any information that is specific to that exam's instance of that question. This would allow you to write queries such as "How many questions belong to one exam" 

It looks like based on your error that the user you are trying to create a login for already has a login in that database with another name, so you are effectively "double mapping" them. If you have access to SSMS you can see what their mapping is by using the Object Explorer and navigating the following path: Security -> Logins -> Double click on the username This should pop up a GUI, you can see the "User Mapping" in the top left hand corner, this should show you to whom the user is mapped. 

The easiest method is (probably) to uninstall and this time run the command line installer again with a different set of flags and it will install to a different location unless you are talking about components not listed below. From: $URL$ Proper Use of Setup Parameters Use the following guidelines to develop installation commands that have correct syntax: 

As a fairly newly minted DBA under the gun, I have run the gamut of free tools and done some experimentation in the paid space (DPA, SQL Sentry, and Foglight) and it really depends on what you want the tool for. In my experience the most important thing was not just communicating performance baselines (management vastly didn't care unless there was someone to yell at), but produce something in an easy to consume format that made the priorities clear and was able to track down performance issues in production. You can absolutely build up your skills by going the free route, and the tools for SQL Server are great. 

For performance reasons, in most cases we must store current balance - otherwise calculating it on the fly may eventually become prohibitively slow. We do store precalculated running totals in our system. To guarantee that numbers are always correct, we use constraints. The following solution has been copied from my blog. It describes an inventory, which is essentially the same problem: Calculating running totals is notoriously slow, whether you do it with a cursor or with a triangular join. It is very tempting to denormalize, to store running totals in a column, especially if you select it frequently. However, as usual when you denormalize, you need to guarantee the integrity of your denormalized data. Fortunately, you can guarantee the integrity of running totals with constraints â€“ as long as all your constraints are trusted, all your running totals are correct. Also this way you can easily ensure that the current balance (running totals) is never negative - enforcing by other methods can also be very slow. The following script demonstrates the technique. 

Once you have created this constraint, you will only be able to insert both rows in one statement. Unfortunately, this is does not work on MySql. 

** The constraint UNQ_IntegerSettings_SettingID_PreviousFinishedAt ensures exactly that. The first interval does not have a previous one, which means that PreviousFinishedAt IS NULL. The UNIQUE constraint guarantees that there can be only one such row per setting. See for yourself: 

I would use the Profiler to monitor connections and determine if a query completed - it can give you a complete information about what connects and what runs. However, could you solve your problem using a more mainstream approach, such as starting a new thread, opening a normal SqlConnection in it, and executing a SqlCommand? 

I am not sure if "the highest customer in the chain" for a leaf node means closest to the leaf or closest to the root. If you meant "closest to the leaf", you may want to denormalize and have a FK make sure your denormalized data is always correct: 

RequiredAmounts can select from a temporary table, table variable, a TVP. It can be anything. Just replace this sample CTE with whatever suits your needs. 

This business rule can be enforced in the model using only constraints. The following table should solve your problem. Use it instead of your view: 

I have not yet seen a single project that used extended properties. IMO the reason is this: even if we want to store documentation in the database, which is usually not the case, there are alternatives. Usually extended properties do not do exactly what we want. On the other hand, rolling out our own solution that does exactly what we need is so easy, so why bother? 

Every time I run integration tests, I build my database from scratch off scripts stored in git. In the past, whenever slowness would become a problem, we would run the server locally on developer's workstation, with the database on RAM Disk - that should be fast enough. We did that many times for our applications powered by SQL Server I have not yet done that for PostgreSql, but I am very new to it.