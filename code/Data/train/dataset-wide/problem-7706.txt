which would be $1-p_{\hat{z}_{10}}=0.9$. Assuming the above reasoning is correct (enough :), now to the question: I recently read "A Tutorial on Conformal Prediction" by Shafer and Vovk, and am curious on how to frame this problem on the Conformal Prediction framework. It appears to me the paper exclusively focus the case of estimating confidence for $z=\hat{z}$, so the question how to adapt it for the asymmetric case. If we were interested in the $z=\hat{z}$ case, a natural nonconformity measure would be $A(B,\hat{z})=|\bar{z}_B-\hat{z}|$, where $\bar{z}_B$ is the mean of $B$ (section 4.1 in the paper). That would define the following prediction regions (from the algorithm of section 4.2): $\Gamma^{0\leq\varepsilon<0.5}=\{0,1,2,3\}$ $\Gamma^{0.5\leq\varepsilon<1}=\{1,2\}$ $\Gamma^{1}=\{\}$ given that, $\alpha_0=A(\{1,2,3\},0)=\alpha_3=A(\{0,1,2\},3)=2$ $\alpha_1=A(\{0,2,3\},1)=\alpha_2=A(\{0,1,3\},2)=2/3$ $p_0=p_3=0.5$ $p_1=p_2=1$ It feels wrong to transport this reasoning for the asymmetric case, as it considers both directions (e.g. 0) as "more extreme". In particularly would mean that we have a 0.5 confidence in the $z\leq\bar{z}_{10}$ prediction, much lower than 0.9 which was given above from the empirical distribution. It appears to me that for solving this problem we have to choose a nonconformity measure $A(B,\bar{z})$ that is monotonic with regard to $\bar{z}$, for example, $A(B,\bar{z})=\bar{z}$. That would make "more extreme" asymmetric, resulting in the following prediction regions for the above example: $\Gamma^{0\leq\varepsilon<0.1}=\{0,1,2,3\}$ $\Gamma^{0.1\leq\varepsilon<0.3}=\{0,1,2\}$ $\Gamma^{0.3\leq\varepsilon<0.6}=\{0,1\}$ $\Gamma^{0.6\leq\varepsilon<1}=\{0\}$ $\Gamma^{1}=\{\}$ given that, $\alpha_i=A(B,i)=i$ $p_0=1$ $p_1=0.6$ $p_2=0.3$ $p_3=0.1$ This means that we can be at least 0.7 confident in the the $z\leq\bar{z}_{10}$ prediction. Notably it still does not match the 0.9 confidence obtained from the empirical distribution. I wonder if I am on the right track... Thanks! Marco 

where A is a square matrix of height and width 3*5*7 which contains the variables x, y, and z but none of a, b, and c, and v is a column vector that lists off the 3*5*7 monomials in a, b, and c. Unless the only solution to your equations is a = b = c = 0, that means that the matrix A is not invertible, and therefore 

Your problem is in the form $\max_x f(x)$ subject to $g(x) < 0$, where $x$ is the set of components $x_i^j$ of the matrix $X$. Use one Lagrange multiplier to write $f(x) + \lambda g(x)$. Both nonlinear terms are quadratic, so this is the most favorable case for the purpose of obtaining solutions. To take the derivative of the expression above with respect to the set of unknowns $x$, index notation as in tensor calculus could be helpful. Then you are in the standard form of constrained optimization. 

Dynamical systems. Roughly speaking, a dynamical system $\dot{x} = a(x)$ is stable if and only if the 1st order linear partial differential equation $\mathcal{L}_a v + \ell = 0$ has a positive solution $v$. Here $v$ is called a Lyapunov function for the system, $\mathcal{L}$ is the Lie derivative, and $\ell > 0$ has to be chosen so that the equation has a solution but is otherwise arbitrary. 

One of my favorite titles from control theory is a 1978 paper by John Doyle entitled "Guaranteed Margins for LQG Regulators." It is memorable because of the abstract "There are none." The paper shows that optimal controls may be fragile; the 3-word abstract says it all. 

In control theory there exists the duality controllability and observability. It is very well understood in the context of linear control theory, not so much for nonlinear systems. It is related to the linear space duality between vectors and functionals, but more work in understanding it from a more general perspective would be welcome. 

I suggest looking at the function to be optimized as a local Lyapunov function for the dynamical system defined by the search procedure. There must be some literature on this point of view, but my knowledge is limited. 

I'll start with a motivating example and only then proceed to the question. Consider a list of total packages of milk that were purchased on 9 consecutive days on a given store, $z_1,\ldots,z_9 = 1,0,0,2,0,1,0,1,3$ Assume I have an algorithm that predicts $\hat{z}_{10}=2$. I'm interested in assessing the confidence associated to this prediction, however, for stock management purposes, I want to measure the confidence that the predicted value is an upper bound on the true value $z\leq\hat{z}$, instead of $z=\hat{z}$. This is because what I'm ultimately after is to be confident in not running out of stock. I guess an estimate of this confidence, based only on given examples, is $1-p_{\hat{z}_{10}}$ where $p_{\hat{z}_{10}}$ is the probability of finding a value more extreme than the prediction $\hat{z}_{10}$ on the empirical distribution of all 10 observations: 

In other words, if you consider each monomial as a variable, then you can write these in matrix form as 

My question is about a method described in Dr.Math forum for simplifying equations involving sums of radical functions. (The following is a transcription of the example given by Dr. Vogler): --- begin example --- We want to convert the equation 

But the determinant is a polynomial function in the entries of your matrix, which means that the determinant of A is a polynomial in x, y, and z. --- end example --- I have tested this method with several examples and it seems to work, at least in the sense that the roots of the radical equation are roots of the resulting polynomial (it seems however that the resulting polynomial may have real roots that are not roots of the original radical equation). The only method I knew about in order to get rid of radical expressions in equations was to carefully manipulate the equation and then raise both sides of the equation to the same power. This is to say I have a very basic math background. So finally, my question is: what is the name of this technique and where can I read more about it, or at least what are the keywords I can use to search google. I have many other questions regarding this technique, but I believe I good book on the subject would answer them. Just for completion, some of the questions are: 

An good one is the old classic geometry book by Jacques Hadamard. The first volume covers plane geometry: Lessons in Geometry by Jacques Hadamard (published by AMS, 2008.) There is also a companion book with the solutions to problems (AMS, 2010): Hadamard's Plane Geometry by Mark Saul. I wonder if the second volume (solid geometry) is available in English? 

I heard good things about Combinatorics and Graph Theory by Harris, Hirst and Mossinghoff (Springer Undergraduate Texts in Mathematics). It's easier to read than Diestel or Bollobas, but not dumbed down. But of course, it's neither thorough nor exhaustive. 

I am not sure what exactly you are looking for. But, looking at the data, a linear one dimensional model does not fit so well. To see this, try CCA (Canonical Correlation Analysis): the linear version computes two linear transforms of the input spaces, such that the transformed data are maximally correlated. I suspect from your data that the maximum correlation you will get is no more that 0.7-0.8. If you are looking to "explain the data", you need a generative model, perhaps a Gaussian Mixture for good results. For the practical aspects of data analysis, you can try Handbook of Statistical Analysis and Data Mining Applications by Nisbet, Elder, Miner. If you look to understand the theory, The Elements of Statistical Learning by Hastie, Tibshirani and Friedman is the standard graduate text (though it might be too difficult, if you are not at ease with probability, measure theory and some functional analysis.) 

Zoom to arbitrary level and keep the zoom between page switches (persistent zoom) Landscape mode with automatic margins cut Multiple-column mode (great for two columns papers in A4 or Letter size) Two PDF viewers (XPDF and Adobe with DRM support) and an excellent DejaView viewer.