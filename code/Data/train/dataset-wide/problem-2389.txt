Save all of the raw ratings (i.e. individual user inputs) with the date on which the rating was recorded. Then you can calculate all of your weekly, monthly and yearly statistics based on the raw data each night. These rolled up statistics should also be dated (with a date range). In this way you can also keep multiple weeks, months and years so that you can look at trends if you like. This is often called a data warehouse. It is a common way of pre-calculating rolled-up statistics for quick and easy reporting. When you get past the longest of your roll-ups (e.g. 1 year) you can start to throw away your raw transactional data if you're worried about how much space it takes up. 

There are a few potential advantages of using an intermediary staging database, which may or may not apply to your situation. There is no perfect, one-size fits all solution. Some of the potential advantages include: 

It avoids the problem that you are concerned about with a potentially being assigned to departments and classifications in different companies. It gives you a locus for other predicates that might be applicable to a position, such as salary grade, etc. It allows you to record the fact that a position exists, even if there are no s currently in the position, which is quite possibly useful information. 

Your table looks to me like an invoice detail table to me. This means that if you know what invoice and which line on the invoice, you know which distinct record you're looking at. If that's the case, what do each of the non-key columns depend on? Let's look at each in turn: 

It is quite possible that their conceptual schema has a lot of overlap with their physical schema. Knowing table names gives you a decent head start in planning your SQL injection attacks. It is very likely they don't have their conceptual schema documented. Organizations that let programmers design their own databases often don't have any rigour in their database design process, going straight to physical implementation without any initial design. They may not want to admit this, or they may not want to go to the time and trouble of back-creating a conceptual document that never existed. 

If you have something based on these columns then you have to have all the combinations of from / to units of measure in both directions in order to look up a conversion factor. You could also, alternatively, only have from non-default units of measure to the standard unit of measure for each . This would mean less data in the table, but you'd have to have conditional logic to go from and / or to non-standard units of measure. In the worst case scenario, you might have to convert from units in A (non-standard) to B (standard) and again from B (standard) to C (non-standard). Which way you go depends on your relative tolerance for more data to maintain or more code complexity. 

Notice in this second alternative is recognized as an entity type. It is an intersection between and the generic line item type: . A SKU is a supertype entity that could be a part, a consumable or some kind of labour. You don't need to have a logical supertype for service line item types, but a lot of systems would be modeled this way because it makes the transactional detail much simpler. This second model introduces abstract entities over and above the concrete entities of the first model. Introduction of abstractions like this is one of the things that tends to happen as you move from an initial logical model, based mostly on tangible things to a physical model. As you gain experience with data modeling, you'll get good instincts for moving past the conceptual/logical model stage directly to a well structured physical model. 

Is there some particular reason you're worried about how much work your computer has to do to join three tables? The purpose of normalizing your schema is to make it much harder to corrupt your data when you insert, update and delete it. Normalizing your schema means that reading your data will be more work (generally), but your code for maintaining your data will be much simpler and therefore much less prone to data corruption errors, as well as being cheaper to build and maintain, of course. If your data is pretty static, i.e. you write it once and then read it a lot, then the benefits of normalization are less important. If your performance or resource utilization is very critical, and you can't get the performance you need from normalized data, then you can consider denormalizing your schema. However, whenever you use denormalization you should do it with full knowledge of all of the potential traps you've set for yourself with respect to data corruption. Normalize by default and denormalize as necessary, and with caution. 

This is a case of a requirement for fourth normal form (4NF), where it is important to isolate independent many-to-many relationships. Note that if you don't want to track an asset's owner over time, then you could simplify the illustrated model and make a foreign key on the table and eliminate the table from the model. 

There is a difference between redundant data (bad) and coincidentally repeated data (not bad). Normalization is a technique which is used to avoid insert, update and delete anomalies. It is not meant to eliminate every repetition of a piece of data. Data which is static doesn't benefit from normalization. A unit of measure, stated as a standardized abbreviation is not the kind of data that you need to normalize out. Think of it this way: If you were to normalize out your unit of measure into a separate table, you'd need a foreign key from your table to your table. Is the unit of measure code going to be unique (probably, yes). Therefore it's a candidate key for your table. If it's a candidate key in you could use it as a foreign key in . The end result is that you have your unit of measure code in your table anyway even if you've normalized out the units. Here's when you would want to create a table. If you have other predicates (columns) that are dependent on the unit code, but not on the nutrient. For example, a or a conversion factor to a base unit of the same type (grams for weight, etc.) This would be a transitive functional dependency in your table and doesn't belong there for that reason. 

Forego declarative referential integrity and just ensure that you have indexes that make your joins efficient. Impose referential integrity procedurally. Take ypercube's suggestion and find a more efficient way to represent . 

You were more or less on the right track with (b), but you want to make sure that latitude and longitude are handled independently. What you need are indexes that cover your query parameters. This is what you are looking for: 

There are a few things you want to factor into your design: 1. Measurements Need a Timestamp Make sure all of your measurements have an indication of: 

The relationship between and is not important to your system! At first this seems a little bit counterintuitive, but think about this: Does your boss care who actually owns a car? No! What they care about is who (PERSON) signed a contract to pay to have which CAR parked in his garage. Therefore the relationships that are actually important are between CONTRACT and CAR, and CONTRACT and OWNER. The relationship between CAR and OWNER is not directly significant to your system, unless you have a business rule somewhere that says something about how contracts stay with cars, not owners when people sell their cars. That is probably dicey from a legal perspective. 

You don't need composite keys to enforce your referential integrity in your case. The reason is that you have a pretty straight-forward three tier hierarchy: 

NoSQL databases are for unstructured data or unpredictably structured data that is often queried using full-text searching. Capturing time-series statistics for graphing sounds like a pretty well-structured data set to me. What do you have, three tables? Users, Servers and Measurements? It sounds to me like you have data which is perfectly suited to a relational database. It may be that you need to think about whether your relational database is going to be fully normalized or partially denormalized for reporting performance. That's something you can decide is necessary once you do some load testing with production volumes in order to see whether denormalization is helpful. 

Edit: Recognizing Teaching Qualifications Based on a comment by OP (below) the question is how can a teacher be recognized as being qualified to teach a topic without there being an actual lesson assignment made (yet)? Issue 1: The relationship of topics to subjects: It isn't clear from the original question whether topics belong to one subject or possibly to multiple subjects. If topics (e.g. fractions) belong to only one subject (e.g. math) then there should be a foreign key like so: . If this were so, then would be redundant and could also be dropped. This would simplify the model and make things much clearer while reducing the risk of inconsistent foreign keys. Issue 2: What teachers can do vs. what they are doing: The table shows which lessons teachers are actually teaching. Before this assignment is made, OP would like to be able to record which topics a teacher is qualified to teach. This could be done with a second M:N intersection, like so: 

You can indicate what language a message body is in, assuming that you know what language it is because the user told you so or because you have a process that reads the message and guesses the language. Therefore your message table could look something like this: 

Depending on how many parking lots you're operating, I wouldn't be too worried about how many records you have to manage. It's not like this data would be fast changing, since you'd probably have to get signs printed up with the prices marked and so on. Still, assuming that you have many lots (many of which have the same pricing) and that you want to minimize data maintenance, you could build a price schedule and map lots to the schedule. Consider something like this: 

You don't say so explicitly, but I think maybe your concern is that you want to have a timesheet that has one column per day and you don't know what to do about inconsistent weekends. If that's the case, then you're approaching it incorrectly. Don't have columns for days. Have rows in a details table for days. Your timesheet should use individual entries at a date level, assuming you only want one entry per day. If you want multiple entries per day (per worker) then have one entry per date and start time. For weekends you keep a separate table for working days (of the week) per worker. Consider something like this: 

Note that the and tables (in blue) are a bit contrived. You could these tables, but really in practice I think a lot of people would probably use something like a bit flag array in the table containing a flag for each day of the week instead. Note however that some of your SQL queries would actually be simpler if you did have an explicit table and an intersection table () that contained a row for each day of the week. This way SQL functions that convert a date into a day of week number could be used as part of joins against this table. 

Most DBMS already do store metadata in the database. This data is general stored in what is broadly referred to as "system tables". The metadata that a DBMS will store already will be what it needs to operate the database. Some of the types of metadata you've mentioned, e.g. ownership, permissions, and possibly even formats and descriptive names are already stored in your DBMS's system tables. Other types of metadata (e.g. date/time of data entry) are record oriented, as opposed to the table or column oriented metadata that you can find in your DBMS's system tables. Record oriented metadata needs to be stored using a mechanism that you design and implement yourself. Typically, this means storing the record oriented metadata with your records directly. In some cases, you might find it helpful to normalize out some kinds of record oriented metadata into a common repository. An event log that tracks events for multiple tables is an example of such a common repository. How you handle your metadata depends on what you need it for. As a matter of best practice, let the DBMS do what it is designed to do. Don't track metadata manually if the DBMS is doing it automatically. Only manually track the extra metadata. 

There are two principals that should govern your thinking about you data modeling options for this application: 1. Never Throw Away Important Information Discarding data that you might need is a terrible idea. It's up to you to decide whether a failed registration is something you might need. I would think it probably is. What would you do if someone showed up for a class saying "hey I registered!"? Wouldn't you want to be able to say "oh, I see your cheque bounced so your registration failed."? 2. Model Close to Reality If you try to abstract your entity types too much, you will find your model gets brittle and hard to adapt to changing business rules. Your system is tracking a couple of kinds of events. In particular, the act of someone registering for a class and the act of someone paying for a registration. You should have a table for each of these events, along with tables for the people and classes. Keeping separate tables for registration (which you already have) and for payment (which you don't really have in any of your options) allows you to handle situations like payment by multiple methods, e.g. some cash, some credit card, some promotional coupon, etc. It also lets you deal with the situation of failed payments, credit notes processed, and other real-world exceptions that are probably important to know about. Instead of keeping columns for registration status, which is ultimately a calculated value, you should be keeping columns (and records) for the various components of the registration status calculation. If your system turns out to have peculiar performance demands during real runtime testing, then consider using denormalization of the calculated registration status value, but beware of all of the potential issues that will raise for data integrity and plan to deal with those issues. Bonus: If you keep a table and a table, as I suggest, and treat registration status as a calculated value, then you can add a column to that allows you to bend the payment time limit rule. You can do this different ways. For example you could have a flag or code that means "consider this registration valid, even without payment". Alternatively, you could have a numeric or date column that gives a payment extension allowance in number of days or by a different, arbitrary deadline. 

You should be able to make in the above table your primary key if you want. Since the value is already unique (by virtue of being IDENTITY) I would question why you want to make a compound primary key. Are you sure that this is the design you want? 

If you can't change the schema of your table to add an column, then start by creating a temporary table that includes the auto increment and fill it with the range of plate scans that you are interested in. Then run the above query against the temporary table. 

What you are proposing is an entity subtyping approach. This would be one common solution to your design problem. Another would be Entity-Attribute-Value (EAV). Type "subtype" or "EAV" into the search box in the top right hand corner of the page and you'll see many questions describing and discussing the relative pitfalls and merits of each. Whether subtyping (or, alternatively EAV) is a "really bad design" in your case depends on exactly how many different unique features and manufacturers you need to account for and perhaps where you stand philosophically (some people insist that EAV is always evil-for example). To answer your specific question about enforcing a match between brands and their unique attributes, the only way to do this is with procedural code. Depending on your DBMS you might be able to do this with triggers. There isn't a way to to it declaratively.