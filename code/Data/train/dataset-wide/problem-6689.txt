The issue of completing incomplete sentences has actually been adressed formally. There are many way to view it. Modifying the grammar is a possibility, but not the best method in my opinion, at least if you are too crude about it. There are very generic techniques for ill-formed input that can be used to modify in various ways a non-grammatical input sentence, so as to make it grammatical. It can even use weights on the types of modifications considered so as to choose modifications with the smallest weight. Your problem is a special case. Existing general CF parsing algorithms (not necessarily as currently implemented) can handle that for you without any modification of the grammar. Instead, to explain it succintly, you can modify the sentence by adding automatically special wildcard symbols between words that can stand either for any part of speech, or for a sequence of parts of speech of arbitrary length, wherever you wish to allow for more words. Then a chart parser can actually parse this input and produce a parse forest that represents all the possible sentences that would fit your sequence of words, with the necessary additions where you allowed it. Actually this parse forest is a grammar of the language specialized so that it generates only grammatical sentences that fit your initial pattern. Any small sentence that you generate with that grammar will be an answer to your problem. It sounds a bit too easy, but it really is, and it works. It is a very general technique for correcting ill-formed input, and actually subsumes several specialized techniques that have been proposed in the literature. You can find a description of this technique in a rather old paper: Parsing Incomplete Sentences - B. Lang - Coling 88 (see section 4). It can also be found in the Grune-Jacobs book on parsing techniques in the chapter "Parsing as Intersection". The general idea regarding ill-formed input is also described succintly in a paper on parsing as intersection: "Recognition can be harder than parsing". The original technical description of the chart parsing technique may seem too mathematical or a bit outdated, and the description by Grune and Jacobs may be simpler to read. If you want to understand it more simply, think of your initial sequence augmented with wild cards as a finite state automaton that generates all sequences of words that you would consider acceptable if they were grammatical. For example, with the notation of the paper where ? stands for a single part of speech and * stands for any sequence, the sequence: 

Of course "he gave the animal to the animal" is as ambiguous as "he gave the animal the animal", short of semantic disambiguation (few mice have eaten live snakes). 

To make a short answer. You have it backward. Linguistics, like other sciences, tries to provide description of what is. Linguistics is descriptive, but not prescriptive. No one invented grammars (except for very few artificial languages), or other language strucutures, They happened because that is how humans, collectively, felt it convenient to talk, for whatever good or bad reason. Possibly it is good, possibly it is bad, but it is not really open to value judgement: it just is. And no one is making a decision one way or another. In this sense it is neither more nor less absurd than many other social rules, and following it is necessary for social life. You do not walk naked to work when it is hot. Why not? Now, many social rules can be described, and some times explained. That is the work of scientists. Do not pick on them, or on anyone. It is a collective thing that cannot be controled. Some people have tried, not even to make the rules, but only to document them and freeze them. This was more or less the role of the Academie FranÃ§aise for French, when it was created some 4 centuries ago. But the language evolved despite the Academie, and the Academie had to follow. You cannot be prescriptive, even with a good description. Evolution of life is based on chance, but still follows some general rules. For example, any trait that will save energy expenditure is favorable. It is probable that, to some extent, languages evolve similarly, under environmental constraints. Also languages represent concepts that people want to communicate. These concepts can actually vary from culture to culture. It may vary in the words. Some Nordic languages may have more words for white or for snow, with nuances that do not matter to us. Some culture give genders to objects, while other do not. Some have tenses that express very specific relation that have less significance for other. The grammar is often only part of a mirror of cultural evolution, of the way people think. And what really matters when learning a language is to learn to think in that language. That is much harder than grammar. 

Additional note: Other candidates for non-context-freeness are the free word order languages. It is not a type of syntax I know well, but I think it should be mentioned. This case is complicated by the fact that some languages have partially free word order. Though there has been significant work to encode free word order into the context-free framework, one may wonder whether it is always really meaningful structurally. The point is that full free word order amounts to considering strings as unordered sets of words. This implies a much greater combinatorics for associating the words, potentially giving rise to exponential algorithms rather polynomial ones, as is the case for context-free parsing, which is cubic. This can be dealt with by using other information in the parts of speech (morphological marking), through different kinds of algorithms with tractable complexity, and some have been proposed (see 4th reference). Of course, if there are no constraints on word order (or limited ones), the language may well be regular, hence weakly context-free. But, in this case, there is little hope to use information from the context-free parsing (there is none for a regular grammar) to recreate a syntactic structure for the sentence being analyzed. Weak context-freeness does not help. References: 

It has been observed since around 1982 that cross-serial dependency is not strongly context-free, which is rather obvious. But there was some debate regarding weak context-freeness of languages with cross-serial dependency. The usual example was cross-serial dependency in Dutch, but there were arguments to show that Dutch was still weakly context-free. In 1985, Stuart Shieber used the case of Swiss-German cross-serial dependency as evidence of a weakly non-context-free natural language. He used explicit case marking of the language and case based ordering rules for subordinate clauses to show that (the cross-serial part of) the language is weakly context-free only if the language of strings of the form {w a^m b^n x c^m d^n y | n,m >0} where a, b, c and d are symbols, w,x,y are strings not including these symbols, and a^m means a repeated m times. But this language is known to be weakly non-context-free, as can easily be shown with the pumping lemma. However, cross-serial dependency can be handled, in the strong structural sense, by a slightly more complex formalism, the tree-adjoining grammars. There is actually a detailed account of this topic (which I found after writing this answer) written by Johan Behrenfeldt as his MS thesis in 2009. 

If you are looking for parse trees according to your grammar, the first tree seems correct, up to the missing symbols, and a missing above . However your second diagram should look as follow, drawing only the relevant part that changes (i.e. under NOM_1): 

I believe it might be possible, provided you give a definition of what is a grammatically correct sentence in English, i.e. you give an accurate English grammar. You may notice that I am turning your words around, in a somewhat tautological way. My intent is to question your question. You can wonder whether something belongs to a set only if you are first careful to define what that set is. Here, your set is defined by "English grammar", which is a bit short. Is there one prototype "English grammar" in platinum and iridium alloy, as there is (or was) for the meter, to serve as reference to decide whether a sentence is English or not? (BTW, what is a sentence? ... but I will skip that). The answer is obviously no. But my intent is to tell you that the first problem is not with the science or technology for checking whether a sentence is grammatically correct, but with defining what is a grammatically correct sentence, if it is definable at all. They assuming that you manage to agree on some definition, with a group of at least two people, you need some formal way of defining a grammatically correct sentence, so that grammaticality is well defined. Many formalizations of natural language have been proposed. One major difficulty is that the subtlety of the language, and the great variety of permissible constructions is such that the formalizations used can reach very high levels of complexity, such as is permitted by Chomsky type 0 grammars. Unfortunately, at that level of sophistication of linguistic description, you may hit limitations of what mathematics, or any technology or device can do for you. You are in the realm of recursively enumerable formalisms. That may mean that you may have a definition that will allow you to recognize grammatical sentences, but it will be impossible tho ascertain that a sentence is not grammatical. The conclusion is that defining a procedure to check grammaticality will hit two kinds of limitations: 

may be read as an automaton with a linear structure, except for a loop (on the state between absolutely and always) that can generate an arbitrary number of parts of speech. 

If adverbs are an independent part of speech that cannot be inflected, it follows from the definitions you give (which I also checked in Wikipedia), that any adverb is trivially its own lemma. That may be often the case in English (not my mother tongue, in case it was not noticed). But not always: for example, some short adverbs can be inflected for comparative and superlatives like adjectives are: such as "fast", "faster", "fastest". In this case, all three adverbs lemmatise to "fast". Correction: "fast" may be seen as an adjective used adverbially, while the corresponding French "vite" is an adverb, and does not inflect, hence my possibly improper choice for an example. I was trying to get an exemple different from "well - better - best" (given in wikipedia) though even well can be seen as an adjective as in "he is well". This seems less the case for "badly - worse - worst". It seems somewhat difficult to find a clear example since words formed and used as adverbs, such as "kindly" from "kind", can be inflected but are also used as adjectives. No other example coming to my mind at this time, I am leaving it open for people who have a better knowledge of English. Actually, there are different kinds of adverbs, playing different roles in sentences, and it seems that some such classes are not inflected at all. It may also vary with languages, though a look at wikipedia seems to suggest that adverbs almost never inflect (at least they never say they do). The only exception seems to be Austronesian languages, the best example being a widely known word "wiki" which has "wikiwiki" as comparative. 

This is an example of a noun phrase composed with two other noun phrases "fish" and "the blue sea", and a preposition, according to a recursive rule. Examples can be more complex and involve different kinds of phrases. Such recursive rules can allow you to make arbitrarily long noun phrases, and thus you can have an infinite number of noun phrases, though all defined with a finite number of rules (of ways to construct them): I added only one rule to your set. Now, whether you have an infinite or a finite number of phrases of a given type depends on the way you understand your language and wish to describe it. In general, people do understand languages as having an infinite number of noun phrases, which can be characterized by a finite number of rules. I insist on the fact that it all depends on the linguist's understanding because there is not "a complete list of rules or something out there". Actually there are usually many such lists for a given language, corresponding to different understandings of the structure of the language, corresponding to different linguistic theories. It does not show too much for simple noun phrases, but the complete description of a language goes far beyond that. Furthermore, there are also different ways of expressing the structure of the language, with rules or otherwise. The world is a complicated place. 

I believe it is very much the case. Just listen to the way some people bark. (belated April first contribution) ... and more seriously, it is most likely that some animal cries (is that language) originated human words by imitation. They can first enter the language via onomatopoeias, and may later evolve on their own. The best known example today is "tweet", but "bark" or "tchirp" could probably qualify too. Though the origin of "bark" could be different. Regarding real language, such as may have been spoken by Neanderthal people, it is probably too far away in time, as was said. Then there is the distinct possibility that intelligent communication requires structures that are universal, not because we are all similarly wired, but because that is what communication entails (which would then cause us to be similarly wired). If a non-human specie evolved language independently of us, it could well be that their language structure would not surprise too much human linguists. Then, if such beings had influenced human languages 5 or 10 thousands years ago, possibly less, it might not even be detectable. 

I am not sure this is the right site for the question. This is my understanding, as a non native English speaker. :-) I am answering because I am interested in comments. I think the major difference is that rephrasing occurs in the same discourse usually to improve understanding or precision of meaning. It is not necessarily done at the same time, or by the same person. Paraphrasing is usually done by another person, who is somehow appropriating the phrase (even though she may quote the original author), possibly for a different context or different discourse. One can paraphrase herself when quoting herself in a different context, hence adapting meaning. I would think the length does not change too much in either case. I would think that rephrasing tries to preserve or improve meaning, hence can add new information, while paraphrasing can adapt it, thus modifying the information. 

which is then ambiguous since there is no verbal inflection to tell the difference. The interesting part, in this, is that the conjunction no longer has a causality undertone. What is left in common between the conjunctions in the examples is that they are related to a logical operator, for which negation also has meaning. Could it be that the distinctive character for relevant conjunctions is that they have an associated logical semantics, which naturally has a negative interpretation. But I am not a native speaker, and you may be able to come up with better examples. 

I am a bit suprised that my previous contribution ( which I am keeping unchanged below theline) seems to be considered irrelevant. Hence I am trying to explain it. It is clear that other answers more or less exclude very formal linguistic expression such as programming languages. The fact is there is very little literature claiming to relate them to linguistic analysis, and even this claim is rather weak. Furthermore it does not seem to involve recognized linguists. Another such paper (in addition to the one below) is: 

I do not know that I have a proper answer, but I would like to summarize my findings. Recall that I am not a native speaker, and it may impact my perception of the language. Also, I am not taking in consideration other aspects that may help disambiguation, such as prosody. Here is the original example, and two others I found, with different conjunctions: 

What is common to all three examples is an undertone of causality in the conjunction and the negative form. The issue is whether the negation applies to the whole sentence, thus dominating (?) the conjunction, or only to the first proposition, which changes the meaning. Regarding syntax, and I am considering only formal languages, I am not sure I like to define it with a Context-Free Grammar. Here is a suggestion (approximately): 

On this site, I would expect that this question is about linguistically meaningful examples of syntactic structures that cannot be represented by BNF. A first minor point is that BNF is a programmer/compiler oriented syntax (introduced with the programming language Algol 60) for representing what should rather be called more abstractedly context-free grammar (CFG). The adequacy of context-free grammars for representing the syntax of natural languages has been addressed by many people in different way. It should be also noted that we are mostly talking of a syntactic infrastructure, based on formal language theory, since linguist often consider as part of syntax various attribute and features that can be associated with this syntactic infrastructure. The most frequently given example of syntactic structure that cannot be represented by a CF grammar is apparently the cross-serial dependency represented in the following diagram. (source wikipedia) Such a syntactic structure is found in some languages, including Dutch and Swiss-German. Here is a Swiss-German example from Stuart Shieber: