Another possibility is that you need to install the Microsoft C++ 2010 Redistributable manually: $URL$ 

You can create tables because the RESOURCE role has the CREATE TABLE privilege granted. To create a view, you need to have granted the CREATE VIEW privilege additionally, because, by default that privilege is not granted, not even to RESOURCE. 

counts values that are not . You can transform the unneeded values into NULL for example with . Combining the two as below: 

Privileges Required for the DBMS_REDEFINITION Package 11.2 You do not need that role, that is just an example. I would not use that role, as it grants execute privileges on a lot of other packages as well. You need the following privileges: 

479 of them is not related to any current session (the remaining one I created artificially just to see a difference). 

Anything else in addition to ? When granting , you implicitly "get" on the (I replaced EABINTEG with U2 in your query): 

Then manually upgrade the database to 12.2: Example of Manual Upgrade of Windows Non-CDB Oracle Database 11.2.0.3 

Then returns , and not , so your policies do not work. Your other policies are non-functional as well. Those events are audited because they are part of the built-in policy which is enabled by default. Just log in with and run the below to verify: 

Each worker thread uses a seperate dumpfile and requires exclusive access to it. If you have fewer dumpfiles than the degree of parallelism you provided, the parallel threads will not be fully utilized. If you have a single dump file, only 1 worker thread can be active. Tables that contain Basicfile LOB columns are not be exported or imported in parallel. This is a known restriction and documented in: DataPump Export/Import Of LOBs Are Not Executed in Parallel (Doc ID 1467662.1) 

A basic overview of feature availability for different database editions: Products and Services - Database - Enterprise Edition - Comparisons Java is not available in Express Edition. 

This is typically caused by some leftover socket files at temporary locations with wrong permission (for example someone started the listener as root). Shut down the database and delete those file as: 

View other columns as well, for example and as suggested. Also, join to to find which entries are current. 

You can not do that. You need to specify the date yourself, as that clause accepts only literals or , and not expressions. 

works, because you did not specify or , and in this case, is the default. does not need to perform comparison. But if you try (which removes duplicates), that will also fail: 

This is incorrect because you need the 32-bit version (x86). Now if you check stage/ext/bin, there is a file vcredist_x86.exe, and obviously no x64, since it is a 32-bit installer. Try editing oraparam.init and fixing the above line to: 

It is not mandatory, do not use it if you do not want to. Creating such symlinks just makes it easier to distinguish the ASM disks. You can even set to with the above. 

Variables you define with the keyword can be used as bind variables, not like PL/SQL variables, so you need to add a colon (). 

The above connection string should work with the addition of . Seems like your SQL Developer connection is configured differently. Alternatively, you can define the connection type as , and use the below custom JDBC url: 

The installation of a PSU is not complete without the execution of the postinstall script. Opatch works at file level, it replaces/relinks files in the Oracle home. Changes inside the database are performed by running the postinstall script. Sometimes I see DBAs forget this step. The postinstall script maintains the table (on which the view is built) with the actual patch versions. In 11.2, you run the postinstall script as (stated in the README): 

If you have access to the logfile in redo group C, and you can copy it to the new server, then you will be able to use it for recovering the database on the new server, so you will not lose the data in it. If you no longer have access to the redo logfile (because not only the database, but the server or storage also crashed, and you are unable to start/repair them), and all you have is your backup, then yes, you will lose the data created after the last backup. 

Types defined in PL/SQL code can not be used as table column types. Custom types to be used as table column types must be defined with the CREATE TYPE statement: 

can be set in the ASM parameterfile and in the GPnP profile as well. Grid Infrastructure needs to know where to search for ASM disks before starting ASM and reading the ASM parameterfile, the GPnP profile makes that possible. dsget 

Also, Unified Audit entires can be buffered in the SGA and be written to disk later and not immediately, in that case, flush manually before querying: 

Comparing apples to oranges. The 2 queries and the results they return are not the same. First query retrieves 999 rows from + , joins to and sorts the result. The result of the 3 table join can be 999 rows or 10 million rows as well, depending on the data. If it is 10 million, it has to sort 10 million rows. Second query retrieves 999 rows from + and sorts them right after. Finally joins the , and does not sort the final result. The first query sorts everything, the seconds query sorts only the result of the subquery. Sorting 10 million rows will most likely need more resources+time than sorting 999 rows. The cost of sorting is often overlooked. The first query returns a sorted resultset, but for the second query, there is no guarantee of returning a sorted result. The second query may return inconsistent results between different database versions because of how the optimizer works in different versions. For example if column is indexed, the optimizer may choose to access the table using the index, and returns the first 999 rows sorted anyway, because data is already sorted in the index. Or it may choose to scan the table, get whatever 999 rows it can, then sort that. This can be easily tested with a hints such as: and providing different versions. The first query is guaranteed to return the same result on all versions, as long as the data is organized the same way. As there is no criteria specified for getting the first 999 rows (first, based on what?), after reorganizing the table (move, shrink, export/import, redefinition), it may also return different results for the same base data. In this aspect, both queries are inaccurate, unless you do not care about consistent results - which is a quite rare situation. 

If the specified dumpfile already exists, this line throws the above error. You can add to overwrite the existing file and avoid this error. 

You are not supposed to edit the spfile manually as it is in binary format and also stores a checksum. Start sqlplus, create a text version of the parameterfile: 

means the redo transport will be acknowledged before writing the received redo to disk (standby redo logs). 

This is normal if you have your FK constraint defined with the option. When you delete from tab1, that will cause deletes from tab2 also because of the constraint definition. If you disable the constraint, rows from tab2 will not be deleted, the trigger on tab2 will not even run. Below is an example: 

Seems simply a password issue. Different instances may have different passwords, the SYS password is not necessarily the same for all the instances on the same server. Make sure is configured properly in the instances, and the password file exists for instance on the server in (Linux-Unix) or (Windows). If you do not know the correct SYS password, you can recreate the password file, for example: 

You can then search support.oracle.com using this information. This looks like Bug 9577499 ORA-7445 [kkqtutlSetViewCols] when query fails to rewrite. The suggested workaround is: This disables that feature at session level, another alternative would be disabling it only for your SQL statement that throws this error with a hint: 

Prior to 12c, Oracle made only the base releases publicly avialable on OTN, to download patch set releases, you needed to have a valid licence and support contract. If you have a valid licence and support contract, you can still download 11.2.0.3 from $URL$ Or using this link: $URL$ The file you are looking for is: 

A script containing a statement does not fall into any of the above categories. Also, keep in mind, unwrapping such source takes seconds (for example: $URL$ Oracle generously classified this method as (see above URL), but I would rather classify this as . 

This is definitely possible both with and and any other mainstream virtualization product as well I guess. On my sandbox, I have created an additional virtual switch that is not mapped to any physical network adapters. This network is not visible/reachable from the outside world, but VMs attached to it can reach each other through interfaces using this switch, so it's perfect for a RAC private network with 1 host machine. In , I use (available since version 4.3) for RAC private network. It's a similar concept, a virtual network not visible/reachable from the outside world, but VMs with interfaces in this network can communicate each other through it. You can use the above for RAC public networks as well, so you don't need to allocate any IP addresses on your real physical network. For example, in , you can use 1 network interface as "management interface", and define port forwarding rules on it, so you can access the VM from your local machine thorugh RDP/SSH without using the VM console. Then use another interface for the RAC public network, and another interface for the RAC private network, so 3 interfaces in total. I would not bother with , both and support shared virtual disks. 

Note that this is a Query Transformation, but NOT a Cost-based Query Transformation. As you can see from the "Final query" part, the optimizer removed DISTINCT from the query. But there is no such optimization for GROUP BY. DISTINCT is used for retrieving distinct values, but GROUP BY is used for producing aggregates, not just distinct values. Even if the optimizer can skip sorting or hashing the data, it can not skip counting, adding, calculating the average, etc, and this is the important difference, so DISTINCT and GROUP BY are not handled in the same way (even if aggregates are not specified). Another case of eliminating the DISTINCT part, when it is obviously unnecessary, for example: 

I had to try it myself, because I did not believe SQL*Plus can not handle this. Apparently, it can. Script for generating the create statement: 

Starting with 11.2, you do not need to create the trigger, you can define a role based service with Grid Infrastructure. : 

You have enabled , but have you enabled Unified Auditing properly? With the default, mixed-mode auditing, setting to , prevents logon failures to be audited with enabled. This is how it works with everything set to default (audit_trail set to by DBCA): 

If this was a one-time error, just try running SQL*Loader again. If this happens repeatedly, you will need the help of the DBA. The above views barely contain anything informative related to this issue. (Also there is no such view as or .) When this error occurs, the database instance creates a trace file on the database server (alert and file location can be found in the alert log), that is what you will need for further investigation. 

After you found the ASM generated name for the disk you want to shrink, reduce it in ASM to the desired size (I will assume it is called : 

This looks like a typical hierarchical query, but it was implemented the "manual" way. I do not think you should focus on the here. You should rather rewrite this as a true hierarchical query by using the clause. Something like this (just an example, not necessarily what you need): 

You have a container database and you are in the container. In the root container, you can create only common users. Local Users in a CDB 

A possible bug for not displaying the constraint name with bulk DML and clause: Bug 7210333 : CONSTRAINT NAME IN SQLERRM IS NOT OUTPUT WHEN SAVE EXCEPTIONS IS USED To catch the records causing the error, you could use DML error logging as well: 

That is the Oracle Cluster Registry (OCR) where information about clusterware components is stored. $URL$ Its size is 256 MB, or 0.26GB, you can check this with ocrcheck: 

Should be, because depending on the scale of inconsistency of the remaining SYSTEM datafile, the instance may not open at all, or it may crash immediately or in best case scenario, it may open and remain stable so you can query , , etc. 

You should compare same datatypes. If you must convert different datatypes, converting the constant or bind variable is way better. The above prevents using a regular index on the column because of the conversino. Do it like this: 

There is no officially published method for this and it is not documented, but still possible, with . Here is an example: $URL$ 

Oracle on Windows creates a service for database instances, called . By default, this service uses the account so a password change of the installation user will not affect the database. It is however possible and recommended to change the service user from to a standard Windows account. If you performed this step, and you wish to change the password of that account, you must update the password in the service configuration. Managing Oracle Home User 

More details at (referring this blog post of an Oracle employee, because I could not find all this nicely collected together in the official public documentation): Component Clean Up Series 

Forget GROUP BY, this is much more effective with analytic functions. Use outer join if needed (employees without departments, departments without employees). 

Modifications performed by a session are not visible to others until they are commited. Issue a after performing these changes. 

Or you can import the table directly from database1 to database2 through a database link. Using Network Link Import to Move Data 

And this is the problem, . But don't ask me why, it is just one of those "unexplainable" things in Oracle. I have seen a case, where the transformation resulted in a similarly ambigiuous final query, and there was an actual error message in the trace file, still, the database returned wrong results and no error to the client. 

The problem with that: you can not grant password protected roles to another role. See $URL$ for details. 

You can disable remote logins by setting to , so only local logins are allowed. This will prevent using Data Guard though for example. You can also prevent by setting in . Finally, you can completely lock yourself out by combining the above with deleting the password file from . This way you will not be able to log in as or at all. But even still, user will not be really locked, its just that you can not be authenticated. If you recreate the passwordfile, you will be able to get in again.