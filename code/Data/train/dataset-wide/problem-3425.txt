If you want consistent IE compatibility for your SSL connection without errors, you will need to have multiple IPs. If you are positive that you are going to stick with the list of hostnames you identified for at least a year, I would go with UCC because it is cheaper. If you anticipate a need to throw up a bunch more SSL vhosts later on hostnames under the same domain, the ROI is with the wildcard cert as you would not have to buy a new certificate. Both UCC and wildcard will be implemented the same way and maintenance will not be variable. 

It's possible to do anything. Network separation and encryption are two of the more important aspects of the architecture to protect important data. The Payment Card Industry Data Security Standard contains a series of practices designed to protect credit card information, which could provide you a baseline for a point of reference. PCI DSS 

It entirely depends on your configuration. A QMAIL install by itself uses local user accounts, which have one-way hashes for the passwords. You could parse out the usernames from and the password hashes from . In all likelihood, you can build the system and copy most of the configuration directly over. What additional software are you using with your QMAIL installation? Are there any particular areas you needed more details about? 

I disagree wholeheartedly with Kyle. If it is not necessary, it should be removed. It's a best practice to not install unnecessary software. The person undertaking the task, such as yourself, should be confident in the implications of the decisions they are making. Removing standard system utilities and libraries is generally frowned upon but that will partially depend on your environment and server role. If there are package dependencies, they will be identified in modern systems. If later, code you are compiling requires a library you removed, you install it then -- as it is then required. I'm not going to leave GTK installed on a server just because some future system administrator might be an idiot. 

Typically on a multi-user system you would simply set the directory executable and leave all files within the directory public readable. If you run the server yourself, you will have the ability to set the group to the same group as the Apache group and set the directory group writable. Additionally, you would want to SGID, so as that anything created within the directory inherits ownership. 

Check permissions, make sure ownership and permissions are correct on . This is typically . If this is not the issue, produce output of your log file. Try the following to correct permissions: 

NFS uses the filesystem permissions across systems. A reasonable solution would to use a group that all users were members of and set the SGID bit on the directory as well. Public writable is for /tmp. If you must, at least set the sticky bit. 

Uh, renaming tables randomly is a horrible idea. Look at the MySQL process list to determine what processes are loading the system. Use: 

A buffer overflow more often than not result in account compromise for the user running the daemon in question. Buffer overflows are sometimes Denial of Service but typically not and often end up being system level compromise after further research is done. If actually an issue, this can be a big deal. I would rank it between medium and high depending where it touches my infrastructure and the size of the footprint. You are going to need to verify two core things: 

You can use to read the time stored in the NVRAM. (I still like to call it CMOS, misnomer or not.) In all likelihood, you rebooted and the time is set incorrectly there. If your mainboard battery is dead, you shutdown for 7 hours, and booted back up; you would lose that time. Otherwise, it's potentially a time skew based compounded from a dying mainboard battery. 

While you might find someone who has created a conventions including color, you will not find it respected universally in IT. Block diagram design techniques are often shared but specific and consistent conventions will be unique to IT departments and individuals. Of course, with specific types of diagrams such as circuit diagrams, you will encounter more rigid standards. This does not apply in IT. There are conventions within types of diagrams but not to the level of detail you are requesting. If you want to explore types of diagrams, this Wikipedia article is a good start. The IEC 61131 standard includes a specification for a Functional Block Diagram in the context of programming languages. Nevertheless, as detailed before, this will not be universal. If you want examples of how other people diagram their infrastructure in IT, the Rate My Network Diagram Web site is a good start. 

You could use SQUID as a transparent proxy to accomplish this. It doesn't have to be HTTP. But really, you have two different hostnames. You can point them to two different IPs and DNAT the traffic accordingly. 

Protocols such as FTP, HTTP, NFS, and SSH. I typically only use SAMBA file sharing for conveniently transferring files between platforms. 

Unless you wiped out your database, permissions are likely incorrect. MySQL often runs as . If your data directory were in you would want to insure that all databases within that directory were owned by . 

You can specify a specific IP in place with the asterisk, as long as the IP is specified with NameVirtualHost. The name is specified in and . The asterisk is matching all IP addresses that Apache binds to in the . 

It depends how QMAIL was installed on your particular server. The thing is, naming conventions and installations are going to vary. I have my own build I compile from source and am not going to make assumptions about your configuration. If you look at the process list, you will see the entry for probably using . You can specify the variable for certain hosts to allow relaying. It is a binary database, so after updating the flatfile you will need to recreate it and then restart the tcpserver instance running qmail-smtpd. The documentation on the QMAIL Web site called "Setting up servers" describes this in detail. If you encounter a particular issue and describe it in detail, I'll likely be able to help. 

There's a command line utility called blat that can be used to send e-Mail in Windows. In UNIX, you could use the userland tool to interact with the queue. Most MTAs have equivalents symlinked from the same location. The mail utility is available on most systems as well. I'm entirely on board with Chris' monitoring recommendation, if you don't already have monitoring. Otherwise, verification of functionality after a change makes plenty of sense. 

While the existing answers are fine, not one person specifically mentioned that setting the home directory is done in the password file. You can change the home directory by editing the file (use ) or running the following command: 

It will be difficult to locate secure infrastructure without being more hands on. You will likely be unable to fully secure the data stored on a hosting environment such as Amazon EC2. Specifically in regards to credit card storage and PCI, it's generally recommended not to store data in those environments. Even having a dedicated server will not meet PCI requirements for storing data, as you will need to physically secure the space and have controlled physical access. To meet most security requirements, you would need your own space with access control and physical security. For example, a fully enclosed cage within a shared data center could qualify. Most lower level products such as virtual private servers and dedicated servers would be unlikely to qualify. The regulations applicable are going to be unique to the data, which will help clarify the exact requirements. If you want to get an idea for what you are getting into, you can look at the PCI DSS. Good luck. 

I am assuming an operational replication configuration with your MySQL servers. First, you create the database on the master. On the slaves, you update your cnf specifying for the new databases. is configured in either the cnf or as a flag () and cannot be changed dynamically as a variable.td From that point, you can populate the schema and master on the data and replicate down. You can also create and populate the database on the master and all slaves then enable . The key is making sure that the data matches before enabling replication on the slaves. 

Xtrabackup is a useful tool. If you have replication, additional tools such as Maatkit will be extremely helpful. 

At first I was not going to respond, as I do not have a specific software package to recommend. Nevertheless, the lack of response does not give this question the proper justice. I tend to lean towards creating or customizing tools to serve the specific needs in question. Currently, I use a combination of tools. Specific to the Service Level Agreement (SLA): My current SLA is focused on production uptime of critical services. The three categories are critical, major, and minor. Critical is revenue impacting, major is internal production/non revenue impacting, and minor is everything else. We base the SLA reporting on critical services. The primary method for tracking this metric is functionality that was developed within the Web application we created for tracking system and network changes. If a change is made to anything, it is logged to this system. It's essentially a fancy MOTD that's designed to be simple, quick, and easy. In case of outage, the log entry records the level of service, the length of the outage, the type of service, and finally the cause of outage. If external we would record but not count against our internal metric. Scheduled changes are identified and not reported against the SLA. Reports and graphs are based off of this. A checkbox e-Mails outage notifications to an e-Mail list, which is utilized for notifications before and after. The additional supplement to this is external monitoring based on availability and response times, which I currently use Web Site Pulse for as well as scripts on external servers. I'd seriously recommend you consider creating and/or customizing tools to meet your exact requirements. It's an incredibly useful approach. You may also find Request Tracker useful, which is something I've used for access and change control as well as a normal ticketing system. It's highly customizable, which may enable you to use it for your SLA reporting. 

MySQL replication is fast, really fast. It's primary limitation is the link layer and then the rest of the hardware. In your case, resuming replication will probably show the binary logs or slave IO catching up fairly quickly. If not, improve your link first. Otherwise, if it is the SQL, you're going to have the physical limitations of the server, which is going to vary between disk IO, RAM, and CPU depending upon the type of load. 

You can hot-swap any drive that the MD3000i supports. It's the controller that allows hot swapping, not the disks. 

Pick a distribution that's small and has X11 available. Do not use a window manager and specify the software you want to execute within . There are typically system-wide scripts that impact the starting of X11 as well if you want to move it out of the rc file but they are often distribution specific. If you want recommendations on client software, you might have better luck on Superuser. 

I'd recommend any combination of: wikis, Jabber, and IRC. Some companies are also comfortable with the use of AIM/ICQ, which I find absolutely necessary to communicate with companies I do regular business with. 

While you can redirect different types of output different ways, for more advanced handling such as digests, you will need to script it. As a side note, important notifications shouldn't be e-Mailed. Use central logging (syslog) or monitoring. (nagios) 

Please describe in detail how all your servers are physically connected including ethernet cable routing and any switch configuration. You may have issues using network address translation via your gateway, as it's daisy-chained to a router. What sort of router? What sort of Internet connection? Can you connect your gateway directly to the Internet? It should be the primary router. Regardless, we should be able to isolate the cause of the communication issue with your gateway and Web server. Do you have any firewall rules on your Web server? Please verify and be certain. What does the routing table and interface list look like on your Web server? Enable logging for easier debugging: 

I hear that Web sites are pretty cool to run and use, there's always Apache. Fun things but could be applied to work as well.. 

If you are using the SFTP SubSystem where it spawns a separate process, you could create a sftp group and only allow execution of the binary for that group. It will not be possible to do this with the newer internalized sftp daemon, which is specified with . If they have shell access, they will still be able to scp. Do you have a particular goal in mind? Edit If you want to restrict your user to only executing or utilizing a specific program, I would probably recommend a shell wrapper instead. might work but it seems more likely to be fallible. I would do more testing to be sure. A shell wrapper, such as scponly, will only allow the end-user to scp. I have modified the source of scponly before to only allow CVS execution, for example. This can also be done with a shell script but it is easier to make mistakes if you do not fully understand the scope of what you are trying to do. 

There are a variety of different limits that can affect the operation of your Web proxy. As sysadmin1138 mentioned, TCP connections is one of them. Another, as Kyle managed to post before me, is the file descriptors. SQUID's default, at least with 2.6, is 1024. To increase this limit, you have to recompile increasing . Even after recompiled with higher FD, the will be in effect for the user starting squid. For example to increase resource limit to 8192 run this before starting squid: 

I have found with Scan Alert (now McAfee) rarely produce false positives. Unless your Approved Scanner Vendor has a history of false positives, I would assume that it is a legitimate vulnerability until you prove otherwise. If vulnerable, it should be easy enough to locate a supported patch for your distribution. 

If PHP is configured properly in Apache and the client is functioning properly, the PHP script may not be specifying the proper MIME type for the content output. The often has to be printed for the browser to understand how to handle the data. What is the content of the file that Firefox is attempting to download? 

If you want to have fine grained manipulation of the interaction between the client and your server, mod_security is the way to go. It's a fine Open Source solution if you want to implement a "Web Application Firewall." (WAF) If you do not want to spend the time properly configuring it and writing rules suited for your particular applications the benefit will be more limited. As with other security solutions such as network and host based IDS, WAF are becoming more common. It looks like WAF have the potential to become a standard in the industry. Nevertheless, as with IDS, mod_security is only as good as the person responsible for it is willing to make it. If you had an application you did not develop and were particularly concerned about, you could limit the potential risk via mod_security without modifying the application. 

If you want to transfer restricted data to the Internet, the solution is not necessarily to give them access to the restricted network segment via the Internet. In fact, I would strongly discourage it as you describe it. What you ask is actually quite complicated to implement in a responsible manner. For illustration, you have two network segments. A DMZ and a private network. Databases live in the private and Web servers live in the DMZ. For security purposes, you fully restrict access to and from the private network using a firewall. If the DMZ is compromised and the authentication data is stored on the server, the cracker will be able to access the restricted data. This is where encryption requirements and key management techniques come in to place, which are exampled within the PCI DSS. If you do not have an advanced encryption architecture, you will still risk the data in case of compromise even if it is not stored in the DMZ. You could potentially implement a ETL and batch out the data. Often, this solution dictates a need to have the data encrypted using strong encryption and then transferring via your preferred protocol. Once the data is encrypted, the methods used to transfer it can be substantially more flexible. Your exact situation is going to dictate how much effort is made to establish a production worthy solution. If you a dealing with a one-off request, you might be best off manually satisfying it via a tool such as GnuPG. Otherwise, you might need to build, find or buy an application. An approach that is becoming more common is using a Web application to satisfy the security requirements, while still allowing the data to be accessible to those with less technical knowledge.