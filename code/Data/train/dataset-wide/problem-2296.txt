in pg_hba.conf, in the lines with localhost IP, replace "ident" by "md5" and restart - then you will be able to use password logins if (1) is not acceptable, sudo to "postgres" user and create another superuser login - then use this login to connect from PgAdmin. 

Solution 2 Using more ANSI-compatible SQL, like UNION and LIMIT. It will work on MySQL, DB2 and some others. Similar solution can be done on Oracle, just replace LIMIT with ROWNUM. 

Decide your vertexes (nodes, objects) and edges (relationships). Convert relational data to cypher, declaring all items and all relationships explicit. 

Only thing I do not understand in your question is "database agnostic" - what does it mean, precisely? 

If you want to see also query duration, you can set instead of . This is very useful for query tuning. Then reload config (restart or HUP) and collect enough log to estimate traffic. Note: neither method will include queries embedded in user-defined functions. 

Exception blocks are meant for trapping errors, not checking conditions. In other words, if some condition can be handled at compile time, it should not be trapped as error but resolved by ordinary program logic. In Trapping Errors section of PL/PgSQL documentation you can find such tip: 

For better effect please post results of and . Also let us know the dataset size and resource configuration parameters (, ) as well as database host parameters (OS, memory, disks). 

Internal representation of larger attributes will be sometimes compressed. More specifically, what works here is the TOAST (Oversized Attribute Storage component used in PostgreSQL). The threshold when values are considered for compression is 2000 bytes. is not a logical length, but the size (in bytes) of actual internal representation of the column/variable. It is documented. PostgreSQL stores array values in a custom, internal, binary format. Command line example below. Details also here. 

In SQL Server, I'm getting the following error "The query has been canceled because the estimated cost of this query (5822) exceeds the configured threshold of 300. Contact the system administrator." This is the result of an execution of a stored procedure, which is pretty complex. I haven't run into this for other stored procedures, only this one. Is it possible to change the query cost for this one procedure somehow? Can I do that in the stored procedure itself upon execution? Or do I have to define this on the server only? I'm using ADO.NET command to execute the stored procedure. Thanks. 

I created an alias named "ProductServer" to point to a SQL Server 2008 R2 named instance on my local machine. I'm trying to upgrade that to point to SQL Server 2014 named instance. I used SQL Server Configuration Manager to change the entry to point to the correct instance, and it shows correctly. The Client Network Utility confirms the change is configured correctly. When I connect to ProductServer from SSMS, after restarting the machine, it still points to the wrong instance (still 2008R2). Any ideas why it won't map to the correct one? 

I have a table in a database with three columns: ID, Key and Value. Everytime I update the record, I have a trigger that sends these updates to another table and creates a new record in it, who's schema is this: 

We have sensitive information (names of people) and we're looking to put into place a script to "scrub" of the names. Has anybody come up with a good algorithm to do this, something other than just making all of the last names "TEST" or something? Thanks. 

Desired result from the sample data above: The resulting data should be the rows in which the version in the database on the server side is greater than the data with the corresponding in the incoming data set. 

The tables in the databases were then filled with random versions from 0 to 5 and a semi-random amount of lorem ipsum text. The table got 100 rows, the table got 500 rows and so forth. I then ran test for both the query using nested statements and using a temporary table with all s and random version between 0 and 5. Results Results for nested query. Number of repeats for each was 1000. 

Data sent from the client: The client then queries the server with the following example data. It is arriving as JSON, but the format not matter for this question. 

Working solution One working solution is to create the following query based on the client data set. 

My server side is , so creating this query when I have the JSON data is no problem. For a small data set, this query is fast. The data set is organic, however, and will increase with time, up to maybe 1000 rows on both the client and the server side. At the same time, the number of clients is likely to grow as well, hopefully up to around 10 000 or maybe more, thus I am not sure that this is the most efficient solution for performing this query. Another working solution Here is another working solution using virtual tables. Will this in any way be more efficient than the multiple solution if the data set is a lot larger? 

There is a lot of good sources on to partition or not to partition. If you are going to store 2 years and more, daily partitions could be optimal. But remember that very large number of partitions will make query planning longer. Threshold depends on CPU speed / queries used. PS. I assume that you ran out of normal ways to optimize: 

notification ID will be a monotonic bigint never going down and store only one the last ID read (which means all previous notifications for given user are also read) - this will be just one field in table. 

In current version (Pg 9.0-9.3) there is no fast "fail-back" mechanism. You will have to follow official guidelines which state: 

This answer assumes that you want to connect via TCP to localhost. (for local socket connection see Erwin's answer) Two options to ease your pain: 

(note: tool might be hidden in default debian/ubuntu setup. Look in to see it) When it's promoted, replication stops and slave is disconnected from primary. See relevant fragments on command in pg_ctl documentation and failover docs. Question 2 

If you really want two queries, you can use special FOUND variable to test if previous query gave any result: 

Why async? I would avoid doing this in trigger due to locking issues under high load. Also, easy to DDOS so permissions should be separate for form insert and form create. 

I'm not yet sure if this is doable in pure SQL (probably - yes), but here is the brute force solution using cursors. This is just a (working) draft, with some twiddling and dynamic SQL you could add more parameterers, like function name. 

You have to setup the permissions so that only one SQL Server account has access, and only you know the password to that account. That's the only way. Or, use impersonation, and only give that one person's windows account access. 

I have a restore database script in a SQL file, and I'm trying to restore a backup generated from one DB to a new DB (that exists). That SQL file is being executed by SQLCMD on the command line. The command window lets me know the restore finished successfully; however, the database is in a restoring state still, and cannot be accessed. Because of that, the rest of my batch process fails. How can I get the database out of the restoring state, and back into an online state so that I can run additional batch process scripts? I'm using this to do it: 

So anytime the first table record's changed, it creates a new record in the second with the table everytime. For every record with a matching ID, I want to add a unique incremental version number. If a record with ID of 1 is entered into this table 5 times, those records should have version numbers of 1, 2, 3, 4, and 5. Is the way to accomplish this through a subquery for existing version numbers, or setting up a partition, or something else? Thanks. 

Anytime you change Oracle's sqlnet.ora or tnsnames.ora files, does the system require a reboot? In my instance, I only have the Oracle client installed on the machine I'm referring to, but out of curiosity what would it mean for an Oracle server installation? 

I try to saturate the server using pgbouncer. I was running a Select-only test, with 1000 clients for 5 minutes. (). PgBouncer was initializet with scale=100 (but a SELECT-only test should not suffer on it). During the test: 

Update: Konrad corrected my misunderstanding of his question. The goal was to count queries, not transactions. How to count queries? Method 1 Use pg_stat_statements contrib. Method 2 Enable full logging of queries for a representative period of time. To enable full logging, for PostgreSQL 9.0 - 9.3, change following settings in 

To redirect traffic from primary to standby you need some external tool which will do the failover procedure - using either dns-based or IP-based or other failover method. PostgreSQL itself does not know how to redirect traffic or do anything outside the database scope. Popular tools are pgpool (in layer 7) or Linux HA or corosync and friends (in lower layers). 

I understand that you want to go with single database (as it is good from management & maintenance point of view), but maybe it's too much integration. I am assuming that: 

You do not need any triggers / rules to maintain it. There are open ends here - that's just a draft... Some issues: 

You can always calculate higher-level information from lower-lever aggregate. For example if you had an aggregate on (app_id, day, collection_id) you could use it instead of (app_id, day). You can materialize your aggregates with feature. But this is not an only way. If old data is static, it could be enough to insert new rows daily, with something similar to