to find a meaningful example of a paraconsistent logic that allows us to think clearly about it and judge the success of our project as it proceeds to answer Niel de Beaudrap's concern as to whether there is a compelling topology on the space of its decisions to think about what kinds of mappings from that topology onto set-theory or measures seems to capture our 'degree of consistency'. 

Stable reference -- we want the definition to be well-defined, so that the things to which it refers do not change without actual changes in the attributes relevant to the problem domain. In the simplest case, when we define something mathematically, we need to prove the definition fits the axioms of an equivalence relation. For less exact domains, we expect appropriately relaxed standards, but with comparable effects. Intuitive model -- we want it to be reasonably easy to invoke the appropriate model in the imagination of others either by analogy or through construction of some kind, at least if they have a good picture the other rules of the domain. The whole paradigm itself needs to be capturable by a wide range of people, and particular definitions should not impede that to no end. Parsimony -- this is dual: a. we want the definition to be concisely statable in terms of the other definitions in the same paradigm and b. we do not want definitions that push "Occams' razor" aside, and create unnecessary possibilities for expansion of the topic without any basis beyond the implications of the definition itself. Testability -- we want the definition to apply to the cases toward which it is aimed, and ultimately for the applicability of the definition to be testable or provable in some way. 

Point 2 seems contrary to recorded history. It is pretty clear that over a vast sweep of time from the 1600's to the 1900's technology increased alongside freedom for the vast majority of society. We escaped much of the control of the system of vassalage, the Catholic Church and various other hegemonizing institutions and greatly equalized access to resources. This seems to obvious to require any reference. Point 3 requires the perspective that we and our technology are not part of nature, which seems odd. We are an animal species which shapes its environment to suit its goals, like ants. We are not a separate thing from nature. Consider the perspective of someone like James Lovelock. Point 4 seems real. As noted by, for instance Ray Kurzweil and his observations about 'The Sigularity', politics changes at a much slower rate than technology, and our biology and politics have not been able to keep up with ongoing change, much less get ahead of the curve in a way that would give our social contracts leverage over the continued change. It is not obvious that this is a bad thing, from a position like that of the Sovereignty movements or The Rainbow Family, or even mainstream Libertarianism. Attempts at human control of human activity tend to backfire due to excessive dependence upon misapplied theory, as noted by the sociologist Robert Merton, and this continuing wave may lead us to ultimately scale back and accept the internal limitations of our politics to domains where its effects can be honestly tested. Point 5 again does not seem to follow recorded history. Soviet attempts to control technology clearly did seem to impede its progress, even though their aims were to advance it, that involved attempting to 'tame' it, and did, in fact, not strengthen it. The political ascendency of Lysenko, for instance, coupled with a planned economy that gave him power over technology planning did ensure that development resources were squandered and that progress was measured inappropriately in a major sector of the world economy. Based on at least three flawed premises, the ensuing points are not warranted. 

We all have parents, real and metaphorical, and most of us have things to prove to them. For some Jungians, sacrifice is an aspect of the archetype of God as the symbol or projection of our relationship to being parented and belonging to a family/culture. So it would be a basic human drive, if not an innate one. (Kleinians might declare the fear of the Other, to which this is a response, to be completely innate, as the natural split-shadow to our basic narcissistic omnipotence, no one can totally lack it.) From an animal point of view, pack animals are often drawn to bring a kill before their pack leader. And psychoanalytically, the family, the religious community, and the culture in general, are packs. And this is not gone from modern religion. We see this in the rules in Leviticus about 'first fruit'. The part of your harvest you bring in before you have to actually start using it goes to be inspected by the priests. And then again in holocaust. There is a component of proof of value, and willingness to give things up for your community. The odds are that all of this food gets eaten or given to the poor, but that is not the central issue. The central issue is proving the willingness to give, and therefore your usefulness to the tribe. The later form of sacrifice, after the order of Malkezedech and rabbinical blessing, and later basic to Christianity, where what is communally sacrificed is then also consumed, is taken by Jung as symbolic of attaining competence at complex tasks, replacing just having things to willingly give, as a higher standard of proof for the same thing. Having them judged and blessed means we must have products of which we are proud (pictures on the fridge) and it is this pride in our work that is validated by the parent and proves our value. We have to have bread and wine to offer, to prove we won't ever devolve back to just hunting and gathering -- living off squirrels, nuts and berries. But there is still the urge to convince the parental figure of your value, and your deservingness to remain within the protection of the community. 

The original German is 'der Wille zur Macht' Re 'Wille': I think the original interpretation is correct. Re 'Macht': Neitsche includes domination in his view of power, but he also includes influence and all other ways of affecting the world. Power, is at root, 'potesse' -- being able, and not necessarily oppression. In fact, he notes that oppression is too much directed at others to really be absolute power. In caring that you dominate, you give power to those dominated, whose responses actually determine your happiness. His less didactic books, like 'The Gay Science' and even 'All too Human' have a focus on art as an aspect of life that makes no sense if one thinks of power as domination alone. Artists do not dominate, they influence. And in fact, in his critiques of Wagner, he points out how, when art dominates, it actually has less power, because it loses the ability to influence on a more detailed level. Power aims to make the world exactly as one wishes it. This cannot be accomplished any better with a maul than with a paintbrush if the essence of your vision has a great deal of detail. Nietzsche also has a vision of the unity of opposites that strongly foretells psychoanalysis (thus the need for a focus that questions values, as captured in Beyond Good and Evil). For instance, in The Gay Science, he admires the motive of domination so strong that it can be adequately served by forgiveness, being 'Drunk off' in imagination so deeply that it leaves one overly full, and calls for its opposite. He points out that doing good for others is also a way of controlling their lives. If you are powerful in a way that suits your nature, then, you might be magnanimous, or endlessly motherly, rather than dominant. Re 'zur': Here 'to' means 'for', 'toward' or 'unto'. Although 'zu' is also used with infinitives, like the English 'to', given the article marker and the capitalization 'Macht' here is a noun and not an infinitive. It is more often translated 'for' in such cases, to remove an ambiguity that exists in English and not in German. However, Schopenhauer's 'Wille zum Leben' had already consistenty been translated "Will to Live' rather than "Will for Life", and this naturally followed that compromised construction. 

If you are a strict dualist or an outright idealist, the answer to the last question is 'no', the ideal forms like the ideas in mathematics and logic remain, independent of material reality, and even if all intelligent species went extinct, so there were no thinkers in the world, the idea of a thinker has been established and would always exist. In fact since it ever happened, it was possible, and so it always existed even before there were any thinkers... The answer to the first question, then, also has to be 'no'. From an idealist point of view there are underlying forms for perfect concepts, even if we will never meet one that has not been compromised by the interference of a culture. Of course, there is a strong trend away from idealism in modern thought, because we keep finding our investment in material reality pays off in ways we did not expect. And that makes the concept of a realm separate from it seem less and less worthwhile or logical. I tend to favor the compromise form of pantheism, a la Hegel, which suggests that intelligence is part of the structure of the world in such a way that if all thinking beings were eradicated other thinking beings would arise. That position is neither idealist, nor anti-idealist, but is largely opposed to 'substance' dualism. It presupposes that information is an aspect of material reality and that complex enough matter will always develop intelligence, because of the nature of time and resources. You can look at that inclination as an extension of Darwinism, or an assertion that intelligence is more basic than matter. The difference is largely without meaning. In that framing, your answer is ambiguous. The knowledge may be gone, but any truly important part of it will return whenever it is lost. So it is lost? And if any important idea returns constantly in various forms, there is surely something behind the various forms making them versions of the same thing. But that 'something' can never be expressed or embodied except through being incorporated into beings with a culture. So it exists through cultures and across cultures, but is never observed independent of a culture. 

The problem is with the whole class of verbs, and not with the quantifier. But the rigidity of the other quantifiers makes the ambiguity irrelevant. "Some dogs are not black" could theoretically mean there are dogs that might not be black or there are dogs that are non-black. But logically these have the same result: We cannot count on all dogs being black. It does not matter whether we know there are actual exceptions, or we only do not know whether-or-not there are exceptions. "No dogs are not black" (besides just sounding stupid) could theoretically mean that there are no dogs that are not necessarily black or that there are no dogs that are non-black. Again, the more definite pronouncement swallows the less definite one logically, because the impossibility of a non-black dog rules out the impossibility of a possibly non-black dog, and vice-versus. "All dogs are not black" does not resolve this for us. The idea of dogs in general being actually non-black, is not related in the same way to that of dogs in general being possibly non-black. The one gets us only non-black dogs, the other just some non-black dogs. These two ideas are much more independent. In this way, all is the 'weakest' or 'most open' quantifier, which is what leads to its use as the default quantifier in open constructions. So of the common quantifiers, only 'all' really shows the weakness of the construction inside it very often. Since it does so, however, you should take your author's advice and work around using it in any canonical form. Beyond that, it pays off, in general, to incorporate negation into an adjectival form (non-A) or resolve it with deMorgan's laws or exclusive constructions (relying on unless, except, only, etc. for clarification) and then pull the negations back out of their embeddings at the time a representation is formulated, after any processing of the language has taken place.