During work hours, take a look at sys.dm_exec_requests and see what the column says. This will tell you what the requests are waiting for. Right before working hours, you could run and then look at sys.dm_os_wait_stats to see the accumulated stats during the day. Do you have indexes defined on any of these tables? If not, then table scans might be clogging your I/O system, and locks might be causing some blocking. If people are running SQL Profiler traces during work hours, those could be slowing down the entire system. You could detect those by quering sys.traces. Running a careful profiler trace yourself might reveal some interesting facts about this query. Try capturing a Showplan XML Statistics Profile event when it runs. Maybe the query processor is choosing a very bad query plan. Maybe the plan is generating intermediate tables that have a ridiculous number of records, or maybe there is a bad nested loop that would work better with an index or with a merge join. Since this is an external server, another possibility is that you are simply overwhelming network bandwidth during working hours. You mentioned in a comment that there was a lot of data coming back. Adding some filters in the query might help-- if that is an acceptable solution for your application. 

If you really want to limit SQL Server memory, look at the Maximum server memory option. Getting memory usage is possible, but it depends on what you really want. Do you want to see memory usage as a percent of the "Maximum server memory" option? If so, look at sys.dm_os_process_memory: 

I am not familiar enough with extended events to know if they would work better-- event pairing, maybe? 

This table has an column on it. I expected that when I selected the values from the table by this monotonically increasing primary key, I would see the timestamps in the same order, too. The timestamps might not be sequential, because there might have been other updates, but they would at least be in order. However, what I am seeing is different. The inserts are interleaving by primary key, but the timestamps are sequential by thread. 

I found the problem. My login had a different default database than the database. When I changed my default database to , the error went away, and I was able to Watch Live Data on the extended event sessions. To change the default database, in SSMS, I expanded the server, Security, Logins. I right-clicked on my user credentials. On the Login Properties page for my user, I changed "Default database" to . The error went away when I closed and reopened the connection in the SSMS Object Explorer. 

Both processes are running the same UPDATE statement to set a flag on this tempdb table. This tempdb table holds information that needs to persist between client calls until the client is done. The table has a fairly long index that starts with a GUID representing a unique process ID. I am having difficulty understanding and simulating this deadlock. I have tried various amounts of records with simulated data. My questions: Why are these processes acquiring U locks and then converting to IX? I would expect the DELETE to acquire IX locks to begin with. How can I prevent the deadlock? The statement causing the deadlock is below. The process has just done a lookup of costs for a list of items at a single store. It is trying to note that there was a cost found. Note that there is a deprecated (NOLOCK) on an UPDATE statement. Would this be a contributing factor? 

The second thread is running identical code, except that it is doing from the function to insert its thread ID. First, a word about the function. This uses a series of cross-joined common table expressions with a ROW_NUMBER() to return a lot of numbers in sequence very quickly. I learned this trick from an article by Itzik Ben-Gan, so credit goes to him for it. I don't think the implementation of the function matters, but I will include it anyway: 

We have a process that generates an inventory report. On the client side, the process splits of a configurable number of worker threads to build a chunk of data for the report that corresponds to one store out of many (potentially thousands, typically dozens). Each worker thread calls a web service that executes a stored procedure. The database process for processing each chunk gathers a bunch of data into a #Temporary table. At the end of each processing chunk, the data is written to a permanent table in tempdb. Finally, at the end of the process, one thread on the client side requests all the data from the permanent tempdb table. The more users that run this report, the slower it gets. I analyzed the activity in the database. At one point, I saw 35 separate requests all blocked at one point in the process. All these SPIDs had on the order of 50 ms waits of type on resource . One SPID has this resource, and all the others are blocking. I did not find anything about this wait resource on a web search. The table in tempdb that we are using does have an column. Are these SPIDs waiting for the IDENTITY column? What methods could we use to reduce or eliminate the blocking? The server is part of a cluster. The server is running 64-bit SQL Server 2012 Standard Edition SP1 on 64-bit Windows 2008 R2 Enterprise. The server has 64 GB RAM and 48 processors, but the database can only use 16 because it is the standard edition. (Note that I'm not thrilled by the design of using a permanent table in tempdb to hold all this data. Changing that would be an interesting technical and political challenge, but I'm open to suggestions.) UPDATE 4/23/2013 We've opened a support case with Microsoft. I'll keep this question updated as we learn more. UPDATE 5/10/2013 The SQL Server support engineer agreed that the waits were caused by the IDENTITY column. Removing the IDENTITY eliminated the waits. We could not duplicate the issue on SQL 2008 R2; it occurred only on SQL 2012. 

I have tried this with my own custom event sessions. I can't watch the live data on them, either. I can query the system_health ring buffer target data from . Why can't I watch live data for any extended events session? (Note that this is also a Microsoft Connect item closed as Not Reproducible.) 

I don't think you can get the CPU usage unless you have the Resource Governor enabled. If you do, look at sys.dm_os_performance_counters: 

I have a SQL script that I run when I want to see what is happening in the database. The script has many queries that return information from DMVs, but the two I use most are "Requests" (sys.dm_exec_requests, sys.dm_exec_sessions, etc.) and "Locks" (sys.dm_tran_locks). The output is similar to SQL Server Activity Monitor, but it displays more information. Sometimes, a request appears in the Requests query, but it completes before the Locks query runs. For example, the Requests query may show SPID 51 is waiting on a lock resource, but the Locks query does not include any lock information for SPID 51. (I know about the and columns from .) Is there a way to ensure that these two separate queries display a coherent snapshot of database activity? I expect commercial database monitoring applications must encounter the same problem. I have experimented with running these queries in SERIALIZABLE concurrency and added locking hints to the DMV joins, but the queries acquire no locks. I would not want to do this in production anyway. The best ideas I have so far are: 

These queries were optimized and ran exactly as I expected-- get the #Keys NULL value first and seek to Order_Details_Taxes. They are the last queries in the query plan linked. Why do the queries in which I used a @Table variable perform index and table scans on this large table, when I am joining using from a table that has a single NULL value to a table with only NULLs in this key value? I assume the answer is statistics and/or cardinality limitations of @Table variables, but the resulting query plan was non-intuitive to me. is on for this table and my SQL session.