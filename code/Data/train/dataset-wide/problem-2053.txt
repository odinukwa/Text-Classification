I am trying to use sp_publication_validation stored procedure to validate that my replication setup is working correctly. This procedure is pretty straightforward and is described here. I am running this command: 

You can check the execution plan of the queries and verify that this index is actually being leveraged. The index helps, but since I have to make changes to dozens of publications at once, it still was not enough. So the second part of the fix was to simply introduce a random delay. I added up to 3 min delay to my deployment script and this scattered the queries enough to significantly lower probability of deadlocks (but not eliminate it completely :( Something like this: 

Obviously it's still looking for the path specified earlier, but why? Again, I set "Delay Validation" = True on all my connection managers and the package itself. I appreciate any help on this. Thank you! P.S. Here is a complete expression for log path: @[User::Log_Path] + "\\" + @[System::PackageName] + "_" + (DT_STR, 4, 1252)DATEPART("yyyy", @[System::ContainerStartTime]) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("mm", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("dd", @[System::ContainerStartTime]), 2) + "_" + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("hh", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("mi", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("ss", @[System::ContainerStartTime]), 2) + ".txt" 

If you were profiling at the statement level to understand the performance of the function, then the profiler would have had severely affected the process of the system. SQL Server applies the function to every row. If you have 60,000 rows, then you will have 60,000 invocations to run through. That is a lot of profiling data. Our application uses functions heavily. We have learned through experience to exclude functions from profiler traces. Any time we profile functions, the performance of the entire system slows to a crawl. 

Run these queries simultaneously from different sessions. Join requests and locks together in one query. Considering that I have seen over 100,000 locks at once, this join would return a lot of duplicate request and session data, but it might work. 

I am running SQL Server 2014 Developer Edition on my PC. I am trying to view the data in the system_health session. In SSMS, I have connected to the database, expanded the server / Management / Extended Events / Sessions. I see AlwaysON_health (stopped) and system_health (running). When I right-click on the system_health session, I get the following error: 

In a two node cluster there is no difference. There is only one other node to go to so it doesn't make much sense. Now, assume you had an X node cluster where X is > 2 and you're still using SQL FCI's (Note: AGs work differently, don't check or uncheck these values). Now, if you choose "Move service to best possible..." it'll go to a preferred first, additionally if there were a fail over it'd move to a preferred first as well. Since it's all about preference and not about actual possible owners, having a preference set is unlikely going to really change anything, in my honest opinion. Some will argue about fail back, which I don't like to enable and requires there to be a cluster event for failback to occur anyway... so it's a wash. 

I started two threads running inserts into this table at the same time. The first thread is running the following code: 

I wanted to share my experience with trace flag 4199. I just finished diagnosing a performance issue on a customer system running SQL Server 2012 SP3. The customer was moving a reporting database away from their production OLTP server onto a new server. The customer's goal was to remove competition for resources with the OLTP queries. Unfortunately, the customer said the new reporting server was very slow. A sample query run on the OLTP system completed in 1.6 seconds. The query plan did an index seek on a ~200 million row table that was part of a view. On the new server, the same query completed in 10 minutes 44 seconds. It performed an index scan on the same ~200 million row table. The reporting server data was a copy of the OLTP data, so it did not appear to be a difference in the data. I was stumped until I recalled that our software (which runs their OLTP system) enabled some trace flags on startup. One of them, 4199, I recalled was a query optimizer fix. I tested enabling trace flag 4199 on the customer's new reporting server, and the reporting query completed in 0.6 seconds. (Wow!) I disabled the trace flag, and the query was back to completing in 10 min 44 sec. Enabled the flag: back to 0.6 seconds. Apparently, enabling the trace flag enabled the optimizer to use an index seek into the view on the 200 million row table. In this case, the query optimizations enabled by trace flag 4199 made an enormous difference. Your experiences may vary. However, based on this experience, it definitely seems worth enabling to me. 

This is completely possible. In windows clustering, it's a shared nothing philosophy so this will require at minimum two separate instances. 

The best practice for scale would be to put SQL on its' own server. If that's too pricey, it would be possible to put it on the same server as your web services... this brings up a few risks (security, performance, uptime). 

Yes, it is possible but will require SQL Server 2017 (I'd also apply the latest CU, which at time of posting is CU4). Please note that without a domain, SQL Authentication is the only available authentication type and there is also the additional endpoint setup of using certificates. If you'd like to read more about read-scale availability groups (Not for HA or DR) the Docs articles are a good start. I've also blogged about it when it comes to things like read only routing. 

For my next test, I now would like to change the location of the log files, so I modify the value of the [User::Log_Path] variable in the parent package to "C:\temp\Log_Run_Time". If I run now, I will have four log files! Two for the parent package in the "Log_Run_Time" directory and two for the child package - one in "Log_Design_Time" and one in "Log_Run_Time" directory: 

Initially everything seems to be ok and the results indicate that all of the tables match. Next I go over to the subscriber and manually delete a few rows from some of the tables. I manually verify that the row counts in my tables are now different between the publisher and the subscriber. Finally I run the sp_publication_validation procedure again and .... it says that everything is still OK. This is wrong! I also tried to return both rowcnt and checksum and it still doesn't detect the fact that there are differences between the publisher and subscriber. I appreciate any ideas. Thank you! 

Those error messages, much as @Nic pointed out, are not really error messages in the log or in a DMV but are surfaced up through policy based management which runs as part of the AlwaysOn Dashboard. If you wanted to see how this works, we can open up one of the PBM system policies for Availability Groups. 

Make sure there are no other logreader agents running for the same publishing database. Second part: 

This isn't saying the default file locations need to be the same, this is saying that you don't have identical paths for all files in the database(s) on the secondary server. Example: 

You've failed over to asynchronous nodes. This means all of the database flows are paused and there is no current way (assuming it's a true disaster) how far behind your secondary replicas were. Now that they've come back up, we know that it isn't going to be 100% the same data (they are asynchronous). This was not mentioned in the question but I'm going to add it to the answer as it's extremely important as it's part of your SLA. 

Once again, thank you to Vladimir for providing the formula! The correct way to accomplish what I need is this: 

I think I’ve figured it out (at least partially). It seems that in order to avoid creation of duplicate log file for the child package the value of “Log_Path” variable in the “child” package have to be made blank. If there is no value, the validation process will not create an extra file and will properly inherit value specified in the “parent”. This still doesn’t fully resolve the issue with the “parent” package, because I can’t run it from the development environment without any value specified for the “Log_Path” variable. The only way I found around that is to make it blank, save it and then execute it from the command line (DTExec) while passing the desired variable value via SET option. This finally results in just two files instead of four. I still don’t understand why validation process (at least I think it’s validation process) creates those “extra” files using design-time values. This just seems like a wrong behavior. 

Model is set to autogrow by 1 MB, so it's 2.2 MB right now... I'm going to guess at 2 MB it grew by 1 MB. The size on disk and the metadata sizes aren't going to match up unless you have a database that is completely and utterly devoid of free space. The log has a size it can't be smaller than, which you're at right now. 

If your entire intranet relied on all 50 databases being on the same instance, then yes I'd put them all together... whether or not you want the AG to failover if a single database (might be the most important one or the least important one) has a state change. I can't make that business decision for you. I also can't change the need to have all 50 together. 

As pointed out by sp_BlitzErik (thank you!), the problem here was the sampling rate was too low for the billion row transaction table. The more rows sampled by the statistics update, the more accurate the estimate, but the longer it takes for SQL Server to generate the statistics. There is a tradeoff between database maintenance time and statistics accuracy. A simple `update statistics MyTable' was sampling 0.13% of the rows. This generated a very poor estimate for the "EQ_ROWS" column that was usually hundreds of times worse than the actual row counts. For example, some EQ_ROWS estimates were over 60,000 when the actual counts were about 150. On the other hand, the default stats update completed in 25 seconds-- pretty fast for a billion rows. A full scan generated perfect EQ_ROWS statistics. However, the full scan required about 5 hours to complete. That would not be acceptable for the customer's database maintenance window. Below is the chart of samples vs. accuracy. We're going to recommend the customer adjusts the sample size manually for the largest tables in the system. I expect we'll go with a sample size of about 10%. The Avg Stat Diff column is calculated as ABS(EQ_ROWS - ActualRowCount). 

This is not accurate at all. Ther, in fact, is GREATER latency is getting the transaction blocks hardened on the asynchronous commit replicas it's just that we don't wait for them to be hardened like we do with synchronous. Thus, there is still a delay (your network doesn't magically become a non-wait environment) it's just not counted in that specific counter as that counter ONLY contains information for synchronous commit replicas and transactions. 

If you absolutely want to do this then I would advise you use a 3rd party product that does this for you. These undocumented functions are just that, along with the log structure. 

It's a differential, so it only holds what little log it needs to be transactionally consistent. It wouldn't have to be that, as that's not the firstLSN where it is starting recovery. This differential could be from a full backup base taken 1.5 years ago (please, please don't let it go that long between fulls) and would not have any LSNs in most likely the last 1.5 years in it. It'll have all of the changed extents, but no log transactions (again, it'll have a tiny amount to be consistent). It's just telling you that it's going to start it's recovery process at that LSN.