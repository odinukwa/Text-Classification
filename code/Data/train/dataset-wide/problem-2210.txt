So expressions in the SELECT clause will not use the expression index. Using a covering index is not as much an improvement over using a normal index for searching/sorting as using a normal index would be over using no index at all, so this optimization has not (yet?) been implemented for expression indexes. 

When there is an index, both queries just take the first entry from the index. When there is no index, MIN() does a single pass through the table, and ORDER BY sorts only as much as is needed for the LIMIT, which ends up the same effort. (Older versions of SQLite did sort everything.) 

This is not valid UTF-8. That database was not created correctly. If you know what the characters are supposed to be, you could manually fix them: 

The index on is no longer needed because the three-column index can be used for any lookups on this column. The index on might be useful if you will do lookups on this column only. The index on is always worthless: a query that searches for one of the 26 possible values is faster if it just scans the entire table. 

Your second approach is not normalized: adding another sensor would require creating a new table, and querying multiple sensors would be horribly complex. When all four sensors get a new value every second, and if you have multiple tables, then the database would have to update four tables. A single table is certainly more efficient. For fast queries on time ranges, you need an index on the column. This implies that every INSERT also needs to update the index. (If you want fast queries on time ranges for a single sensor, you need another index that contains both the and columns.) Please note that you do not need AUTOINCREMENT when you never delete rows. (Autoincrement In SQLite) 

It is not possible to have 'hidden' columns. When you are using GUIDs to avoid merge conflicts, you must use GUID keys in every table that contains data to be merged. And when you already have a GUID, it does not make sense to add another column to the primary key. If you do not want to use GUIDs, just deal with the conflicts, i.e., instead of blindly copying the records, map all old IDs to new ID values. 

In SQLite, table names always are case-insensitive (even when quoted). In PostgreSQL, unquoted identifiers are folded to lower case, but then the search for the table is done case-sensitively. So the only way to get the same behaviour as in SQLite queries is to use unquoted names, which implies that the actual names must be lower case. In other words: the table name must be in order for a query like to work correctly. 

(If you know that there is only one Chris Jackson, use instead of .) Now you want all movies that do not have such a review: 

This is not valid JSON; strings require double quotes. Anyway, the table-valued function json_each() allows to enumerate a dynamically-sized array: 

The WAL file records all changes to the database since the last checkpoint. (Automatic) checkpoints cannot run when the database is locked, so ensure that there is no other connection with an active transaction. The documentation says: 

The documentation page you linked to is for the current version (3.8.9) of SQLite. You could dig up the old version of for 3.7.13, but at that time, was pretty much undocumented. 

In the command-line tool, dot commands are not SQL commands, so you cannot mix them with queries in the same line (and you should not delimit them with a semicolon). can read commands from its standard input, so you can simply use a here document to give it multiple lines: 

SQLite is designed as an embedded database that is accessed from a 'real' programming language, so it has no built-in mechanism for dynamic SQL. You have to do the column name lookup separately: 

returns the of the most recently executed statement in the same connection, which is not what you want. You have to use the ID of the setting that you want to refer to. Also, as documented, the column needs to be the primary key, or have a UNIQUE constraint. 

SQLFiddle When being executed, this query does not require a temporary table (your query does for the result of ), but will execute the subquery for each record in , so should be indexed. (Or use an index on both and to get a covering index.) 

The database will return the value from some arbitrary row in the group. (In the current implementation, it's the value from that last row in the group that was processed, but due to indexes or other optimizations, the processing order might not be predictable.) Since SQLite 3.7.11, using MIN() or MAX() guarantees that values from a row with the minimum/maximum value are returned. 

SQLite can resize field values and records dynamically, so there are no restrictions on updating strings (except for the normal limits). 

You are searching rows whose value is different from the value in the previous row, where "previous" means the row with the same sensor ID, and with the largest date that is smaller than the current row's date. Of all these changes, you want the latest one for each sensor: 

In a trigger body, the result of a SELECT statement is simply thrown away. Using a SELECT statement makes sense only for its side effects, i.e., when using a user-defined function, or RAISE(). 

SQLite's C API has functions to create custom collations. SQLiteStudio allows you to create such collations in any of its supported scripting languages. 

In the SQLite record format, both and the integer need only a type code and no storage for the actual value itself. In any case, you should use whatever value works best with your queries, and optimize only when needed. 

The Unix epoch timestamp is defined as the number of seconds since the beginning of 1970 (in UTC). These timestamps are measured from the beginning of 2001, and appear to use the respective local time zone: 

Multiple ranges lookups cannot be optimized with normal (B-tree) indexes. You have to create an R-tree index for your coordinates. 

With the default settings, when rows (or entire tables) are deleted, SQLite does not remove empty pages from the database file. (This setting can be changed with PRAGMA auto_vacuum.) In most cases, keeping empty pages in the database file is good idea because they will be reused by new rows that are inserted later, and adjusting the database file to remove those pages, only to reallocate them later, would be unneeded overhead. 

If most records will show up in the result, then it would be more efficient to go through in the proper order and look up correspondig records. This would require the following indexes: 

This query is likely to read the rows from the table in index order (i.e., random order), so if there is a large number of matching rows, performance will be worse. 

If computing the count dynamically is too slow, you could try to create a temporary table that contains the data in a format that is easier to count: 

So ensure that all database connections are closed at some time. (If you have disabled deleting WAL files with , you have to set to a nonnegative value to truncate the WAL file to zero bytes in that situation.) 

They just properly normalize their database so that it is possible to store an arbitrary number of numbers; something like this: 

You should create a user-defined function that sleeps for the desired amount of time. If you are restricted to build-in mechanisms, try using a recursive common table expression: 

When the order of pages in the file has no relation to the order of pages on the disk, reordering the file does not make sense. 

The storage size has no effect at the SQL language level, so there is no SQL function to determine it. However, the record format is documented; SQLite always uses the smallest serial type into which the value fits. 

In SQLite 3.8.3 or later, you can use a recursive common table expression to generate the missing dates: 

To check which indexes are actually being used, use EXPLAIN QUERY PLAN. Whenever you change tables, any indexes must be updated too. When you're filling an empty table, it is usually a better idea to create indexes afterwards. When your application does many updates to some table, it might be worthwhile omitting some index(es) that are useful only for seldomly-used queries. 

Instead of using constants, these values can be read directly from the table. To look up the latest matching risk value, we can use a subquery: 

The problem is that you are using table names (). This is not allowed; just use instead. Anyway, the entire statement can be simplified: 

The number of polynomials is the number of distinct values. (You probably have a separate table for graphs; in this case, the subqueries can be replaced with .) To count unique polynomials, we exclude any that are duplicates. A polynomial is a duplicate if there exists any other polynomial with a smaller ID (the smallest ID would be the non-duplicate) and with the same coefficients. Two polynomials have the same coefficients if there are not any differences in the possible x/y/coefficient combinations for both, i.e., for each row of one polynomial, the same row must exist for the other polynomial. In other words, there must not exist any row that does not have a match for the other polynomial. And now that we have the description in the language of set theory, we can translate it directly into SQL. (I've used compound SELECTs for the innermost comparisons to avoid yet another level of negated subquery lookups.) 

In theory, there's nothing wrong with having the rowid as a second column in an index. And in practice, with the current SQLite version, it works as you expected it to: 

Your query is too complex; the join between and is not necessary because the movie ID is already directly available from the subquery. Anyway, using subqueries instead of joins usually is easer: First, these are all reviews by anyone named Chris Jackson: 

To limit the number of rows returned from a (sub)query, add a LIMIT clause. But this is not needed here because in SQLite, scalar subqueries have an implicit LIMIT 1. Column values from are not available inside because that is an independent table. You have to replace the outer join with correlated subqueries, one for each column: 

In SQLite, joins are executed as nested loop joins, i.e., the database goes through one table, and for each row, searches matching rows from the other table. If there is an index, the database can look up any matches in the index quickly, and then go to the corresponding table row to get the values of any other columns that are needed. In this case, there are three possible indexes. Without any statistical information (which would be created by running ANALYZE), the database chooses the smallest one, to reduce I/O. However, the index is useless because it does not greatly reduce the number of table rows that need to be fetched; and the additional step through the index actually increases the I/O needed because the table rows are no longer read in order, but randomly. If there is no index, the lookup of matching rows would require a complete table scan of the second table for each row of the first table. This would be so bad that the database estimates that it is worthwhile to create and then drop a temporary index just for this query. This temporary ("AUTOMATIC") index is created on all colunms used for the search. The COUNT(*) operation does not need values from any other columns, so this index happens to be a covering index, which means that it is not necessary to actually look up the table row corresponding to an index entry, which saves even more I/O. To speed up this query, create this index permanently, so that it is no longer necessary to construct a temporary one: 

In SQL, are used for string values, while are used for table/column names. SQLite allows all types of quotes for compatibility with other databases, but it uses the correct interpretation first, if possible. So in the subquery , it will try to use a column named , and there is one. In other words, the first query is interpreted as: 

The output of your CTE is inconsistent: , , and have different numbers of band members. You should go through the pairs in both directions (and then use UNION to prevent duplicates and infinite loops): 

An index can be used to optimize the GROUP BY, but if the ORDER BY uses different columns, the sorting cannot use an index (because an index would help only if the database would be able to read the rows from the table in the sort order). A COLLATE NOCASE index does not help if you use a different collation in the query. Add a 'normal' index, or use , if that is allowed. Using the OFFSET clause for pagination is not very efficient, because the database still has to group and sort all rows before it can begin stepping over them. Better use a scrolling cursor. Note: there is no guarantee that the and values come from any specific row; SQLite allows non-aggregated columns in an aggregated query only for bug compatibility with MySQL. 

SQLite allows to insert multiple rows with one statment, but only in version 3.7.11 or later. Apparently, your JDBC SQLite driver is several years out of date. 

SQLiteStudio sorts the database objects by type (table, index, trigger, view), but inside each group, sorts them randomly. (If you consider this a bug, report it.) The shell dumps tables in the order in which they are stored in the table, which is typically the same order in which they were (re)created. 

(Having in the second index optimizes for this particular query, but might be not worth the storage and update overhead if you have many other queries.) 

The error message "foreign key mismatch" does not indicate a constraint violation, but that your database schema is wrong. Assuming that the table and the column actually exist, the most likely reason is that the required index is missing, i.e., that is not the primary key (or at least unique). 

The effects of a trigger 'belong' to the statement that triggered it, so everything will be rolled back together (with or without explicit transactions): 

As you have seen, a simple GROUP BY will not work because it would return only one record per group. Your join works fine. For a large table, it will be efficient only if there is an index on the join columns ( and ). Alternatively, you could use a correlated subquery: 

SQLite automatically chooses the estimated optimal join order; the table order in the query has no effect. There are two ways to optimize this query. If the filter on removes most records from the result, then it would be fastest to look up records with matching values first, then to look up the corresponding records, and then to sort the result. This would require the following indexes: 

subtracts two numbers. If the value in the column is not a number, it is converted. From what you've shown, converting the values in the column (which begin with the characters ) to a number results in the value 2015; subtracting 1800 from that results in 215. This is probably not what you want. To group by hours offset by half an hour, just add half an hour to the timestamp, then drop the minutes/seconds: 

(The subquery just refers to the value in the current row of the outer query; this is a rather pointless instance of a correlated subquery.) 

Besides the overview shown by , the is no more information in the shell. You have to use the on-line documentation of the shell or the SQL language. 

ODBC as just an interface between applications and drivers. It is impossible to create an ODBC connection without using some installed driver. 

The expression is the same as , so the database assumes that all values except 0 are searched for, and estimates that most rows will match. If the values in the column are proper booleans, you can get the database to assume a higher selectivity by searching for the value you actually want to search for: 

The and files are used to store data when the DB is in WAL mode. SQLite does not require any specific file name or extension. These file names are chosen by the program that created the database. 

The subquery does not depend on any value from the table, so it needs to be evaluated only once. However, this computation does not need to be a subquery. Just remove the parentheses and the SELECT: 

Now you have all possible combinations of leaders and band members, but you want only one (random but unique) leader per band. So for all rows with the same band member, choose the one with the smallest leader: