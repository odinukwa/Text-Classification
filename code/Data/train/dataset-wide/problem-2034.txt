And grant the permissions to that. Then other services running under the same service account won't be able to connect to SQL Server. 

If you have an MSDN license you can download and install SQL Server Standard Edition for Dev/Test. Otherwise you can use an Azure Pay-As-You-Go instance for testing. See eg SQL Server 2017 Standard on Windows Server 2016 

Partition elimination is not possible here because the partition metadata does not have the information about whether there are any rows for that DataLoggerID in the head partition. Then the QO isn't smart enough to know to scan and sort the partitions in reverse order and stop when it finds a row. You could produce that outcome with a rewrite of the query, but you may have to run one query per partition. 

It may be that with enough joins and subqueries you could get a bad plan. But it's not the first place to look. Use the Query Store (SQL 2016+), or the plan cache DMVs, or XEvents to see the query plans and associated resource costs. For instance on AdventureWorksDW, these two queries use the same plan. 

The main issue you are seeing is based on the nature of the encryption. The Delphi application appears to not be using any native MySQL encryption/decryption mechanisms for the data. This will result in certain disadvantages when querying for data. Although the decrypted data is visible to the application while in use, native to MySQL, it has no what of indexing any encrypted data fields in any optimized way as the values would be unknown. This would result in long query times as most likely all queries would be scanning the whole table to deliver any results. If you can, use the Delphi application, or similarly developed application to export the data unencrypted into a file, easily parsed by MySQL (csv, SQL statements, etc.) that can be imported into your MySQL 5.7 database instance. This will allow you to create indexes on the unencrypted data to run your queries as necessary. If you need to reencrypt the data, you can use the native functions of MySQL: $URL$ However, you will still be unable to index those columns in any workable way to query on those columns. I would recommend if possible that any queries be done on any columns that can remain unencrypted and therefore be indexed accordingly to optimize the execution plans. 

There is no difference in SQL Server between those two statements. Each results in NOT NULL column (not a constraint) with a single check constraint. The only difference is that second one creates a system-named check constraint, something like 'CK__t__ProjNum__4AB81AF0'. 

The CPUs and RAM are only half the story when it comes to writing. Your ability to Write will be gated by the performance of the disk containing your database log, and by your network connection to the AlwaysOn Secondary (assuming it's synchronous). Consider Memory-Optimized Tables which are designed specifically to scale in write-intensive scenarios on big-memory multi-core machines by 1) minimizing logging, 2) eliminating locking and page latching, and 3) enabling native-compilation of TSQL code. Your ability to Read should be largely unaffected by the disks, unless your frequently-read data is significantly larger than the RAM. 

An UPDATE will read with U locks, releasing each U lock immediately if it is not going to X lock and change the row. Profiler will show you this with the Lock:Acquired and Lock:Released events. EG an update on a heap will IX lock the object, then for each page IU lock the page, then for each row U lock the row, and either release the U lock or convert to IX lock on the page, convert to X lock on the row and update it. The page lock will be released when it's read the last row on the page. 

The problem occurs ONLY when I switch default port to another value. When return its value back to 1433 everything goes back to normal. During the execution could capture following waits: 

Finally found an answer: You can create database user, which won't have a SQL Login and won't be able to authenticate, but will have permissions and can be specified in clause for stored procedures and functions. The simple way to create such a user is just to use clause during user creation: 

Might happen you still have some opened transactions, which hold you from shrinking individual files. See who hols them, close them. Reboot server if necessary. Shrink individual files while nobody accessing the database. switch it temporarily to Single User if necessary: 

You create clustered index on only one column. Technically, you can do it on multiple, but the main goal to have it as short as possible. If you created clustered index on you DO NOT NEED any other indexes on that column. I can guess that your performance improved because before there was no Clustered index and indexes were so bad that SQL decided to do full table scan instead. Suggestion: read a book about Indexes, their differences and how they work. 

SQL Server uses CLR internally for several features, and these waits track CLR background threads just sitting around waiting on a ManualResetEvent or a AutoResetEvent. According to High waits on CLR_MANUAL_EVENT and CLR_AUTO_EVENT these are background waits (ie the wait time is not happening to user sessions), and can safely be ignored unless you have unsafe CLR code using these events directly. 

If the content being versioned is not stored as a blob, but is in separate tables, the pattern still holds. All the tables that store the versioned data need the VERSION_ID. It would certainly be an interesting research project to explore all the different ways this can be done, and discuss options for conflict resolution, merging changes and sketch out what a general solution might look like. 

You could pass the data using a Table-Valued Parameter instead. Or you could try something like this: 

You can read about Persisted Computed Columns in BOL, and the related Indexes on Computed Columns. There are restrictions on the expression you can use in a persisted computed column. The Expression "must be deterministic when PERSISTED is specified." 

At first I'd suggest to have group_id and visit_id in INT or at least BIGINT. If that table has only 3 columns it might worth to create not-unique clustered index by group_id and unique constraint on visit_id: 

Try both of them for both of your data sets to see the difference in query cost and amount of I/O using . For instance, when you force for the second data set I/O for table jumps 24 to 215 reads. So, be VERY CAREFUL using any kind of these hints. 

Yes you can. Use SQL $URL$ By doing that you'll have a full control behind the curtains, but please fully document it and be consistent. It is a nightmare to troubleshoot databases with synonyms. As an alternative you can use dynamic SQL, it might be more transparent. 

In your sample case SQL Server made very good choice. Here is what it was actually doing: - In the first set of data it extracts 10 rows from table based on the column. Then it extracts 10 corresponding IDs from table using kind of operation for each ID. - In the second scenario, when there are 100 matching rows in table SQL decided not to do operation, but use instead, which is much cheaper from the I/O perspective. Yes, you can use hints for your queries ($URL$ 

A couple of issues I can see right now. You are trying to join on one column to basically a value between two columns. You are also limiting the subquery to 1 row but you are trying to group by all the customer data so you can get a count of all the duplicates. Your counts, even if this worked would be 1 for everything. You can do this as a subquery, aliased, and with a new column in the select list with the ip address so that it can be joined to the customers table ip_address column. 

This leads to the column typing of the columns you are trying to join on. In customers ip_address is a varchar, but in ip_address_data the fields referenced is an int. I think you should think twice about joining these two tables, as it appears that the ip_address_data might be more broad than specifically to this one user. The core issue is that the select on the ip_address_data will run the function call on all the values in the table for all the data joined in order to generate a value that is suitable to join. You may want to clean the ip_address data more if in fact it is suitable for your needs (see next point). Some ISPs use common IP addresses to represent all the users that subscribe to their service, so you might not be accurately joining the right references. 

Can see it only on my SQL2014 on Win8.1 VM. Error persists even when run query locally as local admin 

You have to be very careful. Assuming your query is not restarting and you have a for that SPID. If you restart SQL Server it won't help, because transaction still would have to be rolled back. The problem with that is following: When you run a transaction in multi-CPU environment with not restricted degree of parallelism it most probably will generate parallel execution plan. Would say your transaction run for 10 minutes on 8 CPUs. When you Killed it, will be processed ONLY by ONE CPU. That means it might take up to 8 times longer to recover. 

However, there are plenty of objects in the database, which were created with credentials of that account. I've tried to run one of these procedures and it was executed successfully. When I've tried to recreate that scenario in my test database it returned me an error: 

You did not get the point of SQL programming. You can have 1000 copies on multiple machines of a script to create/alter an object, but until one of those scripts is executed against a server, that object on that server will be unchanged. Yes, you can have multiple tabs/windows with the same procedure, but for SQL Server all of those are different/competing sessions and whoever is last - wins.On another hand: SSMS. You can have multiple copies of a script of the same object in different tabs, which is very handy - you can try different scenarios just by switching tabs. It is not job of SSMS to track your changes and keep records which script is a "Master Copy". That is YOUR responsibility. However, if you are in a project development, you and your boss can't completely rely only on your discipline. Project manager have to choose which way to do a source control. Here is a not full list of things, which can be done: - Implement Change Data Capture on the server; - Restrict permissions on Dev/Test and allow "playing" only on Dev or personal machine; - Use change tracking/deployment/source control tools, which will keep records of any changes. And do not be surprised with SQL. All developers are experiencing that issue with source control. See it here: $URL$