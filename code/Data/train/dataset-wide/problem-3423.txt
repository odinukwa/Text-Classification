You cannot do that with bare UML, it just does not provide this feature. So you will have to install some kind of server inside the UML guest so it can be accessed from the host. Generally one would go for SAMBA (fairly straightforward to setup) or NFS (harder imo), but there are a multitude of options there, even some esoteric ones like sshfs which are very easy to setup and may just be the ticket for you. 

According to this answer to the same question on the devkit mailing list used by upower, newer versions no longer emit that signal since this handled by systemd. The replacement in systemd-land is logind, which has a signal called PrepareForSleep: "The PrepareForShutdown() resp. PrepareForSleep() signals are sent right before (with the argument True) and after (with the argument False) the system goes down for reboot/poweroff, resp. suspend/hibernate." Here is a simple python script for watching suspend / resume events: 

The Xvnc session and application startup: I would place all this in a script and start it from xinet.d The tricky part is to prevent users from re-connecting to an existing session. This is a unusual requirement since that is one fundamental feature of VNC. You may be able to get away with parsing the output of the Xvnc process and killing it (with the app) when you see a disconnection event. For killing the Xvnc when the app terminates, just wait for the appication to terminate in your script and kill Xvnc if it is still running at that point. 

Are you aware of this root filesystem site? All the filesystems on there were originally developed for use with UML, but they should work with any virtualization solution. Note however that there is no bootloader installed as the images are made of a single loop mounted disk (without a partition table). You can still boot them with kvm using its command line option, for the others you will need to boot another image (recovery cd/image perhaps?) and install the bootloader yourself. Obviously you may have to convert this raw format into whatever format you need (vdi/vmware/..) using the relevant tools. The scripts are included should you want to create the filesystems yourself. 

What you think of for UNC paths are typically for SMB/CIFS aka "Windows file shares", not FTP. You can use a free Samba server to host the files you need and they'll be accessible by a UNC path: Some links for you: $URL$ $URL$ Here is Samba file server setup for Ubuntu: $URL$ 

Is there any way to allow the upload (put) to the default directory, without having to chdir first? I do have flexibility to move these directories around. For example, the sftp chroot dir doesn't have to be in the users home directory, I can even change around the users home dir (but the user must still be able to use authorized_keys to login). Please note: I do understand many SFTP clients, and the command line SFTP client allows for defining a relative path at login. That is out of scope for this question, I'm desiring this config be done server-side and the client simply just needs to login. 

EDIT: I've also tried setting the host definition to a standard ping check, and put in a service definition for the command. The host shows UP but the service is DOWN. Same exact error "Socket timeout after 10 seconds" -- The command line use is nearly instant with the UDP OK return. I am stumped. EDIT 2: I tweaked the debug settings, and have turned logging verbosity up to the highest value (2). It doesn't really tell me much, but it looks like Nagios is interpreting the command as expected... 

At this point I can confirm the blobs are completely deleted from disk and I can no longer call image details (like in step 1 above) so I thought I was done. However, when running: my associated repo still lists (even though there are no images within it)! Obviously it cannot pull or be used, but how can I fully remove that repo from that list now that it has no images associated with it? 

I'm following this guide: $URL$ Which works great! My backups are saving to s3 without issue. However I would love to put an S3 lifecycle policy to purge backups older than 14 days. This is easy enough to do, but there are some other items in the bucket related to gitlab backups (restore instructions, some configs, etc) that I never want to purge. These can be easily skipped by only including backup files via S3 lifecycle policy "prefix" rule which only purge files with a certain prefix in their filename. Great, however the prefix of the gitlab backups using have a dynamically changing prefix. My question: Is there a way to change the file name format of the backups created with ? Or is there a way to alter the Lifecycle policy to take a suffix rather than a prefix? Any suggested alternatives to meet the goal is appreciated as well. 

Probably a silly question, but have you verified that the is also sufficiently high? 4000MB/s would be blindingly fast, that must be the cached speed not the actual disk speed. 

I have to say that this list here is helpful, if a bit confusing: it includes low level protocols (like NX, VNC, and now xpra) as well as high level wrappers (like neatx, freenx, and now winswitch). Also it points to some VNC implementations, but not the more recent TigerVNC fork... 

You can run it before or after calling maildrop, important note: the script will receive the email via stdin, so you will have to buffer it if you intend to pass it to maildrop later (which the example above does not do). I would probably recommend using perl for this as there are more mail handling libraries there than in bare shell. Edit: If you only want to do this for a single user then this is overkill, have a look at maildrop - in particular the section about "external commands" via backticks. Define a maildroprc for this user with the rules required. 

When I have had errors like yours they were normally fixed by replacing the drives (even though smart did not report errors - it's not always 100% accurate and I prefer to be safe). However, since this is a recurrent problem, you should consider the possibility that it is the cables (already changed so probably not) or the controller (try to add a PCI/PCIe controller and see if that helps?). Maybe upgrading the OS kernel would help too if interrupts get lost because of buggy chipset support. 

If advanced routing is a bit difficult for you then I would not recommend this approach. The problem is that by default each dhcp lease will claim the default route and also update the list of dns servers. Then there is the firewall issue: how to detect when the it is down, etc Buying a cheap and easy to configure router might just be better for you. 

You may want to look at Virtual GL Quote: "VirtualGL is an open source package which gives any Unix or Linux remote display software the ability to run OpenGL applications with full 3D hardware acceleration"