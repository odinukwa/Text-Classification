This will mark all the MSDTC transactions as failed and allow you to start the database. It's a better idea to use backup/restore to refresh the database though 

You are stuck there. There is no option to re-size the files during restore. The backup is made by dumping the data pages directly to the backup device. There is the option of exporting/importing the data using SSIS but that takes time and it would probably be cheaper to buy a new larger hard drive and take the test server down and exchange it's drive. 

Start by patching the operating system. Install the latest windows service pack and all updates on the secondary node rebooting as necessary. When the secondary is fully patched then failover the SQL Server to the secondary and if everthing works then start updateing the primary node until both are fully patched. For the SQL Server you need to install a service pack and failover between both hosts before you install a cumulative update, and you do it the same way. Install service pack on secondary/failover, update primary/failover. Install CU on secondary/failover, install CU on primary and celebrate. So the whole thing will take you at least 6 failovers of the sql service, but the good thing about doing the rolling updates is that you can patch the inactive servers during the day and the failover time is really short so you can schedule that early in the morning or late at night by which you prefer. I would also look into updating the server's firmware and drivers while you're at it, check with the server admins (or the documentation), some server like the drivers to be updated before you update the firmware. 

If you are complying with all the RPO and RTO requierments by having database mirroring in place, with daily full backups then there are no data consistency issues with making backups to NUL. So the answer to your question is No. But (there is always a but) you are not having a secondary restore path if a full backup is corrupted and both the servers go down and your restore options are always the point in time when the daily backup is made. By making normal log backups and storing them for 48 hours you can restore past a single corrupted full backup which is not possible to do with differential backups. 

Updated.. The localization is different between the SQL Engine and the window Server as @srutzky points out: 

Yes a partial backup is a backup of the primary datafile and all read/write filegroups in the database This is an online restore, you will need the enterprise edition to do this. But this will function on the dev edition as well You can use the simple recovery model for this and you will not need to do any exercises changing the recovery model, just change your current backups to be partial and go though all the possible scenarios. 

If this is not possible you can override the time when the database is in simple recovery mode by making differential backups 

Yes, if you are using Peer2Peer replication between the primary and secondary servers which is an enterprise only feature I would personally use Mirroring/AlwaysOn for redundancy between primary and secondary sites as this includes ways to failover without changing the DNS records. Either by adding ';Failover Partner=PartnerServerName;' to the connection string or by using Availability groups with multisite failover Mirroring or AlwaysOn, depending on your SQL Server version - or Peer2Peer replication. Using Mirroring or Availability groups this will in most cases simply by achieved by bringing the server online and the service will automatically send all the changes over and then you can manually failover. 

Which creates a linked server named db_instance2 and tells the server to use the logged on user credentials to connect to the linked server. If you have configured Kerberos correctly for DB_INSTANCE1 users with permissions on both the servers will be able to select data from the other server: As stated before, for this to work Kerberos has to work and therefore you will need to setup SPN's and do some active directory magic. The Kerberos configuration manager will help you and your AD administrator to set up everything needed It also means that users that connect with database logins will not be able to connect to the server - For that you will have to create user mappings either in the GUI or using TSQL 

or a primary key on VeryRandomText you would get a scan of that index. See books online or here: $URL$ 

This is an operating system error, reported to the SQL Server service. The SQL Server service account, which creates the files, needs to have at least change permissions on the d:\data folder and it needs to exist. 

Yes, you might have an exclusive rowlock or page lock on different pages or rows. In this case you have a key lock on two different parts of the index. See lock granuality $URL$ 

You can create a 3 node cluster. But in SQL Server terminology active/active clustering means that you can run separate instances on any of the cluster nodes which then can fail back to any of the other nodes as clustering is for redundancy or high availability and nothing is shared between the database services. You can use Always On the create read only replicas of the data which can be used to take some read only load of the server. All of this is explained in details in the High availability whitepaper Now if you want or need to create a distributed load balanced setup you can try to implement peer to Peer transactional replication replication, you can create multiple distributed databases, using in memory objects in SQL Server to handle the transactional load and offload the data to other objects/databases. But the only transactional RDBMS that can, out of the box, run a single database in shared everything mode on multiple hosts is Oracle RAC (or maybe ScaleDB) and that comes with it's own set of problems. 

Just leave it at 10GB, it will grow on you. If you absolutely need to shrink the datafile for the configuration database then don't go to the smallest possible option but leave some headroom. 

If something is still setup on the secondary server you need to run there. You can clear the log shipping configuration on the primary by executing 

Best practises state that the optimal storage configuration is to have at least three volumes: One for the OS, another for the Database files and the third for transaction logs and to that you should add a fourth one for tempdb. Sometimes you just don't live in an optional world though. If you cant possibly get a third pair of disks, I would recommend that you benchmark different configuration using SQLIO. In your case you might be able to squeeze more I/O from the raid controller, without sacrificing redundancy by setting all four disks up as a single volume using RAID 1+0 and create four partitions on that for the os, data, log and tempdb. Only a benchmark will tell. 

You can install both on the same instance, If the performance is an issue you can use the resource govenor to make sure that the web database gets the lion's share of the resouces. Security wise I would reccomend to separate the two by using different instances but if you are careful in setting up security this should be no issue. 

There are many pitfalls. which are well explained here, as you are on 2012 you will want to look into executing the remote query with results set. some version of this. 

If you can setup a linked server to the source database server you can simply do a restore from the last backup device containing a full database backup by finding the last backup file like this: 

The files are stored in a blob in the sharepoint database and as TDE encrypts all the pages in the database all the files will be encrypted there within. It's important to notice that while the database is mounted on the server the database server will serve the files unencrypted to the Sharepoint application server and it's clients. The Sharepoint binary cache will store them unencrypted as well as all the clients. You are only encrypting the data at rest on the SQL Server when using TDE. You can add to the security by using encrypted connections to the database server and HTTPS to connect to the Sharepoint application but after the files leave the database storage they will be unencrypted. 

TDE will add a little extra CPU cost to the server but that's marginal on current CPUs. It will however only protect the data at rest, but that's including backups of the databases. It's fully compatible with failover clustering and AlwaysOn but you have to make sure that all nodes in AlwaysOn are configured correctly ($URL$ I would say that it's more practical than trying to use Bitlocker or similar techonologies.