If you've got a managed switch, look at the switch logs, port statistics and port link state to identify/confirm the symptoms during the failure. Faults with autonegotiation, bad cabling, mismatched speed/duplex and framing errors are easy to pick up with this info. "show log" and "show interface <interfacename>" will do the trick on Cisco gear, I presume managed switches from other vendors will have similar commands. 

You didn't specify who the recipients of the messages are. If they're external to the business and the messages are not customised per recipient, a mailing list provider could be a cheap alternative. The upside here is you're not taking the bandwidth or processing hit draining the messages through your Exchange infrastructure; you send out one copy and the mailing list software off site then explodes it to the 4000 recipients. Security wise, mailing list software can usually be configured with authorised addresses or can implement a method of moderation, allowing external control or review of messages before they are released for delivery. 

Depends on where the servers live. As with most everyone else, PXE booting/install is by far my first choice (and is absolutely trivial with ESXi). In some cases, PXE booting is not appropriate or available (think secure DMZ's). In this case, installing using the server's out-of-band Lights Out Management CD redirection functionality means I can do it from my desk without hanging out in the datacenter or needing to reconfigure one of the host's switch ports to a more netboot-friendly network. As for host configuration, configuration can be scripted using the v(I)MA toolkit or virtual appliance, although these tools have some functional limitations. vSphere (ESX4) makes this even easier if you have licenses which include the Host Profiles functionality (configure one host correctly, define it as your host profile configuration source, apply config to all other hosts in the cluster. It will check daily to make sure the hosts are still compliant with their "golden" configuration). Note that not all configuration can be scripted via v(I)MA or be managed under Host Profiles; some manual processes are required to complete host configuration due to some configuration options not being available through these tools. In this case, a written procedure or checklist will ensure you don't miss anything. 

Check the span/mirror port configuration to make sure it's doing what you expect. Some switches can be configured to capture only inbound or only outbound traffic (with both directions being a third option). As an example, this should verify the state on a Cisco device: 

Start off by verifying your network is behaving itself. Assuming you have managed switches, look at the interface statistics for speed/duplex mismatching or a mismatched MTU. Consider checking / replacing cabling if anything is running errors (eg: trying to run GigE over Cat5 instead of Cat5e will likely give grief). Run some tests to prove you can get wire-speed transfers between the two machines and to the external machine; netcat, ftp or http transfers are a good start here (scp may get CPU bound, and thus, may not be the best test). Test the same query locally on the Postgres server. If it completes in an appropriate timeframe, you know it's not the database. If it doesn't complete or takes "too long", then you have a bad query or other database problem to debug. Make sure to consider the storage I/O side of things; you may be saturating what your disks are capable of providing. Check the VMware performance graphs to confirm / deny. Assuming that works, disable the firewall and run the same query against the postgres server from "box1". If that works, the VM->VM connectivity is likely fine. Assuming that works, bring the firewall back up and test again. If that works, then your problem is likely external to that host, leaving the switch or the external host to debug. 

Your plan is not nuts. As usual, there's more than a few ways to attack this based on what you're trying to achieve and how to protect your data. First up, you can present a raw LUN to a VM using a "Raw Device Mapping". To do this: 

As an alternative, move the directory aside, recreate it with the same name, permissions and ownership and restart any apps/services that care about that directory. You can then "nice rm" the original directory in the background without having to worry about an extended outage. 

Depending on your server, it may perform Console Redirection over an inbuilt serial port; by using a USB-to-Serial adapter (if your laptop does not come with a serial port), you can get access to POST messages and the BIOS by using serial terminal emulator program ('screen' works well for this task, if you're unix friendly). Your server may also support Serial Over LAN, accessible using IPMI tools talking to the server's Baseboard Management Controller. After configuring the BMC appropriately, you should be able to obtain access to the console over the wire. 

You haven't explicitly mentioned which product, so I'm going to assume the tag of "vmware-server" is accurate. I'm also assuming you are running your old server as a VM on top of your new server install. The feature you're looking for is known Raw Device Mapping (Server 1, ESX, ESXi) or SCSI Passthrough in Server 2. RDM's work great in ESX/ESXi; anecdotal evidence on SCSI passthrough in Server is mixed. The procedure, according to the documentation for Server 2 (page 157): 

You don't specify a platform, so I'll assume Windows. Give psinfo (part of the Sysinternals PSTools) a shot - feed it a text file with a list of machines to query, and an administrator-level username / password and output either CSV or plain text. CSV and a little data munging should get you most of the way there. 

There are tools, such as Sysinternal's Process Explorer, that can find and forcibly close file handles, however the state and behaviour of the application (both yours and, in this case, IIS) after doing this is undefined. Some won't care, some will error and others will crash hard. The correct solution is to take the outage and allow IIS to cleanly release locks and clean up after itself to preserve server stability. If this is not possible, you can either create another site on the same box, or set up a new box with the new content, and move the domain name/IP across to "promote" the new content to production. 

From the Add Hardware or New Virtual Machine wizard, click Passthrough SCSI  Device. Select a SCSI device to use. A physical SCSI device must be attached to the device,  and it must be connected to the virtual machine. (Optional) In the Virtual Device Node section, select a SCSI adapter and device node  from the drop‐down menus. Click OK. 

I'd expect that to be management traffic; vCenter Server need to confirm hosts are alive and send them tasks; the hosts need to confirm their licenses are valid and obtain tasks from vCenter. Each node also needs to communicate with the others in it's cluster to confirm which nodes are alive. You might be able to track down what the traffic is by looking at the port numbers, and tracing that back to an owning process (Process Explorer and Wireshark on the vCenter server would probably help here). 

The SD card would be classed as a removable device, which would explain why it's not usable to hold the page file (removing the SD would mean a bunch of memory space just disappeared from underneath the kernel -- and (most*) kernels generally don't like that much). You might get some use out of the SD card as a ReadyBoost device, assuming you can convince Windows to use it. 

A really easy way to find out is to create an ESXi USB stick, assuming your laptop will boot from USB. If it works, awesome! Go find it some storage and see how it goes. If not, you've only lost 15 minutes of your day. The best bit... you haven't trashed your laptop in the process (unless you wipe the drive to become a datastore). Obviously, performance likely won't be stunning, but it might be Good Enough(tm) to test one or two VM's. That said, I run a half dozen VM's reasonably well on a single CPU/single core desktop PC with 3 GB of RAM, booting ESXi4 from a USB stick (used to get around a weird SATA controller quirk). Horribly unsupported configuration, but works great for a lab :) 

As for your VM, your best bet for flexibility is to store your fileserver's boot disk as a VMDK on the SAN so that you can have other hosts boot it in the case of a host failure. Using VMware's HA functionality, booting your VM on another host is automatic (the VM will boot on the second host as if the power had been pulled; expect to perform the usual fsck's and magic to bring it up as in the case of a normal server). Note, HA is a licensed feature. To mitigate against a VM failure, you can build a light clone of your fileserver, containing the bare minimum required to boot and have SAMBA start in a configured state and store this on each host's local disk, awaiting you to add the data drive from the failed VM and power it on. This may or may not buy you extra options in the case of a SAN failure; best case scenario, your data storage will require a fsck or other repair, but at least you don't have to fix, rebuild or configure the VM on top. Worst case, you've lost the data and need to go back to tape... but you were already in that state anyway. 

You can, but watch out for performance issues related to CPU scheduling. VMware has the architectural limitations for guests and hosts well documented in their Configuration Maximums guide, located here for ESX4/vSphere, and here for ESX3.5 (direct links to PDF's). 

Try disabling adaptive quality and forcing the highest colour depth possible in your vncviewer application before attempting to connect to the remote server. I've found that fixes a few issues when one side or the other doesn't get the adaptive stuff right and/or has a different internal implementation. 

Sidenote: I'm aware some kernels can cope with memory disappearing -- they usually need to be told the memory is going to go first. 

I can't speak for DRBD Proxy, but the regular DRBD will not like this much. With even limited activity, you could easily saturate a dual T1 (2x 1.5Mbps; for round numbers, 300KB/s). 300KB/s could be taken up by application logging alone, let alone doing anything interesting on your server. This rules out synchronous replication (Protocol C), let alone adding the over-the-vpn latency into the equation. Async replication (Protocol A) might technically work, but I would expect the secondary to be so far out of date as to not be usable in the case of a failure (the replica might be hours behind during the day) Memory synchronous (Protocol B) won't help as it's still constrained by the bandwidth issue. I expect DRBD Proxy will still suffer with similar issues, primarily causing replication delay due to the limited bandwidth. I recommend you re-evaluate your DR strategy to work out what you are mitigating against; hardware failure or site failure. In the case of protecting against site failure, you may get better mileage out of lower bandwidth / higher density transfers in the case of one (or both) bandwidth constrained site. Some examples of this technique are rsync (over-the-wire transfers are limited to changes in files between runs - rather than per-change for every change - plus some protocol overhead; can be run over SSH to encrypt and compress traffic further) and database log shipping (transferring compressed database logs to replay on the DR box may use less bandwidth than transferring a full database dump). If you're protecting against hardware failure, a local DRBD replica connected with a GigE crossover will work just fine, allow for fully synchronous updates, and permit online verification to prove the data is consistent on both nodes. You can still combine this option with limited file replication to an DR site to protect against a primary site failure. 

(this shows traffic on interface Fa0 is replicated both inbound and outbound directions and send out Fa1 to be captured) 

Addition for clarity: From a business perspective, clients want make sure they can always get in touch with someone to help them, yet have access to specific staff members for client/account management or higher level support/sales when appropriate. To achieve this goal, my recommendation is to use both both techniques; staff should have personalised/named accounts, and be members of a alias or mailing/distribution list for their department, role or branch -- whatever delineation makes sense. What address is given out or used then becomes a business decision of expected follow up / personal contact / historical knowledge required, staff turnover and how "general" client requests are (example, password resets can be performed by any helpdesk staff member) For front line customer-facing roles, departments with high turnover or where requests can be completed by any staff member, clients can be provided the alias (support@, sales@, reception@) for that department, guaranteeing someone from that department will receive their mail and be able to look after them. When a staff member leaves or changes role, it then becomes a matter of changing the alias or mailing list subscription. For roles where a high level of personal contact, historical knowledge, or where the work can only be performed by specific staff members, contact can be made using the named account. I'd recommend the client should be provided with an alias address as well so they are not left out in the cold when a staff member leaves the company or is on leave. 

Note: I've never done this before. The shared storage RDM should be possible, although having five nodes accessing it could introduce hilarity. The technique you want is similar to how two node Microsoft Clustering Services is implemented (with a shared quorum drive); VMware provide a documented method for how to achieve it. The solution looks well documented, if a little hairy. I'd recommend building and testing it in a lab before considering letting it anywhere near your production cluster. Good luck. 

The main reason for role addresses is people change jobs. It's much easier to change a mail server alias so reception@ or sales@ points to the new staff member or a team's mailing list than to contact every client that has ever received that address and request they change their address books. 

This is a more localised, *nix based answer. I've not found any good tools to emulate it under Windows. There's a few ways to implement this ... and to catch it when you forget. Revision control systems like subversion, git, cvs or RCS is a good way of tracking the history of a config file. If you don't want to install a revision control system on your production servers, storing configuration file directories either locally or remotely using something like rsnapshot will give you most of the benefits of a RCS, but you lose the possibility of auditing or leaving commit logs (although this could be worked around with comments inside the files themselves). To help you remember to log the changes, automated reporting of configuration changes via a nightly, cron'ed tripwire run is a good start. After building tripwire's database of the current state of files, any change to them will result in an email during the next run. You will continue to receive this mail until the database is updated, thus "resetting" the tripwire. 

The documentation on QEMU is not really clear; using the "-smp <num of cores>" option allows you to emulate more than one CPU, but it is not clear if this is real smp across multiple host cores, or effectively "threads" bound to a single core. VMware products do spread the load across multiple cores, but be careful about scheduling issues with busy guests which will impact overall performance. More info here. 

Your first example is telling ssh to connect to the ssh daemon (server side process) on a non-standard port, in this case, 25. If the sshd daemon is not listening on this port, the connection attempt will time out or error with a protocol problem, as you're experiencing. The second is telling ssh to make a normal connection to "serveraccount" (which happens over port 22) and remotely execute a telnet command to localhost:25. The output of create-smtp-message becomes stdin to your telnet command, thus allowing your SMTP server to receive your message. 

VMware does have some products for this sort of workflow; Lab Manager or Orchestrator could automate most of what you want. If you want to save some cash on the provisioning / teardown process, you could roll your own solution using their scripting API and V(I)MA(forums here) To automate your app deployment and configuration, Puppet or cfengine will do config management and application installation, and can be used with Capistrano for general purpose automation.