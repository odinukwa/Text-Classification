A utility function can certainly be negative. The utility function is nothing more than a way to represent a preference relationship. This is an important conceptual point. In several theorems that typically show up in introductory texts, we show that sets of preferences that satisfy certain regularity conditions can be represented as utility function. Also, there are different decision theory frameworks that allow the utility function to be transformed. You alluded to something like this in your question. In the traditional framework without uncertainty, the utility function is defined up to a monotonic transformation. Under certain kinds of uncertainty, we get Von Neumann–Morgenstern utility functions that are unique up to affine transformations. You can read more about this elsewhere. For now, the consider the following definition of a utility function. It is taken from Advanced Microeconomic Theory by Jehle and Reny (3rd edition): 

The Wikipedia article for the "weighted cost of capital" (WACC) defines the WACC as "the rate that a company is expected to pay on average to all its security holders to finance its assets." What exactly does this mean? Does this mean that this is supposed to be the expected returns on the company's stocks as well as the expected return on the company's bonds? Also, how are the two equations, given in the Wikipedia article, derived? For convenience, the two equations are the following. The first is $$ \text{WACC} = \frac{\sum_{i=1}^N r_i \cdot MV_{i}}{\sum_{i=1}^N MV_i}, \tag{1} $$ where $N$ is the number of sources of capital (securities, types of liabilities); $r_i$ is the required reate of return for security $i$; and $MV_i$ is the market value of all outstanding securities $i$. The second is $$ \text{WACC} = \frac{D}{D+E}K_d + \frac{E}{D+E}K_e \tag{2}, $$ where $D$ is the total debt, $E$ is the total shareholder’s equity, $Ke$ is the cost of equity, and $Kd$ is the cost of debt. EDIT: Thanks for the answers below. "Derivation" was too strong of a word to use, which was clear after the explanations given. It was just a little unclear what the objects were in practice. The discussions below clarified things. Also, I found this paragraph from a website linked in one of the answers helpful: 

Also, there are some models that are purely statistical. There are also some models that are built solely for prediction. In short, there are many more types of models with different purposes. A model should be judged on its purpose. Also, as for uncovering "the truth." You might consider reading more about causal inference. This is one of the most important parts of econometrics. For starters, read about the method of Instrumental Variables. There are several other questions and answers on this website that deal with this. 

Foundations of Economics Analysis, by Paul Samuelson or Economics, by Paul Samuelson (and William Nordhaus) or here for the original. Maybe I should at least mention this for historical reasons. The book is very old by now (though it has seen some updates with Nordhaus becoming a co-author on later editions), but it's amazing how many Nobel laureates in economics cite this book as an inspiration to go into economics---many of them coming from a background in math. From Roger Myerson's prize biographical, 

Financial Asset Pricing Theory, by Claus Munk. This book takes a nice approach in that each chapter repeats the material 3 times: in discrete time with a discrete state space, in discrete time with a continuous state space, and in continuous time with a continuous state space. This makes it easy to see how each relates to the others. Also, it helps the reader to focus on the underlying concepts rather than the mathematical artifacts of each individual setting. I like Cochrane's book a lot for the intuitions that it provides. In my opinion, Munk's book is very useful as a complement to Cochrane's book. Munk's book provides a lot of the mathematical clarity that I feel is less clear in Cochrane's book. 

I make this change because you when you say that "this shift will increase both the price and quantity of ice cream" you have already said that the production of ice cream has increased (it increased along the supply curve). 

Ok. I made some small computation errors and got confused here. The notes make sense with the following notational assumptions. If I write out the AR(1) process as follows (ignoring drift) $$ r_{t+1} = r_t + \epsilon_{t+1}, $$ then we have $\text{Cov}(r_t, r_{t+j}) = \frac{\rho^j}{1 - \rho^2} \sigma_\epsilon^2$, where $\sigma^2_\epsilon := Var(\epsilon)$. The point that I should have caught on to in the notes was exactly the point that the variance is unconditional and so $\text{Var}(r_t) = \sigma^2 = \frac{1}{1 - \rho^2} \sigma_\epsilon^2$ is the unconditional variance (the time series is stationary only when $|\rho| < 1$, so beware when using $\rho = -1$ as mentioned above). Given this definition, everything works. First, note that $$ \text{Cov}(r_t, r_{t+j}) = \frac{\rho^j}{1 - \rho^2} \sigma_\epsilon^2 = \rho^j \sigma^2, $$ as noted above. Then also notice that \begin{align} \text{Var}(r_{t,t+n}) &= \text{Var}\left (\sum_{i=1}^n r_{t+i} \right) \\ &= \sum_{j=1}^n \sum_{i=1}^n \text{Cov}(r_{t+1}, r_{t+j}) \\ &= \sum_{j=1}^n \sum_{i=1}^n \rho^{|i - j|} \sigma^2 \\ &= \left (n + 2 \sum_{i=1}^{n-1} \rho^j (n - i) \right) \sigma^2. \end{align} So, it all works out. 

To maybe give a little more intuition (and partially to increase the site's answer's per question ratio), I would add the following to @Rusan's answer. If we consider estimating the parameters in a stochastic process like $X_k = \mu \Delta + \sigma \sqrt \Delta \epsilon_k$, then we can do two things. We can either sample more points in a given interval of time (points are closer together) or we can lengthen the given interval of time (by gather data over a longer period of time, for example). When it comes to estimating $\mu$, sampling points closer together doesn't do us much good because over shorter intervals (higher frequency), the process becomes dominated by the noise. This might help us estimate the variance of the process, but if we want a more precise estimate of the trend ($\mu$), then we need to lengthen the total interval of time ($h$ in @Rusan's answer). 

You say that you can measure $\beta_{\text{equity}}$ by CAPM. However, you can measure any cash-flow by CAPM---even debt. Since $V = D + E$, keep in mind that $$ \Cov(X + Y, Z) = \Cov(X,Z) + \Cov(Y,Z). $$ The problem, I guess, is that you need to know the distribution of debts cash flows. This depends on the face value of total debt and the distribution of the underlying cash flows. This can get especially complicated if you're thinking about considering a dynamic setting. This question, I'm guessing, came from a discussion of the Modigliani-Miller theorem. Remember the idea behind Proposition 2. It's illustrated in the following graph: 

In finance, the variance of the returns of a security are used as a proxy for the associated risk of the security. I've seen some books include sentences like "if you take variance as a measure of risk...". In what way might variance be an incomplete measure of risk? What alternatives do we have? 

This equation, written flexibly, allows for many state variables and many (possibly orthogonal) shocks. As such, this cannot be represented by a simple univariate model like ARIMA. Many other examples can be found in "Structural Econometrics" by Dejong and Dave or more in "Recursive Models of Dynamic Linear Economics", by Hansen and Sargent. Some will only have a single dimensional state space. Some will have much larger state spaces. However, again, describing the model in full may not be necessary if you're only interested in simple forecasting. Hence, something like ARIMA is an attractive option. 

Maybe this helps. One method to do this is to use least squares (as was mentioned by @cc7768). Matlab does this by default when using the operator. This is also useful because it's sensible even if the system is overidentified as in the following example: 

Does anyone know of good references to learn continuous-time dynamic programming? The references don't have to be books. They could be links to online resources as well. Links to clear, concise discussions of even just the basics would be helpful. 

So, as we can see, this gives a simple solution $$ f(a,t) = \exp\{-a d_0 + d_0 (a-t)\} \cdot f_0(a-t) $$ Case 2: Log Mortality, $d(a,t) = \log(a+1)$. Here we have 

The following quote might also be useful. The following is quoted from Mostly Harmless Econometrics, by Angrist and Pischke 

In this paper, "Savages’ Subjective Expected Utility Model" by Edi Karni, he gives a definition of "conditional preferences." See here: 

Whereas the first definition was one that dealt with efficient use of inputs of production this second definition deals more with capital structure in the corporate finance literature. This reminds me of the "free cash flow problem." This is an agency problem described in the corporate finance literature that arises in cash-rich firms. I'm assuming that over-capitalization can refer to this situation. That is, they have more cash than is needed to finance their operations. Here is a basic description of the free cash flow problem, given in Jean Tirole's book The Theory of Corporate Finance: 

Ivo Welch at UCLA started the Critical Finance Review (CFR). (I post this maintaining that finance is a subset of economics.) From the website, 

My Questions Consider the following problem. It is almost identical to the classic Merton portfolio choice problem. Here I'm solving it using the so-called Martingale method. I have provided my attempt at a derivation. I have three questions: 

Along these same lines, this is a point that is brought up frequently when discussing Machine Learning. Machine Learning may not be so useful in helping to understand underlying causal relationships. However, often we are more interested in the predictions/forecasts. Sendhil Mullainathan in this talk on "Machine Learning and Prediction in Economics and Finance" gives the useful "rule of thumb" rubric---are you more interested in the $\hat y$ or the $\hat beta$? Why would something like ARIMA forecast better than a model that better reflected economic theory? Consider this useful passage from "A Guide to Econometrics" (6th Edition) by Peter Kennedy (p. 333). 

I am unfamiliar with this literature. I'm wondering, what other explanation exist in the literature to explain the difference in the average leverage between banks and non-financials? 

There is also a good discussion in "Contract Theory" by Bolton and Dewatripont. It is discussed in terms of non-linear pricing in Chapter 2, "Hidden Information and Screening". However, Mirrlees optimal taxation is discussed explicitly in section 2.2.2, "Optimal Income Taxation." This treatment covers the case with only two types. The case with a continuum of types is discussed in section 2.3.3, again in the non-linear pricing context. 

I don't think there is at all a consensus. This is a huge, active field of research in economics. Though, if you'd like to get a feel for the work that's going on, here are two papers by Raj Chetty, John N. Friedman, and Jonah Rockoff (one paper in two parts) that have gotten a lot of attention: 

Why are Epstein-Zin preferences important? How does recursive utility differ from other preference models in general? What do they capture that can't be captured otherwise? What are some good resources to learn more about them? 

Consider the Slutsky equation, $$ \frac{\partial x}{\partial p} = \frac{\partial x^c}{\partial p} - \frac{\partial x}{\partial I} x. $$ A giffen good is the case where the income effect $\frac{\partial x}{\partial I} x$ is negative and large (in magnitude) enough so that $\frac{\partial x}{\partial p} > 0$. From Wikipedia: 

First of all, I realize that "undesirable" is an ambiguous term. So, to clarify, when is a monopoly undesirable under the following metrics? 

They are related and usually fall into the same discussion, but as @Alecos mentions in the comments, the two theorems show different things. I suppose the connection that you're after is the fact that if the derivative $$ \left .\frac{\partial f(x, a)}{\partial a} \right |_{x=x(a)} $$ exists, then because differentiability implies continuity, you might be able to get part of theorem of the maximum out of it. However, to compare and contrast two theorems you mustn't just look at the results. You need to look at the assumptions as well. For example, the theorem of the maximum doesn't assume any sort of differentiability. The envelope theorem does (at least some forms of it). In any case, the assumptions that go into each are different (some stronger, some weaker). Also, there is this. The envelope theorem doesn't tell you that anything about the control function. Therefore, you definitely wont be able to get the result that $C^*$ is upper hemicontinuous. 

A lot of commentary seems to focus on monopoly and oligopoly among ISPs. I am of the opinion that the real issue behind net neutrality is not about market power. If this is really an issue of antitrust, why isn't the solution one of antitrust? (at least directly?) The real issue at stake seems to be one of ensuring the survival of the democratic nature of the Internet. It perhaps seems more likely that an ISP with market power might shape the Internet in ways that we don't think are "democratic", but this could very well happen in a competitive market as well. But it's hard to say, since we can only speculate. The important point to realize is, which is what your question is getting at, is that net neutrality is not solely about monopoly or oligopoly. 

This also gives a simple solution $$ f(a,t) = (1+a)^{-a-1} e^t (1+a-t)^{1+a-t} \cdot f_0(a-t) $$ Case 3: The general case For the case where we do not yet specify $d(a,t)$, 

I think this gives an answer reasonably close to what you're looking for (from the NYTimes article, "In climbing the income ladder, location matters," featuring research by Raj Chetty, Emmanuel Saez, and others). 

If we apply Ito's lemma, then \begin{align*} \dd \xi_t &= -\xi_t \dd X_t + \frac 12 \xi_t (\dd X_t)^2\\ &= -\xi_t \left(\frac 12 \lambda_t^2 \dd t + \lambda_t \dd z_t\right) + \frac 12 \xi_t \lambda_t^2 \dd t \\ &= -\xi_t \lambda \dd z_t. \end{align*} 

The general idea with regard to this question stems from a discussion about how equity is a residual claim. To go through this step-by-step, recall that the value of a firm is given by the discounted present value of its cash flows. Equity is a claim to whatever is left over after other financial claim-holders have been paid (e.g., debt). In this way, equity can be viewed as a call option and debt can be viewed as a put option. (See here and here.) So, if the firm was financed only by equity, then equity would be the claim to 100% of the cash flow. Thus, $\beta_{\text{asset}}$ in that case can be seen as the if the firm was only financed with equity. 

(emphasis added) Note that the study is primarily about income and social mobility, but they also mention that tey find modest or no correlation between tuition rates and amounts of extreme wealth in a region. 

Also, take another example of current interest. Lars Hansen's recent work (winner of the 2013 Economics "Nobel" prize) has focused on the difficulty and ongoing failure to define certain economic concepts, including "bubbles" and systemic risk. See his essay "Challenges in Identifying and Measuring Systemic Risk". I'm a fan of the dictum he relays, attributed to Lord Kelvin, 

the graph of the constraint correspondence is convex, the period return function is concave, other assumptions about the stochastic transition function having the Feller property and being monotone, etc... 

This is a common problem among students first learning economics. The way these graphs are displayed are not meant to tell you anything about causality, let along the direction of causality. The reason that price is on the y-axis is partly because of tradition and partly because that's what makes the most sense from the perspective of developing economic theory. One of the central questions of economics is the determination of prices. What causes prices to be what they are? So, it maybe seems natural to write relationships that give a price as the dependent variable. However, the layout is not meant to convey any information about causality.