GATE is a fairly well known and widely used annotation tool. It allows users to specify customized annotation schemas using XML. I also believe it natively lets you export your corpus to XML. I would caution you that GATE is very much a tool by engineers for engineers. The interface is a bit clunky and the GATE server requires quite a bit of initial set up. It's most suitable for projects with multiple annotators working concurrently. 

To expand on Jeremy Needle's point, the insularity of a community has a great effect on that community's accent and dialect (and in some cases, language). As many people have already pointed out, the reasons this lie in sociolinguistics. First, it is important to note that language change does not happen by divine intervention. Language change happens because a single person or small group of people make a change; inventing a new word, using an existing word in a different context, or pronouncing a word differently. Sometimes these changes make there way out into the world and become popular ("it is speakers that innovate and not languages"). The exact method of change is up for debate (in fact, the book I linked to above has an entire chapter devoted to the different theories) but most of the proposed theories rely on changes spreading through social interactions between speakers. For example, using Social Network Theory it should become obvious why rural areas tend to have more distinct accents/dialects/languages: these communities have relatively few links to outside communities and thus fewer paths through which changes can spread. Additionally, the the generally tight-nit nature of rural communities may mean that innovations can become squashed by social pressure to sound like the rest of the community. There is a flip side to this too. If a high-status member of community creates an innovation, the tight-nit nature of the community may cause this change to spread faster and the lack of links to the outside communities may prevent this innovation from spreading further. The interesting part is that none of this necessarily means that rural accents/dialects/languages need to be harder to understand than their urban brothers. In actuality, "[un]intelligibility" the OP speaks of isn't so much that rural accents are harder than their native accents but rather that they are different. In summary: there are social factors which make rural communities less likely to adopt language changes originating from the outside but which may also encourage the adoptions changes originating from the inside. Additionally, the same social factors which inhibit rural communities' ability to adopt outside changes also inhibit the ability for changes form inside the community to spread to other communities. Over time this can result in accents/dialects/languages which are very different from the surrounding accents/dialects/languages and thus [potentially] harder for their neighbours to interpret. 

Both adjectives and verbs can inflect as noun modifiers but use slightly different particles. Adjectives use for both past and present. Verbs also use for past, but use for present. 

If you are still a student (or if there is a university in your city), I suggest emailing or making an appointment to talk to some professors specializing in AI. Preferably ones specializing in Natural Language Processing (NLP) or Computational Linguistics (CL) but even if they don't they can still give you relevant advice and point you to the proper people. Other than that, you can take an online course. Coursera.org is a website devoted to providing free access to courses written by professors from world-leading institutions. Currently they have not one, but two introductory NLP/CL courses. One from Standford University and one from Columbia University. The Stanford class in particular was co-created by Dan Jurafsky who also co-wrote the textbook that most introductory NLP/CL courses use. Just click the "preview" button on those courses' websites and you should have access to all the slides and video lectures in the course. At the time of writing it appears that the Columbia course is about to begin a 10 week long session, meaning the materials are unavailable for preview. However, if you register for the course you will get access to the materials as the course progresses. Not only that but you will get to participate in weekly assignments, get feedback from other users and TAs, and at the end get a nice certificate of completion which, if nothing else, shows employers and potential professors that you're serious about your studies. These courses should give you an overview of NLP/CL tasks and techniques. You can use this to guide any further research. I would also caution you that linguistics is not learning and speaking languages. It is the study of the underlying mechanics of language which all languages share. I'd start by reading The Language Instinct by Steven Pinker. It's very basic and written for a general audience but it provides a nice overview. If you enjoy that and want to learn more then MIT OpenCourseWare has slides for MIT's introductory phonology, syntax, and semantics courses. With that foundation you can explore all the Linguistics courses MIT offers and see if any interest you. 

Although I am not an ASL speaker (signer? user?), your question interested me. After some quick research, the simple answer to your question seems to be yes, word order matters in ASL. Strictly speaking, ASL sentences follow the a basic SVO sentence structure. Languages typically have a trade off between strict word order and inflectional complexity (see slide 13). In a language with no morphological case markings and free word order it would be impossible tell what was the subject and what was the object. Therefore, languages with free word order require morphological case markings while languages with strict word orders tend to lose case markings over time since they are redundant (or perhaps it is more accurate the say that strict word orders develop to allow case markings to be omitted). So far as I can tell, ASL does not have any case marking signs, thus word order must be respected. I'd actually be interested to know if there are any sign languages which do have such markings. It is important to note that ASL sentences can contain a "topic" which occurs at the beginning of the sentence. Therefore, a signer may wish to emphasize the object of a sentence by moving it to the front of the sentence, marking it as the sentence's topic. English doesn't really have a concept of topic but other languages do. For example, Korean, which assigns case morphologically, uses the 은/는 marker to denote topic. If it helps, you can think of it as a combination of "Yoda speak" and wh-fronting. It is also worth noting that some sources claim that ASL users are typically familiar enough with English word order that they can understand English-ordered ASL, I suspect this is because, in addition to the fact that not all hearing-impaired people are completely deaf and thus may have learned English, even ASL users need to learn written English. However, ASL grammar is not the same as English grammar. Some of the big differences are the ability to omit subjects and the lack of a "to be" verb in ASL. I found this paper that summarizes a lot of the differences between ASL and English Finally, as jlawyer mentioned, sentences in ASL are not typically a linear set of independent signs. Signs may be combined to change their meanings as well as the spacial relationships between signs is important to meaning. 

I suspect you're coming in from a false assumption. Accents are a combination of many factors, the most notable being vowel pronunciation but also including stress patterns, intonation, etc. However, in common usage "accent" also means dialect or vernacular which would include slang and colloquialisms, tag questions (as a Canadian, Americans often tell me that they love how my "accent" means I say "eh?" a lot), etc. Pretty much all of those disappear when singing. They're overwritten by the melody and time signature of the music. Slang and colloquialisms are less prevalent in music (especially pop music which sells to an international audience). As for vowel pronunciation, it's not uncommon to mangle pronunciation in service of the rhyming scheme. So what I am saying is that one could argue that when singing people have neutral accent allowing listeners to fill in the blanks with the accent they are most familiar with (their own). For an example of this, listen to pretty much any song by Adele (who is from Britain). Personally, while she's singing I'd agree that she sounds "American" (actually, I'm not American so I wouldn't say that. I'd actually say "she sounds like me") but if you actually listen, her vowels are very British. I found a source to back up my reasoning but it's hardly a peer reviewed academic paper. ----EDIT---- Also, as @Cerebus suggested, there is a certain amount of imitation of American accents to appeal to American audiences but I don't believe that's the whole story. ----EDIT 2---- It also strikes me that the mechanical act of singing is different from normal speaking. Specifically, trained singers focus on higher-than-normal volume, projection, and range. These all require precise control of the vocal tract which would also help mask the singer's accent. ----EDIT 3---- Even though this question has been inactive for quite some time, a friend recently sent me this radio program which addresses this issue. While it's light on new information, it is worth a listen for anyone stumbling across this question through a search engine, etc. 

Yes, this usage has been around for a long time and you're missing out. A simple look at Google's Ngram viewer shows that the rate of occurrence for both "think that" and "thinking that" have held relatively consistent since 1800. Granted, Ngram Viewer only has data up to 2008, but it does suggest that the progressive construct "thinking that..." is not undergoing any particular surge in popularity. Of course, you might argue that those results are based on books and the phenomenon you're talking about is in informal speech. Unfortunately, I have no data on how this trend relates to informal speech but I would still say the results of Ngram Viewer evidence a historical precedent. Additionally, expanding somewhat on jlawler's answer, the progressive construct seems to place an emphasis on the act of thinking, creating slightly different semantic contexts: 

Natural Languages as Regular Languages Before we can begin, a bit of house keeping. I will be using regular expressions, regular languages, and finite state automatas interchangeably. While this is not strictly true, it is sufficient for the scope of this answer. See here for more information. This is a fairly difficult question as the only way to prove a language is regular is by constructing it from the unions, intersections, compliments, and relative compliments of known relative languages. Obviously natural languages would require a relatively complex regular language to describe it meaning a complicated derivation. It is much easier to prove a language is not regular. A Pumping Lemma is a necessary (but not sufficient) property of regular languages. I.e. if no pumping lemma can be found for a given language it cannot be regular but the existence of a valid pumping lemma does not prove the language is regular. Essentially, a pumping lemma is any x, y, and z such that y is not empty and xynz can be parsed by a given regular language for all values of n ≥ 0. Testing English for Irregularity I was able to find this lecture which takes an intersection of English and a regular language to result in the language AnBn-1died where A denotes a set of determiner phrases (e.g. {the dog, the cat, the mouse, etc.}) and B denotes a set of transitive verbs (e.g. {chased, bit, admired, etc.}). Remember that the intersection of two regular languages is also regular. This means that if no valid pumping lemma can be found then English, by deduction, must be irregular. Given this regular language, we have only three choices for y: 

Sentiment Classification is a still developing field so I don't feel comfortable saying that there is a right or a wrong way to approach your problem. Your approach would theoretically work but it is a bit simplistic. Consider the following: First, how will you choose your word list? Will it be manually compiled? How will you make sure it is representative and has wide enough coverage to actually be useful? Second, how will you choose your coefficients? From your description, I'm assuming all coefficients will be or , but surely a word like "excellent" conveys more positivity than a mere "good" and reflecting this in your coefficients would improve accuracy. How will your coefficients reflect this? How do you know they are accurate? I would suggest you read a few papers on Sentiment Classification (perhaps starting with the Google Scholar link I gave at the beginning of my answer) to get a general understanding of modern techniques. From what I'm seeing, most solutions use some manually annotated sentiment corpus, like the Multiple Perspective Question Answer (MPQA) Corpus and perform machine learning techniques (usually SVM or MaxEnt) to decide which words and features are strong indicators of sentiment and what their optimal coefficient should be. There are lots of machine learning libraries to help you with the programming component. A simple Google search should help you find a suitable language for your programming language of choice. 

The OP is making a very common mistake when it comes to comparing languages. If you can find a copy of Language Myths by Laurie Bauer and Peter Trudgill, I suggest you read Myth #2: Some Languages Just aren't Good Enough. If you can't find a copy then this blog should give you the rough idea. Let's examine the example given in the question: differentiate, differential, differentiation are all collapsed into a single word, 微分. The OP says that this makes the words lose their "subtle difference". But what is the difference between these words? Differential is a noun referring to "a change in the linearization of a function." Differentiate is a verb roughly meaning "find the differential". Finally, Differentiation is a noun meaning roughly "the act of finding the differential". If memory serves, Mandarin Chinese (like English) has a strict word order meaning that the "differentiate" verb form of 微分 should be unambiguous. However, this leave some ambiguity between the "differential" and "differentiation" forms. Except, such an ambiguity will most likely be resolved though context meaning that there's no real loss of meaning. Even in English there is no sentence using the word "differentiation" that can't be restated using "differential". For example, "Show the differentiation" could be restated as "show how to find the differential". The only reason these multiple noun forms exist is because using "differential" constantly could create some aesthetically unpleasing sentences, nothing to do with meaning. To give a more generic example, imagine a language spoken by a small tribe of primitive fishermen in the Amazon rain forest. They don't have words for "nuclear power" or "the internet", so that means they're incapable of talking about them, right? Wrong! The wonderful thing about language is it's living and changing. If you said "nuclear power" or "the internet" to Shakespeare, he'd be dumbfounded. English had to invent those words and if you introduced those concepts to our hypothetical Amazonian fishermen they'd invent a word for it too. Loanwords are a type of invention too. English had to take the word kowtow from Mandarin to describe that act. Similarly, we literally translated the word face to describe the Confucian concept. Given enough time any language can find a way to describe any concept. Therefore you cannot say that one language is "unfit" to describe something. However, I will offer one proviso: English is used as the international language of science. This is not because other languages are unable to describe scientific concepts but used in order to promote sharing and understanding of scientific works. The Ur-example is Gregor Mendel who discovered how inheritance worked as early as the 1860s but because he published his work in Hungarian and in an obscure journal, it was not until the 1900s, after his death, that his work was rediscovered and revolutionized the way scientists viewed genetics. Perhaps if he had published in a German or French journal his work would have been recognized sooner.