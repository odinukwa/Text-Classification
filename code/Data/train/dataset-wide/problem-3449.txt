You'll find that this kind of filter is a lot easier to set up in procmail than in Spamassassin, and procmail will co-exist happily with Spamassassin. 

Elastic load balancers spread the load by giving different IP addresses to different clients. How many machines do you have generating the load? If it's only one, then the load is only ever going to go to one machine at a time. 

That's not a bug, it's how PHP is supposed to work. Restart your PHP server or call apc_clear_cache() from your code to clear the APC cache. 

You can't change an on-demand instance to a spot instance. You'll need to create an AMI from the current instance and then launch a new spot instance using the AMI. 

You've only got one VirtualHost defined on port 443, so any access to port 443 will have to use that config, no matter what domain name they're using. If that's not what you want, set up VirtualHost configs for the other domains and point them somewhere different. The same in reverse for your issue with $URL$ -- you've not defined a VirtualHost for that domain name, so it's using your default VirtualHost, which is the first one, and so it's using /var/www as the root. 

Caveat! Every single time I think I understand the rules for how authentication works, I have to futz with the config repeatedly until I get some nuance correct. Use this only as a starting point. Re-read the apache documentation on mod_auth and mod_access in particular, paying special attention to the Order directive. Therein lies your answer. Hope this helps, and please post your working example if it doesn't match this one, as this is a pretty good recipe to have in an apache cookbook. --edit-- Testing the above shows that restricted area is forbidden to all except for those from the IP address, who must provide authentication. It is not clear from your question if users from other IPs need unfettered access to this 'restricted area' or if they are simply forbidden? 

wiki's tend to have revision control built-in, and many are file-based (vs stored in a database), so rsync should work perfectly fine. I know folks who do this for 'TWiki' for replicating their installations to mulitple servers. Perhaps you only have 'ftp' access to your wiki files? you might consider 'wget' to pull from ftp (rather than the http interface) with the recursive (-r) and timestamping (-N) flags set so that it only transfers file that are 'newer' (which isn't exactly a diff). Once you have a 'copy' of what is out on the ftp server, you'd mark the update time somehow (often with just a 'touch' of a specific marker file). You would then edit normally via your local installation of the same wiki, then use 'find $dir --newer touchmarkerfile' to identify the updates for ftp and transfer them over via a script around an ftp delivery tool. I have used such a solution before (though I had the advantage of sucking the changes back to the main server via 'wget', so just used the recursive timestamping approach again. In hindsight, if I had 'ssh' access (I didn't), I would have simply used 'rsync -globtru[n]cv' to simply pull (or push) the files in each direction. 

When I install a particular program on a computer it creates an overly lenient local firewall rule that allows inbound connections to port 1234 from any subnet. I want to create a more restrictive firewall rule set using group policy that only allows traffic inbound connections to port 1234 from a specific ip address only. However, currently in my environment Windows firewall rules that are set via domain group policy are configured to merge with locally set Windows firewall rules. The result is that the more lenient local firewall rule seems to take precedence over the firewall rule set using domain policy. If possible, I want to use group policy firewall settings to change, delete, or nulify the local firewall setting (and not use a startup script to delete the unwanted firewall rule). The simplest way I think to do this I think would be to specify some sort of firewall rule priority where the local rule is rendered inert by a more specific higher priority rule specified by the domain. But I am not sure that this is possible. Any suggestions? 

The short version is that reverse DNS is used to get a domain name from an IP address, while normal DNS is used to get an IP address from a domain name. The way it actually works is that there's a dummy top-level domain called in-addr.arpa, and to find the domain name for IP address A.B.C.D, the DNS client does a lookup on D.C.B.A.in-addr.arpa. There are various complicated rules for delegation of sub-domains of in-addr.arpa to ensure that those requests go to the correct place. The Wikipedia article is OK, although perhaps a little terse: $URL$ What it means to you is that if you own a block of IP addresses, and you want to be able to create reverse DNS records for those addresses so that their domain names can be looked up, you need to make sure that whoever you got the block from has set up an appropriate delegation so that you manage a sub-domain of in-addr.arpa and can thus create the appropriate DNS PTR records. 

No, different LANs != different subnets. LANs are layer two concepts, not layer three. You can join lots of LANs together with bridges and put them all in the same IP subnet. However, bridges aren't much used any more, because switches do the same job better, and other jobs as well. 

shame that so many monitors are 'widescreen' these days... I wish the 4:3 form factors were more common (and cheaper at resolutions of 1600x1200 or higher)... not everyone wants to watch a movie on their PC. I have a 1200x1600 (portrait mode) 20" and while I enjoy (and depend) upon the vertical size, it still feels a wee bit cramped horizontally. 

You might consider using `date +%w' as part of your tarfile, so you have a tar file for each of the last 7 days and dont have to worry about purging old copies. 

But when I try to map port 80 to a pid I get nothing: When I try seeing what sockets that specific pid is using I get: 

Not saying I know specifically how tomcat performs shutdowns... I would expect a pidfile associated with the process, or a control port that tells the application to shutdown. Barring those, however, it's common for scripts to 'hunt-and-kill' by looking at 'ps -ef' output (or similar). In these cases, it's easy for the kill scripts to be too aggressive and kill off all matching pids (or just the parents of those pids). Cant tell you how many times I've been editing a script in 'vi' only to be killed off by an aggressive 'stop' command someplace. 

Most VPS companies prohibit and block outbound traffic on port 25 to stop them being used for spamming. You'll need to use a third-party mail relay that listens on a different port (which may well be a service that your VPS company can provide). 

You can download a complete list of IP address ranges used by AWS, and get your clients to whitelist the lot. They will need to update the whitelist when the list is updated. Or just tell them to whitelist 0.0.0.0/0. 

The public key doesn't matter. There's no need to keep it secure, it should be widely distributed, and if you lose it you can always recreate it from the private key. It's only the private key that you should be worrying about. And yes, for that key it's a reasonable plan to put it on two or three USB sticks and keep them in separate places. 

You're looking at two different caches. tells you how much memory the operating system is using for the disk cache, not how much MySQL is using for the database cache. And the operating system should be using as much memory as it can for the disk cache -- why wouldn't you want as big a cache as possible? That memory is always available to be used if an application needs it. See here for a good discussion of Linux memory usage for caching. 

I don't know if you've made any changes to the group policy task, but for troubleshooting purposes to make sure that those changes are forcefully applied to the client to try changing the action from "update" to "replace." Also, you didn't supply any information from your actions tab. Check that the shutdown executable exists on the client. Check that you've supplied the appropriate arguments to the shutdown command in the arguments field such as /r to reboot. If something is preventing the client from rebooting you may need to add /f to induce a forced reboot. 

If you do not have administrative privileges you won't be able to stop/kill the windows update service or task. Even if you did have the appropriate permissions, interfering with windows update on a production server seems like a bad idea on a number of different levels. Rather than fiddle with windows updates, I think that there is probably a better approach. It sounds like your script runs for a long time. Are updates being pushed at a known time? Include logic in your script to have it stop running right before the the updates will be installed. Output your script progress to a file. After the server reboots you can configure the same script to run again automatically upon startup/logon or as a scheduled task. The script can read in the script status from your output file and pick up where it left off before the reboot. If you do not know the specific time when updates will be installed, you can include logic in your script to check periodically to see if the windows update client process is running (wuauclt). When the update process is detected, have your script stop running. The obvious problem with this approach is that a startup script requires administrative privileges. Maybe the only thing you can do is configure the script as a logon script so that it will be run the following day automatically when you log in to the server. 

It depends on your application and tolerance for failures. If you are running an oracle database for a financial business, you want expensive servers with hot-swappable parts and built-in redundancy (power supplies, disks, even cpu and memory). If it's a web server or compute servers with NAS storage, go cheap (on the server, not the NAS) as long as you can tolerate the loss of a box without much impact. Dont go so cheap that you are constantly replacing bad hardware. The general rule of thumb for me has been to use raid to protect your important disk-based data, but buy cheap commodity hardware for compute and web farms. Get a good load balancer that can detect when a webserver is not responding and mark it offline. Real life experiences: Bad: Running oracle on commodity hardware was a cheap solution that we were able to put together very quickly, but a bad CPU fan caused a server crash which forced us to restore Oracle from tape (ugh!). Good: We replaced 2 high-end heavily redundant machines with 70 commodity rackmount servers. We were able to drop maintenance on the 2 machines and started just buying $2500 'spares'. Over about 2 years, I think we only ever used about 6 of the 'spares' (the real challenge was avoiding deployment of spares for other purposes). 

Be aware of a potential security issue. It's quite likely that some of your customers will run web servers that are configured to allow cookies to be read between subdomains, and if any such customer has cookies set for domain ".iam.us", then any other customer will also be able to read those cookies. 

If you have a read-intensive site that's failing, you need to look at your database caching strategy. Before doing anything with the hardware, examine your MySQL cache usage in detail and see if you can tune it better. It's also worth checking that all your joins are using indexes. 1000 unique visitors per day is very low traffic, and you really should be able to handle it from your current server. If you do need more hardware, you will almost certainly find that you need more RAM so that you can have a bigger database cache, and that you will do better with a single server with more RAM rather than two servers with less RAM. On Amazon, I'd look at a single Large instance, which would nearly quadruple your memory -- although it would cost a lot more than two micro instances, 34 cents per hour instead of 2 x 2 cents per hour. 

ACK! several folks have suggested the wonderful date +%Y%m%d_%H%M%S style solution but nobody has mentioned the major caveat of '%' in crontabs... '%' it is equivalent to '\n' so your cronjob will likely fire and fail mystereously! You'll more likely want to simply escape it with backslash like this (and I also like to get some kind of inventory or other output to check that it ran). 

We run into this when editing CGIs... the #! interpreter line gets Ctrl-M on it somehow, rendering the executable not found. It looks like a perl error but is really the 'she-bang' interpreter line having 'nearly' invisible characters at the end. In our case, we found this after the file was written. try using dos2unix command to copy to another name and try hitting that. If it works, you've found your root cause. Sorry to say that I have no real workaround except to recognise the problem when I see it. --edit-- Our error message is usually: scriptname: file not found NOT the 'file busy' mentioned in the question. 

I think that you are asking, why use OUs with GPOs when I can use security groups? I can think of a few reasons: 1) Clarity. If you create a logical OU structure (e.g. create "computer" and "user" base OUs, create "server" and "PC" child OUs under "computer", create "administrators" and "accounting" OUs under PC, etc.) and link GPOs to their appropriate OU without changing the security filter, you will be able to understand at a glance what GPOs are affecting what user or computer groups. If you use security group filtering only, you'll need to examine each GPO's security filter or use some other time-consuming method to see what is going on. If you only have a few GPOs and you are the only person managing the GPOs it may not really matter, but as the number of GPOs and the number of people managing GPOs increases the security group filtering only method becomes very confusing very quickly. 2) Efficiency and manageability. GPOs linked to higher level OUs apply to all subordinate OUs. Through a well designed OU structure, you can effectively apply group policies with pinpoint precision and minimal redundancy. For example, more generic computer policies that apply to both computers and servers should be linked to the computer OU (e.g. generic IE/Edge settings, generic security certificate deployment, powershell environment settings, etc.), policies to apply specifically to PCs (e.g. client only Windows firewall rules, MS Office related rules, client software deployment rules) should be linked to the PC OU, and custom rules should be linked at lower level OUs (e.g. specific file share mapping, special permissions, etc.). If all your GPOs are linked to the same OU, you'll need to remember every time to change the GPO priority order so that more important/specific policies take precedence over more generic policies (this is extremely easy to forget). I think you find that troubleshooting "GPOs not applying" problems becomes much easier if you have a logical OU structure with minimized security group filtering. (NOTE: some group policies, like the group policy loopback policy, can behave in unexpected ways if you use security group filtering). 3) Speed. GPOs that use security group filtering take slightly longer to process. GPOs that use WMI filtering take much longer. It may not matter with only a few GPOs, but when your GPOs start numbering in the hundreds you'll need to think about optimization. If you want to minimize GPO processing times OU-based GPO assignment is the way to go.