What I get from a transaction like the one above using the previous bash command are three outputs: , and bash variable. I'll post some extract of them to make it clear what's their content. plog.log 

I've searched through the Internet but found nothing that solves my problem (for example issuing and adding to ). returns: 

You can play with to obtain the merging concept you have in mind. Here's a link to the official documentation (PostgreSQL 9.4): UNION clause. I think that you would like to remove duplicates (if there are duplicate entries in both tables), so probably the is right for you. 

Remember that must be owned by user, so run . This way you can use wathever to analyze the table (if you need to). 

Anyone can help me to figure it out? Many thanks to all Pietro UPDATE I forgot to logout and then login after editing the bash profile. The suggestion made by Daniel Vérité was right. I just edited the env variable in in order to make it visible at a global level and not only from the interactive shell. I added the following line to : 

It subtracts 12 months from and checks if the resulting date is greater or equal to . This query works too: 

You can mess around with the various parameters available in . For example, you can export a gzipped file or a CSV with headers and custom separator: $URL$ . You can also issue a to export the entire original database and run to recreate it on the new database: $URL$ . This should work.. I've just dumped an output file of several tens of GB on OS X (linux-like) without any problem. Se also for more selective database dump options: $URL$ 

PART2 - UPDATING In order to update you must specify values, so you have to know them a priori. This works perfectly, but probably there are other naive solutions: 

I know this topic is pretty complex and involves a lot of different factors. Let's say I have several similar queries running at the same time. These queries involves only read operation and several ordering and windowing. If I'm not wrong, with I can get the memory used by some operations like . I need to know how a single query impacts in order to calibrate (and kind of predict) the amount of queries I can run at the same time. Is it possible to get the maximum amount (not the total amount) of memory used by a single query? Should I get it from the output (which gives me only some memory usage infos) or there's a better way? I'm using Postgres 9.5 and 9.6 on different Unix-like environments like Red Hat 6.7, Ubuntu 16.0.4 and macOS Sierra. 

There's about 1000 to 2000 rows normally. It's just for a hobby project. Any help will be greatly appreciated. 

So basically, three staff all start out with a target of 6.0, but on March, staff with ID 1 has a new target of 7.0. I want to maintain historical targets because it is relevant to other data in other tables. I would like to have a user-defined function that takes a date as a parameter, and this function needs to join the above table with another table based on the date. Say the function is called with 1st of February as the date, I would like the result of the join to include the target column showing 6.0 for all staff. Something like this (I think this won't work because there could be multiple rows before ): 

Reference is used when the customer provides their own ID for the task, typically this is for example "THR#000123123" or "IX#01212". It is always in the format "ABC#123". Now, I need to categorise these into multiple arbitrary categories and subcategories based on only the type and reference fields: 

Create a table with the sole purpose of storing the date Timestamp every row in the table with the same timestamp 

I'm not sure if I've completely fudged the design of my small hobby database (I'm not a DBA by any means), but I have a table like this (primary key is ): 

First off, I'm not a DBA at all, I'm more of a front-end guy, but I'm currently hitting a brick wall and can't figure out how to tackle it. A database I am working on has a table with several columns. The table represents completed tasks, the columns are: 

I'm using PostgreSQL 8.4 and I have a table that is cleared out and refilled with new data every so often. I want to be able to store the date that the table was last filled, but I'm not sure where to put it. I'm really a novice at database design, and the only two things I can think of are: 

A task can only belong to one category, so if a task for company THR was "return-to-base", it must only be under category 2.1.1, not 2.2. I realise these categories seem very arbitrary but this structure represents the most useful break-down of almost any selection of tasks from the table. My question is, is it possible to create a view that contains the TaskID and its category? Or should I just implement this logic in the front-end after selecting every TaskID, Type and Reference? 

After asking on pgsql-performance list, Jeff Janes figured out that the cause was associated to the default collation used by Postgres (see this link for more informations). MacMini was using the much performing collation while Dell T420 was using the en/US collation. T420 (Postgres 9.4.1) 

I'm currently running a OS X Lion Server system which ships with a built-in and not-upgradable PostgreSQL version. After years of usage I've finnaly decided to leave the built-in version and install an indipendent version. I disabled the built-in installation and downloaded the installer from EDB and followed the wizard. After many issues reguarding encoding and locales, I've finally managed how to setup a DB with no locale and UTF8 encoding. I issued the following command: 

It handles models that has the same rank, displaying more than 5 rows per device type just in case. In order to test it, create a table and fill it with data: 

where and are 50% and 75% of total RAM size respectively. should lay between and for your use case.Test it and give us a feedback. There are also two other moves you should make: 

The function is documented here: $URL$ ATTENTION Be aware that those two queries can give different results for periods that are exactly . The following returns : 

The problem is that I can't real-time access the content of variable. I can instead access the content of and , so the usage of doesn't make sense at all because it doesn't appear in both and but only in . Should I stick to parse or in order to find the output value from the function (removing the statement in that function because of it's uselessness) or there's an alternative method? Am I missing something? Thank you 

I have a software that generates transactions and executes them. I would like to asynchronously inspect about the transaction progress in term of the simple proportion of queries executed over total number of queries. I'm able to read both and generated by , so I was thinking that a solution could be printing to some custom messages like "Progress: 3/8" (or another custom text string). I've thought about executing a "logging query" which stores in a proper table the informations about query progress, but this table wouldn't be available until the transaction completes, making it unuseful for me to inspect transaction progress. At the moment I tried the following (suppose to have 3 queries in the transaction which do some stuff): 

First, I always create a wrapper script when creating commands like that. That makes it easier to debug the command without having to signal postgres to reload anything. This also makes it easier to include logging from your archiver script. What you need to figure out first is how you want your data replicated. The way you're doing it now seems ok, at least if you check if the destination file doesn't exist before copying it. That way it can be re-run as many times as you like and it will try to copy it to the external location. 

It looks like something went wrong when you upgraded. You have to check that the and are correct. Perhaps the changed and the instance isn't listening on the host anymore? The reason that and the other tools are still working is probably because they're connecting over the unix socket instead over TCP/IP. should also be able to do that if you just leave out the and options. The is not relevant because your client can't connect to the server at all. You can verify that the server is listening on the expected host and port with . $URL$ 

I think the answer mainly depends on how much time do you want to spend learning new databases vs how much time to you want to spend learning machine learning. For example PostgreSQL has lots of GIS stuff built-in which I assume could be very useful for your queries. CouchDB has many useful features with their map/reduce stuff but I find it a bit limited. If you think you're going to add many columns later on based on new algorithms you might want to look into the column-oriented databases like Apache Cassandra. However, I would recommend splitting the problem into two parts: 

The first part you seem to have a good plan for already. I would write a super simple app that would just suck in the data from twitter and put it in a table as a BLOB. Or just a file on disk. When you have that you can go from the raw data and insert it into any kind of backend. Now you can choose backend based on the current problem (algorithm) you're working on, instead of having one solution to fit all your problems. The key point here is to extract exactly the data you need. That way you shouldn't have to consider the fact that the data is nested inside a document from Twitter as you only pick out the parts you need. If you go that way I think an RDBMS would perform very nice as they can run all kinds of queries. Make sense?