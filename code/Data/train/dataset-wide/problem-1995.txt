I'd second Jonathan's comments. There are many resources (to show the DBAs)showing that automated "missing indexes" analysis is very limited and often misleading. In addition to sp_blitzindex, if you get them to run the following query against each database (after a representative uptime), you can get an idea of which tables are the cause of possibly unnecessary IO due to high number of scans (considering the row counts) and lookups. And you can start to consider the indexes suggested just for those tables, and/or find the offending queries and consider indexing for them. 

The errors suggest SecurityDescription is no longer a valid column to reference. Perhaps VS2008 ignores such errors, or the column was lost in the upgrade. 

$URL$ looks like what you need. Unfortunately I don't know enough to be more specific. Update: I believe hot2use's answer may only apply for background updates, which I don't think your call will trigger. See $URL$ My guess is that the way you are calling the refresh will complete the request before exiting the procedure, but that depends on what "the foreground process" means in Oracle. 

It's almost certainly a network/connectivity issue, even if the jobs don't fail at the same time. Error 14152 is logged when "Agent shuts down after unsuccessfully retrying an operation (agent encounters an error such as server not available, deadlock, connection failure, or time-out failure).", and if the last error was "existing connection was forcibly closed" that suggests there were also earlier tries with the same result even if not logged. There are many reasons this could be happening intermittently. One specific reason can be multiple NICs in the distributor having the same IP assigned. It could also be the network connection is simply overloaded at times. 

UPDATE: Your latest update shows that the problems you're seeing are caused by classic inappropriate parameter sniffing. The simplest solution (if you control the code) is to add OPTION (RECOPILE) to the end of the problematic query. This will ensure the statement plan is recreated on every run, and will also allow certain shortcuts to be taken in creating the plan. The downside is additional CPU and time spent creating the plan on each run, so this solution may not be suitable. Considering the skew of your data (3 mill for one value vs 160k max for others), and assuming that skew will not change much, branching like this may do the trick: 

My guess is that the developer simply didn't consider the possibility of concurrent updates and the complications around that. 

Never seen it done, certainly not for that reason. Speaking for SQL Server, it would work fine with a Bigint Identity column. But you'd have to ask whether it's worth the confusion it could cause, and the need to remember to define the clustered index descending to prevent fragmentation, etc. 

The Agent service depends on the SQL Server service, so it may not yet be running when your startup stored procedure runs. Probably better to monitor the service some other way. See a similar discussion at $URL$ 

When there is enough free space on the page for the record being inserted (whether in the middle or at the end), there will be no page split, as the page can simply be rewritten with the new data inserted. If there is insufficient space, then there will be a page split, with approx half the data moved to a new page. Note that if the insert is to the end of the very last page, and there's no space, then a new page will be created just for the new row and subsequent appends. That's what's known as a "good" page split, though it really isn't a split at all even though SQL server counts it as one. 

Switching on will be near-instantaneous (assuming only one DB connection). Yes, switching off is just as painless. See above. Cleanup happens by itself. Section 2 of the post above has some things to consider. 

For whodunnit, there's a profiler trace event "Audit Add Login to Server Role" under security. Presumably it catches ALTER and not just the SPs specified, but that would need to be tested. I'd suspect the guilty once caught would start assigning specific permissions instead so would trace that too. 

(First two paragraphs are repeats of my comments.) When Amazon specifies "log processing" as a use case, I'd suspect they're talking about infrequently-flushed IIS logs and the like, not database logs. And although ST1 is specified as not suited for "small, random" workloads, my reading of $URL$ would be that it's not suited for "small and/or random" workloads. But this, and your questions of how the limits are enforced/calculated, are probably something you need to get clarified from AWS. "Will SQL 2014 automatically increase the size of units it writes to the log in response to an increase in log file latency?" I'm pretty sure it won't by default. But if you can afford some data loss risk, then delayed durability may be just what you need: $URL$ If you do end up moving to ST1, AWS's online conversion option sounds far riskier than "Add a new ST1 volume to the mirror, take the mirror offline, move the log file from the old volume to the new one, then bring the mirror back up, failover, and repeat with the principal", which should work well for this. (Moving or dropping the primary log file with the database online is not an option.) 

Success - go to step two, failure - exit(f). Success - exit(s), failure - go to step three Success - go to step four, failure - exit(f). Success - exit(s), failure - exit(f). 

If your customers use the new login, they will have full control of all pre-existing databases (including being able to drop them!), as well as any new ones created with that login (since creator login gets mapped to dbo). And they will not be able to drop any dbo user. 

The reason it's not working is that your join causes each row to be updated multiple times, but each update sees only the original values, not the cumulative values of previous updates. So your updates are in effect cancelling the previous updates. (Or maybe SQL only allows one row update per statement. Either way the result is the same, with different resource use.) Here's a quick and poorly formatted repro: 

It's not the fanciest or even best way, but is the easiest to start learning with. By the way, your code won't work if @UDF_1 is null, you need to assign '' to @ALL_UDF before working with it, or add other checks.