That definition sounds a bit obtuse, but it was chosen because it is easy map all of the other Peano axioms onto these two definitions. With this, we gain the ability to use set theory axioms to manipulate "numbers" in very powerful and fundamental ways. One of the most important of these is the concept of the cardinality of a set. This is the "number" of things in a set. Informally {1, 2, 3}, {3, 4, 5}, and {apple, orange, orangutan} all have a cardinality of 3 because they have 3 elements, but {2, 4, 6, 8} has a cardinality of 4. This is where it gets tricky, because it turns out "the set of all natural numbers" is a valid set, typically represented with a capital , so we can ask "what is the cardinality of the set of all natural numbers?" The answer is "infinity," and that statement is made as a definition. We define the cardinality of to be a particular number, known as which is given the English name "countable infinity." Yes, to mathematicians, infinity is countable, because you can theoretically start at 0, count upwards 1, 2, 3, 4, 5... and "reach" ℵ₀ according to the axiom of induction. There are also uncountable infinities, such as ℵ₁, known as the cardinality of the continuum or the number of real numbers (assuming the continuum hypothesis is true... there's even different opinions on this). There's even a school of thought on "transfinite" numbers which can handle phrases like "I double dog dare you infinity plus one times!" Welcome to the rabbit hole of infinity in mathematics. We've defined the word to mean something here. It is defined with respect to a set of axioms. Do those axioms hold in "real life?" Most mathematicians find it convenient to presume they do. The computer you are reading this on today was developed using many models from calculus, and calculus's roots are found deep in infinity (particular its concept of "limits). So far, that assumption has done us pretty good. Is that assumption "true?" That's a more complicated question. There are finitist schools of thought which start from the assumption that the number of natural numbers is finite, usually related to the finite capacity of the human mind or the universe in one way or another. If time is finite, and computation is finite, then one cannot theoretically computer "infinity," so they argue it doesn't exist. Are they right? Well, yes... by their definitions, just as the opposing claim is true by the definitions of the Peano axioms and set theory. Both can arguably be true because they each define the word "infinity" to means something ever so slightly different. As a closing, it may be worth dabbling in linguistic choice: "So, shall we say that numbers are infinite?" We can say a great number of things. Whether those things meet the ideal of truthhood (itself a very hard word to describe formally) depends greatly on one's individual meanings for words. If you accept the definition for "infinity" given by mainstream mathematics, then "numbers are infinite" is true, literally because mainstream mathematics defines "infinity" as such. If you accept the definition given by the finitists, then "numbers are infinite" is false, literally beacuse the finitists define "infinity" as such. You may choose your own definition. It may even be contextual (it is not uncommon to find Christian mathematicians who define "infinity" within their religion slightly differently than they define it within the mathematics, with no ill effects besides two very similar concepts being assigned the same word in their vocabulary). 

The conversation you refer to is not an easy one. What is difficult about it is that you have an individual who clearly believes in something (a religious belief), and they're going on the attack. We know they're on the attack because of "You necessarily believe in something." That phrasing pins you in place, whether they intended to or not. (There are other phrasings which are not attacks, such as those that question how you might live without believing in anything, but that's not the scenario stated) The real question you will want to ask yourself is how much do you want to merely deflect them versus how much you want to challenge them. They went out on a lib, demonstrating that they believe "you must necessarily believe in something," which is a statement that is rather dangerous unless you have precise language. Mixing "necessarily" with layman's speech is a recipe for disaster, no matter the topic. Worse, with their follow up question, "What do you believe in?" They are implicitly assuming that your beliefs can be put into words. Thus, the smallest deflection you could do is to counter one of these statements. You can challenge the use of "necessarily" with layman's speech, and offer to have a discussion with very clear definitions, but that may require more of a philosophical background on your part. Thus, I'd recommend countering the assumption that your beliefs can be put into words. Language is, after all, a construct of human culture. There are plenty of things our subconscious does that defy words. Also, remember that words can have different meanings to different people, so there's a valid argument to be had that you can have beliefs that you have worded for yourself which mean something else entirely to a religious person. For a more aggressive deflection, I'd recommend what I mentioned in the comments. State a belief in the Aggripan Trilemma. This is a trilemma which states that all logical arguments must end in one of: 

I would highly recommend learning QM. QM is science's strongest current theory for "how the universe works," and it has very interesting things to say about determinism. It will be very hard to debate determinism vs. indeterminism without catching up on several decades of QM. (This is a ridiculously high level view of QM. I give this disclaimer because I am sure my natural English approach has technical inaccuracies. I recommend learning the math from a professional teacher to correct any misconceptions I may cause.) QM models everything as a "waveform," which is nice and clean for individual particles, but gets messy quickly as the particles interact. It turns out that its very hard to measure the waveform; the mere act of measuring it changes its state. The fundamental limit of "classical" measurement is the uncertainty principle. Consider this thought experiment: Pluck a string in a dark room. Take a flash picture of it, and develop that picture. You will see the string in some bent shape... whatever position it happened to be in when the flash went off. You cannot simultaneously measure its amplitude and its phase. It could be at a low amplitude, but at 90degrees, where the string is as outstretched as it will get. Or it could be a very high amplitude, but at a much lower phase angle, where the string has a velocity. Now if we wanted to get more information, we could take a second picture, and do some math to determine how the string had to vibrate to satisfy both pictures. We could even take 4 or 5 pictures and get an even more confident answer, watching the string vibrate. We could eventually measure both the amplitude of the wave, and its phase. Then we could do all sorts of cool things. Radar is built on this principle. At the quantum level, things get hairy. Consider photographing a string so tiny that the mere energy of the strobe disrupts its motion, like a gust of wind blowing out a candleflame. This is going to be harder. Each time we photograph the string, we disrupt it enough that the information in the photograph ceases to be very helpful. When we take multiple photos, we find each additional photo is not adding any more information! (As for why this happens, learn the real math. It's not just an empty claim; it is a well recognized effect of QM that frustrated many a deterministic scientist!) What if we turned the strobe down? What if we made it weaker so that it doesn't knock the string around so much? What if we just had a quiet lamp generating light in the corner? Now we wouldn't disrupt the string, but we need a much larger exposure time to get some information. As a result, we can easily see the amplitude, but we lost track of the phase, as it blurred together. This is the Heisenberg uncertainty principle there is a limit to how much you can know both the position and velocity of an object. The strobe version could give position, but not velocity. The bulb version could give velocity, but poor position information. There are an endless set of options in between, but none of them violate a fundamental limit. (This is not just abstract philosophy. These results have been observed many times, and are highly accredited). So, back to our problem of determinism. QM does not actually defend nor refute determinism. However, it does have some harsh things to say about its limitations. These arguments are known as "interpretations," because none of them refute the other. They just look at the problem in different ways. We'll start with your least favorite, and move towards what I believe will be your favorite.