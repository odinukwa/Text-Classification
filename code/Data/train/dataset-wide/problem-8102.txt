yielding $20\times 20=400$ different size rectangles if orientation matters. Orientation matters if a vertical {1x20}-sized rectangle is considered as being different from the horizontal {20x1}-sized rectangle. If orientation does not matter, then in this manner of counting each rectangle is counted twice as an $m \times n$ rectangle and as an $n \times m$ rectangle, so divide that in half resulting in $200$ different dimensioned rectangles could be drawn, one at a time, into a region of 20x20, if orientation does not matter. There is also Scott Carnahan's approach in this question, which yields the answer $27$ using triangle numbers. If that is the correct answer, my answer above should match it, however, my answer is in the form of a heuristic rather than in closed form. 

My preference is detailed pseudocode, at a high-enough level of abstraction to allow understanding the algorithm. Of course, as pointed out by Ryan Budney's comment, it depends strongly on what the journal requirements are and in which journal you publish. However, I feel strongly that the complete code-set which you use should be available from some resource, either through the journal article's publsher, or from your own website, your academic website, or via Arxiv. If the pseudo-code is detailed enough to allow reimplementing the algorithm straightforwardly by another mathematician, then that should be sufficient. If the pseudo-code has to leave out certain details which are germane to the computation, then the interpreted code which implements the algorithm in a numerical computational package (such as Maple, Matlab, Sage, or Octave or Scilab (download link ) which are free open source software packages capable of running code similar to or equivalent to matlab) should be provided. Why not provide both? -- If you can provide a link to your own webpage for the paper, or for its supporting supplemental materials, I don't see why you couldn't provide both the interpreted code and the compileable C or C++ code on your webpage, unless there are copyright issues involved such as if you did not write all of the code yourself and do not have the right to release all of the code source. I am a supporter of free open-source software and the Gnu organization's GPL licensing, which would allow others to benefit from your code and to contribute back to it via incremental improvements. I suggest that you specify which version of software package, operating system, compiler, and/or library you used in running your program or in creating the binary application from your code. This is necessary because different versions of Octave (2.3 vs. 3.0) or Matlab (R10, R13, etc.) or any software package may implement or include different routines and may not be capable of correctly running your software program. I would recommend that if particular packages are necessary in order to run the interpreted code in Octave or Matlab that you list which packages they are. In the same vein, if your C or C++ code requires particular libraries such as LAPACK or BLAS, make sure to list them in a text file or in a header file. If you know how to use the make program, you can create a makefile to help others in compiling your software. The make program, the Gnu compiler collection, and many other development tools are all standard parts of Gnu/Linux distributions, such as Debian. My preference is detailed pseudocode, at a high-enough level of abstraction to allow understanding the algorithm. 

I would recommend Structure and Interpretation of Computer Programming by Hal Abelson from M.I.T. One of the first things I remember learning in SCHEME (a dialect of LISP) was that the $\lambda$-operator was a primitive operator to define a function and that you could write functions which could take functions as arguments and could return functions as results. You could, in fact, nest functions even deeper than this. An example: writing a function called power which when passed a value x returned a function which when passed a value would return the answer value^x. Thus, (defun 'squareit (power 2)) and (defun 'cubeit (power 3)), and (defun 'sqrtit (power 0.5)) were the simple ways to define functions such as square, cube, and square root. 

This is effectively creating the bipartite coloring of a graph $G=(V,E)$, if it is bipartite. There can be no bipartite coloring if there are odd length cycles in the graph which you are considering. You can check to see if a graph is bipartite in $O(|V|\cdot |E|)$, i.e. time proportional to the product of the number of edges and vertices. If your graph is a single component connected finite graph, start by pick any vertex as a starting point and assign it the distance $0$ (or equivalently the label $(-1)$. Then iterate the following two steps alternately until every vertex is labeled, or until you end up attempting to assign two different labels to the same vertex (i.e. you find two different paths to the same vertex which are not the same length modulo $2$. 

Joseph, the envelope (furthest reachable limit) of an unbiased random walk on an $n$-dimensional lattice at time step $t$ is the region containing the origin and $|d_1| + |d_2| + ... + |d_n| \le t$. So for $n=1$, that region is the line segment $-t \le x \le +t$ equivalent to $|x| \le t$. For $n=2$, the envelope region is the diamond-shaped area $|x| + |y| \le t$, or the region bounded by the four lines $x+y=1, x+y = -1, x-y=1, x-y=-1$ or equivalently, the four lines $y=x+1, y=x-1, y= (-x)+1, y= (-x)-1$. For $n=3$, the envelope region is the octohedral shape on the $\mathbb{Z}^3$-lattice contained within $|x|+|y|+|z| \le t$. The probability density region of the unbiased random walk in $n$-dimensions approaches the $n$-dimensional gaussian. So for $n=1$, the region of the envelope grows linearly as $t$, and for $n=2$, the region of the envelope grows proportionately to $t^2$, etc., growing proportionately to $t^n$ for $n$ dimensions. Once $n \gt 2$, the rate of the growth of the envelope rapidly overtakes the rate of the average distance traveled, and it becomes very unlikely that the unbiased random walker will return to the origin. That's how I've understood it to be. A reference off the top of my head would be Margulis and Toffoli's Cellular Automata Machine book from 1984 or 1985, as it gives a good description of cellular automata models of diffusion in $1$ and $2$-dimensions, and I believe in $3$-dimensions also, thought I am not certain. I believe that's where I remember reading about the "envelope"; and I remember running my own programmed simulations to draw the envelope and probability distributions for 1-d and 2-d. 

Lookj up "level sets". The wikipedia page "Point in Polygon" talks about algorithms that can be used when the polygon's coordinates are known. The proposed solution of the length of a path following the fence along the fence (call it $d_0$) and a path following along the fence but maintaining a constant distance of $x=1$ meter (call it $d_1$) will work for a robot that can do the tasks you're asking of it. However, the answer of the difference in length being $2 \pi \sim 6.28$ meters would only apply if the fence is perfectly circular. Given a map or diagram of the fence, generate multiple contours or level sets of points which are a constant distance from the fence. You'll end up with something that looks like a contour map or topographical map that the U.S. Geological surveys generates. Notice that for each distance $x$ (up to a certain limiting value), the level sets for $d_x$ may contain points inside the fence as well as outside the fence. Once $x$ is greater than the radius of the circle, the level sets for $d_x$ such that $x \gt r$ will only contain points outside the circle. For fences with concavities (like a pinched figure 8), the inner level set may break up into multiple non-connected paths. If the fence is square, width edge length $2r$, then the close-fence contour $d_0$ will be $4 \times 2r = 8r$, whereas the 1-meter level set will be 

Following from Beno√Æt Kloeckner's comment above, Place the points at $A=(0,0)$ at the origin, $B=(c,0)$ on the x-axis with the distance $|AB|=c$, and $C=(x,y)$, where we now want to satisfy $|AC|=b$ and $|BC|=a$. Simple application of the Pythagorean theorem leads to $x^2+y^2 = b^2$ and $(x-c)^2+y^2 = a^2$ as the two constraints to be applied. Expanding and subtracting the two equations: $x^2-2cx+c^2+y^2=a^2$ and $x^2 + y^2 =b^2$ $2cx-c^2=b^2-a^2$ $2cx = (b^2-a^2+c^2)$ $x = \frac{b^2-a^2+c^2}{2c}$ Now you can define $y$ in terms of $x$. Simply scale the points $\vec{A}=(0,0), \vec{B}=(0,c)$, and $\vec{C}=(x,y)$ by their respective $(u,v,w)$ barycentric coordinates to get $D=(x_D,y_D)$ as a function of $a,b,c,u,v,w$, apply the Pythagorean theorem again to get $d = |\vec{D}|$ = the square root of $(x_d)^2 + (y_d)^2$. This last step shouldn't need to be spelled out for you, but $\vec{D}=u\vec{A}+v\vec{B}+w\vec{C}$ 

The question is really asking about cross-correlation using matched filters. The known classifiers for objects in the set $S={s_1,s_2,... , s_x}$ are $m \times n$ matrices or subimages of sizes $m_i \times n_i$ for each object $s_i$. So now, $m$ and $n$ are no longer necessarily the same for the different objects being classified. The matched filters can be convolved in 2-dimensions one at a time with the image $M$ and peaks ascertained in order to find candidate locations in which the target objects $s_i$ occur. Once you have the cross-correlation, you look for local maxima in order to select the candidate locations. If there is any redundancy between the targets to be classified (e.g. $s_a$ and $s_b$ are very similar looking), then you might be able to get around having to do cross-correlation for each target with the entirety of the image. But if there are no similarities between the different targets to be detected, it is very likely that there is no better solution than doing full convolutions and cross-correlations for each object classifer $s_i$ over the image $M$. 

You are asking about the embedding of a graph structure into 3-space $\mathbb{R}^3$. A graph structure by itself does not specify its embedding into $n$-space. In chemistry, these two different chiral instances of (tetrahedral) molecules below would be called stereo-isomers or enantiomers of each other. In mechanical engineering, you'd be talking about building trusses and support structures, and a lot is known about the fact that quadrilaterals do not define a rigid structure. Quadrilaterals are easily sheared within a plane, and are not restricted to being coplanar, whereas triangular faces are at least limited to being coplanar. Also, the presence of these constraints (on edge length and vertex-edge connectivity) also does not mean that it would be impossible to build partial structures that meet the specified partial constraints but which cannot be built upon to complete the structure. In other words, a "naive constructor" cold generate a partial assembly which is a configuration which is impossible to continue onto a final desired construction. There could be dead-end partial constructions which could not be completed. This type of problem could partially be avoided by also imposing a temporal constraint, or a sequence constraint, e.g. first add this, then add that. However, there are chirality issues in play which cannot be avoided. If the "vertices" do not impose restrictions on relative angles, then there are no additional contraints beyond edge-length, and the graph-structure and edge lengths will not usually define a single embedding in 3-space, relative to transformations such as translation and rotation. If by topology, you do not also mean chirality, you may be correct. If you allow chirality differences to mean something, then there is a simple counterexample in the tetrahedron. Let this tetrahedron $T_1$ in $\mathbb{R}^3$ be defined with a base triangle $ABC$ with the points $A=(0,0,0), B=(0,1,0), C=(1,0,0)$ and the top of the tetrahedron at $D=(0,0,1)$. Let the edge lengths of the skeleton of this polytope be defined based on this baseline instantiation in 3-space, $|AB|=1, |AC|=1, |BC|=\sqrt{2}, |AD|=1, |BD|=\sqrt{2}, |CD|=\sqrt{2}$. Now note that if $D$ is instead placed at $D_2=(0,0,-1)$, that the this alternate tetrahedron (let's call it $T_2=ABCD_2$), has the same edge lengths as $T_1$, but has the mirror chirality. If we labeled the vertices with $A,B,C,D$, it is not possible to rotate and translate $T_1$ into $T_2$, whereas it is possible to turn $T_1$ inside-out and transform it into $T_2$. If you don't have all triangular faces, e.g. you use the edge lengths of a cube as the only constraints on a skeleton of a cube, you'll quickly see the problem that engineers found in constructing trusses with square faces: parallelograms are not necessarily "rigid" and can be sheared easily and still maintain the correct edge-lengths between vertices. Thus it's not possible to build a rigid skelton with only square faces. Thus, it depends on the axiomatic construction of your objects: if you disallow disassembly and reconstruction, then the tetrahedra $T_1$ and $T_2$ are separate chiral mirror-images of each other. If you allow for disassembly and reconstruction, then $T_1$ and $T_2$ have the same topology. If you also define "topologically equivalent" to allow for elastic stretching (at least for transforming from one 3-d realization to another, then back to being solid and rigid while in a specific 3-d realization), then $T_1$ can be transformed into $T_2$ by pushing the vertex $D$ through the center of the face $ABC$ and onto the other side. If the faces actually have a physical planar object defining that face (like a kite has its tissue paper), then this sort of transform is disallowed and the mirror image tetrahedra $T_1$ and $T_2$ are different. You can also visualize this by allowing the edges to be made of elastic springy rods with spring constants $k_i$. If the $k$'s are very large, then the springs are very stiff and the inversion will be impossible; if the $k$'s are small, the springs have a lot of give and it's easily possible to change between the two mirror-image configurations.