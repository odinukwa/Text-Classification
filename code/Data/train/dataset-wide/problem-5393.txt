I didn't read B as intending to slight C. This pattern of reasoning seems legitimate when we have epistemic requirements for certain social roles — basically, people whose job is to know certain things. Consider this exchange: Person G: I have a cold. Will taking Vitamin C make me feel better? Person H: J is a doctor; she should know. Person J: I have no idea whether taking Vitamin C will help you feel better, G. Person H: Oh. How disappointing. J occupies a certain social role, namely, a doctor. Because J occupies this social role, is expected or socially required to know certain things, such as appropriate treatments for common illnesses. But J seems to fail to satisfy this requirement. So H's disappointment seems justified. In the A-B-C exchange, it doesn't seem like C occupies any relevant social role, so this reasoning wouldn't apply there. 

The first reading treats social construction claims as ontological claims, about what kinds of things exist and why. Ontological readings of "the laws of physics are social conventions" might infer that gravity does not exist at all (in other words, it's a fiction), or it exists but only because social systems decree that it exists (like money). The second reading treats social construction claims as epistemological claims, about our knowledge of the world. These readings would emphasize social contingency in the development of our knowledge. For example, the idea of laws of nature comes out of Abrahamic religion (Judaism, Christianity, and Islam): the laws of nature are rules laid down by God, the ultimate law-giver. If Christian-European societies hadn't dominated scientific development over the last 300 years, then we might not understand physics in terms of laws of nature. Similarly, a number of professional philosophers of science today (none of whom remotely resemble Lyotard or Irigaray) argue that the metaphor of laws of nature is counterproductive and misleading, even in physics, and suggest alternative conceptual frameworks for scientific knowledge. This wouldn't mean that we wouldn't fall if we walked out of Sokal's apartment window if China had dominated scientific development rather than Europe, or if we understood physics in terms of causal powers rather than laws. But it would mean that we would represent the phenomena of falling differently. Sokal's criticism is only relevant to ontological readings of social construction, not epistemological readings. 

There are a number of strategies that scientists and philosophers of science have used to avoid (or at least decrease) tensions between religion and scientific naturalism. You can read more about several of these in the Stanford Encyclopedia entry on religion and science. As a preliminary point, it's important to recognize that the conflict thesis — the idea that there's a deep and longstanding tension between religion and science — was a specifically nineteenth century development, and it's generally not regarded as good history by historians of science today. (More on Wikipedia.) That said, here are five argument strategies to avoid the apparent conflict, in no particular order. 

I'll run with Mauro ALLEGRANZA's example of gene editing to treat cancer. In the first stage, philosophers can critically identify questionable assumptions or framings of research questions. For example, the idea that we can use gene editing to treat cancer might assume that cancer is "fundamentally" a molecular biology process. This is a reductionist way of thinking about cancer. We can also view cancer as a tissue-level process (the formation of cancerous tissues) or an ecological process (exposures to toxic chemicals cause a lot of cancers). Or, indeed, we can view cancer as process that unfolds on all three of these levels. These different framings might lead us to pose different research questions. When designing studies, scientists must build a "ladder" of procedures and data, connecting theoretical concepts (cancer, genes, "gene editing") to the kinds of things we directly observe (Manhattan plots, microscope images). Here again, philosophers can critically identify questionable methodological and conceptual assumptions. For example, philosophers might argue that conventional genomics methods don't allow us discover disease mechanisms. That is, roughly, these methods give us correlation rather than causation, and so have limited clinical value. Philosophers can also point out that different researchers operationalize central concepts or phenomena in different ways. For the third stage, a large body of recent work on "inductive risk" shows that ethical and political values play an essential role in the interpretation of data. Inductive risk directs us to consider the ethical significance of false positive and false negative errors. If we're doing a very rough, proof-of-concept gene editing study, we can probably tolerate very significant errors in our analysis and interpretation. But if we're trying to decide whether clinical trials provide sufficient evidence that a treatment is safe and effective, our standards should be much higher in general. And some kinds of errors (the treatment turns out not to be safe after all) are much more significant than others (the treatment turns out to be more effective than we thought). Finally, with several studies completed, scientists need to synthesize the results and draw an overall conclusion. Many scientists argue that we should do this using meta-analysis. But philosophers have argued that meta-analysis is an extremely messy and contingent process, suggesting that it's much less reliable or impartial than its proponents claim. In addition, meta-analysis isn't well-suited to dealing with evidence from many different kinds of studies, and doesn't give us the kind of knowledge we need to extrapolate across cases. 

(I'm going to circle back to explanation in the second half of this answer.) quen_tin's answer is about a disagreement over the virtues of representational knowledge: do these representations have to be true, or is empirical adequacy good enough? My dissertation was about a disagreement over the relationship between representational knowledge, on the one hand, and practical knowledge and technology, on the other. On what I call the narrow view, representation takes precedence over the other two. Science, in the strict sense, is about producing good representations of the world (whether "good" means "true" or "empirically adequate" or something else); practical knowledge and technology are merely "applied science" or "engineering" based on those good representations. The narrow view is closely related to the linear model of innovation (see here), and like the linear model appears to have developed or been widely adopted during the Cold War. Until a couple decades ago, the narrow view dominated academic philosophy of science; however, things are less settled today. The narrow view contrasts with the broad view. According to the broad view, the three kinds of goods are equally important and interdependent. Practical knowledge and technology don't belong to some subordinate stage of innovation; science, in the strict sense, includes both "pure" and "applied" science, as well as engineering. To illustrate the broad view, think of academic robotics researchers. These researchers generally aren't building whole commercially-useful robots; instead, more often they're working on one small scientific problem — how to get a bipedal robot to navigate rocky terrain, say. This is foundational research; but it's aimed at technology development, not a true or even empirically adequate representation of the world. How does explanation fit into this picture of the narrow view vs. the broad view? First, note that there are many different philosophical accounts of explanation (see the Stanford Encyclopedia article). Among philosophers of science today, I think there's general agreement that (a) explanations support human understanding of phenomena (somehow), and (b) explanations have some kind of representational adequacy conditions. Maybe explanations don't have to be strictly true or even empirically adequate (see here), but in order to support "genuine understanding" they do have to avoid being "deceptive." But "understanding" is ambiguous, and I suggest that there are two senses of "understanding," corresponding to the narrow and broad view. On the broad view, understanding why things behave the way they do is useful for predicting that doing X will cause Y to happen (supporting the development of know-how) or diagnosing why technology A isn't behaving as expected in environment B. Explanations support a kind of practical understanding, and thereby connect representational knowledge with practical knowledge and technology. This means that explanation is quite valuable on the broad view. Indeed, insofar as it can be nearly impossible to develop sophisticated know-how or technology without some degree of understanding, explanations are necessary for science to achieve its goals. (For a very readable example of the broad view, and specifically the importance of understanding, I recommend Cartwright and Hardie's Evidence-Based Policy.) But the narrow view either (a) can't accept this account of understanding and explanation, or (b) concludes that it's less valuable than representation by itself. Understanding and explanation are useful for producing practical knowledge and technology. But, on the narrow view, those are the goods of applied science and engineering, not science in the pure or strict sense. So either (a) the narrow view needs a different account of explanation, that connects it to representation; or (b) it concludes that explanation isn't that valuable. To go back to quen_tin's answer, the realist who defends inference to the best explanation takes option (a): explanations are valuable because they help us reach the goal of true representations. Van Fraassen explicitly chooses option (b). His book The Scientific Image has a whole chapter on "The Pragmatics of Explanation," which is a beautiful discussion of the relationship between explanation and understanding. But, based on this connection to understanding, van Fraassen concludes that explanation is basically irrelevant to science, because the primary aim of science is empirically adequate representation. As quen_tin puts it, "But in any case [explanations] do not constitute the aim of science, rather, for empiricists, a byproduct, or, for realists, an heuristic to achieve its true aim, which is to produce true or approximately true theories." This is why I think quen_tin's answer — again, which was good as far as it went — missed the deeper disagreement. Realists and empiricists share the assumptions of the narrow view, which has trouble accounting for the value of explanation and understanding. But philosophers taking the broad view see explanation and understanding as extremely important. 

A probability of 0 is not the same as logical or metaphysical impossibility. In your urn example, for any sample size N, it's logically possible to draw a sample where all N balls are black, whatever the probability of drawing a white ball. In the same way, (given that it's possible to draw an infinite sample) it's logically possible to draw an infinite sample where every ball is black. This event has probability 0, but it's still possible. (I think this is similar to H Walters' point.) However, certainty isn't the same thing as possibility. Many academic philosophers think it's reasonable to be certain that probability 0 events will not happen even when those events are possible. Or, equivalently, it's reasonable to be certain than probability 1 events will happen even when they're logically or metaphysically contingent. The IPCC — the international body that produces climate change assessments — used the term "virtually certain" to characterize findings that have probability 99%-100% (link). 

Philosopher of science Kyle Stanford calls this the problem of unconceived alternatives: we may have developed a scientific theory that fits well with the evidence available to us now; but we don't know whether some other scientific theory that we just haven't thought of yet might work better. It seems like this should reduce our confidence in our current scientific theories. Stanford's argument has been the focus of recent work on scientific realism. 

The "science of sex differences" that you point to is itself highly controversial. First, many sex differences claims are unstable: they show up if you measure things one way, but not if you measure things in a slightly different way. Chapter 5 of Anne Fausto-Sterling's Sexing the Body shows this with corpus callosum research: there are sex-linked differences in the size of the part of the brain called the corpus callosum, but only if you measure "size" in a certain way. And Helen Longino's Studying Human Behavior is a detailed look at the variety of incompatible ways scientists try to study aggression. Second, many sex differences claims are difficult to interpret. Suppose that there are sex-linked differences in regions of the brain used in working memory. (I'm pulling this example from one of the Wikipedia pages you linked to.) What does this mean in terms of women's abilities or interests? It's not at all obvious. Specifically, for many areas of sex-differences research, the research methods can't disentangle nature and nuture. Brain structure and gene expression patterns are the product of both nature and nurture; so even finding these kinds of sex-linked differences in adults doesn't tell us where and how they developed. So such patterns are not necessarily "innate" or "essential" differences. Third, linking sex differences to competence and interest requires gendered assumptions about what makes for competence and interest. Suppose we grant for the sake of argument the stereotype that women tend to have better people skills and men tend to have better abstract reasoning skills. (I want to stress that this is indeed a gross stereotype. Even the distinction between "people skills" and "abstract reasoning skills" is questionable.) It might seem to follow that men will tend to be more competent and interested in IT work than women. However, this assumes that understanding mathematical systems is more important to IT work than understanding social systems. Yet that is manifestly not the case: IT specialists frequently work in complex teams and need to work with clients to develop and carry out projects. In addition, in many areas — like social media and AI — IT specialists need to understand the social dimensions of the problem they're trying to address or system they're trying to develop. So an understanding of social systems is extremely important for IT.