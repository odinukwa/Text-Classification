However, I have been changing some schedules, adding extra ad hoc runs, as per the script below: (don't run this script below, it is just for illustration - you don't have the same jobs that I do here) 

I have a process(es) hungry for tempdb, but I am struggling to identify this process. any ways I could achieve this? 

Missing Join Predicate the Missing Join Predicate has been discussed here and Here too. Missing Column Statistics I have fixed it, or at least got rid of the warning by simply running 

Thomas Kejser answer this topic well above. I just thought about adding 2 cents. I have seen some filtered indexes only being used (shown in the execution plan) when you exact match the where clause in your query as the where in the filtered index. have you tried to use indexed views? sparse columns? I believe that as far as you have only inner joints you can create an indexed view containing the where clause(s) of your filtered indexes and then you could use the view instead. There could be more than one view. But same as with the non clustered indexes, too many will slow your writing down. In my experience you would have good gains in reading but you would have to monitor writes (inserts and updates) specially if the tables are involved in replication. However, as I understand your main concern are therefore I would suggest you SPARSE columns in your indexes. Sparse columns are especially appropriate for filtered indexes As I have advertised sparse columns I would not feel well if I didnt tell you about its limitations too: 

You should be able to create a stored procedure that executes your query to get the table names. Then you loop over the results and execute your effective date query for each table individually. You might even be able to do this without wrapping it into an stored procedure. 

In general, 7 seconds is not to bad if you have a big report. It takes longer to get a coffee. Does it create any other issue than 6 seconds of wasted productivity for an end user? 

To determine whether it is an temp space or not, the is not suitable, since it is 6 for all of the tablespaces. However the container type seems to be different for temp spaces. is 0 for temporary tablespaces and 6 for all other tablespaces. When I use the parameter, I get another piece of information. 

Can I use the type from this table? It seems to be 1 for temporary tablespaces and 2 for all other tablespaces. I tried to find documentation on the internet that explained the output of db2ckbkp but wasn't successful. Any help in this matter is appreciated. 

Another pitfall could be that the query changes. This can happen if the where clause changes (e.g. you filter by date) and you don't use bind Thinking about, you haven't stated that you checked whether the execution plan is still in cache or not. You should query the cache to figure that out. However, since you say your query runs only once a day, the plan might just be expired. The blog actually mentions the formula on how sql server determines when to expire a plan: 

For having a look at what I have got on my table I just run the following query on the same server and database: 

but so far I have failed to implement it. recursive CTEs should have 2 parts: anchor and UNION ALL. how would I apply these two in the current CTE? any hints? 

For my own records I have a table in one of my servers where I like to save my activities and scripts the table definition is: 

Questions: what is the methodology (the process) we could follow to conclude which one of the queries above is better for this particular situation? Considering the first query needs less physical reads, but uses more CPU. what are the things to consider in order of relevance? physical reads or CPU? what else? 

I would like to drop the unique index, and alter the primary key so that it is CLUSTERED. I will keep the same primary key, just change it from non clustered to clustered. This table is part of transaction replication I would get this done on the subscriber database only.Not in the publisher. It is a table with over 9,293,193 rows. 

When I then run the query, I notice it takes longer than the original without index. the new query plan, after creating the index is as follows: 

I was doing a load test by sending the same web request 5000 times one after the other. Here are the total data transfer amounts I observed: From SQL Server to SAN: 6 MB From SAN back to SQL Server: 1.5 MB From SQL Server back to IIS Server: 71 MB And here are the mdf and ldf file size changes in full recovery mode: mdf size increase: 1 MB ldf size increase: 4 MB As you can notice, despite the 71 MB returned to IIS Server, the total data from SAN to SQL Server was 1.5 MB. After doing some research, I figured that this was due to the SQL Server caching data and not hitting the disk for each request. So, in order to disable this behavior, I used after each request. Here are the results after this change: From SQL Server to SAN: 390 MB From SAN back to SQL Server: 11 GB From SQL Server back to IIS Server: 71 MB mdf size increase: 1 MB ldf size increase: 30 MB My questions are: 1. What is the reason that data transfer from SQL Server to SAN increased from 6 MB to 390 MB? Shouldn't disabling caching have only affected reading from the disk, not writing to it? 2. Data from SAN to SQL Server became 11 GB. Is this a better indicator of the production environment rather than the 1.5 MB? 3. Data from SAN to SQL Server became 11 GB. And I wonder why SQL Server needs to fetch that much data from disk even though it is only returning 71 MB back to the IIS Server. Since I am using appropriate indexes, I assume SQL Server shouldn't be caching a lot of pages. 4. ldf size increase in cache disabled case is almost 8 times the increase in the cached case. Why is this? 

The format in the database is decimal(18,9). Any suggestions on how to avoid this error without manually changing values in the source file? To put it into perspective. The CSV file contains more than 2.2 million rows with 154 columns each. Which results in a CSV file size of more than 2GB. Currently I am working with a test file. When the final go live comes. I need to switch over fast. Which means I can not analyze and edit the file for several days. Update I played around with the values a little bit. 

AFAIK, you cannot force a plan to stay in cache. However, a query can be thrown out of the cache for several reasons. Read a blog about execution plans. It states some reasons why execution plans get invalidated: 

I am trying to move data from a csv file into a SQL server database. Some of my values are in the scientific notation. I figured out on how to get most of them converted but for one value I get the Arithmetic overflow error. The value that is causing the error is . If I change the part before the E by removing the 1 so it reads the import works fine. All the other values I need to import work fine. I use a format file to import the data. Below the line for the column that is giving me grief: