The "log" agent item checks the whole log by default, you must tell it to use the "skip" mode to avoid processing of older data. Simply add a ",skip" to the end of your parameters to the log item. Check carefully this page in the documententation. 

How is it supposed to resolve the domain debian70.vm? looks to me you're using bookings@debian70.vm as the sender address. The spam check is done over debian70.vm, which can't get resolved. 

Caching would use memory anyway (and even Apache can do that), so what's the advantage with that? At most to improve performances you can set nginx to serve the static content, while proxying instead to Apache for dynamic content. Of course if: 

The first part of the initial code fails because the condition is verified again the resource Arn, so you're essentially denying access to all queues. The second part should work, but an explicit access denyal always has precedence. The "policy 2" is wrong as again you'd give access to everybody but the user you need to give access to. But the global deny you used above still has precedence so no effect at all. I think your problem is at the very base of your current IAM settings - you should have never given such a wide access to your systems to all of your users. IAM works on an "allow what you strictly need" basis which is best for security. Instead you got youself tied into a "deny everything but what you need" which is hardly maintenable in IAM because of the "deny" statements always winning. The workaround for now is what you did - apply a deny policy to a group, and apply that group to all the users who shouldn't have access. But, as more queues might be created, you'll find yourself into a never ending loop of deny policies where you'll need to explicity set the names of all of your queues in the deny policies. And still - will that be secure or make sense? You said You've given to many users an administrator role - if they can modify their IAM policies, they can do everything - cloudtrail might tell you what happened, but won't bring your data back. I'd seriously suggest you to take a different approach instead, and define much more limited policies for your users. Create extra policies as needed and make them so what they are cumulative, attach them to groups and from the groups to the users. Take advantage of the wildcard on the arn values to eventually create "reserved" namespaces for your resources with more restricted access. 

This is an idea using iptable's string module to check the http header content, of course it must be adapted to your special case: $URL$ However I'm not really sure about what would be faster between inspecting all of the incoming packets and having your webserver handle them. I'd eventually instead just add a line in the server configuration to immediately return a 404 on that specific url; traffic will self decrease with time, as webmasters and search engines correct their links (and they will, as 404 errors would cause a bad positioning in modern web search engines). 

They seem to be both indicating a RAID 10 to me. Please have a look at $URL$ from page 13 for a nice description of what you got as output from megacli. 

That functionality needs to be supported both by the hardware platform, and by the OS. In your case, the HP Proliant DL385 g7 manual clearly states that a full power off is needed to install new memory modules, and as such I wouldn't even care to check for ESXI support to the functionality. 

The RDP connection should come back shortly after the network disconnects, so you will just have to reconnect, I don't see the problem, your desktop session will remain open. Actually if the blackout is as quick as it should, RD will probbaly reconnect for you. If you want anyway you can also install Virtualbox as an unattended installation, as said here on the virtualbox manual following Microsoft's instructions. 

RAM/CPU? Clustering? Load balancing? Only you can know. I've seen RAM occupation vary from a few megabytes to several hundreds, even for what seemed to be simple applications. A lot depends on what libraries you'll be loading. Same for CPU. There is no substiture for profiling a web application's performances. Start from a sample configuration, get some workload test software like JMeter (but there are many, the choice might depend on what your webservice does), create a decently average weight testing script and launch 50, 100, 150 threads. Keep your java virtual machine monitored (one free tool might be javamelody, and is both free and lightweight, but there are others) and check your logs if it crashes. Then make your proper calculations based on the response times and the machine's status when under load. 

Items are "entries". The "path" to an item is its DN, or Distinguished Name. References: Relative article on Microsoft's MDSN Link from RHEL7 documentation 

There is no real solution. The best things you can do for now is to enable both SPF and DKIM for your domain/mail server, but google's filter is still pretty strict. 

No need to boot up with a live cd as suggested by @Mr Shunz, since the system is working. Just restart the server, in the boot manager select to edit the boot parameters and add an to the end. The system will boot in runlevel 1 with no network and the root user already logged in. Eventually disable system management programs from starting up automatically at startup, so that things like puppet or chef won't eventually enforce particular users or passwords on the system. Change the root password, restart and then do your investigation on what's on the system using the root user. 

KeepAlive normally makes a lot of sense, requires more memory but lowers the number of connections, CPU usage and connections overhead. MaxClients and other stuff must be tuned to your situation instead. Normally you'll want Apache to have a number of idle instances big enough to serve all of your users without having it to spawn new child processes all the time; at the same time you want to avoid excessive values for maxclients in order to keep memory usage under control. Unless you have sudden usage spikes though, the default Apache settings are usually adequate and will self adapt well enough to your environment. If you serve content through https and since you're not using php you might want to give a try to mod_spdy module too, many browsers support that already (waiting for a global support for http 2.0). Finally another option is to use caching/precaching. In that regard some functionality is already available in Apache thanks to several modules (please see $URL$ ), or you can put something else in front of the server (Varnish is a popular option). 

You can modify directly the /etc/sysconfig/iptables file. Reload the iptables service to reload the rules from that file. Yet, as you were told already, firewalld is the new default firewall system for Centos, and this is a good chance to learn how to use it, don't you think? 

Speaking about virtualhosts, we had the same problem for a project of ours. The Apache's "ServerName" directive isn't optional, it's mandatory to use. Apache has no such thing as a "default" virtualhost to load when it has no ServerName match. And if virtualhost servernames overlap, well, that's just going to give you problems. In the end in those cases Apache just decides what to do depending on its configuration files loading order. And if you try to contact the server through an unspecified address (ip address, or something not mapped on the virtualhost) it will load the first virtualhost he got a configuration loaded for that address. The correct way to do it all is to have virtualhosts detailing all of the possible servernames in ServerName and ServerAlias directive. See name based virtual hosting on apache, and consider carefully the evidenced note "Main host goes away".