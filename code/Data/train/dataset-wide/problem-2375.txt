I can't kill this PID (nothing happens when I issue the kill command). I'm not sure what to do to diagnose this further and obviously resolve this. Any input anyone? Running PostgreSQL 8.4 on Ubuntu Linux server. EDIT: As I found other connections in a similar state where the commit was hanging, I looked further and found the following in the server logs: 

If you are referring to the base/pgsql_tmp, then you should be fine. But I don't speak from experience of having done that myself. The only gotcha is that you have to make sure that the location is accessible when your PostgreSQL server starts up. (Ref.: Book PostgreSQL 9.0 High Performance, page 93). The book in question refers to creating a simlink of the base/pgsql_tmp on the OS partition or other less "safe" partition (i.e. non-RAID or the like). 

Not an elegant solution but after installing the package using (which fails creating the cluster but installs PostgreSQL), I switched to the user and created the database using . Then back to , I created the cluster using the command. This moved the configurations to and set it all up. 

Code generators are great! Code generators are evil! 10-15 years ago, I would have said that having a code generator for quickly creating the boiler plate code for database driven applications would have been a great gift to mankind. 5-10 years ago, I would have said code generator sucks, they generate too much duplicate code and that having a data-driven user interface (where the fields on the screen, validation, etc is driven by meta-data in a database instead of coding the screens one by one) would have been a great gift to mankind that supplanted the code generators. Today I would say - write each screen individually. Use existing framework that wire fields and model objects and possibly ORM when doing simple CRUD. But do design each screen to the exact purpose of the screen. Application screens that mirror a RDMS table too much is only good for managing lookup tables. Don't annoy the user with geeky interface that are designed against a computer storage model (RDMS)... make a screen that has only what they need. That may mean that it will save to multiple tables, etc. Who cares. The user isn't a database. So my thought? Don't waste your time making a code generator. 

I agree that the archive logs can be deleted when they are beyond a set period of time. Doing this solved a similar space issue for a database I maintain. However some thought about the "why" would be a good idea. If this is a development database and it is not performing the same way as production then you have an issue with your application stack. 1) Is production performing the same way but you have lots of disk space so you don't care? Or is this only on the development VM image? Just cleaning up log files solves the problem but not the root issue. 2) I have not noticed any differences for Oracle databases that have been virtualized. 3) Check the size of the .dbf files. Do they represent a significant portion of the space used? If so, then you may need to increase the VM disk size. 

Why go to all this extra work? This solution breaks your project into two issues, the upgrade and the character set. Upgrading databases is not to be considered lightly. Many times it is not the upgrade that is the problem but peripheral problems like interactions with applications or other databases. Custom PL/SQL code should have no problems but you never know. If there is a problem people will be looking to you to explain how you were diligent and careful not how you saved a day or two. 

I'm trying to install PostgreSQL 9.4.1 on Ubuntu 14.04.2. I'm using the packages from . The package installs but it fails running . If I run manually, I get: 

To answer your question on those 2 options, neither seem right to me. A) will lock you in and B) is a lot of work. The current schema you describe is not too bad (except for having the information name ("first name", "square foot", etc) as string instead of an ID referenced to a lookup table. However, this seems to me like a good candidate for a NoSQL database ( $URL$ ). While I never worked with such database, what you describe is a typical scenario that this solves. 

You should not have two collections, that will give you headache. Have a user collection which has admin and regular users. You can then query users as a whole or look for a given type only. So if you are looking for user id x you don't have two search two collections. As for the question on duplicate ids with 2 collections, it simply depends how the ids are generated. You say your ORM does it... so you will need to check the doc on how it does it. 

The answer to your question is that yes, you can, provided that the users will have the necessary rights to update the time on their local machine. In your access front end, you would write some code to get the date/time from the SQL Server as in "select getdate()" and then you could use that to update the local time. But, that's not really the correct solution. The correct solution is to have all your computers and servers keeping their time right against an NTP server. And the other aspect of the correct way of doing that - because even then you don't have full control - is to do the timestamping in the query using getdate() or similar so that it use the SQL server date/time rather than the client. 

and moving the job to DBMS_SCHEDULER. Now, when it runs the correct (local) time is entered in the logging table. 

You can add a lot of value to your career at little or no cost except for some of your time. You need a desktop computer or server with at least 4 GB of ram. Download Oracle virtual box or pick a pre configured image here. Install and configure.I learned more by breaking things than by reading in books or videos so don't be afraid to break your database repeatedly as you figure how to use the tools of the trade. With virtual machines you can take a snapshot, break the image and revert to the saved snapshot. Once you have installed the database you can try adding a schema of your design or play with Oracle APEX to get a quick idea about web applications. All the Fusion applications are available for download like BEA weblogic and you can play with some Hello world apps in Java. Then try and duplicate some of the things you need to do at work like backup, tuning, RMAN and anything else you need to practice on. It won't be easy but it will reward you with hands on experience which will give you confidence for work tasks. 

1) When to be sure that your database design is perfect? Your design is never perfect because the business logic and amount of data is always changing. Perfect is difficult to define I've seen systems that were great on deployment but had poor performance after a few years of data were added. The regrettable trend to treat a database like a black box by some application developers means some databases are deployed with critical tables lacking primary keys or indexes. Perfect to the CIO because they got the application delivered on time and on budget could be a pain in the butt to the developer/ DBA who has to deal with the problems. Here are some of indicators I would look for which indicate your design is ready to go a) Extensive use of primary keys, unique keys, foreign keys, indexes, more so on the larger tables ( I only mention this because I've seen commercial products which lack this) b) Application logic is duplicated as far as is practical in the database with the use of constraints and default values. You cannot capture everything but just knowing that there will always be a value for an entry provides peace of mind. c) Test, test, test: test from the user perspective on data entry, test from the manager perspective who wants an overview, test from the analyst perspective who wants to see trends If you find yourself joining 9 large tables which require full table scans in order to find out what work is assigned to a user then maybe you need to reexamine things. Not everything can be simple but excessive complexity to answer user needs is a hint of trouble to come. d) do some daydreaming on how the database could and could not be extended. If I need to add a new property to a unit of work how hard is it? Can it be done without table changes? If you are asked to add a new type of work how hard is it. (Work could be a product, a transaction, a case) e) people and organizations provide hours of work for me. How easy is it to create, edit, de-duplicate and report on them? f) how many user languages are you supporting? What character sets will be required. If, for example, you intend to support English and Spanish, what happens if the design must be extended to cover French and Italian? 2) Is returning to the data base design to change some issues (like adding new column, delete a column or change data type or add new table or ....) considered as a bad practice or is it normal? I would say it is normal for an application where the business logic changes frequently or the end user requirements are being added to. 

You could write a procedure with the EXECUTE and build the statement in the procedure by looping over the list of schema which match your query from the information_schema tables. Then you can call your procedure from psql or other passing it your criteria such as 'ceu_shard_test_merge_%'. You could have a parameter to do it or just dry run and instead of execute then it could output the statements or something along those lines. 

Time for the backup to run is a reason for doing incremental backup on Progress. My backup is running fast enough that I did not need to use this function and I'm still doing only full backups. It also depends on your requirements. For example, if you have heavy financial transactions and you want to keep a backup every hour or something like that, you would need to do incremental (or real time). But unless you have something forcing you to do incremental, I would do full backup, I find that easier to restore. In terms of performance impact, if you have a fast disk array, I haven't seen much impact of doing a backup even during moderately heavy usage. Obviously it depends also on the size of your system. I'm talking about a 37Gb DB. 

Since you are a programmer, not a DBA, I would recommend the correct programmer's way to do such things. Don't update the database directly - never. Maintain all your schema changes, and base data changes in files which are then pushed with your release. I don't know what your language of choice is, but there are many tools to do this, Ruby has it's own tool - rake - with which you do that. And here are two others: 

This query runs on 11.1.0.7 Enterprise and provides similar results to the the OEM Grid performance page. Both require the diagnostics package. 

From this blog you can try this on development first... SQL> spool resolve.sql; SQL> select ‘alter java class “‘||object_name||’” resolve;’ 2 from user_objects 3 where object_type like ‘%JAVA%’; SQL> spool off; SQL> @resolve The clean sweep approach would be to remove and reinstall the java JVM The Java VM is created and populated with system classes during CREATE OR REPLACE JAVA SYSTEM command. Run rmjvm.sql to remove the JVM first, then bounce the database then initjvm.sql I repeat, these should be tried on your development stack first! 

this is a clean design but if each type of thing generates ten or twenty log entries per thing you don't have to do too much to have a huge table for application_logging with a million entries. Users complain it's slow to see the activity log. The real question is: 

I believe this will do the job but the databases I have access to at the moment do not have text indexes. I will confirm later today. 

I did not find an elegant way to solve this question but adding a check constraint on the "Owner" column of the metadata tables accomplished the objective of ensuring that only schema owners would be listed. If a new schema owner is added then all the check constraints would have to be changed but that is not too onerous. I tip my hat to Phil for confirming that backing away slowly from system tables is the best approach and wish he had posted this as an answer. 

PID 9593 is the most problematic one which other users get blocked by this one. As far as the user is admitting to, he truncated his table, then did inserts in batches of 1,000 committing after each batches. Currently this PID shows the following locks: 

I can't find any details as to why fails. If I manually run as the user, that works, but that does not create the config files. I checked everything I could think of, including on-line and can't find a solution. 

I have since upgraded to version 9.4 and a whole new server so I can't debug this further. But I believe the problem was with a drive. I found a bad drive which wasn't reported as bad by the machine. 

The simplest - shell script running psql commands :) Otherwise, any high-level language can be used for the sake of experiment to learn database - ruby, python, java, etc. 

Yes, that would be fine if your design goal is that Persons have only 1 Addresses. With this design, each Persons can have 1 Addresses but two or more Persons can have the same Addresses. It's all a matter of your business needs. If the above is what you are trying to get, then yes, it is correct. However, I think it's most common the other way around where the Addresses would have a foreign key to the Persons because a Persons could have more than one Addresses. As for your constraint to check the postal code - well first off you are missing the space and it's lower case. Whether that will work will depend on which database system you are using. I tested it with PostgreSQL and it does not work. I don't think you can really have such a simple constraint to fully validate a Canadian postal code. For example, there are some letters and some numbers which are never used. I'm a bit rusty on my Canadian postal office codes but I seem to recall that number 5 is never used as it's too similar to S, etc. 

you have not shown the roles TEST_USER is granted. Likely they have been granted the DBA role which allows ALTER USER. see the documentation for details. This query will show the roles a user has 

My opinion is that unless there are performance or other application defects that are only seen on the production stack then access should not be granted on production. A view of the v$ views you mention on development should be fine. Security procedures vary wildly from company to company and factors that are external to Oracle security are more often than not the deciding factors. In short, only give access when you are satisfied that there is a real documented need that is supported by management. In answer to the OP's question of what can you do with v$session I didn't know this till I looked for it but here is an example of what you can do with v$session. That's the thing about security, it's not what you can imagine people can do with information that is dangerous. It's what you don't know about. To a point, less access to interesting system information is more security. 

You should look up the CASE construct. Adrian's article is quite helpful. The Oracle documentation is here. With the kind of query you want to do I can't help but think there are some omissions in the database schema.. Surely you can add some constraints on obj_C and obj_D so that null is not allowed? This would simplify things. 

Every table should have a primary key (I really can't think of a reason not to have one). So having a paymentID column to your payment table is definitely a standard design. 

Your best place would be pgfoundry. But you won't find much. Doesn't look like many plugins are available. 

As Zoltan pointed out, unless you have MANY millions of rows, I don't see a scaling issue. There are also many libraries for scheduling things such as Quartz on Java for example. These will store the recurring schedule as a cron-like expression. Because your example above has a flaw, if the recurrence is every Monday, then it's . So you can store a date, or a recurrence pattern. 

I'm running into an issue whereby I have two connections from a user to my PostgreSQL server which have been running for about 4 hours and have been in a commit state for quite some time (at least 1 hour that I have been watching it). These connections are blocking other queries from running but themselves aren't blocked. Here are the two connections in question. 

I simply removed the + signs and replaced with ||. On the EXECUTE I removed the format and simply concatenated the strings. 

A database server running Postgres 9.4 shows no statistics when running - well only the is non-zero. The is null. I haven't been able to find much as to what should be checked in this case, to find out why statistics aren't reported. and are both set to . Per the manual: