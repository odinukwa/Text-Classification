This will cause the BULK INSERT to process 1000 rows per transaction, commit the transaction, then start the next transaction until the file is processed. Of course, you can change the 1000 to any number you choose. If you are importing data from another database (either on the same server or across a linked server) then you may need to write some looping code to insert 1000 rows (or 1 row, if that is what you want) at a time. 

When the first job starts it could set all columns to 0. Then as job one finishes it would "UPDATE dbo.JobGroupStatus SET Job1 = 1", then Job2 when it finishes would update Job2, etc. Create a trigger on that table that discerns when all jobs have finished. 

Excel specifically provides a message: Dates and times that are negative or too large are displayed as ######. So, that is a Excel limit. Further, you can type in strings that look like dates, but Excel may not think of them as dates. For example, if I make a column of date type and enter into the columns -53689, 01/01/1753, and 01/01/1900 (using my settings) I get this: 

If the OUW is all zeros 00000000-0000- etc then you need a different approach to get rid of the SPID -2. I have notes saying: 

Once you have the snapshot created, make your modifications in the designer and then save your scripts. If all is well, you get rid of the snapshot: 

You did not mention which database you are using, however trying to add a column that already exists will produce an error. All the databases that I have worked with require a column name to be unique within a table. (No room for two abcd_nid columns.) To update the column you use the standard UPDATE command. 

The first ALTER may not be necessary, but it helps prevent someone else getting in as the "single" user. 

SSRS has a number of configuration files, including RSReportServer.config. Both Shared Schedules and Shared Data Sources are good things to create since they help you manage your configuration without giving each and every report it own schedule. View this as saving you some work. Of course, independent Schedules and Data Sources will likely be needed for some reports. Since you are concerned with overall load, note that the MaxQueueThreads configuration will control how many parallel reports can run at once. 0 means run as many as possible, but some other number (e.g. 3) will limit Reporting Services to run that number reports at one time. If you have 10 reports on the same schedule the other reports will run as Queue Threads open up for use. To configure appropriately need to know your resources available and configure appropriately. Probably there will be a little trial and error until you get things balanced. 

It just jumps to a new, uniform extent. There is no mechanism to try to convert the first mixed extent into individual uniform extents. If I understand correctly, when the mixed extent has no data in it any longer, it will switch that extent to being unused. Then it could be used for either a mixed extent or a uniform extent. 

The side-effect of creating a Custom Connection is that you will need to reestablish the Default Connection: 

For what it is worth, we have done it both ways over the years and both ways have their issues. But more systems that were using embedded documents are now using a file share instead. 

However, that is close, but not exact to your expected answer. Likely this is because of an error in datatypes. Please provide enough information to be able to calculate correctly. 

That assumes that you want to save the file name, begin time, and end time. That is up to you, of course. I would log the ImportBeginTime at the start of the import, then add the ImportEndTime at the end. That way you will have some tracking even if the import fails. And, assuming that you are importing from a file, I would actually recommend using because you can set the batch size and let handle the issue for you. Much better than writing it yourself. E.g. 

The Momentus XT is an "Advanced Format Drive" and I see that you are running Windows 7 SP1. If the drive runs 512e mode this can cause you a problem with SQL Server. This is because it can create sectors in some multiple of 512 byte boundaries, instead of always at 4096 bytes. (Your error was caused by a 6 * 512 byte sector.) If that is the case, then you will need to take some corrective action. Microsoft has a blog post and a KB article for Windows 7 and Windows 2009 R2 on this issue which points on to other links. $URL$ $URL$ If this is your problem you may want to download the latest FSUTIL to investigate. $URL$ This has not been the fix for everybody. In the MSDN forums I found a lengthy set of responses with various fixes. $URL$ From this discussion you will see that some finally resolved their problems by updating drivers. 

Download MaintenanceSolution.sql. Execute MaintenanceSolution.sql. This script creates the stored procedures that you need. Create cmd files to execute the stored procedures; for example a script: sqlcmd -E -S .\SQLEXPRESS -d master -Q "EXECUTE dbo.DatabaseBackup @Databases = 'USER_DATABASES', @Directory = N'C:\Backup', @BackupType = 'FULL'" -b -o C:\Log\DatabaseBackup.txt In Windows Scheduled Tasks, create tasks to call the cmd files. Schedule the tasks. Start the tasks and verify that they are completing successfully. 

Kendra Little has discussed some of the problems at: (Link corrected) SSMS 2016 Query Store Missing Index Details Error Document Frame Sqleditors She also has created a link to the Connect issue. You can go there a vote it up, if you wish. Also the Quick Rundown at the bottom includes comments on which behaviors you can expect. Since you are running SQL Server 2016, note: SQL Server 2016 Reset on database offline/restart. 

Yes, you can restore the logs by changing the settings for the Restore Job to be set to STANDBY, instead of NORECOVERY. This would provide a degree of Read Only access until the next restore is run. If you are using the log shipping functions that come with SQL Server you use the settings shown by Ali Razeghi for using . If you have rolled your own log shipping process (which many people have done) then you are already controlling the restore process yourself. EDIT: Since you restore the secondary one time a day, the READ ONLY window will be most of a day. So that eases the scheduling issue. Since the procedure sp_change_log_shipping_secondary_database is not working, either (1) you ran this on the primary instead of the secondary server, or (2) your organization made their own custom log shipping process. EDIT: If you are in the (2) state, then please include the details of the secondary server's Log Shipping Restore Job. However, a database in NORECOVERY can be updated to STANDBY like this: 

I ran into this problem when test restoring databases. We had one database that restored successfully, but failed during the DBCC CHECKDB because of a {00000000-0000-0000-0000-000000000000} UOW. In that case DTC was not running on the restore server at all, but still the {00000000-0000-0000-0000-000000000000} UOW was causing us problems as noted above. So you could say that this was an edge case. Using the approach above finally cleared the problem for us, but if you use this technique you need to make sure that you are not aborting DTC connections that are in active use. 

Think of 'sa' as ruling the SQL Server, but must beg for rights elsewhere. When reviewing SQL Agent job history, be alert to jobs that have been running much too long. That usually means that the SQL Agent does not realize that the process has died. Always plan to use a proxy account for SQL Agent jobs that need to access data or objects outside of the SQL Server. And make sure that the rights are granted to the Credential that the Proxy is using. 

As Sean Lange mentioned, MS Access cannot use a linked server which is only accessible from within the SQL Server. But MS Access is external to SQL Server. If you think of MS Access as the Client in a 2-tier Client/Server project, then it should be clear that you can use the 3-part naming convention (ServerName.Schema.Object) to access the SQL Server. If you have sufficient rights, you can update data in SQL Server from MS Access through the common use of T-SQL to manipulate SQL Server data. If you want the SQL Server itself to fetch data from a MS Access database, then you could create a Linked Server in SQL Server that would reach out through a provider to get data from the MS Access. $URL$ gives a sample provide setup from the Northwind database. 

You must write code to populate the materialized view and to refresh the data as needed. If you must have the most current data, then you will need to write triggers to provide the needed updates. 

Any approach would be best done by creating new tables or a table and file structures. Then migrate the data to the appropriate locations. Once you have the data migrated you can adjust object names, if desired. 

You should not need to DENY CONTROL to a login. Simply do not GRANT the right to that user. Regarding the side effects of DENY CONTROL see the post here: $URL$ Comment from that link: "The only option you have is to pay careful attention to not grant any permission to the principal that you did not indeed intend to include. You can consider using techniques like auditing to help you with this, but you cannot enforce it." So REVOKE your DENY and just make sure that only the needed rights are granted. 

The MSDN Blog "SQL Server SSIS and Replication" has a post from a few years ago. $URL$ How these old snapshot folders are purged depends on the type of replication you are using: 

Here is a link on SQL Server 2014 for you to Grant Permissions to Integration Services Service $URL$ The first step is: Run Dcomcnfg.exe. Dcomcnfg.exe provides a user interface for modifying certain settings in the registry. The following link for DCOM settings were last reviewed in 2006, but the options look unchanged. $URL$ This describes Default Authentication Level (Packet Level) which is the screenshot that you posted. Other subheadings describe several other facets of the DCOM setup. 

The stored procedure also allows any member of a specified group to start a job since it uses to check group membership. 

Remember that the first implementation is not the future implementation. Planning ahead just a little can save headaches in the future. If the database should scale up in the future you might find yourself refactoring tables and code. Since in the columns are named , , et cetera for whatever you are measuring you, you are committing to changing that table repeatedly as new measures are added. 

Already spent incremental CPU and IO to get the data ready for your SELECT. The index on the Materialized View can use the fastest sorting method available to MySQL, namely Use index-based access method that produces ordered output. 

So, it seems that the actual contents of the NVARCHAR(MAX) is what matters to the CREATE INDEX. EDIT: Jon Seigel identified that the TRY_CAST triggers the failure on create index when the string is longer than nvarchar(4000). 

A LCK_M_S is a blocking problem not a deadlocking problem. So you are just waiting on something else. You need to determine what process is blocking you. So, "it suddenly disappeared" because that block either completed or failed and rolled back. This is normal. What sounds abnormal from your notes or your perspective is the length of the block. In addition to finding the blocking process you could also consider using a different transaction isolation level. For example, although not without their own problems, you might consider using "READ COMMITTED SNAPSHOT" or "SNAPSHOT" isolation levels for your report. This would allow it to read the data as of the start of the transaction and would avoid most blocking situations. 

If is directly logged onto then a linked server to is one hop away and the 's authentication will be recognized on . This works with or without Kerberos delegation. Is is directly logged onto his desktop and accesses then is one hop away from the desktop and the 's authentication will be recognized on . However, if from the desktop he runs a query against that uses a linked server to access that is two hops away from his desktop. Without Kerberos (or some similar service) the 's delegation cannot make the second hop and, despite the rights granted on , will not authenticate on . 

SQL Server does not save previous index definitions for reuse at a later time. So query plans are based on the most recent statistics that were used for a compile. Although stored procedures do not recompile constantly, they will in time recompile. You can also individually recompile procedures. The "sp_recompile" can be used to make your stored procedures recompile the next time they are used. I see that you mention recreating statistics and recreating indexes, both of which help to get a correct set of statistics. You do not mention doing index reorganization, which also can be part maintaining the health of the indexes. However, if the definition of the index has been changed to an earlier version, then it means that someone or some process changed them.