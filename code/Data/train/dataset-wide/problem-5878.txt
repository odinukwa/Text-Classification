Two clearer examples: "He may not leave." can mean that he might stay or that he must stay. "He could not finish the course." can mean that he is still in the course and might drop out, or that has dropped out already because he cannot finish it. The meaning is totally dependent on its context. And for your case: "All dogs are not black" can mean that a randomly selected dog might not be black or that a randomly selected dog will not be black. 

Another answer is that you are not a formalist at all, but an intuitionist. There is a range of positions between Platonism and formalism that we generally choose to ignore. To my mind, intuitionism is the most promising among them. If you think many of these 'phantasies' constitute a shared factor in human experience, then they must reflect the composite intuition humans have developed to approach measurement and combination. In that case, they are not completely formal, but are based on something real. Accepting that mathematical entities are not ideal objects, nor simply conventions that are entirely learned, but exist in the mind, and are a shared aspect of human experience makes mathematics an aspect of psychology. Its goal is to discern the shared structure of our common genetic mental inheritance and see how its contents combine. Initially intuitionism was motivated by an approach to exploring the weakness in negation that causes Russel's paradox. The approach was to take negation itself as a psychological habit, and not a law of nature, or simply an aspect of grammar, and to look at other forms it takes in naive interpretations, in the hope of finding an improved habit that might be less audacious, but evade the defects. Its originator was a good mathematician, but not very gifted philosopher, and expressed his intention very poorly. So very few people grasp the approach as an alternative view of mathematics, and instead see it as a precursor to constructivism, or a weird experiment in alternative logic. 

Wittgenstein's notion of the language-game gives a credible alternative more in-line with the observed data. He characterizes informational interactions as a game, the rules of which are negotiated by the players and learned by new arrivals. We can agree the sun is round because we have learned what people mean by round, not because roundness is natural to human beings. Therefore it is not really universal, there may be tiny children somewhere whose notion of roundness is incompletely developed. They just have so little leverage on the game as a whole that we can ignore them until they adapt to the general experience and correct their notion of the label. There are observations that are universal in an objective sense, and always remain that way, but that is only because the humans not holding them do not get purchase on the game. In your example of 'the round sun', the definition does not change, because learning it early and precisely pays off too well. And even the game itself, or the notion of a game and how it is played, is not necessarily a universal. There are humans born who are too autistic to 'get' how to enter any existing game. But it is widespread enough that we simply declare those people defective and run civilization as if they did not exist. (I don't think it is coincidental that descriptions of the people who give us the best models in this domain often suggest they are partially autistic. This includes Wittgenstein himself and also Alan Turing. People who are not a little 'Aspey' simply swim in the game as fish in water. They just don't ask 'What do you mean by an idea?' 'Why can one person make another person think something?' or 'What does it mean to figure something out?' without some prompting. They don't believe the game exists because they are used to thinking of it as reality.) The other formulation of this same idea is Lacan's observation that 'master signifiers' are empty. Anything important to human beings is folded up into layers of references, and the references seem to point back to a single basis. But if you carefully unfold the references, you find that humans do not in fact really share what seems to be shared in the ultimate definition. We don't all really agree on exactly which more or less ovoid shapes are round. The place where each of us puts the cutoff differs, and in the end, the number of people who exactly agree on a precise and complete definition of roundness is going to be zero. Both of these viewpoints lead us to a notion of logic that is less realist and more psychological. We can make and express judgements because the psychological effects on other people are well-predicted by their investment in the game (or in the hierarchy of signification.) And we can judge the physical world by seeing what psychological models of it do and don't lead to success. We cannot actually make judgements, in the sense we would like, only predictions. There are no facts, only the power to safely presume which actions will be safe and effective. We all hold that power to different degrees, dependent upon our grasp of and leverage over the rules of the games we take part in, and we have an internal model that we leverage to do this, but it is not one that is ever completely shared outside ourselves. (Or, if we don't have a model, we lack leverage on the game, and we are not effective.) Classical logic is a lovely model of what we (or at least those with the power to be 'we') wish language were like. By explicitly learning it, we can better negotiate the game. But it is an incomplete model with idealized elements that simply do not refer to anything. The gain is in the coordination of information exchange, and not in describing reality. Reality is better described in a piecemeal and fluid manner. 

Given the "neo-" part, it becomes ambiguous to what extent Neo-Pagan religious views are not so thoroughly shaped by modern philosophy, particularly post-modernist or psychoanalytic takes on various notions of mythology and symbolism, that there is nothing more to say in response. You can have your argument directly with the academic proponents of those positions, and those academics are going to be more willing to answer you. One half of Wicca is pretty much an evolutionary descendent of Jung. If you read someone like Vivianne Crowley, or someone like Starhawk from the front ranks of the Feri tradition, you can see the bones . One half of Thelema is pretty much a sophisticated modern Libertarianism without the obsessiveness, mixed with early Nietzsche. A good piece of Asatru is from a different face of Nietzsche. Etc., etc., etc. The leftover 'halves' are generally not genuinely traditionally pagan either. They are instead the altered versions of paganism trapped within Hermeticism as it tried to allow for the genuine paganism embedded in Plato, Aristotle and the other early Greek and Roman philosophers of various schools. Why philosophy does not take Hermeticism seriously may be the real underlying question. It was once a primary influence on various aspects of philosophy. Terrence McKenna's answer is that the documented tradition itself was built upon lies -- St. Denys was not St. Denys (This 'pseudo-Dionysis' therefore did not have the experiences to which he attributes his insights, he imagined them through the dead St.Denys's eyes). Hermes Tresmagistus was a compiler, not the source of his own material, and he was not an ancient personage but a 5th-century scribe. (Although versions in the Dead Sea Scrolls suggest this decision was largely premature, and this was an intentional redaction of earlier material -- something appropriately traceable to a 5th-century scribe.) And this is compounded endlessly. Of course, those lies often had something to do with being hunted down by the Catholic Church, but we don't forgive them. (Early on, Wicca honored this tradition of having a tradition built of known historiographical lies as a positive thing, by creating a mythological origin for itself in each longstanding tradition, and backed it up with "Don't write anything down." or "Burn your history.". I think the motivation was "If you get all fixated, and can't just get over this when you figure it out, you probably don't belong here.") Somehow, the collapse of the lies behind the traditions trumped whether people were moved or intrigued by the content. 

In an absolute sense, no. Having zombies and normal people is itself adding an entity without effects. If all the effects are covered by assuming no distinction, then we are better off assuming there is no such thing as a zombie. It is just this kind of scholastic effulgence of needless potential distinctions the thing was invented to complain about. It is not actual intricacy the razor naturally removes, it is exceptions and pointless lack of uniformity. A universe with only one conscious being is in some sense less complex, but has one more rule -- it names who is 'real'. Studies of science validate the value of this interpretation. We find good science tends to allow for a lot more arbitrary complexity and chaos, but has fewer and fewer basic principles over time. (We are down to a Tee-shirt full of equations.) 

There are a lot of historical perspectives already given. But setting aside the history, this still makes sense outside the context of Aristotle. From a sort of mathematical view, a composite entity has a set of parts that could have been the parts of a different configuration had things gone differently. When they are taken as parts, with other options for deployment, they only potentially make up the composite that they happen to make up. They are potentially also parts of something else. And yet they are actually parts of what they happen to constitute. But if God is inevitable, or if he existed before time created alternative possibilities, or if any of a range of other assertions about him that the Catholic theology in which Aquinas is embedded asserts are true, then the things that make Him up could not possibly have come together in any other way. His parts do not have a potentiality to have been otherwise, there is only actuality involved. 

Valid arguments preserve the truth of their premises. Given false premises, valid arguments are not sound, and do not prove anything. (This is the point of all the arguments in Alice in Wonderland. A single false premise can make nonsense out of anything.) Your first 'step' in each case above is a different premise, so your result is different. Given that we do not know the truth value of the second premise, none of these arguments, even though they are formally valid, are the least bit helpful. Pure logic cannot indicate the truth value of premises except via identifying internal contradictions. If you combine a premise with another premise which may be false, logic does not even help clarify the premise itself. 

Consider something like an absolute consensus process. (Best elaborated by Quakers, incidentally know as the Society of Friends.) In such systems, fairness matters less than making peace between those involved. If one can always reach consensus, one never needs arbitration or an abstract definition of justice through law or moral code. 

The slave morality does not represent all morality to date. Plato's and Aristotle's moralities were not notably biased toward attaining equality or valuing suffering. They were master moralities. You can see where they lost out on the path from Ancient Roman to modern scientific society -- those arguments are still good arguments. Nor are the two irreconcilable opposites, as many who interpret Neitzsche automatically assume. Several people have taken this challenge seriously and tried to devise moral stances that bridge this distinction and make an even better set of norms. I would point you toward the 'softer side' of Alistair Crowley (Liber Aleph Naught) and pacifist feminists like Starhawk, who inject a multi-faceted notion of power that tries to explain why slave morality garners the power that it does by renouncing power, and to retain the strengths of both slave and master moralities, displacing the artificial master projected by most slave moralities. Somewhat Bowdlerizing the latter position: 1) Slave morality is a form of power that draws from shared reactions to more naked forms of power, it pulls strength from the weak, but from a very broad base, and ultimately overpowers individual masters and takes control of systems of mastery, making them paranoid, guilty and ineffective. 2) Unfortunately it unconsciously depends for its own internal logic upon the notion of a master more deserving than any real and existing master. 3) At the same time, people who are still masters channel that image and distort it into tools that actually maintain the same kind of institution in a different form. 4) Displacing that false master from control of our cultural institutions undercuts the weight of slave morality, which is, as Nietzsche foresaw, destroying our ability to make sense of things and act decisively. 5) It also prevents the holders of power from having a ready cookie-cutter model of acceptable oppression to guide them, giving them the right set of buttons to push to activate deeply channelled instincts in the population. Therefore: We need a clear respect for power and will, guided by a broader notion of care and empathy. We should be rightly selfish and self-aggrandizing, in the clear understanding that we are all parts of one another. 

The basis of the law, in Common Law countries, is not philosophy, but tradition. Traditionally, one's family owned one's body. In various codes of Weregelt, if you were severely injured, it was not to you that the money or rights were paid so that the infraction could be 'for-given'. It was to the clan of your extended family. This moral stance was related to the metaphor behind fealty. The family had cultivated you, like a crop or a herd animal, and it had earned the right to its product. You owed them for your childhood, and for keeping you alive in the present. And in exactly the same way, the clan had cultivated your extended family line. Exogamous marriage itself was a trade between families or clans, sometimes moving money or rights from the clan of the spouse who left home to live among their partner's people, or requiring a ritual theft or contest that transferred responsibility for that spouse's maintenance to the new family. Feudalism gave way to the notion of the nation-state, and clans were concentrated up into monarchies, which eventually distributed their power into democracies of some form. But the traditional basis here remained. You belong to the entity that protects you militarily. The institution that guarantees your rights also determines what they are. And to that degree, you remain the property of the state. It is their responsibility to educate you and to enable a functional economic system that allows you to leverage your work, by creating some degree of peace and order. To the degree they succeed, you owe them. A part of your labor is beholden to them according to the tax code, and they retain the right to make laws that can determine whether you may commit suicide, or sell yourself into slavery or prostitution. In a democracy, this is joint ownership of all of us by one another, but it remains real enough ownership that most Western democracies have at some point recently had military conscription, even in peacetime. If you can be ordered to report bodily to an institution that will then determine most of your future actions until they decide they are done with you, you clearly do not own your own body. 

In Wittgenstein's own development, he moves on from the picture theory to an interpretation in terms of 'games' which capture actions and motives. (I like the "Blue and Brown Books" which are notes from his lectures during the period this change was taking root. Since he was speaking to humans, it is more comprehensible than the "Tractatus", and holds together more tightly than the "Investigations") Actions can in some sense be captured by a sort of 'moving picture', but not in a way that reliably feels like a basic concept has been really captured. We do not so much accumulate these moving pictures, as look behind them for shared structure and how that structure answers to motives. If I teach a child about an activity, at some point fairly early on, unless the motivation is really obvious, she moves on to 'why' before being willing to put up with any more 'what' or 'how'. And internal wishes and motives are not as easily captured in pictures. If a person did not have the direct experience of hunger, you would not be able to depict it, only its effects, so you would have extreme difficulty conveying the actual 'picture'. But by demonstrating the effects, you are only pointing at the motivation to avoid them, and not very clearly. For less physical motives like power or love, things get worse and worse. But the fact we share motives shapes what we do, and those actions coalesce into negotiations around the motives, even when we do not explicitly discern and point at the motives. Carousing, for instance, serves a genuine need that we generally do not bother to name. But it creates a grand and sprawling vocabulary and a set of rules for negotiation and optimization of the experience. (Ask any autistic or any other earnest-enough introvert how stupid these really seem on their surface, or maybe just anyone from a distinctly different generation.) Most other kinds of objects have importance to us to the degree they are parts of schemas that answer to motives. When they do not, we can point at them, but the reference is unlikely to become a stable part of our shared language. Non-farmers do not tend to have multiple of different words for dirt, even though we can and do probably occasionally point at varieties of dirt when they matter to us, whereas farmers do. In the picture theory of language, both people should find the relevant pictures equally cogent and memorable. That indicates that the much more ambiguous category of social motives is what matters to us. (A statement so obvious it is almost a pun.) And we do not pursue any basic approach to just describing reality, including exchanging pictures.