Suppose we have a finite group $G$ with subgroup $H$, a representation $\rho_V$ of $H$ on a finite-dimensional vector space $V$, and an $H$-invariant inner product on $V$: $$\forall x,y\in V, h\in H,\enspace \langle\rho_V(h)x, \rho_V(h)y\rangle = \langle x,y\rangle$$ We will write $V_I$ for the direct sum of $\lvert G:H \rvert$ copies of $V$: $$V_I = \oplus_{i=1}^{\lvert G:H \rvert} V$$ We define a map $L_i$ that lifts $V$ into the $i$th copy in the direct sum: $$L_i: V\to V_I\\ L_i v = 0 \oplus 0 \oplus ... \underbrace{v}_{i\text{th summand}} \oplus 0 + ...$$ We extend the inner product on $V$ to one on $V_I$: $$ \langle \sum_i L_i x_i, \sum_j L_j y_j \rangle = \sum_i \langle x_i, y_i \rangle$$ From each left coset $K_i$ of $H$ in $G$ we pick an element $k_i$, so $K_i=k_i H$. We then have an induced representation $\rho_I$ of the group $G$ on $V_I$: $$\rho_I(g) \sum_i L_i v_i = \sum_i L_{j(g,i)} \rho_V(k_{j(g,i)}^{-1} g k_i) v_i$$ where $j(g,i)$ is the index of the coset $K_i$ to which $g k_i$ belongs. Now, suppose we have an irreducible representation $\rho_W$ of $G$ on some finite vector space $W$. The Frobenius reciprocity theorem says that $Hom_H(W,V)$, the space of $H$-intertwiners from $W$ to $V$, i.e. the space of maps $S$ that satisfy: $$S: W\to V\\ \forall h\in H,\enspace S \rho_W(h) = \rho_V(h) S$$ is isomorphic to the space $Hom_G(W,V_I)$ of $G$-intertwiners from $W$ to $V_I$, i.e. the space of maps $T$ that satisfy: $$T: W\to V_I\\ \forall g\in G,\enspace T \rho_W(g) = \rho_I(g) T$$ Indeed, given an intertwiner $S\in Hom_H(W,V)$ we can easily construct an intertwiner $T_S\in Hom_G(W,V_I)$: $$T_S w = \sum_i L_i S \rho_W(k_i^{-1}) w$$ As we vary $S$ over any basis of $Hom_H(W,V)$, the associated map $T_S$ will map $W$ into distinct subspaces of $V_I$, each of which is invariant under the action of $\rho_I$, and each of which has the property that the restriction of $\rho_I$ to that subspace is equivalent to $\rho_W$. My question is: supposing the dimension of $Hom_H(W,V)$ is greater than 1, does there exist an "easy" strategy to choose a basis $\{S_1,S_2,...\}$ of $Hom_H(W,V)$ such that the subspaces $T_{S_1}(W), T_{S_2}(W), ...$ of $V_I$ will be mutually orthogonal? By "easy", I mean something less computationally demanding than simply finding all the subspaces by means of an arbitrary basis for $Hom_H(W,V)$, and then decomposing their direct sum into orthogonal invariant subspaces. In other words, I am seeking to leverage the fact that $Hom_H(W,V)$ is a lower-dimensional space than the direct sum of the $T_{S_i}(W)$ in order to carry out a less demanding procedure to achieve the same result. Edited to add: One easy way to get a basis of $Hom_H(W,V)$ is to take a sufficient number of linearly independent maps $S^{(0)}_i: W\to V$ and average over the subgroup $H$: $$S_i = \frac{1}{|H|}\sum_{h\in H} \rho_V(h) S^{(0)}_i \rho_W(h^{-1})$$ So by starting with a basis of (non-intertwining) maps from $W$ to $V$, you can project as many as required into $Hom_H(W,V)$ to obtain a basis. My (possibly naive) hope is that there might be some way of modifying this construction to lead directly to a basis with the property I'm seeking. 

One can also show with Fermat's last theorem that $\sqrt{2}$ is irrational - the answer of mt did $2^{1/n}$ for $n\ge 3$. Suppose that $\sqrt{2}$ is rational. Then there is a right-angled triangle with rational sides $(a,b,c)=(\sqrt{2},\sqrt{2},2)$ and area 1. Hence $1$ would be a congruent number. This contradicts Fermat's last theorem with exponent $4$. 

As said, semisimple Lie algebras over a field of characteristic zero do not admit an LSA-structure. There are several proofs. Basically it relies on Whitehead's lemma for semisimple Lie algebras, saying that $H^1(\mathbb{g},M)=0$ for all finite-dimensional $\mathbb{g}$-modules $M$. On the other hand, not all solvable Lie algebras admit a compatible LSA-structure. There are even nilpotent Lie algebras not admitting such structures, see Y. Benoist's paper "A nilvariete non-affine", and this paper here. The proof is difficult and shows that such counterexamples can be constructed by $n$-dimensional Lie algebras not admitting a faithful linear representation of degree $n+1$. 

It seems that the idea of the question partly was to have a counterexample to Whitehead's second lemma. However this can be easily given by considering Verma modules (which are infinite-dimensional). F. L. Williams has computed the structure of the cohomology of a finite-dimensional complex semisimple Lie algebra with coefficients in an arbitrary Verma module, in the paper The cohomology of semisimple Lie algebras with coefficients in a Verma module, 1978. The results show that the cohomology $H^2(\mathfrak{g},M)$ does not vanish in general for Verma modules $M$. 

Profinite groups were first called "Groups of Galois type", see J.P. Serre's book "Cohomologie Galoisienne" of $1964$. The term "profinite" comes from Serre (if I am not mistaken). Of course, some profinite groups have a much older history, e.g., already Hensel defined in $1910$ the $p$-adic integers during his studies of algebraic numbers. As to the definition, a profinite group is a Hausdorff, compact, and totally disconnected topological group. The other (equivalent) definition, better adapted to the name "profinite", is, that a profinite group is a group which is isomorphic to the inverse limit of an inverse system of discrete finite groups. 

This answer is an attempt to slightly rephrase Ilya's argument; I would have written it as a comment, but there's not enough room. Given a path $\gamma$ with endpoints $v_0$ and $v_n$, we have a 1-chain $c(\gamma)$, which is an integer-linear combination of edges of the graph. Let $E_{c(\gamma)}$ be the set of edges with non-zero coefficients in $c(\gamma)$; this need not include every edge that appears in the path, since some might cancel to zero in the 1-chain. If we think of these edges as decorated with the integer coefficients they inherit from the 1-chain $c(\gamma)$, we can associate a 1-chain with any subset of $E_{c(\gamma)}$. It's possible that $E_{c(\gamma)}$ as a subgraph of $\Gamma$ contains several connected components that share no vertices with each other, but since the boundary of $c(\gamma)$ is only non-zero on $v_0$ and $v_n$, and it's impossible for a 1-chain to have a single-point boundary, these components would need to include exactly one whose 1-chain's boundary was non-zero on $v_0$ and $v_n$, and all the rest would have to give 1-cycles. Since they're all disjoint, any one of the 1-cycles $\chi$ would satisfy $\langle c(\gamma), \chi \rangle = \langle \chi, \chi \rangle \gt 0$. Assuming now that $E_{c(\gamma)}$ is connected as a subgraph, we can build up a 1-chain $\sigma$ with a positive inner product with $c(\gamma)$ as follows. Starting at $v_0$, pick an edge $\epsilon_1$ incident on $v_0$, and put $\pm \epsilon_1$ in $\sigma$, with the sign chosen to be the same as the coefficient of $\epsilon_1$ in $c(\gamma)$. Then advance to the other vertex of $\epsilon_1$, and choose an edge $\epsilon_2$ such that $\epsilon_2 \ne \epsilon_1$, and $\pm \epsilon_2$ with the same sign as the coefficient of $\epsilon_2$ in $c(\gamma)$ gives a boundary for $\pm \epsilon_1 \pm \epsilon_2$ that is zero at the current vertex. This must be possible (assuming we haven't ended up at an endpoint of the path), since the boundary of $c(\gamma)$ is zero at the current vertex, so the signs of the edge coefficients in $c(\gamma)$ can't all give boundaries of the same sign here. We continue this process until we reach either $v_n$, the endpoint of the path, or a vertex we've visited before. If we reach a vertex we've visited before, we can drop any earlier edges from $\sigma$ and obtain a 1-cycle with a positive inner product with $c(\gamma)$. If we reach $v_n$, there are two possibilities. If $\sigma = c(\gamma)$ then $\sigma$ describes a simple path $\gamma'$ from $v_0$ to $v_n$ with the same 1-chain as our original path, and we can proceed to use that simple path in place of $\gamma$. If $\sigma \ne c(\gamma)$, it nonethless has the same boundary. Since $\sigma$ is supported on a subset of the same edges as $c(\gamma)$, and its coefficients are of the same sign but never greater in magnitude (and must be less on at least one edge), $\langle c(\gamma), \sigma \rangle$ will be strictly less than $\langle c(\gamma), c(\gamma) \rangle$. So we'll have a non-trivial 1-cycle $\ell = c(\gamma) - \sigma$, and: $$\langle c(\gamma), \ell \rangle = \langle c(\gamma), c(\gamma) \rangle - \langle c(\gamma), \sigma \rangle \gt 0$$ Finally, suppose we have a simple path $\gamma': v_0 \to v_1 \to \dots \to v_n$ with the same 1-chain as $\gamma$. (The following refinement of Ilya's argument is something that John described to me in correspondence.) Since the edge $e_n: v_{n-1} \to v_n$ cannot be a bridge, there must be a path joining $v_n$ to $v_0$ that does not include that $e_n$. If we follow that path only as far as the first of the $v_i$ it reaches, we will have a path $\rho: v_n \to v_i$ which uses no edges of the simple path $\gamma'$. We can then append the portion of $\gamma'$ that goes from $v_i$ to $v_n$, call it $\gamma'_i$, to obtain a 1-cycle $c(\rho) + c(\gamma'_i)$ that must have a positive inner product with $c(\gamma)$: $$\langle c(\gamma), c(\rho) + c(\gamma'_i) \rangle = \langle c(\gamma'_i), c(\gamma'_i) \rangle \gt 0$$ 

The two definitions do never agree for a non-trivial nilpotent Lie algebra, as Ben has remarked. Indeed, if $\mathfrak{g}$ is nilpotent then $\mathfrak{s}=[\mathfrak{g},\mathfrak{r}]=[\mathfrak{g},\mathfrak{g}]$, whereas $\mathfrak{n}=\mathfrak{g}$. Since $\mathfrak{g}$ is nilpotent, $\mathfrak{g}\neq [\mathfrak{g},\mathfrak{g}]$. For non-trivial abelian Lie algebras we have in particular $\mathfrak{s}=0$ and $\mathfrak{n}=\mathfrak{g}$. If $\mathfrak{g}$ is solvable, then $\mathfrak{s}=[\mathfrak{g},\mathfrak{g}]$ and $\mathfrak{s}\subseteq \mathfrak{n}$, and both equality and strict inclusion can happen. For equality consider the $2$-dimensional non-abelian Lie algebra. 

It is known, that the index of a Lie algebra is a semi-invariant for degenerations (by Ooms and Elashvili), i.e., if $L_1$ degenerates to $L_2$, then $ind(L_1)\le ind(L_2)$. This is very useful. For example, it follows that any filiform Lie algebra of dimension $n$ has index less or equal than $n-2$, where only the standard graded filiform $L(n)$, which you have defined above, has exactly index $n-2$. In general, there are many other Lie algebras of dimension $n$ and index $n-2$, e.g., also the quasi-filiform Lie algebras $L(n-1)\oplus \mathbb{C}$. See here also the work Adini and Makhlouf. The Hasse-diagram of complex nilpotent Lie algebras in dimension 6 gives explicit examples, e.g., we have degenerations from the top algebra $L_{6,20}$ as follows (notation of Magnin for the Lie algebras) $L_{6,20}\rightarrow L_{6,18}\rightarrow L_{6,17} \rightarrow L_{6,16} \rightarrow L_{5,5} \oplus \mathbb{C}\rightarrow \mathbb{C}^6$, with index numbers $2 \rightarrow 2 \rightarrow 2 \rightarrow 4 \rightarrow 4 \rightarrow 6$. See my paper arXiv:0911.2995 for this, and a discussion on the maximal dimension of an abelian subalgebra $\alpha (L)$, which is related to the index by $\alpha (L)\le (\dim (L)+ind (L))/2$. 

Let $p(n)$ denote the number of partitions of a positive integer $n$. It seems to me that we have for all $n>25$ $$ p(n)^2>p(n-1)p(n+1). $$ In other words, the sequence $(p(n))_{n\in \mathbb{N}}$ is log-concave, or satisfies $PF_2$, with $$ \det \begin{pmatrix} p(n) & p(n+1) \cr p(n-1) & p(n) \end{pmatrix}>0 $$ for $n>25$. Is this true ? I could not find a reference in the literature so far. On the other hand, the partition function is really studied a lot. So it seems likely that this is known. Similarly, property $PF_3$, with the corresponding $3\times 3$ determinant, seems to hold for all $n>221$, too, and also $PF_4$ for all $n>657$. The question is also motivated from the study of Betti numbers for nilpotent Lie algebras, in particular filiform nilpotent Lie algebras. 

To be clear, although we have an embedding of $G$ in $\mathbb{R}^n$, I am not restricting the question to automorphisms that arise from homeomorphisms $\mathbb{R}^n \to \mathbb{R}^n$; any graph automorphism of $G$ counts. 

Let $M$ be any $n\times n$ matrix. We define the usual cofactors: $C_{i,j}$ is $(-1)^{i+j}$ times the determinant of the submatrix obtained by deleting row $i$ and column $j$ of $M$. We can write the determinant of $M$ using Laplace expansion along column $p$ as: $$\det M = \sum_{q=1}^n M_{q,p} C_{q,p}$$ Now, for any $k, p\in \{1,...,n\}$ consider the sum: $$W_{n,p}(k) = \sum_{q=1}^n {M_{q,p} C_{q,p}^2 \prod_{j\ne q}{C_{j,k}}}$$ Clearly $W_{n,p}(p)$ can always be factored, with $\det M$ as one factor: $$W_{n,p}(p) = \sum_{q=1}^n{M_{q,p} C_{q,p}^2 \prod_{j\ne q}{C_{j,p}}} = \sum_{q=1}^n{M_{q,p} C_{q,p} \prod_{j=1}^n{C_{j,p}}} = \left(\det M\right) \prod_{j=1}^n{C_{j,p}}$$ But in all the specific cases I've examined, $\det M$ appears as a factor of $W_{n,p}(k)$ even when $k\ne p$. For example, with $n=3$, $p=1$ and $k=2$: $$W_{3,1}(2)=\sum_{q=1}^3{M_{q,1} C_{q,1}^2 \prod_{j\ne q}{C_{j,2}}}=\\ \left(\det M\right)\left(M_{2,2} M_{2,3} M_{3,1}^2 M_{1,3}^2+M_{2,1} M_{2,3} M_{3,1} M_{3,2} M_{1,3}^2-M_{2,1} M_{2,2} M_{3,1} M_{3,3} M_{1,3}^2-M_{2,1}^2 M_{3,2} M_{3,3} M_{1,3}^2-M_{1,2} M_{2,3}^2 M_{3,1}^2 M_{1,3}+M_{1,2} M_{2,1}^2 M_{3,3}^2 M_{1,3}+M_{1,1} M_{2,1} M_{2,2} M_{3,3}^2 M_{1,3}-M_{1,1} M_{2,3}^2 M_{3,1} M_{3,2} M_{1,3}-M_{1,1} M_{1,2} M_{2,1} M_{2,3} M_{3,3}^2-M_{1,1}^2 M_{2,2} M_{2,3} M_{3,3}^2+M_{1,1} M_{1,2} M_{2,3}^2 M_{3,1} M_{3,3}+M_{1,1}^2 M_{2,3}^2 M_{3,2} M_{3,3}\right) $$ It might be worth noting that the second factor here cannot be written as a linear combination of products of the cofactors, with purely numeric coefficients. This is in stark contrast to the case $k=p$, when the quotient is simply a product of cofactors. I am seeking a proof that $\det M$ always divides $W_{n,p}(k)$, and a general formula for the quotient in the non-trivial case $k\ne p$. 

I've found a somewhat nicer proof than the version on the Visual Insight blog. This new approach doesn't entail a huge conceptual breakthrough, but it does avoid having to deal with 3072 individual cases. We have some freedom in choosing $R$ and $S$, but this makes no difference to the resulting sets of axes. We will pick: $$\displaystyle{R = \frac{1}{4} \left( \begin{array}{ccc} 2 & 1-\sqrt{5} & -1-\sqrt{5} \\ 1-\sqrt{5} & 1+\sqrt{5} & -2 \\ 1+\sqrt{5} & 2 & -1+\sqrt{5} \end{array} \right)}$$ $$\displaystyle{S = \frac{1}{4} \left( \begin{array}{ccc} \sqrt{5}-1 & -2 & -1-\sqrt{5} \\ 2 & 1+\sqrt{5} & 1-\sqrt{5} \\ 1+\sqrt{5} & 1-\sqrt{5} & 2 \end{array}\right)}$$ All the powers of these matrices can again be written with a denominator of 4 and numerators taken from $\{\pm 2, \pm 1 \pm \sqrt{5}\}$. We can simplify things a bit by working with integer matrices in 6 dimensions. For each of the four powers of $R$ and $S$, we will multiply the matrix by 4 and then write it as a linear map between 6-dimensional spaces with separate components for the rational and irrational parts of each component of the original vector. For example, for the first power of $R$ we get: $$\displaystyle{R_6 = \left( \begin{array}{cccccc} 2 & 0 & 1 & -5 & -1 & -5 \\ 0 & 2 & -1 & 1 & -1 & -1 \\ 1 & -5 & 1 & 5 & -2 & 0 \\ -1 & 1 & 1 & 1 & 0 & -2 \\ 1 & 5 & 2 & 0 & -1 & 5 \\ 1 & 1 & 0 & 2 & 1 & -1 \end{array}\right)}$$ and for the first power of $S$ we get: $$\displaystyle{S_6 = \left( \begin{array}{cccccc} -1 & 5 & -2 & 0 & -1 & -5 \\ 1 & -1 & 0 & -2 & -1 & -1 \\ 2 & 0 & 1 & 5 & 1 & -5 \\ 0 & 2 & 1 & 1 & -1 & 1 \\ 1 & 5 & 1 & -5 & 2 & 0 \\ 1 & 1 & -1 & 1 & 0 & 2 \end{array}\right)}$$ Suppose we have some unit vector $v$ of the form: $$v = (a + b \sqrt{5}, c + d \sqrt{5}, e + f \sqrt{5}) / 2^{N+1}$$ where $a, b, c, d, e, f$ are integers, with at least one of them odd, and $N \ge 1$. We will work with the integer vector: $$w = (a, b, c, d, e, f)$$ Because $v$ is a unit vector, the components of $w$ will satisfy the conditions: $$a^2 + c^2 + e^2 + 5(b^2 + d^2 + f^2) = 4^{N+1}$$ and $$a b + c d + e f = 0$$ If we multiply any vector of this form by each of the eight $6 \times 6$ matrices corresponding to the four powers of $R$ and $S$, then it turns out that precisely one of those eight matrices will yield a result equal to the zero vector modulo 8, i.e. a 6-tuple of integers all divisible by 8. To prove this, we take the lattice of vectors in $\mathbb{Z}^6$ equal to the zero vector modulo 8, and multiply it by the inverse of each of the eight matrices in turn, to produce eight new lattices: lattices which yield 6-tuples of integers all divisible by 8 when multiplied by the appropriate matrix. In concrete terms, for a given matrix $M_i$, the basis for the associated lattice is given by the row vectors of $L_i = 8 (M_i^{-1})^T$. The original claim can now be restated as saying that every vector $w$ that meets the conditions described above will belong to the union of the eight lattices $L_i$, but no such vector will belong to the intersection of any two of the $L_i$. The first part is fairly easy to show. We can obtain a basis for the union of the eight lattices by forming a matrix whose rows are the union of all eight bases, and then reducing that $48 \times 6$ matrix to a $6 \times 6$ matrix by putting it in Hermite Normal Form (the equivalent of reduced row-echelon form for integer matrices), and discarding all rows containing only zeroes. We will call that matrix $L_U$. The test for the vector $w$ belonging to the lattice whose basis is given by the rows of $L_U$ is that the vector $(L_U^{-1})^T w$ has all integer coordinates. When we carry through these calculations, we find: $$(L_U^{-1})^T w = (2a, b-a, 2c, d-c, e-a-c, \frac{a-b+c-d+f-e}{2})$$ Since $a, b, c, d, e, f$ are integers, the only thing remaining to show is that $a-b+c-d+f-e$ will always be an even integer, given the conditions we've placed on $w$. We have the conditions: $$a^2 + c^2 + e^2 + 5(b^2 + d^2 + f^2) = 4^{N+1}$$ $$a b + c d + e f = 0$$ It follows that: $$a^2 + c^2 + e^2 + 5(b^2 + d^2 + f^2) = 0 \mod 4$$ $$a^2 + c^2 + e^2 + 5(b^2 + d^2 + f^2) - 4(b^2 + d^2 + f^2) - 2(a b + c d + e f)= 0 \mod 4$$ $$(a-b)^2 + (c-d)^2 + (e-f)^2 = 0 \mod 4$$ It's not hard to check that the sum of three squares can only be a multiple of 4 if all three of the numbers being squared are even. So we have $a-b, c-d$ and $e-f$ all individually even, so $a-b+c-d+f-e$ will be even, $w$ will belong to the lattice $L_U$, and at least one of the eight matrices multiplied by $w$ will yield a vector whose components are all divisible by 8. To prove that only one matrix yields such a result for a given $w$, we need to show that the intersection of any pair of distinct lattices $L_i$ and $L_j$ cannot contain any vector $w$ meeting the conditions we've imposed. There are 28 such pairs of lattices. Finding their bases is a bit more involved than finding the basis for a union. First, we need to construct the dual of each lattice. The dual of a lattice $L_i$ is the set of vectors $d$ such that for every $v \in L_i$, the dot product $d \cdot v$ is an integer. Its basis is given by the rows of the matrix: $$D_i = (L_i L_i^T)^{-1} L_i$$ We obtain a basis for the intersection of two lattices by forming their dual lattices, finding a basis for the union of those duals (by joining their matrices and reducing it to Hermite Normal Form), and then taking the dual of that union. If we do this for the 28 pairs of lattices, we find that in 16 cases the intersection of the lattices contains only vectors whose coordinates are all even integers. This violates the requirement that at least one coordinate be odd (which we impose in order that the corresponding vector divided by a power of 2 is in lowest terms). For the remaining 12 pairs of lattices, the requirement that at least one coordinate be odd can be satisfied if and only if one particular element of the lattice basis has an odd coefficient $\ell$ in the sum that decribes the vector $w$ with respect to that basis. But that in turn clashes with the requirement that: $$a^2 + c^2 + e^2 + 5(b^2 + d^2 + f^2) = 4^{N+1}$$ The contradiction appears if we require the equation to continue to hold modulo 8. In each case all but one of the lattice coefficients vanish, and what we end up with is: $$4 \ell^2 = 4^{N+1} \mod 8$$ which is impossible for $N\ge 1$ and odd $\ell$. Because $R$ and $S$ are rotations of order 5, the set of their first 4 powers can also be seen as the set of inverses of their first 4 powers. Because the corresponding integer matrices are multiplied by a factor of 4, a result that is a multiple of 8 corresponds to a factor of 2 in the original matrices. So what we have established is that, given any unit vector over the golden field with a denominator of $2^{N+1}$ for some $N \ge 1$, the inverse of precisely one of the powers of $R$ and $S$ will take us to another unit vector with a denominator of $2^N$. As we repeat this process, we will move back through the tree to ever smaller denominators, eventually terminating with the original cube. This means that we can reach every unit vector $v$ of this form as one of the cube axes or their opposites, and also that we can only reach it via a single path. 

Primes of the form $2^n+k$ have been considered, see the talk of Carl Pomerance. Among the square numbers $k=y^2$ the case $k=1$ is the most famous one, e.g., primes of the from $2^n+1$. Then necessarily $n$ is a power of $2$, so that these primes are just the Fermat primes $F_k=2^{2^k}+1$. Another special case are the primes of the form $2^n+n^2$, see sequence A064539 at integer sequences. Then necessarily $n\equiv 0 \mod 3$. 

A reference for this is also "Lectures on Real Semisimple Lie Algebras and Their Representations" by A. L. Onishchik. A first result here is: any irreducible real representation $\rho\colon \mathbb{g}\rightarrow \mathbb{gl}(V)$ of a real Lie algebra $\mathbb{g}$ satisfies precisely one of the following two conditions. $(1)$ $\rho^{\mathbb{C}}$ is an irreducible complex representation. $(2)$ $\rho=\rho'_{\mathbb{R}}$, where $\rho'$ is an irreducible complex representation admitting no invariant real structure. Conversely, any real representation satisfying $(1)$ or $(2)$ is irreducible. For the results concerning highest weigts etc. see Onishchik's book, section $8$, about "Real representations of real semisimple Lie algebras", including the classification of them. 

One "arithmetic version" of the Nullstellensatz states that if $f_1, ..., f_s$ belong to $\mathbb{Z}[X_1,...,X_n]$ without a common zero in $\mathbb{C}^n$, then there exist $a \in \mathbb{Z} \setminus {0}$ and $g_1,...,g_s$ in $\mathbb{Z}[X_1,...,X_n]$ such that $a = f_1g_1 + ... + f_sg_s$. Finding degree and height bounds for $a$ and $g_1, ..., g_s$ has received some attention, see for example here. 

The book of Willem de Graaf "Lie Algebras: Theory and Algorithms" contains some algorithms for determining whether or not a Lie algebra $L$ is nilpotent. Of course, Engel's theorem is one of the main tools. We can very efficiently see that a given Lie algebra $L$ is not nilpotent, by checking first the trace condition, i.e., $tr(ad(x))=0$ for all $x\in L$, or to find a nonzero eigenvalue of some $ad(x)$. There are also algorithms for calculating the nilradical, even for fields of prime characteristic $p>0$. 

Let $\mu(L)$ denote the minimal dimension of a faithful module of $L$. The complex simple Lie algebra $E_8$ satisfies $\mu(L)=\dim (L)$, as Cartan showed. Indeed, $E_8$ is the only complex simple Lie algebra with this property. There are more results in this direction (which are perhaps interesting): suppose that $L$ is a complex semisimple Lie algebra satisfying $\mu(L)=\dim (L)$. Then $L\simeq E_8\oplus \cdots \oplus E_8$. Even more general, let $L$ be a Lie algebra, where the solvable radical $rad(L)$ is abelian. Then always $\mu(L)\le \dim (L)$, and equality holds if and only if $L$ is abelian of dimension less than $5$, or $L$ is isomorphic to $E_8\oplus \cdots \oplus E_8$. (For references see arXiv:1006.2062).