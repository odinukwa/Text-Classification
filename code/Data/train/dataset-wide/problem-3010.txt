The is the "magic location" to load from the bitmap array which will have the correct alignment without needing shifting. We precalculate the offset values (they have a period like almost everything else) and store them in , and take the next offset on each iteration (the last line wraps when we hit the period). How does this one do? 

1 This makes total sense when you eyeball the code: the inner loop has 6 instructions, and the trips to the outer loop increase the branch density a bit. 2 Note that the output indicates the prime density is half that: 11.5% - and that's the true prime density - but the algorithm only examines half the numbers since it skips all even values, so from the point of view of the looping structure the prime density is 23%. 3 Shifts of a by amounts larger than 63 are famously undefined behavior in C++, so this is needed for correctness, but even at the x86 assembly level, we'd need something because x86 shifts are "mod 64", so a shift by 64 is the same as a shift by zero, not what we want. 4 This would be much better as a conditional, but doesn't do it that way, perhaps because there is a read of the array on one branch and doesn't want to do that in the case the value isn't used (even though it can probably prove that is always in bounds). 5 This is just a consequence of 11 and 8 being relatively prime, and indeed since we are only dealing with odd prime numbers on the one hand and powers of two (for the various bitmap arrays), this useful properly will occur repeatedly. 6 At the limit of very large reads, the size of the table goes up proportionally to the read size, but for smaller values it is sub-linear. For example, when I moved from 1 read in the algorithm to 2 reads in , the size increased only from 158 bytes per prime to 190 bytes per prime. You can see the behavior by adjusting the constant and running the command. 

Lazy intialization / factory methods You currently separate the component creation process by immediately instantiating the component at the variable declaration and configuring the component elsewhere. Component creation and configuration should be encapsulated in lazy getters like this: 

Different approaches to determine the domain membership of a computer Usage of command line tools If you use command line tools you have the burden to parse the output of the process. A naive implementation to execute command line tools in windows: 

After that we create a predicate that defines that we are only interested in potentially successful decoded texts: 

The buffer is the resource. So it should take care about the access to itself by itself or another monitor. This monitor should not be the consumer nor the producer. So synchronization should NOT be located in the consumer or the producer. If you want to omit effort (that will be technical debts) the first approach is to make the resource (in this case the Buffer) a monitor by introducing synchronized as the keyword in the method signatures of add() and remove(). size() and isEmpty() should not be available for clients, make them private. Now the buffer will be thread-safe. To make use of wait and notify, you have to exactly know what is happening. wait() will suspend the current thread WITHIN a monitor. A consumer-thread therefore may be suspended with wait() in the method remove() if nothing is in the buffer. A producer may be suspended in the method add() with wait() if he want wants to produce something but the cap is reached. Both the consumer and the producer should notify all other suspended threads with notifyAll() after they changed the state of the resource. A simple notify() is not applicable here because in your current construct you do not distinguish between locks for consumers and locks for producers. After notifying the "notifiers" must leave the monitor by waiting or exiting the synchronized method (this will be maybe your solution). A notified thread should first check, what has happend during its absence. So a consumer should check if something is in the buffer. If not then again wait(), consume() otherwise. A producer should check if the cap is reached. If not then produce(), wait() otherwise. I see two further improvements: First separate the locking responsibility from the resource. Introduce a "BufferMonitor" class for example delegating add() and remove() to the encapsulated real buffer and make its methods synchronized. Second introduce additional locks for consumers and producers to be able to distinguish when to wake up one consumer or one producer. notifyAll() on the BufferMonitor will then become a notify() on the specific lock. Because I haven't implemented this scenario yet I do not know how much effort it is. After all it should be possible. But recognize: The BufferMonitor will not be obselete. One sentence to the remove()-method. I suggest to somehow avoid returning -1 even it SEEMS to be impossible. A solution that does not need to return -1 is the most elegant one and I admit the most difficult. You may need something like a consumption announcement mechanism before calling "remove()". Maybe you take the challenge. But be careful: this is often called "developing gold taps". ;-) Finally you should think about proper shutdown mechanisms of your threads. So waiting threads should be notified on shutdown and evaluate this shutdown request on their own. 

Bring back the Vectors The next step is to vectorize this. This is getting long so we'll skip the first version ( which clocks in at 0.27 cycles per candidate) and just go to my final version, : 

You increased from 2 reads to 6 without adding any index calculation overhead! It's quite similar to the "horizontal unrolling" discussed above, where you just read more consecutive bytes (2 x 32B reads in this example), but that approach nearly doubles the table size (since you need to accommodate a 64B read at all positions), while this approach increases it very little (by 2 bytes, probably, since you just need to accomodate the maximum offset of +2 at every position). The code is different per-prime: the code for adds 4, or else subtracts 1 (they are equivalent, but you need to design your index handling to account for which direction you are going). This approach is promising because it lets you get more reads without greatly increasing the table size. It works best for the smaller primes since the jump amounts (and hence required table padding) are smaller, while the horizontal unrolling described above works best for larger primes since the table increas is relatively less (since it is fixed to the read size and the large primes already have larger lookup tables). Unrolling both the inner and outer loops allows you to even pick and choose different strategies for different primes. Optimize the Lookup Tables I just made the tables 2D arrays, for simplicity, but this wastes a lot of space, since the rows for the smaller primes are often very short (I padded out some of them with to help catch bugs). To optimize this, you'd probably want to first pack the tables more tightly, either as a jagged matrix (i.e., an array of pointers to rows), or just as one large packed 1D array. The latter approach is great if you've done a lot of unrolling as described above since the various instructions can just directly embed the offset into the array in their memory operand "for free". I didn't make any effort to align any of the lookup tables at all (and they won't have any natural alignment since they are all bytes. No doubt about 50% of the 32B loads will be "split loads" that cross a cache line. With a bit of care you can reduce that to 0% for the smaller tables with very little size increase. For the larger primes I think you can reduce it to 0% but at a 100% size increase (just thinking about it, I haven't checked), which may not be worth it. Furthermore, depending on the number of consecutive bytes you are reading (see ) there are opportunities to dramatically reduce the larger tables. For example, if you are doing 64-bit reads like the non-SIMD bitmap algorithms, for any prime >= 67 every 64-bit read returns either an all-zeros value or a value with exactly one bit set. Yet such each such prime is using their own large lookup tables which are mostly zero. To support any possible 64-bit read for all primes >= 67 you need only 8 zero all bytes, and 8 other 15-byte regions with the all bytes zero other than the middle byte which 1 of the possible 8 bits set. You can overlap this all nicely so it takes about 72 bytes. So you can replace all so you can replace the 13 * 134 byte lookup tables for the primes from 67 to 127 by 72 bytes: a reduction of about 25 times! Even better, this scales as you add larger primes: even if you want to add 100 more primes, you don't need any additional lookup tables for the bitmap. For the fully generic algorithm which uses the table on every calculation, this transformation is free. For the unrolled versions that encode prime-specific knowledge into the reads it doesn't work as well. It also doesn't work as well for the larger reads: the version that reads 64B (512 bits) never gets close the "zero or 1 bit" set case for the first 30 primes, so you can't use it there. It would be useful if you wanted to use more primes, however and since this algorithm is so fast, it makes sense to do so. Combine Small Primes Currently every prime is handled separately: although there could be some bitmap sharing for larger primes as described above, each prime still implies at least one to incorporate it. There is nothing particular special about one-prime-per-bitmap, however: why not simply combine several primes together into one pre-calculated bitmap? Instead of having two bitmaps for 3 and 5 like: 

This parser searches for numbers and returns them. Whitespaces separate numbers as whitespaces are allowed to occur multiple times. If you input alphanumeric characters the machine goes into the Error-State. This example uses the state pattern to harmonize the parsing. This corresponds to a state chart (like UML state machine) that should be created before. Interpreting The interpreter will be informed by new semantic elements from the parser and decides when to put functions and parameters together so they can execute. This could be done with a standard listener pattern. The interpreter registers at the parser as a listener so it will get functions and parameters as they are available (parsed correctly). In the parser example provided we have to notify the listeners if a new number could be determined: 

Avoid multiple return statements Try to reformulate your algorithm so you have only one return statement at the end. The problem is that the constructs like break, continue and multiple return are not refactoring-friendly. Extracting methods out of the current method that contains such constructs will lead to reformulating your whole control flow. Naming Try to rename i, j, count and index to match there semantic best. (e.g. startIndex, offset, word2Index, ...) 

Anonymous classes Registering objects (functions) in a HashMap together with its character representation is ok. What I am missing here are structures that have expressive names for the functions. I'd resolve the anonymous class to a named class. Avoid static initializer As parsing itself is a complex thing you should keeps things around it as simple as possible. Static initializer were executed out of your control when the corresponding class is loaded. I do not say static initializer should never be used. But in a business context you should rely on constructors and methods in objects as the primary way to execute code. Multiple parsing concepts You are following different parsing concepts and your parsing is spread all over the place (inheritance, different methods and semantically different classes). 

The first row (for prime 3) means that after doing a 64 byte read at position , the next read should start at position to properly stitch the bitmaps together, the next at 2, and so on. The row for 5 says jump from 0 to 4, then 3. Notice though that these are simple increments the prime. So the series for 5 is just "plus 4, mod 5": . All the rows are similar. So if you are reading 4 consecutive 64-byte bitmaps for prime 3, you can do it directly like this, without any index calculations at all (assuming has the base of the LUT for the prime): 

Each bitmap is "normalized" such that the LSB is always 1. Then the loop is very simple: it loops over all 30 primes, and shifts the bitmap by the right amount to "align" it, and ORs all the results together. In this way, the loop handles 64 candidates. The shift amount is simply the amount need to correct stitch the bitmap from the last iteration so that it is periodic. Using a 16-bit example for sanity, the bitmap for in binary is . In the next iteration, you can use the same one since the effective 32-bit bitmap would be . Oops, two adjancent 1s! You just need to shift it over by 1: and now it stitches fine. In general the stitch amounts for any prime have period and take all the values between and inclusive. The last line in the loop calculates them. Let's try this guy: 

In any of these situations all other code is not touched. That is because this sequence only defines the process. If you do this with your code some changes may be easier than others but all of them will have effects on either compilation units that also have other responsibilities and/or your changes will affect multiple compilation units to make it work again even if you want to change only one aspect. Another indicator that you have high coupling is that you chose an integration test to test your Calculator. I do not say "do not make integration tests". I only wanted to mention that decisions made have reasons. These reasons may be valid or not. But for me integration tests are an indicator for less useful code separation. AskFor... These classes are wrappers for the Scanner. But they also assume that the console output is available. Introducing the console output will produce a second ouput beside the validated return value got from the console input. Removing the console output will make these classes well-defined and reusable even if no console ouput is available. That doesn't affect providing a context message: 

LDAP access If you have access to the LDAP interface of your Active Directory you can query LDAP for a specific computer account: 

Note - Concerning my original post: I am divided and can not choose between either of the two users mentioned in this post as to who to accept for an accepted answer; both were extremely helpful. I up voted both users answers, yet I'm still undetermined on who to accept. 

Questions - Concerns I would like to know from both users who have provided a solid, well detailed, and informative answer if the changes I've made reflect what was suggested. Is the code accurate or does it still need some improvement? As for my next question; it deals with the overloaded . It is supposed to allow a user to define their own to be passed to the seed function in order to seed the generators or engines. This can be answered by anyone who feels that they have critical feedback. The function is untested, the code does compile and run without trying to use it. Does the for the function declaration look correct? If so, kind of a stack overflow question but, how would I integrate the needed parameter(s) for this function into the this class's constructor? This particular function overload has me held up. 

I have previously asked A library of books to demonstrate C++ to a C programmer and I took some of the advise from user: JDlugosz and this is what I've managed to come up with so far: My question(s) are relatively the same as in the original, however I would like to know if the changes that I've made based on some of the suggestions are appropriate, if I had missed anything, are there any mistakes, and is there any more room for improvement? The only major internal changes are: instead of using I decided to use for better performance, and instead of auto generating a number the user now has to type that in as well. I had to update a few of the functions in in which I originally forgot to do. I originally posted this code with the change where the function was updated, but forgot to update the to for and functions... They are now updated as well to reflect the full change of using a instead of an converted to a . Now the is fully accepted by the user via console as a string. No need to convert to a . main.cpp 

You should have a look at BigInteger to avoid overflows and you think about numbers exceeding the Long-Type. This will not be easy. You should not compare Integer-Values with Long-Values as you are using them within the same semantic. Harmonize the algorithm to use only one type for one purpose (in this case Long) 

Write your own OR-Mapper Use an existing OR-Mapper Develop your code towards an OR-Mapper Ignore the problem through adding specific code that makes your solution more inflexible 

To make my point I have to reach back a little bit. I suggest to read this all or nothing. I currently came from a project where they used an "event bus"-like mechanism. It was used in the context of the web ui framework Wicket. Wicket itself provides an event handling. In the special case of Wicket they had to solve some problems. The Wicket internal event handling was based on defining an event and provide it to an event sink. This sink was an ui component in the wicket ui component tree and there was no need to use the root. Furthermore you have to define in which direction the event should be sent: upwards the hierarchy or downwards. So with this mechanism some parts of the hierarchy may not be reachable with only ONE event. So often there were more than two events involved to achieve the wished goal (in their case to update the ui component). In one case they had defined a cycle by making two receivers also senders passing event to each other forever. Then they came up with the idea of a central instance responsible for registering as an event sender and registering as an event receiver. This is nearly exactly the szenario you have. Honestly, this is a much better approach than the original wicket event handling. And many sites write recommendations on that and all in different contexts. But there are also some problems remaining. For example if an object registers as a sender of an event that already was registered you have to decide wether to allow it or not. Will a change of a sender or an additional sender affect the receivers? One other thing is you have to keep your events semantically unique. How do you ensure not to have two events (accidentially differently named or typed) that mean the same. Or vice versa: One event triggers receivers that accidentially listen to this event? This nearly always happens if you have abstract/generic algorithms that fire events like "Update". You have to ensure that your events remain semantically concrete. And because you can only identify an event by its type the information who sends the event cannot be evaluated usefully by the receiver even the event contains the sender. The receiver cannot distinguish between different senders of the same event. This information cannot be reconstructed. Here is the thing: An event bus should replace the observer-pattern but it doesn't. An observer-pattern decouples two objects in one direction. The receivers were nearby when they registered themselves to the sender. An event bus decouples two objects bidirectional. The receiver is now coupled to the event type. Because of that and event types should be semantically unique the receiver is implicitly coupled to the type of the sender. So the receiver is not able to distinguish between different senders of the same type anymore not as the observer-pattern. And if abstraction and inheritence is involved you can easily crash your event bus. I don't say: Do not use an event bus. But I strongly recommend to prefer the standard observer pattern. Everything else will lead to things you have to memorize additionally in every day you use this. You always have to be aware of the restrictions of such an event bus. The observer-pattern really has no disadvantages in its concept. Maybe you have ONE disadvantage within concrete programming languages: effort to write "a lot of" code. Some mainstream developers also address that you have to think about how to reach the sender within the object structure to register as a receiver. Yes, you have addtional effort to think about the composition of your objects. But that you always should do. Why are you thinking about clean architecture? By using the observer pattern you maybe have to write more code but it is not producing ANY PROBLEMS or has ANY RESTRCICTIONS you have to be aware of when apllying it correctly. In implementation deviations from this concept you may write less code but you definitly produce problems. This has something to do with irreducable complexity. No I am not a creationist. You have to think about it like a compression algorithm. An observer pattern is a lossless compression of a modelling problem. An event bus is a lossy compression of the same modelling problem (at least in OO). And because it is a lossy compression must be aware of the errors. The rethorical question is: If you bring up clean architecture as the base and the effort of design why then using a "lazy" implementation of the observer pattern?