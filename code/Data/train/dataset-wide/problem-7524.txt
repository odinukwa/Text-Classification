This would seem to be a common question, but I am surprised not to see it already asked and answered on MO! I am teaching an undergraduate course, and I want to teach them to construct basic epsilon-delta proofs, say, of $\lim_{x \rightarrow 3} x^2 = 9$ and $\lim_{x \rightarrow 4} x^2 \neq 17$. (Elementary, continuous functions only.) This is a serious stumbling block for many students, with good reason, and I anticipate it will be for mine as well. Do other MO'ers have suggestions (beyond what I can find in typical calculus books) for presenting epsilon-delta for the first time? Any success stories to share? Thank you very much! --Frank (Background: I am teaching a discrete math course to American undergraduates who have already had a year of calculus. Whether $\epsilon-\delta$ is on topic for discrete math is perhaps questionable, but we did material on making sense of statements with lots of quantifiers, and also an introduction to techniques of proof, and so the material seemed like a natural fit. I should also mention that I intend to test the students on this material and not just expose them to it.) 

(And a related question: Where should an analytic number theorist learn about Bessel functions?) Bessel functions occur quite frequently in analytic number theory. One example, Corollary 4.7 of Iwaniec and Kowalski, says the following. Let $r(m)$ be the number of representations of $m$ as two squares, and suppose that $g$ is smooth and compactly supported in $(0, \infty)$. Then, $$\sum_{m = 1}^{\infty} r(m) g(m) = \pi \int_0^{\infty} g(x) dx + \sum_{m = 1}^{\infty} r(m) h(m),$$ where $$h(y) = \pi \int_0^{\infty} g(x) J_0(2 \pi \sqrt{xy}) dx.$$ $J_0(x)$ is a Bessel function, and I+K follow with four equivalent integral expressions -- the equivalence of which is not at all obvious by looking at them. Looking at Lemma 4.17, the relevance appears to be that Bessel functions arise when you take Fourier transforms of radially symmetric functions. Another example comes from (3.8) of this paper of Miller and Schmid, and the relevance comes from the identity $$\int_0^{\infty} J_0(\sqrt{x}) x^{s - 1} dx = 4^s \frac{\Gamma(s)}{\Gamma(1 - s)},$$ where the gamma factors come from functional equations of $L$-functions. Okay, if this is true, then I understand why we care, but it seemed a bit deus ex machina to me. There are many other examples too, for example the Petersson formula in the theory of modular forms, etc. There are $I$-Bessel functions, $K$-Bessel functions, $Y$-Bessel functions, etc., all of which seem to satisfy a dizzying number of highly nontrivial identities, and reading Iwaniec and Kowalski one gets the sense that an expert should have the ability to recognize and manipulate them on sight. They also provide references to, e.g., (23.451.1) of a book by Gradhsteyn and Rizhik, and although I confess I have not looked at it, I can infer from the formula number that it is not the sort of thing I might read on an airport layover. Meanwhile, Wikipedia tells me that they naturally arise as solutions of certain partial differential equations. Looks extremely interesting, although I'm afraid I am not an expert in PDE. As an analytic number theorist, how might I make friends with these objects? How should I look at them, and what conceptual frameworks do they fit in? Thank you! (ed. Thanks to everyone for informative answers! I could only accept one answer but +1 all around) 

Make sure that this issue becomes sufficiently notorious that it becomes known to anyone applying for the Analysis positions. If they are good enough to be able to choose among competing offers, then they will think about this, and realize that the axe could be on their head next time. 

A simple illustration of the Mean Value theorem (particularly good for a less theoretical course, but I like it in any setting): A man is photographed at a tollbooth at 12:00, and then arrives another tollbooth, 250 miles down the road, at 2:00. A cop pulls him over and gives him a traffic ticket for driving 125 mph. His defense lawyer claims in court, "You can't prove that there was ever any particular moment when my client was actually driving 125 mph..." 

Here is a recent paper by Dimitris Koukoulopoulos, which obtains the strongest known form of the Prime Number Theorem without heavy use of complex analysis. One can find the symbol $i$ in the paper, but it does not rely on the analytic continuation of the zeta function. This is a direct extension of the "pretentious" methods of Granville and Soundararajan, mentioned in an answer by Thomas Riepe. 

Here is one motivation. By elementary algebraic manipulation, we have $$1 - 2 + 3 - 4 + \cdots = \frac{1}{4};$$ see, e.g. [broken link - ?; search Wikipedia for 1 - 2 + 3 - 4] But (see here) we have $$(1 - 2 \cdot 2)(1 + 2 + 3 + 4 + \cdots) = 1 - 2 + 3 - 4 + \cdots,$$ and therefore $$1 + 2 + 3 + 4 + \cdots = - \frac{1}{12}.$$ Of course, to do all of this, one has to ignore all those rules that pesky analysis professors tell you about. But the last identity is just so cool that one feels compelled to try to prove it rigorously. 

I am curious, what kind of exact formulas exist for the partition function $p(n)$? I seem to remember an exact formula along the lines $p(n) = \sum_k f(n, k)$, where $f(n, k)$ was some extremely messy transcendental function, and the approximation was so good that for large $n$ one could simply take the $k = 1$ term and truncate this to the nearest integer to get an exact formula. Reviewing the literature, it seems that I misremembered Rademacher's exact formula, which is of the above type but which requires more than one term. I am curious if there are other exact formulas, particularly of the type I mentioned? Also, if I am indeed wrong and no such formula has been proved, is some good reason why it would be naive to expect one? Thanks. 

A little bit vague, but I hope useful for the entire community. I am, by training, an analytic number theorist. I have managed to learn some algebraic geometry, by reading parts of Silverman's Arithmetic of Elliptic Curves, the beginning of Ravi Vakil's notes, and a very little bit of Hartshorne (and doing a least a few of the exercises). It has become apparent to me that it would be very useful to learn more -- the subject often comes up in reading, seminars, and discussions with colleagues, and besides I find what little I know to be fascinating. Unfortunately I don't have the time to, say, read Hartshorne (or EGA) and do all the exercises. That said, how would MO readers recommend that I go from knowing a little bit of algebraic geometry to a modestly bigger bit of algebraic geometry? My goals, roughly speaking, are: 

As to "Why is it taught in undergraduate courses?" I think that many constructions needed in advanced mathematics use concepts from multiple different fields. For example, if you want to prove anything about the adeles (in algebraic number theory), you need to have a good background in both algebra and point-set topology. Sylow's theorem, to cite your example, is interesting in and of itself, but I think that one purpose in teaching it is to give some meat to students who have recently learned the definition of a group. It is definitely nontrivial, one can appreciate and prove it without having to know mathematics outside group theory, and it gives students the chance to apply techniques they have learned (counting orbits and stabilizers and such) without asking them to learn new abstractions. 

I'm happy to announce a new result of the shape you ask for: if $F$ is a cubic field (of any signature) then the size of the 2-torsion in its class group is bounded above by $O(D_F^{0.2785})$. The same is also true if $F$ is a number field of any degree $n$, but in this case one has to replace $0.2785$ with a function of $n$ that rapidly tends to $1/2$. This is joint work of Manjul Bhargava, Arul Shankar, Takashi Taniguchi, Jacob Tsimerman, Yongqiang Zhao, and myself. (We did this at an AIM mini-workshop; I highly recommend these to anyone who has the chance!) I regret that the paper is in a rather rough state of preparation, and so I don't have a preprint to share. Once we're done, I'd be happy to e-mail a copy to you or to anyone else who leaves their contact information in the comments. 

To give a vague answer, I think these questions are difficult because they mix multiplicative conditions (being prime) and additive conditions (as in the twin prime case). Basically all results about primes that I can think of come down to unique factorization of the integers. For example, the zeta function is given as $$\zeta(s) = \sum_n n^{-s} = \prod_p (1 - p^{-s})^{-1}.$$ The right hand side is why the zeta function tells you about prime numbers, but the left hand side is what typically helps you prove theorems. For example, Riemann noticed that the left side looked like something similar to what Poisson summation is good for, and hence proved analytic continuation and the functional equation. On a simpler level, one nice proof that there are infinitely many primes is to observe that $\sum_n 1/n$ diverges, by elementary calculus, and therefore the right hand side diverges for $s = 1$ as well. Gerhard Paseman suggested looking at arithmetic progressions, and I think this is an extremely instructive example. Looking at the sum of $n^{-s}$ restricted to an arithmetic progression, you don't have any equation like the above. Conversely, if you take a product over only the primes $p$ in some arithmetic progression, you don't get anything nice like the left side. However, if you let $\chi$ be a Dirichlet character, e.g., a homomorphism from $(\mathbb{Z}/N)^{\times}$ to $\mathbb{C}$, then you get the Dirichlet $L$-function $$L(s, \chi) = \sum_n \chi(n) n^{-s} = \prod_p (1 - \chi(p) p^{-s})^{-1}.$$ In some way this is forcing a round peg into a square hole: the arithmetic progression condition couldn't be handled directly. But it can be written as a linear combination of Dirichlet characters, and once you force everything to be multiplicative, the machinery (Poisson summation, etc.) all works. So in other words, IMHO, the question isn't "why is the twin prime conjecture difficult", but "why can we prove anything about the primes at all?" Our toolbox is, in my experience, still pretty limited. 

Although I don't have the reference convenient, I believe that the last chapter of Halberstam and Richert's book Sieve Methods states (and proves) Chen's theorem with an explicit value of c_1. As I recall, it is roughly 3/11 times the "expected" constant from probabilistic arguments. 

I recommend Silverman's The Arithmetic of Elliptic Curves. Silverman takes the highbrow approach, but writes in such a way as to make his book friendly and accessible for newcomers. 

As Will Jagy explained, $h(-D)$ is roughly $\sqrt{D}$, given by Dirichlet's class number formula $$h(d) = \frac{w \sqrt{d}}{2 \pi} L(1, \chi_d)$$ where $L(1, \chi_d)$ is the L-function associated to the quadratic character of $\mathbb{Q}(\sqrt{-D})$. Upper bounds for $L(1, \chi_d)$ are easy to prove; you can get $\log(d)$ by partial summation, implying the bound $h(-d) \ll d^{1/2} \log(d)$. Effective lower bounds are notoriously more difficult. I am fairly sure that the function $f(d)$ you describe is not known to be surjective, although it is widely expected to be; class numbers are fairly difficult to get a handle on, although a variety of divisibility results are known. However I am not entirely sure of this! 

This is a very interesting book on the subject, written by and for mathematicians, and likely to provoke discussions. I am guessing it would be ideal for your course. 

[Substantial edit: As I mentioned previously, cubic fields can have the same discriminant but not be isomorphic; but I've revised my answer to better address the author's question.] There are a lot of polynomials that will generate the same cubic field. Here is the basic idea, due to Delone and Faddeev. Write down the cubic form $f(x, y) = ax^3 + bx^2 y + cxy^2 + dy^3$, where setting $y = 1$ gives a generating polynomial for the cubic field. Then there is an action of $GL_2$ on such cubic fields; if $\gamma$ is a 2x2 matrix with entries $\alpha, \beta, \gamma, \delta$ then $\gamma f(x, y) = f(\alpha x + \gamma y, \beta x + \delta y)$, optionally with a factor of $(det \ \gamma)^{-1}$. Anyway, if two cubic forms are $GL_2(\mathbb{Q})$-equivalent then the corresponding polynomials (if irreducible and nondegenerate) generate the same field. Moreover, if two cubic forms are $GL_2(\mathbb{Z})$-equivalent then the corresponding polynomials generate the same order over $\mathbb{Z}$. See here, page 10 of Section 4, for a good reference. [Previous answer:] I think the underlying question is how often nonisomorphic cubic fields have the same discriminant. This happens, but it doesn't seem to be very common. You might find it interesting to browse through this database which lists cubic (and other) fields, their discriminants, and their minimal polynomials. Theoretically, I believe it is known that this happens infinitely often, although I don't recall the reference. For upper bounds, it is known that the number of cubic fields of discriminant n is $O(n^{1/3 + \epsilon})$, due to Ellenberg and Venkatesh; the "correct" bounds are believed to be much sharper. 

The Scholz reflection principle says, among other things, that if $D < 0$ is a negative fundamental discriminant, not $-3$, then the 3-ranks of the class group of $\mathbb{Q}(\sqrt{D})$ is either equal to that of $\mathbb{Q}(\sqrt{-3D})$, or one larger. Does anyone know of (and recommend) any introductory reading on this fact? Why it is true, what context to view it in, etc.? Googling reveals some highbrow perspectives on it, some interesting applications, and citations to Scholz's 1932 article (which I'm having difficulty accessing for the moment). All of this is interesting, but there doesn't seem to be any obvious place to begin. Thank you! 

I am teaching Calc I, for the first time, and I haven't seriously revisited the subject in quite some time. An interesting pedagogy question came up: How misleading is it to regard $\frac{dy}{dx}$ as a fraction? There is one strong argument against this: We tell students that $dy$ and $dx$ mean "a really small change in $y$" and "a really small change in $x$", respectively, but these notions aren't at all rigorous, and until you start talking about nonstandard analysis or cotangent bundles, the symbols $dy$ and $dx$ don't actually mean anything. But it gives the right intuition! For example, the Chain Rule says $\frac{dy}{du} \cdot \frac{du}{dx}$ (under appropriate conditions), and it looks like you just "cancel the $du$". You can't literally do this, but it is this intuition that one turns into a proof, and indeed if one assumes that $\frac{du}{dx} \neq 0$ this intuition gets you pretty close. The debate about how rigorous to be when teaching calculus is old, and I want to steer clear of it. But this leaves an honest mathematical question: Is treating $\frac{dy}{dx}$ as a fraction the road to perdition, for reasons beyond the above, and which have not occurred to me?For example, what (if any) false statements and wrong formulas will it lead to? (Note: Please don't worry, I have no intention of telling students that $\frac{dy}{dx}$ is a fraction; only, perhaps, that it can usually be treated as one.) 

The character sum you ask about is $\gg \sqrt{p}$ for at least one value of $n$. I learned the following slick proof from a paper of Leo Goldmakher: We have $$\tau(p) = \sum_{n \leq p} \bigg( \frac{n}{p} \bigg) e^{2 \pi i n/p}$$ and that is a Gauss sum with absolute value $\sqrt{p}$... now use partial summation. In addition, there is an infinite family of characters for which the lower bound may be multiplied by an additional fractional power of $\log \log p$. A result like this was first proved by Paley. Please see the paper I linked to for proofs and references to earlier work. 

As an undergraduate I learned point-set topology from Munkres's book, as did many others. One topic that gets a lot of attention is the separation axioms. For example, a space $X$ is normal if any two closed, disjoint subsets of $X$ can be separated by open neighborhoods. Some of the axioms (e.g. Hausdorff) turn up a lot, but I feel like I virtually never hear topological spaces described as regular or normal (and there are a host of other properties too!) However, there are some fields I don't interact with too much. In what contexts outside of general topology and set theory do these axioms, and the theorems you prove from them (Tietze extension, Urysohn's lemma, etc.), play an important role? Thank you! -Frank