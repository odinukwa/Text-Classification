It shows that the developers are aware of the behavior, even if the reason why it's like that originally is not specified, and its effect on were probably not intended. C-style comments don't have this problem. They're multi-line and nestable, so they're handled quite differently by the lexer. Comments inside functions are also no affected, since the function body as a whole is an opaque string for psql. 

The 1st line concerns the user. It's irrelevant for your pg_dump command since you're using the user with The 2nd line concerns any other connection through Unix domain sockets (TYPE column is ). From the client, it means when you do not use . It says that if the OS user is the same name than the db user, he doesn't need a password. The 3rd line says that if is used (IPv4 TCP connection), a password will always be asked to the client. The 4th line says the same with IPv6. Based on this, this command run by the OS user should not ask or need a password: 

although I think you may use symbolic links to circumvent that limitation, if using a file system that supports them. Here's a working example: 

I believe the settings you mentioned already ensure that geqo will be used, except that the values for and are so low that the results won't be good. Apart from that, to get different plans, you should play with . From the doc's section Generating Possible Plans with GEQO: 

When and are used together, the argument to is not the name of the database to create, it's the name of an existing database to connect to run the statement, because it's impossible to create a database if you're not already connect to another database. This is documented as: 

Personally I had success with when faced with this problem with a MySQL client app, but don't take this as a recommendation, my understanding of TCP being superficial at best. 

Anyway since accumulating results is already built in , you don't need any of the above. Quoting from the documentation: 

Is it really the case that only the owner a large object is ever able to delete it, or I'm missing something? My use case is a db that stores mailboxes that are shared between different db users. All the tables with mail contents and large objects with mail attachments are owned by a "master" user, and other users may or may not delete messages depending on their database role. Deleting a message means deleting everything related to it, including attachments stored in large objects. So it would make sense that a non-owner should be able to delete large objects, just like he can delete from other tables if he has been granted the DELETE privilege on these tables. 

This error generally means that the server is not running. Based on output and the thread of comments, it was due to the main package being somehow uninstalled. Since the uninstall hasn't been called with the option to , the data and configuration files are still there, so can fix the problem. 

Ubuntu or Debian can run multiple instances of PostgreSQL and provide a specific way to autostart/stop/start each cluster. There should be a file named inside (or more generally /etc/postgresql/<version>/<clustername>) with these self-explanatory contents: 

The field separator can also be set with or Once the results are in the file, use alone to redirect the output back to the terminal. 

If the other server is the master, it's normal because having different passwords in the replicated database is not possible. PostgreSQL replicates everything from the master, including user accounts and their passwords. There's no way to divert that temporarily. If someone needs to be superuser on the secondary without knowing this password, a different authentication method should be used on this server, such as Certificate Authentication, or external methods (ldap, radius...) . As a configuration file, the can be different on the slave. 

The command needs an exclusive lock, which affects concurrent readers in a way you may want to anticipate: 

Due to how is restricted, the above should be run as user (otherwise consider querying directly as the non-standard alternative). 

In the first step, is not in the table yet so the INSERT is executed (or if it's in the table already, then it's not executed, but that's not the case you're asking about). In the second step, is now in the table so the SELECT is executed too. See Rules on INSERT, UPDATE, and DELETE in PostgreSQL documentation for more details. The (wrong) idea that the SELECT should not be executed when the INSERT gets executed is a common misinterpretation of how RULEs work, because intuition is misleading here. When looking at the rule, we're tempted to think that it does this: 

It's not technically true that is not installed with postgres packaged for Debian/Ubuntu. It's actually installed in , where is the postgres version. This path is intentionally not in any user's default PATH since it should never needed to be invoked directly. The reason for this setup is that the Debian/Ubuntu packaging supports several PostgreSQL instances running in parallel, either identical or different major versions. To learn about that, it's best to read first $URL$ , before the real PostgreSQL manual or any other non-Ubuntu documentation. is a multi-master fork of PostgreSQL that shouldn't be confused with PostgreSQL itself. The hint suggesting to install it to get is an unfortunate nonsense, probably a machine-generated advice based on the contents of . 

Q: Does PostgreSQL uses a hash function for checking equality of integer arrays or does it perform a brute-force algorithm comparing one-by-one the elements of the array? Not according to Array Functions and Operators in the doc: 

This kind of query is called a relational division, where you "divide" by to obtain a "quotient" that represents a number of films, and then filter out on this number. The canonical way to write it would be: 

The idea behind the large object API is to mimic a file-like API. The OID is like the path of the file, and the file descriptor obtained by or is the equivalent of the POSIX and system calls for files. JDBC provides and the both libpq (in C) and the server have built-in functions. So yes it makes sense to replace LO contents (keeping the same OID) by a truncate followed by if libpq, or if server-side code, or if using the dedicated JDBC class, as you could do with a file on a filesystem. It would work as well to create a new LO with a new OID and then unlink the old one, it's just less elegant and it consumes a new OID for no good reason. 

If you're INSERTing/UPDATing new tuples at a constant rate, the ratio of dead tuples against the total number of tuples () is getting smaller and smaller as the total number of tuples is ever-growing. As vacuum does its job, the number of dead tuples is reduced and kept under control, but not the total number of tuples. So over time, the vacuum threshold will be reached less frequently, to the point where it seems like autovacuum never processes the big table in practice. You may reduce the treshold for this table to increase that frequency, but be careful about the additional I/O bandwidth it will require. A table that grows fast and forever is not sustainable anyway, so you may think of design changes to partition and separate the manageable amount of live data from the much bigger amount of read-only data which can be safely ignored by autovacuum. 

but it's a new feature of PostgreSQL 10, which you probably don't run since it hasn't yet got out as a release (it's still at the moment). With previous versions, you're pretty much on your own to figure out the current name, either by interpreting , or by finding the latest modified, for instance with a combination of and : 

It's just a case folding problem. You're trying to connect with but due to case folding rules, you created in lowercase, as the output shows. Per documentation: