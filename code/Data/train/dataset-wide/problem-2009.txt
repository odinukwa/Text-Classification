The performance drop is probably becase in the 1 day of data, the database engine may be doing a sort 1000* on 1000 pices of data, but the month is sorting 1000* for 1000 pices of data/day * 30 days (where 1000 * 1000 << 1000 * 1000 * 30 ). It may be faster to run the query 30 times for 30 days of data rather than once for 30 days of data beacuse the amount of data that the query has to loop through is less. As others have explaned, you need to analyse your query and try to 1> reduce the amount of data the query looks at by excluding as much as possible 2> Choose indexes so that the data that the query has to search through is optimised. Maybe change the idex type (if the engine has different types) 3> change the order that the joins take place, may change the exection order that the planner chosses. 4> maybe load some of the "where in (" data into a temp table as a seperate query before the main query or create a string if there arnt may values and dynamically create the query (so rather than "... where in (select .. " use "... where in ( 1,2,3...)"). As others have pointed out look at the query plan and see which parts take the longest time - they will be probably be joins and scans (DISTINCT clause) and they to optimise these by making sure there are indexes to assist and maybe restructuring the data. You should have an index on [FACT].[DataMine].PartitionID, but it may make a difference to drop that before the insert and then re-create it, as sometimes this is faster. Possibly try a bulk insert as well. You would need to load the results of this query into a temp table first though, otherwise the sub queies would have no index on this field when they are selecting, which would make the situation worse. 

You can use the import data tools that are part of access to import data from one database into the other. It is under the "External Data" tab. This should work as long as you have network access to both databases. Some more info : $URL$ Before doing this, have a little think about your table design - do you need to have a flag to indicate which store the data came from? If the data is normalized, you may have have issues importing data that may be resolved by the order that the data is imported in. Also, remember to regularly backup these database files before trying to import. This way if you break something you can go back and start again. Also store them on physically different disks just in case.... 

This is something you can use views or queries for. Have one table with the raw data and another with the conversion ratio that you need to apply - maybe with a date or some such other information if the ratio to apply is time or situation sensitive. Then create a query or view that does the conversion you need by joining the tables together. This way you can change the conversion ratio value as required and recalculate. Simple example (PostgreSQL) for a hypothetical scenario, yours will be different: 

From your question, the key is how you intend to replecate the data, and where you will store that. How you do that would depend on how you update your database engine, hardware and performance requirments. You could use your database engine to create backups (differencial or full) that you restore into another database instance, possibly on different hardware (periodical update). You could create trigger functions (real time update) or a regular batch process (periodical update) that port the data across to your reporting instance - either a different set of tables, or in a difference table space or a different database (engine dependant). You state that the data will be logically isolated from the active data, so you wont get any logical contention (table locks) between the main and reporting parts of the application but you could have issues with disk and CPU between the main and reporting parts of the application that would affect the main applications performance if the reporting data is on the same hardware as the main application database - but that would depend on load and hardware spec. 

Do you have any locations that the packages are stored in/at (like shelf number/silo/etc)? If you do, then you can have a table with locations and package at that location. To find the adjacent packages you need only to know the location id +/-1 location. The location ID would need to be unique and sequential. If not, then you would could define a constraint (which probably means using a database engine that is more powerful the Access, maybe SQL server express?) Then you could create a trigger function that checks that the X next to Y == Y next to X on any updates. Another option is to create a linked list structure in your table, so that the data element for X has next filled and Y and the previous element for Y has X, with the prev/next/current fields having a unique constraint on them. Eg 

And then to be real sure put in a constraint or trigger function to check that prevID matches the nextID of the previous element. 

Sounds like a messy thing to do from a management point of view. Just how do you plan to backup that many databases? with a script that loops though each one? Unless you have a really good reason, why not just have one database where the structure is designed so that all the data will link to back to a customer ID. Add Indexes/Foreign Key/Primary Keys based on this field which will ensure data integrity. Then you just need to have a where clause in all your queries to access only one customer ID. This will be much simpler to maintain and is just as easy to develop (cause in either case you need to allow for the customer identification) 

Try the following tables user(user_id(pk), name, email) flashcard_group(group_name(pk), group_tag) flashcard(flashcard_id(pk), question, solution) flashcard_groupings(group_name(fk), flashcard_id(fk)) <- to join flashcards to groups guess(guess_id, user_id(fk), flashcard_id(fk), is_correct) <- user_id and flashcard_id to be unique key. The above assumes that there is 1 user to 1 set of guesses, and that the flash card grouping is grouping the flashcards together (ie cards a, b and c and in group green, etc). If a user can have multiple go's at this then you need to add another table to hold the session information between the guess and user, a bit like what the table flashcard_groupings is there for. To get the queries you are after, join the tables as needed to get the groupings you need. For guess percentage, just join tables user and guess. To get the flash cards join user, guess and flashcard. And so on. 

Some thing not to look at are the disk sizes of tables and indexes as they may be different in the restored database (I would expect smaller in the restored version, but it depends on a lot of things) 

Somehow you need to maintain a list of the stored procedures that relate to each software version. There will be a pile of ways to do this, but the version/procedures will need to be documented clearly somewhere or maintaining the code base will be difficult (expensive) and end up buggy. One method that springs to mind is to maintain a list of the procedures and their versions - as a database table or text list or web.config or compiled into your application. This list would have the procedure name and the version that it corresponds to (maybe with a base version) Then in the code you will need to write a method or class that will get the relevant procedures name as a string that you can return to the Data Access Layer (DAL) to use - I am assuming that the DAL know which query to run. To keep it simple, have a base list and then only put in "version" procedure entries where they are needed and get the most recent one. You could take this a step further, and dynamically create the procedures that you need by passing the query text or variables (procedure names, table names) to be substituted into your query in the stored procedures at run time. ie: if procedure foo needs to call procedure bar.vn the pass bar.vn as a variable to procedure foo. This allows you to dynamically take into account different versions, but wont allow the database engine to effectively create statistics for that procedure. Here is a example of how to code something like this up $URL$ The best solution is to not have multiple versions at once, try to have the different functionality as different execution paths in the application (code or stored procedure). It will mean a few IF..ELSE bits of code, and what looks like a bit of work, but is much easier to maintain because the alternatives are in front of the developer - they do not need to know about which version needs which procedure. 

When it comes to databases, less is more. You are doing the right thing by archiving off old data as this will speed up the performance and reduce the maintenance time of the active database. The archived data needs to be considered. Keeping it in a separate database instance will be beneficial because backups/restoration and other maintenance activities will be completed separately - reducing the effect on the live data. You should consider keeping the archived data on a physically different disk to the live data - this will avoid conflicts if both are being used at the same time. Also consider the type of disk this the archive is on as this has cost implications - does it really need to be on a raid type array or is a single disk and backup tape/dvd all that is needed? Storage may be relatively cheap, but it is still a cost in $, IO, time and network. Next take a look at the archived data - do you really need to keep all the records? do you really need to keep all the columns of data? Would changing the structure of the data result in a smaller disk footprint/faster read query time? Can you summarize the data? Archived data for data mining does not always need to have the same structure as live data. Keeping separate months data in separate databases may be a good way to go, but there other options that may make the analysis easier/quicker. Your options here include keeping the data in one database instance, but with table per month. Another is to have one set of archive tables and use table partitioning (read the manuals). 

I am assuming here that the system is not a highly used transactional system and that you have windows of low usage to run the analysis queries. If you need to maintain high levels of performance, you may like to do the above in a separate database (separate hardware) and port across the new data that you get from backups. 

I half remember a problem like this. I think I ended up migrating the schema then dumping the data as csv and loading the data from the csv file. I remember having to update the csv file (using unix tools like sed or unixtodos) or using open office calc (excell) to fix up some items that were errors on the import step - it could be as simple as opening and re-saving the file. 

Once you have normalised you need 2 queries, one to load parents then one to load children (you can only insert into 1 table at a time using sql) It sounds like you do this ofter, so why not create a file with the sql commands that you use, and each time you do this task open the file up in your databases sql editor window and run (may-be first modifying) these commands. If it always the same sort of task, you could write a script that connects to the database and runs the sql statments you need - then it is a one click or command task. 

Look at the query plan for both queries, you will see that the "order by" clause will result in a sort of the data that is returned by the remainder of the query (which should be in memory, but could be paged if memory insufficient). The time that sort takes is related to the amount of data (it has to walk it at least once) and how well ordered the data already is (it may be correctly ordered if you are sorting on an indexed column, or data joined on a indexed column) 

The CPU at 100% is only a problem if it stays at 100% for a prolonged period of time. More important is disk IO and memory usage. If these are high then you will need to do some tuning of the databases, such as getting better hardware of splitting the databases across servers. The next action to make is to run profiler ( $URL$ ). You could do a couple of passes with this. Firstly you could just look at connections to see which DB's are being used the most and which are not used. Next look for long running queries - these are the ones to tune as they will be using the most resources. If the database is transactional, look for the queries that are run very frequently (put the profiler output into a database table and count by query) and look at tuning them. You could also explore the system tables which store some of the information you need ( $URL$ ) If performance is still an issue, then you need to look at new hardware (bigger server, faster disk, more memory, faster network) or to split the databases across servers. 

For what you want to do, I would recommend the following (which is pretty much what you were thinking). 1> Create history tables for the historic data you have - keep the schemas as similar as possible. Split up by some logical grouping (such as year/month) based on how they are going to be queried (say you need to report with in month/year as well as all). Do not worry about the table size of the splits unless they are getting into the TB size range (your dbms should handle it) just make sure that they are appropriately indexed for the queries that need to be run. You should consider putting these onto a different disk to the active data if performance is an issue. 2> Create a routine to move data from the active table to the relevant historic table. Run this periodically. As a practice rebuild the indexes on the table that has had the data removed from it, and maybe update the table statistics. Easiest way to do this is to write a sql script. 3> Consider the reporting you want to do. If you want to only have to deal with 1 table when writing queries, create a view that joins the archived tables together. Create indexes on all the tables to suite the view. This way if you want all the data, select from the view. If you want data from a specific year/month, query that table. The view will look something like: 

You need a few tables, 1 - The questions ( question id, input type, visible, question type, question text, expected answers....) 2 - The Answers ( question id, user id, activity id, answer....) 3 - The users ( user id, user name......) 4 - A table to hold a question/answer activity (activity id, data/time, user id) You may also like to have a table that specifies the questions that should be applied for each activity - either grouped by user or maybe a question collection. The foreign/primary keys will be the columns that have the same name in multiple tables and should be indexed. If you use this structure, you should be able to add a question or user or change an answer without having to change your schema or presentation code - make sure that the presentation code is dynamically created at run time - you just need to add a record in the appropriate place. This approach may take longer to develop initially than a hard coded approach, but will much simpler to maintain as you only will need to change data to change behavior. (A tip, to create your presentation layer, you will need a query that gets the appropriate questions to be displayed, then loop through this result set and call a method to render to question on the screen, the methods to chose being appropriate to the presentation of that question [text box, radio group, etc])