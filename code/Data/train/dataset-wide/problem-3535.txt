I need to install an older version of gcc on Ubuntu 10.04 (Beta 2); and I'm told I can install a previous package. At present, I can't compile version gcc-3.3.x, and I'm looking to install a package. How do I instruct to pull stuff from previous Ubuntu archives? 

I've configured to automagically mount a cifs share when users login; the problem is if a user logs into multiple times simultaneously, the mount command is repeated multiple times. This so far isn't a problem but it's messy when you look at the output of a command. 

I'm assuming I need to fiddle with either the file or to accomplish this. How can I instruct to avoid duplicate mountings? 

The is actually a script function that checks the status code and calls exit if necessary, nonetheless, How is the above any different than this: 

I've got VMWare ESXi (the free version) machine that is booting off a 9650SE RAID-1 drive setup (two 1TB mirrored drives). I'd like to replace the 1TBs with 2TBs. Is there a safe way to do this? 

I'm putting together an file server which will have a very limited number of users (less than 5) and offer the following services: 

We tried copying imagick.so and memcache.so from the old extension directory to 5.6.7's extension directory, but without any luck. How do we install these extensions to our latest version, PHP 5.6.7? Or is there a way to resolve this without reinstalling anything? Edit: To install PHP 5.6.7, I used the steps given in this gist: 

I have a busy celebrity stock photo website and I am trying to hide the photos behind the website root, so they can't be accessed via URL, but my ASP script can read the photo and sends it to he browser in binary. I'm using Windows Server 2008 R2. I would like to know what security settings should be on the folder that contain the photos. My website is sitting at and the photos need to be stored on the D drive here: . I did manage to get this working by simply clicking 'Share' in the properties of and then giving 'Everyone' read/write permissions but I feel I'm doing something wrong. What should I be doing to make this folder safe, but accessible for the server administrator account and my website scripts? What 'Share' properties should I be applying? Should I be sharing? 

I tested fairly extensively with Ubuntu 10.04 Beta 2 Server in a VM, and was able to simply copy (read ) a cross compiled tool chain from an Ubuntu 8.10 VM. I created the tar myself, which is essentially a lot of stuff in . Now that I've got a bare metal installation of Ubuntu 10.04 proper, the copy isn't working. In particularly, I'm getting the error: 

I have a cgi script that is resending an email on a failed attempt 5-10 minutes after a user accesses a page. My thought was to do this using the command from a python call (). Testing revealed: 

My active directory password changed, now I can't log into Redmine (0.9.4) via LDAP. I haven't installed plugins, so what gives? 

Data reliability is the most important issue; power comes next, and I could care less about speed. For the data hardware I've chosen a 3WARE 9650SE-4LPML which will be hosting two (2) TLES enabled WD RE4-GP drives running RAID-1. The decision I'm facing now is what file system should I use? My preference is for something Linux based, and a file system that attempts not only file system integrity but data inegrity as well. Please comment on which file systems, OS distributions and for that matter hardware chosen that you think may best fit my requirements. 

As the title reads, I'm about to download and install URL Rewrite Go Live extension on my Windows Server 2008 R2 dedicated server, and I need to find out which version of URL Rewrite I should be downloading, x64 or x86. The information that came with the server when I started to rent it said it was 64-bit but when I look at my C: directory I have two Program Files directories, one titled "Program Files" and the other "Program Files (86)" - so this has confused me somewhat. Can anybody suggest how to find this out? 

I'm fairly new to server administration, so please be gentle with me. On my dedicated server, in IIS6 manager, I've just added a new site called "upload-system", and modified the FTP settings so the home directory is \upload-system\www\uploads\, instead of just \upload-system. This is all okay but have one issue: When I connect to the server using an FTP client, for example CuteFTP, I land in my new home directory, \uploads, just like I wanted, but from there, there's nothing stopping me from going back a directory, using the up-level/back button on the FTP client. I'm actually able to go right back to the c: directory on the server. I created this FTP site so my client can FTP his photos there, I really don't want any of their office staff having access to other parts of the server when connecting via FTP. How do I stop this from happening in IIS6 manager? 

I've configured my router to accept a public key SSH session with a passphrase. They key pair was generated using the PuTTY tools on windows. I can open the session from both Windows and Linux using the Putty tools, but I can't do so with on Linux. When prompted by for the keyfile's passphrase, it fails. Ex: 

A lot of script simplification would be had if they are. EDIT: I should add these "processes" aren't daemons and don't actually record their PID in the FS anywhere ... 

I have a server that I use infrequently, so I'd like to a job to shutdown daily if no users are logged in via SSH and SAMBA shares. How can I determine how many active SSH connections there are, and how many active SAMBA connections there are? If both of these values are zero, the cron script will shutdown the server. 

I'm testing a backup to a bare metal restore. The new hardware doesn't have PS2 Mice/Keyboard and needs USB drivers: I can't log into any of the Safe Modes. I can however, put the original SBS2003 install disks in the machine and jump into Recovery Console. How do I go about installing board Chip Set drivers and USB drivers? Is it a simple copy? If so, what to where? 

We have two versions of PHP on our Linux CentOS6 machine, 5.3.3 and 5.6.7, and wanted to install PHP extensions, memcache and imagick, to 5.6.7. Logged in as root, using yum, we installed both of these, but realised they were only usable in 5.3.3 and not in the latest version. This are the commands we used: 

I am trying to build an automated background backup program to backup my entire website and its heavy photo folders, using Amazon's cloud storage service, S3. I am using S3Sync from S3Tools and have successfully tested a dummy backup using only the command line window. When I save the working command line to a batch file (.BAT) and have a Windows schedule call it, it does not work. I can only think this has something to do with the local paths set in the command line. If I go on to my server and open the command prompt from my 'Start' button, the command prompt has a default of: 

Our .co.uk domain isn't used anymore but we still own it. I managed to delete the rule pretty quickly from the config file, as luckily it's in development stages and nobody would have landed on it anyway, and I also deleted the rule from IIS too. However, my browser still gets redirected to the wrong domain but other browsers don't. So I assume everything is OK apart from my browser has cached the rule somehow and somewhere. Have I done any permanent damage to how my site will run in the future and do I need to do anything else to remove this incorrect rule? 

I used a turnkeylinux.org otrs installation and I'm trying to configure the default domain of 'yourhost.example.com'. I tried the following: 

With the value set to true, the module was trying to remove from the system, not, as I assumed, in the directory. 

The next time I logged and tried to create a user, the default domain was still there. How can I change default doamin in an OTRS installation? 

I'm using a fairly vanilla Ubuntu 10.04 server installation, and I'm experimenting with BTRFS. How can I create a BTRFS RAID1 mount? I've got two (2) 1Gig drives that I popped in the server and after running the following commands, it appears I've got a 2 Gig partition, not 1 Gig as I would expect. 

I've got a number of Windows (7/Vista/XP) based machines on a network that have no unifying structure (i.e. no ADC, PDC or for that matter a workgroup). I recently added a Linux box with a fairly sizable raid setup that I'd like to use as a file server. What would be considered a best practice for making the storage on the Linux box available to the windows clients on the network? It seems my options are: 

What is the sensible way to get around this? Can I change the default directory on the command prompt processor, when it's first loaded? Or is there a way of changing my batch command code to use the local paths correctly? Maybe this isn't the problem at all and it's something else? Any help greatly appreciated. My command line code: 

To solve this you need to extend the session's idle time. You can do this by going to: Start > Control Panel > Administrative Tools > Terminal Services Configuration : Right Click Connection > Sessions Tab And then altering the idle timer settings to suit your needs. I changed mine to have an idle of 1 day before disconnecting and logging out. 

I have a growing stock photo website. It's currently hosted on a pretty basic dedicated server from WebFusion and I now need to expand, both storage and performance, and to lose their rubbish support team. We have 500k photos online now and reckon we could make the million or two mark quite soon. I'm now stuck on what options there are for expanding, whether to just upgrade the server we have to hold more files, use a third-party storage service, like Amazon, buy our own server or something else. Any suggestions would be amazing, thanks.