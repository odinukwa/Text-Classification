FAIR have trained word embedding models for hundreds of languages, with the side effect that they have compiled words lists. 

Unlikely, because surnames only became a convention in Central Europe in the middle of the last millennium, and the peoples of Hungary, like most peoples, took their current surnames even more recently, based on: 

Yes, there are even endonyms like Fala that simply mean language/speech with no further qualification. Tok Pisin, Papiamento, Lingua Franca etc are other examples. (Tok in Tok Pisin is from English talk. Papia in Papiamento is from papear (to chat). lingua in Lingua Franca is from lingua (language, tongue).) There is also the Turkic suffix -ce/-ca/-çe/-ça, which is used explicitly for languages but cannot stand alone. Affectionately Yiddish is often referred to as מאַמע־לשון (mother tongue), if you search you can also find German dialects and languages referred to with Muttersprache, Mottersproch, Muddasprach, Muttersprak etc. 

If I understand the question correctly, then answer is yes. In short, languages are not 1:1 and that includes function words. That is why translation is hard and, in some cases, impossible. Note that there is no standard set of words that should be opaque single words, the modern English set is certainly not the standard. 

What's a practical way to programmatically convert questions into statements with placeholders? For example: 

Yes, there are measures. Whether they are consistent or useful is subjective. A probabilistic parser like $URL$ or $URL$ returns one parse. But under the hood, it had a few candidates and just chose the most probable one. If the even the first candidate had a low probability then we could say that the sentence was "hard to parse". If the second candidate had almost the same probability as the first one, likewise. If the first parse is in fact wrong then tautologically it was (too) hard for that parser. If it is grammatically correct but semantically improbable then we enter into a philosophical question about what the parsing task includes. There are also many metrics you can extract from the returned tree itself, like depth. 

The major agglutinative languages like Turkish and Japanese are also notable for being almost strictly left-branching, much more so than, say, English is right-branching. Is it a coincidence, or is there a relationship or correlation between agglutination and branching directionality (head directionality)? What are some examples of mostly right-branching agglutinative languages, if any? I am specifically interested in a language that uses prefixes and prepositions for agglutination, rather than suffixes and postpositions. 

Is there a way to get the underlying probabilities or pass a minimum probability threshold to treetaggerwrapper (the TreeTagger python wrapper module)? The goal is to know when the POS tagging for a given sentence is either low confidence or potentially ambiguous, and as such should be hand-checked or discarded before training a POS LM. 

The relevant section Human-Human Spoken Corpora gives overviews and links to: - Switchboard dataset - British National Corpus ... Further down you will find Constrained Spoken Corpora (domain-specific) and Scripted Spoken Corpora (films). opensubtitles.org is also widely used. 

I see evidence that this is just some relatively modern shift in pronunciation in Persian in some accents. For example, i in the pronunciation of kitab is preserved in 1) the languages which inherited the word from earlier Persian (Turkish, Azeri, Kazakh, Pashto, Urdu, Hindi... and many more) and the 2) Persian outside Iran, eg Dari and Tajik. It also seems it has nothing to do with Arabic-origin words specifically, as it happens in Persian words too, eg emruz in standard Iranian Persian and Dari vs imruz in Tajik and Urdu). Here is a better explanation: 

Chrome/Chromium uses the open-source Compact Language Detector. You could use it too. It does not work perfectly, as it is mainly designed to be compact (small, and fully local - no calls to the network), but Google and Chrome are behind it, and it is improving. It even runs on Chrome for Android. So for your needs, it may be a good fit. Also, in my experience, the right language detection behaviour depends highly on your actual application. Perhaps you can tell us how you intend to use it? (Will the strings you pass be pages/sentences/queries/words? Will it be clean data? Will there be mixed language input? Do you care more about accuracy or recall?) 

It is still under construction, but the initial benchmarks included the StackExchange question tagging task from the fastText supervised tutorial with about 10000 training rows with 734 labels - similar to your task. To avoid false precision and a massive table and the nuances of precision and recall for multi-label, I will not quote exact results, but the improvements across tasks are significant, between 3% and 10%, reducing errors by one tenth to almost one half. The exact results will depend on the parameter values you set in config.json. Noisification is not as effective when the dataset is already large and noisy, and of course training time increases are not a joke at scale. The optimist way to spin diminishing returns is that noisification tends to be more effective on tasks where the dataset size is very small and the initial baseline results very bad. To get the NoiseMix code and required libs: 

There is no evidence that bakır comes from any other living language family, and cognates of it are present in many other Turkic languages. 

You could try doing truecasing first. (There are various libs for that. You will probably need to add a few domain-specific fixes for your data too. And you should try smashing all case before truecasing. It's not a completely solved problem either.) As a general rule, breaking the problem down is the best approach. Most of the components perform poorly when they are expected to do spelling correction and transliteration and truecasing and fix punctuation and ... Named-entity recognition is no exception. There are examples where the caseless version is ambiguous. 

That said, I would not discount randomness too much, and note that there are other countries, regions and cities with similarly varying names (eg Greece, the West Bank, Istanbul...), especially historically, and especially in neighbouring languages. In fact, number of autochtonous historically neighbouring languages is essentially the factor. (Unsurprisingly, the word for "Georgia" is different in all directions.) 

Pluralising after a number is an arial feature, not necessarily a question of language family. For example, Armenian, Georgian, Persian and Turkish do not mark after a number (or after the word "many/much"), although they are from 3 completely separate language families. But there are many borderline cases. Some languages never pluralise with morphology anyway. Some languages have a singular, a dual or paucal and a plural. Some languages do not pluralise after a number in some cases. For example, in German, it is correct to say "2 Bier". Even if we somehow handle the edge cases, the definition of "common" is arbitrary, because there is no objective way to count languages. Does Luxembourgish count? Does it have the same weight as Bengali with hundreds of millions of speakers? So it probably makes more sense to look at map than at a number or even a list. But we should beware of false precision in answers to questions like this. 

This will output a file in the same format but with one generated line from every original line, doubling the dataset size. To increase that, add . But the intuition from the initial results is that it is just as effective to increase the perturbations per line ( in the config) as to increase the number of lines. The lines look totally butchered, but it works. The only supported format is fastText (labels prefixed with ), you can add other formats in formats.py. You can add data for locales besides English / QWERTY in data.py. Python 3. You can also try using pre-trained vectors. Conceptually realistic data augmentation based on general is not too different, NoiseMix is just a bit more tuned for user-generated data, whereas fastText Wikipedia pre-trained vectors are a model of the standard formal language. 

Given that hundreds of languages are supported, at any given point in time the voices for different languages are from different versions of the system, different datasets, or even completely different libraries from different projects or vendors. You happen to have found a language for which the quality is better than for English, and for some reason users are more often surprised by a quality difference that way around than the other way around. 

I count iOS, WADA, Huffington Post... (In contrast, Le Monde or The New York Times will never ever use a Cyrillic, Arabic or Chinese character for a company name. Arguably most non-Latin scripts do use Latin in this way, that is, they would throw in a word like YouTube whereas the NYTimes will never ever ever use a Cyrillic, Arabic or Chinese character. Nil passive knowledge of other alphabets is assumed in any context, they are strictly esoteric. The exception would be the Greek letters for maths. Generally even identifying other alphabets is not required, and plenty of Cyrillic is even pseudo-Cyrillic.) Moreover product codes, URLs, hashtags, email adresses, computer code, keypads, for example building door codes, units, licence plates, stock market tickers are in Latin and rarely in, say, Georgian. The IPA is also in Latin. The larger languages, like Russian, Chinese, Japanese and Arabic, can shield some of their speakers from this a bit more, but not completely. As a consequence, standards for how to say the letters of the Latin alphabet have often evolved in each language, for example 'y' in the Russophone world is ígrek, from French. It is also worth mentioning that many languages have multiple scripts and historically there were other combinations. Georgian has three, one is used more now, but one of the others can be used for titles, a bit like uppercasing or italics. A large portion of German was previously written in Fraktur, and before the Antiqua-Fraktur dispute Antiqua was often used for a few bits, eg Latin titles and Roman numerals, Antiqua being associated with Latin culture and Catholicism, in works that were otherwise in Fraktur. Today Fraktur is still used internationally in mathematics. 

In this simplified example, the output of the round-trip translation - trans('is', 'en', trans('en', 'is', x)) - will likely be x. The two mistakes cancel each other out. This happens more often than it would if translation mistakes were simply random. Even Good Translation Is Lossy A translation, and even a language itself, is lossy. Distinctions of meaning, gender, tu/vous, tense and aspect are not represented equivalently or at all in all languages. Ambiguities are also not necessarily represented. Let us consider an example, with English as the intermediate language to make it clear: 

Iranian Persian and Tajik have lost straight /a/, and you can make the case that some English and some German dialects have too. See: Persian phonology and its historical shifts It is surely the case for many more languages, but asking us or reading eg $URL$ does not scale. The more interesting question is: What sites or tools exist to search over IPA pronunciations of an entire languages or many languages? 

However, one could reasonably disagree with the Unicode Consortium's view that a combined glyph Roman numeral (8554) is not uppercase and (8570) is not lowercase. The Roman letters I, X, M and and Greek letters Stigma and Digamma, Qoppa, and Sanpi and so on are technically letters. (See $URL$ etc) Hence 

So, although there is a correlation between loaning words and loaning technologies, in the case of a specific technology like wine it is better to read ancient reports, find evidence of grapes actually being grown north of the Alps and so on - ie out of the scope of this sub. 

I assume the shift is to preserve a contrast given that collapse, but you will need to ask someone who really knows Persian to find a minimal pair. For reference, there is more variation in the pronunciation of Arabic words within Arabic dialects than variation in the pronunciation of Arabic words used across all of these other unrelated languages. 

Hindustani has two scripts. Chinese has many homographs when written in Chinese, but many homophones when written in Pinyin. In some Hebrew corpora, vowels are indicated, in others only when necessary, resulting in more homographs. German writes noun compounds together, but modern English usually does not. The space character was an innovation that is still not universal, and the definition of a word or token is a matter of debate. So English and Chinese may have distributions more similar to each other than English and Celtic, or Chinese and Old Chinese, even though English and Chinese are completely unrelated, whereas the other pairs are in the same family. That said, some of these typological changes happen because of creolisation or substrate, so there are open questions about whether these languages are not in a different family than their nominal ancestors. But even if we assume English is a fusion language, it is a fusion of IE languages. This is all assuming that the corpora used are actually comparable. English and Scots are very similar languages, but if we use each language's Wikipedia as a corpus to generate the distribution, then there are other variables. 

The lines are blurry, but there are some good reasons to see these as distinct suffixes and not inflections: etymology - Some of the suffixes evolved from previously separate words like determiners and pronouns like ille and tъ. In fact, in Romanian the suffix itself is declined and can also occur separately for emphasis. grammar - In Macedonian, Bulgarian and Balkan Romance, the suffix is or can be appended not to the noun but to the adjective if there is one, again revealing its origin as a separate word. morphology - These languages have or had case, so there as a separate established notion of what qualifies as noun inflection. regularity - Unlike with declensions, there really are no internal nor unpredictable changes to the words, occasionally the final portion changes due to elision but that happens in languages where articles precede the noun too. European tradition - The obvious analogy to the linguists who first worked on this and to the educated speakers of these languages is to definite articles as in Western European and Semitic languages. (The neighbouring Turkic languages and Iranic languages have no definite articles.) All that said, it is common to simply avoid the question by referring to the definite forms of a noun for these languages. (That's how Wiktonary declension tables refer to them for Albanian, Armenian and Bulgarian. For Romanian they say definite articulation.) Although no writing system is phonetic, that is often a local ideal, so where -ë + -a or -ă + -a is pronounced as -a would be, the tendency is to write -a. 

Yes, although I'm afraid to say that then, as today, healthy chaos reigned. Transliteration conventions depend as much on other aspects of the context - the writer's familiarity with Russian, the intended audience, the keyboard or press available - as on the year. So which conventions do correlate with year? These days, we can find the English convention 'sh' being used more, and also the letter 'k' in places where 'c' would work, and 'y' in 'Krylov'. We also see 'y' used in ways that correspond to Latin American pronunciations. For example, Pushkin vs Puchkin: