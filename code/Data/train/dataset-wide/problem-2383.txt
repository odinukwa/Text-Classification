I'm doing what I would consider a pretty straightforward query on two tables that aren't huge (~626k records in one; ~47k records in the other). Both tables have GIST indices on their spatial columns ( and in the query below): these are the columns being used for the conditional part of the query. The spatial columns are WKB MultiPolygons: they have been checked using ST_IsValid(), and are all lovely and clean. The offending query looks like this: 

That query looks an awful lot like the one above, but was working with a 3.45 million-row table ( is the unmatched data from ). I first tried Query 1 with no , using as per Query 2: that ran for upwards of an hour without producing results. I have ed, d, and ed both tables (nothing remotely interesting was found). And now - finally - my question: is there some 'trap for young players' that I have fallen into here? SPEC: PostgreSQL 9.3.5 64-bit; PostGIS 2.1.3 r12457 on localhost under Win7 Ultimate. Machine has 16GB RAM, quad i7-4770 @3.4GHz, and a 500GB SSD (PostgreSQL data is on an internal 2TB HDD). My machine is not under any perceptible load when the query is happening: the postgres process hovers at 5-9% of CPU and 15MB RAM, tops... I would be happy to hand it 8GB if it would just get the job done. SPEC-related question: I have a CUDA-capable card (nVidia Titan X with 12GB RAM of its own - oh hell yeah). I've read that PGStrom can 'smartly' figure out how to push calc load to the GPU. Has anybody here had a crack at that sort of thing? I can't be bothered installing PostgreSQL 9.5dev if it's not going to give a tangible boost. I ask mainly because I've CUDA-fied some Python stuff (big aerial-image analyses and some number crunching) with mind-boggling results. I would be happy to chuck the whole of postgresql and the related data at the GPU and let it whizz around there until it finishes, if that were possible. 

You could be dealing with a delegation/impersonation problem as DimUser suggested. If your SSIS is just fetching data from one DB server and delivers it to the other the solution is much easier and should be to set up an SQL Server Agent Proxy. If you are executing the SSIS Package from Visual Studio or SSMS by hand the package will try your Windows credentials to log on to the SQL Server. If your account has the correct rights it will succeed. If you set up an SQL Server Agent Job to execute that package the service account running SQL Server Agent executes this package. He does not have your Windows credentials so he will try to use anonymous login resulting in this error message. To enable SQL Server Agent to execute a package using a different account you have to set up three things: 

Please replace 'insert FQDN here' and 'insert NetBIOS name here' with the actual FQDN and NetBIOS name keeping the double quotes. 

To be honest there is no definite answer to your question. If you do the last fallback to N1 you tested it on all available nodes. Within limits, more testing seems to be better to me. But to be honest it might not be absolutely necessary. Leaving it on N2 will get you the added benefit of using the N2 node for primary workload until the next update. It might even be feasible to do the fallback to N1 and then do another failover to N2 to test if N1 is running and then have N2 as primary until the next update. In conclusion: I don't think it is absolutely necessary to the fall back to N1 but if you can afford the time during your maintenance window I would consider just doing it (and maybe even do the failover to N2 afterwards). 

I decided to change to LK_ + original table + column as it allows easier navigation when browsing all tables. 

We have a table which has a column defined for set(to group rows) and a column defined for hierarchy(order position), how do we enforce hierarchy ordering position integrity within each set? An example of what I mean. 

In other words, car or serial can only be inserted once for any one pid. Thanks PS, Sorry if my explanation isnt so clear, I will have to re illustrate, possibly by finding a better example too. 

I have read many articles now about natural vs surrogate primary keys, and came to the conclusion there is no single best practice. I have a table that will have around 2000 definite unique values, each value will range from 5 characters to 40 in length. This seems like a partial choice as a natural key, although the values which are 40 characters in length may cause some performance and storage issues when they are referenced elsewhere. As the total maximum rows in this table is fixed as 2000 and 35% of these rows contain value length of 25-40 characters(65% have length 6-25), shall I go with a natural key here? With your experience, what would you do here? 

You gave us a long (and very detailed) question. Now you have to deal with a long answer. ;) There are several things I would suggest to change on your server. But lets start with the most pressing issue. One time emergency measures: The fact that the performance was satisfying after deploying the indexes on your system and the slowly degrading perfomance is a very strong hint that you need to start maintaining your statistics and (to a lesser degree) take care of index framentation. As an emergency measure I would suggest an one time manual stats update on all of your databases. You can get the nessecary TSQL by executing this script: 

Create a credential Connect to the SQL Server that should execute your package. Right-click Security and select "New Credential...". Enter a descriptive name (Credential name), select the Windows account you intend to use (Identity) and enter the password. Click OK. For initial testing, you can use your own Windows account. For production use, I would suggest creating a dedicated AD Service Account (with minimal permissions). Set up a proxy account Expand the SQL Server Agent folder. Open Proxies and right click on "SSIS Package Execution" selecting "New Proxy...". Enter a descriptive proxy name and use the credential you created earlier. Configure the SQL Server Agent step to use the proxy account Open your Agent Job, select the properties of your step executing the SSIS Package. Now you can select your proxy account in the "Run as:" drop-down list. Additional setup: If your package is deployed to the SSIS Catalog you need to grant the Windows login you used for the credential the "SSIS_Admin" role on SSISDB as well. For that, you need to create the account as a regular Windows login in SQL Server (Public) and map the user to SSISDB using the SSIS_Admin role. 

am I wrong in assuming that the two queries are functionally identical? If I'm not wrong, is there any sensible, defensible explanation for the discrepancy? 

The logic of the query is as follows: each is known to have a portion of its area in more than one . The aim is to find all the that each touches. Eventually the proportion of area in each zone will be calculated, but I decided against trying to do that inside this query on the basis that the thing was already taking too damn long. That can happen intra-table (and hence be more faster) once this query finishes. The are known to be multi-zone because the that are a have already been identified in a query that only took 14 minutes (which still seems a long time): 

The problem persisted after VACUUM ANALYZE (I did that just in case, although the tables have not been altered since import, except for the creation of a GIST index on a geometry column in ). When I feed Query 2 as a CTE and look for the 1478 identified in the table, sure enough they ain't there - 

That's as expected, but frankly I can't trust that result given that Query 1 returns zero rows when it should return 1478. Three part question: 

SQL Server will auto update the statistics if the default is left enabled. The problem with that are the thresholds (less of a problem with your SQL Server 2016). Statistics get updated when a certain amount of rows change (20% in older Versions of SQL Server). If you have large tables this can be a lot of changes before statistics get updated. See more info on thresholds here. Since you are doing CHECKDBs as far as I can tell you can keep doing them like before or you use the maintenance solution for that as well. For more information on index fragmentation and maintenance have a look at: SQL Server Index Fragmentation Overview Stop Worrying About SQL Server Fragmentation Considering you storage subsystem I would suggest no to fixate to much on "external fragmentation" because the data is not stored in order on your SAN anyway. Optimize your settings The sp_Blitz script gives you an excellent list to start. Priority 20: File Configuration - TempDB on C Drive: Talk to your storage admin. Ask them if your C drive is the fastest disk available for your SQL Server. If not, put your tempdb there... period. Then check how many temdb files you have. If the answer is one fix that. If they are not the same size fix that two. Priority 50: Server Info - Instant File Initialization Not Enabled: Follow the link the sp_Blitz script gives you and enable IFI. Priority 50: Reliability - Page Verification Not Optimal: You should set this back to the default (CHECKSUM). Follow the link the sp_Blitz script gives you and follow the instruction. Priority 100: Performance - Fill Factor Changed: Ask yourself why there are so many objects with fill factor set to 70. If you do not have an answer and no application vendor strictly demands it. Set it back to 100%. This basically means SQL Server will leave 30% empty space on these pages. So to get the same amount of data (compared to 100% full pages) your server has to read 30% more pages and they will take 30% more space in memory. The reason it is often done is to prevent index fragmentation. But again, your storage is saving those pages in different chunks anyway. So I would set it back to 100% and take it from there. What to do if everybody is happy: