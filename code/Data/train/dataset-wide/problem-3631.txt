As I understand and use baseline performance indicators, they aren't static. Don't consider your baseline something you take once then forget about! What you want to do is create scheduled tasks that take performance metrics during a long enough period that it encompasses your peak and lowest usage. In the vast majority of situations this means a full week of metrics. Keep in mind though, this depends on you being able to take these statistics without major impact (which in almost all cases is quite easy) Then you decide for a retention period, and a refresh period. For example, you retain the statistics for a year, and take new ones monthly. In this example, you would take a week of statistics each month, and keep a years worth. What this gives you is: 

NOIP and other dynamic DNS services give you a way to dynamically sync your connection to your changing IP. They in no way actually provide you with a static IP. 

Depending on your clients environment adding your own router might not even be needed. First off, why are you moving your setup? If this is a web app you can always expose it to the internet and have them connect remotely. It really depends on what your application is / does though. If you want to integrate your setup in your clients environment, the first step is find out what their environment is like. Chances are they have a proper network setup and can easily integrate your load-balancer + both servers without adding your router. If you do intend to add your router regardless, you will need to conform to whatever network structure they have in place. This means you're going to have to ask them what internal ranges they use, and adjust your ranges accordingly. So, to recap: Ask their IT department how to proceed. You can't just add a router to an existing network and expect everything to work. 

As an end user, I found the Yubikey ($URL$ much easier to use than the RSA SecurID ($URL$ End user experience is not the only consideration, and as others have pointed out, there is an ecosystem of products supporting SecurID. Looking over the IT fence to the security guys, however, SecurID never seemed particularly easy to manage. As for "something you are" component, it does seem that the current set of widely available fingerprint readers may not be as securely implemented as a CSO would require. 

A blank white page suggests to me that the webserver is up but not serving any useful data. I would start by assessing network connectivity. Is DNS returning a correct IP address? Am I reaching the intended host? Next, I would check the status of the server. If you control the server, go in and check server logs, lists of processes, standard sysadmin stuff. Note, some people like to check the server first and then check network connectivity. If you have a webhosting provider, you may need to call customer support. 

I would consider this a bad solution - too convoluted. You will want to service contract in order to get security updates for your router and to get access to the Cisco Support Site. If the Cisco device is just too expensive, consider a lower cost alternative, either from a different vendor or "Roll-Your-Own". Just remember RFC 1925 truth #3 

Moving the WSUS db: This is the same as moving the files of any normal SQL Server database. With the added caveat that the connection string uses . You can either use backup and restore for this (recommended as it is safer) Or detach / attach (faster but generally more prone to errors) 

You can then deallocate the resources, and generalize + capture the machine. From the azure documentation for capturing Linux VM's: 

UPDATE 2: By request on a different thread I redid the procedure, making sure the prompt was ran as administrator. After running the commands the following events are found in event viewer: 

As you're being somewhat vague (or at least in-precise) on what updates you're going to do, I'll make some broad blanket statements that will hopefully answer your question. For windows updates on Servers You can generally safely postpone the updates indefinitely, not taking into regard that it is not safe to have your system be behind in security, stability updates and bug-fixes etc... As long as you don't reboot, the following is true: 

I'm trying to set up a script that generates blg files every set amount of time, using powershell. I know this can be done using perfmon XML templates, but for this specific project I have to do it with powershell if at all possible. My main issue is that I am unable to store the list of performance counters I want to use in a variable and reuse them. I've attempted to use the following script to create a list of performance counters to use: 

When you need to clean a system infected with malicious code: 1) Archive the user data 2) Some some other system, or at least some kind of rescue CD, to scan the data for traces of malicious code. 3) Clean install the system, including MBR on the disk. For the paranoid, re-flash BIOS code on all components. 4) Restore the scanned/cleaned user data to the rebuilt system. In my opinion, do not waste time trying to clean a compromised system in place - with today's malicious code, e.g., root kits, this is impossible. 

Don't forget your gpg keys (from the same thread) How to install all the desired packages and uninstall all the undesired packages 

I've been catching up on my security podcast backlog, and I've been hearing about cloudburst attacks on virtual machines. While Google returns info about the Black Hat related press releases, I can't find info about how to defend against this attack. How does one defend against the cloudburst, and similar, attacks? 

Protocol analysis is not hard, but it is tedious. The basic process is iterative, with the results from the previous step serving as the input for the next step of the analysis. Basically, you are always comparing what should be happening with what is happening, and noting the anomalies. I would suggest starting with a raw packet capture with a simple filter to limit the capture to the problem subnets. Depending on the Application Layer Protocol, I would limit capture size to ~100 bytes or so - enough to get the TCP and lower layer protocol headers as well as a little bit of the Application Layer. Once you know that you have an example of the problem behavior, load the raw packet capture into your protocol analyser of choice - tcpdump, wireshark, Netscout Sniffer, whatever. Now you can start looking for more patterns that allow you to isolate the problem traffic. If you can isolate the traffic, then you can analyse it. In the comment, mas made a good recommendation for filtering based on SYN/ACK frames and seeing if there are IP addresses which have a large number of open connections. You can then look at the connections from those IP addresses and count how many sit idle and how many exchange actual data. Take a look at the data being exchanged. Does the Application Layer Protocol data make sense for your application? Count the number of connections where it makes sense vs. the connections with anomalies. For some well known problems, Expert Engines have been created that can automate some of this work. In my opinion, this is larger what IDS is, an Expert Engine, or suite of Expert Engines, that automate the analysis of packet captures. You may find a package that does the analysis you need to do. In the meantime, you can start analyzing the data you have. If all you have is tcpdump, you have to use it, but I prefer the graphical protocol analyzers, exspecially if that have some tabulating or graphing functionality. The GUI helps visual the data, and many conveniently color-code parts of the packet for easier reading. 

The thumbnail photo is not really related to the users profile as such. It is a property that was added with Exchange2010 to allow for a central, manageable repository for user pictures within outlook. As it was quickly determined that you don't want your IT department to be responsible for hundreds of user photos (you're bringing back bad memories of my first 'sysadmin' job here!) there are quite a few tools out there that allow for the users to upload their own image. This requires some minor permission tweaking in AD (nothing that could pose a security issue). You might want to google "AD photo upload". 

It looks like my error is firewall related. I'm unsure why the one command () encounters the error while the older version () does not. 

Sounds like you're using a private virtual switch as your hyper-v switch for that guest. You can review the different types of switches in hyper-v and their effects here. If this isn't the case, try running a tracert and adding the results here so we can have a better idea of what's going on. For this particular instance however the issue was caused by the ICMPv4-In rule being disabled. In other words the guest OS firewall was the cause. 

A Jump Server is intended to breach the gap between two security zones. The intended purpose here is to have a gateway to access something inside of the security zone, from the DMZ. The main reason I've seen this utilized is to make sure that the one known entrance to a specific server that has to be accessible from the outside is kept up to date and is known in its purpose as only having to connect to (a) specific host(s). Usually this is a hardened Linux box only used for SSH. 

I would suggest setting up a linux based router to perform the dual-homing. Then configure source based routing to achieve the equivalent of iproute2. It seems to me that split-access routing is a special form of source based routing. Alternate suggestion: move the server to a linux platform and use iproute2 if that is the routing solution that meets all of your needs. Alternate suggestion: virtualize the Windows server, run on a linux server, and let the linuxe server running iproute2 do your routing. It sounds like you will be accepting limited non-connectivity periods, so you will not need to worry too much about the Layer 3 routing issues that plague VRRP or clustering architectures. One thing to keep in mind, however, is that many applications and intermediate packet inspection points will expect consistent routing of a given connection, i.e., "Persistent Connections". Maintaining persistent routing of connections has been a pain in the sides of network and security architects for some time. If you can't afford a commerical product, have you investigated: 

I think that some of the enterprise desktop search products have a management interface for auditing where files are stored, where duplicates of files are kept, and even where older versions of existing files are stashed away. 

It depends. As mentioned by others, some applications work better on 32bit OS vs. 64bit OS. The reverse is also true As mentioned by others, 64bit OS allows for more RAM access than even 32bit with PAE enabled, although PAE enabled OS can general access a lot of RAM. What I have found, much to my dismay, is that the chipset/BIOS puts a limit on the maximum memory I can use. Even with 64bit Ubuntu, my system still can only access 3.2GB of RAM. 

We have a VNET that is coupled to an Express route, on which we will give our users access to specific subnets. These subnets are created for each resource group, and the idea is to have these users only able to add machines in their subnet. To allow users to add machines to a subnet I gave them the following permissions: 

I've been trying to push Azure NetworkSecurityGroup rules through powershell. Using the console I seem to be able to create what I want, however using powershell I am having little success. Using the following syntax: 

The problem: Our staging environment does not have spanned disks, it has one single volume (1.99TB, as the staging is only 1TB large). I can't replicate the move with anything of equal size, without spending a lot of time and resources recreating the current setup. I'm looking for either any kind of documentation that shows that moving a spanned disk from one VM to another will correctly recognize them as being a foreign disk group. So far I've only found this article. Or better yet, a cleaner solution for moving the databases across servers. Would it be safe to replicate this without the 3TB disks, simply with 250GBx3 disks in a spanned volume? Or can I expect additional issues with the larger disks that simply can't be tested without increasing the size over 2TB (the single volume max size). 

As an alternative to "rolling your own", consider the appliance vendors. Over the years, I used a number of "White Box" distributions and been quite satisfied. $URL$ There are two big benefits to you: 

I think you could add the disks to the array and then use something like Partition Magic (or Partimage) to increase the size of the partition on the array of disks. Regardless, you need a backup. RAID is not a backup strategy (as has been written many times in this site). Schedule the system downtime for immediately after the backup has completed, and be ready to restore. 

Start the capture running, filtering on a windows box. Launch thunderbird. See what comes up. Start a new capture running, filtering on the linux box. Launch thunderbird. See what comes up. Iterate, filtering out stuff that seems obviously not related to the differences between the IMAP connection between the two machines. 

While I haven't tried to configure an Exchange Server to allow ActiveSync connections, I have used Nokia devices and Mail For Exchange to sync email, calendar, and contacts to my device. I would direct you to the Mail For Exchange Manuals, if you don't already have them. I would also suggest trying the connection from a WiFi node inside your firewall, with a network sniffer attached to the line. Basically, simplify the network arrangement so that you can focus on the application configuration. 

You could try querying the ad groups using powershell, then adding the user to group2 if he isn't a member of group1. 

You're focusing a bit much on a single metric! Diagnosing a bottleneck to a single component rarely ends with one counter giving a full explanation. There are quite a few great guides for using perfmon to diagnose performance problems on SQL Server. And unfortunately your Admin could be right, the counter you choose does indeed depend on the underlying hardware. However I can't find any documentation stating that Azure is based on a 6 disk raid. So perhaps focus on other counters? 

And from your client machine to your externally accessible machine in the same way, but replace with your VPS external address. 

We ended up trying to remount the databases, restart the services (SQL Server and Virtual Disk services). But in the end the only solution was a restart of the server. What happens during the VSphere Snapshot process that could cause this chain of events? If this is related to the VSphere Snapshot, why would a reboot fix it? 

This is a common issue when you use a self-signed certificate together with the CAS role. You want to install a certificate that is not a self-signed certificate, from a CA in your organization, or by purchasing a third-party certificate. Alternatively, if you want to use that certificate, and you're in an AD domain, you can use a GPO to distribute the certificate to client computers.