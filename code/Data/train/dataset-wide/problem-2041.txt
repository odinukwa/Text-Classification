To date, I've never particularly questioned the security implications, since these servers have a common purpose and user base, even if the specific user permissions may differ. Now, we're about to add a third instance, let's call it org-sql-rpt. This one is a good thematic fit with the others, but there's one key difference: we're going to allow an external partner (i.e. not an employee of our org) access to this server, like so: 

The typical use case is that a Client goes through a Broker who provides the desired Resource. Brokers have areas of expertise and only sell some of the available Resources, and multiple Brokers may sell the same Resource. So my initial data model looked like this: 

In a nutshell, I'm trying to model a relationship and can't decide whether to allow NULLs or to require values and use dummy objects (in place of NULL). Here are the details. I have three classes: 

I am about to split a large database into a bunch of federated instances. Currently, most of the primary keys are auto-generated identity ints. Obviously, that's not going to work in a federated setup. I've read people using auto-generated GUIDs to replace the ints, but that data type has well-known performance sapping problems on its own. What are some of the strategies I can use for having unique values for primary keys across federated members? 

From what I can tell Procedure Cache Hit Ratio below 95% is a problem. On my box, the values hover from 85 to 95%. How do I fix this issue? The server seems to have plenty of RAM so that shouldn't be an issue. What else can it be? 

So basically the code just blocks until a message is received. This all works fine. My question is about the implication of issuing a command which hangs on to some service broker based resource basically forever. Are there any performance issues associated with this pattern that I should know about? For reference, this is SQL Server 2005. 

fig 2. three-way relationship. Now a Client buys an Instance of a Resource from a Broker. This is what happens most of the time, but there are some exceptions that the data model must accommodate, and that this one doesn't. 

I use domain accounts for SQL Server service accounts. Sometimes, when I have multiple servers that are logically or thematically related, I'll use the same set of domain accounts for the service accounts on all of them. The user account permissions on each server may be different, but there's usually a lot of overlap. The gist of my question is this: can a user with access to one instance, but not the other, exploit the shared service accounts to gain access to the other server? Here's the specific situation I'm trying to address: 

With SQL Server 2005, you could look at the Task Manager and, at least, get a cursory look at how much memory is allocated to SQL Server. With SQL Server 2008, the Working Set or Commit Size never really goes above 500 MB, even though the SQLServer:Memory Manager/Total Server Memory (KB) perf counter states 16,732,760. Is there a setting where it will actually show the server memory in the Task Manager? Or is it a result of them changing how memory is used in SQL Server 

I've inherited a very volatile table which is a map of who holds what resource in the system. At any given moment, there could be a dozen inserts/deletes/reads going against that table. However, there are never any more than 30-40 rows in the system. The system was written in the SQL 2000 era and the access to the table is serialized via sp_getapplock/sp_releaseapplock system sprocs, so that only 1 request is modifying the table. In addition, the INSERT & DELETE statements execute . Reading the notes from a decade ago, it states that without these restrictions, the system would experience non-stop deadlocks. I've ported the database to SQL Server 2016 Enterprise Edition. Now that the throughput of the system has increased 10 fold, this table is easily the biggest bottleneck. What are my options for a table as volatile as this with SQL 2016? I am looking for fast (hopefully concurrent) access and no deadlocks. 

Given that, should I worry about reusing the domain accounts as service accounts for this new instance? Is there any risk that this person could use their legitimate credentials on org-sql-rpt to gain access to either org-sql-1 or org-sql-2? (i.e. any risk that could be mitigated by using different service account credentials?) Or is this just a generally bad idea for other reasons? EDIT The new instance will be hosting Database, Integration, and Reporting services (for now, at least). No Analysis services. The external user will have elevated database privileges, but no explicit instance permissions. They won't be able to create logins, jobs, or reports, for example. 

fig 1. naive many-to-many-to-many. It's basically a many-to-many-to-many relationship. The problem is that I can't tell which Resources have been purchased by a given Client, and vice versa. So I came up with this: 

I have table partitioned on (int). I also created a non-clustered index for on the table. When I run the following: 

I've setup a test SQL Server 2016 server. I've installed 2 instances, restored a backup on the primary. Then restored a backup on the secondary with , then restored the transactional log on the secondary, also with . I then followed the prompts in the Mirroring Wizard off the Database Properties page and ended up getting an error: . What am I missing? 

I have an app that's local to the SQL Server and thus has a Shared Memory connection to it. I was wondering whether the RAM taken by the connection (including data transfers) counts against the max memory limit set for the SQL Server. The reason I am asking is that the SQL Server is maxed out on memory (e.g. Target Server Memory = Total Server Memory and other metrics). If the RAM taken up by Shared Memory connection counts against it, wouldn't I be better off using TCP connection to the SQL Server? 

Basically, I need a way to make any one of the three classes optional in any given instance of this otherwise three-way relationship. So I can associate a Client w/ a Resource, but no Broker; and a Broker w/ a Resource, but no Client; and so on. I can see two different approaches: Option 1: Change all the "1..1" cardinalities in figure 2 to "0..1" so that each class is optional in the Instance relationship. This would allow me to handle exceptions listed above, but then I'd have to deal with nulls and implement constraints at the application layer (to prevent an Instance with only one related object, for example). Option 2: Keep all the "1..1" cardinalities, and use dummy objects to fill in the NULL holes in the model. For example, use a dummy Broker called "self-service" to deal with Clients purchasing Resources w/out a Broker. This would keep my data model simple, but I suspect dealing with all the special cases around dummy objects will be worse than using NULLs and application-enforced constraints on data. So here's my specific question: Is there a standard data modeling design pattern that captures the sort of three-way relationship I've described above? And if not, what are the other pros and cons I haven't yet thought of to each of the approaches above? 

I have a situation where multiple client apps send messages via the Service Broker (utilizing stored procs). These messages are picked up by yet another client app and then processed. The way the messages are picked up is that the app issues the following SQL statement (pseudo code): 

I have an SQL Server 2014 Enterprise Edition with a lot of data. I would like to mirror the data from that to at least 3-4 servers. The mirrors are all SQL Server 2014 Standard Edition (no more money is available for Enterprise licenses). How do I mirror the data from my main box (with the Enterprise Edition license) to other boxes? I tried the mirroring feature, but it seems that it only allows single mirror. I could you use Always On Availability groups, but that would require that all mirrors also be Enterprise Edition (unless I am reading the docs wrong). At least one of the mirrors needs to be there almost real-time (1-2 minute delay is fine) data replication. The other mirrors could have 1-2 hours delay. So what are my choices? P.S. All the secondary servers are just read only. P.S. The purpose of the mirrored boxes are partially to off-load readonly queries to them. These mirrors need to have near real-time data replication. Another purpose is for analytics, which is a heavy load. Today everything is on the same box and we are forced to do analytics at night so as not to disrupt users and there is just not enough time. P.S. The servers are nearby each other - on the same subnet, connected via a 10Gb link. P.S. Our license also allows a no cost upgrade to SQL Server 2016 when it becomes available. Does that change anything?