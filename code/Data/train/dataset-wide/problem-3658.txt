For plain URLs, as there's nothing particularly consistent create a conf file with entries like this: 

is the number of number of server processes you start with, and is the number of threads per process. The Webfaction settings should be reasonable in most situations although a thousand requests a minute may need some tuning, but probably mostly in the application. In normal use would take a request, pass it to and wait for its return, which should be near instantaneous, so what is actually taking its time is whatever the python script is doing. Your worker threads are therefore in a wait state and will build up as requests come in while other requests are being processed, so even a static page will end up waiting if your threads are occupied. Look at what the application is doing and for solutions. You may be able to cache queries using or similar. If the time that your application takes to process a request is unavoidable, look at making it asynchronous using a message queue like Celery, so rather than having your web server wait for responses, you can poll for them using browser side scripting. Splitting the static page serving from the dynamic will also improve the response. If possible you could run multiple sets of workers, or pass static page and object serving to , which is a more common way of handling . Another method would be to serve python through a native web server such as or and use apache as a reverse proxy, which may improve the backend response although still won't help if processes are causing large numbers of waiting threads. 

You would also need a URL redirect record for www.imgshare.org as that's actually a different process from the DNS lookup. I'm guessing you also would also need to remove the CNAME for parkingpage.namecheap.com. However, you should also configure your actual server correctly rather than relying on Namecheap. 

The mutex locking isn't needed for a single server as it's basically to prevent write conflicts, but if you are going to scale out and will potentially be writing any files into your shared filesystem from multiple servers consider using it. Use the apr default or for most purposes. 

From the point of view of nginx you can add a rule to manage the in-memory lifespan of static files alongside in-browser caching with something like this: 

should do it. There are also header control extensions for django that can do the same thing. Cloudflare should honour these headers without any changes. 

Send to tell receiving browsers to release a cache entry. This has to be sent for every object in the cache so is probably better done with a script. 

Delivery seems a lot more successful now and it did make sense as an issue in this case. The function isn't documented officially as far as I can see. 

You can point as many DNS A records to an IP address as you want. In addition, nginx can be configured to listen on different ports, but it will also handle virtual hosts for HTTP so can listen for for different DNS hostnames on the same port. 

The obvious issue here is that these users, by doing that, are affecting service on the server, which should be part of your terms and conditions. However, if you really need to disable files because they are throwing errors, you can set for specific directories. 

If you want to do it in wholly in httpd, then use Alias. If you have dynamic requirements, look at using .htaccess in your document root to add and remove URI paths to your URLs. If you want more control, there should be a way of managing URLs in your programming language of choice. 

Assuming that your phtml files are PHP, they would need to be interpreted by php-fpm as with your php files so this should work: 

However, a location block won't handle query strings. They are returned in the variable or named in the variable so if you have specific query URIs that need to return 410, create a map outside of the server something like this: 

You can try IntoDNS to identify any configuration problems as shown to the outside world, but my guess would be that one or more of your name servers isn't responding correctly to queries. 

The PERC boards have a battery that often fails, usually after about five years of use, and this can cause the effect you are seeing. This can also happen if your disks are mismatched for a number of reasons. You should be able to install and use the Dell diagnostic tools (, etc) and MegaCLI to detect any issues. The PERC will also probably warn in and certainly will show problems in POST on boot. I would also strongly recommend a backup as soon as possible as there is the potential for loss of data. You are correct about importing foreign configs and this may fix the issue but it can also wipe the array depending on the state of the system. I inherited a couple of hundred Poweredge machines and this is a regular problem. 

The CSR needs to be validated and turned into a certificate, which is usually what you pay an SSL company for. You would usually buy a certificate and upload your CSR, which would then be returned as a validated certificate. 

SSL certificates are not assigned by IP address but by name, and nginx has supported SNI for a long time so you can have as many SSL certificates per IP address as you want as long as you issue them for names that resolve to the IP address. Create a virtual server per IP address: 

Broadly speaking you would configure the Huawei routers to be gateways on each VLAN and send routed traffic to them, but with the information you have given that is about as much as I can tell you. 

These requests are for domain control validation, hence DCV. I assume you are using AutoSSL with a web control panel, which is worth specifying in your question. AutoSSL is validated by Comodo although I wasn't aware that it did the check so regularly - I would guess it's daily.