Indirect estimates of vertical velocities can be obtain in several ways. The most common approach is to take advantage of the principle of conservation of mass (continuity equation, as described in the answer by Neo). $$ \dfrac{\partial w}{\partial z} = -\dfrac{\partial u}{\partial x}-\dfrac{\partial v}{\partial y} $$ where $w$ is the vertical velocity. Assuming $w=0$ at the bottom, then measuring the horizontal velocities throughout the water column will give you an estimate of the vertical velocity. This method, while useful, tend to produce really noisy results as the vertical velocities in the ocean tend to be quite small and are masked by the noise in the horizontal measurements. A completely different approach was recently introduced by Klein et al. (2009). It uses the surface Quasi-Geostrophic approximation to estimate low-frequency vertical velocities in scales between 20 km and 400 km from the surface down to 500 m. The needed data is some high-resolution Sea Surface Height (SSH, usually from satellites) and an approximation of the large-scale vertical stratification. Vertical velocities are estimated from the buoyancy equations at the surface and at depth in a modified version of the Omega equation (Hoskins et al., 1985). 

I think the best option for sediment transport modeling is the Community Sediment Transport Modeling System (CSTMS) package that was developed for ROMS. CSTMS was created by a group of sediment transport modelers lead by the USGS. One of the many benefits is that it is open-source and, thus, free. The model was designed for realistic simulations of processes causing sediment transport in the coastal ocean (estuaries, nearshore regions, and the continental shelf). Details on the package and its implementation are found in Warner et al (2008). The people from FVCOM implemented this package. 

Since 2009, two satellites (SMOS from ESA and Aquarius from NASA) are providing global salinity measurements. While ocean temperature is easily derived from infrared radiation for instance, what parameter is measured by those satellites to estimate the salinity of the ocean? 

As can be seen in the figure the compressibility is maximum at low temperatures for any salinity (not just seawater) and pressure. So in general, the compressibility of water is maximum at low temperatures due to the internal structure of water. The inverse relationship with temperature seems well accepted (Fine & Millero, 1973) and has a direct relation with other properties like the speed of sound. In a typical liquid, the compressibility at low temperatures is reduced as the structure becomes more compact. In water, there is a change at low temperatures toward more open-structure clusters (expanded icosahedral water cluster) versus denser clusters (collapsed icosahedral water cluster). As the water structure at lower temperatures is more open, the capacity for it to be compressed increases. As cold liquid water is heated it shrinks, it becomes less easy to compress, the speed of sound within it increases, gases become less soluble and it is easier to heat. The compressibility above is calculated using the formula in the figure (same as in the wiki article) and I have included the code to reproduce the results: 

A very good analysis looking at the 3% relationship is an article by Weber (1983). His main result is that: 

The relationship of the observed "aquamarine color" near the coast and the Ekman pumping in the open ocean, while possible is rather unlikely. Ekman pumping is confined to the areas affected by the cyclonic wind stress curl and by definition it is mostly effective in the open ocean, where there is no bottom boundary effect. The color of the coastal waters is likely more affected by the upwelling-favorable wind creating upwelling conditions that provide nutrients to the coastal areas. The fact that the color is affected by the sea-breeze is even more justification for the upwelling mechanism. While transport of nutrient-rich waters to the coastal areas is not uncommon, the time scales associated with primary production especially in coastal areas are quite fast and tend to use the open-ocean nutrients in short time scales. Phytoplankton changes especially on the order of hours are more likely associated with purely coastal processes. While there might be an input of open-ocean nutrients to the coastal areas, the coastal primary production tends to differ from open-ocean communities and tend to respond rapidly to extra nutrients. The aquamarine color could be the result of many coastal processes. Without knowing the area, I don't feel qualified to make a strong case about the causes. Reasons for the color could include 1) vertical phytoplankton migrations that settle to the bottom/lower water column when the breeze comes up; 2) the sea breeze resulting on a local near-shore downwelling; 3) a breakdown of the stratification by the breeze that mixes down the surface layers; 4) regional features (e.g., sea level pressure gradient) that become active when the breeze is occurring. Too many unknowns for me. It would be good to know the extent and duration of the "aquamarine color" patch, the width and slope of the continental shelf in the area and the ind and stratification conditions in the near-shore and coastal areas. 

As far as I know in the ocean modeling community the terms diagnostic and prognostic have quite different meanings from what you are stating. A prognostic simulation is used to predict future state of the system (forecast) using the model equations. The model is initialized with the initial conditions and it is used to predict using the Navier-Stokes equations the future ocean state under specified boundary conditions. There is no constraint by observations because the model numerically follows the specified equations. In the case of models with data assimilation, the prognostic solution is statistically modified (e.g., Kalman filter, adjoint method) to optimally adjust to the observations, but the new solution can't be considered prognostic as there has been a departure from the primitive forward (Navier-Stokes) equations. On the other hand, a diagnostic simulation does not provide a prediction of the fluid state. There is no time-evolution based on the model equations. An example of such simulation is the case in which the velocity field is calculated based on a fixed density (temperature and salinity) field. While these simulations can be useful for short-term ocean state estimation, they can't be used for ocean state evolution as the dynamics are restricted. As far as I understand it, this is the standard in the ocean modeling community, but I don't know if it can be expanded to other fields. 

The simple answer is that you just subtract the nearest value from the glacial isostatic adjustment (GIA) model. The values provided by Peltier in your netCDF file are in a grid that is 1 degree x 1 degree. So with the lat/lon position from the tidal gauge sea level data, you choose the closest value to your station and apply the correction. In the PSMSL GIA correction page they already explain that you should be using the centered value (Â±250 years). If you decide to use the ASCII files, those are already interpolated to the tide gauge station locations. You have additional information in the U. of Colorado GIA websites(1, 2) and in the Peltier (2001) reference. 

Before defining wind velocity, I think it is necessary to explain the continuum assumption. The basic idea is that even though gases are composed of discrete molecules occupying a small fraction of the total volume filled by the gas, gas flows are made up of many individual collisions between gas molecules. In most applications, the flow field is assumed to behave as a continuum and the discrete molecular nature of gases is ignored. By doing so, fluid properties (e.g., density, velocity) can be defined anywhere in the flow. In order for the continuum assumption to be appropriate, the length scale of the application must be much larger than the mean gas particle path length. In most atmospheric conditions, including in anemometers, the mean path length is on the order of micrometers, while the anemometer size is much larger. While most anemometers (cup, vane, laser, acoustic resonance) measure wind speed some measure the wind's pressure (e.g., tube, plate anemometers). In the case of cup anemometer, like any other measuring device, there is an error associated with the measurement that depends primarily on the cup size. In general, the anemometer factor (the ratio of wind speed to the rotation speed of the cups) has a value between two and a bit more than three and it depends on the size of the cups and arms. 

I think you should just follow the guidelines provided by the USGS for this calculations ($URL$ From there you can read: 

My suggestion would be to do a Principal Component Analysis (PCA) or Empirical Orthogonal Functions (EOF) analysis on the wind data. The result of the analysis would be a set of modes of variability. You would be looking for modes that show areas that are large in magnitude but out of phase. As for the time scale, you need to check the eigenvectors and eigenvalues of the analysis to determine the scale of temporal variability. You can find appropriate code in many places, for instance here.