TFS\TFSREPORTS is a local user (running in a workgroup not a domain). How can I re-establish this user as an appropriately permissioned user for WSS? 

I had this problem as well. After seeing $URL$ I killed the wmpnetwk.exe process and sysprep worked fine 

I have a file win7.vhd that I've been using in a Boot to VHD scenario. Now I want to run that Windows 7 instance within Hyper-V Server alongside a few other VMs I have. I sysprepped win7.vhd and copied it over to the the Hyper-V Server and tried creating a new VM using that .vhd file. I get the "Boot failure. Reboot and Select proper Boot device or Insert Boot Media in selected Boot device" error. Any suggestions? 

My Hyper-V Server 2008 R2 box has 2 partitions, one for the OS and a second for data. Can I configure my guest virtual machines to have access to that data partition? Preferably so that the guest OS sees it as a physical drive, not a mounted network share. 

I had to restore a TFS 2010 virtual machine and am having trouble getting WSS running. When I try to go to the Sharepoint Admin portal localhost:17012 it fails. In the event viewer I see a bunch of 

While installing package I get some additional packages on dependency. But when I remove installed package with yum, the dependency packages are not removed, though I don't need them anymore. How can I remove them automatically? 

I've 2 remote HP Proliant DL180 G6 servers with HP Integrated Lights Out enabled on both and I have to install RHEL5 on these servers. I try to do it with ILO - mounted ISO file from local hard drive as Virtual CD-ROM, and performed installation from ILO console. OS installation on the first server was successful. But I have to make 3 tries to achive this. The channel between me and the servers is too slow - they are in datacenter. The installation takes a long time - 5-6 hours. So sometimes Virtual CD accidently unmounts and installation hangs. How can I perform the installation on the other server in more convinient way? 

Maybe some program is locking this file? It can be another copy of your program. Does show anything? 

I have HAProxy 1.5 running on Ubuntu 14.04 (modified). It accepts connections on http and https ports. Two backend applications process requests using persistent connection. When I create around 2200 client connections haproxy stops accepting additional connections. But I want this system to accept at least 10K simultaneous connections. Here is connection statistics: 

i thought this should be a piece of cake, displaying the quota usage in horde webmail (IMP). I have found several guidelines on the web - but no one seems to work in the latest version of horde. The last configuration i tried was this in /imp/config/backends.php: 

Ok i have not much time, the business is not waiting for geeks and solution-finding :) So i searched further on my own and i found a really, really interesting project -> for me personally this is filling some of the biggest gaps in aws: $URL$ With that, you have shared directories, synced in seconds, usable from all your running instances simultaneously and best one, on S3, directly integrated in your Unix-FS! I'm not sure but for me it seems that that is the always-wanted missing piece for all scenarios like this, and i think it's funny once again, that it needs a third-party-developement to solve a scenario "right" that is so much propagated by aws-dudes... :) I will give it a try and i'm almost sure, that this will perfectly solve my most-wanted scenario... 

I have VMWare ESXi 4.0 preinstalled on the server with IP A. I also can use IPs B and C. I have KVM access to this server and I can connect to it using vSphere Client. CentOS iso-image isn't on the server. My task is to create two virtual machines on this host and install CentOS 5.4 on both. Then I have to configure network, so guest1 is accessible on IP B and guest2 is accessible on IP C. Can anyone provide me a brief howto on this topic? I tried to find it, but found nothing. 

I have a 500 GB hard drive with one NTFS-partition on it. I can mount it with Ubuntu and view the contents. But when I try to copy something, I get an I/O error. Ok, I tried to make its image with . I/O error as soon as it starts. I have installed , but its manual page says not to use it with drives, failing on I/O. Can I manage to get some information from this drive and how to do this? 

It is a killer requirement for us that we do not loose the existing two nameservers because they are already configured in customers top-level domains where we don't have access directly. Now, i'm wondering what my options are: 

Is there any other failover scenario with AWS which i don't know yet? Sorry for the long text but i wanted to share all my thoughts so far... Thanks for reading if anyone does! :) 

But - it's not working at all.. i can imagine that the implementation was changed completely for horde5, but i can't find helps and guidelines for horde5. Maybe anyone out there has already solved this problem? ) 

Find out what ip's the two cloudflare servers really have and change the entries for our existing nameservers to these ip's -> pro, solved; con, the ip's can change and silently break my dns setup. More lookups are necessary to finally lookup a customers top level domain. Making an CNAME alias for my existing nameserver to point to cloudflare servers directly, but this is against the RFC of DNS, no one will do that for me.. Pay a business account for at least 200$ per month at cloudflare to get custom named nameservers 

You need to use non-standard ssh-ports for your VMs. I use openVZ on my server and have three virtual machines with VIDs 101,102,201. I thought, that it would be convinient to ssh into 101 VM through 101 port, into 102 through 102 and so on. To connect to the hardware node you can use 22 port. So I added this routing rules to iptables on hardware node: 

I want to connect to VPN every time goes up. I have already a script to do connect to VPN. How can I run it on interface up event on Fedora Core 12? 

I think, this potentional issue is called load-balancing router. It routes your request sometimes to the 1st interface and sometimes to the 2nd. They have different latency. For example, we have optics on wan1 and ADSL on wan2. For one particular server ping through wan1 was 17 ms and through ADSL - 112 ms. 

I think you should start of monitoring your memory usage/cpu usage and then monitor your server network activity. Try this tutorial. It describes the main linux monitoring tools that will help you. 

Are there other DNS providers which has an API for automation and offers custom/vanity nameserver for a payable price per month? 

I'm researching in this topic really hard the last few days and i just want to discuss this with a few specific questions - i did not find any suitable thread here that is covering my needs and especially, that is quite actual - the most posts about this topic are around 2010 when, i guess, the last time AWS had a big failure (a whole region in murica was down when i remember right) The current state: We're running a Mailserver based on Ubuntu with Postfix/Dovecot/Horde, reading all mailbased configs out of a MySQL database. This is running as an EC2 instance with an EBS Storage where the OS and currently also the mails are stored. So far so good, but we're a startup and not just a private person who needs this server - so it is a Mailservice for our customers, super critical and verry important for us. After a few fails and downtimes in the first year, i will dramatically improve the setup - so i thought about "redundancy", basically.. The requirement: The server must be "redundant" in some way, a fail of a single EC2 instance should not break the whole service anymore. My research so far and options i see to solve: 

I ssh on remote host but terminal performance is poor. Symbols I am typing are not shown immediately, but with some delay. Sometimes two symbols are shown at one time after delay. 

I need to monitor a single process (e.g. be warned when there are more than 3000 connections established) and collect statistics on it (e.g. to determine how many connections were established today 01:20 AM, when the server worked too slow, as client said). What tools should I use? 

to Log into the remote system and type xclock &. This starts a X clock program that can be used for testing the forwarding connection. If the X clock window is displayed properly, you have X11 forwarding working. 

This command is available in most modern Linux distributions. It fills your disk with zeros 10 times. Information on the disk can be partially recovered only using very expensive techniques, that buyer of the old netbook, I think, doesn't posses. 

You need user account on the server (login/password may be different, than for your ftp-account). This user must be or have permissions to run .