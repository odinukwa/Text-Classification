As you can read in the directive docs: The matching is performed against a normalized URI, after decoding the text encoded in the “%XX” form, resolving references to relative path components “.” and “..”, and possible compression of two or more adjacent slashes into a single slash. You can prevent nginx from merging slashes at block level with , although I would recommend caution doing such a thing and would generally advise against it. Your regular expression does not match the normalized URI. You would need to use: 

(owner) has rights --> Use it for content modifier(s), ie developers in development environment, publishing tool in staging/production has rights --> Use it for content accesser(s) (ie Web server, backend applications, etc...), making sure all those belong to 

Thus, all relative paths are computed against this one. To check the value for the binary you use, check . In your case, I see several options, from best to worst: 

I guess you misunderstands nginx' design. This Web server is not following the one-process-per-connection classical scheme as Apache, for one, does. Thus, there is little to no use in adding more workers than you number of CPU cores, a number of worker process being equal to this machine property being the most efficient configuration. Put another way: in Apache times, you had to add more workers to handle more traffic since 1 process <=> 1 connection. Now, this nginx, only the relation 1 connection => 1 process remains, but 1 process => 1 connection is not true anymore. The event-driven design of nginx allows each worker to accept many connections instead of waiting (remaining idle) until he is totally busy (100% CPU on the core-s- being used). The baseline is: you do not need to change the number of workers of your nginx instance if it is properly configured (cf. , being usually enough). If the workers are all saturated, then you will have a machine property being the bottleneck (CPU, I/O), thus adding new workers won't do much but add to the problem. Now, for the part in which you want to 'exchange workers one by one', nginx' master allocate sockets to workers, so you would not know which requests/clients you are affecting. Think of if at a group which spawns (and dies) together. You could have several binary version and/or several configuration loaded on several masters at the same time by using the on-the-fly upgrade capability of nginx. You can then switch between different configuration seamlessly and almost instantly by communicating with the masters through signals. I strongly suggest you read on nginx' design to have a clear idea of what you are dealing with and why what you asks falls flat. You have not specified what your real intent is, so I was stuck with providing you with the exact questions you asked. There is probably a simpler set of questions/answers if you told the community what you wanted to achieve. 

As for the unknown directive "location", that is a totally unrelated error simply indicating your configuration is not valid (thus won't be loaded/applied). You made a syntaxic/lexicographic error while modifying your configuration. Check your changes. 

RFC 2818 states the separation of ports between HTTP/HTTPS comes from the fact the first data sent is different, since HTTPS starts the TLS session before sending HTTP over it. There is no way for the server to understand you if you are not talking the right protocol. Moreover, RFC 7230, updating the previous one, states: 

While 502 corresponds to backend timout, 500 is a server error, indicating a misconfiguration. You might wish to fix the following on first hand: Before the sleep, you actually replaced the first line of with 

Special cases such as directories where Web server/applications need write access, such as an upload directory, make an exception by adding the for permission to this directory (or make this specific directory locked to anyone else by making it owned by ) Now, to automatically make new files being readable by the Web server/applications, you can use the flag from the filesystem permissions. This will automatically change the group of any added file to . Sum up Here is a quick example so sum up everything: Your web server and any backend application belong to the group. You have a Web root on 2 environments: 

I am sorry this answer kicks in late, bu as of now, with the current v1.8.1 stable version, the configuration you provided should work without any talisman. If you provided the version which you were experimenting on, it would be an idea to see if a bug has been corrected or if the configuration was flawed. I suggest you double check your configuration, since you definitely do not need this location nor its contained directive. Take a very special care at removing everything which is not part of this test (and that you do not show) as it might interfere. As a last resort, you could try this following, successfully tested, configuration snippet, and slowly integrate changes and see at which point results diverge from expectations: 

If the right locations are triggered, then you can put your dynamic content back as they won't have an impact. Ensure your configuration is applied by: 

To solve the problem of multi-access to files, I would recommend using the access control mechanisms built in the Linux filesystem mangement, with the default rights: 

Mind the ending , replacing the original URI. If you wish to forward the original URI to backends, remove anything after the pair. 

I cannot reproduce your problem. I tried the following minimal configuration, every working as expected (HTTP 302s): 

Here is a quick sample config removing all the dynamic parts to test that very basic feature of nginx that location matching is: 

To update content in Development, use the account or any tool using that user. To update content in Production, use the git tool rightly configured to push data at the corretc location, using the user. 

That should do it. Now, where does that empty PID file comes from is an interesting question one could ask him/herself... Have you played around with it? Multiple instances of nginx does not apply for it, you should have something there! 

Make sure all the configuration you showed here was found/loaded by nginx Use to validate your configuration Monitor your error log when you send a signal to the nginx master process to spot any error message popping up If you are using nginx v1.9.2+, you might wish to use to dump the loaded configuration to standard output 

I would bet $1000 on the fact your filtering rules accepting traffic are bound to some interface, thus rejecting traffic incoming on . I would pay attention to where the testing client in relation to the server. If you are running your test client on the same machine, it most probably either uses the IP address or the domain name which usually resolves to the same IP address. That will send traffic on the special loopback interface () and not on the one. Unless you bound nginx to a specific interface by asking it to listen on its IP address, nginx will listen by default on , which is every interface. Thus, you won't notice if it accepts connections on or not. You could try to force nginx listening on your IP address just to be sure. When locally testing your server, ensure you use your one of your external interfaces () IP address, or a domain name resolving to one of them. 

Note: is to be preferred to wherever possible, as the latter might result in an unwanted behavior, conflicting with other directives such as . 

is related to host matching. On the nginx side it is done through . The syntax might be different: check the docs. means you explicitely want the QUERY_STRING value to be empty. If that is mandatory to you, use an block matching against being empty. That is an exception because I cannot see a better way of doing it: should be avoided as much as possible. You should put the block in a which isolates the right URI. RewriteRule can either be replaced by a combination of and (preferred way) or (which is to be avoided as much as possible). When you use , try to use prefix locations as much as possible, since regex locations evaluation is indeed slower. Based on performance, it is thus best to avoid them whenever possible too. 

This is not nginx caching file content, this is your filesystem (Unix?) changing the inode when you update the file. Since file descriptor information is cached in nginx, inode update is not propagated to it, thus continuing reading the old inode. On cache invalidation ('s parameter) or revalidation (), file descriptor information gets respectively removed (will be up-to-date on next opening) or updated. This behavior is specific to each filesystem, this is why using file descriptor cache on NFS, for one, is advised against. 

nginx runs as user as your configuration states. You say your directories are owned by . Ensure that each and every directory from the root to the file attempting to be accessed by nginx has the right execution (and maybe reading?) permission. In your case, the other permission group (). If that is not enough, try to add the reading permission for others on socket files. Normally now nginx will be able to those files. The fact that everything runs fine with nginx being set to is that those files have been created by the nginx package with an adequate mix of user/group/permissions... Now, I see another (and bigger) problem: you try to work with nginx workers running as and files owned by , for which there is no write permission for nginx whatsoever. Any attempt by nginx to write on a socket will thus ultimately fail... You should really rethink the whole construction of yours. Make nginx user/group and directories/files ones match in some way so nginx can open what it need to open (execution right on directories), read what it needs to read and write what it needs to write without relying on the other permission group which basically allows any user to do the same. 

1. Handling arguments Despite what Tero Kilkanen says, nginx is perfectly capable of handling arguments: this is done by using the and variables from the core module. 2. context As Glueon pointed out, is usable in , and contexts, as the documentation states. is to be avoided at all costs, so I won't be going inot details about it here. Now should you use in or ? In Blocks of s will be considered sequentially, and the first matching will be used. Your configuration thus depends on the order of directives, which is bad (it is one of the things which are wrong with Apache). See for yourself: 

The only way to load-balance in nginx is having a single frontend (reverse-proxy) host load-balancing backend servers. The idea/hypothesis behind this design is that load will happen on the backend only and that your single entrypoint will always be able to cope with whatever amount of traffic it is supposed to deal with, since it simply redirects and never processes anything itself. What you are talking about is actually failover, not load-balancing. Your concern is the failure of your single entrypoint. As @coding_hero explained, this has nothing to do with nginx, it is something to be dealt with at the underlying layers (OS/network). One way of doing it might be read on the following page (old example talking about Debian oldstable though, commands might need to be freshen up): $URL$ Heartbeat is a well-known technology allowing several identical servers to monitor each other, electing a master and failing-over to slaves with needed. You even have dedicated network hardware doing the same job by rerouting (or maybe reconfiguring routers on-the-fly to reroute?) traffic to the currently elected master. 

When load-balancing the requests with the default round-robin, nginx might face a problem there... You should ensure that the always contains valid data as soon as it is used. You may add s in there later, but letting a partially written placeholder will won't produce any good. 

does not replace existing header but adds a new one, possibly leading to duplicate that might be filtered when it is supposed to be unique and/or only the first one of its type is read. Another possibility is that the HTTP status code from your response does not match the list defined in the docs for this directive, since you did not use the parameter. The text/html you see might then be nginx' default 404 page. Anyhow, what you want to use for your need is the directive: 

It runs flawlessly, so the most probable cause of your trouble is that your configuration is not applied correctly. I would guess that changes you made on the side of adding the directive broke it, making the variable inexistent. Try to: 

As for the lack of security, I hosted a talk about nginx and PHP-FPM at nginx.conf 2014 end October. Slides are available: $URL$ Video soon available. 

Note: I suspect the configuration snippet you provided is not working, since the URI prefix of the directive does not start with the mandatory character. 

You did not specify a directive in the configuration snippet you shared. It is supposed here that this directive exists, because otherwise your question becomes nonsensical. You specify not wishing the redirects being done to the reverse-proxy, but rather directly to the frontend. However, your configuration tells exactly the opposite. I suggest you read its documentation (again). The log snippet you shared in the end seems to come from the frontend component, not the reverse-proxy one (). However, if it were (because of docker container + misleading port-binding), it would lead towards the absence of a directive, cf. bullet #1. 

merely does an internal redirect to the last parameter if no file specified before is found. Therefore, you should not notice any change to the URI. TeroKilkanen asked for a complete configuration. You should provide a minimal working configuration reproducing the alleged behavior. Regex s support PCRE, thus why not? If you encounter trouble with PCRE, you should specify your question. 

You are mistaken about the way nginx operates. Requests are processed neither one after another nor simultaneously (which is impossible). nginx reacts on events corresponding to differents stages during requests processing, which kind of multiplexes them. The upstream module (and its directive of the same name) you point to is the way to go, along with the proxy_pass directive. The center of your focus might be the variable. As its name indicates, it stores response time of the upstream servers and might be dealt with to change the selection of the backend server nginx shall choose. To do so, you can tweak your upstream servers group to add a weight to each and different flags to impact the default round-robin mechanism. You can then change weights according to the response times you collected by generating files you include in your upstream servers group configuration. Note that changing the configuration will need nginx to be reloaded. 

You need to identify where the request producing the original error was processed. 'Upstream' might be a lot of different things (proxy, fastcgi, uwsgi) You need to tweak the proper upstream and to make the proper buffer(s) bigger 

TCP layer just route traffic with added headers/control over network packets from the underlying layer. It has no knowledge of what it transports, thus does not need to deal with its content, and whether it is HTTP(S). HTTP/HTTPS does not matter. When dealing with the application layer, you are deep down within content, since you deal with an application. In case of HTTPS, you will need SSL termination. HTTP/HTTPS does matter. Thus, if you want to load-balance at application level, you will then need to do SSL termination on the same load-balancers. nginx allows you to do that. If you wish to separate those tasks, you will need a network/routing-level load-balancer. 

As pointed out, your configuration snippet looks good and accessing in port should be served in the 3rd block you provided. If it does not work as you describe, it might be because it is not loaded. 

I suppose and are empty, making the subshell fail/being empty, meaning that this line is actually interpreted as