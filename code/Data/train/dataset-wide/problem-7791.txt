The answer is no. Here is an explicit example. Let $R=\mathbb{Z}[\sqrt{-5}]$. Consider the matrix $$\left(\begin{array}{cc}1+\sqrt{-5}& 2\\ 2& 1-\sqrt{-5}\end{array}\right).$$ It represents an element of $PGL_2(R)$ that is not in the image of $GL_2(R)$. The motivation for this example is that the ideal $(2,1+\sqrt{-5})$ is not principal in $R$ and this should be relevant because the next term in the long exact sequence is $H^1(R,\mathbb{G}_m)$ (and so the sequence written in the question will be exact whenever this $H^1$ vanishes). I would love to see a more conceptual proof of the failure of $GL_2(R)\to PGL_2(R)$ to be surjective. 

I can give a general solution under the additional assumption that the long word acts by -1 on the root system. Let $\alpha_i$ denote the simple roots and $\beta_i$ be a vector orthogonal to all $\alpha_j$ with j different from i. Let (,) be a W-invariant inner product. The point: For any vector v and any w in the maximal proper parabolic corresponding to i, we have $(v,\beta_i)=(wv,\beta_i)$. A simple Lemma: If $(\gamma,\beta_i)=(\alpha_i,\beta_i)$, $(s_i\gamma,\beta_i)=-(\alpha_i,\beta_i)$ and $||\gamma||=||\alpha_i||$, then $\gamma=\alpha_i$. Since rank at least two, there exists a root $\alpha'_i$ with $(\alpha_i,\beta_i)=(\alpha'_i,\beta_i)$. and $||\alpha'_i||=||\alpha_i||$ Now if $w_0=w_1s_iw_2$ with $w_1,w_2$ in the max. parabolic, lets act on $\alpha_i$ and $\alpha_i$. Since $w_1,w_2$ can't change value of inner product with $\beta_i$, and applying $w_0$ must multiply this product by -1, by our lemma we must have $w_2\alpha_i=w_2\alpha'_i=\alpha_i$, a contracition. 

Suppose that $x\in \mathbf{f}$ and $r(x)=x\otimes 1+1\otimes x$. Recall $\mathbf{f}$ is graded by $\mathbb{N}I$, WLOG $x$ is homogenous, $x\in \mathbf{f}_\nu$. We know we get primitive elements if $\nu=i$ for some $i\in I$ so let us assume that $\nu$ is not of this form. Then $\mathbf{f}_\nu$ is generated by products of the form $yz$ with $y\in \mathbf{f}_\lambda$, $z\in \mathbf{f}_\mu$ with both $\lambda$ and $\mu$ nonzero. Now we consider the symmetric bilinear form on $\mathbf{f}$ and compute $$(x,yz)=(r(x),y\otimes z)=0$$ since $x$ is assumed primitive. This shows that $x$ is orthogonal to all of $\mathbf{f}_\nu$. But the bilinear form is nondegenerate, so $x=0$. Thus the Chevalley generators give a basis of the primitive elements of $\mathbf{f}$. 

Piggybacking on one of Pierre's answers, I once had to teach beginning linear algebra from a textbook wherein the authors at one point stated words to the effect that the the trivial vector space {0} has no basis, or that the notion of basis for the trivial vector space makes no sense. It is bad enough as a student to generate one's own false beliefs without having textbooks presenting falsehoods as facts. My personal belief is that the authors of this text actually know better, but they don't believe that their students can handle the truth, or perhaps that it is too much work or too time-consuming on the part of the instructor to explain such points. Whatever their motivation was, I cannot countenance such rationalizations. I told the students that the textbook was just plain wrong. 

It seems that Joseph's approach would require computing the set of vertices from the standard form. I don't recall what the complexity of this is in the worst case, but I dimly remember it being pretty bad in the extreme case. In any case, I wonder whether that is in the spirit of the original question. Sad to say, I don't have an idea to propose. 

I will second Thierry's reply. Of course, when I see the term "CV" I think of academic positions, whereas if one is seeking a job in industry one would instead produce a "resume". Different beasts, though in both cases the purpose is to some extent the same, namely to give an employer some idea why they should be interested in you to fill their opening. As Thierry said, a hiring manager in a corporation or industrial firm will have a very different set of things he is looking for in a resume compared to what a hiring committee in a math department at some college or university expects in a CV. 

Let $V_D$ and $V_Q$ be the two dimensional simple representations of $D_4$ and $Q_8$ respectively. Let $1_D$ and $1_Q$ denote their trivial representations. Suppose that there is a tensor equivalence between $\mathbf{Rep}(D_4)$ and $\mathbf{Rep}(Q_8)$ commuting with the fibre functor to $\mathbf{Vect}_\mathbb{C}$. This equivalence sends $1_D$ to $1_Q$ (as they're the unit object) and sends $V_D$ to $V_Q$ (as they're the unique simple of dimension 2). In particular there is a $\mathbb{C}$-linear isomorphism $g$ from $V_D$ to $V_Q$. Consider $$g\otimes g:V_D\otimes V_D\to V_Q\otimes V_Q.$$ It must send the unique copy of $1_D$ in $V_D\otimes V_D$ to the unique copy of $1_Q$ in $V_Q\otimes V_Q$. It is easy to see that there is no such $g$. The slickest way I can see to prove this is to note that the flip map $v\otimes w\mapsto w\otimes v$ acts by -1 on $1_Q$ and by 1 on $1_D$. 

[Bo2] is the same reference to Borel's "Linear Algebraic Groups" that was provided in a comment by Uri Bader. 

Consider the cohomology ring of the Grassmannian of k-planes in complex n-space. It has a standard presentation as a quotient of the ring of symmetric functions. In this presentation, the Schur functions are mapped to the Schubert classes, thus have a nice geometric interpretation. One can generalise the Schur functions to skew-Schur functions. Do these also have a nice geometric interpretation? 

I will give the following positive answer: Let H be a split reductive subgroup of the split reductive group G of the same rank. Let WH and WG be their Weyl groups. Then every fundamental invariant of WH divides a fundamental invariant of WG. Unsatisfactionally, this proof will not give a bijection between fundamental invariants satisfying a divisibility condition. This includes all the usual examples of Levi subgroups of reductive groups, as well as some funkier examples such as Spin(9)⊂F4. Let PG(t) be the Poincaire polynomial $$P_G(t)=\sum_{w\in W_G} t^{\ell(w)}$$ and similarly for PH(t). Since H(q) is a subgroup of G(q), Lagrange's theorem, together with the formula for the order of a reductive group over a finite field, gives us the divisibility relation $$P_H(q)\mid q^NP_G(q)$$ for some positive integer N. (I've already cancelled the common factor of (q-1)r since G and H both have rank r). And as PH has constant term 1, we can strengthen this to PH(q) dividing PG(q). Once we have this divisibility relation for infinitely many q, this implies that PH(t) divides PG(t) in ℤ[t]. This may not be immediately obvious so here is the proof: Since PH is monic, you can use the division algorithm to write PG=qPH+r with the degree of r less than the degree of PH. For infinitely many prime powers q, |r(q)|<|PH(q)|. Thus the divisibility relation implies that for such q, r(q)=0. r now has infinitely many roots, so is the zero polynomial. Now let d be a fundamental invariant of WH and let ζ be a primitive d-th root of 1.Then PH(ζ)=0. The divisibility result we've just established implies that PG(ζ)=0 as well. Since PG(t) divides the product of te-1 as e runs over all fundamental invariants of WG, it must be that d divides one of these fundamental invariants. QED. 

Alan, I have been in a situation very similar to yours. After getting my Bachelor's in math (also from a small, essentially no-name school), I spent 23 years in the software industry (Silicon Valley). Boredom and frustration with my work, and an ongoing interest in math since my undergraduate days all motivated me to consider returning graduate school. Being laid off during the recession in 2002 gave me the final nudge. I immediately entered a master's degree program in math at Cal State Hayward, full time. At first, I was thinking that the master's would be my terminal degree. It was only after I gained a mentor while in the program that I learned that it was feasible to go on to a PhD program, even at my age. In particular, the vast majority of PhD grad students in math have a teaching assistantship, which pays for tuition and supplies a (very modest) stipend. One bit of advice to me was to not go to any school which did not offer a TA-ship. Working on my master's provided me recent reference letters. (Some of my best undergraduate references were deceased.) I cannot say that working on a master's before the PhD program is required, or even recommended. Be aware that many schools have programs where a student enters under a master's degree program, and then before completing the master's the student can decide, together with faculty consultation, whether to continue on to a PhD program. (My PhD school, the University of Washington, offers such a program.) In fact a few schools require all their students to complete a master's before formally being accepted into their PhD program. These programs, where master's and PhD are both done at the same school, will save some time overall since all units are guaranteed to be transferable. (None of my master's degree work at CSUH transfered to UW, but it did equip me to pass one of the preliminary exams at UW.) I finished my PhD in 2009. I was hoping to enter academia, and I did a 2-year postdoc at UBC. But as everyone has noted, the academic market is dismal at this time, and I took a position in government. So it is good that you are aware of the job market conditions and planning accordingly. As David White noted, there are several industries that hire mathematicians. He also noted that the NSA and affiliated contractors hire a great number of mathematicians, doing interesting work. I would also note that there are other government labs and agencies that also hire some number of mathematicians. In any of these, I would suggest that while it's OK to seek "math-oriented" work, you should at the same time remain open to leveraging your industry experience, especially any software skills you may have acquired. All in all, I have no regrets for pursuing a PhD. Indeed, I would have had regrets had I not. Best to you in your pursuits. 

Suppose f:X→Y. If I decorate that first sentence with appropriate adjectives, then I get a pushforward map in cohomology H*(X)→H*(Y). For example, suppose that X and Y are oriented manifolds, and f is a submersion. Then such a pushforward map exists. In the de Rham picture, we can see this as integrating a form over fibres. In the sheaf cohomology picture, we can see this via the explication of the exceptional inverse image functor. The question is how else can we think of this pushforward map. I'd be particularly interested in an answer from the algebraic topology point of view, because I'm hoping that such an answer would eludicate the appropriate level of generality in which a pushforward in cohomology exists (perhaps not only answering the question of for which maps f, but also answering the question of in which cohomology theories can we carry out such a construction). 

Starting with the affine case, if you try to define infinite dimensional affine space as Spec of k{x1,x2,...], then you realise that this is not a vector space of countable dimension, but something much larger. If you want a vector space over k of countable dimension, then this will not be a scheme, but instead will be an ind-scheme. A similar description should hold in the projective case. Edit: Regarding why I am saying that Spec(k[x1,x2,...]) is too big: A (k-)point of Spec(k[x1,x2,...]) is an infinite sequence a1,a2,... of elements of k. If I wanted a vector space of countable dimension, then I should be asking for sequences a1,a2,... of elements of k, only finitely many of which are non-zero. This latter space is the inductive limit of affine n-space as n tends to infinity. 

Yet another way in which people could use arXiv is as a repository for material which otherwise cannot find a home in a journal. Sometimes in the course of working on a project I wind up with some material which did not make it into the published article -- or perhaps some notes which record my growing understanding of articles already published by others -- which look like they could be useful to the community as expository or supplemental information, but which in my opinion are not otherwise significant enough to warrant submitting to a journal. I have sometimes wondered whether it would be appropriate to post such material on the arXiv. (I have not done so yet.) For that matter, I wonder whether others have done this very thing. One drawback of this use of the arXiv is that everyone knows that most arXiv articles have not been peer-reviewed at the time of first posting, so they must be taken with a grain of salt. If an article is never published in a journal, you must read the arXiv article with a more critical eye. But as I said, I do believe that there exist some notes that are perhaps worth sharing but not worth wasting the effort to peer review. 

Don't you need some strict inequality somewhere in your definitions? For example, a constant function meets your definitions of antimonotonic, submodular, and supermodular, but does not induce a metric (assuming your lattice has more than one element) since $d^s$ and $d_s$ would then always evaluate to zero. 

I think the gap(s) outlined in the original post of this thread are inevitable since there is a limit to how many courses an undergraduate student can cram into his available time before graduating. Pertinent material for this discussion is the book "All the Mathematics You Missed But Need to Know for Graduate School" by Thomas Garrity. The goal of this book is ambitious, and so naturally it has its shortcomings. It has a number of errors, and one might dispute Garrity's particular selection of topics. But overall I like the book, and would recommend it to any undergraduate who is considering a professional career as a mathematician. The main body of the book is a series of short chapters, each a very brief introduction to a particular area of mathematics. This can be a useful read even for the mature mathematician who suspects that their mathematical experience may be a bit parochial. But one of the most useful parts of the book for me, which I read while working on my master's degree, and which I wish I had been exposed to as an undergraduate, is the introductory material that presents some of the broad patterns that are common across all branches of mathematics, and which form an outline for each of the following chapters. For example, he notes that every branch of mathematics is the study of some particular set of mathematical objects. This study includes questions such as how to tell when two objects of some class are essentially the same (isomorphic); when one is a sub-object of another; how new objects can be constructed from old ones; a notion of maps or morphisms between objects of a class that preserve the essential properties that the objects are supposed to capture; the notion of quotients; etc. As "muad" noted above, some teachers plan on students learning these principles more or less by induction, from having many concrete examples presented to them, and never mentioning them explicitly. While that may be an ideal way to learn the principles, there is no guarantee that a given student will learn them, regardless of the number of examples given. And I have met some mathematicians who, as far as I can tell, aren't aware of them. As an undergraduate, I came away with the sense that the various branches of mathematics were rather disjointed, with no real common patterns. Blame it on poor instruction, or more likely, my being dense. I think I would have benefitted a lot from someone pointing out these patterns to me in an explicit way. 

Certainly for any choice of convex ordering you can pick out a basis of your simple Lie algebra. You do this by specialising the construction of PBW bases for quantum groups to q=1. (This construction can be presented without knowing about quantum groups). With your proposed formula for $E_{\alpha+\beta}$ as a commutator, I would encourage you to modify it by dividing by $p_{\alpha,\beta}+1$ where $p_{\alpha,\beta}$ is the largest integer $p$ such that $\beta-p\alpha$ is a root. This way you will get the correct integral form of the Lie algebra. See e.g. these notes of Bill Casselman. I don't know if the formula $E_{\alpha+\beta}=[E_\alpha,E_\beta]/(p_{\alpha,\beta}+1)$ holds for all $\alpha$ and $\beta$. Certainly it holds up to sign. If you make a further assumption that $(\alpha,\beta)$ is what I call a minimal pair, then this formula is correct, even in the quantum case, see Theorem 4.2 of arXiv:1210.6900. 

I assume all groups are discrete and ignore Q3. Also I assume that π comes from a group homomorphism from H to G. By Poincaire-Verdier duality π! is the same as π*. Both of these are identified with restriction. Then π* is the right adjoint of restriction, hence is what I would call coinduction, while π! is the left adjoint of restriction, hence induction. In some contexts (e.g. p-adic groups), I've seen the right adjoint be called induction. The dualising sheaf is the same as the constant sheaf, hence corresponds to the trivial representation. 

As mentioned in Kevin's comments, the standard proof for reductive G is in any of the standard texts on reductive groups. One slick way to go about it is to show that you have a Tits system/BN pair, after which the Bruhat decomposition is known to fall out. If G=GL_n, then it is not hard to show that this is the case, while for more general groups, one still must require some development of the structure theory of reductive groups.