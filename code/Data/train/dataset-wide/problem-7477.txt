The implications $1 \implies 2$ and $2 \implies 3$ are straightforward. For $3 \implies 1$, write $A = v w^T + n B$, and expand out $\det(v w^T + n B)$ as a sum of products of minors from $v w^T$ and from $n B$. All the terms which involve a minor of $v w^T$ larger than $1 \times 1$ are zero, so every term is divisible by $n^{k-1}$. When $n$ is not prime, we still have $3 \implies 1 \implies 2$, and $2$ does not imply the others; look at $\begin{pmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{pmatrix}$ with $n=4$. Also, for $n$ nonprime, $1$ does not imply $3$, look at $\begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix}$ for $n=4$. From your described motivation, it sounds like it would be interesting to know whether your matrices obey $3$. 

The above cited work of Kedlaya shows that $\max(2g,18)$ values suffice, where we are allowed to use conditions (1) and (2) to eliminate solutions. 

That contsruction is very explicit, but it uses the fan. There is a more abstract construction, which I don't understand as well, which works for any normal variety and does not use the torus action. Specifically, for every $h \in H$, choose a specific Weil divisor $D(h)$ representing $h$. For any $h_1$ and $h_2$, choose a rational function $u(h_1, h_2)$ with divisor $D(h_1+h_2) - D(h_1) - D(h_2)$, subject to conditions to be named later. Define the ring $$R := \bigoplus_{h \in H} H^0(X, \mathcal{O}(D(h)))$$ with multiplication $H^0(X, \mathcal{O}(D(h_1))) \times H^0(X, \mathcal{O}(D(h_2))) \to H^0(X, \mathcal{O}(D(h_1 + h_2)))$ given by $f_1 \times f_2 = f_1 f_2 u(h_1, h_2)$. My understanding is that (1) it is always possible to choose the $u$'s such that this is a commutative and associative ring and (2) $R$ is independent of these choices, up to nonunique isomorphism. However, you should check these claims before using them. We set $Y = \mathrm{Spec} \ R$. This comes with an obvious action of $G$, where $G$ acts on $H^0(X, \mathcal{O}(D(h)))$ by the character $h$. I am not sure how to define $U$ or the map $U \to X$, but I think there is a way. Note that none of this uses the torus action. 

No. Let $f=z^6 + x^4 y^2 + x^2 y^4 − 3x^2 y^2 z^2$. By the AM-GM inequality, $f$ is nonnegative. Suppose that $f=\sum g_i^2$, with the $g_i$ smooth. Expand each $g_i$ in a Taylor series around $0$: $g_i = a_i + b_i(x,y,z) + c_i(x,y,z) + d_i(x,y,z) + O(|x|+|y|+|z|)^4$, with $a$, $b$, $c$ and $d$ homogenous polynomials of degrees $0$, $1$, $2$ and $3$. Comparing terms of degrees $0$, $2$, $4$ and $6$ in a Talyor series around $0$, we see that $a_i=b_i=c_i=0$ and $z^6 + x^4 y^2 + x^2 y^4 − 3x^2 y^2 z^2 = \sum d_i(x,y,z)^2$. It is well known (see e.g. Wikipedia) that the left hand side is not a sum of squares of cubics. 

All your conditioned are unaltered by acting on the matrix by $GL_3$ on the left, so we can think on the space $GL_3 \backslash \{ \mbox{$3 \times n$ matrices of rank $3$} \}$, also known as the Grassmannian $G(3,n)$. The advantage of the Grassmannian is that is compact. If we build a family $M(t)$ of rank $3$ matrices, parametrized by $t \neq 0$ and preserving all the rank $2$-ness conditions, then it will have some limit as $t \to 0$, which we can lift back to a rank $3$ matrix. EG: $\left( \begin{smallmatrix} 1 & 0 & 0 &0 \\ 0 & t & t^2 & t^3 \\ 0 & 0 & t & t \\ \end{smallmatrix} \right)$ looks like it is approaching a matrix of rank $1$ as $t \to 0$, but it is the same family up to $GL_3$ action as $\left( \begin{smallmatrix} 1 & 0 & 0 &0 \\ 0 & 1 & t & t^2 \\ 0 & 0 & 1 & 1 \\ \end{smallmatrix} \right)$, whose limit is rank $3$. So I can approach your question by building families $M(t)$, passing through $M$, and preserving the rank $2$-ness of various submatrices, and be guaranteed that my limits will exist. An easy way to build a family of matrices that preserves the rank of all $3 \times (\mbox{whatever})$ submatrices is to look at $M \cdot X(t)$, where $X(t)$ is a one parameter subgroup of the diagonal matrices in $GL_n$. For most one parameter subgroups, some columns will become $0$ in the limit, so we can't use them. The set of one parameter subgroups for which none of the columns die is called the Bergman complex. The most common elements of the Bergman complex are indexed by "complete chains in the lattice of flats". Removing the matroid jargon, in the case of the plane, this means "a point $x_i$, and a line $\overline{x_i x_j}$ through the point". What I wrote out was the limiting $3 \times n$ matrix for that case. 

Imposing that you can resolve by a length $2$ sequence of vector bundles is too strong. What you want is that there is some $N$ so that you can resolve by a length $N$ sequence of vector bundles. By Hilbert's syzygy theorem, this follows from requiring that the scheme be regular. (Specifically, if the scheme is regular of dimension $d$, then every coherent sheaf has a resolution by projectives of length $d+1$.) Here is a simple example of what goes wrong on singular schemes. Let $X = \mathrm{Spec} \ A$ where $A$ is the ring $k[x,y,z]/(xz-y^2)$. Let $k$ be the $A$-module on which $x$, $y$ and $z$ act by $0$. I claim that $k$ has no finite free resolution. I will actually only show that $A$ has no graded finite free resolution. Proof: The hilbert series of $A$ is $(1-t^2)/(1-t)^3 = (1+t)/(1-t)^2$. So every graded free $A$-module has hilbert series of the form $p(t) (1+t)/(1-t)^2$ for some polynomial $p$; and the hilbert series of anything which has a finite resolution by such modules also has hilbert series of the form $p(t) (1+t)/(1-t)^2$. In particular, it must vanish at $t=-1$. But $k$ has hilbert series $1$, which does not. There is, of course, a resolution of $k$ which is not finite. If I am not mistaken, it looks like $$\cdots \to a[-4]^4 \to A[-3]^4 \to A[-2]^4 \to A[-1]^3 \to A \to k$$