A ring $R$ is called $\textit{equidimensional}$ if $\dim R = \dim R/p$ for any minimal prime $p$ of $R$. 

I believe that you are looking for a description of the defining equations of Rees algebras in commutative algebra. In general, this is a hard question. The complete intersection case can be generalized to ideals of linear type. 

To support J.C. Ottem's answer, let me present one example. Let $R = \mathbb{C}[x,y]$ and $I = (x,y^2)R$. What is the minimal graded free resolution of $R/I$, equivalently $I$? That is, $0 \rightarrow R(-3) \stackrel{d_1}{\rightarrow} R(-1) \oplus R(-2) \stackrel{d_0}\rightarrow R \rightarrow R/I \rightarrow 0 $ where $d_1 = (-y^2 \;\; x)$ and $d_0 = (x \;\; y^2)$. Now, ask what are the graded Betti numbers and minimal number of generators for $R/I$. I agree with J.C. Ottem's opinion on reviewing the definitions. I hope this helps. 

If $I_n$ are ideals and $I_0 = R$, then the answer is yes. Consider the following ring extensions; $$ R[Jt] \subseteq R[It] \subseteq \oplus I_n. $$ Since $J$ is a reduction of $\{ I_n \}$, the extension $R[Jt] \subseteq \oplus I_n$ is module-finite, i.e., $\oplus I_n$ is a finitely generated $R[Jt]$-module. Therefore, it is Noetherian. The module $R[It]$ is a submodule of a Noetherian module. Hence $R[It]$ is a finitely generated module over $R[Jt]$. This show that $J$ is a reduction of $I$. 

Hi Guillermo, If we mean an ideal generated by a regular sequence by "regular", then there is an example. Let $R = k[x,y]$ be a polynomial ring over a field and $m = (x,y)R$. Let $J = (x^2, y^2)$ and $K = m^2$. Then one can check that $J K = m^4$. Notice that $JK : K = m^4 : m^2 = m^2$, but $xy$ is not in $J =(x^2, y^2)$. Hence $JK : K \neq J$. I believe the questions you have might have a close connection to the integral dependence of ideals (or reduction of ideals). For instance, the condition (1) implies that the ideal $J$ is integral over $I$ if $K$ contains a regular element. The technique is exactly the same as in Steven's proof. Also, check out the m-full property which has to do cancellations in some cases. I recommend an excellent book by Huneke, Swanson on this subject for reference. Chapter 1 contains a good overview of the theory. 

Hi! I have a proposed answer for 2). You can construct an infinite series of counter examples. Erdos proved that there are primes between $n$ and $2n$. Now consider $2n=2^k$ for some $k>2$. We can observe that the binomial number $\binom{2n+1}{r}, 1< r < n+1$ is always even. The upper product have powers of 2 as the following sequence: $\lbrace k,1,2,1,3,\dots,s \rbrace$ while the bottom product have powers of 2 of the form: $\lbrace 1,2,1,3,\dots,s,k-1\rbrace$. This can be reflected for cases $\binom {2n+1}{r}, n < r < 2n$. The difference of the sum of these 2 sequences is at least 1. As a result, the binomial number may be divisible by $p$ but it will not be even. (Edit) Exception: When the binomial number is $\binom{2n+1}{1}$ or $\binom{2n+1}{2n}$ you have either no powers of 2 or same powers of 2. But in this case it will not be divisible by $p$. 

It is easy to generate all of them without repetition using $n-1$ generators, by the Steinhaus-Johnson-Trotter algorithm. It is easy to generate all of them, with repetition, using two generators. However I was unable to find a way to generate all without repetition and using only two generators. As this approach seems natural, I suspect someone should have worked on it but I was unable to find any references online. Does anyone knows the status of this problem? 

Inria has published a paper factoring 50-200 bits integers. The four methods of choice are: SQUFOF, ECM, SIQS and CFRAC. Within the paper a short pseudocode is given for each method, so you may be able to judge whether they will suit your computation restrictions. From their experiments, SQUFOF is indeed the fastest up to $2^{60}$ range, where it is tied against SIQS. Note that these experiments are done on integers known to have few prime factors (Something like 2-4). For your case, it appears that you are factoring random integers up to $2^{60}$. Hence, except for ECM, it is more efficient to trial factor small primes up to a certain bound. (See comments below on the case for ECM) One additional point: ECM (specifically GMP-ECM, also from Inria) might be a good idea since the software is highly optimized. (With a research team coding it for a few years) On the other hand, the arithmetic is done modulo $N$. This might mean that there will be overflow during the multiplication steps. If you choose ECM, I can elaborate a little more. A common problem is the fact that ECM is probabilistic. This can be resolved if you know which curves to choose to cover all the prime factors. Inria has also published its work on this area, showing that 124 carefully chosen curves finds all prime factors up to $2^{32}$. The numbers are quite suitable for your case. Statistically speaking though, you find the factors with probability around 0.15 per curve, so usually 20 curves will do the job. As mentioned earlier, using these parameters you can skip the trial factorization part. During the search for bigger primes, you will also find the smaller ones in the process. If you want to push the efficiency even higher, the optimal approach is to first do a filtering based on $P-1$ and $P+1$ factorization method, followed by the actual ECM. This is a little tougher since you need to calibrate the parameters correctly. EDIT: Did not notice that all numbers are of the form $x^3-y^3$ with $ 0 < y < x < 50000 $. There are about $50000^2/2=2^{36.86}$ possibilities. Out of all these, a portion will be primes. A portion will be smooth with factors $< B$ for some $B$ that can be easily trial divided. The remain cases are composites with primes $> B$. Suppose you have $n$ remaining entries. Sort them in the following format, by $N_i$: $N_i L_i$, where $N_i$ is the entry's value, each a composite of factors $> B$, and $L_i$ is the offset to find the factorization. At offset $L_i$, store the distinct prime factors (may choose not to store multiplicity by doing trial division). Each time you get a number: 1) Do primality test 2) Do trial division 3) Do another primality test 4) If residue remains, do a binary search on the factorization I think this should be able to handle your 4 million factorization. Clearly there are more ways to tweak the method, but the idea here is you can do a one time computation and store the difficult computations as a look up table. 

If you write the problem of finding the closest point to $x_0$ on the cone with a Lagrange multiplier, the solution must have the form $x = (\lambda A + I )^{-1}x_0$. If you start by diagonalizing $A$, the inverse can be computed efficiently and you can search for $\lambda$ by dichotomy. The algorithm will run in $O(n^2 \log{1/\epsilon})$ 

It's intuitively desirable for the answer not to depend on a unitary transform of the matrix. To estimate the distance of our estimate to the other matrices, a natural choice is the Kullback-Leibler divergence. The equivalent of a mean is then to pick: $$\hat{\Sigma} = \text{argmin} \left( \sum _{k=1}^{n} \text{tr}\left(\Sigma^{-1}\Sigma_k\right)-\lg \left(\left|\Sigma^{-1}\Sigma_k\right|\right)-d\right)$$ Matrix calculus actually tells us that $$\hat{\Sigma} = \frac{1}{n}\sum _{k=1}^n \Sigma_k$$ too see why differentiate with respect to $\Sigma^{-1}$ Handwaving follows: In a way, the KL-divergence plays the role of the squared distance here, since the average matrix minimizes the average KL-divergence. Note that this is similar to the Riemann metric, but instead of looking at $\sum_i \lg{(\lambda_i)}^2$ we're looking at $\sum_i \lambda_i-\lg{(\lambda_i)}-1$. If the matrix are contained in a small ball, the $\lambda_i$ are close to $1$ and the difference between the two functions - up to a scaling factor - is $O((\lambda_i-1)^3)$. The KL-divergence has a probabilistic interpretation which isn't clear with the Riemann metric. We could get a median by using the square root of the KL-divergence. $$\hat{\Sigma} = \text{argmin} \left( \sum _{k=1}^{n} \sqrt{ \text{tr}\left(\Sigma^{-1}\Sigma_k\right)-\lg \left(\left|\Sigma^{-1}\Sigma_k\right|\right)-d}\right)$$ It's easy to compute iteratively since $$\frac{df}{d\Sigma} = \sum _{k=1}^{n} \frac{(\mathbf{I} - \Sigma^{-1}\Sigma_k)\Sigma^{-1}}{2\sqrt{ \text{tr}\left(\Sigma^{-1}\Sigma_k\right)-\lg \left(\left|\Sigma^{-1}\Sigma_k\right|\right)-d}}$$ 

Answer is yes, it's Sklar's theorem. The copula is merely a form of normalization that makes all your marginals U(0,1). Given a copula and the marginals, you can reconstruct the original distribution, and a fortiori get any marginal you're interested in. 

Is there some way to express: $$I(t) = \int_{-\infty}^{t} e^{-2\mathrm{cosh}(x)}~\mathrm{d}x$$ From Bessel functions? By substituting $y = \mathrm{cosh}(x)$ we get $$I(t) = \int_{1}^{\mathrm{cosh}(t)} \frac{e^{-2 y}}{\sqrt{y^2-1}}~\mathrm{d}y$$ In this form, Mathematica will give $I(\infty) = \mathrm{BesseK}(0,2)$ but not indication of what $I(t)$ might be. I can write the integral as: $$I(t) = \int_{1}^{\mathrm{sinh}(t)} \frac{e^{-2 \sqrt{z^2+1}}}{\sqrt{z^2+1}}~\mathrm{d}z$$ and try to expand $f(x) = \frac{e^{-2 \sqrt{z^2+1}}}{\sqrt{z^2+1}}$ as $$f(z) = \sum_{i=0}^{\infty} (-1)^j\frac{2~\mathrm{BesselK}\left(\frac{1}{2}+j,2\right)}{j!\sqrt{\pi}} z^{2j}$$ Unfortunately the series diverges for $z \ge 1$, as does the series for $f(z) z e^{2 z}$ 

Some background (not needed to answer the question): For factorizations using the Number Field Sieve, one would construct 2 polynomials $f(x)$ and $g(x)$ such that $f(m)\equiv g(m) \equiv 0$ (mod $N$) for some $m$. Usually, one of the polynomial will be of degree 1, the other ranging from 2 to 8 for practical purposes. There exists methods to construct 2 polynomials of similar degree, such that both are non-linear. The problem stated essentially gives one a way to construct 2 cubic polynomials. This method may have some advantages over the traditional linear case. Note that the problem do not ask how can one find the actual geometric progression, as that is believed to be hard and at the same time an open problem. Motivation: I am posting this problem as the sources I read suggests that such series do exist by "counting argument". The part I am wondering about is: how does the counting argument work? Initial Thoughts If I may humbly present some of my naive ideas: We start off with $2*N^{2/3}$ possibilities for the first term $a$. Next, since we seek a second term $b$ bounded by $O(N^{2/3})$, we similarly allow $2*N^{2/3}$ possibilities for it. However, this fixes the common ratio $r=ba^{-1}$ (mod $N$), whence we have no control over the 3rd, 4th and 5th terms. We reason that such an $r$ must exist for each pair of $\lbrace a,ar \rbrace$, as failure would imply that $a^{-1}$ does not exist, which implies that we have found a factor of $N$. If we were to assume that the effect of multiplying by $r$ (mod $N$) results in a random permutation of the integer, the 3rd, 4th and 5th terms each have $(2*N^{2/3})/(2*N)=N^{1/3}$ chance of being bounded by $O(N^{2/3})$. This gives us a grand total of $2*N^{2/3}*2*N^{2/3}/(N^{1/3})^3=4*N^{1/3}$ valid series. Suppose this line of thought holds, the difficult part is to account for determinant not zero. (This is stated as to ensure the series is not a second-order linear recurrence over $\mathbb{Q}$, which is a requirement for the Number Field Sieve to work.) It is clear that if the series is of the form $\bigl(\begin{smallmatrix} ar^4&ar^3&ar^2\\\\ ar^3&ar^2&ar\\\\ ar^2&ar&a \end{smallmatrix} \bigr)$ with $ar^4 < N^{2/3}$, such that no modulo reduction happens, the determinant is zero by default. So in some sense the series "must exceed $N$ at some point". My guess is that if we remove all series without the modulo reduction then the remaining probability presumably will tell us that the geometric progression should exist. Is my argument sound? How does one go about counting the series to remove? 

cis.syr.edu/~wedu/Research/paper/duthesis.pdf My favorite protocol is the 2nd one. One variation of it would be: Alice breaks down u as $u_1 + \ldots + u_p$ and transmits set $( \{u_1^0, u_1^1\}, \{u_2^0, u_2^1\} \ldots, \{u_p^0, u_p^1\} )$ to Bob where $u_i^1$ are just a random vectors. Bob computes the dot product of each vector with a $v$ and adds random number $\epsilon_i$. Using the oblivious transfer protocol Alice gets back $(u_1^0.v +\epsilon_1, \ldots, u_p^0.v + \epsilon_p)$ and thus can compute $u.v + \sum \epsilon_i$. Bob can then divulge $\sum \epsilon_i$ and $u.v$ is known. I'm a little disappointed there isn't an answer that doesn't rely on modular arithmetic. I was hoping for a purely geometric protocol. 

The solution is the Poisson estimator. Let $a_i$ = $\log x_i$. We would like to approximate $\exp (\sum_{i=1}^n a_i) = \exp (n \hat{a})$ Draw $\kappa \sim \textrm{Poisson}(\lambda)$, then draw with replacement $a_{i_1}, \ldots, a_{i_\kappa}$ and compute $y = e^{\lambda}\prod_{j=1}^{\kappa} n a_{i_j}$ $$E(y) = \sum_{k=0}^{\infty} \frac{1}{k!} E\left(\prod_{j=1}^{k} n a_{i_j}\right) = e^{n\hat{a}}$$ 

Suppose I have a random variable $X_0$ with a p.d.f $f_0$ supported on the real interval $[a_0, b_0]$. $X_1$ is the restriction to $[a_1, b_1]$ of the sum $X_0 + g$, where $g$ is normally distributed $g \sim \mathcal{N}(0,1)$ $$f_1(y) = \frac{\int_{x=a_0}^{x=b_0} f_0(x) e^{-(y-x)^2/2}~dx }{\int_{x'=a_1}^{x'=b_1}\int_{x=a_0}^{x=b_0} f_0(x) e^{-(x'-x)^2/2}~dx~dx'}$$ or $$f_1 = \frac{1}{Z} L( f_0, (a_0,b_0,a_1,b_1))$$ Where $Z$ is a normalizing factor. $$Z = \int_{a_1}^{b_1}L( f_0,(a_0,b_0,a_1,b_1))(x')~dx'$$ $L$ is a linear operator over functions in $\mathcal{L}^2$ What are its eigenvectors? What happens when I replace the interval with a $n$ dimensional box and the normal distribution with a multivariate normal? Thanks! Clarification: I'm looking at f in $\mathcal{L}^2(\mathbf{R})$, not $\mathcal{L}^2([a_0,b_0])$ otherwise $L$ is obviously not an endomorphism. 

Define a sigmoid as any bounded, odd, increasing function from $\mathbb{R} \rightarrow \mathbb{R}$, and a pretty sigmoid as a sigmoid which is convex over $\mathbb{R^-}$ and concave over $\mathbb{R^+}.$ Let $P$ be the smallest set of functions from $\mathbb{R} \rightarrow \mathbb{R}$ containing polynomials and closed under exponentiation and product. If there a function $f$ in $P$ such that $\lim_{x\rightarrow -\infty} f(x) = -\infty$, $\lim_{x\rightarrow\infty} f(x) = 0$ and $e^f - \frac{1}{2}$ is a (pretty) sigmoid.