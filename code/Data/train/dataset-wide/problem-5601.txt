The question what makes certain models distinguished or less complex than others has certainly drawn some attention. However, I only know of mathematicians (like Kolmogorov) who tried to propose answers to these questions, but haven't even read their original papers. 

Many attempts to interpret quantum mechanics do so by looking at three nested systems. The largest system is essentially the universe or the environment. The smallest system is the one being observed and following the laws of the theory, and the middle system contains the measurement device or the observer. At least Schrödinger remarked in his famous article with the cat that properties of bosons and fermions might make this setup questionable (even if he wasn't very committal about whether bosons and fermions are really the last word), because they don't seem to allow for such a clear separation. But because already interpretation of probability theory can cause controversies, I wonder whether this same setup with three nested systems could also be used to interpret classical probability theory. And one might also try to interpret the classical static deterministic and causal theory with this setup, after all even the interpretation of causality might cause controversies. But why should one expect to learn anything at all about causal or probabilistic theories, by looking at three nested systems? Is this because we can see ourselves as being in the middle between the universe and what we observe, or has this setup something to do with quantum mechanics itself, or is this setup simply misguided? 

can be considered to be a valid answer to the question: Why did the chicken cross the road? I don't understand why Aristotle's final cause has become controversial in modern science. Of course, there may be more than a single valid answer to a "why" question, possibly subjective, or there may also be no valid answer at all. But claiming that those questions are meaningless seems to imply that there is no way to distinguish valid from invalid answers. Can the answer above be distinguished from an invalid answer like the one below? 

Extreme solipsism is not an interpretation of quantum mechanics at all. So even if extreme solipsism should turn out to be valid, it won't become an interpretation. The fact that it is compatible with quantum mechanics doesn't mean much either. Even if your favorite alternatives to it would not be compatible with quantum mechanics, it would just mean that either quantum mechanics or your favorite alternatives to extreme solipsism are questionable. 

It seems to me that for this semantics, the negation should exchange the degrees {⊥ , ⊤} and ∅. However, in the description of Dunn/Belnap's 4-valued system in the same article a conflicting definition of negation is given: 

Let's assume that in response to a question or problem, a certain type of idealized finite Turing machine can be presented as an answer as if it would really (=physically) exist. Is the corresponding ontological commitment from this assumption strong enough to prove certain (foundational) first order theories like Robinson arithmetic, WKL0, Peano arithmetic or ZFC consistent? Is it at least strong enough to prove the completeness of first order predicate logic in a reasonable sense? The type of idealized finite Turing machine I have in mind here is one with an infinite tape, empty except for a finite number of cells (the initial state of the tape), a finite number of internal states and a finite deterministic transition table. The machine can runs as long as is required to halt, or run forever if the HALT instruction is never encountered. This machine is idealized, because it can potentially run forever, and because the initial state and the transition table are allowed to contain an arbitrary finite (potentially unimaginable huge) amount of information. I wonder especially whether the assumption of the existence of such idealized Turing machines is stronger than the mere assumption of the existence of the potential infinite corresponding to the initial state of the tape. By stronger, I mean that more statements about first order predicate logic can be proved based on this assumption. 

Because I read the claim that intiutionistic logic is the logic of open subsets, let's describe a first-order semantics for a logic of open subsets, without equality and functions, but with many-sorted typing. 

The axiomatic argument is unclear about the situation where additional means for knowledge are postulated as axiomatic arguments. In my case I wondered about transfinite induction, but I think the problem is easier explained by one of Shakespeare's characters: The ghost of his death father appears to Hamlet and tells him that the new king is a murderer. Now even if Hamlet, by an act of faith, accepts that listening to the ghost is a way to acquire new certain knowledge, the knowledge itself will not be entirely based on the axiomatic argument, because also the fact that the ghost claimed it is important. However, Hamlet is only willing to believe the ghost, because he has other indications that the claims of the ghost may be true. So Hamlet also accepts the knowledge itself by an act of faith, and hence this example (and also my own example) is unable to disprove the axiomatic argument. 

I think you are intentionally misquoting the section about PAC learning of Aaronson's paper, in order to ask a question about that nicely written paper. The intention of the quoted section is not to prove Occam's razor, but to explain how Valiant's theory of PAC learning can help with clarifying the following questions regarding Occam's razor: 

I'm not sure whether "a solution" itself will have philosophical implications, and whether we can know anything about these implications without knowing the solution itself. If by solution, you just mean the knowledge that P is not equal NP, then there are some philosophical implication that we can know. At least some people have elaborated on these consequences, but let me just cite from Scott Aaronson's Why Philosophers Should Care About Computational Complexity: 

In On teaching mathematics, Arnold becomes even more explicit that mathematics has to solve real problems instead of just being consistent: 

The fact that you have to deal with issues of (non-)existence and (in-)validity might actually be an important feature of modal logic. For any discussion of "existence" in the context of classical single-sorted first-order logic somehow seems to fail to capture the meaning of the word "existence" as it applies to (what we perceive as) reality. The structure of reality is better captured by one actual world from which we talk about this actual world and the worlds reachable from this actual world, instead of talking about the single static single-sorted universe of classical logic. However, even this "Kripke structure" of modal logic described above doesn't fully capture the structure of reality, because we often also talk about the worlds from which the actual world is reachable, and about the "abstract" truths valid in any world. Note however the disclaimer in the question that I have not yet read a systematic introduction to modal logic, so some of my statements above might be inappropriate as a description of the current practice and interpretation of modal logic. However, I thought it would be appropriate to write down these thoughts as an answer to my own question, because they look at the problem from a significantly different perspective than I had initially in mind when I asked this question. 

A reasonable proof in ZFC would be to prove 1 + 1 = 2 for the corresponding ordinal numbers. The first few ordinal numbers in ZFC are 0:={}, 1:={0} and 2:={0, 1} with the order 0 < 1 on {0, 1}. The sum of two ordinal numbers is the disjunct union of the two well-ordered sets, with the concatenation of the well-orders as the well-order for the sum. For example, we would have {a, b} + {c, d} = {(a,0), (b,0), (c,1), (d,1)} with the order (a,0) < (b,0) < (c,1) < (d,1), if WLOG a < b on {a, b} and c < d on {c, d}. Note that the Kuratowski definition (x,y)={{x},{x,y}} is used here. So 1 + 1 = {(0,0), (0,1)} with the order (0,0) < (0,1). How can this be equal to 2 = {0, 1} with the order 0 < 1? Well, two ordinal numbers are equal if there exists an order isomorphism between them. It's easy to check that {((0,0),0), ((0,1),1)} is the required order isomorphism. This concludes my informal proof that 1+1=2 for ordinal numbers in ZFC. How difficult is it to convert such an informal proof into a formal proof? For me, the first difficulty would already be that I'm not sure in which form I should specify the order. I guess the correct way is to use a set of pairs, similar to how I specified the order isomorphism above. The formal proof for 1+1=2 from metamath uses cardinal numbers instead of ordinal numbers (as DBK indicated in a comment, that's also what Principia Mathematica did), but that seems to make the proof even more difficult. Note however that already formalizing and proving a simple formula like (a,b)=(c,d) -> (a=c ∧ b=d) in ZFC is quite some work. So maybe the informal proof given above is not so bad after all. 

A significant difference to a real living being is the time scale (and development over time) on which a pile of ants acts like a single intelligent being. However, the same question can also be asked for fungi, where it becomes a serious biological question. My biological knowledge is a bit too limited to give a definitive answer here. I believe that the spores, which can generate multiple (visible) mushrooms, count as a single intelligent being. And I think that they count as a single being, because they really are a single being from a biological perspective, whereas a ant colony is not. 

Those approaches fight with problems like their perceived triviality, with getting reinvented multiple times with minor variations under different names, with the "ugly" classifications you get in cases where no nice classification exists, and the non-triviality of appropriate efficient mathematical algorithms. But those approaches get many basic things right: 

Even if it is not directly related to the question, what is my own guess how understanding and meaning can arise during complicated computations? It might be related to the structure of space and time, where communication of information between different points is necessary during a computation (because the amount of information stored near any given point is finite) and takes a certain finite amount of time. The consequence is that compressed messages with more or less clear meaning in the context of the computation are exchanged and understood (and sometimes remembered for later reference) during the course of a computation. (And because the computation which our universe with its space and time structure seems to execute is unlikely to end anytime soon, there is no need to worry about the meaning and interpretation of its final output.) 

This description of emotions gives a hint of the nature of emotions, and why they are needed. Emotions are global states, which are normally communicated to all involved parts of the body by hormones. They are needed to get the different parts onto the same page. I wouldn't dismiss this global communication aspect as an implementation detail, because it is closely related to why emotions are needed in the first place. And emotions have meaning exactly because they are needed. Having meaning is one important aspect of emotions that should be kept by any proposed simulation of emotions. For me, a jet aircraft with global states like "normal on ground operation", "starting", "normal in air operation", and "landing" is much closer related to a true simulation of emotions than that artificial Mario AI described in the question. Those different global states of the aircraft are needed, because the engines and wings operate at different working-points there, and use different calibrations for control and feedback in those working-points. So the global states are needed and do have meaning, and they affect everything (each part of the plane) and not just the central processing unit. And because of the different calibrations, the thinking (or behavior) of the central processing unit in each of these states is also different. 

I recently tried to use the trilemma to better understand the limitations of full semantics of second order logic. I have now the opinion that the simplicity of the trilemma is treacherous, and it is not really clear what it says exactly. I will now try to explain how the axiomatic argument caused me some confusion, and why I think that the regressive argument is treacherous, unclear and needs further elaboration. 

This answer has become long and excessively cites external material. I initially wanted to answer this question, because I had an own opinion about set theory. Then I was afraid that my own opinion might not actually reflect the historical reality, so I tried to read at least a bit about the actual historical development. Then I had so much material that I struggled to turn it into a coherent answer, even if I accepted the fact that my initial opinion regarding set theory was completely irrelevant. So let me conclude by giving two quotes for V.I. Arnold, which at least partly reflect my own opinions about axiomatic theories when they lead to a separation between mathematics and the real world: 

The constants and variables of QML are rigid designators, and equality seems to be independent of the considered world. An obvious approach to add functions to QML would be as global functions on the domain of individuals, independent of the considered world. (I say obvious, because this is what I expect from functions for many-valued logic). In that case, function too would be rigid designators. What worries me about this approach is that the available language doesn't allow to state this fact. When I say "equality seems to be independent of the considered world", I mean that it is independent for the simplest model of QML, but the available axioms for equality don't allow to prove this. In fact, the available language doesn't even allow to state this fact. This leads me to suspect that functions can't be emulated by predicates/relations for that approach, unless appropriate modal operators allowing to talk about all worlds at once are added to the language. A failure to emulate functions by a language without functions might not be a show-stopper, but it would be sort of unexpected for me. Another approach to add functions would be to have different functions for each world acting on the domain of individuals. However, I have no idea about the advantages or disadvantages of this approach. It seems to me as if it would be possible to emulate this type of functions by predicates/relations. Functions of 0 arguments would now yield a second sort of constants which wouldn't be rigid designators. But these were already available before, simply by emulating functions via predicates. However, I'm less sure how to make sense of this approach in the context of many-valued logic.