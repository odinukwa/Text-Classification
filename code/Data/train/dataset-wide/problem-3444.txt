And that when there was clearly more than 197GB of data on the disk. So, we tried out the two methods mentioned above (chkdsk and recreating the disk) on two individual clean setups, and on each of those the weird behavior would no longer appear. Our best guess is that at some point, when the AMI was created, something went wrong in the snapshot process - most likely because we had taken a "snapshot without restart" (though we don't usually, and I have no evidence to back this up, so I hope our DevOps don't go mad at me for blaming her without cause!). All in all, an interesting experience. 

Running returns (62M), which could reach some OS limit; on a similar healthy system it's more like 1800000 (1.8M). The hugely occupied subfolder appears to be (~62M items compared to ~1.7M on the healthy system). This is likely just a reflection of my 100K fds (x2, for both fds and fdinfos) over 300 individual "task" folders. This appears at the end of my dmesg dump (my java pid in this example is 105940) - not sure how this might relate: 

Now, this didn't catch our attention before, because the disk still has plenty of space left. But it was the exact same disk usage (197G) on both setups, and that has no reason to happen. From here things quickly unfolded. As mentioned before, our AWS instances had been created from an image that has a disk snapshot of 200GB, which is extended on individual instances using - usually to the maximum size of 1TB. We were finally able to recreate a "bad state" by launching a new instance, resizing to 1TB, and creating a big file of 300GB. When this was done, the system didn't freeze, but it did show the same strange behavior: 

I am running into the following Linux error: . This seems to be caused by the system limit of 8 maximum link-chain size, and I'm looking for a way to increase this limit. Some background: I am writing a system which makes use of symlinks to pass on file resources between working elements. This results in long chains of symlinks (e.g. ). I am creating a chain intentionally, as I'm interested in preserving this structure of who-provided what. It should be noted that this system is physically disconnected from the outside world, so I have practically no concerns for security or exploit prevention. All help would be greatly appreciated! 

Longer answer: the disk file system (ext4) appears to have reached some unstable state when the disk's snapshot had originally been created. When later the original snapshot of 200GB had been extended (using ) to 1TB, it appears that in some sense it kept remembering internally the original size of 200GB, creating all sorts of weird phenomena which ended up with the OS unable to close handles, thus making the Tomcat reach its file limit, thus having all hell break loose. 

The question: I have a tomcat running a java application that occasionally accumulates socket handles and reaches the ulimit we configured (both soft and hard) for max-open-files, which is 100K. When this happens, the java appears to still be alive, but we can no longer access it. However my question is about a bizarre phenomenon that accompanies this situation: I cannot inside the tomcat folder. 

The browsers may be caching the DNS results. It might be worth trying the query directly to the configured DNS servers to see if they are the problem -- perhaps one of them is particularly slow or down. Take the IP address from the line in and run 

Of course, anyone who can elevate their account to root on computer B can defeat this protection, so you need to lock down NFS client computers sufficiently. 

You can configure ssh to run a command of your choice when you log in using public key authentication. To do this, generate a pair of keys: 

You want to replace the existing header (which may contain other recipients or the name of a mailing list) with the computed final destination of the email? You already have some code to figure out where the email has to end up, so use that: 

The parameter determines which domains Postfix relays to -- that is, the domains that your email server will accept mail for in addition to its mailname -- so this line isn't going to help you here (I assume this is what you are trying to achieve with it, anyway). You are restricting the MAIL FROM address to those listed in -- make sure that includes . You have fairly early in your line, and according to postconf(5), that rejects unless either 

Note that this is on by default. Any user connecting from computer C to computer B will be connecting as an ordinary user. The NFS connections forwarded through SSH as you described look like the originate from a process running on B as that user. This means that connection is subject to the usual security controls on B, in particular that normal users can't originate connections from ports lower than 1024. Tested on one of my systems, I see the following: 

You could add a second IP address to the system and point the DNS name at that new IP address. The server will still talk on both IPs, so none of your users will be disrupted, but you will know that anyone connecting on the old IP is not using DNS. 

I've always believed the administration advantage of SSH (I use push_check) outweighs any additional load. Modern CPUs are so fast that the cost of encrypting a handful of bytes is pretty minimal, so it comes down to running two processes (SSH and the check script) vs one (check script fired off by NRPE). For check scripts written in an interpreted language, I would expect the overhead of firing up the interpreter (Perl, Python, Bash) to exceed the CPU cost of starting an SSH session. Given modern CPUs, your machines are more likely to be disk or memory limited rather than CPU limited. Provided your Nagios machine is coping -- it has to set up 20 SSH connections every second -- I would err on the side of convenience. Not really an answer to your question, more of an argument that life is too short to worry about it :) 

The obvious solution here is to run x11vnc on the same display as Firefox so that you can at least see what Firefox is complaining about.