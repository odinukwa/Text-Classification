I have the same problem when I am listening to something challenging(i.e. not the undergraduate math club). For me, one of the most helpful things I can do is that once a definition is given, I construct both examples and constructions that almost fit the definition but do not fit one of the criteria. I then check how these work(or fail) with the theorems presented and the claims made. If you find a simple example and are having trouble following, the person lecturing will probably be happy explaining how the concepts presented relate to your example. Another idea that helps is for me to read all the material beforehand, no matter how confused I am. Then I can pay attention to how it is structured in class. Even if I do not understand the lecture, contrasting the structures of the reading and of the lecture can give me deeper understanding of the material. In addition, since I (like to think I ) understand how to structure a lecture, it gives me something I can understand to pay attention to. Lastly, I think it is important to be forgiving of oneself. Think of paying attention to a lecture like being focused when meditating. If you find your attention wandering, just refocus on the matter at hand. Try not to get caught up in a cycle of being frustrated that your attention wandered. 

I think it is better to have the seminar focused at too low a level than too high. When the material is at a low level, people may be bored, but at least everyone learns something. When it is at too high a level, people end up trying to fake knowledge, which is just a waste of time. I think it is hard for people to let their egos out of the way and really let this happen. You need a friendly and understanding atmosphere in order to let a seminar be at a level where everybody learns. One of the best ways to achieve this sort of camaraderie is to adjourn to a coffee shop or pub afterward where people can continue discussing what they learned that day. 

Let $X\subset \mathbb{R}^2$ be a polygon (possibly nonconvex, but not intersecting itself) with all the sides parallel to one of the axes. I am interested on whether $X$ can be tiled by (finitely many) squares. For example, if all the sides of $X$ have rational sides, then it can be tiled by squares. On the other hand, it is known that every rectangle that can be tiled by squares must has that its ratio of height/width is rational. My question is, what are the known obstacles for $X$ to have a tiling by squares? 

Viete formula gives $\sum_{k=0}^{d} t^k e_{d-k}\left(\alpha_0,\ldots,\alpha_{d-1}\right)=(t+\alpha_1)\ldots(t+\alpha_d).$ Therefore (substituing $t=-(x-d)^2$ and $\alpha_j=(x-j)^2$), LHS is equal to $\prod_{j=0}^{d-1} \left(-(x-d)^2+(x-j)^2\right)= \prod_{j=0}^{d-1} (2x-j-d)(d-j)=d!\prod_{k=d}^{2d-1} (2x-k)$ which is exactly the RHS. 

Let $Ab$ be the category of abelian groups, and let $F: Ab \to Ab$ be a covariant functor which is left-exact and limit-preserving. Is $F$ necessarily naturally equivalent to a functor of the form $\mathrm{Hom}(A,-)$ for some $A\in Ab$? 

It is known that the Dedekind-finite cardinals are closed under addition and multiplication, so one may do arithmetic in them, as opposed to only natural numbers. How much can those two arithmetics be different? For example, can there be a Diophantine equation which is not solvable in the natural numbers but solvable in the Dedekind-finite cardinals? Can there be two nonempty Dedekind finite sets $A,B$ such that $|A|^2=2|B|^2$? 

Let $Q_1, Q_2, R$ be quadratic froms over $\mathbb{Z}$ such that $Q_1 \oplus R \cong Q_2 \oplus R$ as quadratic forms. Is it necessary that $Q_1 \cong Q_2$? I know that by Witt's theorem it is true for fields. 

Let $X$ be a set, and let $F(X)$ be the free group generated by $X$. I will say that an element of $F(X)$ is positive if it is in the monoid generated by all the conjugates in $F(X)$ of every member of $X$. For example, if $X=\left\{a,b\right\}$ then $a, b, a^2 b a^{-1}$ are positive, but $aba^{-1}b^{-1}$ is not positive. Is there any algorithm that can detect, given an element of $F(X)$, whether is it positive? 

After some work, I have come up with the following answer. In the general case, the following holds for any two multivariate normal densities: \begin{array}{rcl} f(\mathbf{x}|\mathbf{0},\mathbf{\Sigma}_{1}) & \geq & f(\mathbf{x}|\mathbf{0},\mathbf{\Sigma}_{2}) \\ &\Updownarrow & \\ -\frac{1}{2} x^{T} \mathbf{P}_{1}x + \frac{1}{2} log [|\mathbf{P}_{1}|] & \geq & -\frac{1}{2} x^{T} \mathbf{P}_{2}x + \frac{1}{2} log [|\mathbf{P}_{2}|] \\ & \Updownarrow & \\ x^{T}[ \mathbf{P}_{1}-\mathbf{P}_{2} ]x & \leq & log [|\mathbf{P}_{1}|] - log [|\mathbf{P}_{2}|] \end{array} The set $A:=\{x| f(x|\mathbf{0},\mathbf{\Sigma_{1}})\geq f(x|\mathbf{0},\mathbf{\Sigma}_{2})\}$ is a compact and convex set if $\mathbf{P}_{1}-\mathbf{P}_{2}$ is a positive definite matrix as in special cases (i). Moreover, if $\mathbf{P}_{1} = k \times \mathbf{P}_{2}$ for $k>1$ then $ \begin{array}{rcl} f(x|\mathbf{0},\mathbf{\Sigma}_{1}) & \geq & f(x|\mathbf{0},\mathbf{\Sigma}_{2}) \\ &\Updownarrow & \\ x^{T} \mathbf{P}_{1} x & \leq & \frac{k}{k-1} n \times log [k ] \\ &\Updownarrow & \\ x^{T} \mathbf{P}_{2} x & \leq & \frac{1}{k-1} n \times log [k ]. \end{array} $ In A Generalized Error Function in n-dimensions, M. Brown defines an n-dimensional generalized error function $erf_{n}(.): \mathbf{R}_{+} \rightarrow \mathbf{R}$ by $ erf_{n}(x) = \frac{\int_{0}^{x} e^{-u^{2}} u^{n-1} d u }{\int_{0}^{\infty} e^{-u^{2}} u^{n-1} d u} $ Equation [47] in the paper concerns diagonal variance-covariance matrices and states that $Prob[\sum_{i=1}^{n} \frac{x_{i}^{2}}{\sigma_{i}^{2}} \leq \beta^{2}] = erf_{n}(\frac{\beta}{\sqrt{2}}). $ If this equations extends to the general case so that $ Prob[ x^{T} \Sigma^{-1} x \leq \beta^{2}] = erf_{n}(\frac{\beta}{\sqrt{2}}) $ then the total variation distance under special case (ii) can be expressed as $\begin{array}{rcl} ||f(.|\mathbf{0},\mathbf{\Sigma}_{1}) - f(.|\mathbf{0},\mathbf{\Sigma}_{2})||_{TV} & = & erf_{n}(\frac{\sqrt{ \frac{k}{k-1} n \times \ln [k ]}}{\sqrt{2}}) - erf_{n}(\frac{\sqrt{ \frac{1}{k-1} n \times \ln [k ] }}{\sqrt{2}}). \end{array} $ As per the Brown paper, the error function is given specifically by $ erf_{2m}(x) = 1 -e^{-x^{2}}[1 + \frac{x^{2}}{1!} +\frac{x^{4}}{2!} + \ldots+\frac{x^{2(m-1)}}{(m-1)!}] $ if $n$ is of even dimensions and by $erf_{2m+1}(x) = erf_{1}(x) -\frac{e^{-x^{2}}}{\sqrt{\pi}}[\frac{(2x)0!}{1!} + \frac{(2x)^{3}1!}{3!} + \ldots+\frac{(2x)^{2m-1}(m-1)!}{(2m-1)!}] $ if $n$ is of odd dimension. Considering the one-dimensional case and letting $k=(1+\epsilon)^{2}$, we get $ \begin{array}{rcl} ||f(.|\mathbf{0},\mathbf{\Sigma}_{1}) - f(.|\mathbf{0},\mathbf{\Sigma}_{2})||_{TV} & = & erf_{1}(\frac{\sqrt{ \frac{k}{k-1} \times \ln [k ]}}{\sqrt{2}}) - erf_{1}(\frac{\sqrt{ \frac{1}{k-1} \times \ln [k ] }}{\sqrt{2}}) \\ & = & erf_{1}(\frac{\sqrt{ \frac{(1+\epsilon)^{2}}{(1+\epsilon)^{2}-1} \times \ln [(1+\epsilon)^{2} ]}}{\sqrt{2}}) \\ & & - erf_{1}(\frac{\sqrt{ \frac{1}{(1+\epsilon)^{2}-1} \times \ln [(1+\epsilon)^{2} ] }}{\sqrt{2}}) \\ & = & erf_{1}(\frac{(1+\epsilon) \sqrt{ \frac{1}{\epsilon(2+\epsilon)} \times 2\times \ln [(1+\epsilon) ]}}{\sqrt{2}}) \\ & & - erf_{1}(\frac{\sqrt{ \frac{1}{\epsilon(2+\epsilon)} \times 2 \times \ln [(1+\epsilon) ] }}{\sqrt{2}}) \\ & = & erf_{1}(\frac{(1+\epsilon) \sqrt{ \ln (1+\epsilon) }}{\sqrt{\epsilon(2+\epsilon)}}) \\ & & - erf_{1}(\frac{ \sqrt{ \ln (1+\epsilon) }}{\sqrt{\epsilon(2+\epsilon)}}) \end{array} $ which corresponds to one of the answers to a univariate version of this question. The above can be partially extended to all of special case (i) to provide a bound on the total variation distance in terms of error functions. Define a function $g_{1}(.): A \rightarrow \mathbf{R}_{+}$ by $g_{1}(x)=x^{T}P_{1}x$. Let $b_{1} =\max_{x\in A} g_{1}(x)$. Since $A$ is a compact convex set and the function $g(.)$ is continous, this maximum is well defined under the present assumptions. We we can hence define a set $A_{1}^{*}:=\{x \in \mathbf{R}^{n}| x^{T}P_{1} x \leq b_{1}\}$. Clearly, $A \subseteq A_{1}^{*}$. Similarly, define a function $g_{2}(.): A \rightarrow \mathbf{R}_{+}$ by $g_{2}(x)=x^{T}P_{2}x$ and define a parameterized family of sets $A_{2}(.) : g_{2}(A) \rightarrow \mathbf{R}_{+}^{n}$ by $A_{2}(b)=\{x \in \mathbf{R}^{n}| x^{T}P_{2} x \leq b\}$. Let $A_{2}^{*}= \{x \in A|A_{2}(g_{2}(x)) \subseteq A)\}$ and $b_{2}=\max_{x\in A_{2}^{*}} g_{2}(x)$. This maximum exists under the present assumptions since the set $A_{2}^{*}$ can be shown to the non-empty, convex, and compact. Clearly, $A_{2}^{*} \subseteq A$. We can now provide a bound for the total variation distance in terms of error functions that generalizes the expressions for the total variation in special case (ii): $ \begin{array}{rcl} ||f(.|\mathbf{0},\mathbf{\Sigma}_{1}) - f(.|\mathbf{0},\mathbf{\Sigma}_{2})||_{TV} & = & \int_{x \in A} f(x|\mathbf{0},\mathbf{\Sigma}_{1}) dx -\int_{x \in A} f(x|\mathbf{0},\mathbf{\Sigma}_{2}) dx \\ & \leq & \int_{x \in A_{1}^{*}} f(x|\mathbf{0},\mathbf{\Sigma}_{1}) dx -\int_{x \in A_{2}^{*}} f(x|\mathbf{0},\mathbf{\Sigma}_{2}) dx \\ & = & erf_{n}(\frac{\sqrt{ b_{1}}}{\sqrt{2}}) - erf_{n}(\frac{\sqrt{b_{2} }}{\sqrt{2}}). \end{array} $ In special case (ii), $b_{1}=\frac{k}{k-1} n \times \ln [k]$ and $b_{2}=\frac{1}{k-1} n \times \ln [k]$. While not generally available in closed form solution, increasingly good estimates for $b_{1}$ and $b_{2}$ can be generated through repeated simulation from centered multivariate normal densities with precision matrices $P_{1}-P_{2}$. To generate estimates for $b_{1}$ and $b_{2}$, let $B=\{x^{(i)}\}_{i=1}^{L}$ be a set of (non-zero) random draws from a mean centered multivariate normal with precision matrix $P_{1}-P_{2}$. For each $x^{(i)} \in B$, define $\begin{array}{rcccccl} t(x^{(i)}) &= & \sqrt{\frac{log [|\mathbf{P}_{1}|] - log [|\mathbf{P}_{2}|]}{(x^{(i)})^{T}[ \mathbf{P}_{1}-\mathbf{P}_{2} ]x^{(i)} }} & , & \tilde{x}(x^{(i)}) & = & t(x^{(i)}) \times x^{(i)} \\ \tilde{b}_{1}(x^{(i)}) &= & ( \tilde{x}(x^{(i)}))^{T} P_{1}\tilde{x}(x^{(i)}) & , & \tilde{b}_{2}(x^{(i)}) & = & ( \tilde{x}(x^{(i)}))^{T} P_{2}\tilde{x}(x^{(i)}) \end{array}$ and estimate $b_{1}$ and $b_{2}$ by $\hat{b}_{1} = \max_{x^{(i)} \in B}\tilde{b}_{1}(x^{(i)})$ and $\hat{b}_{2} = \min_{x^{(i)} \in B} \tilde{b}_{2}(x^{(i)}) $ respectively. As the number of draws from the multivariate normal density increase, $E[\hat{b}_{1}] \rightarrow b_{1}$ and $E[\hat{b}_{2}] \rightarrow b_{2}$ where the former convergence is from the below and the latter from above.