There is simply no fallacy here, taking assertions to their logical extremes is a good test of whether or not there are premises missing from the remainder of the argument. If your argument does not hold on a desert island, or on a plane (or in a box or with a fox...) then there is some reason the ordinary case is the ordinary case, and it may very well be important that you find it. Omitted premises do make for false arguments, and they are a very common cause for two reasonable people not to see situations in the same light. There may be ideas in play that you have not recognized, which will sometimes provide a basis for a much more direct argument. 

From a Kuhnian point of view, the reliability of theory depends upon the system by which it is negotiated. Kuhn proposes that theories are based upon paradigms, which are simply slightly more general theories. But paradigms are agreed upon by communities, and get re-evaluated on a regular basis when a truly new idea cannot fit with the existing course of the theories currently in use. So the negotiation process by which the community dismantles old paradigms and vets new ones is the basis of the reliability of the theory. Science as system, then, is the producer of theories. But science itself is not a theory, it is a political process. Paradigms are judged by a broader philosophical construct that relies upon human intuition and negotiation to evaluate what is more likely to be productive and what is less likely. Of course, that political process is going to depend upon human experience, and that is related to practice. But the basis for negotiating paradigms is not the practice itself, only the experience of practicing. Otherwise we would never abandon an old paradigm no matter how hard it got to use it. Practice would always encourage the retention of the theory upon which it is based, and ad-hoc rejection of ideas that don't fit. But we have witnessed historically how paradigms shift. For instance, Galileo's argument that physics needs to have a single set of rules for near and far objects, for mundane and celestial observations, was a paradigm shift. It took away the foundation upon which astronomers to that time based their calculations, presuming the circularity of celestial orbits, but using different conic sections for flight paths near the ground. A critical mass of people needed to be convinced that this unified system was intuitively more likely to net better results in the long run, (without seeing what it was going to look like when it was finished) or folks would never have given up the precision of the existing system of epicycles that had been worked out over centuries before then (and would never have started working on theories based on Galileo's new assumption). We have some historical record of how this actual process happens, and a revival of the History of Science in the 1960's led to interesting theories about how things work when a science is reformulating its own basic theories. A very detailed and interesting account of Galileo's success is laid out in Feyerabend's "Against Method" (with strong emphasis on its political nature, and its lack of objectivity). And shorter accounts of many other paradigm renegotiations are recounted in Kuhn's "Structure of Scientific Revolutions" (with less political spin). 

These are just normal properties. The ones you are thinking of are abstractions that do not actually apply to anything. Even your examples don't work as context-free properties if you think them through: 

The question is whether my expectation is a real or ideal thing that exists, which is being indicated, or whether it is always a potential duty of yours, being recognize by me. If you allow for the kind of idealism upon which mathematics is based, all told, I think the category of 'non-indicative' itself does not make sense in anything but a linguistic sense. In a world with mathematical ideal forms that can be referenced as real things, everything is indicative, and my also be deontic, or otherwise subjunctive. 

I would say that things that cannot be changed do not necessarily have time. And this is not an antique or purely philosophical notion. Lucretius's notion of the unchanging atoms remains with us in elementary particle theories. Elementary particles in our current physics seem to lie outside of time. Richard Feynman has noted that the invariant nature of the electron means that we cannot know there are actually multiple electrons, rather than a single one that has travelled back and forth in time, moving backward most of the time when it is far away from us and forward when it is near us. These particles have quantum numbers to which we assign continuous movements, but we have come to accept that those movements are not coordinated in ordinary time the way that we imagined them for modelling purposes. The 'angular momentum' of a particle in its 720-degree 'rotations' is not a continuous temporal event that we can observe. Particles 'in phase' with one another are 'more real' to one another and more directly interfere. But is that an aspect of them, or of their incomplete presence to our observation? Really, this model for the varying degree to which particles can interfere with one another is more likely to be an aspect of the nature of their immersion in our space, than any actual motion that would be observed in the frame of reference of the particle itself. According to special relativity, photons, notoriously, have to have a frozen time frame, since they travel at the speed of light. But to the degree the Schroedinger equation is realistic, any particle can theoretically transit out of existence and move to another location in ways that violate relativity. Things that can violate the limit of the speed of light do not seem likely to be able to experience a time shared with us in any way. If they did, information 'available to them' in their own 'timeline' could be borne various places in ways that violate relativity. Instead, we must see not just photons, but all other things small enough to be captured by a single wave equation, as things that do not experience time, or that do so only in ways that cannot be shared with us. 

Induction is the move from data to rules. Obviously, we can observe correlations. But we cannot know what correlations constitute causes unless we compare them to one another. But on what basis would you compare them? You would need rules that indicate which kinds of correlation look causal and which kinds of correlation look derived or spurious. How would you determine those rules? It obviously involves observation. But that assumes you can get from observation to rules. The only way out of this loop is grounding in some basic assumption about cause and effect which does not originate in observation. Ultimately, in order to get traction, we have to appeal to our own emotions - some things just feel more basic than others, and we consider those more reliable and build from there. But even discerning the pattern in our own reactions is ultimately observation. So for individual reactions to build up into a rule, we are already making the assumption that one can get from data to rules. We have just pushed that assumption into unconsciousness. It is altogether possible, given this difficulty, that rules just are not the way the universe works, but are only the way our minds work, and the notion of causation itself nothing more than a useful illusion humans naturally share. 

In a space without value, why would one choose dialectic or synthesis as an approach? The choice of a procedure is attributing value to the perspective it fosters, which denies any true sense of nihilism. Adorno is simply being abstruse. Valuing deconstruction is holding a value. What we tend to call nihilism itself tends to be the attribution of value to freedom from constraint. Nietzsche is one of the first people folks tend to label nihilist. But he has a definite value system, to which he is so attached that he mocks the degree to which he ends up writing like a religious figure. Wittgenstein is another person whose thought gets labeled nihilistic, but language-games are repositories of meaning, and they represent an ingenious solution humans have made to being individualistic and social at the same time. Empiricism, Cynicism, Academicism and the other cousins of Stoicism are equally not nihilistic. Each has an attachment to a given value at its root, usually a variety of freedom, authenticity, or independence. As Sartre points out in his own defense, Existentialism is the ultimate Humanism. So yes, there is a point to denying obsessionality, seeking authenticity, honoring detachment, and there are synthetic approaches which incorporate those aims. But no, there is not a point to any synthetic approach to nihilism, because approaching nihilism implies a vector of approach, which is, in itself, not free of presumed value. This notion of a nihilism that still supports decisions about what is and is not reasonable procedure makes me think of an old New Yorker cartoon -- "We at St Aubrey adopt a totally neutral position on religion. If you say you are not Anglican, we whip you until you change your mind. But it is not a value judgement, it is simply a methodology." 

It seems clear that Nietzsche himself alternately used and questioned his own principles. To some extent, the genealogy of morals indicates largely the dangers of promulgating a single set of principles too broadly. A critical mass of people not submitting to them without question is a possible solution to this problem. Moral principles are, in effect, thoughts that control people. Submitting to any control that does not accord with one's self is not in accord with real honesty, as expressed in the will to power. Higher men should be honest, and express their deepest selves, to "make of the self a work of art". And acting in accord with one's self does not strictly require principles, although they can help one analyze oneself, and make 'sculptural' decisions, in the analogy of morality with art. To the degree that one has decided upon a decision as a way of shaping one's will for better effect, one can both be an agent of principle and create value. But other applications of principle to oneself should be considered problematic at best. 

One generally only does mathematics 'up to isomorphism', so that when you talk about the elements of the cyclic group on 12 elements you are really talking generically about tons of other things, including the groups of roots of all kinds of equations, the factor groups of all sorts of larger groups, various constructions in the surreal numbers... No one of these representations is 'more real' than any other. So the idea that you have to map the actors in some physical system onto the points in some constructed geometry in order to work with them mentally does not mean that they are actually being handled in the mathematics itself. From an intuitionistic point of view, the mathematics is all psychological idealization of the intuitions that allow us to communicate about the outside world. From that point of view, as an idealization, all of the abstract representations that are isomorphic are 'really' what the underlying mathematics is made of. Any of them is all of them. In that sense mathematics never handles real objects and 'impure' sets are an idealization of the notion of naming. So you are perfectly safe with the simplest model of space as a product of geometry-imbued continuua made of convergent sequences of ratios of enumerations of multiple copies of the empty set. Science is always using approximations and idealizing them. The electron, treated mathematically, is not a real thing, it is a collection of behaviors mapped to a point. From a lower perspective, the Schroedinger wave that gives the distribution of the apparent behavior over space is more real, and so on down the rabbit hole. The approximations involve potentially infinitely many named anchor points, but we have no problem creating infinite names since we have models of the reals and the integers. 

The flaw in the notion that sentience would reduce God to something less reliable is based in the notion that all of God would have to be equally sentient or non-sentient. But that is not realistic. There are two ways around it. 1) The theology of C.S. Lewis' Mere Christianity addresses this in his theory of the necessity of the incarnation. God has the necessity to relate perfectly to sentient beings. To some degree, though not nearly as absolutely as you propose, Lewis agrees that sentience is incompatible with omniscience. Sensation is a variety of learning, and what is to be learned? The inability to truly be changed obviates a lot of possibilities for God. Yet he must also have those possibilities, to be omnipotent. And have the experience of playing them out first-hand, to be truly omniscient. The experience of many mental states can only be known by living through them. (From a Christian angle, he should also relate immediately to those inferior beings who have them, especially if he is ultimately to be their perfectly fair judge.) Lewis therefore deduces it was necessary for God to fully incarnate himself at least once as each species of truly independent and moral intelligence (subject to final judgement.) Since temporality is an aspect of the notion of sentience, God would not need to always be sentient, but would only need perfect experience of having been so, and having faced the full range of sensory experiences. (So by Lewis's logic, Jesus needed to have extreme experiences like extended starvation, corporal punishment and a dramatic death, in addition to a lot of very positive experiences like the gratitude of the multitudes he fed, etc.) (Any Gnostic would then step in and say that God, while incarnated then also needed to experience being evil, or at least considering himself evil, to truly understand the intricacies of guilt. So this may lead many places Lewis does not intend.) 2) A simpler approach is pantheism or some weaker relative like "perfect immanence". Such an approach would immediately imply that while the whole of God is omniscient and thus not sentient, we as parts of him are sentient, and the whole can always access the parts. Then even though the whole of God is omniscient, the fact he has both unity and parts allows for the full range of experiences of incompleteness to be known by the perfect whole (including evil). 

If he channelling a philosopher, it is mostly Boltzmann, in his careful attempts to avoid Loschmidt's paradox. Boltzmann felt that we needed to presume that time flowed forward only because our local part of the universe had somehow fallen into a state of quite low entropy, which he saw as a kind of 'well'. Phenomenological time then followed entropic time because the brain uses exothermic chemical reactions to store information. (As Hawking reiterates.) In an attempt to placate or escape Loschmidt, he carefully presented his texts on thermodynamics in such a way as to allow for time to travel different directions in different parts of space. Two of the strongest framings of the Big Bang indicate that the low entropy comes from either the initial emptiness of space, or the confinement of motion when space was quite small. So the idea of an 'entropy well' is no longer important to folks like Hawking.