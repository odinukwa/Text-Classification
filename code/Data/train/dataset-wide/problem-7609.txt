You want to solve this problem $$ \min \{V(f,\Omega)\ : f\in BV(\Omega),\ f|_{\partial \Omega}\equiv J \} $$ and there is quite some background required to analyze this problem. First, one should clarify existence of solutions, then uniqueness, then one could also think about regularity of solutions and finally, analytical or numerical methods. First, you should look for solutions $f$ in the space $BV(\Omega)$ and not in $C^\infty(\Omega)$ (it may well be, that there is a solution that has additional regularity, but that's a question for a later stage). This may feel like making the problem more complicated than it is, but in fact the framework of $BV$-functions is indeed quite flexible and somehow, only $BV$ reveals what is really going on here. Then one would clarify in what sense a function $f\in BV(\Omega)$ has boundary values. In fact, there is a well defined trace operator taking $f\in BV(\Omega)$ to $f|_{\partial\Omega}\in L^1(\partial\Omega)$ (cf. Theorem 10.2.2 from "Variational Analysis in Sobolev ans BV spaces" by Attouch et al.). Then existence of solutions is clear from standard arguments along the lines of what is called "the direct method in the calculus of variations" (you know, minimizing sequences, weak cluster points, lower semi-continuity and so on). I should not that the problem you want solve is pretty famous in mathematical image processing and know as inpainting: You have some image $u_0$ defined on some domain $D\setminus\Omega$ (i.e. some domain $D$ and the part inside $\Omega$ is missing). You want to fill in the image $u_0$ in some meaningful way. One idea (and not the best) is to look for some $u$ defined on all of $D$ such that $u$ is equal to $u_0$ outside of $\Omega$ and has minimal total variation in $\Omega$ (that's equal to your problem with boundary values). A point to get started about algorithms is the IPOL-Article by Pascal Getreuer Total Variation Inpainting using Split Bregman. There you'll find code for a numerical solution and also some pointers to relevant literature. Regarding regularity of solutions (e.g. smooth solutions for smooth boundary values along a smooth curve), I am not sure and this may be difficult. I suspect that solutions may not be unique and that there are situations in which there are both smooth and nonsmooth solutions. Indeed in the one dimensional case there are multiple solutions for the boundary values $f(0)=0$ and $f(1)=1$: all increasing functions with these boundary values have total variation equal to one which is minimal. Note that there are smooth solutions but also nonsmooth ones (even ones that jump). It is not clear to me, if the situation in higher dimensions could be any better… 

and related papers. Sounds like something different, but from a technical viewpoint inverse problems and optimal control are very similar. In optimal control you interpret some parameter as a control that you want to choose optimally to achieve a specific goal, while in inverse problems you interpret it as an unknown quantity that you like to identify from given measurements. Technically you can turn an optimal control problem into an inverse problem by replacing "control" by "unknown quantity" and "goal" by "measurements" (and vice versa). 

I thought, I could turn the comments into an answer… The approach by couplings does not work without modifications and the reason is that couplings do not exist if the measures have different total mass: If $P$ and $Q$ are two measures on $X$ with different total masses which were coupled by $\mu$, then $$ \int_x\int_y d\mu(x,y) = \int_x dP(x) \neq \int_y d\tilde P(y) = \int_y\int_xd\mu(x,y). $$ However, for a given metric $d$ for probability measures one can build a metric for measures with different masses as follows: If $P$ has total mass $P(X) = p$ and $Q$ has total mass $Q(X) = q$, define $$ D(P,Q) = |p-q| + d(\tfrac{P}{p},\tfrac{Q}{q}) $$ (see Gromov's "Metric structures on Riemanian and Non-Riemannian Spaces", Chapter $3\tfrac12$.B). There are also other approaches to metrics on measure spaces such as the Kantorovich-Rubinstein norm $$ W(P,Q) = \sup\bigg\{\int f\, dP - \int f\, dQ\ :\ f\ \text{Lipschitz with constant}\ \leq 1\bigg\} $$ and others (cf. Villani's "Optimal Transport - Old and New", Chapter 6). 

(Picture from $URL$ but note that the obvious bilinear interpolation is not the minimal surface. There are formulae for the minimal surfaces such as the Weierstraß-Enneper formula but I haven't come across a formula for this particular case of a quadrilateral. In fact, I am not interested in a formula for the surface but only look for an answer to the question: 

Well, not a full answer, but in general a strictly convex function does not need to be strongly convex around its minimizer. An obvious example is $f(x) = x^4$ in the real axis. While this is "locally strongly convex" away from $x=0$, its "local modulus of strong convexity" decreases to zero for $x\to 0$. However, I am not sure if one can produce this situation in is your case of Bregman divergences but I would guess it could be possible. 

With all this, the problem is $$ \begin{array}{lrl} \sup\limits_{(\tilde f,g)\in L^2(\Omega)^{2n}} \langle \tilde f,g\rangle_{L^2(\Omega)^n} & 0&\leq \tilde f_i\leq p_i,\\ & 0&\leq g_i\leq 1,\\ & \tilde A\tilde f &= c,\\ & Bg &= c. \end{array} $$ The objective is not a convex function in $\tilde f$ and $g$ jointly but it's smooth. Hence, your problem classifies as a non-linear optimization problem with linear constraints. 

The function $\ell$ is concave (it's a minimum of linear functions). Its conjugate $\ell^*(\theta)$ is the supremum over a linear function minus $\ell$. If I see correctly, $\ell^*(\theta) = \infty$ for any $\theta$ and thus, $k$ is the supremum of the empty set, usually set to be $-\infty$. 

(For some reason I plotted the coefficients after double repermutation of the root on the left and the coefficients of the original polynomial on the right.) In other examples the difference was not that drastically but a pointwise error between the coefficients above 10% was seen frequently. 

To expand Robert Israel's comment: The class of log-nonexpansinve functions on $]0,\infty[$ is the image of the nonexpansive functions of $\mathbb{R}$ under conjugation with $\exp:\mathbb{R}\to ]0,\infty[$, that is, every log-nonexpansive function $h:]0,\infty[\to]0,\infty[$ is obtained by a nonexpansive function $g$ via $h = \exp\circ g\circ \exp^{-1}$. Since nonexpansive functions are central in optimization of real-valued functions, it seems natural, that an important class of functions for optimization of positive valued functions is obtained by conjugation with a bijection from $\mathbb{R}$ to $]0,\infty[$. I wonder, if other bijections from $\mathbb{R}$ to $]0,\infty[$ would also work in your context (although $\exp$ seems very well suited). 

Edit: A small clarification for point 2.: First, the submitted paper is not publicly available and hence, writing to the author would disclose the reviewers identity. Moreover, it is not about improving the actual paper (and wishing to become a coauthor) but about further work inspired by the paper. In view of the current answer, I would also like to expand the current question a little bit (which is not worth a whole new question, I think): 

Also a borderline suggestion since it is rather multilinear than just linear: Recent progress on low rank tensor approximation for all kinds of different applications within mathematics. A list of applications from this preprint includes 

That does not seem to be true. Here is how you can build a counterexample: Define $F(q) = \sum_i q_i^{p_i} - p_i^{p_i}$ and note that $F(p)=0$. To find $q$ such that $F(q)> 0$ try to set $\tilde q = p + t \nabla F(p) = p+tp^p$ (exponentiation applied componentwise) for some small $t$, and renormalize to get $q = \frac{\tilde q}{\sum_i \tilde q_i}$. I used a random $p$, $t=0.01$ and got 

The heat equation is the gradient flow of the energy in the Hilbert space $L^2$ (classical). The heat equation is the "gradient flow of the entropy in Wasserstein space $W_2$" (Jordan-Kinderlehrer-Otto, also Ambrosio-Gigli-Savare). 

Not a full answer, but some pointers on how to get a numerical method: If $\|\cdot\|_2$ denotes the spectral norm, then, by unitary invariance, your problem is equal to $$ \min_{T\in O(n)}\|AT-TB\|_2. $$ As such, the objective is the concatenation of the linear map $T\mapsto AT-TB$ and the norm and hence, convex. Subgradients of the objective can be computed with the help of 

While @Zander is right about the fact that "these functions cannot make a basis for the functions space" there are still a lot of families of translates of "bell shaped functions" which for a dense set in $L^2$. You may consider the paper "On aproximations by shifts of the Gaussian function" which treats exactly this problem. Indeed, one knows that for the Gaussians $\phi(x) = \exp(-\pi x^2)$ it holds that the span of the functions $\phi(x-\lambda_k)$ is dense in $L^2(\mathbb{R})$ if the shifts $\lambda_k$ fulfill that the sum $\sum_k \lambda_k^{-2}$ diverges - imho a quite surprising result... 

Here is my comment extended into an answer. In scale space theory one considers a family of mollifiers $\phi(x,t)$ where $t$ is a positive scale parameter. Under a number of axioms (see scale-space axioms), one of which is "non-creation of local extrema of $\phi(\cdot,t)\ast f$ if $t$ increases" and another is the semi-group property $\phi(\cdot,t)\ast\phi(\cdot,s) = \phi(\cdot,s+t)$, one can conclude that $\phi(x,t)$ has to be a Gaussian with variance $\sqrt{t}$. If one discards some axioms one obtains more possible functions. References are the classical "The structure of images" by Koenderink from 1984 or a paper by Weickert et al. on an older appearance of this concept (1959 by Iijima): "Linear scale space have first been proposed in Japan" from 1999. 

I know that eigenvalue estimates involving products of matrices are in general tricky, but probably this question has some hope: Let $A$ and $B$ be two real symmetric positive semi-definite $n\times n$ matrices (with $A+B$ positive definite if needed). Let $\lambda(X)$ denote any eigenvalue of $X$. I am fairly sure that $$ |1-\lambda\big( (I+A+B+AB)^{-1}(A+B)\big)| < 1 $$ but I would like ask: 

The Plancherel formula does almost all the work when you show that the continuous wavelet transform or the short-time Fourier transform are isometries (up to a constant). 

For sure the number is exponential in $n$ as is shown by $D = I$ with $2^n$ facets. But how is the scaling in $p$? A more computational variant of the question is: 

Certainly, there is not a standard method - and be aware that the calculation will be sensitive to noise. A straightforward of calculating the gradient would be: Take some point $x$ and choose a numbers of neigbors (the nearest $k$, say). Now fit a hyperplane through the $k+1$ points (e.g. by a least squares fit) and take the slope of this plane as the gradient in $x$. 

I guess the prime reference for questions like this is Bogachev's "Measure Theory I & II". Theorem 8.9.4 ii) states that the space of Baire measures is separable if $X$ is separable and the proof uses discrete measures for a dense set in $X$. 

In these posts and the reference given there, Dustin discussion issues of solvabilty and indeed, the squared system is often solvable even if not quadratically overdetermined. There are also some algorithms around, e.g DOLPHin, PhaseMax or PhaseLift. 

I doubt that there is a simple bound without some other restriction. Consider $f(x) = C|x|$, i.e. $\partial f(0) = [-C,C]$. Then there is a convex $g$ with $\|f-g\|_\infty\leq\epsilon$ but $g\equiv 0$ in a neighborhood of $0$, hence $\partial g(0) = \{0\}$. 

Not a full answer but too long for a comment. We rewrite the problem as follows: Define $A:\mathbb{R}^n\to L^1(0,1)$ by $Aa(t) = \sum_k a_k\sin(2\pi kt)$. Then the problem is $$ \min_a \|f-Aa\|_1 $$ with $f(t) = \sin(2\pi t)$ and this is a convex minimization problem in finite dimensions. A necessary and sufficient condition for $a^*$ to be optimal is $$ 0\in A^*\operatorname{Sign}(f-Aa^*) $$ where $\operatorname{Sign}$ is the multivalued sign, i.e. $$ \operatorname{Sign}(f(x)) = \begin{cases}\{1\} & f(x)>0\\ [-1,1] & f(x)=0\\\{-1\} & f(x)<0\end{cases} . $$ The adjoint operator $A^*:L^\infty(0,1) \to \mathbb{R}^n$ is $$ (A^*g)_k = \int_0^1 g(t)\sin(2\pi kt)dt. $$ In other words: $a^*$ is optimal if there is a $g\in L^\infty(0,1)$ such that i) $g(t) \in \operatorname{Sign}(f(t)-Aa^*(t))$ almost everwhere, and ii) $\int_0^1 g(t) \sin(2\pi kt)dt = 0$ for all $k$. I did not try to solve this for your actual $f$, though. In fact, approximation in $L^1$ is a well studied subject and if you search for "trigonometric approximation in L^1" you'll find the paper "Interpolation and L1-approximation by trigonometric polynomials and blending functions" which may be helpful. 

I think Carlo Beenakker digged up the right reference for the notation of the set, but I think more can be said. First, there is some meaning for the subscript $0$ which can be found in the same paper of Moreau a little later: 

You could dualize the $h$ to get a saddle point problem. To be specific: Write $h(x) = H(Ax)$ with $H(y) = I_{\cdot\leq b}(y)$ and write $H(Ax) = \sup_y (Ax)^Ty - H^*(y)$. The resulting saddle point problem (min over $x$ max over $y$) could be solved by several primal-dual methods, e.g. the one by Condat (similar to the one by Pock and myself) which uses a primal proximal-gradiet step and a dual prox step). 

For your specific question, I do not really understand, what you are after, but be aware that this question is probably the wrong one to ask, because you did not specify over which class of problems you want your bound to hold. Edit: I realized that my previous example was wrong - here is a one-dimensional example. To illustrate that: here is an example class where the simple gradient method (with steepest descent) performs better than Nesterov's accelerated method: Simply take the one-dimensional function $$ F(x) = \begin{cases} \tfrac{L+\mu}2 x^2 & |x|<\tfrac1L\\ |x| - \tfrac1{2L} + \tfrac{\mu}2x^2 & |x|\geq \tfrac1L \end{cases}. $$ Then $F$ is strongly convex with condition number $k = L/\mu$, and steepest descent always finds the solution $x=0$ in one step. Nesterov's accelerated method one works with specific stepsizes and, the larger $L$, the smaller the stepsizes. It always needs "infinitely many steps", i.e. never terminates in finitely many steps and convergence get smaller when $L$ gets larger. The crux is, that one method may perform well one some examples, while bad on others and the other way round for another method. Another example of this, that may be surprising: There is a convex function in two dimensions where gradient descent with steepest descent stepsizes, i.e. going along the neg-gradient until the function is smallest, does not even lead to a convergent sequence (but one with four accumulation points) while gradient descent with fixed stepsize does indeed converge (and also decreases the objective value faster in the long run). 

I do not know a definitive "weakest" condition and I doubt there is one. Many results in the realm of Hahn-Banach do the trick, i.e. there is the general result for $A,B$ convex and $A$ open (both open giving strict separation) and there is also Eidelheit's theorem saying that you can separate a point from a closed convex set (or a compact convex from a closed convex one). The latter one also holds for convex $A,B$ such that the interior of $A$ is non-empty and does not intersect $B$. 

I don't think so. Consider the rescaling $f_\lambda(x) = f(\lambda x)$ and $g_\lambda(x) = g(\lambda x)$. Then the term $\|f_\lambda\circ g_\lambda\|^2$ scales like $\lambda^7$ while $\|f_\lambda\|^2\|g_\lambda\|^2$ scales like $\lambda^6$. 

$\newcommand{\RR}{\mathbb{R}}\newcommand{\calF}{\mathcal{F}}\newcommand{\diam}{\mathrm{diam}}$ In geometric measure theory there are various notions of $m$-dimensional measure for sets $A\subset \RR^n$ for $m\leq n$ (some of them also for non-integer $m$, but this is not the point here). They all build on Carathéodory's general construction which works by covering $A$ with countably many sets $E_i$ from a specific base family $\calF$, measuring the sets $E_i$ with a function $\zeta$ and then building the infimum over all these coverings that are $\delta$-fine (i.e. $\zeta(E_i)\leq \delta$), and letting $\delta\to 0$. Among these specific measures are