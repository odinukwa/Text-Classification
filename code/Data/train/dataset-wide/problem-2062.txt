Basically the provider keeps connections open, even if the code that opened them closes them. The next time the provider needs a connection the already open one is used. These open connections are grouped into pools. A new pool is created for each combination of connection string and credential used to access SQL Server. Having two logins per second could mean that connection pooling isn't working correctly or it could mean that there are multiple connections using a different combination of connection strings and credentials. 

I found and played with this single sub select with no CTE. max_elapsed_time in query stats shows 1036 

You can then schedule this to run daily with a SQL Agent job or if you have SSRS you can easily schedule and format the report. If you go the SQL Agent route you will need to configure Database Mail and use that as your delivery service for the results of the query. 

I'd say you should have fields on the and so that it's possible to see when the status of the ticket changed. This would allow you to track changes from start to end. I would also recommend that use the same name for columns that are used to join tables. So the table should have a column that matches the column of the same name in the table. This is standard practice as it avoids a lot of confusion that can happen when writing and bug tracing queries. Those two points are my only comment as the general design seems to meet your requirements. 

The image in this article is used quite a bit around the internet in various different forms but it highlights which systems fall in each category of CAP So your are basically asking which horizontally scalable systems don't rely on eventual consistency. To which MongoDB, HBase and many more would be a good answer. 

I had to build something like this recently and I eneded up using an AlwaysOn Availability Group to create read-only replicas on the databases that I needed. If Enterprise is not an option then log shipping can do the job but it will put your datawarehouse further behind and your ETL would need to be able to deal with the restores to the log shipped replicas. From these replicas I had a custom ETL process that made heavy use of statements that ran every 5 minutes as a SQL Agent job to pump data new data into the datawarehouse. You could have this run at smaller intervals depending on the amount of data you have to process. If your records are datetime stamped then the only thing that should slow this process down over the months and years would be a big increase in transactions. All ETLs will suffer like this in some way if the transactions increase but there are many improvements that can made to scale-out the deployment. 

From my limited knowledge of how query plans are compiled, stored and retrieved by queries I understand that a multi statement query or stored procedure will generate it's query plan which will be stored in the query plan cache to be used by the query in future executions. I think this plan is retrieved from the query plan cache with the query hash, which means if the query is edited and executed the hash is different and a new plan is generated as no matching hash can be found in the query plan cache. My question is: If a user executes a statement that is one of the statements in the multi-statement query can it use that relevant part of the query plan already in the cache for the multi-statement query? I expect the answer is no because the hash values will obviously not match, but would it be better to hash each statement in a multi-statement query so they could be used by users running individual statements from the query? I expect there are complications that I'm not taking into account (And it's these that I really want to know about) but it seems like we could be storing the same 'statement plan' in many query plans taking up more space and taking more CPU and time to generate. Could just be showing my ignorance though. 

Linked servers aren't usually a good idea from a security perspective. See distribution statistics section here I think SSIS would provide a better way of doing this. 

The following two queries give index usage stats per table which should be a good indicator of which tables are causing the most IO operations. 

I was getting this error when attempting to backup a database with multiple file groups without specifically specifying each file group in the backup statement on SQL Server 2008. As soon as I added a FILEGROUP = N'FileGroupName' for each file group it worked. 

Instead of SSIS I think you might want to look into SQL Server Service Broker. This lets you send messages asynchronously. See here or here for a start. MSMQ or RabbitMQ may also be options you should consider. 

I would wait until the end of the day and then turn off auto shrink and never turn it back on. If you are low on disk space you will need to look into to getting some more and then turning auto shrink off. The reason for this is it creates a lot of fragmentation. See this to see how bad this can be. 

If you expand the SQL Server Agent in SSMS and double click the Job Activity Monitor you should see what you need. 

Right click the databases folder in the tree on the left in SSMS and select restore database. The restore task will remove the existing database and then create the restored database. Remember to select overwrite on tick box on the options tab. 

I don't think a cursor is good solution to this as it would have to be running 24 / 7 and you would need to check it was running. A better solution would be to have a SQL Agent job that runs at midnight each night. The Agent job would call a stored procedure that checks the flag, field and field for each employee and then increments the mentioned values for each employee. The stored procedure could send an email after each run to show you how many records were updated. It could also incorporate error handling to alert you if there was a failure for some reason. This way you can be sure the process is running correctly. 

I don't think is what you want here. I think you are looking for or . This will rank the rows your query produces starting at 1 up to the total amount of rows returned. The example below illustrates the different way and handle the ranking of rows with the same value. 

I'm not an expert when it comes to collations but the following does provide the sorting your require. It may cause other characters to be sorted unexpectedly so please test. 

As the data in a table changes the statistics objects that represent the distribution of data become outdated. This is a problem because it's the statistics objects that the optimiser uses to make it's estimates when building execution plans. If these estimates are inaccurate your performance will suffer. SQL Server does automatically update statistics objects when the amount of data in the table that has been changed hits certain thresholds, but if you have data that is changing frequently or tables with millions of rows you will better off performing regular index maintenance. This can be done with maintenance plans or scripts. Ola Hallengren's suite of scripts is very good for this and something that I use in all of my environments. There is also the Minionware suite that I have not used personally but I hear good things. 

I don't think that your schema is a bad way to track when issues are opened, closed, paid, etc. It wont track when people update or delete rows. To track these you could use a trigger. If you're on SQL Server Change Data Capture (CDC) is another option but it can be a little inflexible to work with. 

You haven't joined the users table so isn't available. But is in the other tables so you could use or . You will also need to add all columns in the to the other than . You may also want to review your schema design because having and in each table doesn't seem right.