Yes, this is possible. Most DDL statements can be rolled back in SQL Server (There are a few exceptions such as ) 

From looking in the transaction log I noticed that the initial inserts against empty local temporary tables seem even more minimally logged (at 96 MB). Notably these faster inserts had only transactions ( / pairs) compared with over in the slower cases. In particular operations seem much reduced. The slower cases have a transaction log entry for this for each page in the table (about ) compared with only such entries in the fast case. The log used in all three cases was as follows (I have deleted the log records for updates to system base tables to reduce the amount of text but they are still included in the totals) Logging first insert against (96.5 MB) 

Yes. The query in the question would in principal be able to do this more efficiently than the obvious alternative of just doing . For the same reason as is generally preferable to . It can short circuit. The underneath the aggregate operator can stop requesting rows as soon as the second one is received rather than requesting more and causing additional irrelevant rows to be read. 

However the estimated rows depend on which parameter value is passed when the plan is compiled. If had been passed the estimated number of rows in the final operator is . In this simple plan the fact that 218 rows can be emitted for a subtree with estimated 1.37 rows probably won't make much difference but it could do if this was part of a larger query. And even in this simple example it could mean memory grants for the final hash join are incorrect. Logically an outer join can't reduce the number of rows but for some reason the initial 3 gets scaled down to 1.37074 after passing through the two other outer joins here. If somewhat accurate cardinality estimates are required then does solve this issue - the cardinality estimates are either or dependant on the nullability of the parameter. But would be preferable. As well as being targeted at the specific statement it also allows additional simplifications to be made. With in place the execution plan when calling with is as below. 

The below shows an example along with a calculated column showing how the is calculated. As the highest is when ordered descending and there are 7 rows. The is = = 

You need to mark your stored procedure as a system object to get the behaviour that you want (full example below tested as working on 2008 SP3) 

Just to add to @MartinC's answer that the row count for table variables is maintained in and can cause this to be used but it doesn't have any more granular statistics to use so will need to fall back on guesses based on that. You have only shown the population code rather than any code that uses it. Another limitation of queries that insert to table variables is that they cannot have a parallel plan so that could explain why the population query might run faster. 

What is the datatype of itemTimestamp? Is that column indexed? Is the column nullable? Assuming the answers are , Yes, and No respectively you might consider 

Notice that was not touched in the query execution. The reason for this is that the seek on this table is under a filter with a start up expression predicate of . As this condition is not met the seek is never actually executed. Both the Startup Expression and number of executions are shown in the operator tool tips as below. 

You are performing an index rebuild on Standard edition. As documented here you would need Enterprise or Developer edition to see parallelism for this. 

you may be able to encourage the intermediate materialisation without creating a table explicitly by changing the definition of to 

Which appears to show that for large inserts out performs . It doesn't test cache size 1,000 however and also those results are just one test. Looking specifically at cache size 1,000 with various batch sizes of inserts I got the following results (trying each batch size 50 times and aggregating the results as below- all times in Î¼s.) 

They are semantically identical and the optimiser should have no trouble recognising this fact and generating identical plans. I tend to put conditions referencing both tables in the and conditions referencing just one table in the . For moving the conditions can affect the semantics however. 

This adds a few leading () and trailing () characters but otherwise leaves the data intact and without characters being replaced by XML entities. 

And that your existing code that references the table just performs basic DQL and DML commands against the table (i.e. /,,,) And that isn't referenced by a foreign key anywhere. Then possibly the best way of getting up and running quickly (if you can't afford the downtime of rebuilding the whole table in one go) would be to rename that table (e.g. as ) and create a new table 

The test was carried out in a database with full recovery model against a 1,000 row table with one row per page. The table consumes 1,004 pages in total due to the root index page and 3 intermediate level index pages. 8 of these pages are single page allocations in mixed extents with the remainder distributed across 125 Uniform Extents. The 8 single page de-allocations show up as the 8 log entries. The 125 extent deallocations as . Both of these operations also require an update to the associated page hence the combined 133 entries. Then when the table is actually dropped the metadata about it needs to be removed from various system tables hence the 22 system table log entries (accounted for as below) 

Yes. At least in current versions of the product. SQL Server will not pick apart the statement and reverse engineer it to discover that if the result of the computed column is then must be . You need to make sure that you write your predicates to be sargable. Which almost always involves it being in the form. . Even minor deviations break sargability. 

The operation which has the potential to cause lots of logging is the of all rows in the table however that does not mean that this will always occur. If the "before" and "after" images of the row are identical then this will be treated as a non updating update and not be logged from my testing so far. So the explanation as to why you are getting lots of logging will depend upon why exactly the "before" and "after" versions of the row are not the same. For variable length columns stored in the format I found that setting to always causes a change in the row that needs to be logged. The column count and the variable length column count both are incremented and the new column is added to the end of the variable length section duplicating the data. is fixed length however and for fixed length columns stored in the format the old and new columns both seem to be given the same slot in the fixed length data portion of the row and as they both have the same length and value the "before" and "after" versions of the row are the same. This can be seen in @Aaron's answer. Both versions of the row before and after the are 

At least in the test I did the differences between the two balanced out fragmentation wise (though similarly to your test I did find that rebuilding the index online led to a higher page count.). I found that the version always used uniform extents and had zero single page allocations whereas the always seemed to put the index root page and first index leaf page in mixed extents. Putting the first index leaf page in a mixed extent and the rest in contiguous uniform extents causes a fragment count of 2. The version avoids the fragment caused by the lone index leaf page but the contiguity of the leaf pages is broken by the index root page that shares the same extents and this too has a fragment count of 2. I was running my test on a newly created database with 1 GB of free space and no concurrent activity. Perhaps the version is more vulnerable to concurrent allocations causing it to be given non contiguous uniform extents.