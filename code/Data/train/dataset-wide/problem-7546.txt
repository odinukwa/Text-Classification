I agree that sometimes authors present a concept simply because it's a standard example in the subject, but then spend a single page on it and just move on to other things. One example that comes to mind is a particular text on undergraduate real analysis which introduced Fourier Series in a few pages and then had a single sloppy exercise related to applications to PDEs. I'm not saying the book should have dedicated a chapter to PDEs, but one ugly exercise seems like a travesty and makes you scratch your head about why you're wasting your time on this stuff. I don't expect incredibly motivated concepts in graduate texts on the same subject simply because by then I should have already been motivated enough to study onwards. However, motivation for what you're doing is one of those dangerous phrases in mathematics. For the more difficult and abstract stuff out there, it's not always straightforward to communicate the direct usefulness of an idea. Just because I tell you a result is incredibly useful in say, the sciences, does that make all the difference? When I learned the Radon-Nikodym theorem in real analysis, I could not for the life of me see a genuinely useful application of it, until I came to the formal definition of conditional expectation in probability. In short, the proof of existence and uniqueness of conditional expectation is by the abstract nonsense argument of the Radon-Nikodym theorem. I certainly think it would have been quite nice if somebody told me in my real analysis class why we were learning the Radon-Nikodym theorem, but at the same time I don't think I would have been ready to learn the substantial amount of probability to really understand what the heck the formal definition of conditional expectation is (let alone why it's useful!). In the end, you're going to need to find a textbook which suites your needs. Each person has their own style for absorbing the material they need. Some people love the straightforward definition - theorem - proof approach while others like to see a section on "applications" after every idea presented (I personally fall into the latter category). If you want to learn the nitty-gritty version of complex analysis, you pick up Complex Analysis by Ahlfors. If you want to learn complex analysis from an engineering point of view, you pick up Complex Analysis For Engineers. It's up to you which applications you want to see, so supplement your knowledge accordingly. Plus, much of the time I don't come to appreciate a textbook until I've read it all the way through. If you're curious about "applications" of what you're learning, try going ahead 20-30 pages, and hopefully the author will have started subjects which apply what you have learned. 

If you want something more on the expository side, "An Invitation to Modern Number Theory" by Miller and Takloo-Bighash builds up both L-functions and random matrices from the ground up, later connecting the two. 

Or if your random variables are nonnegative and nicely padded away from zero, you can take logorithms and come back to standard convolution. 

where the edges and vertices are labeled by the structure of the $L$ matrix there. My question is, what are the possible ways of giving this hexagon a sense of distance (thus being able to draw Toda flow trajectories). Essentially this should somehow depend on the relative size of $a_i,b_i$'s, how close $a_i$ is to a particular eigenvalue, etc. For example, the $b_i$ are all exponentially decreasing, so it seems like one needs to look at ratios of $b_i$ to conclude which side of the hexagon we are closer to. This still seems very arbitrary and results in singularities at the boundary (which in higher dimensions become an issue since Toda flows perfectly well cross over such boundaries). I know that the $n$ dimensional Toda flow is diffeomorphic to the $n-1$ sphere via looking at the vector $(u_{11},u_{12},\ldots,u_{1n})$ of normalized first-components of the corresponding eigenvectors of $L$. Is it as simple as projecting these onto the embedded permutahedron? Here's the picture for $n=4$, 

My thoughts on both these questions are limited to thinking that perhaps there's a way to apply (modify?) the Novelli-Pak-Stoyanovskii shuffling algorithm to go from $q$ to $t$. Specifically, the algorithm presents a bijection for the identity: $$|\lambda|!=f_{\lambda}\prod_{(i,j)\in\lambda}h(i,j)$$ via $S_{|\lambda|} \rightarrow (T_{\lambda},H)$, where $S_{|\lambda|}$ is the space of permutations of length $|\lambda|$ and $H$ is a complementary space of certain multiplicities involved during the algorithm. So maybe the right thing to look at is: $$|\lambda|!\cdot f_{\mu\cup\lambda}=(|\lambda+\mu)!\cdot f_{\lambda}\cdot \frac{1}{h(1,j)}?$$ To use the algorithm for a shape $\lambda$, one picks a uniformly random permutation of size $|\lambda|$ and then applies a kind of bubble-sorting algorithm to the result. It's clear to me that if one looks at $\lambda\cup\mu$ instead, then the only difference is when this bubble sorting is working with entries on the first row $\mu$. Unfortunately, I can't quite see how to make sense of this because somehow one needs to keep track of what's going on in row $\mu$. Is there a better way? 

Writing up the comment: You just need to "pixelate" the line by finding all lattice boxes that it crosses: Then the answer vector $v$ must connect to one of the corners of the shaded boxes. Instead of first collecting the boxes, try the following zig-zag algorithm: start from the origin and poll the up and right nearest lattice points. One of them is closer to your vector $u$, so pick that one. Again, looking up and right, pick the closer vertex. Repeating this procedure will give you a list of distances for each point reached. Pick the minimum distance and that's your answer. This algorithm is $O(Mn)$, and perhaps can be sped up by someone more clever than me. 

I would recommend reading through Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering by Steven Strogatz. He does a great job of motivating the applications of the field to various branches of science with a plethora of exercises. I wouldn't say it's entirely rigorous at times, but it covers a huge amount of material which should give you plenty to think about if you want to do a research topic. Out of all the books I've seen on the subject, his is probably the only elementary one that tackles renormalization techniques and universality, involving Feigenbaum's constant. 

Riemann's work with curved spaces, particularly their applications to physics was at least 40 years ahead of it's time. He pushed forward ideas that space is perhaps curved and that forces such as gravity could be thought of as bending in space. He gave a few lectures on these ideas, but fellow mathematicians and physicists didn't really know what good could come of them and didn't pay much attention. Of course, Einstein finally solved the puzzle many years later. Also, Joseph Fourier was laughed at when he proposed the notion of Fourier series for solving the heat equation, particularly at the lack of rigor and the overall scope of it's applications. Opinions on the matter changed a decade or two later when the theory began to root itself in rigor thanks to Dirichlet. 

It sounds like the functions you're dealing with are pretty nice and admit Laplace transforms. So we know that for two functions $f$ and $g$, the Laplace transform of their "convolution" (as written below) is: $\mathcal{L}(f\star g)=\mathcal{L}\int_0^t f(t-x)g(x)dx=F(s)G(s)$ where $F$ and $G$ are the Laplace transforms of $f,g$ respectively. Look at the proof in this link here. For $f(2t-x)$, following the notation in the link and working bottom up, the only difference that occurs is that instead of $t=\sigma+\tau$, you replace with $2t=\sigma+\tau$. This gives $\mathcal{L}\int_0^t f(2t-x)g(x)dx=\frac{1}{2}\int_0^\infty \int_0^\infty f(\sigma)e^{-s(\sigma+\tau)/2}d\sigma g(\tau)d\tau=\frac{1}{2}F(s/2) G(s/2)$ So for your equation, you get: $H(s)=\frac{1}{2}F(s/2)G(s/2)$ and you can now solve for $g$ by using the Laplace transform inverse. As far as references go, try "Introduction to integral equations with applications" By Abdul J. Jerri. 

I remember from Folland's PDE book an anecdote about Green convincing himself of the existence of a Green's Function: Let $\Omega$ be a vacuum and $S$ a perfectly conducting shell grounded to zero potential. Place a negative charge at $x\in \Omega$. This induces a positive charge on the shell $S$. Indeed, the Green's Function $G(x,y)$ is the induced charge at a point $y$. I would also add that the physical interpretations of gradients, divergence and curl are indespensible for REMEMBERING the various theorems of Gauss, Stokes and Green. For example, the curl of a velocity field is twice the angular acceleration, a fact that facilitates the order of differences of partials in the curl operator. 

How about the classic 20 prisoner problem. A warden tells 20 prisoners that they can live if they survive the following game: tomorrow the warden will line up the prisoners single file (at random), facing forward and place either a red or blue hat on each prisoner. Each prisoner can only see the color of hats infront of him. The warden starts from the back of the line and asks each prisoner in turn what color hat they think they are wearing. If a prisoner answers correctly they live, otherwise they die. No communication is allowed between prisoners once the game starts and they can only shout "red" or "blue" when it's their turn. The prisoners have the evening to discuss a strategy.What is the best strategy for the prisoners? How many prisoners can be guaranteed to live. 

If you are interested in applications of fixed point theorems, you'll find entire journals dedicated to them. For example, you see fixed point techniques pop up in approximation theory where one is interested in finding the best approximation. For a specific problem, consider the following: You decide to take a break from the fast paced world of academia to climb Mt. Fuji. You begin your ascent at sunrise along a narrow path. Along the way, you stop a few times to take in the scenery and eat, maybe even work out a math puzzle or two. You reach the top at sunset. The next day you begin your descent at sunrise, again making leisurely stops along the way. It's reasonable to assume that going downhill is easier than uphill so let's assume your average downhill speed is greater than your average uphill speed. Show that there must be a place along the path that you occupy at the exact same time of day during your uphill and downhill trips. 

My vote goes for calculus and in particular the Fundamental Theorem of Calculus (FTC) and Stirling's approximation for the factorial. Can you imagine doing basic mathematics in any scientific field without FTC? What about quick and dirty approximations in physics without Stirling's formula? Perhaps modern science would have gotten to it's current level without the help of FTC or Stirling, but I bet it would have happened a thousand years too late! 

Let $\mathcal{F}$ be a sigma algebra over $\Omega$ and $M$ the set of all probability measures on $\mathcal{F}$. Let $\mathcal{C}$ be some collection of pairs $(A,B)$ with $ \ A,B\in\mathcal{F}$. Now I can declare the pairs in $\mathcal{C}$ independent by defining $M(\mathcal{C})$ to be the set of probability measures which satisfy my independence constraints, so that $P\in M(\mathcal{C})$ iff $P(A\cap B)=P(A)P(B)$ for every $(A,B)\in \mathcal{C}$. Note that it's probably enough for $\mathcal{F}$ to be a field by invoking Caratheodory extension.