Your questions is closely related to the concept of so-called well-distribution of sequences. A sequence $(x_n)_{n \geq 1} \in [0,1]$ is called well-distributed if $$ \lim_{n \to \infty} \frac{1}{N} \Big\{ 1 \leq k \leq n:~x_{m+k} \in [a,b] \Big\} = b-a $$ holds uniformly in $m$. Asaf's comment says that the Kronecker sequences is not only equidistributed, but even well-distributed. Many other results can also be carried over from uniform distribution theory. See for example: B. Lawton, A note on well distributed sequences. Proc. Amer. Math. Soc. 10, 1959, 891–893. He proves a variant of van der Corput's difference theorem, and of Weyl's theorem on the equidistribution of polynomial sequences. 

In metric Diophantine approximation you are often interested in finding conditions on $(\phi(q))_{q \geq 1}$ which guarantee that $$ \left| \alpha - \frac{p}{q} \right| < \frac{\phi(q)}{q} $$ has infinitely many integer solutions p,q for almost all $\alpha$. (Often you also assume that $p$ and $q$ must be coprime, but this is not the point in my question). A problem of this type is for example the famous (unsolved) Duffin-Schaeffer conjecture, see $URL$ The problem can also be written in the following form: Let $A_1, A_2, \dots$ be intervals on the torus (of length $\leq 1$), which are symmetric around 0. Let $\psi_1, \psi_2, \dots$ denote the Lebesgue measure (that is, the length) of these intervals. Under which conditions on $\psi_1, \psi_2, \dots$ do we have $$ \sum_{n=1}^\infty \mathbf{1}_{A_n} (n \alpha) = \infty $$ for almost all $\alpha$ (here $\mathbf{1}_A$ is the indicator function of $A$. Also, everything is understood to take place on the torus, so the indicators are extended with period 1.) Now you may generalize the situation in a first step to the case when the intervals $A_1, A_2, \dots$ are not necessarily symmetric around $0$. Then you get a problem in inhomogeneous Diophantine approximation. This type of question is also quite well-investigated. Now my question is the following: what happens is if don't assume that $A_1, A_2, \dots$ are intervals, but if they may denote any measurable sets in $[0,1]$. Again, write $\psi_1, \psi_2, \dots$ for the measure of these sets. Under which conditions on $\psi_1, \psi_2, \dots$ do we have $$ \sum_{n=1}^\infty \mathbf{1}_{A_n} (n \alpha) = \infty $$ for almost all $\alpha$? Are there any sufficient conditions? (Note that a necessary condition is the divergence of the sum of the measures, by Borel-Cantelli). Does it help if the sequence $(\psi_n)_{n \geq 1}$ is monotonic? Are there any known results at all about this general problem? (Remark: This question is distantly related to Khintchin's conjecture, which was disproved by Marstrand in 1970. See $URL$ (Remark 2: The sequence $(n \alpha)_{n \geq 1}$ mod 1 has an interpretation in terms of ergodic theory, but I don't think that this will help here.) 

What is a good upper bound for the maximal cardinality of $\mathcal{A}$? In words, what I am looking for is a version of Sauer's lemma under the additional assumption that not all possible sets obtained by intersection are counted, but only those which are "separated" in the sense of the symmetric difference being not too small. (Sauer's lemma is the special case $m=1$.) The application which I have in mind requires $m \approx \varepsilon n$ for some small $\varepsilon$, but I think it is in general an interesting question. 

Is there anyway to obtain the Fourier Power Spectral Density from a [wavelet transform][1] of a time series? I am particularly interested in this problem because I was wondering if there is any possibility to obtain the local Power Spectral Density from the wavelet transform. By local I mean to obtain the Power Spectral Density as a function of the time. If I am not wrong, according to Torrence and Compo, the average of all the local wavelet spectra tends to approach the Fourier Spectrum of the time series. However, I tried some numerical tests for the signal: $$x(t)=cos(t*2 \pi/10)+cos(t*2 \pi/5) $$ I performed the wavelet transform using different python packages (pywavelets and others) and also I wrote a code myself. With all the different packages I get similar results; the main frequencies seems to be the same in the Fourier Spectrum and in the averaged wavelet spectrum, but the bandwidth of the two frequencies is much widther for the wavelets average spectrum. I computed the Wavelet Spectra by: $$ |W_x(s)|^2 $$ where $s$ are the different scales. To rephrase the question again, I would like to know if I can approximate the Fourier Spectra with the wavelet transform without this widthness "error". And again: I am particularly interested in this problem because I was wondering if there is any possibility to obtain the local Power Spectral Density from the wavelet transform. Thank you very much. 

I am trying to compute the First passage time for the stochastic process $ \begin{equation} dx = \mu dt + y(t) dt \end{equation} $ where $y(t)$ is the Ornstein–Uhlenbeck process with zero mean. Ideally, I would like to compute the First Passage Time for the process $X$, across the boundary $x=\tilde{x}$ whith $x(0)=0$ in a general case when the OU process is not stationary and stills remembers the initial condition $y_0=y(0)$. In this case the statistics of $Y$ are $E(y)=y_0 e^{-\theta t}$ and $Cov(y(s), y(t))=\sigma^2/(2 \theta) (1-e^{-2\theta t})e^{-\theta(s-t)}$ when $t<s$. However I would be more than satisfied if I could compute the FPT when the process is stationary and has the following statistics: $E(y)=0$, and $Cov(y(s), y(t))=\sigma^2/(2 \theta) e^{-\theta|t-s|}$ . Assuming that $X$ is normally distributed I know how to compute the mean and the covariance, $E(x) = x_0 + \mu t + \frac{y_0}{\theta} (1-e^{-t\theta}) $ and $Cov(x(s),x(t))=-\frac{\sigma ^2 e^{\alpha (-(s+t))} \left(-2 e^{\alpha s}+(2-2 \alpha t) e^{\theta (s+t)}-2 e^{\theta t}+e^{2 \theta t}+1\right)}{2 \alpha ^3}$, and using this results I can compute the pdf for $X$. According to some numerical simulations that I did these results seems to be fine. My first approach was trying to use the images method following Molini 2011, but then I realized that this process does not satisfies the requirements to use that method, so really I have not much idea how to continue and I am kind of lost. I would really appreciate any kind of help or hints. 

I need to compute the first passage time for a stochastic process which has the following statistics: $ \begin{eqnarray} E(x(t))&=&x_0 + \mu t \\ Var(x(t))&=& a + \sigma^2t \end{eqnarray} $ It's like a drifted diffusion process but also the variance has an additional constant term. I know how to compute the first passage time density for the this process but when $a=0$, and It is given by $ \begin{equation} FPT(t)= \frac{\tilde{x}-x_0}{\sqrt{2\pi \sigma^2 t^3}}e^\frac{-(\tilde{x}-x_0-\mu t)^2}{2\sigma^2 t} \end{equation} $ Is there any transformation, change of variables, or something that I can use to compute the first passage time at the barrier $x=\tilde{x}$ knowing the result for the simpler case? Or if not, is there anyway to compute the FPT density for this problem. Thank's a lot. 

It is well-known that the Riemann zeta function can be approximated in the critical strip by a finite Dirichlet polynomial. More precisely, $$ \zeta(\sigma+it) = \sum_{n \leq X} n^{-\sigma-it} + \frac{X^{1-\sigma-it}}{\sigma+it-1} + O ( X^{-\sigma}) $$ uniformly for all $X$ satisfying $2 \pi X / C \geq |t|$, where $C$ is a fixed constant greater than 1. (This is Theorem 4.11 in Titchmarsh's book). In particular this holds for $\sigma = 1/2$. Question: is there a similar (simple) approximation of zeta by a finite Euler product? 

Is there a sort of distributional estimate in Diophantine approximation which allows to estimate the number of solutions which provide a certain quality of approximation? For example, how large is the measure of the numbers $x \in [0,1]$ for which $$ \# \Big\{ 1 \leq n \leq N: \|n x\| \leq 1/N \Big\} \geq \log N? $$ (Here, of course, the $\log N$ on the right-hand side could be any other term as well, say $(\log N)^{1/2}$ or whatever. Or the $1/N$ inside the brackets could be $1/(N \log N)$. Note that the usual methods from Diophantine approximation allow to estimate the desired measure in the case when the right-hand side is 1. By $\| \cdot \|$ we denote the distance to the nearest integer.) If one believes the events to be somewhat independent, then the distribution we look for should follow roughly a Poisson distribution. Is something known in that direction? 

The solution should be $b-a$. I don't know how difficult this is to prove from scratch, but I think it follows for example from work of W.M. Schmidt, see: Schmidt, Wolfgang M., A metrical theorem in geometry of numbers. Trans. Amer. Math. Soc. 95, 1960 516–529. This is also contained as Theorem 4.1 in Harman's book on metric number theory. (In this setting, the result is stated for counting $\|n \theta\| \leq \psi(n)$ for some function $\psi$, where $\| \cdot \|$ is the distance to the nearest integer, but you should be able to translate this to your question, I guess.) 

Let $(\Omega, \mathcal{A},P)$ be a probability space, and let $(\mathcal{F}_k)_{k \geq 1}$ be a filtration which converges to $\mathcal{A}$. I suppose it is true that $$ E \left( \big(E \left( X | \mathcal{F}_k \right) \big)^2 \right) \to E \left(X^2 \right). $$ How to prove this? I guess one will need Jensen's inequality and a convergence theorem for submartingales, but cannot find the right reference (I am working on number theory, not probability). (If you know an answer, please also provide a citable reference.) 

A precise solution to this problem is known. In Khintchine's book on continued fractions it says: $\mathbf{Theorem~30}$ Suppose that $\varphi(n)$ is an arbitrary positive function with natural argument $n$. The inequality $$ a_n = a_n(\alpha) \geq \varphi(n) $$ is, for almost all $\alpha$, satisfied by an infinite number of values $n$ if the series $\sum_n 1/\varphi(n)$ diverges. On the other hand, this inequality is, for almost all $\alpha$, satisfied by only a finite number of values of $n$ if the series $\sum_n 1 /\varphi(n)$ converges. (quoting from: A. Ya. Khintchine. Continued Fractions. University of Chicago Press, 1964)