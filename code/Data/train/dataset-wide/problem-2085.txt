So basically the code just blocks until a message is received. This all works fine. My question is about the implication of issuing a command which hangs on to some service broker based resource basically forever. Are there any performance issues associated with this pattern that I should know about? For reference, this is SQL Server 2005. 

I have table partitioned on (int). I also created a non-clustered index for on the table. When I run the following: 

We had a plan to capture table changes with CDC until we realized that it doesn't support In Memory tables. Is there another way (preferably as straightforward as CDC) to capture data changes (from in memory tables) in SQL Server 2016? 

Must the partition column be part of the primary? Is it recommended one way or the other? Do I have to create an index for the partition key, or does the DBMS do it automatically on its own? 

Rather than putting the data in a case statement where you might have to repeat that case statement in more than one query, you could create a table with static data, then joint to that table. The advantage would be that you can change the data in the table easier than changing all of the queries that have that case statement. 

Do both servers have similar hardware and OS? Is there enough storage space for the 11g database? Have you thought about using data pump instead? With data pump and enterprise edition you can export and import in parallel as well as compress. With 10g you can compress the meta data but not the data. With 11g you can compress the data and meta data. How does the SGA compare between the databases? Are you using AMM on 11g, which isn't available on 10g? 

Rather than using tnsnames, I have used openLDAP to do the names resolution. You can replicate across two nodes if you want. As soon as you change the entry in openLDAP every server has the new information. You can even get the resolution from an ssl encrypted port if you want. using openLDAP the name resolution 

What does Table Scan (HEAP) mean for a partitioned table? Does it indeed use an index, perhaps behind the scenes? Is there anything I need to do to improve efficiency? 

I've inherited a very volatile table which is a map of who holds what resource in the system. At any given moment, there could be a dozen inserts/deletes/reads going against that table. However, there are never any more than 30-40 rows in the system. The system was written in the SQL 2000 era and the access to the table is serialized via sp_getapplock/sp_releaseapplock system sprocs, so that only 1 request is modifying the table. In addition, the INSERT & DELETE statements execute . Reading the notes from a decade ago, it states that without these restrictions, the system would experience non-stop deadlocks. I've ported the database to SQL Server 2016 Enterprise Edition. Now that the throughput of the system has increased 10 fold, this table is easily the biggest bottleneck. What are my options for a table as volatile as this with SQL 2016? I am looking for fast (hopefully concurrent) access and no deadlocks. 

You should check to see if the user that you exported had a job scheduled using a program owned by someone else. If so you are trying to schedule a job for a program that does not exist. You can create the program, then do the import as long as the user who will own the program exists before you create the program as that user. 

You may also want to query dba_segments filtering by table space name, looking for the largest object. You should look specifically at the sys.aud$ table. 

This code could throw the no_data_found error. You should avoid using "WHEN OTHERS", since you may not know exactly what caused the exception. 

By having your database is no archive log mode and doing a daily warm backup, you are saying that is acceptable to lose a day's worth of data. If that is the case then fine, you don't need archive log mode. Doing a data pump export will give you a logical backup of your database at a point in time. The next question that you should ask yourself is that if it takes 11 hours to export your data, how long will the import take. If you are in a recovery scenario and you need to import your data the process could take awhile. You would probably be better just doing RMAN warm backups. Having said that, there are two thoughts that I have about your current issue. Is there a process that is running during the export that is changing and possibly locking data. If so your export won't be consistent and won't have all of the data that was changed for that day. For example if you are running an ETL process during the export you might get some of the data from the ETL, but not all of the data. The second thought is, if you are using enterprise edition, are you using more than one channel? You should be using multiple channels and compression. But I would guess that there is a locking issue that is causing most of your issue. 

I run on box box and get multiple rows per single SPID. For example, see below. Does this mean that SQL Server has broken the query into 23 parallel sub-queries? If that's the case, why is it ignoring MaxDegreeOfParallelism setting of 8? Or is this something else? 

With SQL Server 2005, you could look at the Task Manager and, at least, get a cursory look at how much memory is allocated to SQL Server. With SQL Server 2008, the Working Set or Commit Size never really goes above 500 MB, even though the SQLServer:Memory Manager/Total Server Memory (KB) perf counter states 16,732,760. Is there a setting where it will actually show the server memory in the Task Manager? Or is it a result of them changing how memory is used in SQL Server 

It states Table Scan (HEAP). Not quite sure what it means in the context of a partitioned table. I also don't see that it uses any kind of index. And yet, it must, because the query comes back fairly fast (e.g. the table has 6 million rows). So, my questions are: 

Generally when someone wants to create a user to run an application, but not own the tables, they create a group that has select on the necessary tables and views and execute on the necessary stored procedures. They can then either create synonyms for the objects that they need or create public synonyms for the objects that everyone needs. You might want to have a separate roll for admin users compared with regular users. Once you do that then when you create a user they would need connect or create session and the role that was created for all users. If they have temporary tables they either need the create table privilege or to just have the tables created for them. Generally giving each user a separate login directly to the database is something that was done with client server apps. Most web based apps handle the logins on the app server and don't give direct access to the database for users. They just pool the connections and have business logic to control which user can use which function. You may want to rethink why you are creating individual users directly in the database. 

I have an app that's local to the SQL Server and thus has a Shared Memory connection to it. I was wondering whether the RAM taken by the connection (including data transfers) counts against the max memory limit set for the SQL Server. The reason I am asking is that the SQL Server is maxed out on memory (e.g. Target Server Memory = Total Server Memory and other metrics). If the RAM taken up by Shared Memory connection counts against it, wouldn't I be better off using TCP connection to the SQL Server? 

I have an SQL Server 2014 Enterprise Edition with a lot of data. I would like to mirror the data from that to at least 3-4 servers. The mirrors are all SQL Server 2014 Standard Edition (no more money is available for Enterprise licenses). How do I mirror the data from my main box (with the Enterprise Edition license) to other boxes? I tried the mirroring feature, but it seems that it only allows single mirror. I could you use Always On Availability groups, but that would require that all mirrors also be Enterprise Edition (unless I am reading the docs wrong). At least one of the mirrors needs to be there almost real-time (1-2 minute delay is fine) data replication. The other mirrors could have 1-2 hours delay. So what are my choices? P.S. All the secondary servers are just read only. P.S. The purpose of the mirrored boxes are partially to off-load readonly queries to them. These mirrors need to have near real-time data replication. Another purpose is for analytics, which is a heavy load. Today everything is on the same box and we are forced to do analytics at night so as not to disrupt users and there is just not enough time. P.S. The servers are nearby each other - on the same subnet, connected via a 10Gb link. P.S. Our license also allows a no cost upgrade to SQL Server 2016 when it becomes available. Does that change anything? 

I would guess that you have indexes that are unusable. Check and see if you have unusable indexes. If you do, then you need to rebuild them. 

You can't use utl_file, since it has a maximum line length of 32767. It's possible that sqlplus has a similar limit. If you are tying to move data to a different Oracle database you can use data pump. You can eve use datapump on a materialized view. If you want to migrate to a non Oracle database you can use Perl or heterogeneous services to create a database connection to mySQL, SQL Server, etc. Then you won't have to worry about the line length. UTL_FILE 

For me too many indexes means that it is more expensive to insert, update, or delete rows. Because, if the indexes are of any use they need to be maintained. Find all of the indexes with columns that you either don't query, or don't query very often and drop them. Find all of the indexes where the columns are the same, but in a different order drop all but one version of those. For example a good index would start with low cardinality and get more specific. For example country, state, city, zip code. Another good method would be that if you search by date and other fields lead with the date column to eliminate the largest number of rows first, then choose the values where the other fields match. Try to get the number of indexes down to less than 10 per table. Although the number of indexes that are useful depends on whether this is OLTP, DSS or data warehouse. 

I am partitioning a table based on a column that is not a primary key? I've read some conflicting information today on whether the partition column must be a part of the primary key. My gut says no, but I am not 100% sure. So questions... 

I've setup a test SQL Server 2016 server. I've installed 2 instances, restored a backup on the primary. Then restored a backup on the secondary with , then restored the transactional log on the secondary, also with . I then followed the prompts in the Mirroring Wizard off the Database Properties page and ended up getting an error: . What am I missing? 

I am about to split a large database into a bunch of federated instances. Currently, most of the primary keys are auto-generated identity ints. Obviously, that's not going to work in a federated setup. I've read people using auto-generated GUIDs to replace the ints, but that data type has well-known performance sapping problems on its own. What are some of the strategies I can use for having unique values for primary keys across federated members?