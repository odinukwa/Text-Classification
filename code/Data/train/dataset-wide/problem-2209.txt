Checking our production logs from yesterday, we discovered a period of about 5 minutes when a whole bunch of really simple queries were timing out. Further investigation on the server logs showed a huge spike in disk activity, which led me to the conclusion that an automatic CHECKPOINT was being run on the DB at that time. That's something I really don't want to happen during peak hours. So I was thinking of scheduling a daily CHECKPOINT every day during off-peak hours. Is that a good idea? Bad idea? Waste of time? If not that, then what? 

I tried to include the execution plan, but that sent me over the 30,000 character limit. The crazy thing is that I'm supplying AssessmentID = 1538, which is the most useful limiting information in the query, but the execution plan is all but ignoring this fact, and scanning almost every other joined table, only filtering down by AssessmentID kind of as an afterthought... 

I have often found that when I am having performance problems on my MSSQL database, I can resolve them by running . I learned this because that's always the first line of defense from tech support, right after, "Is your computer turned on?" My understanding is that SQL Server is supposed to keep stats automatically; I shouldn't need to nanny it. So I'd like to understand: what circumstances might lead to the stats falling out of date so badly that I need to update them manually? 

I have two machines, each with a default instance of SQL Server 2008 installed; each exhibiting a different behavior. 

Yes, there is existing data in SerializedValue. But what, pray, can be truncated by creating an index on a computed field? 

I'm investigating the benefits of upgrading from MS SQL 2012 to 2014. One of the big selling points of SQL 2014 is the memory optimized tables, which apparently make queries super-fast. I've found that there are a few limitations on memory optimized tables, such as: 

I believe the mystery is cleared up. The documentation only says that the SQL Server account needs full access to the filestream folder, but when I checked who had access to the main DATA folder, I saw that the SYSTEM account also had access. I gave full rights to the FS folder to the SYSTEM user, and now I don't get this error anymore. 

I was having an issue getting SQL Server Reporting Services 2008R2 SP3 working under a domain service account on Windows Server 2008R2. It started with this What am I missing for my SSRS Service account local server permissions? and I have progressed from there some. I have a domain account setup as the service account for SSRS and the service is running. However on the /ReportServer and /Reports pages I am getting page cannot be displayed on my local server. Looking at the logs under "C:\Program Files\Microsoft SQL Server\MSRS10_50.MSSQLSERVER\Reporting Services\LogFiles" I saw two errors 

That gap you see above prodkbp is in the output as well. Not sure if that matters either. My production server has that anomaly as well so I can't imagine that has anything to do with it. Basically everything looks right but something is still wrong. I cannot install Enterprise Manager Environment 

As part of the troubleshooting I was doing to try and address the issue I ran Process Monitor while I was trying to start the service. I tracked as event that also had a result of "Access Denied" which was the service trying to read files inside the directory where reporting services was installed an running from. In my case it was: "C:\Program Files\Microsoft SQL Server\MSRS10_50.MSSQLSERVER\Reporting Services" I check the security of the folder and the service account I was using had no rights to the folder. That is why giving it local admin rights fixed it because that group did have access. I gave my service account Modify access to the folder and its contents. After that I was able to start the service. 

For testing I have been trying to install Oracle 11g on a test system with a single database several times with the aid of snapshots. This most recent iteration I have been having issues getting the database to register with the listener. I am to understand that PMON, as long as the database is running, will attempt to register itself at regular intervals with a listener that is running locally on the same machine using port 1521. I have such a listener running but the service is not registered it seems. 

That tells me that the database registered itself dynamically correctly. prodbkp is my SID. This might be a case issue since I named the DB "PRODbkp" but everything else seems to be fine since I can connect with just fine. Case should not be an issue with service names as per docs.oracle.com tsnnames.ora 

Now all these different cases for different values of @calcType seem to be wasting a lot of space and causing me to copy and paste all over, which always sends shivers down my spine. Is there some way of declaring a function for CalcField, similar to lambda notation in C#, to make my code more compact and maintainable? I would want to do something like this: 

Now I want to create an index on LongValue, so that I can easily look up on serialized values that represent numbers. 

Thanks to all who answered - really appreciate the points you've given me to think about. The general feeling I got was that a single database is preferable, but I would like to add some countervailing points in favor of the sharded architecture, and addressing some of the concerns that other people have mentioned. Motivation for sharding As mentioned in the (updated) question, we're aiming for massive sales worldwide, with literally millions of users. With the best hardware and indexing in the world, a single DB server won't take the load, so we have to be able to distribute across multiple servers. And once you have to look up which server any given customer's data is on, it's not much more work to give them a dedicated database, which makes things simpler in terms of keeping people's data neatly segregated. Response to Concerns 

In other words, for all records where is 1, I want to see the and , alongside the values of and that are returned from for an input of . What's the syntax to do this? 

What am I missing? UPDATE: here's a clue. If I delete as a user from under the database's security context, then I go to the user under the server's security context and under "User Mapping" I grant access to the database, then it starts working. It could be that the database was originally restored from a different server, which also has a user. I guess then that the user identification is not based on the username? 

I have a business requirement to have a FK field on referencing . If there is a value in , it should be unique, but it's not mandatory, so there could be multiple records with null values. Is it possible somehow to enforce this type of uniqueness on the DB level, using either an index or a constraint? 

Windows 2008R2 with firewall disabled for testing (not that it changed anything). Listener.log doesn't show anything of worry. Just evidence of me restarting the listener many times. I am aware that I could configure a static listener with but I would like to understand why this is not working. This is the 4th time around that I have been doing this and I have not run into this problem yet. I am basically stuck since I am not sure where else I should be looking. 

Really new to Oracle and being a DBA in general. I am trying to set up a development environment so that I can play an learn oracle better. Enterprise Mananger failed to configure itself when I first created the database using the Database Configuration Assistant. No biggie. Just need to user emca.exe I had some issues with the Listener but those might have just been me being impatient in waiting for the service to register or the service not running. Right now my issue is this from the emca log: 

So it would seem the issue was not the service name specifically but that the request was going to the IPv6 address which was not set up in any of the required files. Looking at listener.log ( which for me was located D:\app\Administrator\diag\tnslsnr\dvp-oracle\listener\trace\listener.log) I found these entries associated to my connnection attempts. 

According to post here and on SO it should just be an issue with the service name. Problem is it looks right to me. Listener.ora 

I can connect to the db using and running shows the DB in question. So between that tnsping and I know the listener is at least running. does not appear to have an effect either. Not that it should matter since I should just have to wait a minute for it to work on its own. I then went looking for any PMON related logs. It should have been in the trace folder ($ORACLE_BASE/diag/rdbms/database_name/SID/trace/SID_pmon_PID.trc) with other trc files but it was not there. I then used the following query to verify where it should be 

The important portion was . Since I would never need to support IPv6 I removed it from the network adapter and restarted the server to ensure all services were updated accordingly. While it might not be the ideal solution I was able to recreate the EM dbcontrol successfully after that change. 

And no data is imported. From what I've Googled, one possible cause of this is that I need to specify . But I have no idea what the name is of the schema in the dmp file. Any easy way to find out? EDIT: I didn't find a solution to this question, but I did find a workaround... I tracked down the guy who made the DMP, and beat got the schema name out of him. Specified according to his definition, and Hey Presto! 

Indeed, that was the answer. I set the db recovery model to "Simple", waited a minute for the filestream data to clear up, and then I could remove the filestream file and filegroup. 

I have a query that was generated by C#/Linq-To-Entities code. As with this kind of query, it looks seriously ugly and I doubt it would help to include it here. But I have run it through a query profiler and found that 50.5% of the processing time is happening in a "Nested Loops (Inner Join)" step, which is joining a clustered index seek on (0.3%) and a clustered index scan on the same table (13.1%). I confess I don't truly understand what this "nested loops join" is doing, but why would the query be doing a scan on the table right after doing an index seek, when the fields it's looking up are available directly through another index on exactly the required fields? And yes, I have run , to no effect. Not being an expert in understanding the query plan, I'd appreciate if you could prod me for answers to relevant questions; I might just not know enough to have supplied you with enough useful info. Thanks! 

This works... but the coalescing of and fools the compiler into not using the indexes, and I get a horrible execution plan involving table scans. How can I create this view in such a way that querying it will still use my indexes? 

I have been given a DMP data pump export file to import into my local Oracle instance. I've tried running this command line: 

These all qualify as nuisances, but if I really want to work around them in order to gain the performance benefits, I can make a plan. The real kicker is the fact that you can't run an statement, and you have to go through this rigmarole every time you so much as add a field to the list of an index. Moreover, it appears that you have to shut users out of the system in order to make any schema changes to MO tables on the live DB. I find this totally outrageous, to the extent that I actually cannot believe that Microsoft could have invested so much development capital into this feature, and left it so impractical to maintain. This leads me to the conclusion that I must have gotten the wrong end of the stick; I must have misunderstood something about memory-optimized tables that has led me to believe that it is far more difficult to maintain them than it actually is. So, what have I misunderstood? Have you used MO tables? Is there some kind of secret switch or process that makes them practical to use and maintain?