As per Installation documentation the correct version of each available plugin is always provided in the SQuirreL SQL Client install jar. 

Now for creating memory optimized tables you have to create table with Getting Started with SQL Server 2014 In-Memory OLTP will give you all the detailed steps to get you started. For more details, check this BOL article : In-Memory OLTP (In-Memory Optimization) 

Above will generate scripts for bulk insert using SSMS set in TEXT MODE as output. Copy the generated script and run it in another window using SSMS. 

Based on your comments, your best option would be Database Mirroring or Log shipping. You have to evaluate your RPO and RTO. Since you are using VPN to your actual machine - meaning it is point-to-point connection : 

Best and most reliable way is to use backup / restore If your database is big and you are on sql server 2008 R2 and up, standard edition also has compression feature to allow you to compress your backups. Dont forget to sync up users after the restore :-) 

Paul Randal has described how to read from T-log with excellent examples -- Using fn_dblog, fn_dump_dblog, and restoring with STOPBEFOREMARK to an LSN . 

You have to get the status using to get the status. There are many scripts available that will send out HTML email through SQL Agent job like 

The memory allocations for AWE in 32bit use the AllocateUserPhysicalPages() API. The memory allocations for lock pages in 64bit use the same API, but they aren't using AWE because the VAS in a 64-bit process is 8TB for user mode space and it doesn't require special mappings through AWE. Queries timing out has least to do with AWE. There might be missing indexes, outdated stats, poorly configured memory settings and many other stuff that can lead to queries timing out. What specific version of sql server are you running (along with patch level) ? 

No they are not linked. - is sysadmin which is a special high privileged SQL login and is disabled by default. - this means that you want to log in to your local machine having database engine installed. means local machine and is the instance. 

The above query takes 12 mins and 38 secs to run and returns 34 rows. Below is the statistics IO and time output: 

There are couple of things to consider : backup and restore is the safest option that you can use for migration from SQL 2005 to 2012. Refer here for more details of what you are experiencing. Full text has undergone major chance in 2008 and up. from BOL, Attaching a SQL Server 2005 Database to SQL Server 2012 In SQL Server 2008 and later versions, a full-text catalog is a logical concept that refers to a group of full-text indexes. The full-text catalog is a virtual object that does not belong to any filegroup. However, when you attach a SQL Server 2005 database that contains full-text catalog files onto a SQL Server 2012 server instance, the catalog files are attached from their previous location along with the other database files, the same as in SQL Server 2005. The state of each attached full-text catalog on SQL Server 2012 is the same as when the database was detached from SQL Server 2005. If any full-text index population was suspended by the detach operation, the population is resumed on SQL Server 2012, and the full-text index becomes available for full-text search. If SQL Server 2012 cannot find a full-text catalog file or if the full-text file was moved during the attach operation without specifying a new location, the behavior depends on the selected full-text upgrade option. If the full-text upgrade option is Import or Rebuild, the attached full-text catalog is rebuilt. If the full-text upgrade option is Reset, the attached full-text catalog is reset. EDIT: You can attach the detached database along with the fulltext catalog using Create database ... FOR ATTACH. 

Lastly some good references that will help you : Deep Dive on Initialize from Backup for Transactional Replication Replicating Non-Clustered Indexes Improves Subscriber Query Performance Follow the Data in Transactional Replication - Whitepaper Troubleshooting Transactional Replication Scaling Out the Distribution Database 

since this is bigger than comment ... If you have already done .. I will delete my answer ... You need to alter your AG replica to allow read only connections e.g. Below is referenced from BOL : 

Also, you can use the Data Conversion transformation, which is probably easier if you are only changing the data type. EDIT: You can also look for an option of SSIS GoogleAnalyticsSource from codeplex. 

Install sql server 2016 developer edition, its free for dev purpose only. Restore the database on 2016 instance and then use this method to move your data back to older version. Note: be mindful of enterprise features vs standard edition feature incompatiblity and limitations. 

You don't have to use Access as a mediator. You can look into SSIS or import utility to import data from .TPS to SQL. You have to select Flat file data source. At the end of the import utility, you will have the option to save the package and then schedule it using SQL Agent job. Refer to : How to Migrate Data From Top Speed Database Files Also, look into SSMA (SQL Server Migration Assistant). I have used it for Sybase to SQL Migration, but not sure if it will be good fit for TopSpeed migration. 

Set it on Primary replica only and it gets applied to all secondaries. You can test it out on a small db .. add it to availability group and change the autogrowth setting. 

--- using Old Detach / Attach method (not preferred, but still people use it.. unfortunately I used it recently on a NON prod server). 

Refer to Replication, Change Tracking, Change Data Capture, and AlwaysOn Availability Groups (SQL Server) From the reference : The order in which CDC and AlwaysOn Availability Groups are configured is not important. CDC enabled databases can be added to AlwaysOn Availability Groups, and databases that are members of an AlwaysOn availability group can be enabled for CDC. In both cases, however, CDC configuration is always performed on the current or intended primary replica. Harvesting Changes for Change Data Capture Without Replication If CDC is enabled for a database, but replication is not, the capture process used to harvest changes from the log and deposit them in CDC change tables runs at the CDC host as its own SQL Agent job. In order to resume the harvesting of changes after failover, the stored procedure sp_cdc_add_job must be run at the new primary to create the local capture job. 

Supplementing to the current answer... Detach database will leave you without any backup. If for some reason, you cannot attach database, you will end up with a detached copy of mdf and ldf. A best practice should be to do a backup and restore of the database. This way you end up with at least a copy of the database. Read up : $URL$ 

Answering my own question, as future readers will benefit from it : Seems like we might be hitting Longer latency for SQL Server 2012 database when you use Service Broker, database mirroring, and Availability Groups. This is fixed in . The has a typo(AlawysOn). So if you are searching by AlwaysON, it wont show up. After the patch was applied, the issue was fixed. 

There are many free tools - native and opensource available for comparing schema between 2 databases: 

As I show , I have server database version-10.50.1600 and local database version-10.0.1600 . How can I copy database (with all tables and data(s)) from server to local which has different version ? Since you are trying to load data from higher version to lower version, this is not supported. As others have pointed out to script out database objects + data, depending on the database data size, I would take a decision. If the database size is few MB then, that is a good approach. If the database size is huge or in GB, then the generated script wont be either handled by SSMS nor SQLCMD. Below is my suggestion : 

No automatic failover does not happen in this scenario as the current principal will still have a quorum forming a partner-to-partner quorum. Without a witness, automatic failover is not possible - ONLY Manual failover is possible. 

You can set up mirroring as normal. But when you want to failover, then use SYNCHRONOUS - High safety without automatic failover. 

From my experience, its worth to invest in Redgate's Schema and data compare tools as they have commandline options and they integrate well with Powershell as well. 

As a side note, depending on what version and edition of SQL server you are using, in 2012 and up, you can leverage AlwaysON with readable secondaries to offload reporting to the secondary server. Let me know if you want any further clarification and I will be happy to help you out. Edit: 

I would say do not suppress, but have your scripts intelligent enough to not generate false/ unnecessary alerts. What can be done ? Before taking backup, put some validation e.g. check if database exists in sys.database, is online and accessible (not secondary or offline), etc and then kick off your backup. Ola's backup solution is quiet intelligent in those terms. 

Suggest you not to remove people but disable their login or just revoke access. Then after a full business cycle, if no one complains then remove those users. 

Below is the script that I wrote for myself to make copy of database. Its flexible and can be converted into a stored procedure. The comments will explain what it does. Test it on a Test server before running it in PROD !! 

Go for something like Redgate's sql data generator that will generate test data -- OR -- You can do a 1 time import of data as I suggested and then backup database and for your rest of testing you can just keep restoring the database from the backup (keeping in mind the data that you imported will not change). I cant think another way of doing what you want to achieve, so will keep this for others to answer ... 

I would suggest you to run a - for your entire business life cycle. There might be cases where some reports are ran end of month or end of quarter. So you will need below event in a server side trace with these columns . You can filter out stuff that you dont need using (note that it is announced deprecated). Alternatively you can use Extended events - Tracking SQL Server Database Usage. Note: Currently in my environment, I am using Server side trace and its a very low impact. Also, I have a job that dumps the profiler info into a table for analysis. You can have below table structure for loading trace data : 

Yes, if you are replicating a new database or subset of data to other server, it should be fine. You need to think about if the distributor server is same on the publisher server or you are having a different distribution server. When distributor and publishers share the same server instance, then it is highly likely that you will hit a performance bottleneck, depending on how much data you are replicating (all data vs subset of data) and your network latency between your publisher databases and subscriber databases. 

Apart from the fact that you cant change the recovery option for tempdb, You dont need a loop for what you are doing : Run in SSMS by pressing CTRL+T 

Merge replication is a bit tricky to remove as compared to Transactional replication. Below are the steps that I follow successfully to remove obsolete/leftover merge replication bits and pieces : 

You can still see (atleast in SQL Server 2008R2) using , and you can set the database option as well 

You don't have to be fancy/worried or scared when you are restarting sql server. Just make sure that you dont have any long running transactions. Best is to restart sql server using console or shutdown command during a low/minimum activity period also called maintenance window to minimize impact on your business. If you have any DR setup and you dont want to be down, then best is to failover and then restart the passive or secondary node. Clean Shutdown SQL Server occurs in below scenarios :