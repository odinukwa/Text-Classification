I have a Catalyst 2900 that's sending out an STP packet about once ever 1-2 seconds. Is that excessive? 

I've got a Linux file server set up by the previous IT department with permissions set up to allow Jane to access a certain folder with the username "jane" My new AD domain that I put in has everyone's username set as first initial, last name so her username in the domain is "jdoe" For reasons I don't want to go into here, I can't just add "jdoe" as a user on the file server Can I set her SamAccountName to "jane" and keep her User UPN Logon as "jdoe"? Will that work to let her into the Linux file share? 

As you all probably know, ipv4 route cache has been removed in 3.6 Linux kernel series, which had serious impact on multipath routing. IPv4 routing code (unlike IPv6 one) selects next hop in a round-robin fashion, so packets from given source IP to given destination IP don't always go via the same next hop. Before 3.6 the routing cache was correcting that situation, as next hop, once selected, was staying in the cache, and all further packets from the same source to the same destination were going through that next hop. Now next hop is re-selected for each packet, which leads to strange things: with 2 equal cost default routes in the routing table, each pointing to one internet provider, I can't even establish TCP connection, because initial SYN and final ACK go via different routes, and because of NAT on each path they arrive to destination as packets from different sources. Is there any relatively easy way to restore normal behaviour of multipath routing, so that next hop is selected per-flow rather than per-packet? Are there patches around to make IPv4 next hop selection hash-based, like it is for IPv6? Or how do you all deal with it? 

I have a colocation facility with my main firewall in it. It has a bunch of site-to-site VPN tunnels built from the main firewall to the firewalls that are in the remote facilities. I want to replace that main firewall with a Cisco ASA but I want to reduce downtime. I want to put the ASA in and configure VPN tunnels to the new firewalls in the remote facilities without taking down the VPN tunnels that are already created on the old firewall. I was thinking that I would split a switch into two VLANs and hook the uplink and the outside interfaces of both firewalls to one VLAN and then the inside interfaces of both firewalls to the other VLAN. But then it hit me that I need a public IP address to set up a VPN and both firewalls can't have the same public IP address on their outside interfaces... So what's a guy to do? How can I put a second firewall in the heart of my network and still keep the original up and running? 

I can't think of any tool that would do that out of the box. This is quite rare scenario, as you can't create correct two-way NAT mapping if you only change port. Do you really need just one-way traffic ? However you can always write your own netfilter module (it's not that difficult) and alter packet headers in any way you want. 

Just use "DEVICE partitions", it will try all devices listed in /proc/partitions, and you wouldn't have to worry at all what the device names are. UUID of an array is stored on each device belonging to it, so each array will be assembled correctly even if you have several of them. 

I've found the answer and it was quite funny - ARP table overflow. The traffic in the test environment was gerenated from many IPs that resided in directly-connected networks, so the system had to use ARP first to figure out MACs, and the default hard limit of ARP table in Linux is just 1024 entries, which gives number of connections between networks connected to 2 different interfaces close to 512. When I increased net.ipv4.neigh.gc_thresh1 and also .gc_thresh2 and .gc_thresh3, the problem was solved. 

I'm protecting a SQL Server with ASR and doing backups with Commvault. Commvault is telling me that transaction log backups are being converted to fulls because it detected a break in the log chain probably caused by another backup product. I dug into it and found that ASR's VSS writer is taking a snapshot once an hour and SQL Server is seeing this as a full backup. Has anyone had experience with this? I feel like the ASR VSS snapshot shouldn't be counted as a full backup of the database 

I have a AD domain and have redirected the Documents folder for each of my users. I have the "Grant the user exclusive rights to the Documents" checkbox ticked so I can't get in to see their documents. I also have Windows Server Backup running a complete backup to a network share every night. I run the backup as a user that's part of the Backup Operators security group. I tried doing a Files and Folders Recovery and went in to look for a single file in one of my user's Documents folders and nothing showed up. What I'm wondering: does Windows Server Backup backup the files that are in the Documents folders that users have exclusive rights to? If my DC dies, and I do a full recovery, will all of their files still be there? 

Well, things are a bit more complex. Modern hard drives don't just detect errors, they have some spare sectors and smart controllers that try to relocate bad sectors. That is, when you try to read some logical sector and it doesn't read at first time, the controller tries to read it several times, and sometimes it can read it after some retries; then it writes the data back to the spare sector, remaps logical sector to the new one and marks old sector as bad, and finally gives you your data. All those processes are completely transparent to the reader, you wouldn't notice any error. However this will normally be reflected in S.M.A.R.T statistics, and if this happens more and more often, you can see that the drive is going to fail before it actually fails. That's why it's really important to use SMART monitoring tools on your system. When a sector doesn't read at all, or the controler runs out of spare sectors, read error will be returned by the drive. Error detection is now pretty bulletproof, it uses some kind of CRC for sector data. When read error is returned, mdadm will see it, mark the drive as unusable and switch an array into degraded mode. 

I have a GPO applied to my conference room computers that forces Outlook to use non-cached mode (for quicker opening and to save HDD space by preventing everyone's OST from being created). When I try to open Outlook it gives me an error message that says 

I have seen a consistent spike in traffic over my network since Monday morning and I don't know where it's coming from! I don't have netflow routers (like I would like), I have IPCop firewalls. Is there any way that's built in to Linux that I can see where the packets are coming from/to? Like a built in packet capture? If there's not, how do I go about finding where this traffic's coming from? 

You need 32-bit version of libraries to run 32-bit applications on 64-bit system. Unfortunately Redhat doesn't have package like ia32-libs which would install most of them, it is supposed that you should install all 32-bit applications with yum and it will install the appropriate libraries for you. If your application is third-party, try installing 32-bit version of each library it needs, they usually have .i586 suffix, so you execute something like "yum install libusb.i586". 

You can setup transparent proxy on your client machine that will have your company's proxy as a parent and add authentication information when forwarding requests to the parent. You will need to install a proxy server that supports transparent proxying, I'd recommend squid; and you will need a firewall that will redirect your traffic to a proxy server, many windows firewalls can do that. Google for "squid transparent proxy", there are a lot of manuals. 

I have network enabled printers and I'm wondering what the best way is to deploy 'em I can deploy them using GPOs that map the TCP port with the printer's IP address (how I'm doing it now) I could deploy them using Print and Document Services and AD I could deploy them using the "Deploy Printers" in GPO What are the benefits and drawbacks of each and what are you using? 

so when the browser sees that this cert is trying to cover $URL$ it says the cert's not trusted. What amy I supposed to do about that? 

I had to create a Join on the System Resources/Desktop Monitor ResourceID to get the System Resources Name attribute to tell which computers the monitors are connected to 

I suppose you know how hashing generally works: it calculates some function out of the data (IP, pair of IPs, etc) and uses value of that function as an index in the table to locate the structures associated with that data. Each cell in the table (which corresponds to one possible value of hash function) is usually called hash bucket. Unfortunately different sets of data may produce the same value of hash function, and will be associated with the same hash bucket. That's why hash bucket may contain several hash entries, which are usually stored as a linked list. Thus, when a lookup is done, hash function is calculated first and a hash bucket is selected, and if it contains several hash entries, they are analyzed one by one to find the approriate hash entry. Thus hashlimit-htable-size limits the number of hash buckets (size of hash table itself), and hashlimit-htable-max limits the number of all hash entries (stored in all hash buckets).