For the two sites on the same box, it depends on how they're setup. If they're on different ports, define the ports, if they're paths, then you'll need to add the path on the end of the ProxyPass and ProxyPassReverse entries: 

I've recently been moving our instances to EBS instances (CentOS) and still have a bit of confusion on what's happening when I "stop" and instance. I have some of my services with runlevels 345 on but when I start a stopped instance the services don't start. What's actually happening when I issue a stop command to the instance, and how do I get my services to start automatically when I start the instance up again? 

For an internal network, if you don't want to pay for the wildcard cert (or dedicated cert for the server), simply create a self signed certificate. Since it's your company, it shouldn't be too much of an issue for the users to accept the certificate. And depending on how your network works (and your admin friendliness), you might even be able to push it to the users so they won't be bothered. If you're dealing with any sort of sensitive information, you should be doing it over HTTPS. 

The solution ended up being to run commands through a simple PHP page. The connection was internal to our network and we weren't worried about encryption, just remotely triggering a command and getting the output. There must have been another command line scripting tool we could have used but the approach we took ended up being very simple and met our needs. 

Am I correct in understanding that generally IaaS (looking primarily at rackspace cloud servers) will provide a virtual server where OS updates, software updates, anti-virus, managed backups and other common non-hardware server admin tasks will be required by the client. With PaaS (looking at heroku) on the other hand almost all of that is handled by the host with only maybe occasional external backups necessary. Flexibility and customization as the main trade offs? 

There have been 2 instances now where the Script checkbox under "IIS > Sites > Site > Handler Mappings > Edit Feature Permissions" has become unchecked. What could possibly be causing this to occur? Haven't been able to find any information online about others experiencing this problem. This could just be a coincidence but I just realized that this has happened twice now within a day or so of posting a full update to the folder which the site points to. Usually we upload a new folder of code, rename folders and the new folder takes the place of the old folder. The permission does not immediately uncheck, it is hours later. Is there any reasonable explanation why this might cause the script permission to disable automatically? 

A reboot will keep it's settings. If you bring up a new instance then you'll need some way to manage the IP's. A startup script is probably the way to go. Pull the instance metadata and update a config file (perhaps in S3) or DNS entry. You can also use an elastic IP and configure a cname which will resolve to the internal address within the EC2 environment. To do this, assign an elastic IP and note the public DNS string (there's a pattern but it's good to check) and create the cname record with it. The advantage with elastic IP cnames is you don't have to wait for DNS propagation. 

You can also add ports to the ProxyPass directive if you don't want your friend to change firewall rules, service setting etc. Or you can even be a reverse proxy for another external IP address too. 

I'm wanting SNI on my CentOS 5.5 dev server and since it looks like I'm out of luck with the current repo versions of openssl and apache, before I go compiling custom RPMs I thought I'd see if I could get it working with gnutls. Anyone know how to do this? According to yum, gnutls is already installed but I don't see it in my apache modules directory. 

I'm trying to create a process for comparing two folders that should contain exactly the same files. Would like to be able to modify a file's content without changing the attributes as if the file was accidentally corrupted. Any way to accomplish this? 

One of our Active Directory servers rebooted after a windows update this morning multiple hours out of the expected time period for updates. I found the automatic maintenance task "Regular Maintenance" in Scheduled Tasks under Microsoft/Windows/TaskScheduled. It is set to run at 3 AM which lines up with what the GUI shows and our intentions. However the "Next Run" value is 4:15 AM tomorrow and looking back in history the task is never triggered at the scheduled time, always running 1-4 hours late. What could be causing this and is there any way to remedy? I could find no useful info in event logs. Edit: our primary Active Directory server just restarted at 9:30 AM, more than 7 hours after the scheduled maintenance is set to run 

For what you're doing, you're probably fine running on regular hardware. But it's always worth learning about things. As mentioned, server hardware is really designed for reliability. You get things like ECC RAM, dual or quad CPU sockets, redundant power supplies and fans, RAID for your hard drives among other things. You can also generally build "bigger" boxes than you can with "standard" hardware which is beneficial for maintenance (fewer boxes) and solutions like Virtualization (pretends to be more boxes). Google doesn't do that sort of thing and takes the approach I think you'd be comfortable with, use cheaper hardware and replace it when it fails. Google's redundancy and scaling is horizontally (more boxes). The advantage here is it's cheaper to buy at the cost of complexity of overall architecture. Applications need to be designed differently and how the overall system functions can get more complicated. You need to add load balancers and such which may or may not be possible. Assuming downtime isn't a huge concern of yours, I'd say stick with the low cost hardware, backup things properly and replace stuff if/when you need to. 

This has been a recurring issue for us both on our office network and production. It rarely matters since normal traffic rarely comes close to utilizing 100mbps but when doing vmware conversions it is particularly annoying for it to take 8 hours instead of maybe 2 due to this limitation. My laptop and one of our office servers are showing as gigabit on our switch and we are seeing such speeds, however another server and NAS we are testing goes down to 100mbps mode when going through our primary office gigabit switch by way of wall drops (same wall plate as laptop). When we hook up the NAS to a local "desktop" gigabit switch it switches to 1000mbps mode and tests as such speeds to the laptop. In production only half of the nics report as 1000mbps though they all should. Any insight into why this might happen or what should be taken into account when troubleshooting would be appreciated. 

Create or edit the vhosts.conf in your apache conf.d (or equivalent depending on OS). Use the NameVirtualHost directive to handle the DNS names. 

Checkout the documentation for ProxyPass and ProxyPassReverse. If you just do a name based virtual host for both then you could add something like this to your virtual host definition (or replace localhost for your IP if your box will be hosting apache): 

I'm trying to write an install script for my Amazon servers and I'm getting stuck on some environment variable issues. I have a set of scripts to configure things and some of them depend on environment variables that I create in profile.d scripts. I create the profile.d script (or copy it over) and need to use the variables it sets in scripts that run later (without logging out and back in). Is there a way to load these (in a script) so future scripts take advantage of them? In the script after I create the file I tried: source /etc/profile.d/scriptname.sh and . /etc/profile.d/scriptname.sh but it only sets the environment variable for the duration of the currently running script, so any other script that gets run later can't use the values being set. How do I get them to get set for the session instead of the script? I have one master script that calls a series of small scripts to do all the configurations. 

We are in the process of setting up a spam filter (SAVASM). One change we are making is to push incoming email on port 25 through our spam filter/server but have users actually send their email on a different port. I am attempting to make this happen by using port address translation to send port 25 traffic to the SAVASM server IP. As a step in making this change I setup port translation without actually changing the IP addresses. The NAT rules for the email server went from one Static NAT rule with no port specified, to multiple Static NAT rules each with a port or group matching the Access Rules for that server (smtp, pop3, http, https, and some other custom ports). The problem we are running into is confusing. Some outgoing mail through this server is failing when the router has the multiple NAT rules with port translation settings. Email goes through fine FROM our email to our internal accounts and to Gmail. However email fails when FROM our client's email address TO our client's email or their personal Comcast. The only situation that worked for them was if they changed FROM to Comcast and then messages went through fine to both Comcast and the client's accounts. Switching back to regular Static NAT rule everything then worked for them. Does anyone have a clue as to what might be going on? We are on a Cisco ASA 5500 box. 

I'm working on getting some servers running in the EC2 environment and I'm noticing some errors with ntpd trying to sync (using CentOS). I was reading on this site and the impression I get is that I don't need to run ntpd since EC2 is Xen and the host takes care of the time for the virtual servers. $URL$ Is this accurate or do I need to figure out how to get around the error I'm having? cap_set_proc() failed to drop root privileges It looks like it involves building a new kernel and other stuff I'd rather not do if I don't have to. 

You could also do this using virtual hosts and setup Apache as a reverse proxy. This is how I have it setup where I work. 

We've had a server (CentOS) running in EC2 for a few months. It had been going pretty smoothly until today when we got an alarm that the server was unavailable (HTTP service couldn't be reached). So I tried SSHing into the box but that timed out as well. I logged into the EC2 console and it said the instance was running and there wasn't anything in the system log. One odd thing I noticed is that even though we have an Elastic IP attached to it (which shows in the Elastic IP management area), the instance detail is not showing that there is an EIP associated with the instance. I looked through the message log and the last thing I see around the time we got our alert was the dhclient renewed the lease. I'm guessing there may have been some sort of issue with the networking. How might I check if that was the problem, or if there were any other issues that may have caused our instance to stop responding? 

We are pulling an Active Directory server out and modifying DNS settings on all systems. Attempting to update DNS list on a vm host results in a validation error that the domain value is not filled. How could this have been setup originally without, what does it do and is there any harm in adding our domain? 

Trying to use plink for a project and running into problems on Windows 7 x64. It simply goes to a blank line and hangs there. On a 32-bit XP development machine it runs as expected. We have tried XP compatiblity mode but it did not help. 

Was in the process of some big copies from usb drive directly connected to nas and suddenly one of the servers could not connect to nas device by name but others can. Forcing in host file didn't help. What could cause this and anything that can be reset to resolve? UPDATE - triggered failover to backup NAS and connection by (same) name was resolved, but still want to figure out what happened and why only this server was affected 

Are there any ntpd messages in /var/log/messages? As I was dealing with my EC2 NTP issues I noticed that NTP didn't like being too far out of sync and wanted a manual update. Perhaps you're too far out for it to decide it will update for you. 

We've been moving servers to EC2 lately and I ran into an issue recently involving locales. We use a script to build an AMI from scratch that is largely based on a simplified RightScale script. However, we recently worked on an international project and I discovered that the locale was not set during the scripted install (issuing locale at the command line results in posix). It appear there is no i18n file by default. However, checking a development server that I installed locally (via GUI) the i18n file exists. What package(s) do I need to install and which program can I run (command line) to configure this during the scripted install? We're running the current version of CentOS. (5.4)