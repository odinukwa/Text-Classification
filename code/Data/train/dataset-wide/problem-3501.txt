Pardon, because I'm not super experienced with various Linux distros, but from the Active Directory side, this sounds like you may have a problem with your RID Master. If the pool is depleted and not being replenished, it may be that the other domain controllers cannot communicate with it for some reason. What happens under the hood is the RID master allocates a pool of (i think) 500 RIDs at a time for new objects, and when that gets to less than half a new request is made and a new block of RIDs is assigned. I would recommend trying a full dcdiag and looking through it for any errors (or asking your AD admin to do so) as a starting measure. 

Is the Windows 10 system part of a domain? When you load this module by default, it attempts to look at your service records, find an efficiently located domain controller and map it to the PSDrive "AD:\" so that you can navigate it within powershell and review records. It is possible to disable this: $URL$ 

So this makes no sense to me. This means that I have 64 CPUs with 16 cores each, or a total of 1024 cores. However, online documentation for the Intel E5-2686 v6 claims that it has 36 cores and hyperthreading, for 72 virtual cores. What's going on? How many cores are there? 

I have a Centos VM running with SELinux enabled. I wish to have sshd listen to another port --- says, 993. I've modified the sshd_config file to listen to another port, but SELinux is getting in the way. I don't want to disable SELinux. How do I tell SELinux that it's okay for sshd to be reading TCP connections on port 993? The correct command to use is but I cannot use that command because port 993 is already in use in another policy: 

We're currently using Mailman as a mailing list manager. Mailman modifies the content of mail messages. The problem is that some of our users are sending digitally signed messages and the modification makes the signature break. I've seen this behavior with Apple Mail, Outlook, and Thunderbird. The problem seems to be this: S/MIME signed messages are implemented with a MIME Content-Type. Mailman wraps this inside a MIME Content-Type. None of the mail readers look inside the outer for the inner . We won't be able to get the clients fixed. Is there anyway to modify Mailman so that it doesn't have this behavior? 

I think you might be looking for 'erroractionpreference', this article explains through a few examples of how it works. $URL$ Given the other answer, if your script is larger than this function and you need to continue troubleshooting, then you'd need to wrap this bit of code by setting it at the beginning and setting it back to 'continue' after you are done processing. This will suppress non-terminating errors like that so they are not printed to your host. 

Your best solution to this problem is absolutely a conditional forwarder, although as someone pointed out above a delegation would probably work as well if your DC can route to it. Of course, if it can't you'd have bigger problems. Otherwise, just typical split-brain DNS scenario where you're answering most clients for what you have, but anything specifically to www.contoso.com you can forward to this other server. With the contoso.com it gets a wee bit hairier for kind for the same reason the other commenter mentioned. Your AD domain is using this. If your domain were instead a subdomain such as 'ad.contoso.com' you could also simply use a CF here. That, of course, assumes your dept1/dept examples are correct and not like dept1.ad.contoso.com If your servers are at least 2012 R2 I believe there is a new functionality for this: $URL$ Hope this helps. 

RAID5 and RAID6 can detect and usually correct bit corruption if you verify parity of the entire drive. This is called "scrubbing" or "parity checking" and typically takes 24-48 hours on most production RAID system. During that time performance may be significantly degraded. (Some systems allow the operator to prioritize scrubbing over read/write access or below it.) RAID6 has a higher chance of correcting it, because it can correct it if you have two drive failures, whereas RAID5 can only handle 1 drive failure, and drive failures are more likely when you are scrubbing because of the increased activity. 

RHEL 7 includes in the official repository. However, this module does not install mod_php. I have tried all of the approaches for installing rh-php70-* and none of them installs and registers the appropriate php module. How does one do it? 

It appears that Hive, Impala, Pig, and others all provide SQL or SQL-like access to data stored on Hadoop clusters. They all seem to have support for HDFS, S3, and other forms. So why are there so many different ways for accessing Hadoop information by SQL, how are they different, and how does their performance compare? Do we have so many different versions because all of the projects were started at the same time for more or less the same reason? If so, is there an advantage to knowing more than one of them? I have found several articles that attempt to explain the differences (e.g. 10 ways to query hadoop with SQL and Selecting the right SQL on Hadoop, but mostly they just list features. 

data 52e is the key, this means that the AD server is saying 'Invalid Credentials.'. I'm not readily familiar with JXplorer so I'm not sure how you set the credential for the session, but something is going awry there. 

I believe this will answer your question: $URL$ In short, windows permissions vary based on whether the target is a folder or a file. The tables in this explain it in detail. The information is dated but for the most part this should still be intact. This is a bit more up to date and applies now but a bit more complicated. $URL$ 

You could install a second NIC and multi-home the server if the software will support it. The risk to your AD domain depends on whether the credentials are used to log in to that server. If so, they are at risk and you probably want to manage it with an account that only has permissions to that box. Yes, someone owning it could be a further risk if you don't notice it, but as long as you're not logging into it with domain admins and the like, it wouldn't necessarily immediately compromise the whole domain. Long run though, it would be best to get the extra network gear and configure a DMZ and separate it from your internal network and open only absolutely required ports back to AD or anything else internal. 

We would like to have a network backup system with a user that can read any file on our servers but not write any file. Is there any way to do this under Linux (and specifically Fedora)? We would rather not have a remote that can erase any file... 

Is there a way to read the certificate in a consumer credit card EMV chip and use this to authenticate to an Apache web server using the client-side certificate support in SSL/TLS? 

I need to use a port that is already defined becasue I am upstream from a middlebox that only allows connections on ports allocated to specified services, and port 22 is blocked. 

An Amazon Machine Image contains an EBS volume. Is there a way to get the EBS volume out of the AMI without booting the AMI? 

What are the network protocols that can be used to measure the system clock (time) of a remote server? So far I have: 

The problem here is the is part of the modern and Centos 6.4 doesn't seem to have it. I've tried building my own but as soon as it gets installed and in my local I can't run any other programs, because all of the existing executables on the system try to link against it and they fail. I want to use the new compiler because it has dramatically better handling of C++ STL code, and because the optimizer in GCC 4.8 makes my code run in 1/2 the time as the GCC 4.4.7 compiler that comes with Centos. Any suggestions on how to do this? 

Sure. Create a GPO with a WMI filter scoped to Windows XP, and apply the deny logon interactively/locally rights to DOMAIN\Domain Users, that should prevent them from being able to logon. Although you might get some loud and rowdy responses, be careful of that if you have any concerns of more senior management frowning upon it. However, they cannot actually "hide" this from you. The operating system is included in the computer object and you can perform a powershell query to detect the systems that still report as that version of Windows. As I recall this value should update if they use install a new operating system. A powershell script that seeks out the values of each computer object's attributes called "OperatingSystem" and "OperatingSystemVersion" would tell you what you needed to know. For example on my workstation, these return: OperatingSystem: Windows 8.1 Enterprise OperatingSystemVersion: 6.3 (9600) (Major,minor and build#) Does that help? 

If I'm reading you right, then I guess it depends on how you're assigning IPs from your VPN and whether or not you have the capability of setting the DNS Server address, guessing you would but not every implementation is the same. You'd want to configure the zones that you want to be answered internally on that DNS server and leave recursion intact (although this is generally a bad idea if that DNS server is exposed to the public, as it opens you to potential denial of service attacks, and you might need to be more complex if so). This way your DNS server will answer what it can first and if it has no zone for the query it will forward to the internet roots by default. 

Amazon's marketing materials claim that the m4.16xlarge node has 64 vCPU. When I look at on the system, however, I get the following information: 

I am trying to get a modern GCC to compile on Centos 6.4. The problem is that Centos does not have a modern glibc and GCC 4.8.x and 4.7.x keep giving me the following compile error: 

We are using Lustre in a cluster with approximately 200TB of storage, 12 Object Storage Targets (that connect to a DDN storage system using QDR Infiniband), and roughly 160 quad and 8-core compute notes. Most of the users of this system have no problems at all, but my tasks are I/O intensive. When I run an array job that has 250-500 processes that are simultaneously pounding the file system typically between 10 and 20 of my processes will fail. The log files indicate that the load on the OSTs are going over 2 and that the Lustre client is returning either bad data or failed function calls. Currently the only way we have of resolving my problem is to run fewer simultaneous jobs. This is unsatisfactory, because there is no way to know in advance if my workload will be CPU-heavy or I/O heavy. Besides, just turning down the load isn't the way to run a supercomptuer: we would like it to run slower when running under load, not produce incorrect answers. I'd like to know how to configure Lustre so that clients block when the load on the OSTs goes too high, rather than having the clients get bad data. How do I configure Lustre to make the clients block? 

Not sure if this answers your question because there's not a lot of detail but this is just asking you how you'd like to configure this new domain controller. Do you want it to be a DNS server? Do you want it to be a Global Catalog? I'm guessing the plan is not to have an RODC since you already have one there, but those are recommended if the site does not have a secure computer closet or data center. You only have to have the DNS service on one domain controller technically, but in all honesty with most scenarios I set them up as both a DNS server AND as a Global Catalog. DNS for the local site if there are enough users, and Global Catalog so it can answer queries through 3268/3269 TCP (GC queries). The GC is a read-only port that returns partial data for your entire forest, if you have a multi-domain environment this is especially useful. Old documentation says you should only use it as you need to, but this was based on slow connections in days gone by, these days with reliable network speeds that's usually a non-issue.