In SSMS right click on Database, select Reports, Standard Reports, Disk Usage by Top Tables. The report will give you number of rows and kilobytes used per table. 

create a separate db and move your log tables there. set a max file size. create a trigger which checks row count or table size and limits inserts when value exceeded and/or deletes old data. create a foreign key constraint on an id column pointing to a table with a set number of rows. When your log table reaches the maximum it will stop. add a check constraint. 

SELECT REPLACE(value,'apple','orange') from table1 Or to update: Update table1 set value = REPLACE(value, 'apple', 'orange') If you have multiple substitutions you could use a cursor and variables. 

If that value is unique and is used to identify a single row in a table ( in your example to uniquely identify a single client record) it would typically be considered a unique or primary key. If it has some meaning and you already have a separate primary key it could could also be described as a business key or natural key. Without a constraint to enforce uniqueness you cant guarantee the data is unique. You should be careful to avoid developing with that as an assumption. 

Another reason to use a data warehouse or dedicated reporting server is to avoid placing load on a production OLTP server. Or if you have to write reports which include data from different systems. The way you have described your db I doubt there would be much advantage in denormalising unless you wanted to build OLAP cubes on it. In which case an OLAP friendly schema might have significant benefits. 

The scheduled cache refresh at 08:00:19 & 08:10:19 Followed by my report request about 8:15:01 I don't get the report back until about 8:17:06 

What you are describing is data migration. To do this successfully you need to understand both the old data and new schemas, business rules and the data itself. To migrate the data the most common option is to write conversion scripts which insert data into the new schema that respect the new schemas rules, referential integrity and business logic. If your old data isnt a perfect match for the new schema you may lose data, or you may have to fabricate data to fullfil the new schemas requirements. Data migration can be complex and time consuming and requires testing in a safe environment. The right tool for the jobs depends on several things like how often you expect to do this. Is it a "one-off" or will you be repeating this many times for customers? Edit: Given comments below. I'd import the data into a separate db on same server. Id then write sql scripts which insert into new tables. Id probably use a staging data base that has a copy if the new schema to ensure i met all of the constraints and data type rules. After i had tested that and was happy I would insert each table into the target. The order that you migrate each table will be important because of parent/child relationships. Queries could be as simple as: Insert targetdb.target_table (fieldA, fieldB) Select field1, field2 From sourcedb.source_table 

It's making me consider performance tuning of my ETL process and the pros and cons of temporary tables and how you could influence execution plan behavior. My ETL process uses a number of stored procedures which use mix of standard and temporary #tables as staging tables. The #tables are typically used once and then dropped. Some are only a few thousand rows. Some are millions. SSMS advises that there are missing indexes, but on smaller tables would they make enough of difference to be worth the effort of adding them? Are better statistics sufficient? I've just read this Brent Ozar blog post about Statistics on Temp tables, and Paul White's article on Temporary Tables in Stored procedures It says that statistics are created automatically, when the #table is queried, and then presumably used by the optimizer. My questions are: Is there much point or benefit in creating an index on a #table. And/or: Is it worth explicitly updating statistics as a step in the stored procedure before using it in queries given they're only used once. Are the additional steps and overhead worth it? Would it result in significantly better or different execution plans? 

If you don't want to do this, then you will have to find a way to do this periodically as you suggest. You could write a script to up date all statistics, or as you suggested use Ola Hallengren's maintenance scripts to update all stats. 

If the field is NULL, Do I need to update the field first to insert a base for the xml i.e before I can insert to it? Whats the name of an XML node that doesn't use an opening and closing pair. Are there other/better methods of inserting nodes into XML using SQL? 

I think we solved this or at least found a workaround. I believe the problem is a timeout when connecting to SSAS when it has been inactive for some time. It has nothing to do with the actual processing of the cube. We added a step to our ETL process which queried the OLAP cube before the processing step was triggered. This causes SSAS to 'wake up' before it is needed. Since we added this step we haven't had a repeat of this problem. 

This is difficult question to answer. The right solution for you will depend upon several factors such as cost and availablity of resources and expertise. But the solution should be driven by the requirements. Before you try to select a technical solution it is worth considering what the end result should look like. Where is the data going and for what purpose? Do you want reporting and analytics? Who is going to use this data and how? (Executives? Analysts? Marketing? Sales? Customer Service?) What level detail does each group need? (Raw ? Aggregated?) How often? When? How fast? Does it need to be integrated with Salesforce? If you decide the solution is a data warehouse I would seriously consider a cloud based alternative. As you already have cloud based software and you are spread across different countries. 

You mention that you already have a piece of sql that does what you want. So i would suggest you modify that to become an update statement and implement it as a sql task. Another option is to save the update within a stored proc and call/execute it from ssis. With a bit of thought you could probably write an update statement that only sets the values for new records or for records that have changed. Or perhaps do it as part of your ETL insert/merge. However going back to my other comments: i don't think it makes sense to have a single key or dimension for such diverse attributes. I suggest you have a look at some sample schemas to see if it could be done differently. 

I'd like to create a SQL agent job to restart SSRS each day. In the past i've used a Powershell script executed by a Scheduled task taken from here. This has worked fine, but I would prefer it to be a single job in SQL agent to simplify deployment and admin maintenance. I have taken the powershell code and added a job with a single step. 

As your data is already multi dimensional i would create a schema that follows along those lines using a datawarehouse star schema with a central fact table. In your example the fact table would have the lat, long, pressure, timestamp, observed value and attributes. If you wanted to extend your model to use an olap cube you could add a time dimension and others as needed. 

The short answer is: define your end result ( requirement) and work backwards from there. Normalization goes out the window pretty quick with a DW schema. In my case we have 3 schemas for our ETL. 

What do you mean by archive? Do you want to delete the data? Moving it to another db or server will still take up the same space. It may improve performance but you're just moving the problem around. Do you need the data in its original form? Can you aggregate the data to reduce the number of rows? If you do move the data to another table you may find a clustered columnstore may give better compression and save disk space. Personally i would be using ssis and not a sp using linked servers. As you have a primary key id be using that as a way of identifying what has been inserted and then can be deleted from the original table. An index on the column you are using for date range would be extremely beneficial. Id also consider using smaller batches such as one days worth at a time. 

I would highly recommend that you take the time and effort required to connect your app to a newer version of SQL, preferably one that is officially supported. i.e 2012+. (SQL 2000, 2005 and 2008 and 2008R2 are all officially obsolete.) If you have issues with deprecated features or connectivity, SQL 2008R2 supports database compatibility back to 2000. It's still obsolete, but is still a much more sophisticated product than 2000. Edit: Technically Support for 2008 & 2008R2 is still available from Microsoft through their MS Lifecycle Support Site. In most cases apps are oblivious to which version of SQL they are on as long as they can connect and the features and functions still work. Even if you can't do a backup and restore, you should be able to create a new db and use SSIS to import the data. If this is a current application I highly recommend you go back to the vendors for assistance and ask/insist they support current db's. Requiring a 17 yr old db to be able to install or run their software is terrible. If this is a legacy app and no longer supported I suspect you have a much bigger issue looming than just moving a db. If this is a legacy app and you just want to keep it running but need to replace hardware, I would consider virtualising the server. 

A select on a single table should always be faster. As soon as you have found your vehicle you already have all the details. However you lose the efficiency of normalization. For example if 1 car had many models with different options. Is this a reference db of all cars? Or a list of second hand vehicles? Would there be many examples of the same make/model with the same options? Edit: i should qualify my answer as being generic rdbms rather than postgres specific. I defer to @Erwin's detailed answer specific to postgres 

You need to refer to the parameters in your main sql query. Something like: Where databaseName = @databases Or if you want to be able to select multiple databases. Where databaseName in (@databases) Here is an example using date parameters and multi-select parameters in a report $URL$ If you Google " ssrs parameter sql query" there are lots of working examples and even some youtube videos giving step by step instructions. 

Systems I have worked with in the past stored for each customer a row for each recurring product they have. Each row has a product id, start date and anticipated end date, and a "billed to date". Each month when you bill them you increment the date. From that you generate an invoice (storing the values to make up the invoice), and they customer makes payments against that. Their balance is calculated from opening balance + invoices +/- payments. It gets complicated when product prices change over time, or discounts only apply for a specific period. You need to store enough detail that you can recreate the invoice correctly at that point in time. If you give a refund that you don't want to refund too much, and if required can you pro-rate adjustments? A good approach is to ask yourself what level of detail do you need to see on an invoice. If I had to recreate the invoice, adjust it or provide a refund, where would I get the data from. You need to either store the detail or be able to reproduce it reliably. Can you? The next level to consider is, what accounting information to I need to report? How much did we sell last month? what discounts applied? how many customers have paid? who owes us money and how much?(debt mngt) How much tax have i charged? (is anything exempt? different rates?) Look and the end results you need and work back from there. 

What you are asking about is broad architecture rather than specific issues. I dont believe there will be a single document that gives you a step by step guide. If your vendor is making this reccomendation you could ask them to explain thier design with at least some basic documentation and diagrams that help you understand the layout. If you are unfamiliar with either of these products you have a big learning curve ahead of you. MSDN is probably the best place to start. If you have specific issues then you should google those. Assuming your main database is already running, i would start by installing an instance for SSRS. Then work through designing and deploying a report that connects to your source db. Then repeat for SSIS. Set up the server and design and deploy a basic package. A key part of learning about both SSRS & SSIS is learning about datasources, connection managers and connection strings. If you are unfamiliar with these concepts and your company is expecting you to install configure and develop with these products they might be expecting too much of you and I suggest that you ask for assistance. 

Depending upon the size of the table you could create an SSIS (.dtsx) package and execute in on the target server. This would also avoid the use of the linked server. You could create this with the import wizard in SSMS and save the package. If the table is massive, there are a few options and features within SSIS that will help you break the job into more manageable chunks. SSIS can also handle bulk insert. If this table is the only object in the target database you could also consider putting the database into simple recovery mode or bulk logged. That may help minimize logging while you are copying data. Provided you are correct that the source table is only inserted to, and not updated this should be relatively straight forward. After the copy you could use a linked server query to compare. (outer join on primary key) BCP may have better performance, but db->db has the advantage of 1 less step and the storage requirements that go with it. The biggest advantage is that the ssis approach can be interrupted and you could easily resume where you left off.