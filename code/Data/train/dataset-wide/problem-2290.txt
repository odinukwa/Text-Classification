I had a database that was working in FULL recovery mode but never had transaction log backups done. The consequence of that was the fact that the log file kept growing and now its 10 GB in size. I configured scheduled backups for this database (full + differential + log) and now log % space used is around 1%. I want to shrink the 10GB file to a sensible size (maybe 200MB with autogrow 5 MB?). How can I do it and what are the consequences of shrinking the file? 

Create a network share and assign permissions (I go with full control) to the SQL Server database engine service account (it has to be a domain account) Backup database to this share like that: 

Do not use the SHRINK option. It results in serious performance degradation. Also after you shrink the data file, it will later grow even more. Please watch this video by Klaus Aschenbrenner (Microsoft Certified Master in SQL Server): SQL Server Quickie - SHRINK I would recommend to kill the SPID but be ready for the rollback operation. It can take another 3 hours, maybe less. 

I have a sample query generated by an application (Microsoft Dynamics AX 2012 in this case) which is ineffective performance-wise (cross joins, order by etc.) I would like to display its execution plan and attempt to tune it by indexing or rewriting some parts of it. I cannot just copy/paste it into SSMS because there are numerous parameters of many data types. I don't even know what are the values for these parameters. Is there a way to quickly identify the execution plan of this query? Maybe by querying some DMVs? I got the query text from monitoring software which must have done it. 

I have a few production SQL Server instances and I get around 100 deadlock alert emails a day (up to 100 for one server, 150 in total). Is this a problem I should investigate? How many deadlocks are problematic and what can be the possible consequence of ignoring this issue? 

There are multiple backup options (eg. WITH COPY_ONLY). I don't think however that you should list any other than full/diff/log as "other kinds of backups". 

pg_dump contains only the SQL statements required to recreate the database. The actual data files contain the data you inserted and all other database objects, especially indexes: clustered indexes (the data itself) and nonclustered indexes: selected columns sorted by a specified key. The pd_dump contains a CREATE INDEX statement and the data files contain the index itself (it may be very large). 

and Microsoft websites, there are 4 Cumulative Updates available for SQL Server 2012 SP2 and the newest build is 11.00.9000. Why didn't Windows Update get Cumulative Updates too? Should I download and install them manually? 

This is a simplified version of the query. I have multiple subqueries, often referring to the same table under different conditions. If I didn't use subqueries I would get many rows for one customer but I need just one row with all the available data. How can I rewrite this query so it runs fast? The current version generates high CPU usage, over 200MB/s IO operations and it takes around 6 hours to collect data about 70 000 customers. 

This way you should get the data you need and then you can try to pivot the table to present it like you want. I didn't test the queries so let me know if it doesn't work. 

I'm a DBA who is responsible for exporting data from an ERP system for Business Intelligence purposes. The data from ERP system is being sent to the data warehouse and they are used in the ETL process. We used to run the export process at the end of the day. The queries which collect the data used the WHERE clause with a document creation/modification date = yesterday. This mostly works just fine - there isn't too much data to export from the source system (less resource consuming on both source and destination systems). But some source tables don't have the modification date column (only the create date). At the moment I can see only 2 possibilities: 

SQL Server backup operations do not acquire locks, see - Paul Randal's post. Therefore or cannot become deadlock victims. Probably some other queries that take part in your backup jobs take part in deadlocks. If you can describe your backup solution in detail, we might be able to help you. I personally recommend Ola Hallengren's backup solution to do the job. 

For production instances it is recommended to create separate partitions for the following elements of SQL Server: 

I have manually resized mdf/ndf files to a big size to avoid autogrow operations on SQL Server databases. Since the files are bigger there is very little free space on disk partitions and the sysadmins keep alerting me that I'm running out of space. Because I resized them, there is a lot of free space in the data files but one can't notice it looking at file sizes/disk free space. How can I monitor the real % usage of data files? I would prefer using perfmon counters. I am conerned that when the file really runs out of space SQL Server won't be able to allocate enough space and will crash. 

The VMs I run SQL Server on are regularly backed up by the VMware backup tool. I would prefer not to use this backup solution but I have no choice but to cope with it. The problem I've come up across is the fact that every time the VM backup us ran, SQL Server thinks it has performed a full backup of the databases to a virtual device (physical_device_name = GUID). I can see those backups in msdb (they are copy_only backups so I don't have to worry). Do you know why it works this way? Is VMware aware of SQL Server existence? Or maybe SQL Server has wrong data in msdb? Can I run into any problems with that solution (other than trying to restore the DB from the virtual device)? 

You definetely have to separate the files logically using different partitions (I recommend separate partitions for: system, data files (mdf/ndf), transaction log files (ldf) and tempdb files (mdf/ndf). Where you put the tempdb transaction log file is up to you (tempdb drive or log drive), I'd go with log drive. Backup files should not be on the located same device and especially the same VM (I recommend a different SAN/NAS). Placing your data, log and tempdb files on different drives is mostly a way of improving performance. Transaction log is heavily written and is probably the slowest component of SQL Server. The second bottleneck is by design the tempdb database. If these 2 components are placed on different drives (especially if they reside on SSD/RAID10 disks), the performance will be definitely better. You can keep the transaction log and tempdb on the same array as the user database files if you arenâ€™t experiencing performance problems (at least not yet). I would look at the sys.dm_io_virtual_file_stats DMV to verify if there really is a problem with IO performance. If you want to move these files to a different disk in a virtual environment you can create a separate virtual disk for every partition you want to move and assign these disks to different LUNs in the array. 

First it is ranked by the case statement and then by the second predicate which is specified - date. Look at this code sample: 

I have read a lot about disaster recovery planning of databases (mainly SQL Server). There are multiple articles/book chapters about backing up databases and recovering them from backup. But what if the operating system fails? After you bring it up you will restore the databases fairly quickly and successfully (if you were prepared for that). What are the options when it comes to recovering OS in: