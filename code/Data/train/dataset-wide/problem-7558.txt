The Strong Law of Large Numbers guarantees almost sure convergence of the sample mean to the population mean. If your distribution has large variance then yes the convergence is slower. However, the probability of being away from the population mean is bounded by: $P(|s_n-\mu|>\epsilon)<\frac{\sigma^2}{n\epsilon^2}$ Where $\mu$ and $\sigma$ are true mean and standard deviation and $s_n$ is the sample mean from $n$ points. 

Singular Value Decomposition, probably one of the most useful and ubiquitous concepts out there. Half the time can be devoted to listing all the synonyms it goes by in various fields such as statistics and finance. 

The pair correlation function between Riemann zeta function zeros is the same as the pair correlation function between eigenvalues of random Hermitian matrices. 

Let $M$ come from an ensemble of $N\times N$ matrices. The Wigner surmise is density function $p^W_0(s)=\frac{\pi}{2}se^{-\pi s^2/4}$. From a random matrix point of view, we can write $\rho^W_0(s)=\frac{d^2}{ds^2}E((0,s))$, where $E(I)$ is the eigenvalue gap probability: $M$ has no eigenvalues in interval $I$. $E(I)$ and it's derivatives are intimately related to the correlations between nearest neighbors. 

Is there a sensible and useful definition of units in mathematics? In other words, is there a theory of dimensional analysis for mathematics? In physics, an extremely useful tool is the Buckingham Pi theorem. This allows for surprisingly accurate estimates that can predict on what parameters a quantity depends on. Examples are numerous and can be found in this short reference. One such application (pages 6-7 of the last reference) can derive the dispersion relation exactly for short water ripples in deep water, in terms of surface tension, density and wave number. In this case an exact relation is derived, but in general one expects to be off by a constant. The point is that this gives quick insight into an otherwise complex problem. My question is: can similar techniques be used in mathematics? I envision that one application could be to derive asymptotic results for say, ode's and pde's under certain asymptotic assumptions for the coefficients involved. For any kind of Fourier analysis, dimensions naturally creep up from physics if we think about say, time and frequency. I find myself constantly using basic dimensional analysis just as a sanity check on whether a quantity makes sense in these areas. Otherwise, let's say I'm working on a problem involving some estimate on a number theoretic function. If I have a free parameter, can I quickly figure out the order of the quantity i'm interested in in terms of my other fixed parameters? 

How about The Theory of Remainders by Andrea Rothbart. I remember back in the day I was struggling with the concept of modular arithmetic and randomly came across the book above. It's really well written in an unorthodox way as a dialogue between two people talking about modular arithmetic. The book introduces basic concepts of abstract algebra and has plenty of "simple, but serious" exercises. If I recall correctly, it did a really good job of motivating the concept of fields. Above anything, it was written with a high school audience in mind, so incoming freshmen should not be deterred by the level of difficulty. I also found the style of the book engaging. I dare say I was bitten by the number theory bug shortly after reading it. 

In this paper by Nordenstam, it is shown that a certain interlacing particle process that arises from uniformly random Aztec diamond tilings is amazingly similar to Warren's process. One of the results (theorem 1.1) is that each level converges to a Dyson Brownian. The main conjecture is that the full process converges to Warren's process. I have two questions concerning this. 

Let $S_n$ be the symmetric group. For any sequence of numbers $y=[y_1,y_2,\cdots,y_k]$, define the flattening operation as $\mbox{flatt}_{k}(y)$ as a relabeling of $y_1,y_2,\cdots,y_k$ in terms of their relative order with numbers from $[k]$. So for example $\mbox{flatt}_3([4,2,5])=[2,1,3]$, $\ \mbox{flatt}_5([2,1,3,4,5])=[2,1,3,4,5]$. For a permutation $\pi\in S_n$, denote by $\pi(2,n-1):=[\pi(2),\pi(3),\cdots,\pi(n-1)]$ Define $X_1:=\{[1]\}$, $X_2:=\{[2,1]\}$. Now define $$X_n=\{\pi\in S_n:\ \pi(1)=\pi(n)+1, \ \mbox{flatt}_{n-2}(\pi(2,n-1))\in X_{n-2} \}$$ For example, $$X_1:=\{[1]\}$$ $$X_2:=\{[2,1]\}$$ $$X_3=\{[3,1,2],[2,3,1]\}$$ $$X_4=\{[4,2,1,3],[3,4,1,2],[2,4,3,1]\}.$$ It's easy to see $|X_n|=(n-1)!!$. Does this class of permutations have a common name? Have they been studied before? I've checked the lists at oeis.org (by cardinality) and findstat.org but didn't see anything. Edit: Here's the OEIS for (2n-1)!!, the even $X_{2n}$'s. 

If you interested in some back-of-the-hand order of magnitude estimates, you might consider looking at how $\binom{n}{k}$ behaves when $k=k(n)$ has a certain size. The idea I have in mind is to break down $\sum_{k=0}^m\binom{n}{k}$ into a sum over intervals of $k$ satisfying a certain regime. For example, look at terms where $k=\Theta(n)$, $k=\Theta(n^{1/2})$, etc. In general, using Stirling's approximation, you'll get: $\binom{n}{k}=\frac{n^ke^k}{k^k\sqrt{2\pi k}} A$ where $A:=\frac{n_{k}}{k^k}=\prod_{i=0}^{k-1}\left(1-\frac{i}{n}\right)$ and $n_k$ is the falling factorial. In particular, it's nicer to work with $B:=\ln(A) = \sum_{i=0}^{k-1} \ln\left(1-\frac{i}{n}\right)$. Now the idea is that each of the logarithm terms in $B$ can be Taylor expanded up to "sufficient" order depending on the size of $k$ compared to $n$. For example if $k=o(1)$, then $B\approx \sum_{i=0}^{k-1}\approx -\frac{k^2}{2n}$, so you get $A=e^{-\frac{k^2}{2n}(1+o(1))}$. In fact, you can do better than this if you expand $B$ to higher orders. In particular, if $k=o(n^{2/3})$, then $B=\sum_{i=0}^{k-1}-\frac{i}{n}+O(i^2n^{-2})=-\frac{k^2}{2n}+o(1)$ which gives $A=e^{-\frac{k^2}{2n}}(1+o(1))$ where now the $o(1)$ is no longer exponentiated. For other sizes of $k$, the exact same procedure works as long as you expand $B$ to sufficiently high order. 

The holographic principle has been mentioned so I'll just add a little more info from the physics perspective. The Bekenstein entropy bound implies that there is a finite number of information (entropy) in a finite volume of space with finite energy. Speaking in terms of entropy, one can see for example that there must be a limit on how many fundamental particles there are. The reasoning is that given a particle made up of sub particles, the total number of degrees of freedom of the particle is the product of the degrees of freedom of each sub particle (are there sub particles with only one degree of freedom?). Since the total number of degrees of freedom must be finite, this implies that one cannot subdivide particles forever. There are some particularly striking consequences of these entropy bounds. For example, Verlinde argues that the force of gravity between particles is a result of the holographic principle. This can be thought of as (indirect) physical evidence of the holographic principle at work. 

The point is that $\mathcal{C}$ gives a kind of functional equation. It seems that uniqueness will not be guaranteed on sets which do not contain any independent subsets. In particular, null sets could screw things up. However, what if for every $A\in\mathcal{F}$ with $P(A)>0$, I were to guarantee the existence of a $B\in\mathcal{F}$ with $0 < P(B) < 1$ such that $A$ and $B$ are independent? For example, suppose $\Omega$ has cardinality $n$. Then there are $n$ points on which to define $P$ and one constraint: $P(\Omega)=1$. I would then need $n-1$ equations to determine $P$. So when $\Omega$ is finite, things are much easier. My motivation for asking this is to better understand the concept of independence. 

The following isn't an example of showing directly that the coefficients are symmetric but, establishing that the generating function is symmetric is instrumental in the calculation of an auxiliary quantity involved. I think an amazing example of this kind of proof is Stanley's proof for the number of reduced decompositions of the long permutation. Specifically because a bijective proof of this fact (such as Edelman and Greene's) is considerably harder to establish. Moreover, this proof uses what seems like a minimal amount of knowledge about reduced decompositions (again compared to the bijective proof). What follows is a very rough outline of the proof. See "On the Number of Reduced Decompositions of Elements of Coxeter Groups - Stanley" for details. Setup: Let $S_n$ be the symmetric group and $s_i:=(i,i+1)$ be an adjacent transposition. For any permutation $w$, write $w=s_{a_1}s_{a_2}\cdots s_{a_p}$, where $p$ is equal to the length of $w$ (the number of inversions in $w$). Call this sequence $(a_1,\cdots,a_p)$ a reduced decomposition of $w$, and denote by $\mbox{Red}(w)$ the set of all reduced decompositions of $w$. Let $w_0:=(n,n-1,\cdots,1)$ be the long permutation (the one with the most inversions in $S_n$, so $p=\binom{n}{2}$). For example, in $S_3$, if $w=321$, then $$\mbox{Red}(w)=\{(1,2,1),(2,1,2)\},$$ so $|\mbox{Red}(w)|=2$. Let $\lambda:=\lambda(n)$ be a tableau shape $(n-1,n-2\cdots,1)$ (a staircase tableau) and $f_\lambda$ to be the number of Standard Young tableau of shape $\lambda$. Theorem (Stanley): $|\mbox{Red}(w_0)|=f_\lambda$ Sketch of Proof: The key idea is to define the so called Stanley Symmetric function (the umbrella term for these is Gessel quasisymmetric functions). For any $w\in S_n$, again let $p$ be the length of $w$. Then define $$F_w(x_1,\cdots,x_p)=\sum_{(a_1,\cdots,a_p)\in \mbox{Red}(w)}\sum_{\substack{1\leq k_1\leq k_2\cdots\leq k_p\\ k_i<k_{i+1} \ \mbox{if } a_i<a_{i+1}}}x_{k_1}x_{k_2}\cdots x_{k_p}.$$ An immediate observation is that the coefficeint $x_1\cdots x_p$ occurs exactly once for each reduced decomposition of $w$. Thus we're interested in calculating $[x_1\cdots x_p]F_w(x_1,\cdots, x_p)$. Key Theorem: $F_w(x_1,\cdots,x_p)$ is symmetric in $x_i$'s. The original proof of this theorem is purely constructive and relies on the fact that adjacent transpositions form a Coxeter group, along with a number of facts of how one can, in a sense, decompose a permutation $w$ uniquely into an increasing product of adjacent transpositions. Assuming the above theorem for a moment, we can expand $F_w$ in terms of Schur polynomials as: $$F_w(x_1,\cdots,x_p)=\sum_{\lambda \vdash p}\alpha_{\lambda p}s_\lambda(x_1,\cdots,x_p),$$ where one can further show that $\alpha_{p\lambda}$ are nonnegative integers. Since $[x_1\cdots x_p]s_\lambda(x_1,\cdots,x_p)=f_\lambda$, we get: $$|\mbox{Red}(w)|=\sum_{\lambda \vdash p}\alpha_{p\lambda}f_\lambda.$$ The final step is to show that when $w=w_0$ is the long permutation, that everything in the sum vanishes except for the staircase partition $(n-1,n-2,\cdots,1)$ and that the corresponding $\alpha$ for this partition is simply 1. This is done by showing that this particular staircase partition encodes the inversion structure of the long permutation $w_0$. Specifically, the only partitions that survive in the above sum are intimately related to the patterns that occur in the corresponding permutation. As well, the $\alpha$'s are also connected in this way. There are still some details left but the proof follows through. 

Let $\mathbb{Z}^d$ be the usual $d$-dimensional lattice and let $\mathbb{H}:=\mathbb{Z}^{d-1}\times Z_+$, where $Z_+:=[0,1,2,\ldots]$. If we now consider bond percolation on $\mathbb{H}$, it is a well known result that at the critical probability for $\mathbb{H}$, $p_c(\mathbb{H})$, equals the usual critical probability $p_c$ for the $d$ dimensional lattice. Following Grimmett's Percolation textbook, we learn that for $d\geq 2$, $\theta_\mathbb{H}(p_c)=0$, where $\theta_\mathbb{H}$ is the usual probability of 0 connecting to infinity, in this case for the half-space. The conclusion of this is that if $\theta(p_c)>0$ for $\mathbb{Z}^d$ percolation, then there exists a.s. a unique infinite open cluster in $\mathbb{Z}^d$, which is a.s. partitioned into only finite clusters by ANY division of $\mathbb{Z}^d$ into two half-spaces. My question is, how does this fall short of proving say, $\theta(p_c)=0$ for $d=3$ and so on? It's hard for me to imagine configurations of an infinite cluster even in 3-dimensions that satisfy the slicing criteria above. If I understand correctly, the cluster would need to spiral out radially in all directions and at the very least cross the $x-y$, $y-z$ and $x-z$ planes infinitely many times. It's easy to see that such a construction fails in two dimensions (and rightfully so, $\theta(p_c)=0$ for $d=2$). Where does the difficulty lay? Or rather, if we attempt to rigorously approach a proof in this direction where do we get stuck? 

Let $T$ be a uniformly random Standard Young Tableau (SYT) of shape $\lambda=(\lambda_1,\cdots,\lambda_k)$ with $|\lambda|=n$. Let $T_{ij}$ denote the value in box $(i,j)$. I'm interested in what can be said about correlations like $P(T_{ij}=x,T_{kl}=y)$. Very roughly speaking, if $|\lambda|=n$ is quite large and the $\lambda_i$'s are also large, then I would expect that $P(T_{ij}=x,T_{kl}=y)\approx P(T_{ij}=x)P(T_{kl}=y)$ for $(i,j)$ far apart from $(k,l)$ and for $x,y$ "not unreasonable" (see next part). As a working example lets just take a square partition $\lambda=(n,n,\cdots,n)$ with $|\lambda|=n^2$. We know that such a partition has a well defined limit shape, so that if $x\approx an^2,y\approx bn^2$, then we expect $x,y$ to fall on limit curves, via what's more-or-less a semicircle distribution, i.e. being more likely to to fall near the middle of the limit curve than at the ends. My question is, on what scales do we know the error term $\delta$ of: $$P(T_{ij}=x,T_{kl}=y)=P(T_{ij}=x)P(T_{kl}=y)+\delta_{ij,kl}(x,y)?$$ In other words, what does $\delta$ look like for $x,y=\Omega(n^2)$ and in particular when $|x-y|=o(n^2)$ or $|i-k|^2+|j-l|^2=o(n)$? Specifically, what is the order of $\delta$ in terms of $n$ and $(i,j),(k,l)$?. I'm purposely leaving a lot of room for interpretation because I'm guessing overall these are very difficult questions to answer for all possible cases of $x,y$.