I would like to create a trigger before inserts and updates to ignore any value given by these statements and instead use the next value in sequence - as if no value was defined in that statement. I don't want to use since this throws an error. I would rather ignore any value given and use the next possible value. What I am looking for is a way to identify the sequence used for this identity ans use in a before-insert/update trigger. In this article a SQL statement is given to get the name of the sequence that is used for the identity which I maybe could use within the trigger (???) but the statement doesn't work for me: 

But this goes well only for a short time. After couple of minutes it starts to throw ORA-01110 when I connect to the first instance. The data files are not missing. They are in the location in which Oracle reports them missing. 

error. Also confusing was that if the pfile was inside the Oracle Directory the command failed saying it cannot access the file. EDIT After I have deleted both instances, rebooted and created both instances again I can currently access them both. When I try to login to the first instance I get the following error: 

EDIT The Mirosoft Service Manager reports both inscantec as runngin. Setting ORACLE_SID does not make any difference. After the installation of both instances I can connect via 

If the statement would work I would have the sequene name as a string but I guess I would need the object itself to request the nextval. Is a trigger the right way to go or should I look for something else? For Oracle 11g I have implemented this functionality using trigger and sequences. The trigger looked like this for inserts: 

I have found the problem which was that the databases started by the Windows service manager were started with a user that appears not to have sufficient permissions. This is confusing since I have selected the default setting while installing to use Oracles virtual user. This virtual user seems not to have write permission to some files of the database. I have changed the user by which the services are started from Oracles virtual user to the Windows "Local System" option. Since that both databases are running seemingly fine. 

this is a clean design but if each type of thing generates ten or twenty log entries per thing you don't have to do too much to have a huge table for application_logging with a million entries. Users complain it's slow to see the activity log. The real question is: 

1) When to be sure that your database design is perfect? Your design is never perfect because the business logic and amount of data is always changing. Perfect is difficult to define I've seen systems that were great on deployment but had poor performance after a few years of data were added. The regrettable trend to treat a database like a black box by some application developers means some databases are deployed with critical tables lacking primary keys or indexes. Perfect to the CIO because they got the application delivered on time and on budget could be a pain in the butt to the developer/ DBA who has to deal with the problems. Here are some of indicators I would look for which indicate your design is ready to go a) Extensive use of primary keys, unique keys, foreign keys, indexes, more so on the larger tables ( I only mention this because I've seen commercial products which lack this) b) Application logic is duplicated as far as is practical in the database with the use of constraints and default values. You cannot capture everything but just knowing that there will always be a value for an entry provides peace of mind. c) Test, test, test: test from the user perspective on data entry, test from the manager perspective who wants an overview, test from the analyst perspective who wants to see trends If you find yourself joining 9 large tables which require full table scans in order to find out what work is assigned to a user then maybe you need to reexamine things. Not everything can be simple but excessive complexity to answer user needs is a hint of trouble to come. d) do some daydreaming on how the database could and could not be extended. If I need to add a new property to a unit of work how hard is it? Can it be done without table changes? If you are asked to add a new type of work how hard is it. (Work could be a product, a transaction, a case) e) people and organizations provide hours of work for me. How easy is it to create, edit, de-duplicate and report on them? f) how many user languages are you supporting? What character sets will be required. If, for example, you intend to support English and Spanish, what happens if the design must be extended to cover French and Italian? 2) Is returning to the data base design to change some issues (like adding new column, delete a column or change data type or add new table or ....) considered as a bad practice or is it normal? I would say it is normal for an application where the business logic changes frequently or the end user requirements are being added to. 

Hopefully, there will be a reliable way to copy all the backup files to a single folder -- even when we add a new database. EDIT: changed title of the question - used to be "Move SQL Server backup files to a single folder" EDIT: added option #4 ("find new FTP software...") 

The database and data warehouse servers are on separate servers – which we are planning to move to SQL Azure. So, how do I know when to run the ETL for dimensions? And, any suggestions on technology would be helpful (we are currently using SQL Server linked servers). 

We plan to backup our SQL Server 2012 databases every night, FTP the backup files to our test servers and restore them. Then deploy the latest schemas to those databases. The reason for this is so we have the latest data with the latest schemas to test on. I'm using Ola Hallengren's SQL Server Maintenance Solution to back up the databases and Core FTP to FTP the backup files. The problem is that Ola Hallengren's SQL Server Maintenance Solution puts each backup file in a different folder and the Core FTP has been setup to FTP all the backup files from a single folder. As I see it, I can: 

On a schedule, checking all the definitions on the database server with dimensions and updating any changes. And assigning transactions to “Unknown” is the dimension hasn’t arrived yet. Putting triggers on all the definition table and updating the data warehouse as the changes happen Updating the stored procedures to update the data warehouse every time a change is made Using something like Microsoft Sync Framework, and incorporating the transform in there. 

For SQL Data Collector: Do you need SSIS (SQL Server Integration Services) installed on the server that you are collecting data from. Or can the packages be run from another server -- like the one that the MDW database is on. 

There are multiple solutions to this problem but the driving issues are cost, allowable downtime and complexity. 

By Oracle 11 everything you can do with a CTXCAT index you can do more with a CONTENT index. Why not use that? Edit: The op asks if CONTENT indexes must be synced. Yes, this is correct. I find the time and load factor to do this is entirely acceptable for smaller records sets. Your mileage may vary based on record size. Further reference: 

I believe this will do the job but the databases I have access to at the moment do not have text indexes. I will confirm later today. 

I upgraded a 9i install to 11g with only one problem. Database links between 11g and our legacy 8i database are no longer supported. I have to transfer data from 11g to 8i two or three times a day so I thought it might be possible to use 10g express to link the databases. The data would be created in 11g, written to a table in 10g and then written to 8i. (I agree this is not the most elegant solution and I can improve it using advanced queue tables). Does 10g express support database links to Oracle 8i? Edit: thanks for the link to the Oracle documentation. Has anyone actually done this with 10g express? Edit: The 8i database is Oracle8i Enterprise Edition Release 8.1.7.0.0 - Production The intermediate would be the 10g express for windows The origin database is Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - Production 

Grant select on DBA_SCHEDULER_JOBS TO the owner of the schema where the function resides. The code needs the permission to be granted directly. If you are running as user other than the one who owns the function you must also grant execute on fn_gss_chk_scheduled_jobs to your_user; 

This is not a complete solution as you need to create a type for the message and a queue, and grant permissions to manage the queue. The last portion of the solution is to pop the messages off the queue and insert the changes into DB2. 

We have a suite of restaurant web applications that stores data in SQL Server 2012 on database server. We have now started to move data for reporting onto a separate data warehouse server. The data needs to get into the data warehouse quite soon after being updated/created. Knowing when to move transactions to the data warehouse (and into fact tables) is easy -- we do that periodically or when certain events occur (like the manager finalising something). But how do I know when to move the definitions to the data warehouse (and into dimension tables)? We are storing definitions as SCD Type 1. We’re considering... 

change the location the databases are backed up to change the way Core FTP is setup, or add some SQL script to the main store procedure to copy the backup files to a single folder find new FTP software that can FTP files from many subfolders with a single folder (any suggestions?) 

On our SQL Server, we have a database for each of our web apps. For reports, we use Reporting Services and all report data (including report parameters) come from stored procedures. The stored procedures are in the same database as the data in the report. So, for example, the procs that serve the Stock reports are in the Stock database. Some reports show information from more than one database and then the proc will be in one of those source databases. The report parameters get their data from procs in an Enterprise database that has data like stores, employees etc. This means that all reports have at least a connection to the Enterprise database and another connection to another database -- and sometimes more than that. My question is: is there a benefit of moving the reporting procs into a separate "Reports" database. I know the benefits of moving reports onto another server and I'm not talking about that -- this would be on the same server. Things that might affect this are: 

I agree that the archive logs can be deleted when they are beyond a set period of time. Doing this solved a similar space issue for a database I maintain. However some thought about the "why" would be a good idea. If this is a development database and it is not performing the same way as production then you have an issue with your application stack. 1) Is production performing the same way but you have lots of disk space so you don't care? Or is this only on the development VM image? Just cleaning up log files solves the problem but not the root issue. 2) I have not noticed any differences for Oracle databases that have been virtualized. 3) Check the size of the .dbf files. Do they represent a significant portion of the space used? If so, then you may need to increase the VM disk size. 

We use Lucene.Net in our ASP.NET application. It indexes the database fields using the Query Parser methods and I believe it will do what you want. My only proviso is that with the number of records you want to search I would think a dedicated server or two would be required to index the data. And it is quite possible that at the end of the day when you have spent some time configuring Lucene and the hardware and writing the connectors you could have just made a data warehouse and got something similar. 

--add the primary key of PK_ID, are contents larger than 4000 characters? use a CLOB --create a trigger on the table if the PK_ID is null add a sequence value 

What you can do is not always appropriate for the various usages of your relational model. If you were creating a data warehouse in order to analyze customer sales then derived attributes would be appropriate. I have done this for a summary table for a reporting tool. The query would have joined upwards of thirty tables with many aggregations such as sums and show all values an entity has had. A summary table, refreshed daily, listed derived attributes was a great solution for reporting. For an online transaction processing database using derived attributes is not always the best solution. For example: now your total is price * quantity Next month management decides to implement a discount of 10% for customers who order more than $1000 in a calendar year. Your total column now looks inflexible. Unfortunately the things you can say today that "will never change" such as Total = price * quantity are really an example of business logic. Business logic can change anytime in unexpected ways. To continue with your example....if management institutes a discount and you have a customers table, orders table then all you have to do is add a discount table. Then you can create a view which encapsulates the business logic of the day to derive total sales. When the logic changes you can change the view much easier than recalculating the derived attributes that are fixed in a table. And if you really want to be prepared you could store the changes in the business logic in a table in the database and cover off "Who, What, why". So if the Bob the Manager offers a discount and five years later Sue, the new manager, says "When did we start offering discounts and who authorized it?" you are a database star.