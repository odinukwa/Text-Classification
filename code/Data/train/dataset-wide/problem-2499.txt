Header file the current source file implements (i.e. for a file called ) if any Headers from the same project you're working on Headers from different projects/libraries (in your case, , for example) Headers from the standard library 

There are a lot of different opinions on when to leave space, but most developers agree that more whitespace is better than less. Use telling variable names. What are 

A Serious Bug I am sorry to be the bearer of bad message, but your whole programs behavior is currently undefined. You have stepped into the C++ reference-lifetime-trap. Lets take a look at this piece of code: 

free make whatever was pointing to (either the passed as a parameter to the function, or the pointer of the previous node) point to 

The point of open / closed is that if you expect something to change, you should define an interface around it. In your case you could have be an interface with an method and just do 

This method finds the common ancestor of two nodes in a binary tree. Any style suggestions or ways to make it more idiomatic? Is the algorithm correct? Rubocop says the cyclomatic complexity is too high, how would you break this up? 

Prefer over . It is easier to read and generally more C++-like. Do not include C headers directly, and do not use quotation marks for system and standard library headers (looking at ). Use the headers that the C++ standard library provides (in this case, ), and remember to put before the types and functions you use (i.e. => ). all necessary headers. Your code uses , but you never (and even then, you code should use ). headers in the order: 

but I think this is needlessly complicated because adding a new field doesn't break what users of the method expect. If you had several predefined kinds of searches, like and in some imaginary clothing app, then it would make sense to have a interface and have each kind of be responsible for generating a query. tl;dr the open-closed principle is really vague and trying to guess in advance which points of your program will vary is one of the hard things in software. what you should really be wary of is the possibility of making breaking changes to , which isn't the case here. 

Make the first object the centroid for the first cluster For the next object, calculate the similarity with each existing cluster centroid If the highest calculated similarity is over some threshold value then add the object to the relevant cluster and re-determine the centroid of that cluster, otherwise use the object to initiate a new cluster. If any object remains to be clustered then return to step 

I have a pretty simple problem. A large set of unique strings that are to various degrees not clean, and in reality they in fact represent the same underlying reality. They are not person names, but they could be. So for example Dr. John Holmes and Dr. Jon Holms are more than likely the same person. From the list of unqiue strings I want to cluster together those that possibly match the same underlying reality. There is no restrictions on the size of the cluster (i.e. I am not expecting only one match per string). I calculate the distance between strings using the Jaro-Distance with jellyfish in python. But then I wondered how I cluster them together? I was unable to figure out how to do this using scipy clusters as I am working with strings, not floats, so I decided to work according to the following algorithm. 

I assume you meant "Precision" here. In general, I would like to appeal to you to make more use of commas and punctuation in general throughout your comments. For example: 

instead. About Time Complexity You state that your code is \$O(\log n)\$, which is obviously incorrect. The reason for this is that you need to iterate over every char in your input file, which is an \$O(n)\$ operation. Although you are right that inserting into a BST is an \$O(\log n)\$ operation, this term considers the size of the BST for \$n\$, which is separate from the length of your input. About Memory Problems You say that you are afraid of memory problems, but you really do not have to be. Apart from the missing in your -loop in (see Memory Leaks), I did not spot any problems with forgotten s or the like. The functions and are a bit of a performance issue because they do so many allocations, but if you stick to the tips I gave you then that should be fine. If you are worried about running out of memory, don't be. There is hardly anything you can do against it, and exiting gracefully if a call to or fails is really the best you can do. One could argue that you might run into memory fragmentation problems because of the tree data structure you employ, but this is very difficult to work around. If you are really concerned about memory usage, e.g. because you process gigabyte-sized files, there is one optimization that you could apply: Instead of making separate copies of each word you find, just store offsets to where the word begins and ends in your memory buffer. In the best case, this could roughly cut memory usage in half (although maybe a 20% improvement is more realistic, but I am just guessing anyway). As I said before, you could also try replacing you BST with a normal array, although this will, of course, increase the cost of inserting elements quite drastically. 

In general it's not that useful to separate the and abstractions. If you want a list just pass around the first node of the list. It makes many algorithms much simpler to read and write. I'd start with 

This becomes much easier if you realize that those two cases are identical, all you are doing is freeing something and changing what a pointer points to. To change what a pointer points to, you can simply keep a pointer to the pointer that needs changing. The pointer to pointer starts off as the input and becomes a pointer to the pointer of the previous while iterating down the list. That way when you find the element you just save its pointer, free the element, and use your pointer to pointer to update the previous element OR to point to the new . 

Also, I decided to strip out generic components of the strings to improve the matches. In the name example above, this would give Dr. John Holmes, and J. Holmes a better chance of matching if 'Dr' is stripped out as a generic component. I created a Stripped Class that has an attribute that is the original unique string, and then attribute that is a stripped version (stripped of the generic components). This is becasue once stripped many of the strings could have exactly the same values, and I need to be able to identify which original string they identify. The code is posted below. It works well, except that the size/number of clusters is not independent of the order in which the strings are evaluated. So the results do differ. In the final function, the SLink fucntion is called inside another function I would specifically like to know if anyone knows of a better way to implement this type of clustering? If anyone has comments on how I could improve my code, and if anyone has any information about clustering strings in scipy. In an IPython notebook I go through in more detail how I could refine this system, including using another function to verify the clusters using alternate data points relating to each string (such as age). If anyone would like to view that and offer guidance, then please see the following link: $URL$ I think as the functions were somewhat trial and error and unplanned, they may have got a bit out of control. Any guidance appreciated. I am a researcher living in Bangladesh, a total programming data tripper sadly, but trying to get better every day. 

You don't need to track the end of the list. You can just check that you've reached it if an element's next is . doesn't do anything. Every time you use it you set instantly after, so should be a parameter. Setting to isn't that useful because if you don't change it you can simply check and get the same thing, while using less memory. Try 

Problem Given a square matrix of size , calculate the absolute difference between the sums of its diagonals. Input Format The first line contains a single integer, . The next lines denote the matrix's rows, with each line containing space-separated integers describing the columns. Output Format Print the absolute difference between the two sums of the matrix's diagonals as a single integer. Sample Input 

GenericAll is a list of substrings to be stripped from each string wherever the sub string appears in the string. GenericWhite is a list of substrings that are stripped only if they are surrounded by whitespace. And the function that implements the clustering 

ClassList is a list of Stripped Class obejcts created from the unique strings that are to be clustered. Threshold is the threshold above which strings will be clustered when distance between the centorid and the string being evaluated is calcuated as the jaro-distance. Lastly I wrote a function that sort of combines all of the above, but also clusters the clusters, and calls the clustering function recursively until no more clusters are found. It also references the data frame from which the strings are pulled. In theory, if two non-indentical strings represent the same undelying reality, this third data point should be equal for the non-identical stings that are above the Jaro-Distance threshold. This function directly modifies the data frame from which both the list of unqiue strings and the verifying data point are drawn, until no more modifications are possible. That looks like this: