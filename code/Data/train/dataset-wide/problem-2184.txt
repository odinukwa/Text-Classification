So basically I'll have to do a migrate a database live over the course of a few days, while simultaneously updating the application live such that it can handle the new application, without being able to search for the usage of each table. With all these handicaps taken into consideration I came up with the following plan: 

However, I already defined using my keys that table1.a references to table2.b, so it seems to me that it shouldn't be to hard to make a DBMS system automatically use table1.a and table2.b as the join columns, such that one can simply use: 

Using this strategy I will have 2 synced databases after step 3. Initially all queries will go to the old database, but while I'm updating the queries slowly the old database will be used less, and the new one more. Taking this "sub-optimal" situation in mind, is this a good strategy to fix some of the problems? What things should I take into consideration while doing this? Note that I fully understand that this is a very risky suicide mission. However, I'll have to do something in a short amount of time, otherwise the website becomes entirely unusable. 

I try to create a good database design to represent this data, however there are quite a few difficulties. A design I came up with is as followed: 

Create initially empty tables to store the data in a sane way. Create triggers on the old tables which can sync the data with the new ones. Export the data from the old tables to the new tables & enable the triggers. Replace the queries one by one. 

So in fact I want to group by message_type and user_id, but instead of generating multiple rows per user, I want to create multiple columns, one for each message_type Can I achieve this without hardcoding the message types in my query? 

However, there is a problem with this design: a row should only be allowed to reference a connection when the education type matches. E.g. a row can only reference a row in the table that references a row in the table with type == master. Would it be possible to add a constraint which can check exactly that? If not, what other options are available? 

As far as I could find out many DBMSs (e.g. mysql, postgres, mssql) use fk and pk combinations only to constrain changes to data, but they are rarely natively used to automatically select columns to join (like natural join does with names). Why is that? If you've already defined a relationship between 2 tables with a pk/fk, why can't the database figure out that if I join those tables I want to join them on the pk/fk columns? EDIT: to clarify this a bit: suppose I have a table1 and a table2. table1 one has a foreign key on column a, which references to the primary key on table2, the column b. Now if I join these tables, I'll have to do something like this: 

I have a messages table in a database, which include a sender id and a message type (and of course many more columns not relevant for this question). I try to create a query which counts how many messages of each type a user have send. e.g. if I have the following table: 

I "inherited" a web application which is designed and implemented horribly (both the application and the database). For example, the main data is stored using a sort of emulated key-value storage in a Postgres 8.2 database, making it virtually impossible to extract useful data from it in a reasonable amount of time. Currently I'm working hard on replacing the entire application + database, however it will take a few months before the new application is finished. This is a problem since the website is really slow due to the extremely bad database design and even worse queries. Therefore I'm planning to fix the database design and the queries on the live site until the website has an acceptable load time as a temporary solution. I do however have a few limitations to work around, the most problematic ones are: 

Since there is a limit on the length of the index, if you want to guarantee the uniqueness of the whole field's value, you may want to add a hash field of that varchar field. Steps: 

Because it is a CSV file (Comma Separated Values), it thinks that the each of the marks is a different field. Your initial export should either separate values by a different char, like "#", or quot the values bu . 

is better to be , unless you want to store the price in "cents". i.e. if the ticket price is $5.99. you either have to store it as and let the application process it when doing calculation, or viewing it; or, you store it in field is better to be an integer, and the user ID in the users table is better to be an integer Your primary key is good. Don't change it to . Maybe a flag field would be more clear to know which tickets are already used (This is personal preference) Depending on the queries you run against this table you may create the required indexes. 

The clause after is the condition which is if met, the row will be returned with the result set. In your example, the condition is always met, so the result is the Cartesian product of the joined tables. This is similar to saying , which is always true. You will get the same result if you don't add the condition with the inner join. 

InnoDB table should have a Primary Key defined, or else it will try to find a unique combination of fields to create one. The best is to explicitly add an Auto_increment Integer Unsigned ID. 

Copying data directory would work if you are using the same version of MySQL. As per you question, you are copying from 5.6 to 5.7, so this way wouldn't work. Now: you mentioned you are "migrating", and the steps you are following are for "creating new slave"! If you want to "migrate", a better way is the following: 

In short, it is sufficient. As far as the searched field is the first in a composite index (PK, unique, or regular index), no need for other separate index. However, you mentioned that you 

Queries that do "full table scan" are the ones that don't use indexes. Try to log them in the slow query log using this option log_queries_not_using_indexes Be careful though that small tables that have frequent queries running against will fill your slow query log files. You may want to enable this option for limited amount of time. To reduce this chance, you may set min_examined_row_limit variable to a reasonable value, depending on your small tables. 

Group by both user IDs Inner join to the users' table two times, use different alias to join it with different user id 

You can run the following statement in a loop in a script that triggers the statement every 10 seconds for example. You can customize it to give you more or less info depending on the query you issue. 

I believe you are using PHPMyAdmin to add the index, and in PMA, unique is the default choice when you want to add an index, and that is why you are getting the duplicate key error. 

In your question, there is not enough information to see where the problem is from technical point of view; however, I can see that there is a problem in communication between you as developer and the DBA. This is not abnormal though as DBAs and devs do not always 'speak the same language'. Let's go the some points in your question: 

This specific command, "SHOW VARIABLES" is usually run a lot by "monitoring tools". Check if your monitoring tools are using this account. Some other symptoms to tell if it is a monitoring system: 

If in your setup you have two or more servers, like master-master, or master-slave(s), I think it is better to do it offline, one server at a time. 

You should use function if you need to get the average Using function will return the minute part from the date. i.e. 10:05:03 and 11:05:33 will give the same result. (It is fine if the interval is less than 60 minutes, however, it is safer to use the following query:) 

A query is typically like . You could use an expression like the formula part of a query if you consider any free variables/attributes to be implicitly universally quantified. (This is also sufficient.) 

If you do want to involve the previous Payment meaning in a query then you use JOIN to get the table whose meaning is the AND of the simpler meanings. PS You don't need to declare a FK because it will be enforced by the combination of FKs in Payment and Enrollment. 

Natural join "automatically" joins on equality of common columns, but you should only write that if that's what you want based on table meanings and your desied result. There's no "automatically" knowing how two tables "should" be joined or in any other way any table "should" appear in a query. We do not need to know constraints to query. Their presence just means the inputs may be limited and, consequently, the output may be too. You could define some kind of join_on_fk_to_pk operator that "automatically" joins per declared constraints; but if you want the meaning of the query to stay the same if only constraints change but not table meanings then you'd have to change that query to not use the new declared constaints. Just giving the query you want using join & conditions already leaves the meaning the same despite any constraint changes. What constraints hold (including PKs, FKs, UNIQUE & CHECK) don't affect what tables mean. Of course, if the table meanings change then the contraints might change. But if the constraints change it doesn't mean that queries should change. One does not need to know constraints to query. Knowing about constraints means we can use further expressions that without the constraint holding wouldn't return the same answer. Eg expecting via UNIQUE that a table has one row, so we can use it as a scalar. These queries can break if the constraint was assumed but not declared. But declaring a constraint that the query didn't assume cannot break it. Is there any rule of thumb to construct SQL query from a human-readable description? 

There should be a table for every box, ie entity type/class, and every diamond, ie relationship type/class. There should be a column for every oval, ie attribute/property. Each line from a diamond to a box is indicates a foreign key. Relationship PKs are composite and consist of FK columns to associated/participant/referenced entity types. 

That has a "for-all for which there-exists", just like my solution. But your proposed answer 2 has a "there-exists for which for-all": 

The general approach of the SQL standard is that many things not imposed by chosen semantics are left to the vendor to optimize or specify. Under the chosen policy users choose names when they care what they are. SELECT without FROM isn't standard SQL either. MySQL chooses to use the expression that evaluated to a column's value as its name. Those PostgreSQL "?column?"s aren't the column names, they are strings output instead of the column names. If the system defaults to internal unique names then clashes are avoided compared to defaulting to patterned names. In your example you could have selected "column1" but with your expected behaviour you'd need "t2.column1". Ultimately, they just chose a certain design for the language. And even where people leave a rationale, engineering and ergonomics are about pragmatic timely tradeoffs where one decision cannot necessarily be justified as "best". 

That asks for tuples P where there is an offer S1 where [P's sid is S1's sid and for all Items S1's iid is that item's iid]. I hope you can see that this involves a single iid being the same as all the ones in Item, not what you want. Your answer 1 

A FD is a MVD "in disguise". If an FD holds then a certain MVD also holds. With MVDs 1 is a special case of multi(ple). This can be seen from the rules of inference for MVDs at the definition link: RA → B implies A ↠ B. Hint: t1 & t2 can be t3 & t4. Hint: In the example there are only two tuples, so t1 & t2 must be those, and you know the MVD so you know A & B, and there are only two tuples, so if t3 & t4 exist they must be those. So fill in the equations and see whether given those two tuples (t1 & t2) there are also two tuples with the properties needed (t3 & t4).