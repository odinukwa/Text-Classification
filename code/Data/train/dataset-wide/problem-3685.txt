I use Quantum DLT-S4 tapes (800/1600GB) in my library, which has two Quantum tape heads in it. I have had quite a few bad tapes i.e. tapes that come up "CRC error" or "cannot read tape" etc and the backup software marks them as bad. I've sent back around 20 to Quantum out of 100 for a refund. The library is only 18 months old, and one of the drives has already been replaced a while back. Still get bad tapes. How many bad tapes do you guys get? 

One of the best I've used is PRTG from Paessler. It is relatively cheap for a small number of servers. They charge based on the number of things being monitored (ping, free disk, cpu load etc). It's completely web-based and no agents are required. It comes with a load of predefined WMI queries and of course you can define your own (they supply templates you can modify). Plenty of charting and reporting available to impress management. 

You would have to get the hashes (either LM or NTLM) and do a dictionary attack on them. Anyone with a moderately complex password would be nearly impossible to discover. Don't enable "reversible encryption" - you need to change the way you manage passwords. 

What you're describing is a pretty standard setup for Microsoft intranets. Security is a big topic...do you know how to configure IIS security, .NET security, domain security, Internet Explorer security, workstation security...you get the picture. Someone needs to be across each of these, if you are not the sys admin (are you a programmer?) then you should speak to the sysadmin and discuss these issues. I guess one thing to mention is that many places just use NTLM for authentication if they are a Microsoft shop, and NTFS for securing web folders. 

We are thinking about purchasing 4 x EqualLogic PS6510X SANs (the Sumo boxes). Each has 48 x 600GB 10k SAS drives. They will be stacked to form a logical pool of storage (all in the same location). I understand that when you create a RAID group its done on a "per box" basis. So one box could be Raid 50, another Raid 10 etc. My question is, should I make one box a "performance" box ie Raid 10, and the other boxes "standard" ie Raid50? How do people configure their EQL arrays in the real world? 

I've just checked our BladeCenter H AMM options and unfortunately there doesn't seem to be a way to do this. 

Say I install a driver (FC HBA) that immediately blue screens the server (Windows 2003). If I reboot and select Last Known Good, will that enable me to get to the login screen? I know it loads the previous registry. 

You can try enabling the Traverse Folder/Execute File permission (need to go to Advanced display in the file security properties). At the same time disable read permission. I haven't confirmed this, but give it a go and let us know if it works. 

Currently we use HP servers with ILO boards to give out of band remote access and cold boots. IBMs have the same functionality with RSA boards. APC powerstrips are excellent for remote manageablilty. Throw in an IP KVM and you've got all the remote access you need. 

Where to begin? This is a disaster waiting to happen. A Sysadmins primary job function is to ensure data is backed up and recoverable. Everything else is secondary. No if's no but's. Here are a few things you can do: 

Backups are one thing, but long term archival is another. For example, you might be required to store emails for 7 years, or keep all project data indefinitely. I used to save archives to tape, but then I've had tapes get destroyed (drives rip the tape out). So...write to 2 tapes I hear you say. Is that what others do? Have 2 (or more) tapes of the same data for redundancy? But then the other issue is that tapes cannot usually be read by different backup software vendors. Eg if you go from Arcserve -> Backup Exec -> Commvault over 10 years you would need to keep all 3 systems so that you could restore old data. Likewise for hardware. Old tapes might not be barcoded. Might not be compatible with the new library etc etc. So do you keep old tape hardware AND old software just in case you might need to restore a 10 year-old file? Or...when you move to a new backup system do you migrate all archived data to the new system and re-archive it onto new tapes? That could be a huge job. Any thoughts? 

I got my first job in 1990 by reciting the OSI layer model. They were most impressed. I didn't actually understand it but boy could I recite it well. 

We have some old servers with 8GB C: drive. I end up deleting stuff like the $NTServicePackUninstall$ folder from c:\windows\system32 which frees up a few hundred meg. Also a server can end up with a lot of admin profiles on it, these can go too (of course they might come back again). What else? Move pagefile.sys to another drive if you can. Move the spool folder to another drive if there is heavy printer use. I use TreeSize Pro because it has a handy "File Ages" bar chart where you can quickly see if any files in a folder are over 6 months old for example. 

The problem now is that DFS-R is broken and not replicating, because it is still referencing the old computer name, server-old. There are some Active Directory attributes related to DFS-R still attached to the old computer account. Am I able to fix DFS-R by associating the old computer account with the new server but keep the original name (server1)? I think this would work as it would fool DFS-R into thinking nothing had changed, and the DfsrPrivate folder still exists. I don't want to have to recreate the replication group as that would mean an initial resync. 

My client has a Mac at home and wants to remote control (run apps) on a Windows XP PC at the office. What is the best software to enable this? Any Mac <-> Windows weirdness to be aware of? 

I need to backup many sites over slow, high latency satellite links. We are talking pings of 650ms to each site. I can get a dump of the initial data by sending out USB disk to site and restoring it at central office. From then on I intend to use rsync or DFS-R for byte-level incremental copies across the link. All the machines are Windows 2003 SP2 R2. I have read that rsync can hang on large files and is still a bit flakey for Windows? Alternatively should I use DFS-R which also does byte-level copies? I have tried DFS-R in the past and was not impressed by the lack of logging, it was very hard to find out what was actually going on. That's why I'm interested in rsync. Has anyone any real world experience of both methods? 

In a single domain environment without Exchange you don't need a GC, even with multiple sites. GC has got nothing to do with locating domain controllers in this scenario. 

There is no quick way to do this. Explorer will happily trundle off for hours (days?) applying the new permission to every file and folder (if inheritance is set). Enabling a share is much easier, the user just needs at least read permissions on the share. The underlying NTFS permissions will determine what the user can actually do. Note that if the share permission is read-only, then that is the maximum access even if the NTFS security is set to modify (r/w). 

Why are you letting them install software themselves? Usually there is a system is place to do this, either with servicedesk (level 1 type support) or software like Altiris which installs apps with admin rights even if the user doesn't. Also, your users would need admin rights on the local PC which is also a bad idea. We have an app share that is locked down by security group of which only a few IT staff are members. Users are not admins and get an error if they try to install anything. Otherwise they would install all sorts of crapware. Even the "skilled" ones. Actually they are usually the worst. In addition if you let users install whatever they want how do you enforce licensing? 

I need to remote control a PC that is behind a SOHO ADSL router (netgear or similar). I do not have access to the password for his router so I cannot make changes on it. I can install software on his PC though. What remote control software is best that can work through http/https (I am assuming his router only has those ports open)? It also needs to work without having a person at the other end, as I will be accessing the PC out of business hours. 

Generally I ask "What has changed that might have caused this problem"? Most issues are caused by changes to known good configurations. If you can isolate who did the change then you usually get your answer. 

If you log into the server then you aren't accessing it through a share. You're just hitting the NTFS security directly, share perms have no effect. 

We have IBM blades. Is your chassis the newer 'H' type? It will say "Bladecenter H" on the front if it is. It's also 9u high instead of 7u like the older chassis. Anyway, as regards power to the chassis - yep there is no power button or software control. You just cut the power to the chassis. Sounds brutal, but you would only do this after powering down all the blades anyway. We've done it several times with no issues. As for docs, you'll want to peruse the IBM BladeCenter Redbook site. This will have all the info you need. New blades come with the IBM ServerGuide setup CD which lets you install an OS with all the correct drivers. Did you not get this CD? If not you can download the latest for the appropriate hardware from here. Good luck with the blades! Make sure you do upgrade all the firmware, we had plenty of issues with old firmware. 

If you have I'm interested in the user experience and admin experience. Did users prefer Outlook over Notes? Did the sysadmins prefer Exchange over Domino? If so why? 

What Windows backup software do you use? Most have Linux agents (Backup Exec, Arcserve etc). If you are using NTBackup then there is no Linux integration. We have a similar situation. We just have a cron job that runs on the Linux box and copies files to a Windows server using Samba. From there the Windows backup software backs it up. Yes it's not very "Windows-ey". For that you really need a Linux agent which will have the smarts and integrate into the main backup console. 

If so do they work well? I seem to recall that quotas under W2003 work on a per-volume basis which limits there usefullness. Is W2008 more flexible, maybe allowing per-folder quotas? 

In my experience what works best is the Master IP Spreadsheet to Rule All Others. That is, there exists one spreadsheet and it is everyone's responsibility to update it. If it ain't in the spreadsheet, it ain't live. No ifs no buts. People soon learn to update it. Also, I wouldn't use DHCP for any servers, even test ones. The last thing you want on a server is the chance that it's IP might change. 

How big is your intranet? As an alternative you could try the free MS Search Server Express which we use on our intranet (2000 employees). There are some docs on the web comparing the two. 

It will copy the entire file each time you make a change to it. What are the file types? Microsoft has made delta sync to SkyDrive (now OneDrive) available on Windows 8 with Office 2013 (for office docs). $URL$ But outside of that, pretty sure you are stuck with uploading the entire file each time. 

The jobs are stored in the system database MSDB. Did you back that up? You need to restore it to get your jobs back. 

What speed is the link between the 2 sites? What is the rate of change? 100MB/hr? 1GB/day? That will determine the tool to use. Tools like robocopy (and FRS) copy the entire file even if a single byte changed. I suspect rsync does too. DFS since Windows 2003 R2 is the successor to FRS and does byte-level replication. That is, only the changed bytes are transmitted, not the entire file. This can lead to substantial transfer time savings. 

I hate using SSMS because it is slow and cumbersome. The older Enterprise Manager in SQL 2000 was quick to load and much snappier in reponse to actions. That's progress for you. 

You could still run a local login script on each pc, that connects to a "master" pc with the software on it and installs it from there. Checkout gpedit.msc and go to User Configuration -> Windows Settings -> Scripts (Logon/Logoff). Edit the Logon item and point to your script (batch file would be ok). The user would need admin rights locally and rights on the master pc. 

Every VM conference I've been to has the VMware techies claiming that there is no type of server that can't be virtualised. They claim large Oracle VM installations that handle 3 times the amount of daily Visa credit card processing (this is with VShpere 4.0). But they live in an Ivory Tower. For us in the real world I would avoid virtualising database servers. Otherwise the DBAs will start accusing you of "slowing down their server" every time something happens (whether it is VM fault or not). It's not worth the hassle. 

For example, say I have a full backup done using robocopy. Can I then use rsync to replicate just the changes? Or will rsync do another full copy? I don't want that to happen because it's over a slow WAN link. 

There is no open-source alternative that can do all that. Samba can do a useful subset. Why are you asking? 

Do you have a "delete first ask questions later" policy for music and movie files? Or are you more lenient? Just wondering if there's a difference between how large and small organizations deal with the issue. 

Easiest method is to run newsid.exe from Sysinternals on each distributed VM. It changes the SID (and optionally renames the PC) so conflicts don't occur. It's best to create the VM while the guest is an a Workgroup. Then distribute the VM, run newsid.exe and then add the guest to the domain. Alternatively you can use Sysprep to prepare the VM, it achieves the same thing and is more of an automated process. 

I always thought the fact that you could change NT Workstation 3.51 to NT Server with a registry change was pretty cool. And says everything about Microsoft's market segementation strategies. 

The only proof is to measure. Take timings on a laptop with no encryption and compare with one that does. Of course there will be overhead in the encryption, but don't rely on a subjective "feels slower". What encryption are you using? Bitlocker? 3rd party app? As to the final question, it is too easy to accidentally miss (or even define) what is sensitive data. So I would keep the whole disk encryption if possible. 

The problem is that most of the new "dumb" terminals like Wyse thin client terminals are actually quite expensive. It can be hard to persuade management to buy machines with reduced functionality compared to a desktop PC of almost equivalent price. 

Yes the Western Digital NAS boxes can do this. Just plug in an ordinary USB external disk. However I think the new disk is only available as a separate share, you can't extend the internal disks to include the external one. 

I have a large Windows file server with about 2TB of data, about half of that is over 2 years old (based on modification date). What would be the best way of archiving off that old data, using scripts or whatever, but without spending big money on a full archiving (HSM) system? The purpose is to reduce the backup window, because all that old data is backed up every week when really it never changes and can be backed up much less frequently, thus reducing tape requirements. BTW the archive would be on another disk, with read-only permissions. Has anyone implemented something similar? How would users access the archive easily? 

Track KPIs for restores. It should be possible to produce a report showing how many requests for restores have been successful. Anything less than 100% should be investigated thoroughly. Management love reports and this is hard evidence. There should be documented procedures for all backup and restore operations, including all systems and their backup strategy, tape rotations, schedules, escalation paths, test restores etc. Ask to see them. Speak to the manager of the sys admins and voice your concerns. Go armed with proof that restores aren't working. If no joy go higher. 

I am using the latest VMware Converter Standalone to p2v a physical Windows 2000 Professional SP4 PC. The PC is a standard Pentium with IDE disk from circa 2001. The disk is 20GB partitioned logically into C: and D. It converts with no errors (I did both disks into one VMDK). When I power on the VM in VMware Workstation 6.5 (or Vmware Player 2.5) it gets to the Win 2000 boot graphic then I get a BSOD with the classic 0x7B Stop error: inaccessible_boot_device. Is there anything I can do to get the vm to boot? I am lost for ideas, normally p2v of a basic IDE pc works flawlessly. I'm willing to put a bounty on this as I am trying to sort this out for a client urgently.