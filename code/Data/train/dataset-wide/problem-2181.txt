If I were doing this myself, I'd install the Docker Community Edition for Windows from the Docker store, and then install the SQL Server Docker image that way. If your Internet connection is fast enough, you can be up and running in under 5 minutes, and be able to allocate resources in a much easier way. EDIT: Ah, networking. 

With this structure, when you add a new company, you add it to the table, and then add a reference to it in the link table. So if you had 100 countries, you would have several hundred rows in this table. I can assure you that querying this will be far more efficient than a very wide table. When you want to query the data, you'll need to write an additional , but this makes managing the data so much easier, and less error-prone. Notice that I've added a column, which I would use for a two- or three-digit ISO standard (e.g. "USA", "CAN"). This means you could even use the code as a foreign key instead of the . 

As a data professional, I'm going to tell you straight up to leave the constraints in the database. However, there's no right or wrong answer, because it depends on what risk you're prepared to deal with. If you don't want relational integrity, use XML or JSON or some other thing that isn't an RDBMS. But if you want to use SQL Server or Oracle or PostgreSQL, go right ahead and manage it in the app. Somewhere along the line, though, there will be data corruption and orphaned records. That risk is real, but I can't tell you how big of a risk it is. If the effort of fixing corruption like that is less than the effort of maintaining constraints in the database, that's a manageable risk. On the other hand, if you can't bear the thought of fixing corruption or orphaned data, then leave the constraints in the database and spend some money on the right tools to help you with database changes and migration. P.S. "Agile" is not an excuse for bad design or lack of documentation. 

There's a lot to unpack here. You said in the comments that it's a vendor-built SQL Server database, around 1 TB in size, and performs poorly. You're hoping that moving to the cloud can address performance and allow remote access. That's two separate things, so I'll answer them separately below. (My answers are tilted to Microsoft, because that's what I know. Prices are roughly equivalent for Amazon Web Services.) Performance With vendor systems it can be tricky to make code changes. For that reason, I'd focus on making sure SQL Server (and the operating system under it) is configured correctly: 

You will need to have the Full Recovery Model enabled on all four databases. Now you will issue a full backup on all four databases, let's say ten minutes before the point in time you require. Now you will do a transaction log backup of each database, at a point in time immediately after the exact moment you need. On the other end, you will restore each full backup , and then transaction log backup, with the switch to specify the exact moment in time you need, as a DATETIME. Remember to at the end to bring them into active use. 

Short answer: yes it can help, because it's theoretically instantaneous. You would insert your data into a staging table with the same definition as your main partitioned table, and then switch it into the partitioned table, which is a metadata operation (schema lock). Long answer: it might make performance suffer over the long run to have such a large table, and unless you move to later versions of SQL Server, statistics updates are quite difficult to manage. If this is a read-heavy table, I'd consider using view partitioning instead. Each month (for example) would get its own table, with check constraints on a date column to help the query optimizer know where the data is physically stored, and a view over the top of that: 

Using the GRANT Shema permissions on my made-up domain group, I can assign SELECT permissions as follows: 

For the Reporting schema, that can be tackled in a few ways. Do you want to give them full control over the schema? Is it just for creating views? 

I would enable Read-Committed Snapshot Isolation, which does row versioning under the covers by making use of tempdb. Readers will not block writers, which is what you're looking for in this scenario. This is a per-database level setting, which doesn't require a reboot. However, you must have tempdb configured correctly (appropriate fixed-size file growth, sufficient data files, etc.). 

I don't think you're doing what you want to do. It looks like you're striping your backups across two paths, which means your backup is evenly split between the two locations. I suspect that your actual backup size is not 3.5MB, but rather 7MB. The difference in size is most likely due to the local storage being quicker. Per Ola Hallengren's documentation: 

Assuming the values for your initial query result are 1 and 2, you have two values returned back, which you can now plug into a clause. You would use this to search for the number of times T3ID appears where it has two matches. This code is written in T-SQL, but I'm sure you can convert it quite easily to Access, as it's for demonstration purposes anyway. 

The question originally contained an extra set of brackets around the statements, making my above comments about the keyword moot. With the brackets, the two statements are functionally identical, as pointed out by several other people, and result in the same query plan. 

Change the to . There's a chance there are missing foreign key values that are causing it to not find any matches across the whole set. Where you see NULL in the result set, will be an indicator where there is no matching foreign key. 

You need both SQL and Windows patches, as well as CPU microcode updates, to be fully** protected. GDR is meant to be security-only patches, whereas Cumulative Updates are bug fixes as well as security patches. Once you go from the GDR path on to the CU path, you're stuck there, i.e. you can't go back to GDR only. (Confusingly, this latest Meltdown / Spectre update shows up on certain SQL Server versions as a GDR and CU update.) If you are already on a CU, then stick with CU update. ** Note that Microsoft is rolling back the Meltdown / Spectre updates on Windows Update because there are some issues with certain CPUs, so it's hard to tell whether you were one of the lucky ones to get them while they were up. See the warning below. Use this link to check if you're fully patched, using a PowerShell script from Microsoft. The link includes the following statement: 

Once you move data out of a filegroup on SQL Server, you must run a command to reclaim that physical space. The official documentation has all you need to know about this, but the general technique is as follows (this example is from the documentation, unedited). 

The Page Life Expectancy counter tells you for how many seconds an 8KB data page will remain in the buffer pool, before being flushed out. A low number can be a sign of memory pressure, but it may also be a sign of nothing. If your data access patterns involve a lot of report-style queries, reading a lot of data from large tables, this could be normal. Monitor the value over several days, at different times in the day, and you may find that this is an outlier. I have a customer whose PLE goes to zero every time a certain large index is rebuilt, but it steadily increases after that. My first sense is to review indexes, and see if a well-placed index can help with large reads. Another best practice is to avoid using if you can help it. You may need to increase the physical RAM in the server, but that shouldn't be your first reaction. (Read more here about PLE and NUMA nodes: $URL$ 

If you have, say, 64 GB of physical RAM, your Max Server Memory setting would be 54272 MB (53 GB), which means your BPE should be a maximum of 848 GB (53 * the ratio of 16) or less, with 424 GB or 212 GB recommended for optimal size. When you configure the BPE size with , that is the size of the cache, and is how large it will be. It will not grow unless you change it. So make sure you pick the right size to begin with, and make sure your SSD is fast enough. In summary (from the comments):