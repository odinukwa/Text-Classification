Suppose I have a divisor $D$ on a curve $X$ (Hartshorne curve - smooth, projective, dimension one over an algebraically closed $k$). If the complete linear system $|D|$ is basepoint free then I get a map $\varphi:X\rightarrow\mathbb{P}^n_k$. My question is, say for simplicity our map ends up being to $\mathbb{P}^1_k$, what if anything is the relationship between the degree of the divisor $D$ and the degree of the morphism $\varphi$? It seems for many cases that we have $deg(\varphi)=deg(K)$, however I can't find anywhere that proves that this is always the case. Thanks 

Say we are working over some $K=\overline{K}$, of characteristic $p>0$. Let $\phi: Y\rightarrow X$ be a nonconstant map of smooth projective curves. To this map we can associate a map $\psi: Z\rightarrow X$, where on the level of fields this is the Galois closure of $k(X)\subseteq k(Y)$. I would like to know about the tameness of this map. Let $e_P$ denote the ramification indices (with the maps understood to be either $\psi$ or $\phi$ depending on where $P$ lives). Now obviously if $p|e_P$ and if $Q$ lies above $P$, $p|e_Q$ as well, so $\psi$ has wild ramification at $Q$. I am wondering when we can ensure this map is (everywhere) tamely ramified. For instance if $d=deg(\phi) < p$, then the degree of the Galois closure of $k(Y)$ over $k(X)$ has degree dividing $d!$, and hence $\psi$ remains tame. My question is this: Suppose we can show for each $P\in Y$ such that $e_P \geq p$ that each point above $P$ is tamely ramified. Can we conclude that $\psi$ is (everywhere) tamely ramified? It seems to me that this isn't true but I cannot produce a counterexample. It would be fortuitous if it were true, however. Any help is greatly appreciated. 

and replace every occurrence of a^3 by x, of b^5 by y, of c^7 by z. For example, the first few equations would be 

where A is a square matrix of height and width 3*5*7 which contains the variables x, y, and z but none of a, b, and c, and v is a column vector that lists off the 3*5*7 monomials in a, b, and c. Unless the only solution to your equations is a = b = c = 0, that means that the matrix A is not invertible, and therefore 

My question is about a method described in Dr.Math forum for simplifying equations involving sums of radical functions. (The following is a transcription of the example given by Dr. Vogler): --- begin example --- We want to convert the equation 

In other words, if you consider each monomial as a variable, then you can write these in matrix form as 

But the determinant is a polynomial function in the entries of your matrix, which means that the determinant of A is a polynomial in x, y, and z. --- end example --- I have tested this method with several examples and it seems to work, at least in the sense that the roots of the radical equation are roots of the resulting polynomial (it seems however that the resulting polynomial may have real roots that are not roots of the original radical equation). The only method I knew about in order to get rid of radical expressions in equations was to carefully manipulate the equation and then raise both sides of the equation to the same power. This is to say I have a very basic math background. So finally, my question is: what is the name of this technique and where can I read more about it, or at least what are the keywords I can use to search google. I have many other questions regarding this technique, but I believe I good book on the subject would answer them. Just for completion, some of the questions are: 

First, I am by no means well-versed on cohomology so I apologize if this is too elementary. I have been going through some basics of etale cohomology, with my ultimate goal being an understanding of some basic applications. I have gone through the Kummer and Artin-Schreier sequences, and wanted to get an idea for how these sequences can help us classify $\mathbb{Z}/n$ and $\mathbb{Z}/p$ torsors. I found a short exposition by Artin that said $H^1_{et}(X,\mathbb{G}_m)=Pic(X)$, and this was was labelled as Hilbert 90. This presumably has something to do with $\mathbb{G}_m$ being related to $\mathcal{O}_X^*$. Can someone tell me what the additive version of this is, ie what $H^1_{et}(X,\mathbb{G}_a)$ is equal to? Also if anyone has a reference for this being worked out that would be great as well. 

I have been trying to learn some deformation theory, and came across the following in a paper: The first order deformations of a morphism of smooth curves $f:X\rightarrow Y$ is in bijection with $H^0(X,f^*(\mathcal{T}_Y))$. I would like to understand a proof of this. I understand some simple facts, like $H^1(X,\mathcal{T}_X)$ being bijective with the first order deformations of $X$. The reference given was to a paper of Ravi Vakil's, but I am unfamiliar with anything but the very basics of stacks. Does anyone know of a reference for this fact that doesn't use stacks? 

Let me rephrase your question in the special case $a=0$ and $b=t$. Your sum is given for $t$ a nonnegative integer, and you want a general and natural procedure to define a function $F(t)$ on $t\ge0$ such that $F(n)$ is equal to the sum when $n=0,1,2,\dots$. Now, there is a 'natural' and I would say standard wey to give a meaning to a sum $\sum a_n$ up to a real value, just by smoothing (if you prefer, convolution): fix a smooth function $\chi(t)$ equal to 1 for $t\le1/4$ and vanishing for $t>3/4$, and define $$F(t)=\sum a_n\chi(n-t).$$ In your original problem, the values $a_n$ arise as special values of a function $f(n)$, but I think this point of view is misleading; there are $\infty$ functions with the same values at integers, sometimes even nicer than the one of your choice, so what does it mean to deduce the form of $F$ in a natural way from $f$? when your data does not keep into account the precise form of $f$? I'm only trying to say that your original question is ill-defined. 

One version of the spectral theorem states that if $A$ is a selfadjoint operator on a separable Hilbert space $H$, then we can find a set $X$, a $\sigma$-finite measure on $X$, a unitary operator $U:H\to L^2(X)$, and a measurable function $a:X\to \mathbb{R}$, such that $A=U^*aU$. In other words, any selfadjoint operator, in essence, is nothing but 'multiplication by a real valued function', in suitable 'coordinates'. The spectrum of $A$ coincides with the essential range of the function $a$. Now, an eigenvalue must be a real number $\lambda$ such that the set $a^{-1}(\lambda)$ has a positive measure. Since $X$ is $\sigma$-finite, the eigenvalues can be at most a countable set. But you can have a continuous spectrum of course, only the points will not be eigenvalues. You can play with real valued functions (and with measures on $X$; any $\sigma$-finite measure is ok) to train intuition. 

I'll start with a motivating example and only then proceed to the question. Consider a list of total packages of milk that were purchased on 9 consecutive days on a given store, $z_1,\ldots,z_9 = 1,0,0,2,0,1,0,1,3$ Assume I have an algorithm that predicts $\hat{z}_{10}=2$. I'm interested in assessing the confidence associated to this prediction, however, for stock management purposes, I want to measure the confidence that the predicted value is an upper bound on the true value $z\leq\hat{z}$, instead of $z=\hat{z}$. This is because what I'm ultimately after is to be confident in not running out of stock. I guess an estimate of this confidence, based only on given examples, is $1-p_{\hat{z}_{10}}$ where $p_{\hat{z}_{10}}$ is the probability of finding a value more extreme than the prediction $\hat{z}_{10}$ on the empirical distribution of all 10 observations: 

Now if you treat x, y, and z as constants and consider only the variables a, b, and c, then you will end up with 3*5*7 polynomial equations in the 3*5*7 monomials 

which would be $1-p_{\hat{z}_{10}}=0.9$. Assuming the above reasoning is correct (enough :), now to the question: I recently read "A Tutorial on Conformal Prediction" by Shafer and Vovk, and am curious on how to frame this problem on the Conformal Prediction framework. It appears to me the paper exclusively focus the case of estimating confidence for $z=\hat{z}$, so the question how to adapt it for the asymmetric case. If we were interested in the $z=\hat{z}$ case, a natural nonconformity measure would be $A(B,\hat{z})=|\bar{z}_B-\hat{z}|$, where $\bar{z}_B$ is the mean of $B$ (section 4.1 in the paper). That would define the following prediction regions (from the algorithm of section 4.2): $\Gamma^{0\leq\varepsilon<0.5}=\{0,1,2,3\}$ $\Gamma^{0.5\leq\varepsilon<1}=\{1,2\}$ $\Gamma^{1}=\{\}$ given that, $\alpha_0=A(\{1,2,3\},0)=\alpha_3=A(\{0,1,2\},3)=2$ $\alpha_1=A(\{0,2,3\},1)=\alpha_2=A(\{0,1,3\},2)=2/3$ $p_0=p_3=0.5$ $p_1=p_2=1$ It feels wrong to transport this reasoning for the asymmetric case, as it considers both directions (e.g. 0) as "more extreme". In particularly would mean that we have a 0.5 confidence in the $z\leq\bar{z}_{10}$ prediction, much lower than 0.9 which was given above from the empirical distribution. It appears to me that for solving this problem we have to choose a nonconformity measure $A(B,\bar{z})$ that is monotonic with regard to $\bar{z}$, for example, $A(B,\bar{z})=\bar{z}$. That would make "more extreme" asymmetric, resulting in the following prediction regions for the above example: $\Gamma^{0\leq\varepsilon<0.1}=\{0,1,2,3\}$ $\Gamma^{0.1\leq\varepsilon<0.3}=\{0,1,2\}$ $\Gamma^{0.3\leq\varepsilon<0.6}=\{0,1\}$ $\Gamma^{0.6\leq\varepsilon<1}=\{0\}$ $\Gamma^{1}=\{\}$ given that, $\alpha_i=A(B,i)=i$ $p_0=1$ $p_1=0.6$ $p_2=0.3$ $p_3=0.1$ This means that we can be at least 0.7 confident in the the $z\leq\bar{z}_{10}$ prediction. Notably it still does not match the 0.9 confidence obtained from the empirical distribution. I wonder if I am on the right track... Thanks! Marco