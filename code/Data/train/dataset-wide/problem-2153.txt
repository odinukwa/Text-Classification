Now your database should be visible in the listener services list listener.ora should look like this - note the PORT = 

It looks like the hardening was a little to hard. Oracle does need some access to the server. You can revert back the chmods for ping, Oracle rdbms does not need it. /tmp/ is not required, a usable location for tmp is, mostly for scripting. The rdbms has it's own tricks and storage for sorts. Never install Oracle as root; use the root.sh at the end of the installation to do the system tweeks required. On some platforms there is a root pre script that should run before installation. The Oracle installation scripts refuse to run using the root account and can be run using any other account. The access to some server config files has been made a little too tight for the install user to check the prerequisites. If you know they are ok, it's ok. Proving they are ok is a little easier when support issues arise. Check the group ownership of the critical config files and add the Oracle installation user to that group to give it read access. 

Without controlfile it's a little trickier but still possible. A nice explanation is here How to restore an rman backup without any existing control files Tip: always end your backup by creating a copy of your current controlfile: 

You can not compare the performance of 2 databases by just looking at their sql plans. The same queries can have very different plans. When the versions are different all kinds of computations for the cost are different. The plan that a query follows in execution could very well be different than the plan you get from explain plan from your client. The only way to compare the performance is to measure the execution of the queries or processes and compare those timings. You could use AWR to compare the database performance. A certain workload will generate a certain thoughput and consume more or less dbtime. If the dbtime spent and the elapsed time of your jobs is lower, you can safely say that the performance in that database is better compared to the one where the times are higher. In 11g, with the auto tuning tasks enabled and auto accept sql profiles enabled, it could very well be that the next run already shows better timings. 

rule #1) use rman for backup, recovery and cloning of databases. since this was a user managed backup, and you need your datafiles on a different location, there are a few things to do. 

This problem occurs when your auxiliary (the clone target) database instance is started using a static init file. Smarter is to use the more dynamic spfile so the control_files parameter can be changed by the cloning procedure to where it is restored on the target. I guess the question was: why this happened? ;-) 

Your database instance is open. Your database open_status is a different thing, it could be open READ ONLY. The database is expected to be open for READ WRITE when you switch a logfile. So you might conclude that the error message is not complete. 

A partly online backup is certainly possible but switching to and back from archivelog mode needs downtime. As an alternative you could run in regular archivelog mode and trash the generated archived log files, until you run a backup. For the backup to be recoverable it is important that it has ALL archived log files that were generated during the backup. End the backup with a log switch and also include those archives in the backup. You must see this kind of backup as partially online. It does allow you to restore to the point in time where the backup completed, not to any other point in time. Very similar to regular offline backups, only, taken online. It saves downtime. Why do you want this? This is a lot of effort just to safe maybe 10G of archive storage per day. It also introduces an extra error source and many dba's won't be able to work with this and expect to have all archives. My advice would be to keep it regular online backups, with regular archived log backups and keep things simple. 

The error in you script is that the script now expects a table named tablename, having a column named columnname. In this case you don't know the table and column names so you should use dynamic sql to run this. Next to that, if possible, forget about LONG and implement lobs instead. For docu see $URL$ sample code slightly modified to fit your needs: 

The number one tuning rule AMM (Add More Memory) is a simple one. It is also one that is very costly and at the end one that is not effective when there are problems in selectivity. Even if a database fits completely in memory, the performance of the application can be bad. In a worst case scenario because of locking and latching during very a-selective SQL executions. Those should be fixed first. One reason is concurrency which is like hitting - and holding - the breaks if every SQL accesses all data in a table every time. Make sure no SQL accesses more rows than needed. That is giving the most effective way to keep performance good. A normal database knows how to handle io and does some form of caching of most used data. If your application has already minimized all possible accesses, and you already use the fastest disk systems, consider using real flash memory arrays. They can crank-up performance an other level. 

This depends on the flavor of *nix where you run on and whether the installation was customized or not. If something of Oracle was installed on your system, the directory /etc/oracle or /var/opt/oracle normally would have to exist. Normally they contain a file /etc/oraInst.loc or /var/opt/oracle/oraInst.log that points to the inventory that contains the central registry of the installations done on the host, if it has been done in a standard way. If you found the oraInstloc you know at least that some installation has been taken place. The inventory contains the details about what was installed and where. Normally this directory is protected. If the /etc/oratab or /var/opt/oracle/oratab file has been maintained, it contains a list of all instances running on your system, including the software locations. This is the file that is used by the oraenv utility that sets the minimal environment variables you need to be able to use that software for the specified ORACLE_SID. The oratab, if maintained shows all defined ORACLE_SID's, also when they are not running. But again, asking your dba might save a lot of time. 

No matter what infrastructure we support, we have to support the users of it. A lot of users are developers, so we support the developers to enable them to make the best possible use of that infrastructure. To be able to do this we need to understand each other, with the different ideas and points of views in mind. Having insight to the views from both sides helps to make things better for the business and that is our combined goal. Make IT support the business as effective as we can. In many organizations we see some dba types running in god mode. Most of the times these are not the ones that score very well if competency is measured ..... Often they just hide their - lack of - knowledge behind a wall of words. To my opinion it has nothing to do with being 'programmer friendly' more with being professional. For a dba it means we need to be able to explain why we do the things we do and be prepared to at lease reconsider decision if it helps, without loosing the normal goals like availability, scalability, recoverability and performance. For the programmer it means he has to communicate to the dba, sometimes to teach the dba, sometimes to learn from the dba. My motto on this is: let the first day that I don't learn a thing be the day that the coffin closes above my head. Normal collaboration, having combined teams with developers and dba's certainly help make things easier. 

It depends on the exact version. From 11gR2 we can use job_queue_processes = 0 to prevent any job from running. Before 11gR2 you could use services for that. To use services make the job classes that you want to use depend from a service that is controlled using the instance parameters and leave it out when starting the service. 

You can do most of this by using a login.sql. login.sql is executed during - surprising - login and is loaded from your SQLPATH or current directory. For the examples you gave, your really chose the worst case. Problem is the sqlterminator. Whatever you put in there, the forward slash is maintained as a free sqlterminator. Next to that, sqlplus first scans for the sqlterminator and does this before scanning to the string terminator. A bug if you ask me. The forward slash can be used in a string as long as it is not alone on a seperate line. As soon as sqlplus finds the character specified as sqlterminator, it ignores everything else and stops reading. The forward slash can be handled, as long as it is not alone on a line. login.sql contains: