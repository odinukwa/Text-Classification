I will cover only free will here, as I don't believe anyone has a good enough handle on what morality even is or ought to be for there to be a "widely accepted" view; people seem to do a lot of talking past one another, and also a lot of confirmation of whatever particular cultural and personal notions of morality they have ended up with. The problem is not that you can't have morality in a completely material world; there are just too many different ideas about what "morality" should be. But with free will, I think one can get something close to satisfying, even if there is not perfect agreement. I have been unimpressed with the rigor and groundedness of Harris' philosophical arguments thus far, so I would not recommend e.g. his book Free Will (calling it "No Free Will" would have been nearer to his thesis). If it is the flagship book (I wouldn't know), it would be rather sad, as even if he ends up being correct, you probably couldn't know it from that text. I am somewhat more favorably disposed towards Dennett's stance, though I wouldn't get them from Freedom Evolves (a long book, and one which in my opinion has more than Dennett's usual share of slightly odd conclusions). An article will suffice, e.g. this recent one in Prospect Magazine which doubles as a sort of book review for "Free: Why Science Hasnâ€™t Disproved Free Will" by Alfred Mele (which sounds worth reading, though I haven't done so). To really understand free will in the context of mind-as-a-thing-implemented-by-brains, it helps to have a good understanding of what the consequences are of having mind implemented by brain. Patricia Churchland has, I think, the clearest and most scientifically and philosophically accurate account of this at the moment in Touching a Nerve: The Self as Brain. This also nicely treats the free will problem in a quasi-compatibilist way (chapter 7). And covers morality also (Chapter 4) in probably as fair of a way as you'll find anywhere. (She also has a whole separate recent book devoted to morality.) 

In one sense this is true, because if you ask me to identify a particular word, I need a finite representation of it. But in another sense we can't even identify countably many words; I have finite lifetime and memory and so on, so it seems implausible that I could even check whether two 1080 character long words were the same. And in another sense we can identify uncountably many words by, for example, rules that tell us whether or not a word belongs to some set. The words then need not be finite in length or in a countable set; we can't necessarily manually check their properties or identity, but we can manipulate finite sets of rules that describe them. So already by (2) you're mired in ambiguity, and this sinks the argument when you get to (4) and (5), where in (4) you use a limited sense of identify and in (5) you implicitly use an expansive one. Incidentally, point (3) is either false or adds absolutely nothing: if we encrypt each of our user names, I'll be able to tell that they are different (distinguish) without being able to tell which is which (identify). Or if by "distinguish" you mean "check the equivalence relation of", that's exactly what you have to do to identify something. So you're not even using a different concept here. "Humans cannot well-order items whose equivalence relation they cannot check." (This is true, but you still have the check-by-hand vs. check-using-logic dichotomy that sunk point (2).) 

Anyway, there are plenty of treatises from various people (Daniel Dennett comes to mind), but I don't think you've fully grasped even, say, Dawkins' arguments. If you say 

Maybe there was a minimum level of civilization required to sustain an accurate belief--too early and it's futile because oral traditions keep getting scrambled. Or any of zillions of other reasons. The point is that you cannot conclude anything definite from this. It is a little peculiar; it is probably a little more expected if there is no eternal God and just bands of humans making stuff up (usually without realizing that they're just making it up, I imagine). But you can just toss that into your gigantic intuitive Bayesian probability calculation along with all the other factors. It's not a deal-breaker. Decisions about the truth or falsity of religious belief are subtler affairs than you propose, if approached rationally. 

This is more of a biology question (or possibly English, if it's a definitional issue) than philosophy, but: The change in genetic material is still random. Selection has been nonrandom since the beginning: if it's too cold for you, you are unfit; if it's too hot, you're unfit; if it's too acidic, you're unfit; if you can't escape predators, you're unfit; etc. etc.. Organisms all react to their environment in non-random ways in order to try to increase their fitness (this ability having been selected for). Learning Chinese and upregulating your lactose metabolism enzymes are really just two different instances of this. So, no, the mere fact that ideas are important for our reproductive success these days does not fundamentally change how purposeful evolution is. If we started making genetically modified humans, then one could argue that we're bypassing the random component of evolution and making it purposeful instead. Or if our ideas were focused on how specifically to breed ourselves to encourage certain traits, then one could argue that we are injecting purpose into the process in a novel and nontrivial way that might warrant our changing the way we speak of it. But we're not doing either of those things routinely. Culture and technology are not (biological) evolution, nor is learning. These things are alternative ways to become better adapted to one's environment than random genetic changes. Biologists maintain a distinction between these concepts (where evolution is restricted to mean change in allele frequencies across time). 

(Some rules might be more important than others, so we could assign a weight to getting a certain answer right if we wanted.) A faulty system of morality would have this count (let's call it for short) rather lower than , while a moral system that is as good as we can measure would have a score of . Now we have a relation that can be used to compare moral systems: 

That sounds about right. The definition is far more general than any particular belief system; as is typical with colloquial language, "wisdom" is not precisely defined but is some blend of the four points above such that it seems sufficiently distinct from other things we recognize and give names to. If you get too attached to using "wisdom" to mean "particular insights regarding my non-universally held beliefs", you're liable to end up with conversations like this: 

you do not appear to really be understanding the issues involved--speciation at various levels, drift, the distinction between abiogenesis and evolution, etc. etc. etc.. There is a huge gulf between explaining the first cell and everything just being "maintenance", for instance. I'm not sure that better structured philosophical treatises are what's going to help you here as opposed to a much deeper understanding of the history of life on earth and the physical processes that underlie life and evolution. Planet Earth: Cosmology, Geology, and the Evolution of Life and Environment by Cesare Emiliani is a decent starting point. (If you read various books by Dawkins carefully, you can get a lot of the same material, but it's not very efficient or complete.) Without approximately that much background, I'm not sure it's possible to even understand why Dawkins, for instance, seems to think there are not any gaps for God to hide in any more. 

No, it doesn't work to stop the argument, but it does introduce a pair of extra parameters which are the encoding efficiency e and the reality-simulation partition coefficient p. The latter measures what fraction of resources is devoted to "real" stuff vs. "simulated" stuff (1 = all real), while encoding efficiency measures how many simulated self-conscious philosophizing beings you can have per real being of that sort. These are both functions of layers of simulation, too; one might imagine that the outermost layer would have very inefficiently implemented "real" entities, and could compute existence much more inexpensively. Now, if p << (1 - p) * e then the original argument still holds: there's just way more capacity devoted to simulated beings than real beings. And note that we couldn't tell because we don't know if we're on the outer layer or not. So you just need an extra premise about the efficiency of the simulation to recover the original argument (flaws and all). In the absence of such a premise, the argument is incomplete; you can imagine a counterexample but unless you actually demonstrate it or take it as a premise, it doesn't "prove" the opposite. (Quotes because the whole argument is a bit suspect because of dubious assumptions about the nature of simulations.) 

The advocate and objector in that passage are talking past one another by using different notions of "is"; the advocate is using equality, while the objector is using subset. Which one is correct depends on what you mean by "is" (or, rather whatever word or phrase was used in the original). Modern set theory can express these ideas so clearly that it renders the argument pretty worthless. 

It just shows that after this thought experiment, there would be two of you, one in each of the two bodies. In particular, clone A will respond as you would if you were in clone A's place, as will clone B. But A and B do not have the same place. 

tl;dr Science is the process of finding things out. The sociology of science is linked, but isn't intrinsic to science; it's what we need to do as biased social animals to make sure we don't mess up the process. Full version Science a process that can be conducted in a social or individual context that increases the likelihood that the practitioner(s) will be able to understand and predict recurring phenomena in the world. When we become particularly good at our predictions ("in a vacuum, things experience a force towards each other of G*m*M/r^2"), we like to abbreviate our certainty of reliability by saying that what we know is true. To mistake this for absolute truth is to stop engaging in science. The scientific process includes a variety of sociological components. As social animals--and simply because there is not enough time or talent for individuals to do enough experiments and theorizing themselves--we need ways to share what we've found, and the mechanics of peer review and so on are all there to preserve and enhance our ability to perform reliable predictions in the face of many human tendencies that would naturally undermine them. There seems an unhelpful trend among many (especially among those who study human tendencies under the banner of "social science" or "humanities") to get so fixated on the social aspects of preserving and enhancing scientific results that that gets called science instead of the part which actually contributes knowledge. Perhaps this is what you're getting at, but I think you have the labels wrong (and others may also). Of course one wants to understand these social trends, just like one wants to understand what trends lead to effective governance vs. corruption and waste, or to compassion vs. violence. But engineering is the practice of building things, not the sociology of how to manage human tendencies so we can build things; medicine is the practice of improving health, not the sociology of how to arrange human affairs so that we can enable people to improve health; dance is the practice of artistic motion of the human form (typically to music), not back-room dramas of the Bolshoi theater; and so science is the practice of obtaining reliable knowledge about the world, not the sociology surrounding the core act. For example: when replicating an experiment it doesn't fundamentally matter who does it. You can do it yourself. It's hard to do it yourself, though, in an unbiased way. It's much better if you can get another mostly-disinterested party to do it. But fundamentally it's not the sociology of getting (relatively) independent replication to happen which is science; it's that it happens. You query the world repeatedly: is this how you work? The sociology is just our way of getting those queries done in a reasonable way. If you could meditate and do it yourself, program robots to do it, or whatever, it would be just as much of science: it would still test that the phenomenon is reliable and (if done properly) that you know how to elicit it. One could of course define one's terms differently. This would be a mistake, though, as it would leave us without a word that we could use to compactly describe the process of finding reliable knowledge. Let's not! We do need to understand the sociology of science in order to do science better (and to understand society better), but we also need to understand how to do science--how to find things out--for the sociology to even matter. 

It wouldn't prove anything except that at least it's a simulation that uses something equivalent to adaptive mesh refinement. Basically, the simulator detects when it needs to divide time more finely to get accurate (i.e. not-quantized) results, and then does so. So it doesn't really matter how accurately we can measure. Also, the universe could be not-a-simulation and yet still have quantized time. So it really doesn't matter what we measure. (As an aside: Planck time is a theoretical idea of how finely time can be divided and still be sane quantum-mechanically. But whether or not people find ways around it, it still doesn't solve the adaptive refinement issue.)