$P$ and $\lambda$ the Lebesgue measure are both $\sigma$-finite measure, so you can use the decomposition theorem and the Radon-Nikodym theorem. By the decomposition theorem, there exist two disjoint measurable sets $E_1$ and $E_2$, and two unique measures $\lambda_1$ and $\lambda_2$ such that: $$\lambda = \lambda_1 + \lambda_2$$ with $\lambda_1$ absolutely continuous w.r.t. $P$ (and both $\lambda_1$ and $P$ concentrated on $E_1$) and $\lambda_2$ concentrated on $E_2$. Using the Radon-Nikodym theorem, we then get that $\lambda_1$ has a density $\frac{d\lambda_1}{dP}$ w.r.t to $P$. Let $A_\beta = \left\lbrace x \in E_1 \ : \ \frac{d\lambda_1}{dP}\left(x\right) \leq \beta \right\rbrace$. $A_\beta$ is measurable and the function $f$ defined by $ \beta \mapsto P(A_\beta)$ is increasing and it goes to $1$ as $\beta$ goes to $+\infty$. If $f$ is continuous, let $f^{-1}\left(1 - \alpha \right) = \sup_{f(\beta) < 1 - \alpha}{\lbrace \beta \rbrace } = \inf_{ f(\beta) \geq 1 - \alpha}{ \lbrace \beta \rbrace } $, your minimization problem is minized at $A_{f^{-1}\left(1 - \alpha \right)}$ which has probability $P(A_{f^{-1}\left(1 - \alpha \right)}) = 1 - \alpha$ and has volume: $$\lambda(A_{f^{-1}\left(1 - \alpha \right)}) = \lambda \left( \left\lbrace x \in E_1 \ : \ \frac{d\lambda_1}{dP}\left(x\right) < f^{-1}\left(1 - \alpha \right) \right\rbrace \right) $$ If $f$ is not, there is still a generalized inverse $\tilde{f}^{-1}\left(1 - \alpha \right) = \sup_{f(\beta) < 1 - \alpha}{\lbrace \beta \rbrace } = \inf_{ f(\beta) \geq 1 - \alpha}{ \lbrace \beta \rbrace } $ and the minimization problem has volume $v$ bounded by: $$\lambda \left( \left\lbrace x \in E_1 \ : \ \frac{d\lambda_1}{dP}\left(x\right) < \tilde{f}^{-1}\left(1 - \alpha \right) \right\rbrace \right) \leq v \leq \lambda \left( \left\lbrace x \in E_1 \ : \ \frac{d\lambda_1}{dP}\left(x\right) \leq \tilde{f}^{-1}\left(1 - \alpha \right) \right\rbrace \right) $$ Ok, I'm stuck there. Can one show that $f$ is always continuous ? Or if not, is it always possible to decompose the set $\left\lbrace x \in E_1 \ : \ \frac{d\lambda_1}{dP}\left(x\right) = \tilde{f}^{-1}\left(1 - \alpha \right) \right\rbrace$ into smaller measurable sets to improve things ? 

Max-Cut, at least, in cubic graphs is NP-hard even to approximate to some factor .997. This is due to Berman and Karpinski, 1999: On some tighter inapproximability results. In Proceedings of the 26th International Colloquium on Automata, Languages and Programming, Prague, Czech Republic, pages 200â€“209, 1999. I wouldn't doubt it if the optimal cut is a bisection in the "yes" case of the reduction. 

I always prefer to have error bounds for the CLT, so my favorite reference for your question is the paper "A Lyapunov type bound in $\mathbb{R}^d$" by Vidmantas Bentkus (Theory of Probability & Its Applications 49(2), 311--323, 2005). From the abstract: Let $X_1, \dots, X_n$ be independent, mean-zero, $\mathbb{R}^d$-valued random variables. Let $S = X_1 + \cdots + X_n$ and let $C^2$ be the covariance matrix of $S$, assumed invertible. Let $Z$ be a $d$-dimensional Gaussian with mean zero and covariance $C^2$. Then for any convex subset $A \subseteq \mathbb{R}^d$, $$|\Pr[S \in A] - \Pr[Z \in A]| \leq O(d^{1/4}) \cdot \beta,$$ where $\beta = \sum_{i} \mathbf{E}[|C^{-1}X_i|^3]$. This is a $d$-dimensional generalization of the Berry--Esseen Theorem. 

This question deals with approximating a convex body (a compact convex set of $\mathbb{R}^d$ with non-empty interior) by convex polytopes. For a given $\delta$, let $n_\delta$ be the number of faces needed to approximate any convex body by a contained convex polytope with at most $n_\delta$ faces and Hausdorff distance at most $\delta$. I'm interested in non-asymptotic upper-bounds on this number $n_\delta$ of faces. In the following survey: $URL$ the author gives in (6) a non asymptotic upper bound, on the number of vertices $n_{\delta,v}$ needed to approximate a convex body by a polytope with Hausdorff distance at most $\delta$: $$\delta \leq \frac{C}{n_{\delta,v}^{\frac{2}{d-1}}} $$ He then states "Certainly the same estimates hold for" approximation by polytopes contained in/containing the convex body, and also replacing vertices by faces. So when taking "faces" and "contained in", this is exactly the result I want. Unfortunately, there is no clear reference given to such results. Since this problem is really far from my field, I don't have access to many sources through my department's subscriptions to scientific journals and through my libray. If I knew a precise reference, it would be no problem for me to ask my department to purchase the precise reference or my library to borrow the precise book, but it is very difficult for me to find such a precise reference. So I'd be very grateful if anyone could point a precise reference to this result out to me. 

Alternatively, I believe Hoeffding's original paper on Hoeffding(/Chernoff) bounds treated sampling without replacement. 

My guess is that the optimizer is actually a "strip"; i.e., a set of the form {$x : -t \leq x_1 \leq t$}. But I'm somewhat sure that the solution to this problem is not known. You might take a look at the discussion surrounding after Corollary 3.6 in this paper by Klartag and Regev: $URL$ Barthe may also have some relevant papers. 

This is a nice question. I have to think that the answer must appear somewhere, but I'm not sure where. Here is, I think, a solution for $m = 3$. I guess it could be generalized to any $m$, but possibly with a bad dependence on $m$. By scaling and translating, we can assume $f$ takes on the three values $-1$, $+1$, and $c \in (-1, 1)$. Case 1: $c \in [-\frac12, \frac12]$. In this case, $f$'s range is sufficiently "discrete" that the "usual" proof for Boolean-valued $f$ should work. I.e., things should be okay because $f$'s derivatives $D_i f$ take on largish values when they're nonzero. I can elaborate on this case if you want. Case 2: $\frac12 < |c| < 1$. In this case, let $g$ be the Boolean-valued function $\mathrm{sgn}(f)$. Now $f$ is a degree-$d$ "approximating polynomial" for $g$ in the sense of Nisan--Szegedy, so $g$ itself must have degree at most $\mathrm{poly}(d)$; I forget what's best known these days, maybe $d^6$. Now $fg$ is two-valued and of degree $O(d^6)$. By a simple translation/scaling we can get an $h$ of degree $O(d^6)$ which is the Boolean indicator function that $f$ takes on the value $c$. Thus by Nisan--Szegedy $h$ (and also $g$) depend on at most $\widetilde{O}(2^{d^6})$ coordinates. I think by some simple playing around now you can conclude $f$ also depends on at most $\widetilde{O}(2^{d^6})$ coordinates. I wonder if it's possible to get it down to $2^{O(d)}$ coordinates. 

I eventually found the answer to my question. The answer is no: take $\nu$ the uniform distribution on $[0,1]$. Let $\hat{\nu}_n$ be the empirical distribution obtained from $n$ samples of $\nu$. Let $\Gamma$ be the set of all discrete signed measures. It is a convex subset of the set of all signed measures. For $x \in \Gamma$, $I(x) = KL(x,\nu) = +\infty$ so $\inf_{x \in \Gamma}{I(x)} = +\infty$ and $$e^{-n \inf_{x\in \Gamma}{I(x)}} = 0$$ Yet $$\mathbb{P}\left(\hat{\nu}_n \in \Gamma\right) = 1$$ since the empirical distribution is of course discrete. (Note that it is possible to find a topology making the space considered a Polish one, see Dembo and Zeitouni for example.) So topological arguments are needed, at least in the infinite dimensional setting. Still wondering if the result holds in finite dimension though (it does hold in dimension $1$). 

I think it might still be unknown whether the constant can be reduced below $e$. By the Central Limit Theorem, if it can be so reduced, then it can also be reduced below $e$ for functions on Gaussian space. In Remark 5.11 of Janson's book Gaussian Hilbert Spaces, he says that the best possible constant in the inequality $\|f\|_q \leq c(p,q)^k \|f\|_p$ (for $f$ of degree $k$ and $p \leq q$) is only known in case $p = 2$ (in which case it is $\sqrt{q-1}$). In particular, I guess that means the best possible value for $c(1,2)$ was unknown at the time of his writing, 1997. Note that he gives the argument for $c(1,2) = e$ in Remark 5.13. (It's the same argument that is reproduced in my book in the Boolean case.) Finally, as Janson notes in Remark 5.12, even in case $p = 2$, it's not true that $\sqrt{q-1}^k$ is the best constant that can be put on the right-hand side; it's merely the best constant of the form $C^k$. In particular, when $q$ is an even integer you can slightly sharpen the inequality, by a factor of roughly $k^{1/4}$. (The arguments for this are sketched in the exercises of my book.) 

Let $X$ be a real random variable with c.d.f function $F$. Let $g$ be an increasing measurable real function and assume that $\mathbb{E}\left[g(X)\right]$ exists (and is finite). What additional assumptions do I need on $g$ for the following equality to hold? $$ \mathbb{E}\left[g(X)\right] = - \int_{-\infty}^{0}{F(t) \ dg(t)} + g(0) + \int_{0}^{+\infty}{\left(1-F(t) \right) \ dg(t)} $$ I have seen people using these kind of equalities, but I have never seen a rigorous statement yet. So I would like to know when can I use this transformation, and furthermore I am looking for a reference I can cite when using it. Thank you for your help. Edit: Equality corrected thanks to Alexandre Eremenko's comments. 

For functions that are not boolean-valued, I don't have a lot to say; the main thing I can suggest is taking $p$ in the hypercontractive inequality as you stated it very close to $1$; if it is, say, $1+\epsilon$ then the LHS will have $\widehat{f}(\emptyset)^2$ (which usually you have information about), plus $\epsilon$ times the weight at level 1, plus at most $\epsilon^2$ times the $2$-norm (neglectable if $\epsilon$ is small enough). So this may allow you to "isolate" the level-1 weight after subtracting $\widehat{f}(\emptyset)^2$ and dividing by $\epsilon$. 

How much time does it take to compute $\pi$ to $t$ bits of precision? How much time does it take to compute $\sin(x)$ to $t$ bits of precision? How much time does it take to compute $e^x$ to $t$ bits of precision? How much time does it take to compute $\mathrm{erf}(x)$ to $t$ bits of precision? How much time and how many random bits does it take to generate a (discrete) random variable $X$ such that there is a coupling of $X$ with a standard Gaussian $Z \sim N(0,1)$ for which $|X - Z| < \delta$ except with probability at most $\epsilon$? 

Let $X_1,\dots,X_n$ be $n$ i.i.d random variables taking values in a Polish vector space $\mathcal{X}$ and with (Borel) probability distribution $\mu$. For any convex, compact $\Gamma \subset \mathcal{X}$, it can be proved using Sion's minimax theorem (see for example Exercice 4.5.5 in Dembo and Zeitouni) that: $$ \forall n \quad \mathbb{P}\left(\frac{1}{n}\sum_{k=1}^{n}{X_k} \in \Gamma\right) \leq e^{-n \inf_{x\in \Gamma}{I(x)}}$$ with $I$ the Fenchel Legendre transform of the moment generating function. Using the concept of dominating point (see for example the works of Ney), it can also be shown that for any convex, open set $\Gamma$ (and a few more assumptions): $$ \forall n \quad \mathbb{P}\left(\frac{1}{n}\sum_{k=1}^{n}{X_k} \in \Gamma\right) \leq e^{-n \inf_{x\in \Gamma}{I(x)}}$$ The additional assumptions are actually needed to ensure the existence of a dominating point which implies a much stronger result than just the inequality. It is not obvious to me whether the additional assumptions are needed if we're just interested in this inequality? Actually, let me ask a more general question: are there examples of convex (measurable) $\Gamma$ such that: $$ \forall n \quad \mathbb{P}\left(\frac{1}{n}\sum_{k=1}^{n}{X_k} \in \Gamma\right) \leq e^{-n \inf_{x\in \Gamma}{I(x)}}$$ doesn't hold? Thank you for your insights.