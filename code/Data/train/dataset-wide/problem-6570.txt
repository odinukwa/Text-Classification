Yes, Basque (in the batua dialect) has ala "or (exclusive)" and edo "or (inclusive)", although ala can only be used in questions. You may also be interested in the WALS maps of languages with an Inclusive/Exclusive Distinction in Independent Pronouns and an Inclusive/Exclusive Distinction in Verbal Inflection. 

This seems like a weak case. I'm sure there's a much more thorough rebuttal in the literature, but I'll give at least a few contrary remarks. The second point, that some words in the two languages seem to have some sounds in common, is the most frequent unscientific argument presented in linguistics circles for the existence of a language family. The problem is that between any two sufficiently large sets of data (language vocabularies have thousands of items), there will nearly always be a couple dozen pairs of words that can be extracted that are "similar enough" to be cognates. But that isn't how sound change works - to demonstrate shared ancestry, you must reconstruct words of the common ancestor language, and give a series of regular sound changes that output words in the daughter languages - Grimm's law is a model example of this. The late Basque linguist Larry Trask gave a quite readable defense against the ceaseless proposals attempting to connect Basque, a language isolate, with nearly every other language family, in "Origin and Relatives of the Basque Language" (1995), which I highly recommend for its applicability to this case. I also fail to see how a Proto-Semitic influence would have induced PIE to replace *[k] and *[t] with *[x] and *[θ] when both languages had *[k] and *[t]. Furthermore, PS doesn't even have an *[f] sound, so PIE could not have borrowed it. (You gave *[h], instead of *[x], as the reflex of PIE *[k] - this was a later development in English). Proto-Indo-European had only two tenses, present and past, on verbs in the imperfective aspect. Tense on other verbs was unmarked. The rich tense systems of its descendants are modern innovations. "Ablaut" refers to a morphological alternation already present in PIE - you're thinking of Germanic umlaut, a process which is uncontroversially understood as the product of fronting a stem vowel before a suffix containing [i] (the suffix is later dropped). A substrate influence is not needed to explain this. That being said, you're right that the loss of complexity in the case marking system and the large number of words unique to Proto-Germanic may indicate a pre-IE linguistic substrate. Unfortunately, barring a revolutionary discovery of a new trove of data, we will most likely never know. 

As you can infer from my examples, the split affected not only Middle English */o:/ but also Middle English */u:/: 

American, Australian, Canadian, and New Zealand varieties of English are all strut~put splitting dialects, because most of the initial settlers of the colonies came from southern and western England after the split. The split was probably originally phonologically constrained, but then something happened to make it phonemic in many dialects. 2 Lexical Diffusion Most sound changes start in a set of words, and then spread to more words. This is called Lexical Diffusion, and was described in depth by Kiparsky (1995). It is pretty obvious from pieces written in Early Modern English, that the Great Vowel Shift did not happen all at once to every word in the lexicon. In 1582, John Hart wrote his book An Orthographie in which he proposed a spelling reform due to the change in the language in his lifetime. He designed a phonetic orthography based on his own speech, and spelled the vowel in TIME and BY as , but the vowel in MIGHT and TITLE as <ị>. It is evident, that the Great Vowel Shift started in a group of words, and then spread to others. With the back vowels, we see the least-complete shifts. Kiparsky (1995) noted many patterns with the back vowels. Kiparksy's original explanation for this, was that some words have underlying phonological forms, while others are unspecified in the lexicon, and are instead built by analogy. I personally find that explanation far-fetched, but there are some experimental phonology papers which would support it, I'll get back to those. Another explanation would be an Exemplar Theory approach, where a speaker produces vowels based on their input. The entire system is analogy, but some more high frequency words might either be more innovative, or more conservative. There is a bit of evidence supporting the idea that some words are more lexically encoded with phonological information, while others aren't. In nonsense word vs real word production experiments such as Hay, Drager, and Thomas (2013), speakers often are more conservative with nonsense words than with real words: 

Your analysis is correct - the typical criterion is whether or not the affix/particle seems to act as part of the word it attaches to. When nice phonological demonstrations like vowel harmony aren't available, prosody is the next recourse. If prosody isn't helpful, you're pretty much left with an open question - you have to reason from the rest of the language's typology, and if that provides no further help, you're basically stuck. Many languages with a long history of being written don't follow the above criteria exactly, and often divide words from affixes based on either the etymology of the morphemes in question or on historical spelling patterns. Some languages, in either their native systems or in romanisations, largely ignore the criteria, though - Japanese particles seem very much like they ought to be considered affixes (certainly they're not separable prosodically, and they've undergone some word-medial sound changes to their initial consonants), but the prevailing non-scientific romanisations (and several scientific romanisations) treat them as separate words. 

No, there is no specific term for how well an onomatopoeic word approximates the sound it refers to, because there is no way to measure how alike two sounds are. Remember that nearly every utterance made by the human vocal apparatus is a combination of several articulators sounding in unison (or phased relative to each other), making their output even more difficult to compare to any given sound in nature. 

According to Twitter's new Terms of Service (see paragraph (I)(4)(A)), you may not export any data retrieved from Twitter, whether through its public API or by scraping. So you will probably never find a readily-available corpus of tweets - and if you do, it's in violation. Twitter has already taken a lot of flak for essentially shutting down academic research into a whole new frontier of linguistic data. However, if you'd like to publish your results after analysis, you may provide a database of tweet IDs and usernames, which other researchers can cross-reference with the official Twitter archive. I'd recommend the Streaming API to start aggregating data. 

It's purely based on the fact that the standard variety is based off the speech of a region these speakers don't come from. If theirs was the standard and yours wasn't, they'd say the same thing about your 'accent'. Every form of speech is intelligible to those that speak it - and if parts ever become problematically unintelligible, there are natural mechanisms for fixing that problem (e.g. the rampant compounding in North Chinese making up for a drastic reduction in the number of available syllables, making distinctive bisyllabic words out of problematically less distinct monosyllabic ones). Indeed, areas farther away from sociopolitical centres tend to change slower, due to less pressure to keep up with prestigious norms; though this is at times counteracted by a tendency for areas that are largely isolated from most other speakers to change faster, as they have less of a need to remain intelligible to passers-through. So regional variation can involve varieties that are both closer to and farther from their common ancestor. In Appalachia, Shakespeare remained intelligible with little effort as recently as the last century; and the same remains true of the sagas in Iceland (indeed, I knew a guy who spoke Faroese and took Old Norse as a blow-off class) - yet for me, who speaks enough Norwegian to get by, they are largely nonsense. On the other hand, the languages in the Ryuukyuu Islands south of Japan are -spectacularly- more innovative than mainland Japanese - to me they look like they ought to be spoken by time-travellers from the future. There's also a tendency for similar-enough varieties of speech to become more intelligible simply through exposure. In the early days of sound films, British films had to be subtitled for American audiences and vice-versa, but once people had heard enough of each others' varieties, the need for subtitling vanished. So it's just an effect of the fact that many people speak versions of a language that isn't the version of a language the majority of people are most familiar with, and their unfamiliarity creates a general perception that non-standard dialects are somehow more universally hard to understand. 

My copy of A Course in Phonetics isn't on me at the moment, but if you're open to using software, I'd recommend Praat for its automatic formant tracking feature. 

Yes, the same answer that was provided in the previous question applies here. Several Slavic languages have an animate/inanimate distinction in addition to masculine/feminine/neuter. In some languages, only certain combinations are permissible - for example, Russian only distinguishes between animate and inanimate in the masculine. It's also been hypothesized that Proto-Indo-European possessed only an animacy distinction, with masculine/feminine developing out of the animate class of nouns and the inanimates becoming neuter. This idea appears to originate from Meillet (1926), and was further developed by Gamkrelidze and Ivanov (1973, 1984). Gamkrelidze, T.V. & Ivanov, V.V. (1973). "Sprachtypologie und die Rekonstruktion der gemeinindogermanischen Verschlüsse". Phonetica 27. 150-156. Gamkrelidze, T.V. & Ivanov, V.V. (1984). "Indoevropejskij jazyk i indoevropejcy". Rekonstrukcija i istoriko-tipologicheskij analiz prajazyka i protokul'tury. Tbilisi: IzdatePstvo Tbilisskogo Universiteta Meillet, A. (1926). Linguistique Historique et Linguistique Generale. Honore Champion, Paris. 

One language I speak, Polish, has a fairly asymmetric vowel inventory despite having only 9* vowel phonemes (if you count the nasal diphthongs as vowels, that is) /i/ /ɪ/ /ɛ/ /ɛ̃ w/ /ɔ/ /ɔ̃ w/ a/ (The vowel [e] occurs as an allophone before /j/ or a palatalised consonant, the and /ɔ/ occurs as [o] before /w/ or /l/) There are two high front vowels, and the default tends to be +lax/-tense, rather than +tense/-lax (which is fairly unexpected). So there are languages with asymmetric vowel inventories, but none as extreme as only having front vowels. There are many reasons why this wouldn't happen, or would be highly improbable. For example, there is a tendency for languages to end up with an asymmetric inventory if [u] shifts forward to [y] or [ʉ], and sometimes to even shift forward to [y] and then un-round to [i]. Usually, what happens though, is the vowel does not shift forward before [k] [x] or another velar consonant, and then they wind up in allophonic variation, or over time, even in phonemic variation. Another tendency is for [o] to raise and fill the place of where [u] used to be. So let's say a 5 vowel inventory language has: /i/ /u/ /e/ /o/ /a/ It shifts: /i/ /y/~/u/ /e/ /o/ /a/ Then, a few generations later we have: /i//y/ /u/ (o shifted up) /e/ /a/ Or even: /i//y/ /u/ (the vowel became contrastive) /e/ /o/ /a/ *There is debate on even this in Polish, sadly. The controversy is over the front vowels, and whether the contrastive vowel is [ɪ] or [ɨ]. In many Slavic languages, such as Russian, there are two vowels in allophonic distribution: one occurs after palatalised consonants and /k/ [i], the other after non-palatalised consonants [ɨ]. Sanders (2003) identified the Polish vowel as [ɪ], at least in Warsaw Polish. In Polish, the only consonants with a palatalisation contrast are the bilabials, and before a back vowel. Some Slavicists like to transcribe the bilabials before /i/ as being something like [mʲ] instead of [m]. If you don't do this for pure consistency-with-other-Slavic-languages concerns, it is fairly obvious that /ɪ/ and /i/ are minimal pairs: /mi/ 1sg-DAT /mɪ/ 'we' /bitɕ/ 'to beat' /bɪtɕ/ 'to be' They are phonemic at least in the environment of bilabials. References: De Boer, B. (2000). Self-organization in vowel systems. Journal of phonetics, 28(4), 441-465. Lindblom, B. (1986). Phonetic universals in vowel systems. Experimental phonology, 13-44. Sanders, R. N. (2003). Opacity and sound change in the Polish lexicon (Doctoral dissertation, UNIVERSITY OF CALIFORNIA SANTA CRUZ). Schwartz, J. L., Boë, L. J., Vallée, N., & Abry, C. (1997). The dispersion-focalization theory of vowel systems. Journal of phonetics, 25(3), 255-286. 

The problem with the language/dialect distinction is that it always, always, always involves politics. Even when trying to do things on a purely linguistic basis, there are far too many borderline cases and questionable situations (e.g. dialect continua, one-sided intelligibility, etc) - you will never end up with a wholly unbiased classification. That said, most (maybe even all) non-Japanese linguists recognise Ryuukyuuan as a group of languages separate from Japanese, regardless of how many Ryuukyuuan languages they count. I would guess that the 'canonical' Japanese view, that Ryuukyuuan is just a set of Japanese dialects, comes from nationalism and the very inaccurate and outdated view of Japan as an almost totally homogenous country - even Ainu was called a 'dialect', and it's totally unrelated to Japanese! I don't know how many Japanese linguists still regard them as dialects instead of languages, though. The linguistic criteria fairly unambiguously support the separate-language analysis - there is a clear gap in intelligibility between the Kyuushuu mainland and the Amami archipelago. It actually turns out that Ryuukyuuan as a whole is genetically closer to Northeast Kyuushuu than anywhere in the south, but it took until 2003 (in a paper by Leon Serafim) for anyone to notice. If you show Japanese speakers a video of someone speaking Ryuukyuuan (without explaining it) and ask them if it's Japanese, they will definitely say it's not - I actually came across a video on YouTube recently that was a quiz asking the viewer to distinguish audio clips of Japanese dialects and Ryuukyuuan languages from clips other East Asian languages :P Now, it may turn out that Ryuukyuuan is genetically a sub-branch of a Japanese-internal dialect group, but it has been innovative enough since it split off that it still counts as a group of separate languages. I don't know about Hachijou - I know it's very very different from the rest of Japanese, but I don't know if there's a consensus as to a separate status. If I were to venture a guess, I would bet that not only Hachijou but also a number of other dialects from Japan's main islands ought to be considered separate languages rather than dialects - though these might be a bit more muddled by the dialect continuum problem than Ryuukyuuan or Hachijou. 

A diphthong is one sound segment created by a smooth transition between two targets within the same syllable. As a phonetic definition, this makes no theoretical claims about which phoneme(s) represent the articulation in the mind of the speaker. This statement is a bit vague, but is, as far as I can tell, true in some situations; see below. The number and character of underlying phonemes that a diphthong corresponds to varies by language and which researcher you talk to. Different kinds of evidence for a particular interpretation of field data are evaluated differently by each specialists, and mainstream phonology has yet to produce a theoretical framework that gives one clear answer to this question for each language. In English, for example, the diphthong [e͡ɪ] could be represented underlyingly as either the single phoneme /e͡ɪ/ or a sequence of two phonemes /ej/, where /j/ is the same segment that appears in /jɛs/ "yes". The latter analysis is tempting, since it would reduce the size of the phonemic inventory (all possible underlying segments) of English, but would have to explain the phonetic differences between the /j/ in "yes" and the /j/ in "made", which is much more like an [ɪ]. Of course, the addition of a transformation rule would be standard practice when faced with this situation, but we are then left with the basic question of which is simpler (and thus, more likely to be adopted as a strategy by native speakers): fewer phonemes, or fewer rules? 

That is just a /t/ sound, not a morpheme. This makes as much sense as asking where the /n/ in near and nickel come from. The words night, fight, and others like right historically had a fricative before the /t/, and words like through and rough ended in that fricative (compare English through [θɹu:] with German durch [dʊɐ̯çç]). In Middle English, the fricative likely varied regionally, and it inconsistently either deleted (through, plough, night, fight, wight, drought) or became [f] (rough, trough, enough) in Modern English. Just because night and fight end in -ght it does not mean that sound was one morpheme. It just happens to be the consonant cluster they ended in. 

Under an exemplar theory approach, we would expect new words to be less innovative than high frequency words because a speaker hears less of them. References: Hay, J., Drager, K., & Thomas, B. (2013). Using nonsense words to investigate vowel merger. English Language and Linguistics, 17(02), 241-269. Kiparsky, P. (1995). The phonological basis of sound change. The handbook of phonological theory, 640, 70. Wells, J. C. (1982). Accents of English (Vol. 1). Cambridge University Press 

I'm trying to rationalise (General American) English vowel pronunciations as a system with seven-or-so underlying vowels, varying in realisation due to length/stress (unstressed, short stressed or long stressed) and environment (e.g. being deleted before /ɹ/, and so on). I haven't come up against any counterexamples yet, but they may well be out there; and I'm not wholly sure that my methodology makes sense (where the underlying phonemes are so very far removed from any of their realisations - e.g. /aː/ typically realised as [ɛj]). I suppose my question is this - is the fact that you can fairly well analyse Modern English vowels as having length just an artifact of history, or are length distinctions still part of Modern English's active phonology? 

A sentence is infelicitous when it's grammatical but nonsensical. See, for example, Chomsky's famous 'colourless green ideas sleep furiously' - there's nothing wrong with the sentence on a grammatical level, but actually attempting to turn it into useful information fails nonetheless. If ungrammaticality is a grammatical problem, infelicity is a semantic problem.