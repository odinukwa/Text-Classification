There are several ways that you can do this: Use a build server I've heard of teams using CCNET.net or FinalBuilder Server for this. Basically, what happens is that the build script has code to push the latest build, every time somebody makes a check in. I wouldn't recommend this for production though. This should work fine for a staging environment though. I'm not very familiar with build servers on Linux, but I know that there are a few. There's also Ant, and even Make scripts that could be used for this. Put a working copy on the staging server Just do a check out, and map the correct folder to be the root of the web app. Somebody on the team will have to manually update this working copy, but this gives you the flexibility to revert to a previous version if necessary. Caveats You'll need to make sure that you exclude the .svn folders when you pointing the website to the working copy. I wouldn't recommend using a Subversion hook script for this. You can pretty much use any scripting engine that available in Linux to do post-commit scripts. I typed "post-commit hook script linux" in Google, and got some good hits. 

You don't write anything about the kind of failure on the non working server. I'll assume that the problem is not a broken RAID due to failed hard drives. Obviously, in that situation, it would be of little help to move the broken disks to another server ... Generally the RAID configuration is stored on the disks by the H700 (and also most other RAID controllers nowadays). This is supposed to make it easy to move RAID sets between similiar controllers/servers. You just need to move the disks to the working server (I would make sure to plug them in the same slots nevertheless). When booting up you'll have to enter the RAID BIOS. There will be a menu "Import foreign config". In a normal situation (i.e. all disks working perfectly) the controller is even supposed to detect this by itself: 

One way of getting this to work is to download a copy of SQL Server Express, and restore the backup there. Sometimes this doesn't work, so you could try downloading a trial version of SQL Server Standard. I think it has something like a 180 day trial. Once you got the database restored, then you can use your favorite ETL tools to move the data into PostgreSQL. 

There's several good alternatives now days: Free: CollabNet Subversion - $URL$ VisualSVN Server - $URL$ Commercial: PainlessSVN - $URL$ 

There are many more servers out there. The ones I listed here are the ones I had tried at one point in the past. 

If you don't mind a commercial solution, there a business telnet client called TeSSH, that has a rich scripting language, and can be run from the command-line. It has support for writting scripts in vbs, perl, and lua. It's fairly inexpensive at $34.95. You can also visit the TeSSH Support Forums. 

The task I want to accomplish now: I want to build a offsite server that is a perfect mirror of this backup server and which is syncing daily with the above described system. The Problem: I haven't found any solution yet, which lets me easily sync the whole server / zfs filesystem including all snapshots. I only know that you can send a single snapshot to a remote zfs filesystem via zfs send. Obviously it would be great to not have to recreate the complex pool & snapshot structure on the offsite server because I feel this would be a total mess maintenance wise. Is there any solution which allows to mirror a complete zfs filesystem including snapshots? 

Does write-through cache mode cause a performance degradation of the array over time? I don't think so. With an activated write-cache, the controller is able to perform certain operations to optimize the pending write operations. For example this description of the cache operation is taken from the whitepaper for HP Smart Array controllers: 

I am currently having an m1.large instance acting as a memcache server. I would like to replace that with 2 m1.small nodes for High Availability more or less for the same cost. But I am concerned about the impact of the moderate IO performance of m1.small. I read the following as if moderate IO would affect network system as well. Not sure if the fact that there would be more nodes counterbalance the more limited IO capacity of the small instances. Anyone had experience with memcache on these instance types that can comment? Many thanks in advance 

A bit late on this but you don't really want to have memcache nodes autoscaling. Hashing will drift so you will be losing large percentage of your cache everytime there is a scale up or scale down event. 

I'm afraid I must answer my own question and the answer seems to be No. Using the command it turns out the original cert delivered by Comodo already lacks the needed flags in the field. It doesn't have the feature. The Comodo cert looks like this: 

Most MySQL installations come with networking disabled by default, e.g the my.cnf contains a line similiar to which limits the network access to localhost for security reasons. If you choose to enable networking by altering this line you need to make sure that you reconfigure all your MySQL users, to distinguish between local and remote MySQL users. You'll want to make sure all your remote users also have in their GRANT statement to enforce encryption for all remote connections. MySQL has a step-by-step setup for secure remote connections here. On most (all?) linux based webservers MySQL remoting is disabled and the commonly accepted way to open up a remote MySQL connection here, is to tunnel SQL through SSH, so MySQL can be left unchanged and configured to allow only local connections. This is also possible on Windows Servers, but of course only with the added overhead of installing an SSH server (for example via Cygwin) first. I would personally prefer to use the SSH variant as it allows MySQL to be left unchanged - you don't need to mess with your users & permissions inside MySQL. Also as a matter of personal opinion, I would think opening up MySQL as a protocol over the network is widening the attack surface of the server to a greater extent, then having SSH running does - especially if you are using key based authentification with SSH. 

We are currently preparing for a move of a reasonably high traffic web site to the cloud. We are thinking of using scalr to help us manage the whole setup especially since we dont have experience with amazon. We are unsure about whether we should use Scalr's MySQL functionality that relies on EBS backed EC2 instances or whether we should be making use of RDS or even xeround and enjoy much easier maintenance and management. Our dataset is about 40GB and we consume a bandwidth of 4000 GB per month between the application server and the database server. any experiences on similar setups? thanks in advance 

we are designing a cluster of 2xnginx, 2xapache, 2xmemcache, 2xmysql servers for a high traffic web site. Current web site runs on 1 dedicated web server and 1 dedicated db server. the current web server sees a peak of ~60Mbps incoming traffic and 25Mbps outgoing traffic. the current db server sees a peak of ~60Mbps outgoing traffic and a peak of ~20Mbps incoming traffic. For our new setup we were thinking of having 1Gbit connections between the various nodes but the provider supports 1Gbit only if we dont use KVM cards as they only run at 100Mbit. Based on the figures above I have the feeling that 1Gbit connectivity will not give us any benefit and the above limitation is not a show stopper. Can any seasoned web cluster admins confirm or advise against this? 

One way of doing this is through path-based authorization. This way you could setup the archives to be viewed only by those people in your team. Here is a page from the Subversion Red Book about Path-Based Authorization: $URL$ You will be able to set the permissions using what's called an authz file. Just make sure to test this with different users. Play with the different settings. Once the authorization settings are to your liking, then you can open your firewall port to the WAN. Subversion has 2 servers, svnserve (svn:// protocol) and Apache-WebDAV (http:// protocol). I recommend that you use VisualSVN, if you choose the HTTP protocol. I have a product that handles the svnserve server. It's called PainlessSVN. There's a couple places where you may be able to get help. WanDisco Subversion Community Subversion Forums 

Regarding Scalr's open-source vs hosted version, GUI is the same - I am just not 100% sure if the open source version is 100% up to date. We are using the hosted version. Scalr itself (if you chose to host it yourself) needs to run separately in order to manage your EC2 instances. This will give you a nice overview: $URL$ In theory you can migrate your current instances to scalr but I am sure devil is in the details. Some careful planning would be required in order to complete the migration: $URL$ If you use standard components, you might prefer to use Scalr's ready server templates (Roles in scalr terminology) those as starting points and modify as needed. In general I can highly recommend Scalr as a value for money alternative to Rightscale. You will surely appreciate the automation and configuration capabilities vs managing things manually via the aws interface. 

My favorite utility is RAdmin by Famatech. My favorite feature is the ability to work behind NAT firewalls. It is mainly a remote control administration tool, but it has different connection "modes" that lets you work with the specific computer in different ways. There's another tool that I want to try, but haven't yet. It's called Network Administrator by IntelliAdmin. It will let you a lot of stuff in the background, without interrupting your users. These company also has several other useful tools. 

That looks like as if the Subversion server is not running. I've done that quite a few times myself. I've done this so much, that I created a page for me to look it up. I use the custom svnserve.exe server. If this is what you're using then look at the URL below for how to turn it into a Windows Service. $URL$ I've settled on the CollabNet binaries, as they have a nice install package for both the svnserve and Apache editions of the server. 

Basically this error message only tells you that the battery of your RAID controller is bad/gone/not present and that your virtual disk caching mode was therefor set from write-back to write-through. The reasoning behind this is that your controllers cache is not backed by the BBU (anymore) so it's unsafe to use it for write-caching in the event of a power-loss. In the moment the only problem which should occur is slightly degraded write performance. Normally the PERC5 also beeps on boot and I'm pretty sure to remember they also have a special error message for battery low-power, which makes me think that in your case: 

The existing system: I have a FreeBSD based backup/archive server, which pulls the backups from the live servers via rsync every night and does a zfs snapshot to archive the backup contents for a specific time. This results in the situation, that on this server there are lots of ...