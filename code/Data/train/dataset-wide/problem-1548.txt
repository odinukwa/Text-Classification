There are multiple formulae for how to calculate this angle. The simplest is to construct the unit vectors: $$\begin{align} \hat{n}_i & = \left[\begin{array}{c} \cos \delta_i \sin \alpha_i \\ \cos \delta_i \sin \alpha_i \\ \sin \delta_i \end{array}\right] \end{align}$$ take their dot product and use $\hat{n}_1 \cdot \hat{n}_2 = \cos \theta_{12}$. The problem with that approach is that it loses numerical accuracy very quickly when $\theta_{12} \ll 1$ radian. Thus the Haversine formula: $$\theta_{12} = 2\operatorname{arcsin} \left(\sqrt{\sin^2\left(\frac{\delta_2 - \delta_1}{2}\right) + \cos \delta_1 \cos \delta_2 \sin^2 \left(\frac{\alpha_2 - \alpha_1}{2}\right)}\right),$$ is often preferable. There is an algorithm that covers all cases, Vincenty's formulae, but it is much more complicated to implement. You can find a good discussion of practical differences at this Geographic Information Systems Stack Exchange Question. 

No. The moon is an extremely hard vacuum with an atmospheric pressure of $3\times10^{-15}\operatorname{bar}$. The 777 has a service ceiling of $13,100\operatorname{meters}$ where Earth's air pressure is about $1/10^{\mathrm{th}}$ ground level, the air pressure is about $0.1 \operatorname{bar}$. So, there's no oxygen to burn in the engines to generate thrust, no atmosphere to generate lift against the wings. That's why rockets carry both fuel and oxidiser, and don't really bother with wings. 

Not likely. The sun shade will always be pointed towards Earth and the sun. Thinking about the design, that means the telescope will never be pointed in a direction where it will have gegeschein in its field of view, because otherwise the heating from IR radiation from Earth would cause it problems. The term for the angle between where you're looking and where the sun is is "solar elongation". I don't know what the ranges of solar elongation JWST will be limited to, but I'm sure it excludes a substantial angle around $180^\circ$. The Spitzer space telescope is similarly limited in the parts of the sky it can view at any one time by the need to keep the sun behind it's shade. 

Drizzling is just one technique in the field of super resolution imaging. Note that claims that you can't "pass the resolution limit" of a camera are wrong, in principle. In practice, it's really hard. What limits your ability to construct an image of what the camera is looking at are: the amount of random noise in the pixels, and your understanding of the camera's point spread function (PSF) (though some astronomers prefer to use the term "point response function" [PRF] if you include the effects of both the optics and the detector). In principle, if you invest enough time and money to nail down the PSF and increase the signal to noise ratio without limit (beware of bias), the object you're imaging isn't changing, and you have enough accuracy in measuring how you point the camera when dithering, then, in principle, you can push the resolution of the reconstructed image as far as you want. In practice, it's usually cheaper and more time efficient to just get a bigger camera. For a concrete mathematical example of how image reconstruction works, consider the sum of two offset Gaussians, with $\sigma=1$, $\mu_1=-0.1$, and $\mu_2=+0.1$. Here's a graph of such a curve overplotted with a single Gaussian with the same $\sigma$, scaled to have the same height at $x=0$. You can only see one line plotted, though I assure you there are two. 

Yes. This is is actually something that is done in the analysis of redshift data. If no correction to the redshifts are made, then the redshifts are known as "geocentric." When you correct for the motion of the Earth around the Sun, the redshifts are known as "heliocentric" (see, for example, the description of data in the 6dFGS database). Less common is to correct redshifts for the motion of the Milky Way all the way to the cosmic microwave background's (CMB) frame. This is done by applying a shift to a frame where the CMB dipole is zero. There are two reasons why this is less common. First, the CMB dipole was only well measured relatively recently by the WMAP and Planck missions. Second, correcting for the solar system's motion relative to the CMB fixes only half of the problem. The target galaxy will also be moving with respect to the CMB at speeds that are probably comparable to the Milky Way. This fact leads to distortions in when we try to use redshift as a proxy of distance, some of which are called "Finger-of-God" for their tendency to make plots of galaxies appear to contain filaments that point at the observer. 

Yes. The energy density in the radiation field scales like $a^{-4}$, and the volume scales like $a^{3}$. Since the total energy is the density times the volume, the total energy scales like $a^{-1}$. Note that $a$ is called the "scale factor" and $a = \frac{1}{1+z}$, where $z$ is the redshift, assuming $z=0$ now. 

What you're missing is that the resolution of a prism isn't high enough to resolve the relatively narrow spectral lines. What's more, the light that gets generated by fusion reactions doesn't reach the surface of the sun for a very long time, and it get's scattered and split a large number of times along the way, removing any signature of fusion from their origins. The reason we can be confident that fusion is happening in there is because we can detect solar neutrinos that exit the sun almost completely unimpeded, and have the expected spectrum from the fusion process. For more, have a look at Kirchhoff's laws of spectroscopy. Back to the sun's spectrum. We have a few reasons to be confident in our ability to measure absorption lines of the sun independent of the atmosphere. First, the amount of atmosphere we look at the sun through depends on the angle of the sun in the sky (a concept called airmass). So, if we observe the sun at different angles, and keep track of how the absorption changes with airmass, we can extrapolate the spectrum back to zero airmass. Second, there is at least one element that was first discovered by it's absorption lines in the solar spectrum: helium (named for the Greek god of the sun, Helios), so we can be confident that the measurements we're making correspond to physical reality. Third, we have performed measurements using satellites that are above the atmosphere. 

When analyzing an image I'm used to thinking of the pixels as a grid of boxes the photons have fallen into. I assume that there's zero space between the boxes. That seems obviously wrong, since there needs to be some separation on the detector to prevent electron bleed between pixels. So, what's the real geometry of pixels in a detector like (especially for: CCDs and infrared arrays)? Is it closer to a grid of points than a grid of boxes? 

There's a quick and dirty example of the effect I'm describing in Figure 2 of the Ghez et al. (2008) paper Rob Jeffries linked to below. The difference between the red points and blue points is both exposure time (more photons) and higher Strehl ratios (a measure of how close to diffraction limited the image is and, hence, a proxy for resolution given the same optical system). Generically, you could say that the question is: how do we determine the confusion limit in images with point sources? This is just specifically applied to the field of view on the sky with the highest density of resolvable point sources available. Concretely, say we wanted to resolve $22^{\mathrm{nd}}$ magnitude sources in views of the galactic center, what resolution is required to do so? Getting to the full answer to the more concrete question "how deep would we have to look to see a background quasar" would require additional information: size of field of view, and dust extinction along the sight line. So, for simplicity, I'm assuming that if you're resolving $22^{\mathrm{nd}}$ magnitude sources in $K$ band, you're probably seeing a quasar. Thus we only need the projected density on the sky of sources brighter than that cutoff, roughly, and some description of how resolved sources need to be from the background brightness to detect them to answer the question. 

I'm pretty sure that the radial pattern found in the data is a result of WISE's approximately 90 minute sampling cadence (dictated by the satellite's orbit), astrometric precision (about 0.2 arcseconds in the stacked images around launch, see Wright et al. 2010), and the number of free parameters in fitting the asteroid orbits based on that data. See, in the actual images the asteroids appear as points of light that shift appreciably between frames. IIRC, they expected 7 to 12 observations per asteroid. So you have 10-ish observations spanning 15 hours or so to fix the asteroid's orbital parameters around the sun. As you can imagine, there will be more parameters than can be perfectly fit with a single pass in this data set alone. At a guess: it's related to quantization in uncertainty estimates and how that feeds forward into the orbit fitting algorithm. I don't know the details behind the striping, but I'd bet it's related to the numerical precision used in the early processing of the data. They've either refined the orbits since using observations from passes separated by ~6 months, or modified how they're handling the numerical precision in their astrometric measurements since. More likely the former, but I'm sure if you asked Amy Mainzer (PI of NEOWISE, and head of the asteroid hunting part of the mission), Roc Cutri (head of the database creation and data processing part of the team), or any of the people on Mainzer's team, they could tell you more. Relevant background: I was Ned Wright's grad student (original PI of WISE), and he had me design and test an asteroid hunting algorithm in the run-up to launch (we ultimately didn't use it - it scaled like $N^2$, IIRC, and existing efforts in the literature scaled like $\log(N)$ or $N\log(N)$). I ended up working with the extragalctic part of the team, hence my uncertainty on the precise details of what the solar system team did. I think I asked them about the striping during a presentation, but I don't recall the answer, so I'm pretty sure the answer was mundane and unavoidable. 

There are between 90 and 254 stable nuclei all the way up to element number 82. In discussions and graphs about big bang nucleosynthesis nothing above lithium is even mentioned. It's a pretty safe bet that none of the heavier elements have been observed in reasonably primordial gases, since the implications would be profound enough to require mention in even popular level books about the big bang. That said, has anyone bothered to perform searches for all of the heavier elements? As in, do we have explicit experimental upper limits on the concentrations of some or all of these heavier elements, or just a more qualitative, "No evidence seen for anything else"? 

Use this equation: $$F_\nu = \frac{L_\nu}{4\pi D_L^2}.$$ This is the relationship that defines the luminosity distance, $D_L$, in a static Euclidean universe (i.e. not ours). Hogg has a good review on the arXiv of how to handle the relationship in an expanding universe, including when you need to perform something called a "$K$-correction" for spectral densities like the question asks. 

My understanding is that the matter is not settled, definitively. There is a chain of logic that goes something like this: 

There are two main explanations put forward for the inability of the gas in really large galaxies to collapse and form stars. First, the most massive galaxies tend to be found in clusters of galaxies, and the gas in the galaxy clusters is too hot to collapse and form stars. Second, when large galaxies of comparable mass merge simulations suggest that the merger process heats up the gas in the galaxies, cutting off star formation. 

The mass of the sun is just a unit of convenience in a astronomy. The sun's mass and luminosity, in particular, are relatively easy for us to measure precisely, and when we're talking about stars, provide a convenient scale where the numbers won't be too "astronomical" (be too high a power of $10$ to picture easily). You could derive the Chandrasekhar limit in grams, kilograms, or slugs from the relevant physics, if you wanted. The relevant equation (from the Wikipedia article) is: $$M_{\mathrm{limit}} = \frac{\omega_3^0 \sqrt{3\pi}}{2} \left(\frac{\hbar c}{G}\right)^{3/2} \frac{1}{(\mu_e m_\mathrm{H})^2},$$ where $\mu_e$ is the average molecular weight per electron (stellar composition dependent), $m_\mathrm{H}$ is the mass of hydrogen, and $\omega_3^0$ is a numerical constant that is approximately $2.018236\ldots$. It should be noted that the Chandrasekhar limit is a limit on the mass of the final white dwarf, not of the object that will produce the white dwarf. 

Pixel based detectors, particularly optical CCDs like what was used in the SDSS camera, are ubiquitous in astronomy. Is there any dead area on the detectors? Not the obvious gaps between each individual sensor, but on the sensor itself. That is, does a typical detector have any gaps between the pixels from microscopic wires laid on the front, alternating doping regions in the silicon, or just areas where a photon can hit and the photo-electron is unlikely to be collected? What fraction of the area is dead? 

As you can see, the theoretical difference is not zero. So, if what you're imaging isn't changing, you can model the background accurately enough, and you have a sufficiently accurate model of the camera's PSF, you could detect the difference between the high Gaussian and two slightly offset Gaussians scenarios if you invest enough time and energy into the project, in principle, even if your pixels are twice the full-width at half maximum (FWHM) of the Gaussian. $\mathrm{FWHM} = \sqrt{8\ln(2)}\sigma$ for Gaussians, so in this graph twice the FWHM is about $4.7$. One way to further the goal of understanding the PSF is to intentionally make it easy to model (as opposed to 'as small as possible'), as is done in many forms of non-redundant aperture mask imaging. I believe that the trade-off there is that you lose both field of view and the (obvious) loss of the light that gets blocked. In practice, though, you're usually better off getting a bigger camera, or compensating for the atmosphere (e.g. lucky imaging and adaptive optics) if your imaging is seeing limited. Thus, the limits imposed by the Nyquist sampling theorem are more practical, not theoretical. It's also prone to mislead you if you don't remember that pixels are not points, they're little buckets that collect light that falls into a finite area. Talking about it "sampling" some function, implicitly uses the mean value theorem to say that the amount of light collected divided by the pixel's area gives the value of the image's brightness there. What's often dropped is the caveat that it's the brightness somewhere inside the pixel, not necessarily at the pixel's center. This is usually a very small effect, which is why it's often ignored, but when you're talking about working at the resolution limit and having pixel sizes comparable to that resolution, the distinction may become important. Roughly speaking, if the PSF is highly curved within the pixel, the offset from pixel center of the mean value can become significant. One way to think about what drizzle gets you is that by moving the pixels between images, you move around the light buckets. In principle, with enough images at different offsets you can probably push the pixel density of the image to the limit of your ability to reconstruct the size of the offsets (usually the pointing of the camera/telescope, though I know of no reason why the sensor couldn't be moved around on some stepper motors attached to micrometers), or the sharpness of the divides between the pixels (the ability to gather light is neither uniform over a pixel, nor could the pixels fill the detector from edge to edge completely, these are just really good approximations most of the time).