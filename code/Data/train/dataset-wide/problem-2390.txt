Of course the overall design and query is much more complex and more details are in action in selecting next new word for a given learner. Now, imagine that words list contains 100K words, and a learner has already learnt more than 5K words. Using the given query, this gets slower and slower and slower by more learners learning more words. Is there a better design for these types of business requirements? How to design for scale in this case? 

And this query works lightening-speed fast for past 2 years. But when I change the part to a closer date, it freezes out and takes more than 2 minutes to complete. In other words, based on different inputs, it behaves differently, sometimes even hanging out and not returning for more than 10 minutes. I expected a consistent behavior. What do I miss about indexing? What can cause this inconsistent performance? Update: I changed names of columns and table, so I can't attach execution plan as a picture. But here's the issue. Thanks for guiding me. when I change value of date parameter, SQL changes index seek from to . I never thought that SQL creates execution plan based on the value of parameters. How that could be? 

You don't generally need to. When the session is closed you or the connection reused it will be reset. Also you should know the isolation level your sessions normally runs at, so even if you do need to revert it, just hard-code it. And it should be READ COMMITTED. 

The SQL Server Log File is not publicly documented, but there is a partner program for 3rd parties that want to build solutions that use it directly. EG Attunity, Quest, etc. 

A database backup contains the allocated extents plus the parts of the log required to replay any activity that occurred during the backup, and restore the database to the point-in-time at which the backup completed. So your first backup file could contain more log records than your second backup. David 

It may not be loading the TNSNAMES.ORA file you think it's loading. Instead prefer Easy Connect Naming. Here something like '//aaa.cedd.net/creds' 

The issue with Query 1 .. unknown. It uses the index; if the index does not exist, it takes as long as 5s. The issue I see with Query 2: it does not use the index but the index. However, when I do the following changes, Query 2 uses the index: 

I've marked the time where I started the query and where it ended. You can clearly see where I deleted the WAL archive files :-) To me, why so many are generated is a mystery and it currently is a problem because it's hardly foreseeable how much is needed and when operation strike which need one. What am I missing to better understand how much space is needed? Are these things avoidable? Am I doing something wrong? 

I'm using pgloader to perform a one-time migration from MySQL to Postgres. For that purpose, I want to temporarily configure Postgres specifically for that workload. 

The missing piece is the 1) The number of logical reads of pages with no relevant data 2) The number of relevant result rows on each page And these two are determined by your indexes. If this table has no relevant indexes, then you will have to scan each page, and you will have to read a lot of completely or mostly irrelevant pages. Even if your index supports a query plan where you read only pages (mostly) full of relevant rows, if those rows are very wide, then there may not be very many (A,B) pairs per page. If you had an index on (B,A) on this table then SQL could seek to the first row where B=1 and start scanning rows until it hits a row with A>SysUtcDateTime(). This would only require ~4 Logical IOs to get to the first page, and then every Logical IO would result in ~700 (A,B) pairs. 

As soon as I start this query, it finishes. Thus, I get no chance to see to find out what type of lock this query has applied on database objects. I can inflate this query with transaction statements: 

SQL Server database which is very large (4TB) is stuck in recovery state. Reason: Data center had a planned downtime and we had to turn the physical machine off and when it turned on, as I read in error logs, SQL Server couldn't access the files, because files were on a SAN machine, and probably things didn't went smooth. 

And one hundred times it worked. Now I'm stuck. I've done all the steps, and client can't connect to the engine. It simply times out. Here's what I've done: 

This of course results in tremendous problems, because I'm working on a text-processing application and data comes almost from everywhere and I need to normalize text before processing it. If I know the reason of difference, I might find a solution to handle it. Thank you. 

There is very little metadata you can get across databases. OBJECT_ID works, so you can see if an object with a specified name exists in a specified database, but that's about it. EG: 

Sure. You can write a proc that uses sp_helparticle to compare the articles with the sys.tables, and then runs sp_addarticle to add articles for the new tables. But there's no replication setting that will do this for you. 

Keep track of the row count in a variable, and return that to the client with an output parameter or a result set. 

The Primary. The secondary doesn't have to run the SELECT,INSERT,UPDATE, and DELETE statements for the sessions that are modifying the database. The log records resulting from those operations are sent to the secondary which copies them to its log file and then applies them to the database using the REDO process. The Secondary can sometimes perform more IO than the primary, as the primary writes the log file, and lets the Checkpoint/Lazywriter write the database. But overall the Primary does more work. 

10 Mio rows, table size ~2GB, index size 3,4GB The runtime was about 55 minutes (not complaining here). The amount of generated WAL files was unexpectedly huge. The aforementioned WAL archives are on a dedicated partition with has 100GB and at the point the query started, around 30GB were free. It was not enough as after 15-20 minutes disk space was <10GB and I started to delete "older" WAL archives. I had to do this constantly up to the point were I was pretty sure I already had to delete WAL archive files generated by this very statement. The tables in question were not used by any other process during that time but "normal" operations of other tables continued. 

I'm constantly caught by surprised how certain operations generate me huge amount of WAL files. I want these WAL files for point in time recovery (I also perform a nightly full dump in addition) so the basic functionality provided is wanted and I don't want to change that (i.e. I'm not searching for a way to turning WAL archives off, etc.) Using Postgres 9.5 with these settings: