The Full Text Indexes are solving a different problem from creating plain text. However, my experience is that large text bases usually benefit from Full Text Indexing anyway, since someone is always trying to find something I understand that once you get the text you will redundantly store it in a SQL Server table so as to expedite your use of the plain text data. Automating the Extract and Load This will require some work. But, since your documents are currently in a FileTable you should be able to access them from the file system using file tools. The "off-topic" answer to plain text extraction below includes several tools that are being used by others. Perhaps some of these tools would be useful to you. 

In the simple view, this pretty much looks like Transactional Replication from {Publisher1, Publisher2, Publisher3} to Subscriber. I assume that Subscriber includes the . Therefore, as the is updated it can, in turn, become a publisher using Transactional Replication to {Subscriber1, ..., Subscriber5}. Of course, depending on the way data is maintained, you might need to use Merge Replication instead. Paul Ibison has a brief article that discussed the differences in republishing with Transactional or Merge Replication at: $URL$ The big issue is being sure that you clearly understand which sources of data are authoritative, how the data flows (and what happens to it in the ), et cetera. 

You really need to make a best guess and then give it a try. You need to be sure that you have enough memory kept free so as to support your BufferCount and MaxTransferSize options. The tuning includes both getting enough speed and avoiding using too many resources for the other operations on the server. I would suggest that you work with your 150 GB database and then try applying that to the 4 TB database and compare the overall results. A useful technique for testing the read speed of the backups, which is what you are trying to oversome, is to backup to: 

In addition, you can create an audit table and use a DDL trigger to track the changes being made to the database by tracing the details you want to know about. You can also filter out nuisance changes, such as disabling and enabling indexes and so forth. Sample DDL Audit Trigger: 

Keep (or set) DB in SIMPLE recovery model. Take FULL backup (5-6 hours) on primary Restore FULL backup to secondary (leaving in NORECOVERY) Set DB in FULL recovery model Take DIFF backup on primary Restore DIFF backup to secondary (still in NORECOVERY) Initialize log shipping using 'The database is already initialized' 

From Thomas LaRock at $URL$ "The fn_dblog() function (formerly known as the DBCC command) is one of several undocumented functions for SQL Server; it allows you to view the transaction log records in the active part of the transaction log file for the current database." The fact that you can no longer find your data suggests that the desired part of the log is no longer in the active part of the transaction log file. One way to get these 4 rows back would be to restore a copy of the database to a temporary database name (e.g. DbName_Disaster) at a point in time that contained these 4 rows. Then insert the 4 rows back into your production database and drop the DbName_Disaster database. Or you could look for some database recovery tool that fits your need. The prices and features vary. 

No, there is no such configuration available. Of course, you are not the only one concerned about style. You might find some posts by Alexander Kuznetsov helpful in pursuing your overall reliability goals. One of his posts is: Developing Low-Maintenance Databases $URL$ He has an online book entitled: Defensive Database Programming $URL$ But SQL Server has no 'strict checks' although Erland Sommarskog has proposed that something should be done about this. (No traction yet, though.) $URL$ So, perhaps you can create a process that examines the text of a stored procedure, but it will not be simple. And you can have rigid code review. 

As you can see if you run the script, the contents of that you inserted will be displayed. Actually if you change fields from to then the data will be of the proper data type for your calculation. Here is my calculation: Code: 

If Kerberos is needed because of multiple hops between computers: Microsoft® Kerberos Configuration Manager for SQL Server® has some guidance on setting up and configuring in your environment. 

This does not work, because there is no row to return, so there is no column to COALESCE. If this is what you need, then you will need to force a row to be included. E.g. 

In order to get the full path name, used the `DIR ... /B /S' In order to get the file name and size, used the `DIR ... /-C /TC /A-S /A-D /S' Since in step 2 the first column of a row with a file is the date and time of the file. Used this to filter out all rows that do not start with a numeric. Since step 1 and step 2 return different name formants, this results in needing to join a full path name with a file name only. A file name can appear more than once on a server, if it is in a different location. So there may be some ambiguity in the join if that case arises on one of your servers. 

Obviously when you check in a changeset, you know who the customer is (or was) according to what you describe about your process. Even TFS requires something to be the same, so that it can maintain history. In any case, in multiple versions of the same object, what is constant? Is the "Name" constant? (Probably not.) If the Customers table were that simple I do not see what you are tracking other than a name change. But likely the table is much more complex. But as long as you know this is the same Customer, you should carry a constant value in the new Customer rows. Something like: 

No, do not switch to SIMPLE recovery model since that will result in a break in your backup chain. If you are doing in FULL recovery model, do the following: 

Why do you expect nearly instant replication? That is not in the definition of replication. See the SQL Server Replication page. It says in part: "remember that replication is an asynchronous data-movement process that is likely to have some latency." If you are using Availability Groups with synchronous connections, the secondary server will keep in synch with the primary as much as physically possible. This is not "instant" but is the design to keep the transactions in synch. 

Learn to use the functions such as to move through time. Also, gives you options in how you format dates to match the current purpose. (Which could include formatting for other languages.) Then, if needed, the predefined formats can be further manipulate as needed. Though formatting for printing is best done in the presentation layer. 

Any serious development process should include some Source Control software and a plan for using the tool. Since Git, Mercurial, SVN, etc are freely available choose one. Git is commonly used by some of the largest organizations so you could choose to use that. There are, of course, many other source control tools. If you use this approach, regularly checking in the new versions of objects (schemas and code), you will have a trail of previous versions to compare against. Then you can look for changes that affect your code. Of course, Microsoft's SQL Server Data Tools (SSDT) includes Schema Compare, as do many tools such as Red Gate, Idera, and so forth. This should give you the tools you need to track the changes in your schemas and changes in the code as well. 

It is asynchronous, but I have never experienced a dramatic change. As the buffers are flushed, the memory is released to the OS. Of course, this means that the data cache is smaller and less data is kept in cache which will affect performance since it may result in more storage I/O. Jonathan's post at $URL$ makes the general default recommendation: 

SQL Server has tooling to provide information on the deadlocks and what was happening at the time. You can use TRACE FLAG 1222 to capture the details of the deadlock in the error logs. Of course, you can make a better analysis tool by following Jonathan Kehayias's detailed discussion at: Handling Deadlocks in SQL Server In my environment I am always checking for deadlocks and other problems. I would recommend using extended events to capture and store the details of the deadlocks. By reading the XML data for the deadlock you can get better insight on the problems you are facing. I would recommend using extended events to capture and store the details of the deadlocks. For a GUI method of setting this up see: Monitor Deadlock Using Extended Events in SQL Serve 2008 and Later You will never be 100% free from deadlocks, but you can trim down the problems by using the captured data to resolve many of the deadlocks. 

Since you are using SQL Server 2014, once you have plain text files you should be able to import them in a number of ways: , , and other tools are available to load data. Depending on your approach, you might choose to use a staging table to further prepare the results before moving the data into the destination table. And if you are also doing document versioning you will likely need to create some metadata that allows you to track different versions. 

Install a new instance of SQL Server. Shut down the SQL Server, delete the msdb data and log files. Copy the old msdb data and log files to the same location as the files you deleted. Bring up the new instance of SQL Server and script out the jobs. (You can use SSMS Object Explorer Details to select all jobs and create one massive script, or do them one at a time.) Uninstall the new instance. 

Not every mechanism in SQL Server is like we believe it should be. Paul Randal has provided a strong recommendation on managing the issue. $URL$ 

Of course, if you have several columns with conflicting collations you will need to define their collations as well. 

In this case the OR of candidate tokens provides the list to Full Text Search. The performance is based on the ability of the function to return those values which depends in turn on the size of the total text list. (You can also create a physical table that you periodically repopulate with the contents of the function, so that you can query it more quickly.) 

This script does include the job schedules that were set up on the origin server. From the scripted output you can pick out the SQL Agent jobs that you want to duplicate and create them on the other server. 

Batching large data sets into smaller data sets, e.g. 100,000 rows or 1,000,000 rows will make the load run better than One Big Insert. But the same is true from SSIS, of course, as it batches inserts. The difference in time that you show in your example is fairly small. That does not give much hope for great speed improvements, but it does encourage you to know that you can use other methods for import. EDITS included below. If you are willing to first extract to a .csv file, perhaps use BCP to extract from a view to the .csv file. Then you can use to load that file into your database: described: $URL$ If you want to do the imports all in TSQL then you might use the command. When you look as its parameters, you will see that it includes and as methods of controlling batch size. Because of the speed of and , along with the control of the batch size, I believe that this would be one of the quicker methods to use. But it does require the intermediate .csv file. Other issues: Is your data coming from the same server, or from another server? If the same server, then access is fairly east. If another server and you do not want the intermediate .csv file, you can create a linked server to query the data from the other server. However, as you noted, this will mean that you have to manage the batches yourself. This is similar to what you described as a 'cursor loop', although it probably does not need a cursor, just a loop selecting the next 'n' rows that you want to copy. If the data is on another server the overhead on this approach will be higher. 

Your code for the query is using two servers, identified by IP address as shown below. Where is the server [123.456.7.890]? And where is the server [098.765.4.321]? My guess is that [098.765.4.321] is the [RemoteServer] and [123.456.7.890] is the [LocalServer]. First of all, the slowness of the code is not SQL Agent's fault. Code run from SQL Agent is pretty much as efficient or inefficient as code run directly from SSMS. The problem is that you are moving much more data across the linked server than you might think. Here is the outline of your code: 

Regarding Connection Pooling, Thomas Stringer has posted: $URL$ The simple thing to know is: "The answer that enterprise DBAs need to be giving to these inquiries is that it is provider-specific. In other words, it is on the client/application side that connection pooling is handled." The CONTEXT_INFO, which is stored in binary, needs to be set by the application. CONTEXT_INFO is just a small bucket available to contain useful information such as the user's Name / ID / Login. (Or whatever else you want to stuff in the small space available.) The overhead is small, but it is an extra step to insert and extract the BINARY data in CONTEXT_INFO. (But useful for systems that run under a service account, so that you can know whose connection this is at the moment.) Thanks to srutzky for clarifying that a pooled connection is reset, but it is not dropped. At least not immediately. A post by Nacho Alonso Portillo offers clarification: $URL$ In part it says the "sp_reset_connection... does the following tasks: 1) cleans up the session context ... which includes ... resets the CONTEXT_INFO; ... 2) notifies them of the occurrence of a successful logout... 3) initiates the process of redoing the login." 

The post that Roni Vered referred to in 2013 did not say simply say: "change the connection pooling in the application's connection string." The text said to that there was a problem with using "an Application Role ... with connection pooling." The answer was: Turn Off Connection Pooling. This could be done in the application's connection string by including "; Pooling=False". This worked back in the mid-2000's. Likely it will still work for you if you are still using SQL Server 2008. The no longer correct path to the original post was: $URL$ A suggested workaround from 2007: On each call if the approle is already in use, then skip the setting of the approle again. Sample code: