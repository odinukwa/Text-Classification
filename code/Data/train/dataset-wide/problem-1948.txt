As alluded to in some of the comments, all of the CMIP5 models will have been run on supercomputers, and it takes a lot of effort to get one of them running on a new platform, even for a team who know both the model and platform well already. In addition to that, many of those models are closed-source and are not generally available to an individual researcher. However, there are several older models such as MITgcm and EdGCM that it might well be possible to get running on a cluster. It's something I've thought about but never quite got around to. One group have even been running CESM on Amazon's EC2 service. Depending on what aspect of leaf phenology you're interested in, you may need to run quite long simulations (e.g., several centuries), which is prohibitive with a full ESM. You can make a lot of progress in this area by running just the land surface components. This is several orders of magnitude simpler than running an ESM: the source codes are often freely available where their host ESM code is not, and they can be run globally on desktop machines or clusters. Some example land surface models you might want to look into: 

Each of these will require a set of forcing boundary conditions, and there are several commonly used by the land community: 

As the other answers indicate, however, the carbon footprint is much more complicated that simply these direct GHG emissions. It involves reassigning portions of one country's direct emissions to another country based on, say, some notion of who is ultimately responsible for the demand for that emission. There is currently no agreed method for making that calculation. 

How the models do at this sort of interannual variability for the current climate is described in Section 9.5 of the Fifth Assessment Report (AR5). For example, the section about ENSO says, 

Well, I think if it starts at T=30, and the dew point is T=15, then once it has dropped 15 degrees clouds will start to form. The DALR is about 10 degrees/km. SO, first, I think you need to solve for the altitude at which T will have dropped from 30 to 15. 

I think there are a few more options than bedrock/loose. But I owned a house 3 miles from the San Andreas fault, and a geophysical engineer explained it to me like this after telling me earthquake insurance would be a waste of money. The worst situation is landfill, like the Marina district in San Francisco that took much of the brunt of the 89 quake. That is just loose sand and gravel, and when shaken it acts like a liquid - in fact, they call it liquefication. The foundation shifts and sinks in it, like if you had a jar of sand and gravel and put a heavy rock on top and shook it. However, if you are on bedrock - or something tightly bound above it (like Franciscan shale), you get all the shaking transmitted, but the earth stays in place under and around the foundation. Earthquake damage can happen a few ways: from shaking up and down as well as sideways. The Mendocino quake in the 90s (I think) had more side-to-side movement than ever seen before, and house held together but slid off their foundations. After that they upped the degree to which house are bolted to the foundation to avoid lateral shifts. I am not sure soft soil 'amplifies' the shaking. There are a bunch of failure modes in buildings: they can come off the foundation; the foundation can shift and sink; the shaking can cause framing to collapse (which is why they put in shear walls); etc. Incidentally, wood structures often do better because they can flex. You'll never see a real brick house being built in a seismic zone anymore. In the end, it is about making the building live through the accelerations, and the hardest thing to deal with is loose soil that itself moves and flows under the structure. 

The key text here is "for $z>z_0$". It's telling you that, while you can evaluate the equation for other values of $z$, outside of that range the equation is not a valid description of the physical system. The equation could be written piece-wise to be complete: $u(z) = \begin{cases} (u_*/k) \ln(z/z_0)& z>z_0 \\ 0 & z\le z_0\end{cases}$ But this doesn't really add anything useful. In practice, the "log-law" is used to describe the wind profile over 10s of metres and values of $z_0$ range from 1 mm to 2 m, so values of $z$ are likely to be in the valid region. If you do need to make calculations that close to the surface (in the interfacial sublayer) then you'll need a different equation anyway. 

First of all, these netCDF files follow the CF Metadata Conventions, which describe the use of and in section 8.1 Packed data of the conventions description. In short, you're applying them correctly: 

The ERA archive description document, Berrisford et al (2011) "The ERA-Interim archive Version 2.0", provides additional information about what is available. It's less explicit on the why, but we can read between the lines a bit. The archive consists of re-analyses of the system state at 00, 06, 12, and 18 UTC and re-forecasts out to 10 days initialised from those 00 and 12 UTC states. Table 9 of that document lists the variables (including the ones that you're interested in) that are stored as accumulations since the start of the re-forecast. They are stored every 3 hours for the first day of the forecast (steps 3, 6, 9, 12, 15, 18, 21, 24) less frequently thereafter (every 6 or 12 hours). The variables are not stored for step 0 because their accumulations would be exactly zero. Regarding using ERA Interim for model forcing, you might consider what Balsamo et al (2015) do for ERA Land forcing. They use state variables from steps 3, 6, 9 and 12 and accumulations from steps 12, 15, 18 and 21 (see Fig 1 of that paper). This inconsistency is because the fluxes (accumulations) are particularly prone to spin-up artefacts in the first few hours of the forecast. The trade-off is that the states and the fluxes used in the forcing come from slightly different forecasts. 

Take a look at this: $URL$ You can see a large spike in CO2 beginning with the Industrial Revolution. Now, the earth has natural cycles. But this is a very rapid change. And, we know that coincident with this, (1) the population grew a lot on earth, and (2) humans did a lot of things (mostly burning fossil fuels, but also burning wood, deforestation, out-gassing from coal seams, etc.). We can, in fact, estimate the amount of CO2 production caused by humans by looking at oil and coal production - and the amount line up between what we probably have produced and how much we see. And don't forget other gases are even more powerful (like methane). A large part of that comes from agriculture, particularly meat for human consumption. Animals that eat grasses can't digest the grass. Rather, they are really walking fermentation units, with bacteria that breaks down cellulose - which the animal can digest - but which give off methane as a side-product. So another approach is to use what economists call "instrumental variables": by looking at global GDP, for example, as a proxy for human 'production', we can see that changes in these gases has a high correlation with human activity. One thing to note is that it only takes a small CO2 (or other greenhouse gas) change to drive a big temperature change. CO2 is only a few hundred parts per million of the atmosphere; methane is even less. The 'natural' greenhouse gases keep earth as warm as it is to allow life as we know it (by about 30 degrees Celsius) by reabsorbing long-wave radiation from the surface (hence the name greenhouse gases). SO, it only take a small change in those to drive a relatively large effect. People often don't know that. I often compare it to 'sterile procedure' in medicine. You won't get a surgical infection if a few bacteria get into the wound; you need to have a small colony enter (hundreds or thousands) to get a foothold. But, once you cross a threshold, the problem grows fast. 

The example that you've linked to (Larsen, 2003, "An investigation of the Faroe Shelf Front", Univ. of Bergen) is a description of various processes that contribute to the existence and variability of an ocean front around the Faroe Isles. As an oceanography thesis, it's mainly concerned with the ocean state (e.g., temperature, salinity) and local, internal process that affect the front. But the introduction also mentions external process that affect the local ocean state, such as the wider-scale ocean state/fluxes, topography and atmospheric forcing. You could loosely think of these three respectively as the lateral, lower and upper boundary conditions for the local block of ocean of interest around the Faroes. In this context it could also be phrased as the forcing of the ocean by the atmosphere, so how the ocean is affected by the exchange with the atmosphere of quantities such as heat, water and momentum. Larsen summarizes these through descriptions of air temperature and heat flux, precipitation, evaporation and runoff, and wind speed and direction. Often when we're interested in a particular physical realm (e.g., atmosphere, ocean, land or ice-sheets) we prescribe (i.e., force) the interactions it has with the other realms to make it easier to investigate processes within the realm. 

If you go down the land surface route (and I suggest that's a good place to start), you should probably just pick a model and ask it's community directly about the details of getting it running. In my experience, several of them have very good tutorials especially for people with a bit of Linux experience. 

A hindcast, also known as a historical re-forecast, integrates the model forward in time just like with a forecast, so you'd initialise the model at $t_{-1}$ and run through to $t_1$. If you have an assimilation system that can make use of observations at $t_0$, then it would use them in the same way that it would with a forecast. The point of a hindcast is to do the forecast again using something that wasn't available originally. That new something might be observations (for assimilation or for verification), the assimilation system or the forecast model. They can be used to calibrate the modelling system or just to check that updates to the modelling system do actually improve the forecast. They're often used for cases studies of extreme events or situations that are known to be tricky to forecast; after all, why wait for the next 1 in 30 year event to test your new system when you have one in the archive, probably with lots of verification data accumulated over the years.