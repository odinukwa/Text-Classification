These are the reasons why using nginx on Windows for high performance, high scalability environments is a bad idea. Switch to nginx on a non-Windows operating system as soon as possible. 

Once you know who these mistaken requests are being addressed to, what to do next may become clear. For instance, it might turn out to be some big company in which case you can go find their network admins and yell at them. 

The reason this isn't working (and isn't going to work) is that iptables operates on the IP address of the machine that directly connected to yours. If you're using CloudFlare, this means you are receiving connections from CloudFlare, not directly from the end users. Here's an example, taken from one of my sites on CloudFlare: 

This might be a long-standing bug in OpenSSH which was only fixed in 6.0p1. In that case you can safely ignore it. However, if you want to be safe, the original answer (assuming you aren't affected by this bug) is: 

The command is correct, but you can't remove a physical volume from the volume group until it's empty. That is what the error message that you received means. As the man page states: 

Docker doesn't have authentication on its socket. Anyone who can access the socket can control all containers, and can effectively break out of the container and become root on the container host (if SELinux is not in use). First, be extremely careful if you decide to do this, that you are only running trusted code. Second, forget about TCP; just bind-mount the Docker socket to the container. This way, only that specific container can access Docker. 

A Windows program built with Visual C++ that uses the redistributable library is tied to that specific version of the library. They are not ABI compatible with each other. So, if you have one program that uses the 32-bit 2008 redistributable, and another program that uses the 32-bit 2015 redistributable, and a third that uses the 64-bit 2015 redistributable, you need to have all three versions installed. Note that Windows Update will provide security and critical updates for these runtimes, so after installing one, you should check for and install its updates as soon as possible. 

That looks like it might be an IPv6 address, though it's difficult to tell since you have mangled it. 

Since such a malformed request can get past the TLS handshake and need to be rejected in HTTP, an HTTP response code is necessary. Of all those that exist, only one really fits the situation: 

Those are third party resources, and you have no control over whether they use compression or not (they will not, because doing so would potentially expose private data in a BREACH or CRIME attack). You can safely ignore messages suggesting compression of resources loaded from third party sites, and messages suggesting compression of content served via HTTPS. 

The log file format is different between your two examples. Since webalizer is expecting the first format, it can't parse the second format. In the second example, fields 2 and 3 (each of which is a here) have been removed. You have a couple of options: You can edit the log file to replace the missing fields, or you can change webalizer's configuration to ignore the missing fields. Either way, you'll almost certainly have to split the log file at this change to work with it. 

Merely adding or removing vCPUs after installation of the guest OS is not an issue with any version of Linux or Windows that's recent enough to still be vendor supported. This warning dates back to the very early days of VMware and is mostly irrelevant now. In the early days of Linux, though, the kernel had to be specifically compiled with SMP support, and occasionally UP kernels didn't like running on SMP/NUMA systems, or vice versa. Those days are long since mostly forgotten. These days Linux kernels are almost always compiled with SMP/NUMA support by default and run fine even with one processor. This has been true for all of 2.6 and most or all of 2.4. Windows has behaved similarly since Server 2003. I wasn't able to quickly find definitive information on the Internet about how 2000 and NT 4.0 behaved, though I seem to recall from distant memory that they may have had issues when switching from a single CPU to multiple CPU configuration. If you plan to P2V a very ancient system, though, it's theoretically possible you might run into such issues. 

Change the server's hostname. There's no valid reason for a hostname to ever be equal to a naked domain name. 

Just insert a rule to leave the chain when ssh traffic comes in. This should appear before your DNAT rules. 

You also installed some packages from the repository, but you disabled -- and removed! -- that repository. I'm going to guess that you actually want to keep the newer version of MySQL that you got from remi, and advise you to put the repo back and enable it. 

You don't. The web server is allowed to decode percent-encoded characters which don't decode to special characters before applying rewrite rules, and is allowed to decode the remainder of percent-encoded characters before passing the data to your web application. (See RFC 3986.) What you should be doing is applying the front controller pattern in your web application, and handling all the percent decoding (and request routing) yourself. In this case you will simply redirect all requests that don't match a file or directory to and then read the URL out of . This is how major PHP-based web apps like WordPress and MediaWiki handle this. 

The official statement regarding the plans to obsolete net-tools was made on the debian-devel mailing list in early 2009 by one of the net-tools maintainers. True to their statement, net-tools has been hardly maintained at all since that time. 

is set too low. This controls how many PHP processes will be spawned to handle simultaneous incoming requests. If more requests come in than children are available, the new connections have to wait... eventually they will wait forever. Raise this value; find it in . This is the actual cause of your immediate problem, though you have some more insidious problems which you probably haven't noticed yet... means PHP is crashing while servicing requests. Depending on when in the processing it crashed, this may have no visible impact, or may result in blank pages, aborted transfers, users being mysteriously logged out, etc. Consider updating PHP to the latest point release (5.3.x or 5.4.x) and removing or replacing known problematic extensions such as APC. 

The default shell for PASE for i is ksh, and if that's what you're using you can simply edit the file in your home directory like any other system. For example, add: 

You can set the timeouts in any , or block, as shown in the nginx documentation. They will be inherited from the containing block, which is why the snippet you posted did not work. 

Contact the provider who assigned the IP address to you. Unless they have delegated reverse DNS to you, then they will have to set it for you. 

Your VPS provider has disabled the default CentOS repositories and replaced them with repos which do not actually provide access to the base system packages. This seems to be a common OpenVZ configuration issue. I worked around it by: 

mdraid support for discard on RAID 5 wasn't available in the 2.6 kernel series; it was added in the 3.7 kernel. I'm not aware of Red Hat backporting this functionality. Once you get a kernel with this functionality, you may find that it is disabled, because some (older) SSDs don't actually implement TRIM properly. In this case, you need to set the kernel module option and then test thoroughly before putting the system into production. 

We can see from your updated output that you have 34 PHP workers running (not counting the master process), all of which are taking up memory and completely idle. This means you have tuned php-fpm to run on a much larger capacity machine with far more than the traffic you are actually getting. Or, perhaps, have not tuned it at all. At the moment, those processes are taking up over 3/4 of your VM's RAM and giving you absolutely no benefit. So, check the tuning in the php-fpm pool configuration, e.g. the default . For a small VPS like that I would probably start somewhere around: 

The CentOS "minimal" disk is customized and may do unexpected things, as it was intended and customized only to install a single set of packages. It should not be used with a kickstart installation; use the netinstall or DVD images instead. 

And Apache did, with the directive. You can use this if you wish, but again, don't think that it's going to magically prevent you from getting attacked. 

Use a filesystem which is capable of detecting and repairing corruption, such as ZFS or btrfs, or Windows ReFS. 

Since you aren't on any blacklists that I can find, and your Sender Score is 100, the next place to look is at the characteristics of the messages you are sending. Again, as the URL that Hotmail gave you said, after you've resolved all of your issues, if you still can't deliver mail, you can submit a support request. 

That's caused by a kernel bug. It was patched over a year ago in the distribution kernels, but you are not running the kernel provided by your distribution. Either switch to a distribution kernel or contact whoever built your kernel. 

Your block hasn't got a directive to tell nginx where your web site's files are located. Thus it serves the files from the default location, which are the sample files shipped with nginx. To solve the problem, add a proper directive in the block. 

You have nothing to do to your server. According to Red Hat, the versions of Apache shipped with RHEL (and by extension, CentOS) are not vulnerable to this attack. You do need to provide this information to your PCI compliance auditor. 

I don't think anybody uses NIS anymore - or at least, wants to. The fastest and easiest way to get a nice LDAP+Kerberos environment up is FreeIPA. It's easy and light enough that I even use it at home. Red Hat's Identity Management Guide is a great introduction to FreeIPA and will get you up and running quickly. Note that while Ubuntu has FreeIPA, the version in 12.04 LTS is older and may have bugs or missing features compared to more recent versions. 

You have a simple syntax error; you should be using brace brackets instead of parentheses. You currently have: 

Someone has been using your VPS's IP address as a proxy to abuse the various ad networks shown here. If you just got the thing, it was probably whoever last had the IP address running an open proxy. If I ran into this situation, I would shut down the web server, and then contact the provider to let them know of the issue and ask for a different IP address. 

Apache recommends against using a hostname in the directive for a variety of reasons. The best practice is to specify the IP address or in and the hostname(s) of the virtual host in the and directives. So to resolve this: 

The named location needs to appear last in . You should not have anything past it (and indeed, it would never be reached anyway). 

Metabase doesn't know how to interpret the string that you provided on the command line. It is interpreting that as a command, but it doesn't appear to actually be a valid command. It appears you intended to redirect the program's output somewhere. But you can't do that with or other shell constructs in systemd, because no shell is being used. Thus it was passed to Metabase as a command line argument. Rather, you should use and to redirect stdout and stderr, as you already have. 

Connection refused means what it's always meant: The connection was never established to begin with, as nothing answered on the remote end. Specifically it means a SYN packet was answered with either an RST or an ICMP port unreachable. This happens when no server is listening to that remote port, or when a firewall is configured to explicitly refuse connections rather than drop traffic. Obviously no SMTP conversation has taken place in this context, as no communication channel was ever opened. It doesn't explain why your recipient claims to have received the messages. To answer that, you'll likely have to go further into your logs, and quite probably into their logs. And it's rather odd that they don't seem to have any... 

Not much really changes when you switch to a DVCS. The big difference is that the copy of the source code on every developer's workstation is now its own repository as well. I doubt there's much reason for IT to worry about this. Just because git and mercurial are distributed, doesn't mean you will be deploying/delivering directly from some developer's desktop. It's still possible - and almost certainly necessary - to continue using some central repos that everyone checks into, for testing, QA and eventual release. Whether IT is monitoring source code on developer workstations or not, nothing really changes. You're still going to get the change history into the central repos as soon as they're pushed, which is what I suspect they're really concerned about. What you gain from this from an IT standpoint is that you aren't hitting the central repos every few minutes (or seconds!) whenever the developers make a commit. Developers can finish working on something and then push it up only when it's ready, but still have complete version control on their workstation without having to hit the network nearly so much. Finally, you need to have a more detailed chat with IT regarding the exact nature of the compliance issues they're talking about. This could be anything from managing intellectual property, ISO 9000, or some government laws/regulations. (To all: feel free to improve this answer; this is not my area of expertise...) 

This is just a common CR1632 coin type battery which can be bought virtually anywhere in the world for a couple of dollars US. It keeps the system clock running whenever the system is powered off. What's surprising is that (1) it lasted this long, and (2) your company is still using that ancient server. You replace it like any other such battery: pop the old one out and pop the new one in. Then be sure to reset the system clock, as it will most likely be wrong. But there's no need to hurry about this; you can do it whenever the system is next powered off for some other maintenance. Of course, given its extreme age, you should just decommission it before it blows up... 

The usual solution is to stick the destination site in a frame. But keep in mind that anyone with half a clue can figure out what the site actually is. 

I have often heard it recommended that a user account should be disabled by setting its shell to . But, on my existing Linux systems, I see that a great number of existing accounts (all of them service accounts) have a shell of instead. I see from the man page that prints a message to the user saying the account is disabled, and then exits. Presumably would not print anything. I also see that is listed in , while is not. The man page says that FTP will disable access for users with a shell not listed in and implies that other programs may do the same. Does that mean that somebody could FTP in with an account that has as its shell? What is the difference here? Which one of these should I use to disable a user account, and in what circumstances? What other effects does a listing in have?