At the end of this, contains the query for the current request. Also, you could just as easily use a table variable instead of a temp table. 

Yes, you have two clauses with a multi-table statement. This is essentially just an anti-join turned into a deletion. 

I checked those names on 2008, but I seem to recall they have the same names on 2000 (though the structure may differ). 

If I had to manage this, I'd look at what, if anything, modifies the data in the tables outside of the ETL process. If nothing else besides your ETL modifies the data, I would simply update the ETL process to insert the finished data at both locations (and likewise carry out whatever index maintenance you're doing in both places). If something else updates this data, but only one one server, then transactional replication is probably the most lightweight way to get the data to the secondary server. Even if the data isn't being modified outside of the ETL, then this wouldn't be a terrible alternative to modifying the ETL process to update two targets. It sounds like a relatively small percentage of data that's being inserted daily. If the data is being modified on both servers, then you'll probably want to consider merge replication. The simplicity of this will largely depend on if the table has any identity columns. 

Before I just go ahead and put a really awful check constraint on the Catalog table, I wanted to solicit some better ideas first. I want to ensure that all shared data sources on our report server are deployed to "/Data Sources". Every now and then, we get one mistakenly deployed to some other directory (particularly if it's a report upgraded from SSRS 2000, which didn't allow specifying a different data source deployment location). I can put an ugly check constraint on Catalog (, or similar) if it comes down to it, but if there's a better option, I'd rather use that. 

Differential backups include data that has changed since the last full backup (ignoring any full backups taken with the COPY_ONLY option). If you take a differential backup immediately after a full backup, it will be very small, as little (or no) data will have changed. As time goes on, the differential backups will become larger and larger, until you do another full backup, which resets the differential change map. Note that you must still have the last full backup taken prior to the differential backup, or the differential backup will be useless. 

The general idea is that you create a subform that's bound to your link table, and make one of the fields in the subform a combo box bound to the child record ID that's in your link table. Fill the combo box using records in the child table. Then you handle the NotInList event on the combo box to prompt the user to add the record to the child table (or whatever is appropriate for your application). NotInList Event Then if you want to actually remove rows from the child table when they are no longer referenced by the link table, handle the appropriate deletion event on your subform, and check for unreferenced records. I've implemented something like this once, but I think it was in Access 2000 on a machine I don't have with me at the moment. I'll try to check it later out of curiosity. 

And I get 'Can't find any matching row in the user table', for the obvious reason that '%' isn't a host mask I've explicitly allowed a connection from for this particular user. So what's the proper syntax to grant a table privilege to a user from any of its allowed host masks? How is MySQL Workbench getting away with it for schema privileges? I don't have to manually insert rows into mysql.tables_priv, do I? UPDATE: To clarify, here's what the current user/grant tables look like. I've anonymized some names, obviously. Note that the host in the schema privilege table is '%', but there aren't any users with that host. How do I get MySQL to let me do that with schema object grants? Preferably without mucking around directly in mysql.tables_priv, but I'll do it if it comes down to it. 

If you're the DBA, you're probably working in the realms of performance and security (among others). This could involve profiling and assisting with tuning client applications that the developers are working on. Thus it certainly wouldn't hurt to know the basics of the languages the developers are using, and how to interface with the database server. For SQL Server, that's probably .NET. For MySQL, probably some combination of php or java. For example, if the server is pressured for client threads, then it helps to be able to inform the developers that they should be using singleton instances for their ObjectDataSources so that they can reuse a single database connection. And also make sure they're using parameterized queries properly to limit SQL injection. Stuff like that. You won't be doing any programming in the sense of developing client applications if you're strictly a DBA, but you should know how they work (generally speaking). 

If you put the database into Restricted User mode, then only members of the fixed server roles or , or members of the fixed database role can access the database: 

Don't do the thing; you can run into concurrency problems there. If the patterns are simple enough, and have only a single increasing numeric portion, you can probably get away with using an identity column plus a persisted computed column to create the formatted string value: 

As a general rule, use higher DOP for an OLAP system, and lower (or no) DOP for an OLTP system. Many systems are somewhere in between, so find a happy medium that allows the occasional large workload to get enough CPU to complete quickly, without strangling your OLTP workloads. Also, be careful about using the column to get a core count. If hyperthreading is enabled, this column seems to reflect the number of logical processors exposed. Generally speaking, you don't want DOP to be higher than the number of physical cores. Spreading a heavy parallel workload across logical processors will just increase overhead with no real benefit. There's also a column, but I'm not certain what it represents. The documentation isn't very clear either. The number I see on our system suggests it could either be the number of physical cores in the entire system, or the number of logical processors per chip. The documentation claims I should be seeing a different figure entirely. 

I have a number of triggers that do this, and I find that is generally the best way to do it. Caution: the output is limited to 4000 characters. Very long queries will be truncated. 

If Page reads/sec skyrockets, and the other two drop significantly, the troublesome query is probably trashing your buffer pool and causing a ton of disk I/O. You could then either work on tuning the query, or throw more hardware at it (more RAM, faster disks). 

Apart from performance, they all have rather different meanings. will give you the last identity value inserted into any table directly within the current scope (scope = batch, stored procedure, etc. but not within, say, a trigger that was fired by the current scope). will give you the last identity value inserted into a specific table from any scope, by any user. gives you the last identity value generated by the most recent INSERT statement for the current connection, regardless of table or scope. (Side note: Access uses this function, and thus has some issues with triggers that insert values into tables with identity columns.) Using or can give you entirely wrong results if the table has a negative identity step, or has had rows inserted with in play. Here's a script demonstrating all of these: 

catalog_category_flat_store_1 has 627 rows, and core_url_rewrite has 8,687,266 rows (kill me). The EXPLAIN shows me that catalog_category_flat_store_1 is "Using index condition; Using where; Using filesort", and core_url_rewrite (the huge table) is "Range checked for each record (index map: 0xF4)", despite allegedly using the index on category_id as its key. On the old server, catalog_category_flat_store_1 is "Using where; Using filesort", and core_url_rewrite doesn't have anything listed for "Extra", and appears to be using the correct index. I noticed MariaDB has a lot of additional options for optimizer_switch, and I'm wondering if any of these options are causing the different optimizer behavior, so I figure it's worth testing. The problem is that I don't know what the values should be to make MariaDB act like MySQL. Here's the old server, where the query runs just fine: 

It's certainly not required for any functional reasons, but it's not a bad idea for some quick and dirty documentation. It sounds like they're confusing it with the Access relationship designer, which is where you actually declare your foreign keys. The SQL Server database diagram designer looks similar, but has a completely different purpose (documentation rather than DDL). 

The output has eight columns: the table and column names for the foreign keys (FK_table, FK_column), the names of the foreign-key constraints (FK_name), the referenced PK or unique index table and column names (PK_table, PK_column), the name of the referenced PK or unique index (PK_name), and the update/delete cascade actions (Delete_Action, Update_Action). (Edited to add some more output columns.) 

In this case, yes, they appear to be redundant. The second index will satisfy any queries that would benefit from the first one. Also, both indexes appear to contain the same columns, so rows within the index leaf pages should be about the same size in either. Your non-leaf index pages will be a bit larger with more key columns, but that probably won't amount to a significant size increase, unless the index rows are quite large and only fit a few per page. It's important to consider the size of the index rows when deciding if an index is truly redundant. If you have an index on a couple of int columns, a decimal, and a datetime, the rows will be much smaller than if you have another index on those same columns plus a character column that averages a couple hundred bytes. Any analytical queries that would aggregate a large number of rows would benefit from the index that has rows much more densely packed on the data pages, even though either index could technically satisfy the query. So, make sure you consider the difference in average row size, and whether or not that has any performance implications before you go dropping any redundant-looking indexes. 

Also, make (providername, typename) a unique key of Provider, if you haven't already. Or better yet, normalize them into separate provider and type tables, with a many-to-many join as appropriate. 

Source: $URL$ I don't recommend Single User mode in these cases, as the application update process may need to open multiple connections simultaneously. 

Setting up an Access DB as a linked server gets pretty ugly, but if you absolutely have to do it, you can split that one table into a separate database file, and create a linked table pointing to it from the original Access DB file. Then set up a linked server in SQL Server pointing to the Access database file with that one table. Better option: Build an SSIS package that periodically imports/synchronizes data from your Access database. Best option: Use the upsizing wizard in Access to migrate the data to SQL Server, and continue using Access as the front-end application, but not as the storage engine.