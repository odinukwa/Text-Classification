I am trying to create a simple user with the rights permission to access to any database and can do any actions. When I trying to execute the command I got this error: 

I want to migrate my database instance from AWS RDS MySQL to Aurora, but I have a doubt about replication and how Aurora management the write/read operations. I have my application and I want to separate the write operations from the reads. I want to create a Master instance only for write operations and other instance (Read Replica) only for read operations. The problem is here, I read on AWS documentation that I need to do this separation on my Application and I think or I hope to find a way to do that and be transparent for my Application. I draw a simple schema do I have to do to get with Aurora (from AWS): What AWS says do I have to do: 

What do I need: I need the Master always keep with the write operations and it redirects the read to the Read Replica instance. 

Well I want to configure the mongo server to allows access for http. I put this on my mongo config file: 

So If the log says that it waiting for connection on that port give me a respond but I can't see the http admin view, any idea or tips? 

Do I need to run pgpool with other user? Do I need to create an specific user to pgpool configuration? It works manually with sudo!!!. Do I have to change my failover script command? 

The problem above only happen when I enable the auth configuration and I need it. So, How do I create an user with admin permission for any database. I want it because I configured my mongo services to uses authentication connection. If I want to execute a dump of my data I have to use this authentication parameters. Please any help? Using mongo version 3.0.5. the service is on Amazon Linux AMI 2015.03 (HVM), SSD Volume Type - ami-1ecae776 

The replication is always running. But I want to separate the write and read process. So my summary: The master instance detecte the different between writes/reads operations and all read operation will be manage by Read Replica. I need this solution because aurora offer a good features to improve my RDS, but the only problem is that I need to create a balance between write and read operations: The write operation be processed in the master and it send the read operation to the Read Replica. I don't want to define this process and make selection between them in my application code as Amazon propose. 

Is it possible to figure out which tables contribute the most to read / writes operations if all of them in same file? I been looking at the following DMV 

so why is it when I have added .5 second wait time to each loop iteration it actually runs faster and allows the print message to come out one at the time? 

In SQL Server it is best to store DataTime as one field. If you create an index on DataTime column it can be used as Date search and as DateTime search. Therefore if you need to limit all records that exist for the specific date, you can still use the index without having to do anything special. If you need to query for time portion you will not be able to use the same index and therefore if you have a business case where you care more about the time of the day than DateTime, you should store it separately as you will need to create an index on it and improve performance. 

I used the following query that I got from EXCHANGE SPILL, but slightly modified it to convert time from UTC to local. 

which made no sense as it is self-join back to the table. After carefully looking at your plan I realized that you are bringing back data from a view and then joining it back to the base tables that are present in the view. It is a known issue that nested views will always produce estimated row count of 1 no matter how much you update statistics. My recommendation is to rewrite query without use of which just hinders performance. Remember: Views are not for performance, they are for simplicity and easy of access of information. 

When I look at messages, the command does not return row every loop but shows up as multiple rows at ones. I have added one line and now it shows up 1 line at the time and the the whole process is much faster. 

An easy way to get job to execute only on primary node is to put simple check for the job to verify which node is at the time job is being executed. It can be done with simple sql statement: 

I've recently audited a SharePoint server and found configurations that are against best practice. I'm hoping there is better and quicker way than right-clicking and changing these settings manually on over 100 database. For instance, all the databases are set to autogrow by 10MB and the log files are set to autogrow by 1%. I'd like for these to be a set number and not a percentage. Any resources or recommendations would be greatly appreciated! 

Full disclosure, I'm not getting anything out of this "plug" and I don't work for Red Gate, it is a free real time monitoring tool that'll monitor log shipping. Red Gate Log Shipping Monitor Microsoft TechNet Article 

Situation: We recently had a consultant company build new Lync 2013 Servers in a mirror setup. There are two security principles set, SA (disabled), and a AD group that is only associated with the Public server role. 

In short, put the OSes on slow storage and all the data/log & backup files on fast storage. I know that my environment is different that yours, but I think similar principles apply. We have a SAN and the storage pools range in different speeds, 7.2k, 10k, 15k. We install the OSes on the slower drive pools and have all backup files and data/log files stored on the 15k drives. There was a time when we accidentally put everything on the 15k drives and we didn't notice a performance difference. It was brought to our attention by our SAN admin after an audit that the OS was stored on the faster drives. We monitor SQL Server performance with perfmon and Dell Spotlight. For grins, we went and looked at the historical metrics that mattered, and saw nothing to note. 

You're correct about how the VLFs grow in size. Check out the following video from Jes Borland for some more tidbits. How SQL Server Works: Log File (Video) 

Having 10s of thousands of error messages per week makes it pretty difficult to peruse and check other possible issues 

We use Quest Spotlight for database server monitoring. Depending on when SQL server decides to fail over automatically, we will get an alert stating that a database has not been backed up in over three days (the factory default) if that server has not been primary in over 72 hours. My suspicion is that SQL Server does NOT replicate this data, but I wanted to hear from the community since a quick Google search didn't provide anything. Plus, I am travelling and do not have time to test and I need to provide a fairly immediate response. 

but it only shows statistics for the whole file and not sure how to split into individual tables. Trying to do this in order to identify tables that should be places in their own files on different disk. 

I'm trying to understand what effect database mirroring can have on replication in SQL Server 2008. Here is current environment: (don't ask me why it was done this way or to change it) Server A: SQL Server 2008 (Publisher) Server B: SQL Server 2008 R2 (Distributor) Server C: SQL Server 2008 (Asynchronous Mirror) Server A is setup for SQL Replication and for Database Mirroring. Mirror is going to DR site and replication is used for different purpose at local site. When database mirroring fails (Server C goes off-line or network issue or anything else) and I can see state on Server A (principal, disconnected). At that time replication failed until mirroring was turned off. My understanding was that High performance (asynchronous) database mirroring, should not prevent transaction from being committed on source. The only time transaction should not be committed to the source is when High Safety (synchronous) mode is on. Which would explain that replication not being able to pick up the changes as nothing is being committed on the source. Is there any scenarios in which High-Performance (asynchronous) database mirroring can break database replication? 

Not sure if the question title describes perfectly what I am trying to achieve. I am looking for better way of doing what someone else implemented and I am trying to simplify it, secure it and make it easier to manage. Developer has built a website for displaying information to end customer, part of this information is presented internally through SSRS report. Since the site usually resides on DMZ there is no direct connection to SSRS report, therefore developer wrote CLR procedure that calls SSRS server to generate PDF file of SSRS report that he stored in the table temporarily, then just to return that binary to site which in turns displays that report. This process is extremely complicated to debug and maintain as new reports require new CLR procedures which creates problems when trying to distribute CLR to different clients that are using the product. In conclusion, if I want to present SSRS report outside of the network in a secure manner, what is the best route to go about doing that on a webpage? 

Our Process: We perform/make/implement our updates to the SSIS packages using Visual Studio (v 15.7.0) on the local SSIS SQL server (Microsoft Windows Server 2016 Standard) save the package to a file system drive not located on the server. We then open SSMS on the local SSIS SQL server (v 17.6) connect to the Integration Services on the Local (SSIS SQL Server) and import the File System Package into the “File System Stored Packages” using the GUI. i.e. Right clicking the “File System” note and selecting Import: 

We created a SQL Login account for reporting use only (SSRS), we'll call it 'MyReportAcct'. We've used this account for some time and it is created for every newly installed instance. It has been working as you'd expect, except on a couple of mirrored instances. I've not seen this issue on any standalone instances in my environment. Randomly, reports will stop working and there is nothing being changed based on our internal processes for Change Management, Windows Logs, and SQL Error logs. Here are two of the error messages we receive: 

Short answer: The SIDs for the ‘MyReportAcct’ SQL login are different on each server instance. Long answer: The database houses the security database for each instance of SQL Server (think Active Directory for a database server). In this scenario we have two different databases, since it is a mirrored pair. Each time you create a SQL Login, there is a different SID created along with the account, this is no different than how Active Directory has always worked. When a mirrored database fails over, the SID in the database matches the SID stored in the database of the principal server (SERVER1), not the mirrored server (SERVER2). This natural, and default process of failover causes the SQL Login (MyReportAcct) to become an orphaned account on the principal server (SERVER1). This orphanhood process happens EVERY time there is a failover. The start of authority for SQL Logins are the instance that they were created (SERVER1). The failover to the mirrored server (SERVER2) is now a different start of authority, so the SIDs will be different, hence why the reports fail. Solution: 

I forgot to update this post with what ended up working for me. I don't know why, but running SSMS as Administrator allowed the backup to complete as you'd expect.