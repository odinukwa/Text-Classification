The lexicalization of loans is a longer process, you have many words which are undiscernible from native words by a layperson because they have been so completely internalized. In English, you have words like ditty and because which can be traced back to stems of Latin origin, but which have been so completely adapted to the form of modern English that you need specialist knowledge to establish this. In Swedish, a lot of the old Norse vocabulary has been displaced with German loans which sometimes make little sense in Swedish. Everyday words like anhörig and begära are somewhat opaque (though some morphemes trace back to shared Germanic roots which were also present in older Norse) but make perfect sense in German. Some others which are outright peculiar in Swedish include fogsvans (literally "joint tail", where the German word means "fox tail" -- still a weird term for a saw). Later, French loans have entered the language in the 18th century, originally in forms like lavoir and manquements, but now have modernized spelling and native-like pronounciation lavoar, mankemang. Finnish is pecular because even more than Swedish, it has replaced much of the original Fenno-Ugric vocabulary with loans of Indo-European origin. A large number of words like pallo (ball) and ranta (beach, cf Swedish strand and German Strand) are clearly of Germanic or even earlier Indo-European origin, and there is a set of trisyllabic words like mehiläinen (bee) and kuningas (king) which are of somewhat later origin, and sometimes attest to forms which are no longer straightforwardly recognizable in the neighboring IE languages. In all of these languages, you can find recent loans which are used in a form which is much closer to the origin. With reading and writing skills approaching 100% of the population, the process for internalizing these is likely to look quite different -- it is not unlikely that résumé and coup d'état will remain in English in their French form for a very long time. Similarly, obviously English loans like jazz and policy may well have come to stay for the foreseeable future without any further adaptation to the host language. 

Let me post a slightly tongue-in-cheek answer, then perhaps expand on it in useful directions. If your corpus is 2.3 million words, the ideal gold standard is 2.3 million words. Now, to try to frame that, what I really mean is that you are approaching this from the wrong direction. You are not creating a corpus to satisfy your statistics teacher; you are creating a corpus for a particular purpose, and you want to maintain the highest quality you can with the available resources. Ideally it should be perfect, but we all live in reality - and if you can't make something perfect, at least strive to make it useful. That means, document what you have; explain to your prospective users what they can realistically expect. To approach this from a different direction, let's say you have a toy corpus - for example, written weather reports in English from a single weather station. What can we hope to do with this? Well, a linguist might look for variations in linguistic expression. In really formulaic (and these days, probably mostly automated) language, you will get bored after 5 or, if you are really patient and/or easily amused, maybe 20 reports. That's good. Now we don't need any more data because we have a reasonably representative - not to say exhaustive, even exhausting! - sample of the phenomenon we wanted to investigate. Maybe that's not the only linguistic inquiry we could think of for this example (aberrations in punctuation? Diachronic evolution, if you have reports from different decades?) but let's stop here. The point should hopefully be clear - you have enough data when a lot of it is redundant. Expand this to a real-world corpus and there will always be too little data for some of the hypotheses somebody will want to test - too few samples, of course, but also too few samples in some particular subcategory, and too little metadata (was this sample old or new? Produced by a native speaker? Produced by a member of some minority? From the Midwest? In a noisy environment? Under stressful conditions? Male or female? Young or old?) and there's only so much you can do to improve this. There are two obvious conditions I want to point out for your specific question. You want to make sure there is as little garbage as possible in the full 2.3M, and you want to take as much as possible of it into the golden set. The procedure I'd like to propose is iterative. If you decide to try to start with 0.5% for the golden set by random sampling, it is somewhat likely that some part of those samples are unsuitable for various reasons. That's good! Now you know what problems to look for in the rest of the corpus. Then maybe try to push it to 1%. By and by you will also get a feel for where there are problems which you can't solve by just discarding bad data. That's not so good, but definitely something you want to mention in the accompanying documentation. We really cannot tell if 0.5% is reasonable. If each sample is a single word then even 0.1% should be sufficient. On the other hand, if samples are sentences (on the order of 23 words) or paragraphs (on the order of 230 words) an off-the-cuff estimate of a good number of samples to review thoroughly for inclusion in a gold standard set would be higher. But again, this depends a lot on the types of samples, and the cost of processing and reviewing them. Without seeing your data or understanding your requirements, we can really only reason in very general terms. But a good rule of thumb at least to get you started is to assume that you have a logarithmic distribution -- if not exactly a Zipf distribution then at least a few phenomena with a lot of hits and a long tail of different types of outliers. And in the absence of any better data to guide you, remember the 80/20 rule -- 20% of the explanation will already cover 80% of the samples (so in your case, if you have a set of rules which capture 1/5 of the corpus, you can expect to put in roughly 4x this amount of rules to achieve reasonably full coverage). This also extends to error estimation -- when you start seeing mostly the same error over and over again, you have covered roughly 20% of the total errors. (Sorry if this is a bit rambling; I seem to have gotten a bit over-enthusiastic here.) 

There is a so-called anti-lexicalist view of our knowledge of words. The idea is that while it is still impressive that we hold thousands of morphemes in our long-term memory, the size of our lexicon might be much smaller than we thought. For example, there is simply no need to hold past tense forms of verbs in memory because most of them can be derived by attaching -ed to a stem. So instead of having, say, 1000 verb roots and 1000 corresponding past-tense forms in your memory, you only have 1000 verb roots and one "reusable" -ed morpheme. Apply this to many other multimorphemic words and you get a severe decrease in size of lexicon. I also have a feeling that not all of the words we "know" are equally easily extractable - in fact, we do not use more than just 1000 distinct words in everyday life. So, I will be not surprised if thousands of words are not even stored in our memory. 

First of all, it is not the case that "who" cannot raise over "did" in T (or more precisely - over the tense affix), because it does so when moving from Spec-VP to Spec-TP anyway (under the VP-internal subject hypothesis). Second, X-bar theory itself says nothing about the constraints on movement; it simply states how the structure is organized - basically that every head has a bar-projection and then there is a specifier and a complement. Third, Doubly-filled COMP filter uses the conception of COMP present in 70-80's, where both wh-words and complementizers like "that" both were of COMP category, that is C. So it does not really apply to cases where there is a wh-word in Spec/CP and "did" in C. Now back to the main question - why? Radford in his 2006 textbook "Minimalist Syntax Revisited" suggests that wh-words are attracted by an edge feature [EF] to Spec/CP and tense affix T is attracted to C by a tense feature [TNS], both of features are part of C. He then refers to Pesetsky & Torrego's analysis where wh-subjects can bring [TNS] with them when moving from Spec/TP (unlike wh-objects which never pass that position) and thus satisfy both [EF] and [TNS] of C in Spec/CP. Another option is that wh-subjects never really possess [TNS] and that [TNS] in C simply "needs" to attract something from T or its projections. In wh-object questions [TNS] can only attract whatever is in T and not the subject that is in Spec/TP because Spec position of C is already occupied by the wh-object. In wh-subjects cases, though, both [EF] and [TNS] would be satisfied by just moving a subject to Spec/CP. I'm not aware of any other analysis of this and honestly I would be very unsatisfied by the one above since it posits the [TNS] feature which has no justification and is mostly unneeded in the analysis of wh-movement in other languages. 

it's historical. Obviously there are only two choices. some languages evolved one way, some the other way, and in some languages like latin word order is (mostly) free. It's related to whether your language has case endings,but there could also be other factors, like gender. human languages are incredibly diverse. it has nothing to do with logic or rationality. 

old joke: a language is a dialect with an army. terms like "the German language" are practically useful; they help us to communicate, which is to say, to help us coordinate our actions. But in reality there are no such things. What counts as the German (or French, Chinese, etc) language is always driven by norms established by a dominant social group. Real people speak whatever they speak, which is always unique to the individual: an idiolect. once you have a community of speakers whose idiolects enable successful joint action (communication), then you can start talking about dialects, languages, etc. but in all cases what counts as the language is a matter of norms, not any kind of objective rule or grammar. If you find German perplexing, try Chinese or Arabic. Both of those terms are very broad, umbrella terms that cover enormous variation. It's useful to have them so we can say, e.g. "he speaks Arabic", but in fact nobody speaks "Arabic", since there is no such thing. What he speaks is one of millions of idiolects that enable people in a broad community to communicate. so terms like "the German language" are really more sociological than linguistic. PS. I don't have the citations handy, but there is good reason to think of dictionaries and grammars a indoctrination devices. 

Arabic has no fewer than 4 forms: independent, initial, medial, and final. but in all cases there is a "kernel" form common among them all. one advantage of this design is that you do not need spaces to separate words - not a minor consideration if writing materials are expensive. Caveat: in Arabic, final forms are not always distinct. for example final Waw can look just like medial or initial or independent Waw. but the system works. 

whether or not "tense" is even meaningful in (Classical) Arabic is an open question. Arabic verbs do not generally have tense, although they have aspect. to give a definite future meaning to an imperfect like yadbribu you prefix the invariant "sawfa", a separate word. but "yadribu" can have any "tense" depending on context. "daraba" is taught as past tense (he struck) and "yadribu" as present or future, but that's wrong. the main distinction is aspect not tense.