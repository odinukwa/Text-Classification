The whole point of NoSQL is that it doesn't use the same type of datastructures as an SQL database. It requires a whole different way of thinking and approach with your app, so there isn't something to handily copy the schema over because it is going to be unique to your solution. 

This turned out to be a red herring. When pgadmin gets disconnected midquery, it returns the last error again even if it was an entirely different command! :) 

I don't know how to write this out in the above format but you are going to have a table for books, and a table for categories with a foreign key to the book. If there were more columns for the category, to normalise this you would need to add a third table to allow a many to many relationship with foreign keys to both book and category. Following the example of what you have said above and the statement that the right hand side can not be empty, this can't be represented in that way! Plus the reason it generally doesn't seem to make sense is that in the real world you would probably want to implement this using surrogate keys. 

They can analyse past queries and suggest/create indexes however this doesn't work optimally because indexes strike a balance to speed up what you want optimised at a cost and the server can not know your intentions. 

Where info is personalised it should be normalised into linked tables that only join to rows where that optional column has been added. Also if you might be adding many columns or long data it is common to have a core user table with extended info tables mapped on a one to one basis. These methods allow for smaller indexes more specialised to each query scenario. Generally, good normalisation and matching tables well to real world entities rather than theoretical ones gives the most adaptable and future proof design. 

Rows are not returned in any specific order in SQL (in reality they are likely to be in the same order most of the time) so without an ORDER BY any of the rows could be returned first. So it is non deterministic because this first record can change each time you run the query. Wrapping it in a block doesn't make the answer deterministic, it just hides the warning. 

Depends on who is allowed to access your application. Probably not a good idea to give DB access to users since they'll be able to connect to the database directly which is probably not what you want. If its a whitelist, maintain a list of users and if applicable which roles they hold in the database. Run your web application using a Windows account, and grant access to that identity to the database. Authenticate users using Windows Authentication, dont perform password management on your own. 

However SSAS itself supports 128 axes I believe. What can I use to run ad hoc queries against SSAS with more than 2 axes? 

However, it requires me to write the last line in a customized manner for every table I apply it to Instead of the above, is it possible to write something of the sort: 

I've read about it and want to play around with it for a bit. I think it requires a minimum of 4-5 machines to run, which is feasible with Azure IaaS, but how do I actually deploy it there? 

From your application layer, stamp each query with a timestamp and batch ID. Instead of executing the queries, store them in a temporary table with the format 

In the code below, the fails when executed as-is, but removing the in the and replacing it with a static value works fine. Is there a different syntax required when using with an ? 

Best practice would be to install and use it from client machines, since every tool running on the server is consuming some resources which will not be available to the Oracle server Installing it on the server -- probably no penalty as pointed out in the comments 

However, in 2 separate sessions, if I run a long running select with in the 1st and an in the 2nd (or vice versa), whichever query starts first blocks the second query (as per ) Looking at , both queries have a transaction isolation level of read committed (2) As per my understanding, with snapshot isolation on, tempdb usage should increase however blocking should not occur in this situation. Am I missing some configuration steps to achieve this behaviour? 

When you know all queries from a batch have been received, kick off a stored proc that retrieves all queries from the batch, orders by timestamp asc and executes them You'll probably want to add more logic about establishing dependencies between batches and how to deal with failed error,etc