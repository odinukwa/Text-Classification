Before such an alphabet could be created, there would have to be agreement as to what the features are, and there is no such agreement. An additional problem is that the resulting system would generate a superset of the IPA, thus would not be the equivalent of the IPA, since the content of the IPA is essentially arbitrary in not including symbols for all articulatory configurations (e.g. there is no symbol for a bilabial flap). Moreover, IPA is not just a collection of letters, it is letters organized according to an articulatory classification. There would be no logical problem in creating a general feature-based alphabet (assuming you can decide on a theory of features), which would require about 2 dozen combinable graphic elements. The problem would be coming up with a system that could be visually parsed. 

First, you need to explain what you mean by "phonetic", "phonological" and "analysis". You also need to explain what you mean by "before". To simplify matters, I reduce "phonetics" to acoustics, because that provides the necessary and sufficient basis for any statement about sound systems of spoken languages (and I also restrict the discussion to spoken languages). In order to analyze a language at any level, you need data, which comes in the form of acoustic waveforms. The first piece of analysis that you perform is a segmentation, where you approximately divide the waveform into sub-pieces that correspond to "segments". Now you (Teusz) need to explain what you mean by "phonological" versus "phonetic". Presumably, you agree that anything that deals with numeric measurements of the waveform, such as the duration of a sub-part defined by certain properties, or anything having to do with frequencies or amplitude, these things are all "phonetic". Questions about rule-ordering, underlying forms, cyclicity and so on, are questions about phonology. It is completely unclear where to slot questions such as "is that a cluster [th] or an aspirated stop [tʰ]?"; "is that [d] or [t]?"; "is that sequence [tua] or [twa]?", along the phonetic-phonology divide. The basis of all analysis is a phonetic object, the acoustic waveform, but the very first analytic product is a crude phonological categorization into segments. It is obviously premature to be thinking in terms of contrastiveness at this point: the initial goal is to develop an adequate system for reducing the massively variable acoustic waveform to vastly fewer discrete symbols. Is this a phonological analysis or a phonetic analysis? The input of analysis is a phonetic thing, but the output, a discrete segment symbol, is a phonological thing. So it depends on what you mean by "phonetic analysis" versus "phonological analysis". Suppose the first word you hear is nivaka "knife". The objective fact is that there is about 30 msc VOT after the release of the stop, though you are not yet doing an acoustic analysis. There is no phonetic basis for deciding whether to record this as [nivakʰa] versus [nivaka]. It's not until you encounter ehalakha "basket" with 70 msc of VOT that you understand that nivaka and ehalakha contrast in terms of the phonetic property of VOT duration (a.k.a. aspiration). The thing that you have to attend to in the data is a phonetic property, namely VOT, and the product of your observations is a phonological fact, namely a contrast. Discovering this contrast allows, indeed forces you to review your data to discover that you had previously encountered [kʰ] in solokho "mung bean", which you believed to be soloko. Since you happened to be recording everything, you determined that the speaker produced a somewhat atypical token with 50 msc VOT. There is no absolute minimum or maximum VOT value that defines phonetic aspiration, as shown by the fact that the aspirated stops of Thai have a shorter VOT than the unaspirated stops of Navaho (both languages have an aspiration contrast). One consequence of the fact that phonetic values of segments are not absolute is that in lieu of a contrast, you will have a hard time justifying a transcriptional choice. In very many languages, the highest back vowel sounds somewhere between Jones-defined [u] and Jones-defined [o]. IPA letters like [u], [ʊ], [o] represent ranges of formant values, and any attempt to numerically define the range presupposes a phonological categorization -- it would be invalid to include the values of the contrastive phoneme /ʊ/ from a language having the contrast /u, ʊ/ in a data-pool of formant values of [u]. It would be equally invalid to exclude languages which have no such contrast and where the highest back vowel sounds more like the Norwegian high back round vowel in the word "2". The problem with the word "before" is that it may falsely lead you to believe that you must create a repository of phonetic conclusions which you can subsequently further analyze into phonological conclusions. There is no strict separation of levels of analysis: creation of some phonological conclusions is a prerequisite for creating phonetic conclusions, which can be used to justify further phonological conclusions. The basis of analysis in sound systems is a phonetic object, the acoustic waveform, but the kind of analytic object that results from doing the analysis may be phonetic or phonological, depending on the methods that you use. 

Element theory differs from SPE theory (including autosegmental versions prior to Clements 1985) in two fundamental ways. Elements are in a way more abstract w.r.t. phonetic definitions, and they are privative. Additionally, the claim is that any single element is a well-formed segment and can be pronounced as such. Subsequent to Clements 1985, the difference between the theories has largely disappeared. Privativity was introduced partially via the node / feature distinction in geometric theories (nodes are labels with no features), and partly in response to radical underspecification (the ternary theory with +, - and 0). Following the logic of Lombardi (dissertation), it was realized that [+X] can be represented as X dominated by Y (dominated by Z) and [-X] can be represented as terminal Y dominated by Z: spreading of [-X] simply requires spreading Y, which wipes out any subordinate specification of X. There being no factual issue at stake, it has become more a matter of style or deeper-seated theoretical principles (such as a requirement to minimize the number of dominating nodes allowed in the theory). Since there are no clear and compelling arguments for or against either theory, we have reached a stalemate. A classical example of the move towards more abstract features is seen in the adoption of "labial" which encompasses bilabial and labiodental in consonants, and rounding in vowels and glides. SPE theory required [round] for round segments, and [+anterior,-coronal] for labial consonants – Post-Sagian feature geometry, especially Unified Features theory, requires just Labial. The Parallel Structures Model (PSM) is even more abstract, in positing very few features which have only minimal phonetic content, and is just a step away from Radical Substance Free Phonology which does not assume phonetic feature definitions at all. Element theory started with just a very few elements and they employed relational combinations to derive the same phonetic outputs: that is, feature theory converged on ET. The closest theory pair that I can see is Element Theory and PSM. They differ in what the primitives are, and I think they differ in some minor bit inter-translatable formal ways, but the one main difference between PSM and ET is that in PSM, you sill need a language-specific interpretation device that tells you what a segment with a single feature is. In ET, |A| is [a]: that theory retains a predefined universal phonetic core. One of the motivations for ET is it's program of reifying crosslinguistic probability distribution in representations: uncommon vowels are structurally more complex. Since PSM and RSFP are strictly theories of computation, probability distribution is a non-issue. So to put the matter somewhat differently, if you want your theory of grammar to encode Markedness Theory, then you would like ET. If you want a simpler theory which shifts the burden of explanation to grammar-external factors (various phonetic factors shaping the primary data that are the inductive base for grammar-learning), PSM / RSFP would be more appealing. 

Word-formation rules are generally assumed to combine an existing element with something. The initial element in a derivation is inserted by a lexical insertion rule. However, nothing important follows from distinguishing "word-formation" and "lexical-insertion", so they can be subsumed under one general rule concept. 

If you were asking about a distinctive feature analysis, those classes can be unified as the [-continuant] segments, although the analysis of trills is not entirely established (the issue is whether a separate feature [trill] is necessary: let's say that it is). You can then call them the "non-continuants", as long as it's understood that you're referring to an SPE-style analysis of features. There is otherwise no term that puts stops and trills together. The term "occlusive" almost fills the bill, except that trills are not typically included under that umbrella. 

Actually, [ð] is not so rare across languages. Apart from English, it exists in Icelandic, Swedish and Norwegian dialects, Danish, Kven, Saami, Welsh, Spanish, Catalan, Basque, various Italian languages, Albanian, Greek, Arabic, Tiberian Hebrew, Berber, Hawrami, Mari, Bashkir, Turkmen, Somali, Dahalo, Moro, Didinga, Kikuyu, Kamba, Swahili, Tamil, Burmese, Thao, Iai, Palauan, Fijian, Aleut, widely in Athabascan, Shoshone, Lakota, Osage. It is not as common as [t] or [s], but certainly not rarish like [ʕ, ƛ'] or rare like [ʘ]. The main thing that [ð] has working against it is that it is acoustically harder to detect. Because of the lingual constriction, it has reduced airflow which means less acoustic energy, and the flow is not turbulent (which makes a sound noisier, thus easier to detect). The energy is spread out over the frequency spectrum (compared to [ʒ]) and such peak as it has is relatively higher (compared to [γ]). The latter fact is relevant because sounds in the "frequency center" are most audible and those at the high and low ends are less audible to human ears, given the same sound pressure level (and as I said, it also has a lower net amplitude). It is possible that one can accurately say that a phonemic contrast between d and ð is rare (often, as in Spanish, d and ð are in an allophonic relation), though there is no systematic database of phonemic contrasts that we can consult to see what its frequency is. 

It does seem to be a problem of loanwords, and apparently language change, since I understand that older loanword patterns differ from contemporary patterns. You might look at this and that, and references therein. I've been told that the phonetics of tense consonants has been changing over the past 50 years, which may underly these conflicts. Since p is somewhat aspirated word-initial (but not as much as ph), then pp is really a better match for an unaspirated p. 

is not a definition of "sentence", it is a claim about GG and what a grammar does does. I will leave aside the question of whether it is accurate as a characterization. If this claim is true, we might infer some definition of "sentence", but it isn't a definition. I presume that your real question is about some claim, and not the definition of "sentence", but I'm not entirely sure what the claim in question is. The primary question, I think, is a version of the old competence / performance problem: is "what to do?" grammatical, or merely acceptable? If this string is grammatical, then (by definition) it is generated by the grammar, and the claim that (grammars only generate sentences and all sentences have subjects and verbs) is false. That is, at least one of those claims is false, possibly both. If the string is ungrammatical but acceptable, then we learn nothing about what a grammar does. The claim that every sentence has a subject and a finite verb has a consequence, that imperatives are also not sentences. Forget that, I say. It also means that many utterances in languages without verbal copulas are not sentences. Perhaps (indeed, almost certainly), the claim was intended to be about English, not the concept "sentence", but GG is not a theory of English, it is a theory of language (despite initial appearances). We could say that the claim is not a claim about surface word order, it's a claim about logical form, so that there are always elided finite verbs or deleted subjects, but that "What to do?" would come from "What should we do?". Similarly, "Banana" could be the sort form of a number of sentences, such as "I'll have a banana" or "The English word for 'banana' is 'banana'". There is no good way to assess the social construct of conventionality, because there's no system of opinion-polling / voting. One may reach certain surmises by looking at the literature to see what certain people say, and perhaps that it the only operationally-sensible way to talk about conventionality of concepts. Each publication is weighted according to impact factor of something like that, and if high-impact papers tend to converge on a particular conclusion, that is what counts as "conventional". I presume the problems with this approach are obvious.