I guess it is clear that the Walrasian equilibrium is not Pareto efficient. Notice that this "result" highly depends on the definition of the consumption space. If you add 

The idea is precisely that players do not chose actions, but only chose one action at the time at every node at which they play, based on their beliefs about the way other players and themselves will play at future nodes in the game (where beliefs are conditional on the history that led to that node). The interpretation is letting players choose full-fetched strategies is equivalent to letting players rely on a computer program to plays the game in their place. That is, they can commit via this computer program to playing a given action at each node. Such games with commitment devices are in essence very different from games in which the actual players have to repeatedly chose an action at each of their decision nodes. When actual players play at nodes, players have to form beliefs about the way other players and themselves will play at future nodes, and these beliefs may depend on the history that led to future nodes. For instance, in a Stackelberg game, the leader could believe that the follower will be rational (i.e., utility maximizing) if the leader plays "Low production", but will be irrational (i.e., non utility maximizing) if the leader plays "High production". Maybe the leader anticipates that the follower will be angry if the leader plays "High production", and that, blinded by her anger, the follower then then want to retaliate. If the follower could have committed through a strategy, the game would have been completely different. Maybe the follower could have committed not to retaliate before she gets angry, and she cannot help her desire to retaliate anymore. But here the idea is that the actual follower has to choose an action later in the game given what the leader chose at the root node. Therefore, the behavioral rule through which the follower chooses an action at a node (e.g., utility max vs. non utility max), and the beliefs of the leader about these procedures may depend on the history that led to that node too. This opens the way for many new outcomes of the game that would not have emerged from classical game theory. From a conceptual point of view, it also switches the focus from solution concepts to epistemic and behavioral assumption (i.e., from classical game theory to epistemic game theory). Instead of identifying a set of reasonable outcomes (e.g., Nash equilibrium outcomes) and look at the strategies that match these outcomes, one identifies reasonable properties of players' behaviors and beliefs (about each others' beliefs and behaviors), and derives the conclusions of these epistemic and behavioral assumptions for the outcome as the game unfolds. Now, this is just to give some meat and intuition to Battigalli's framework, and it is does not do justice to the richness of the framework (in part because I don't know much about his work other that the video you linked to). If you haven't done it yet, I strongly recommend that you watch the whole video. I think Battigalli does a great job at making his framework accessible. He also present helpful and intuitive examples to connect his epistemic approach to "classical" game theory by identifying simple conditions on players beliefs and behaviors that allow to recover classical solutions to games such as backward induction. 

to the consumption space, then you would recover efficiency (although it might take a little care to define a meaningful notion of Walrasian equilibrium in this case). 

This might not exactly get at the get of situations you are interested in, but you might be interested in the famous "Fairness-study" on capuchin monkeys. You can find a quick account of the study in this video $URL$ Of course, this is a lab, not strictly speaking in a natural environment. Also, there is no trade through money between monkeys themselves, only something that resembles monetary trading between the monkeys and the experimenters. Maybe this only suggests that monkeys potentially have the ability of engaging into monetary trading? But that might be taking the results of the experiment a little too far. 

However, I would already be very happy with code for Boston and TTC, even if the two last features are not implemented. 

@Oliv's comment lead me to the following wikipedia article $URL$ which indicates that the term "inverse" is used to define the kind of relation between $R$ and $R'$ that I described in the question. The wikipedia article recommends to use $R^{-1}$ to denote the inverse relation of $R$. 

What I would like to do is create a flexible experiment in which the number of choices that players have to make varies with a parameter . That is, I want the whole framework to generate possible choices by just setting a parameter , and not having to go update all the files manually. This is in order to make my life easier if we ever change the experimental design, and to make the code useful to others with different experimental design (I plan to release it on Github at some point). It seems like it should be fairly easy to do with a couple of loops, but I am having trouble using lists given the way is structured. From what I understand, I only see one very nasty way to have the number of choices depend on a parameter . I first give each choice a different name in , e.g. 

Because the notion of pairwise stability is so important in the theory of network formation and equilibrium, there has been a few attempts to test it in experimental setups. I know at least two papers: 

The reason is that at the same time the wage of the high-skilled increases. By reducing the minimum wage, the number of people working in the low-skilled sector increases (involuntary unemployment is reduced) which leads to an increase in the wage of the high-skilled. The corresponding proposition in the paper is Proposition 3, the argument of which is 

Difference between the two There is not much connexion between the two notions as defined above. Every combination of the two notions is a priori possible. Both a mechanism and a contract can be 

Another example is Economies with Public Goods: An Elementary Geometric Exposition by William Thomson, which relies extensively on the geometry of equilateral triangles. In general, William Thomson is known for making a heavy and interesting usage of geometry in his papers. See for instance a recent working paper of his : Compromising between the proportional and constrained equal awards rules (although this one might be further from basic high-school euclidean geometry than what you're looking for). Other somewhat similar examples include papers relying on the geometry of simplices. An example I came across recently is 

It turns out that the VWR is sufficient to derive many important policy implications. For instance, under the VWR (and single crossing), marginal tax rates must be non-negative at a solution of the taxation problem. Also under the same assumptions, the marginal tax rate of the highest type should be zero, and any type different from the highest type whose consumption and income are strictly positive must face a strictly positive marginal tax rate. Provided the non-normative assumptions of the model hold (e.g. single crossing, but also the fact that decision maker's preferences are representable by a SWF, more on this below), these are policy recommendation you can safely make to any policy decision maker with some form of egalitarian concerns, even if you do not know the exact degree to which they are egalitarian. Another question you are raising is whether policy decision makers' preference can be represented by a SWF at all. To say the least, I am not a big fan of the cardinal utility framework of SWF. This is mostly because hidden in the cardinality assumption is the idea that one can somehow "objectively" compare utilities. But it is also partly because some decision maker's preferences might be not representable through SWF. However, there are alternatives to both these problems, with applications to optimal taxation. See for instance the last chapters of A theory of fairness and social welfare, by Fleurbaey and Maniquet. The approach described in A theory of fairness and social welfare, by Fleurbaey and Maniquet is not only ordinal, it is also purely axiomatic. Decision makers may have a hard time figuring out which SWF represent their preferences. With an axiomatic approach, they simply have to decide on the axioms they like the best to "discover" their preferences. This is often much simpler than trying to figuring and spelling out your full preference relation. 

From real wages and price data, he then infers a measure of (extreme) poverty. Look at Figure 5 in the paper. The scale of the y-axis is a measure of poverty in which 1 is the poverty threshold. Values above 1 on the y-axis correspond to real wages lower than survival rate (i.e. that do not allow a survival caloric intake), and values above 1 indicate a real wage above survival rate. 

I think I am guaranteed to confuse the hell out of them, because the textbook essentially tells them that "we should never talk of a 'change in quantity demanded' unless the demand function remains unchanged". At the same time, is there any other ways to formulate 1)? 

I think you are right to say that an economy is always a subsystem of a larger ecosystem, but it needs not always be modeled as such. It may be interesting to have a precise and elaborate model of the solar system taken in isolation, independently of its interactions with the rest of the galaxy, other galaxies, etc. In very much the same way, there are many good reasons to build a model of an economic system without accounting for how it fits in a more general ecosystem. The same kind of question applies to many other topics. One could for instance ask "why partial equilibrium and not always general equilibrium?", "why small open economies instead of replicating the actual size structure of the global economy?", etc. I guess the only answer is that the smaller, less general models are considered useful in one way or another. This is not to say that the opposite endeavours (the ecological economics attempts to model economic systems as part of an ecosystem) are worthless, quite the contrary. But there is only so much you can do and there is always a trade-off (in terms of estimability, tractability, additional assumptions, you name it ) to increasing the generality of a model. 

Social and Economic Networks, by Matthew O. Jackson, and its associated MOOC at $URL$ (videos from the 2014 session available, new live session starting September 2015). 

$S^N_i = \{$ pure strategies of $i$ in $\Gamma^E$ $\}$ , and $u^N_i(s) = u^E_i(c(s))$, where $c(s) $ associates every profile of pure strategy in $\Gamma^E$ with a terminal node of $\Gamma^E$ resulting from the pure strategy profile $s$. 

No, not necessarily. Without the independence axiom (or something else to replace it) there is not much you can infer about the preferences over lotteries if you only know the preferences over outcomes. For instance, let $p^L_n$ be the probability of outcomes $n \in \{1,\dots, 3\}$. Then preferences over lotteries $\succeq^*$ represented by the utility function $$U(L) = p^L_1 + \beta [p^L_2p^L_3],$$ are continuous and rational, but do not satisfy the independence axiom. For $\beta$ large enough, it is not even the case that $(1,0,0)$ is the best lottery, although $(1,0,0) \succ^* (0,1,0)$ and $(1,0,0) \succ^* (0,0,1)$. To see why, observe that $$ U(1,0,0) = 1, $$ $$ U(0,1,0) = 0, $$ $$ U(0,0,1) = 0, $$ However, for $\beta > 4$, $$ U\left(0,\frac{1}{2},\frac{1}{2}\right) > 1 .$$ Violation of the independence axiom can be seen from the fact that, when $\beta > 4$, $$ [1,0,0] \succ [0,1,0] ,$$ although $$ \left[0,\frac{1}{2},\frac{1}{2}\right] \succ \left[ \frac{1}{2}, 0, \frac{1}{2}\right]. $$ 

I ended up putting together some code to compute the assignment under Boston and deferred acceptance. It can be found at $URL$ The code there uses and modifies former code from Jeremy Kun, stable-marriage, (2014), GitHub repository, $URL$ described in one of Jeremy's blog posts at $URL$ I hope to expend on the code that’s currently available and add more functionalities soon. Any participation is welcome. 

If you are only looking for references, try google scholar: $URL$ Among the papers on the first pages of results : 

I believe insights from repeated game theory have been used in designing the assignment scheme for security patrols in many occasions. In this matter, the main research projects with close connections to applications that I know of are (a) PROTECT for the US Coast Guard (b) TRUSTS for the Los Angeles Sheriff's Department (c) IRIS for the Federal Air Marshal's Service (d) LAX for the Los Angeles Airport Police (e) GUARDS for the Transportation Security Administration which all revolve around the teamcore research group at the University of Southern California. Examples of papers involving members of the group and repeated games (usually repeated Stackelberg security games) include: 

These are new journal (the second is planning its first issue sometimes in 2016) but they are managed by serious people and are likely to become good references. 

First-best ex-post efficient (i.e., efficient when incentive compatibility constraint is not imposed and the outcome of the mechanism/contract must be deterministic) First-best ex-ante efficient (i.e., efficient when incentive compatibility constraint is not imposed and the outcome of the mechanism/contract can be random) Second-best ex-post efficient (i.e., efficient when incentive compatibility constraint is imposed and the outcome of the mechanism/contract must be deterministic) Second-best ex-ante efficient (i.e., efficient when incentive compatibility constraint is imposed and the outcome of the mechanism/contract can be random) 

This is only a partial answer because it does not exactly fit your framework, but I hope it will still be helpful (and it's too long for a comment). If you are ok with discretizing your cake into (possibly arbitrarilly small) pieces of cake, then you will find an answer in 

The bold part is important. If it is not satisfied, there may still be a non-truthtelling equilibrium in the equivalent direct mechanism. An example is provided in Repullo (1985), Review of Economic Studies pp 223-229. $$A \equiv \{a,b,c,d\}$$ $$ \Theta_1 \equiv \{ \theta_1', \theta_1''\}$$ $$ \Theta_2 \equiv \{ \theta_2', \theta_2''\}$$ $$ \begin{array}{c |c c c c} & a & b & c & d \\ \hline u_1(\cdot, \theta_1') & 2 & 4 & 2 & 4\\ u_1(\cdot, \theta_1'') & 1 & 0 & 2 & 4\\ u_2(\cdot, \theta_2') & 2 & 2 & 4 & 4\\ u_2(\cdot, \theta_2'') & 1 & 2 & 0 & 4\\ \end{array} $$ $$S_1 \equiv \{s_1',s_1'',s_1'''\}$$ $$S_2 \equiv \{s_2',s_2'',s_2'''\}$$ The game form is $$ \begin{array}{c |c c c} & s_2' & s_2'' & s_2''' \\ \hline s_1' & a & b & b \\ s_1'' & c & d & c \\ s_1''' & c & b & a \\ \end{array} $$ On can check than the following is an equivalent direct mechanism $$ \begin{array}{c |c c } & \theta_2' & \theta_2'' \\ \hline \theta_1 & a & b \\ \theta_1 & c & d \\ \end{array} $$ Yet, when the types are $(\theta_1',\theta_2')$, although telling the truth is a dominant strategy, any other report of preferences is also a dominant strategy. This may be quite bothersome as it means that for some configuration of the types, saying the truth is only one equilibrium among others. As a consequence, we have no real guarantee that "telling the truth" will be played (it may even be that truth-telling is Pareto dominated by another equilibrium. Using a focal point argument, this may further undermine the relevance of the truth-telling equilibrium). The above issue is due to the fact that in the original game, some strategies are never played, which is ruled out if $s^*$ is surjective. So Repullo's version of the Revelation Principle (requiering that every dominant strategy equilibrium outcome of the equivalent game be among the equilibrium outcome of the initial game for every possible configuration of the types) only holds if the equilibrium selection function be surjective, and fails otherwise. 

I could try to write a complete proof but this seems to be basic enough a statement and I don't want to re-invent the wheel (or fail to pay tribute to former proofs). So my question is: 

Then look at the papers who cite the above papers. That usually allows you to uncover other relevant references. 

If you can access the paper, useful complements on the general equilibrium dynamics determining wages are in Appendix A. 

This is only for Europe unfortunately, but Bob Allen has a famous paper in which he presents reconstructed data for real wages in different European cities from 1350 all the way to 1799: 

I once wrote an example of Kreps criterion using the canonical signaling model and The Simpsons. I think it goes along the same lines as @Ubiquitous' answer while being much less precise and general. But I thought the Simpsons' context might help in a pedagogical setting. Suppose that Hank Scorpio must decide of a wage schedule for employees at Globex Corporation depending on observed education. There are two candidates : Martin Prince, a $H$ type (for "high") with an elementary school degree $e_1$, and Homer, a $L$ type (for "low") with a degree from Springfield University $e_2 > e_1$ (cf. Season 5, episode 3}). A third possible signal would consist in getting a PhD in nuclear physics from MIT, which we denote $e_3 > e_2$. Suppose Scorpio's believes that the productivity associated with the twho lower education levels are $\rho(e_2) > 0$, and $\rho(e_1) = 0$. Assume that this forms a sequential equilibrium, that is, at this equilibrium, neither Martin nor Homer find it worth it to get a PhD from MIT (I assume that if you are at the point of explaining Kreps criterion, you already covered sequential equilibira). Martin would not need to exert much effort to get $e_3$ (see children's power plant competition, season 8, episode 23), and he would not mind doing so if it was the case that $\rho(e_3) = 1$. On the other hand, Homer is far better with his $e_2$ than he would be with $e_3$ even if $\rho(e_3)$ was $1$ because getting a PhD from MIT would be a huge pain for him (cf. aforementioned episode). Because $(e_1,e_2,\rho)$ is an equilibrium, $\rho(e_3)$ must be sufficiently small to deter Martin from obtaining his PhD. This means Scorpio attaches a high probability to the fact that agents choosing $e_3$ are $L$ type. Is this equilibrium supported by reasonable beliefs? Not according to Kreps criterion : under the assumption that Scorpio knows that Homer would never try to get $e_3$ while Martin would not mind getting $e_3$, if Scorpio observes someone getting $e_3$, he could logically infer that this person is Martin, a $H$ type.