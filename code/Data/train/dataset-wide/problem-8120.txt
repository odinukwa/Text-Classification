If you ask the matrices to be unipotent, then, first, you can only get matrices from $SL_n$, and second, three is not enough, the torus is the obstacle. But if you take four upper and lower unipotent triangular matrices, you can do it (still in $SL_n$). For this the keyword is "unitriangular factorisation". 

Here "nice" means that there is an explicit description of the action of the standard generators of $\mathfrak{g}$, so one could actually write (infinite) matrices for the Lie algebra elements. I have checked some sources including Lie Algebras of Finite and Affine Type by R. Carter and Affine Lie Algebras, Weight Multiplicities and Branching Rules by Kass, Moody, Patera and Slansky, but none contains such details even for the smallest cases like $\tilde{A}_1$. Here is a picture from the second: 

Corollary 7.10 extends this result to rings of the form $A[x_1^\pm,\ldots,x_k^\pm,x_{k+1},\ldots,x_n]$ for $A$ regular. For other Chevalley groups the situation is complicated. One has the stability theorems for $K_1(\Phi)$ in terms of stable rank (or its ramifications such as absolute stable rank or $\Lambda$-stable rank), but they give pretty bad bounds for polynomial rings. There is, however, the following version of Suslin's theorem for symplectic group in a paper "On symplectic groups over polynomial rings" by F. Grunewald, J. Mennicke and L. Vaserstein: 

By locally principal ring they mean a commutative ring such that its localization at any maximal ideal is a principal ideal ring. For euclidean ring $A$ this gives $K_1(\mathsf{C}_\ell,R)=0$. As a byproduct they also prove a stronger version of Suslin's theorem for $SL$ and a locally principal ring. They also have a version for Laurent polynomial rings and claim that by using stability theorems for $K_1$ as in M. Stein's papers one can prove the same results for classical simple algebraic groups of relative rank $\geqslant2$, but the latter has never been written in full details. 

Consider the root system $\mathsf{B}_n$ with the standard numbering of the fundamental roots (that is, $\alpha_n$ is short). Take $\alpha_{n-2}$ and $\alpha_n$ as a pair of orthogonal roots (indeed, $\alpha_{n-2}+\alpha_n$ is not a root), and take $\gamma=\alpha_{n-1}+\alpha_n$. Then $\beta_1=\alpha_{n-2}+\alpha_{n-1}+\alpha_n$, $\beta_2=\alpha_{n-1}+2\alpha_n$ and $\alpha_{n-2}+\alpha_{n-1}+2\alpha_n$ are all roots. But among the differences of the form $\beta_i-\alpha_j$ the only roots are $\beta_1-\alpha_{n-2}$, $\beta_1-\alpha_n$ and $\beta_2-\alpha_n$, so this gives a counter-example. The beautiful pictures of Hasse diagrams you refer to provide a good way to spot such examples, but for this one should draw them in a different way, which is easier to read. The keyword here is a weight diagram. For examples, see this collection of the diagrams along with a description of their various usages. Namely, Figure 14 on page 35 is the weight diagram of the adjoint representation of a group (or of a Lie algebra) of type $\mathsf{B}_n$, which also describes the structure of the root system (its vertices are the roots, plus the "zero weights" corresponding to the fundamental roots). One looks for a square which has non-consecutive label on its sides (say $i$ and $j$), such that the bonds joining its top or bottom vertex to something on the right are also labeled by either $i$ or $j$. Such a square is immediately found on the very bottom of the picture slightly to the left from the middle. By the way, the quick inspection of the weight diagrams shows that there is no counter-examples besides the one above (I'm not sure, but I haven't spotted any). 

International Journal of Group Theory is a free peer-reviewed journal published by the University of Isfahan in English. 

This is an expansion of my comment. The Smith normal form is a normal form of a matrix with entries in any given PID (but this probably works for non-domains and for Bezout rings in general). It goes as follows: given an $m\times n$ matrix $A$, there exist invertible $m\times m$ and $n\times n$ matrices $B$ and $C$ such that $BAC=\operatorname{diag}(a_1,\ldots,a_r,0,\ldots,0)$. Moreover, the entries $a_i$ satisfy $a_i\mid a_{i+1}$ and are unique up to the multiplication by a unit. This decribes the representatives of $GL(m,R) \backslash M(m,n,R) / GL(n,R)$. From here you can obtain the decription for $SL(m,R) \backslash M(m,n,R) / SL(n,R)$ — no independent multiplication by a unit anymore, so the difference is the same as between $K_1$ and $SK_1$. When $R$ is a Euclidean ring, one has an algorith for computing the Smith normal form. This works for any PID, in fact, but the resulting matrices $B$ and $C$ are not necessary elementary in this case. For a Euclidean ring the algorithm gives you the chain of elementary transfromation for obtaining the SNF. As you mentioned, $E(n,R)=SL(n,R)$ when $R$ is Euclidean, but this is not the case for a PID. This equality fails, for example, for the ring $S^{-1}\mathbb{Z}[x]$, where $S$ is the multiplicative system generated by all cyclotomic polynomials. This is a result of D. R. Grayson — $SK_1$ of an interesting principal ideal domain. Here are two other links with examples: 

Since Michael Lugo has found evidence that the Taylor series has zero radius of convergence, it's not a very good way to describe or even define $f(x)$. Is it clear that there is a unique $f(x)$ which is convex (at least for $x \ge 0$), and that that $f$ is smooth at 0 and real analytic away from $0$? There is a book on fractional iteration of functions that presumably addresses these issues. 

My answer that I gave on StackOverflow: The problem of finding the exact best error-correcting code for given parameters is very hard, even approximately best codes are hard. On top of that, some codes don't have any decent decoding algorithms, while for others the decoding problem is quite tricky. However, you're asking about a particular range of parameters where n ≫ k, where if I understand correctly you want a k-dimensional code of length n. (So that k bits are encoded in n bits.) In this range, first, a random code is likely to have very good minimum distance. The only problem is that decoding is anywhere from impractical to intractible, and actually calculating the minimum distance is not that easy either. Second, if you want an explicit code for the case n ≫ k, then you can do reasonably well with a BCH code with q=2. As the Wikipedia page explains, there is a good decoding algorithm for BCH codes. Concerning upper bounds for the minimum Hamming distance, in the range n ≫ k you should start with the Hamming bound, also known as the volume bound or the sphere packing bound. The idea of the bound is simple and beautiful: If the minimum distance is t, then the code can correct errors up to distance floor((t-1)/2). If you can correct errors out to some radius, it means that the Hamming balls of that radius don't overlap. On the other hand, the total number of possible words is 2n, so if you divide that by the number of points in one Hamming ball (which in the binary case is a sum of binomial coefficients), you get an upper bound on the number of error-free code words. It is possible to beat this bound, but for large minimum distance it's not easy. In this regime it's a very good bound. 

Any projective variety is also a real affine variety, by using the real and imaginary parts of the coordinates $x_{jk} = z_j\overline{z_k}$. You should first normalize the projective coordinates to have Hermitian-Euclidean length 1. Ordinarily the projective coordinates are sections of a line bundle that is only defined up to a scalar. But with the length normalization, the only ambiguity is the phase. The phase cancels out in the definition of $x_{jk}$, so it is a well-defined function rather than just a section. This realification of $\mathbb{CP}^n$ is a map to $(n+1) \times (n+1)$ Hermitian matrices. It is important in quantum probability: The image of the map is the set of pure states of a quantum system; its convex hull is the set of all states. This convex region is the quantum analogue of the simplex of states (= measures = distributions) for classical probability on a finite set. The matrices have unit trace, so the image lies in an $(n^2+2n)$-dimensional real subspace. You can check that it is a sphere when $n=1$. Another viewpoint that is important is that of toric varieties. The phase part of the toric action on $\mathbb{CP}^1$ is rotation about the $z$ axis, and the moment map is the projection onto the $z$ coordinate. This too generalizes to any projective toric variety. 

The relaxed quadratic programming problem is a red herring. It is true that quadratic programming over $\mathbb{R}$ with linear inequalities can be solved in practice, for one reason because it is a special case of convex programming. But in the stated question, the inequality $0 \le x_i \le b_i$ came from nowhere. The correct relaxation is even simpler: You should just minimize $\Phi(z)$ over all of $\mathbb{R}^n$, and the minimum is directly at $z_0 = Q^{-1}r$. After that, Mitch is roughly correct. The question as stated is exactly the closest vector problem, which is related to the shortest vector problem that Mitch mentions. The constant $s$ is not important. The question is to find the integer lattice point $z$ which is the closest to $z_0$ in the metric defined by $Q$. If you like, you can change distance to Euclidean distance, and change the lattice from the standard integer lattice to something else, by applying the operator $Q^{-1/2}$. $L = Q^{-1/2}(\mathbb{Z}^n)$ is a certain lattice, and you are looking for the point which is the closest in Euclidean distance to $Q^{-1/2}(z_0)$. In any fixed dimension $n$, the closest and shortest vector problems can be solved in polynomial time. There are various lattice reduction algorithms that only search polynomially many points. If the dimension $n$ is a parameter, then the situation is very different. For many purposes, people are happy with just a close vector or a short vector, not necessarily the closest or shortest one. The problem varies greatly in difficulty depending on how close is good enough, or equivalently whether there are few lattice points that stick out as much closer than all of the others. Close vector is intuitively harder than short vector, but there is a theoretical result that they are roughly equivalent in difficulty. Taking the strictest possible requirements, finding a close vector is NP-hard. There are intermediate levels of closeness, given by some tolerance that grows with $n$, that seem hard but are probably not NP-hard. Other levels of closeness can be done in polynomial time. There are lots of papers on the these two problems, and the Wikipedia page that Mitch mentions is a pretty good review: The GapCVP section addresses the approximate versions of the question that I mention briefly here. One weakness of the Wikipedia page is that it has more to say about hardness than practical algorithms. But it does mention two important algorithms: Lenstra-Lenstra-Lovasz and Ajtai-Kumar-Sivakumar. 

By algebras is meant the category $\mathbf A$ of commutative unital rings and homomorphisms over this field, but it is likely that these will have to be topological. Often in models of linear logic the functor $? A$ (why not in Girard) is the free monoid on $A$, ie the free algebra generated by $A$ qua vector space. The functor $! A$ satisfies $!(A^\bot)=(? A)^\bot$. However, there is a great deal of freedom in the choice of these operations. There can be models with the same underlying linear algebra but different $!$ operations. We do need Seely's equations: $!(A\times B)=(!A\times !B)$ and $!\mathbf{1}=I$. As a first step we also need to identify $\Sigma=!I$ and $R=!\Sigma$. Very likely, $\Sigma=K[x]$, the polynomial ring in one variable. $R$, considered as a variety, has as points the elements of the ground field. (Please would some commutative algebraist replace this with a more precise description.) 

No. There is no need to say any more than that, since the answer is in the question, except that MathOverflow will not let me submit something with so few characters. 

The article on Heyting algebras and frames is one of many that are truly awful in Wikipedia. Frames and complete Heyting algebras are completely different things. They are algebras for different theories and (so) their homomophisms are different. Johnstone's convention, which Tom has described and to which there are now few dissenters, allows one to use the algebraic machinery to speak in topological language, but without mentioning points. For example, in Johnstone's book you will find definitions of locally compact locales and of open (continous) maps. 

In order to have any hope of getting a universal property you have to think in terms of order-preserving functions (ie categorically) and not picking elements one at a time (graph theoretically). The subject that would have results like this is called domain theory. It assumes that the poset already has (honest) joins of directed subsets. The most likely kind of universal property is that $E$ is the image of the least co-closure. A co-closure is an order-preserving function $f:X\to X$ such that $f(x)=f(f(x))\leq x$ for each $x\in X$. In the example where you construction arose, can you construct such a function? If so, that's your universal property. If not, you're at sea. Edit in response to Werner's: Look up SFP or bifinite domains, on whom the main authors are Gordon Plotkin, Mike Smyth and Achim Jung (though the word bifinite was mine). However, you still haven't told us where the question came from. 

So the question comes down to whether you can prove $0=1$ in it. In general this is undecidable. Indeed, this is exactly the Entcheidungsproblem, if I recall correctly. 

Whilst the definition of addition of Cauchy or Dedekind real numbers is "obvious", multiplication is rather more tricky. Unfortunately, most accounts, including [RD], leave it as an "exercise for the reader", without even giving a hint about what the problem is, so the questioner is right to ask about this. The difficulty is intrinsic to multiplication: the only difference in the intuitionistic setting is that we must do the job properly, instead of bodging it by treating positive, zero and negative numbers separately. The point is that, if you want to achieve precision $\epsilon$ in the product of two numbers, one of which is bounded by $B$, then the other must be given within $\epsilon/B$. [MD] is not quoted verbatim in the Question, but it is close enough, whilst the accounts in [AH] and [AT] are essentially the same. [TD] gives a more general account of uniform continuity, taking explicit account of the modulus of convergence of Cauchy sequences (the function that says how far down the sequence you have to go to get a desired accuracy). This is needed elsewhere in constructive analysis. [BB] has by far the clearest treatment that I have seen of the arithmetic of Cauchy reals, building the modulus into the definition. (I admire this book for its "can do" attitude, not dwelling on the counterexamples.) It gives the explicit (but snappy) proof of correctness for multiplication. [BT] defines multiplication for Dedekind reals and proves correctness. It shows how Dedekind reals are the limiting case of intervals and also considers "back-to-front" (Kaucher) intervals, which are related to existential quantification just as ordinary intervals are related to universal quantification. [JC] defines multiplication in a completely novel fashion for Conway (surreal) numbers. This is adapted to multiplication of real numbers in a topos in [PJ]. Since [MD,AH,AT] do not give the explicit answer to the Question, here it is. As above, $\langle r_n\rangle$ is a Cauchy sequence if $\forall k.\exists n.\forall m.|r_{n+m}-r_n|\lt 2^{-k}$. We write $\alpha(k)$ for such an $n$ for each given $k$; this is the modulus of convergence. In particular, with $k=0$, for any Cauchy sequence $\langle r_n\rangle$ there are integers $N=\alpha(0)$ and $K=\log_2(r_N)$ such that $\forall m.-2^{K}\lt r_N-1\lt r_{N+m}\lt r_N+1\lt 2^{K}$. Let $M$, $L$ and $\beta$ be the corresponding integers and modulus for the Cauchy sequence $\langle s_n\rangle$. Now, given $h$, let $k\geq h+L+1$, $l\geq h+K+1$, $n\geq\alpha(k)$ and $n\geq\beta(l)$. Then, for all $m$, $$\begin{eqnarray} |r_{n+m} s_{n+m} - r_n s_n| &\leq& |r_{n+m}| |s_{n+m} - s_n| + |r_{n+m} - r_n| |s_n| \\ &\lt& 2^K 2^{-l} + 2^{-k} 2^L \leq 2^{-h}. \end{eqnarray}$$ Hence $\langle r_n s_n\rangle$ is a Cauchy sequence with modulus $\gamma(h)=\max(\alpha(h+L+1),\beta(h+K+1))$. We can avoid considering equivalence of sequences explicitly, by observing that two Cauchy sequences are equivalent iff they are both subsequences of the same Cauchy sequence. Rationals are represented by constant Cauchy sequences and the new operation for them agrees with multiplication of rationals. This argument amounts to saying that the new operation is continuous with respect to the Euclidean topology. Also, the rationals are dense amongst Cauchy reals. Hence the new operation is the unique continuous extension and it follows that it obeys the usual algebraic laws for multiplication. [BT] Andrej Bauer and Paul Taylor, The Dedekind Reals in Abstract Stone Duality, in Mathematical Structures in Computer Science, 19 (2009) 757-838. [BB] Errett Bishop and Douglas Bridges, Foundations of Constructive Analysis, Grundlehren der mathematischen Wissenschaften, Springer-Verlag, 1985. [JC] John Horton Conway, On Numbers and Games, Number 6 in London Mathematical Society Monographs. Academic Press, 1976. Revised edition, 2001, published by A K Peters, Ltd. [RD] Richard Dedekind, Stetigkeit und irrationale Zahlen, Braunschweig, 1872. Reprinted in [DW], pages 315–334; English translation, Continuity and Irrational Numbers, in [DE]. [DE] Richard Dedekind, Essays on the theory of numbers, Open Court, 1901; English translations by Wooster Woodruff Beman; republished by Dover, 1963. [DW] Richard Dedekind. Gesammelte mathematische Werke, volume 3. Vieweg, Braunschweig, 1932; edited by Robert Fricke, Emmy Noether and Oystein Ore; republished by Chelsea, New York, 1969. [MD] Michael Dummett, Elements of Intuitionism, Oxford University Press, 2000. [AH] Arend Heyting, Intuitionism, an Introduction, Studies in Logic and the Foundations of Mathematics, North-Holland, 1956. Third edition, 1971. [PJ] Peter Johnstone, Topos Theory, London Mathematical Society Monographs 10, Academic Press, 1977. [AT] Anne Troelstra, Principles of Intuitionism, Lectures presented at the Summer Conference on Intuitionism and Proof Theory (1968) at SUNY at Buffalo, NY, Lecture Notes in Mathematics 95, Springer-Verlag, 1969. [TD] Anne Sjerp Troelstra and Dirk van Dalen, Constructivism in Mathematics, an Introduction, Number 121 and 123 in Studies in Logic and the Foundations of Mathematics, North-Holland, 1988. If you know of other explicit accounts of multiplication for Cauchy or Dedekind reals then please give the references in comments below.