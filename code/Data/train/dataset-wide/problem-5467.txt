Especially in introductory logic textbooks, deductive arguments are usually defined something like this: "the conclusion must be true given the premises" or "it's impossible for the premises to be true and the conclusion false." Then inductive arguments are defined as arguments that aren't deductive. Using this definition, the Miracle-Gro argument counts as inductive, because it's possible for the premises to be true (plant A was fertilized, plant B wasn't, plant A flourished, plant B didn't) and the conclusion false (Miracle-Gro doesn't stimulate plant growth). If you're asking in the context of an introductory logic course, that answer is probably good enough. In the rest of this answer, I'll introduce some limitations for the standard definition of inductive and deductive. The first limitation is that it doesn't distinguish different kinds of inductive arguments. Both enumerative induction (the sun rose yesterday, the sun rose 2 days ago, the sun rose 3 days ago, ..., therefore the sun will rise every day) and argument from analogy (Hypatia was a woman, Hypatia was mortal, Hillary Clinton is a woman, therefore Hillary Clinton is mortal) are not-deductive, and are grouped together as "inductive" by the definition. But there are important and interesting differences between enumerative induction and argument from analogy. A second limitation is that the definition places all bad arguments in the inductive category. Consider a complete non sequitur: Hillary Clinton lost the election, therefore some cats like to eat fish. This doesn't pass the test for deduction, so the definition places it in the inductive category. But it's not even remotely plausible as an argument, which makes it very different from enumerative induction and argument from analogy. A third and related limitation has to do with enthymemes, or arguments with an "implicit premise." Consider this argument: Hillary Clinton received fewer electoral votes than Donald Trump, therefore Hillary Clinton lost the election. Taken on its face, the argument is not deductive, because the premise could be true and the conclusion false (if the US had a different electoral system). But many readers would recognize an implicit premise (roughly, that any candidate who received fewer electoral votes lost the election). Once that premise is stated explicitly, the argument becomes deductive. The problem is that, if implicit premises are allowed, every argument counts as deductive. Take the argument Hillary Clinton lost the election; if Hillary Clinton lost the election then some cats like to eat fish; therefore some cats like to eat fish. This is now a deductive argument (and it's even sound!). A fourth limitation is that the definition depends on modal assumptions; that is, assumptions about what must be the case or what's impossible. Consider the argument that this table has mass, therefore this table cannot travel at the speed of light. Is the argument deductive? It is if general relativity sets the boundaries on what's possible and impossible. But that means general relativity is necessarily true, and we typically think that's not the case (in other words, we typically think general relativity is contingently true). Next consider the argument that I am a bachelor, therefore I am unmarried (a version of one of the standard examples of an "analytic truth"). Whether this argument is deductive depends on whether a certain definition of the term "bachelor" sets boundaries on what must be the case or what's impossible. But it seems like that definition could be contingent — we might have used the term "bachelor" differently. (This limitation also applies to formal logic. $p \& (p \to q) \to q$ is a tautology only because of the definitions of the operators $\&$ and $\to$, and we could have adopted different standard definitions. See Etchemendy, The Concept of Logical Consequence.) Given these limitations of the standard definition, I usually suggest a different approach to my introductory logic students. Good arguments are ones in which the premises provide good reasons to accept the conclusion. Then we can talk about different kinds of arguments (modus tollens, enumerative induction, analogy, and so on) and the supporting and undermining conditions for each kind of argument. So, instead of asking whether the Miracle-Gro argument is inductive or deductive, we focus on asking what makes for a good or bad experimental test of Miracle-Gro. 

Among professional philosophers, "philosophy of computer science" would typically be used to describe philosophical discussions of issues arising from computer science. Major examples here include what it would mean to say that computers are "intelligent", whether so-called "strong" artificial intelligence is possible, the use of computers in mathematics and empirical science, and the social impact of computer technology. There's also the metaphysics of computers and computing. But it seems like your question is about the impact that computer science has had on philosophy. In other words, have philosophers taken up ideas developed in computer science, such as MVC architecture or the "test driven" vs. "event driven" paradigms? Here the answer is generally "no." The idea that cognition is computation (meaning, the manipulation of symbols according to formal, not-inherently-meaningful rules) has been important in philosophy of mind. Some logicians work across the disciplinary boundaries between philosophy, mathematical logic, and computer science. And a few philosophers of science construct agent-based simulations or use computational social science methods (looking at coauthor networks or using natural language processing, for example). But these examples are exceptional. Not many professional philosophers have formal training in computer science, especially computer science that goes beyond the practices of software engineering. Also, many of the ideas developed in computer science have been developed specifically to describe and analyze computational phenomena — the way computers work. Many of these ideas may be too specialized to extrapolate to other kinds of phenomena (e.g., human minds; the link under "cognition is computation" is mostly about whether the idea of computation can be usefully extrapolated from computers to human minds). You might think that I'm wrong here — that many ideas from computer science can indeed be extrapolated to the kinds of phenomena that philosophers are interested in. I don't want to discourage you from exploring the connections. However, I do want to encourage you to find a professional philosopher as a collaborator — someone who is familiar with the literature I've linked too above, and can help you avoid philosophical mistakes. 

The literature on model-based science is extremely relevant to Box's aphorism. I would specifically recommend Wimsatt's "False Models as a Means to Truer Theories", Weisberg's "Forty Years of 'The Strategy'", Parker's "Scientific Models and Adequacy-for-Purpose", and Potochnik's "The Diverse Aims of Science". None of these papers are about statistical models, but I still think they're relevant to the way Box's aphorism plays out in the way scientists and others use statistics. (I'm currently writing a paper arguing that this is because many philosophers of science know relatively little about statistics, and see it as just "curve fitting.") Potochnik argues that idealization plays an important role in the ways models support understanding. One implication of both Box's aphorism and the model-based science literature is that realism/instrumentalism fades away as a problem. Good models don't need to be True, in the strong sense that realists want. But they also can't be black-box prediction machines, like instrumentalists want. (That's a key difference between inferential statistics and machine learning.) In statistics, a good model needs to represent the data generating process "accurately," but there are different aspects to accurate representation (see especially Weisberg), and the standards of accuracy depend on what we're trying to do with the model (Parker). We also care much less about finding the one true model, and instead working through a series of models that get at different aspects of the target system (Wimsatt). 

It seems to me like you're asking about social science methodology, rather than the philosophical use of these terms. Let's start with three important distinctions: 

It's useful to remember that frequentism was rooted in Machian or positivist radical empiricism. On this radical empiricist picture, all we can talk about are series of observations; we can't observe anything like "real chances" or stochastic causal connections, and so we should avoid that kind of language. So, yes, #1, that really is all the most rigorous nineteenth century frequentists like Karl Pearson mean by probability. Later frequentists were less rigorous empiricists, however. For example, Fisher was an indeterminist, and sometimes used stochastic causal connection language. The distribution of coin flip results in X can be described completely by an unobserved parameter rho (which you can interpret as the probability that any one coin flip is heads). For Fisher, rho has a true value, and rho stochastically causes the distribution of coin flip values that we actually observe. By virtue of this causal connection, we can use the percentage of observed flips that are heads to estimate the value of rho. For Karl Pearson, by contrast, this percentage is just a convenient way to summarize the data; since we can't actually observe rho, it's (roughly) meaningless to talk about its "true" value. So #2 is true for an indeterminist frequentist like Fisher. There are "real chances" and stochastic causation, and we can estimate the true values of unobserved stochastic causes using the properties of long-run tendencies of sequences of observations. 

I'd offer this as a comment, but for some reason I can't comment until I have 50 reputation, and right now I only have 41. There might be a relevant implication of the Incompletness Theorems that I'm forgetting, but this sounds to me more like a version of the problem of induction than incompleteness. Spurious references to incompleteness are pretty common; for examples, see the Wikipedia and the Stanford Encyclopedia entries on the Incompleteness Theorems. 

Aristotle argues somewhere in the Nicomachean Ethics (I can't seem to find the exact reference at the moment) that it's pointless trying to teach ethics to someone who doesn't have the virtues at all. If we haven't already learned to be honest in general (and how honesty can come into conflict with other virtues and how to navigate some of the simple cases of those conflicts), then theoretical arguments about the value of honesty in a life well-lived aren't going to change our behavior. Even more — if I'm remembering the argument correctly — Aristotle thinks that we won't even really understand basic ethical concepts like honesty if we aren't already at least somewhat honest. Aristotle's moral psychology is radically different from Kohlberg's scale. (Let me note in passing that there are some important feminist critiques of Kohlberg's scale. See this Stanford Encyclopedia article.) But if it's fair to call possessing an Aristotelean virtue "moral intelligence," then on his view ethics does require moral intelligence. But not everyone is an Aristotelean, so here's a related perspective from my experience teaching Introduction to Philosophy. I've taught Intro at both very prestigious private universities and as night classes at community colleges. As you might expect, the students at the prestigious private universities were overwhelmingly 18-22-year-old white and wealthy or upper-middle-class children of the suburbs. And the students at the community colleges were very diverse in terms of age, race and ethnicity, and class. Many of the community college students were in their 30s or older, worked full-time, had kids, and took night classes to get a degree that would get them a raise at work. My Intro to Philosophy course had a loose theme of deep disagreements, and so we covered a lot of controversial issues — socialism, sexism, racism, mass incarceration, climate change, vaccine skepticism, etc. The students at both kinds of schools would get engaged in our class discussions. But almost none of the privileged young students at the prestigious universities could relate personally to these issues. We were continuing intellectual debates they had in their private high schools. On the other hand, most of the disadvantaged students at the community colleges could relate personally to the issues we were talking about. The philosophers we read were providing them with concepts and theories that helped them understand the challenges they faced in their lives. (I'm a democratic socialist, so for me this was the real point of the class.) Standpoint epistemologists (including but not limited to feminists) argue that the different social positions of knowers have different epistemic consequences. My community college students were often able to understand the philosophy they were reading better than my prestigious university students, because they had been required to learn to navigate the world described and criticized by that philosophy. That intelligence — which you might or might not want to call "moral intelligence" — definitely helped some of my students understand philosophy better. 

One of the major relevant discussions in political philosophy is with "original acquisition" or the appropriation of natural resources, which amounts to taking them out of the commons. For idiosyncratic historical reasons, much of this discussion is framed in terms of various versions of libertarianism; see this Stanford Encyclopedia article. Here are some highlights from the discussion in that article, with quotations in italics followed by my commentary: 

As I read your question, the two methods (counting text features, such as nouns; and expert judgment) differ in their intersubjectivity, or the extent to which two different people using the method in the same circumstances will get the same result. Intersubjectivity doesn't require quantification. For example, pretty much everyone who meets my cat agrees that he's black, but not because we've quantified the radiation he reflects. On the other hand, quantification through reliable measurement can improve intersubjectivity. You don't spend part of every supermarket trip negotiates how many apples you're buying because a (state-certified, reliable) scale simply measures the weight of the apples. Now, the substance of your question is whether intersubjective methods are better than expert judgment. What does "better" mean here? I think here "better" means something like "tends to give a more accurate estimate of the billable hours required to complete projects." By definition, intersubjective methods will be more precise than non-intersubjective methods; but they are not necessarily accurate or true estimators. For example, suppose your method was to count the number of "e"s in the proposal, then multiply by 700 hours. That method would be extremely precise, but probably wildly inaccurate (unless your company does really complex engineering that takes a lot of time.) (A counting method might have other advantages over expert judgment, even if counting is less accurate. The counting method might be easier to adjust — when it turns out that multiplying by 700 hours gives numbers that are way too high, you can modify the formula to multiply by 200 hours instead, and then test whether that works better. The counting method can be implemented in a software tool, and then anyone can use it, so junior engineers would be just as good as senior engineers at estimating the time required for a project. And so on. I'm going to assume all you care about is accuracy, though.) So you can't say a priori (without any empirical evidence) which method is more accurate. But you can do a simple experiment to test and compare the methods. Over the next six months or so, for every project that comes through your company, keep track of the expert estimates, the estimates produced by your counting method, and the actual billable hours needed to complete the project. (Don't tweak your method to improve it while the experiment is running!) This experiment will produce intersubjective evidence that you can use to determine which method is more accurate.