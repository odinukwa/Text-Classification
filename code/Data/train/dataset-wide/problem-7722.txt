You face the curse of dimensionality. Besides the pretty old but simple and robust Monte Carlo integration and its relatives there are also methods based on sparse grids. For an overview see 

Since that question popped up on the screen again I add something to the list (although I am pretty late to the party): The fact that pointwise almost everywhere convergence is not topological is nicely described in 

It is probably worth mentioning that the the heat equation is inherently linked to energy and entropy in two ways: 

Complementing the previous answer and comment: These metric is used in the context of shape optimization problems (in the case of $X$ a subset of $\mathbb{R}^d$ with the Lebesgue measure). In "Shapes and Geometries" by Delfour and Zolesio and you can find several interesting results in that book. E.g. the $L^p$-norm can be used ($1\leq p < \infty$) instead of $L^1$ in Pietro's comment and lead to the same topology, you can approximate any Lebesgue measurable set by $C^\infty$-domains in these metrics, the characteristic functions of convex sets form a closed subset. 

I second Czenek's recommendation. Moreover, it could be helpful to know this cute little characterization of invexity (due to Craven and Grower, see also here): Theorem: A differentiable function $f$ is invex if and only if every stationary point is a global minimum. Proof: Let $f$ be invex and $u$ stationary. Then $\nabla f(u) = 0$ and hence $f(x)-f(u)\geq 0$ for all $x$. Conversely, let every stationary point be globally optimal. Then $$ \eta(x,u) = \begin{cases}0 & \text{if}\ \nabla f(u) = 0\\\\ \frac{f(x)-f(u)}{\|\nabla f(u)\|^2}\nabla f(u) & \text{else}\end{cases} $$ shows that $f$ is invex. Hence, invexity is just another way to say the every stationary point is globally optimal. Since the latter property is fulfilled by quite "arbitrary" functions, one may conclude that invexity does not impose too many structure. 

I have a question about the fine structure of convex functions. Convex functions behave very regular in the interior of their domain of definition (e.g. they are locally Lipschitz continuous there) but otherwise some weird things can happen. My question concerns convex functions that possess at most one subgradient at each point. Let's fix notation: Let $X$ be a Banach space and $J:X\to\mathbb{R}\cup\{\infty\}$ be a convex, extended valued function. Denote by $\newcommand{\dom}{\mathrm{dom}}\dom J = \{x\ :\ J(x)<\infty\}$ and assume that the subdifferential $\partial J$ of $J$ is at most single valued and denote its unique element by $\nabla J(x)$ (if it exists). Moreover, denote the Gâteaux directional derivative at $x$ in direction $h$ by $DJ(x;h)$. My question is: 

I guess, Chapter 2, §1 of "Perturbation Theory of Linear Operators" by Kato will answer your question. 

Yes, there are. First note that the Wasserstein metric is, after discretization, the solution of a linear program (LP) that can be fed into any LP solver. Moreover, there are specialized algorithms, try googling for "Earth-Movers-Distance" (mostly Wasserstein-1). Then is the Benamou-Brennier framework which puts optimal transport for Wasserstein-2 into the framework of fluid-mechanics, see here. Also you may want to look at the numerical method here for the Wasserstein-1 distance. In the end, it really depends, what your goals are, and how you discretize your density functions. 

I hope this question is focused enough - it's not about real problem I have, but to find out if anyone knows about a similar thing. You probably know the Heisenberg uncertainty principle: For any function $g\in L^2(\mathbb{R})$ for which the respective expressions exist it holds that $$ \frac{1}{4}\|g\|_2^4 \leq \int_{\mathbb{R}} |x|^2 |g(x)|^2 dx \int_{\mathbb{R}} |g'(x)|^2 dx. $$ This inequality is not only important in quantum mechanics, but also in signal processing for the short-time Fourier transform, see here. One can derive this by formally using partial integration $$ \int_{\mathbb{R}} 1\,|g(x)|^2 dx = -\int_{\mathbb{R}} x\tfrac{d}{dx}|g(x)|^2dx \leq 2\int_{\mathbb{R}} |xg(x)|\,|g'(x)|dx $$ and Cauchy-Schwarz. Now, changing just the order of the functions, you obtain this inequality $$ \int_{\mathbb{R}} |g(x)|^2 dx \leq 2\int_{\mathbb{R}} |xg'(x)|\,|g(x)|dx \leq \left(\int_{\mathbb{R}} |xg'(x)|^2dx\right)^{1/2}\left(\int_{\mathbb{R}} |g(x)|^2dx\right)^{1/2} $$ which gives $$ \|g\|_2\leq \|xg'\|_2. $$ Ok, this was just playing around. However, this inequality can also be motivated by an abstract consideration about uncertainty principle associated to group-related integral transforms (see my two blog posts). Interestingly, the Heisenberg uncertainty principle derives from the short time Fourier transform and the last "uncertainty principle" derives from the wavelet transform. The last fact bothers me: In contrast to the fact that both inequalities can be derived from two conceptually very different integral transforms (indeed both underlying groups are very different), they have a very similar formal derivation. I have the following questions: Is anyone familiar with the last inequality? Could it be useful in any context? Is there some reason why these inequalities seem so entangled? 

In the applied field there is SIAM Undergraduate Research Online. Although it is mainly for for research papers written by undergraduates they write that 

If $A$ is invertible and $BA^{-1}$ is, by any chance, symmetric positive definite, then you could write $$ \langle B(y-x),\nabla f(Ax)\rangle = \langle BA^{-1}A(y-x),\nabla f(Ax)\rangle $$ and use that $\langle BA^{-1}y,x\rangle =: \langle y,x\rangle_{BA^-1}$ defines an inner product. Use your first inequality in this new inner product and norm to get $$ -\langle B(y-x),\nabla f(Ax)\rangle \leq f(Ax)-f(Ay) + \frac{L}2\|A(y-x)\|_{BA^{-1}}^2. $$ 

Every book which treats dual spaces of normend spaces states that $(c_0)' = \ell^1$ and $(\ell^1)' = \ell^\infty$ and some also describe $(\ell^\infty)'$. However, is anything known about higher order duals in general? Does taking the normed dual of a given normed space $X$ stabilize? In other words: Defining $X^{(1)} = X'$ and $X^{(n)} = (X^{(n-1)})'$. Is there always an integer $n$ such that $X^{(n+1)} = X^{(n)}$? 

I find it a mayor drawback of Google Scholar that it also finds things which are not publications at all (e.g. sometimes it finds talks which end with a bibliography page). Another drawback of GS is that it gets confused by some preprint-series I know, namely the ones which attach the list of all the preprints in that series. In these cases GS counts these as citations which is just wrong. However, I find GS helpful and use it frequently but I would not consider its output reliable. (Another related issue is, that GS sometimes identifies weired things as the author, e.g. try to find papers with "Mathematik" as author...) By the way: Wouldn't it be a great feature of both GS and MathSciNet to give snippets of the places in which a paper is cited? Imagine, if you click on the "cited by"-button and see a list of paragraphs or sentences is which the citation happens... 

The book "De wilde getallen" by Philibert Schogt ("The wild numbers") is a great story about a young mathematician and his struggle with an (imaginary) theorem in number theory. It illustrates the emotional rollercoaster one sometimes goes through while trying to prove theorems. 

I suspect that you mean $0 < t < 1$ in your question. Then the answer in still no under the added condition that both matrices are not of full rank. Consider $$ A_1 = \begin{pmatrix}1 & 0 & 0\\\\ 0 & 0 & 0\\\\ 0 & 0 & -1\end{pmatrix}\quad A_2 = \begin{pmatrix}0 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & -1\end{pmatrix}. $$ 

I haven't seen translation of the five other "Mitteilung" under the same title (but I haven't seen version of the second, third and sixth Mitteilung even in German…). The other two papers in the pdf are Fredholm's "On a Class of Functional Equations" and Schmidt's "On the Theory of Linear and Nonlinear Integral Equations. Part I: The Expansion of Arbitrary Functions by Prescribed Systems." 

You have discovered the alias effect! For the discrete Fourier transform all this is worked out in detail - for the discrete Hartley transform, I don't know... 

It makes a considerable difference if you need to project a vector just once or repeatedly inside some loop of another algorithm. If you would be in the latter case than it would be indeed a good idea to follow suv...rits suggestion and use an iterative algorithm to solve the normal equations (conjugate gradients, probably with preconditioning, suggest themselves). In this case you can often just use a small and fixed number of CG iterations and restart the next CG iteration with what you've got from the previous run. If the point you want to project does not change very much in the outer loop, CG should be converging eventually. However, your EDIT2 suggests that your only project you data once. Without knowing your precise approximation problem, it could be possible to make a reformulation of the problem: If $y$ is your signal but you need to put $z = A(A^T A)^{-1}A^Ty$ in your algorithm, you could just put in $z$ and add the constraints $z = Aw$ and $A^T Aw = A^T y$, i.e. you introduce two new variables and two new linear constraints. Then use some algorithm for your new inversion which can handle linear constraints. Without knowing more details I can't be more concrete here… 

which can be obtained via the website of Heinz Bauschke. It's almost 20 years old, but even then a lot was known. You may also check the book 

I do not think that this can be converted to a convex problem: There are multiple solutions in general and these do not form a convex set: If $c>0$ and $x^*$ is a solution, then $-x^*$ is also one. However, $0$ is convex combination of these solutions, but not optimal. In principle there is the convex relaxation of the problem, which means to replace the objective function by the largest convex function below the objective, but usually this is even more difficult to compute than solving the original problem. Note that $c\leq 0$ leads to the trivial solution $x^*=0$, so you can assume that $c>0$ If you could solve $$ \max\{|Ax|_1\mid |x|_2\leq b\} $$ with some solution $\bar x$ and observe that the objective $|A\bar x|$ is smaller than $c$, then any solution $\bar x$ of the above problem would be also a solution of the original problem. If $|A\bar x|_2>c$, then find $0<\lambda<1$ such that $c = |A\lambda\bar x|_1 = \lambda|A\bar x|_1$, to get an optimal solution. It remains to solve the maximization problem (convex function over a convex domain, so the maximum is assumed at the boundary) and this is essentially the computation of the matrix norm of $A$ as a mapping from a space with $\ell^2$-norm to a space with $\ell^1$-norm, denoted $$ \|A\|_{2\to1} = \max\{|Ax|_1\mid |x|_2\leq 1\}. $$ I vaguely remember that this norm (as well as most other mixed induced norms) is hard to compute but can't find a reference right now. 

So $\Gamma(H)$ are all functions that are pointwise suprema of continuous affine functions. This is a neat space since it's elements are build from very simple objects, but is also quite general as it includes all convex and lower semi-continuous functions. However, it includes two not-so-nice functions, namely $f(x)\equiv \infty$ and $f(x)\equiv -\infty$. Both are not really needed and complicate the theory a bit, hence $\Gamma_0(H)$ is introduced which is just $\Gamma(H)$ without these two functions (and adding a subscript $0$ to specialize a set seems pretty common). Also, one can say a bit more about the letter $\Gamma$. I don't think its origin has something to do with projections. I checked Fenchel's lecture notes on convexity from 1951. There he did not use convex function that were allowed to assume the value $+\infty$, but always includes the domain of definition into the description of the convex functions, i.e. he wrote $[C,f]$ to denote a convex function $f:C\to{}]{-\infty},+\infty[$. In this lecture notes he describes dualities between convex sets (Chapter II, Sec. 8) and also between functions (Chapter III, Sec. 5). For the duality of sets he used $C$ for sets of points and $\Gamma$ for sets of hyperplanes and then describes the duality of the two descriptions convexity by either 1) the points in the set or 2) by the hyperplanes that separate the set from outer points. In III.8 where he describes Fenchel duality he used $[C,f]$ for "convex $f$ defined on $C$" and denotes the dual function as $[\Gamma,\varphi] = [C,f]^*$. Here $\Gamma$ somehow describes (a part of) the affine functions minorizing $f$ which may be the motivation for Moreau to use $\Gamma$ for the set. 

Microlocal analysis is used in computed tomography and other tomographic imaging techniques e.g. in medicine . Specifically, it is used to describe which wavefront sets (here: boundaries of objects, e.g. of organs in the human body) can or can not be detected by a specific tomographic measurement setup and also helps to understand reconstruction artifacts and develop strategies to overcome these. Details can be found for example in 

Not an answer but too many equations for a comment: $\newcommand{\vec}{\mathrm{vec}}$ To compute an element $v\in\mathbb{R}^{3n^2}$ of the kernel of $$M = \begin{bmatrix} A\otimes I\\\\ I\otimes B\end{bmatrix}$$ we use the formula $\vec(ABC) = (C^T\otimes A)\vec(B)$ which holds whenever the products are defined. Now we can write $v\in\mathbb{R}^{3n^2}$ as both $v=\vec(V)$ with $V\in\mathbb{R}^{n\times 3n}$ (stacking the $3n$ column vectors of length $n$ on top of each other) and $v= \vec(U)$ with $U\in\mathbb{R}^{3n\times n}$ (stacking the $n$ column vectors of length $3n$ on top of each other). We write $$Mv = \begin{bmatrix} (A\otimes I)\vec(V)\\\\ (I\otimes B)\vec(U)\end{bmatrix} = \begin{bmatrix} \vec(VA^T)\\\\ \vec(BU)\end{bmatrix}.$$ Hence, the vectors $v=\vec(V)=\vec(U)$ in the kernel of $M$ are characterized by $$AV^T = 0,\qquad BU = 0.$$ Note that $V$ and $U$ are related by a reshape of their elements. Looks like this could be exloited algorithmically but I did not see yet how... 

I suspect both numerical issues and issues with the random number generator to be at work simulaneously. I also get crazy values (i.e. very large ones and also some structure as shown above) for the coefficients even when I put the zeros at the $N$-th root of unity. This is seen better, the larger $N$. This suggests that the calculation of the coefficients from the roots is performed in a numerically unstable way. My pictures of coefficients did not change qualitatively when I changed the random number generator. However, I observed a different outcome when I permuted the roots randomly and then calculated the coefficients again. They should be the same, but they were not. Moreover, the pictures looked qualitatively different. This suggests again that the computation of the coefficients is unstable and also that "permuting random coefficients randomly gives another type of random". For example with