And this returns: 32 How can that value actually be contextualized? Is it an "average of 32 batch requests per second in the last minutes"? Or is it "in the last second there were 32 batch requests" 

You can not perform a downgrade in the same way SQL Server allows you to do an in-place upgrade, so you're stuck with having to manually move your database(s) between two installs. If you've got a named instance, you'll need to back it up, perform an uninstall of the Enterprise edition, and a fresh install of Standard - restoring the databases thereafter. Of course that's a basic theory, and it's not always that simple. Are you using any Enterprise specific features, such as partitioning, snapshots, some aspects of Always On Availability Groups, Resource / IO governance, compression...? You can see the full feature list by edition here here You can also run the following query to check for any version specific features you are currently using: 

Here's an example which does what you want and will get you started with using PIVOT. Sorry it is a bit rushed, but hopefully it will get you started and show you how it can be done fairly easily. There will be limitations and I would fully advise researching and playing with the pivot functions because they are really powerful. 

Firstly, forgive my poor question title. I couldn't think of the best way to summarise this. Essentially I'm putting together some scenario based DR documentation, with this specific scenario being a SAN issue where we have lost disks which hold either log or data files. I'm assuming that MASTER has been affected (ie. the drive where it's files exist isn't accessible) and SQL services won't therefore start. My specific questions are as follows: 

I have a weird problem I am struggling to troubleshoot. I have a development server with 18GB RAM and two SQL Server 2012 SP3 instances with @@version output: 

I have a SQL Server (2014 SP2) with a linked server to an Oracle 11 database. I have a very simple select statement to an Oracle view which I know should return around 140k rows. But here's the thing, when I run it in SQL Server I immediately see records in the results window, but all of a sudden, the query hangs when it has so far returned only around 2000 rows and it just sits there forever doing nothing else. Sometimes, another few thousands rows appear before it again hangs. The wait state on the query is on OLEDB which is to be expected. I'm thinking some sort of Network bottleneck...? Sometimes I do get an error after a while: Cannot fetch a row from OLE DB provider "OraOLEDB.Oracle" Before anyone suggests it, 'allow in process' is ticked on the provider settings. I'm looking for ideas to troubleshoot this at either the SQL Server side or the Oracle side to check that it's not a database config issue. 

FRA will be purged automatically when free space is needed Put the archived logs in FRA as well Use RMAN to make the backups and use RMAN commands (REPORT OBSOLETE / DELETE OBSOLETE) to manage the purging of backup pieces + archived logs from FRA 

This can and will work (theoretically :) ...), but you will need a recursive function (in application or in the database) that can return the parent group id (ex. B-1), or the top group id (ex. A-1) for any group id value given as an input parameter (C-1). But ... like I said: The more info you give, the better responses you get. Hope that helps. 

Replication was made for small changes, so you're on the right track. To keep the systems in sync, at the database level, you will need to setup a mysql master-master replication scenario. I've used the tutorial from this linke ($URL$ to setup such configuration. Depending on how much data is changed, you can choose between [statement] or [row-based] replication. More info about the differences between the 2 types: $URL$ 

... And so on. Best Bet: Partitioning the existing table by [station_id] and [created], you will have "A" partitions for each station, "B" partitions for each month, and a total of AxB posible number of partitions. Once you partition Table_A, do the same thing for Table_A_Archive, and on the end of each year, move the data from Table_A to Table_A_Archive. ** IMPORTANT:** After you make the partitioning schema, keep in mind that all queries should have in the WHERE clause the conditions necesarly so that the query will hit as little partitions as posible. Ex. 

Why not recreating the primary key as (nid, vid), and creating a new index on the column "order" (just for fast retrieval / ORDER BY clause). --> The worst that can happen is to have 2 ingredients with the same "order" value, but the order-change-logic should be already correctly defined in the application. 

you can partition the table by certain criteria. Because your using the table for a "weather station", and your data is time series values, your best option would be to partition by [station_id] then by [created] Create a Table_A_Archive, where you can move data that would be to old to keep in Table_A. How long do you intend to keep data in Table_A ? would it make sense to delete old rows that become obsolete for you application 

At the moment I have around 125 production instances, each with a script-based maintenance plan running as an agent job. The tasks run are Index Reorg/Rebuild, Stats updates and Checkdb. Backups are looked after by Netbackup so they dont form part of the plans but for a couple of exceptions. I moved all the instances last year to script-based maintenance plans from plans created with the SSMS wizard (hate those) and they're efficient and effective so overall I'm pleased. I'm wondering whether it's feasible to take things a little further. I've recently been working on a powershell script that, when pointed at an instance, iterates through the databases on that instance and performs those three tasks on demand. My question is whether anyone can see any downside by doing this for all instances, I.e. Having a single powershell script on our DBA server that iterates through a list of instances on a windows schedule and executes the maintenance tasks. Any errors would be handled / written out to logs etc. The main benefit of this in my eyes that we won't be deploying mp jobs to new instances and configuring schedules. We will just be adding the name of any new instance to the instances the script must iterate through. I'd welcome your thoughts. 

I've never tried this but I'm reliably informed it is 100% not possible. It is certainly not supported by Microsoft, for good reason, so why would you do it? 

This article gives all the filter arguments and available columns including the code. I use it often as a reference to create server-side traces. 

I've just inherited about 20 instances of SQL Server, as part of a wider acquisition project. I'm in the process of assessing performance and I don't like the way maintenance plans have been implemented. I'm seeing daily blanket index rebuilds (I can deal with this one) and also daily manual updating of statistics. Around half of the databases have been set to Auto Update Statistics = False, for reasons which are not clear other than I am told it is to reduce 'Performance Issues'... I always thought, and worked to, best practice of setting this to True and felt the Manual Update was not necessary if this setting was True. Am I wrong? Can anyone explain what the benefit would be in having this set as False, but doing a daily manual update instead? I should mention that some of the databases are highly transactional (millions of Inserts, Deletes, Updates per day) Others are low in terms of transaction rates, and some are all but read-only. There is no rhyme or reason though as to which have the Auto Update setting set to False. It appears to be a lottery. 

(Obviously -E is for window Authentication) This connects to the SQL Server. But I often find I want to connect to an Analysis Server instance instead. Can anyone tell me the command line option please? 

My problem is that Iâ€™m trying to understand where to best put the TempDB and OS and the Log. My experience is limited in optimal configuration of these two This is not an online transactional system. It has heavy data write (new data + indexes rebuild/reorg) then heavy data read (I'm estimating at about 50/50) processing for about 13 hours, and then just quiet. My understanding is that the TEMPDB is heavily used during normal processing compared to the log. My idea is the following 

I can connect SQL Server Management Studio to a SQL DB by passing command line parameters like this: 

Does this sound like a good idea? I could then swap the Log + TempDB if needed. Am I breaking a cardinal rules like never put TempDB on OS disk due to paging concerns, or perhaps never put log on slower disk than data? Edit: We also have a SSAS on the system and the end users access only the Cube. The 50% read above is based on the time it takes to process SSAS database. 

So from a logical perspective I believe allow SQL to take as much as it needs, but I'm not so sure about AS' . I'm not sure if AS will relinquish it's memory. In fact reading more leads me to believe that it is wrong to let it take it all. Does this mean that I need actually set limits for both? I'm confused as to what the best practises should be, and what we need to be measuring considering the processes don't overlap. Hope this makes sense. 

So when I run the job it fails with the message that NT AUTHORITY\SYSTEM wasn't allowed to proxy 1 (I only have one credential) for CmdExec. 

Each night the SQL Server will do about 2-3 hours processing, followed by 2-3 hours of AS processing. Then throughout the day only the AS are queried. Assuming this is a dedicated server, and no other apps are of concern, and that the two sets of processing are completely synchronous - no overlaps always one after the other - how can I best set the SQL and AS server memory limits. The reason for asking is that if I don't set a limit for SQL it will grab all the memory it can. However - my understanding is that SQL will happily relinquish this memory if: