One option is that under structuralism, where language is reducible to a formal system, language is 'dead'. From Paul Ricoeur: 

I suggest taking a gander at Edouard Machery's 2005 Concepts Are Not a Natural Kind; he wrote a 2011 book in this strain of thought: Doing without Concepts. I took a "Philosophy of Neuroscience" course with Machery; I wrote a final essay Concepts Contain a Natural Kind, and he graded it well (this mostly gives you a tidbit about his character). Fauconnier and Turner's The Way We Think: Conceptual Blending And The Mind's Hidden Complexities looks very interesting to me; WP's Conceptual blending has a bit of extra content. Grossberg's 1999 The Link between Brain Learning, Attention, and Consciousness provides a possible model of consciousness, which offers a way to separate between concepts and percepts. 

Furthermore, the idea that religion is merely a linear combination of { explanation, control } is not clearly supported by the evidence. For example, is the following 'control': 

Who is "rational" and who is "mad" is decided by the powers that be; it is not decided based on 'Reason' or anything like that. Part of Feyerabend's response to philosophers and scientists attempting to declare him mad was to write The Tyranny of Science. The editor introduces the work: 

Whether or not this is cruel depends on whether preventing a person from becoming what he/she could have become is considered unjust. Do humans have natural teloi which are evil to thwart? Aristotle certainly thought so; see Teleological Notions in Biology. However, the current trend is to deny ontic status to teleology; this started with Francis Bacon's rejection of final causes as relevant to the conducting of science and can now be seen by the existence of the word teleonomy. My own argument follows that of Alasdair MacIntyre in his After Virtue: morality doesn't make sense without teleology. I reject the idea that the cruelty from Putnam's example is only cruelty because society is injured; I say that the young person himself/herself is injured; his or her opportunity to thrive in life was damaged. On this basis: Yes, I argue that said discrimination is likewise unjust (and it is "truly discrimination"). P.S. There is room for discussing the tension between individual rights and 'societal' rights; for such a discussion I would refer the reader to Nicholas Wolterstorff's Justice: Rights and Wrongs, which works through the history of how 'justice' has been conceived, moving from justice as "right order in society" to justice as "fulfilling individual rights". 

Practical reason is the ability to form long-term goals and then successfully achieve them. So, it would seem that we aren't reliable when we are without our emotions! Furthermore, the animus toward emotion which I have identified and documented has very likely actively thwarted research into your precise questions. Now, given that Descartes' Error has so many citations and is now twenty years old, hopefully there is more and more research on your question. Indeed, looking at the citations may guide you to good work. For example, I found Rosalind Picard's 2000 Affective Computing (5500 'citations', Wikipedia article): 

I suggest looking at the von Neumann–Wigner interpretation interpretation of quantum mechanics, also known as "consciousness causes collapse". A key section in that article is: 

Quantifying whether someone is happy is not the same as being able to stimulate the happy feelings. The experience machine is critical to your question: 

has total ordering ? having zero is important now a ring now a field Dedekind-complete ordering linear ordering is lost commutativity is lost associativity is lost alternativity is lost 

Regardless of right vs. left mythology, human psychology does seem to break down into the above two natural kinds—at least, the above two categories form helpful extremes. Now, many modern scientists tend to accept philosophy that is, or is derivative of, Logical Positivism. Today people tend to call themselves 'naturalists' or 'empiricists' or 'physicalists' ('materialist' has gone out of fashion, given the preeminence of thinking in terms of energy in physics today), and while only LP strictly entails being 'tough-minded', many seem to be nonetheless. An example can be found in the article Heretic: Who is Thomas Nagel and why are so many of his fellow academics condemning him? Roughly, rationalistic philosophy values coherence and beauty over specific matching of reality in all respects, while empiricist philosophy values correspondence with reality over coherence and unity. This needs verifying, but I suspect that rationalistic philosophy has not been credited with much advancing of science in the last several hundred years, while empiricist philosophy has. Massimo Pigliucci suggests this might be a problem: 

Check out U. Wash David Levy's Google Tech Talk No Time to Think (pdf). I especially suggest a book to which he refers, Josef Pieper's Leisure: The Basis of Culture. Pieper warns of the notion of "total work", which I think connects to your idea of "mental slavery". Pieper wrote the German form of Leisure in Germany in 1948, during reconstruction. He was worried that the Germans would get locked into a world of "total work", and have no time for what he calls "leisure"—which is not to be confused with popping open a beer, plopping down on the couch, and turning on one's favorite professional sports. He anticipated the objection that the Germans needed to work really hard now, and could enjoy their fruits—including leisure—after they had finished rebuilding. Allan Bloom describes this as having happened to the West—specifically, America—in The Closing of the American Mind: 

No. Creating a perfect object would be akin to creating a universal—like a perfect circle. But there is no way to create a perfect circle; if you zoom in enough to any actual circle, you'd find bumpiness. A fun description of near perfection is the design of Gravity Probe B. They were able to create spheres 1.5" across with surface never further than 40 atomic layers from a perfect sphere. Close, but not perfect. When it comes to physical creations, we would ultimately be thwarted by Heisenberg's uncertainty principle: there's just inherent 'noise' when you zoom in far enough. Unless you define perfection to fit this kind of noise, humans will never be able to create it. When it comes to formal systems—creations of pure thought—we will likely forever by plagued by Gödel's incompleteness theorems, which show that once we've accepted basic arithmetic in our formal system, it won't be possible to prove the system consistent and complete. Stated differently, no set of axioms that we can discover will be able to prove every true statement. There is no perfect set of axioms, contra Hilbert's Program. 

Since empiricism and positivism are both false I don't see the problem here. Empriricism and positivism are both unavoidably ambiguous about what exists in reality because they do not and cannot have any clear explanation of what counts as an observation. For example, nobody can currently observe the core of the sun and maybe nobody ever will. So does the core of the sun exist? Also, nobody has ever seen a dinosaur, only fossils, so do dinosaurs exist? Empiricism and positivism both treat observation as a primitive: they are raw material to invent and prove ideas, but this makes no sense. Real observations have to be conducted by setting up a suitable physical system using an explanation of how the system works and it is adjusted until it is in a working state using that same explanation. Explanation is a result of a complicated chain of reasoning and can be used to test ideas but it can't provide you with ideas or prove them. The important issue is that the existence of such information is a part of the only existing solutions to problems. See "Objective Knowledge" by Popper chapter 1 and "The Fabric of Reality" by David Deutsch, chapters 3 and 7 and "The Beginning of Infinity" by Deutsch, Chapters 1 and 2. There is another point of confusion in your post. The wikipedia definition of knowledge, which I think is the same as the standard philosophical view, is wrong. The idea is that information is distinguished from knowledge by conscious observation. This view totally neglects what is actually remarkable about human knowledge: it is highly adapted to solve problems. For example, the computer on which I am typing this is set up in such a way that if you make a change to it without understanding it you will break it. For example, if you change the design of its CPU or hard drive or any other piece of equipment without detailed knowledge of how it works it will turn into a useless piece of junk. But lots of non-human systems that existed long before people have the same property, e.g. - the chemical machinery in cells, the way eyes are put together and so on. So there has to be an explanation of this adaptation that applies not just to human knowledge but to the complexity in non-conscious systems like amoebae. The explanation is that both human knowledge and biological complexity arise by processes that involve variation and selection. It's appropriate to think of knowledge as being the property of being well adapted to solve problems, rather than thinking of it s well adapted information that happens to be in a human brain. For this reason and others Popper holds a theory of objective knowledge, much of which is not known by anybody. See "Objective Knowledge" by Popper and "The Beginning of Infinity" by Deutsch, Chapter 4. Philosophers like to say subjective theories of knowledge are common sense, but they're wrong. By the subjective knowledge theory, any book or computer program or design of a complicated piece of equipment doesn't count as knowledge unless somebody happens to have memorised all the details. 

Any physical system can be simulated by a universal computer. For some purposes the computer in question would have to be a quantum computer. However, the human brain is a wet, warm system in contact with the environment and on the timescale on which though takes place it won't exhibit any distinctively quantum mechanical effects like interference or entanglement. So the human brain can be simulated by a classical computer and the Turing machine can simulate any classical computer. The sense in which the Turing machine can simulate a system not not just that the initial and final states of the computation are the same. The computer can be set up in such a way that there is a mapping between the states of the computer and the way information flows within the system while it is computing the result. For any pattern of information flow in the brain while it is instantiating consciousness, that pattern could be instantiated in some function of the Turing machine's tape. For reasons that I think Dennett explains it might not be a good idea to simulate the brain in that way. The brain's architecture looks nothing like a Turing machine and there is no particular reason to translate what it is doing so that it can be run by the Turing machine if some other architecture would work faster and use less memory, e.g. - a network of computational gates. You say 

This paper takes for granted that the right way to count instances of yourself is to count the number of simulations. For a start, it doesn't explain what counts as a simulation. My brain may represent information about my thoughts in a highly redundant way for the purposes of error correction because brain hardware is noisy. A simulation of me on less error prone hardware might represent my thoughts less redundantly. Should I then say I am more likely to be the real me rather than a simulation? My point isn't that one way is right, and the other is wrong, but that we have no argument for deciding between them. Depending on the relevant way of counting instances we may continue to exist indefinitely far into the future. 

We should apply the same moral standards to the aliens as to human beings. Dropping bombs might be a reasonable way to wage a war even if it sometimes kill civilians. Raping aliens wouldn't help win a war and would be evil and stupid. 

If this is all a description of Krauss's position, and it is accurate, then he is wrong. Induction is an alleged process that starts with observations, derives a theory from them and then proves that theory or makes it more probable by doing more observations. None of the alleged steps in this process can be carried out in reality. First, you can't start by observing stuff since some things you could observe will be irrelevant. For example, there may be a particular number of atoms in the pen sitting on my desk, but it's not worth observing how many since it has no relevance to anything important. Second, there is no way that any theory can be derived from observations. For any given set of observations, multiple theories are compatible with it and so no particular theory follows from them. For example, both general relativity and Newton's theory of gravity accurately predicted many of the observations made before the 19th century, but they are not the same theory. Third, any particular theory is either true or false and we have no way even in principle of showing a theory is true for two reasons. (1) Many theories make predictions about a whole class of situations, e.g. - theories about the behaviour of some particular type of star, say. If you misunderstand those stars, then something you find works for one star or 100 or 1 million might not work for others. (2) All observations use explanations. These include explanations of how the observation itself works, e.g. - explanations of the optics of a telescope. Another important issue is the link between the observation and the theory. Under what conditions does the theory predict X and are those conditions realised in your experiment? As such, whether your interpretation of the observation is correct is dependent on other ideas that may be true or false. If you want to show the theory true you would have to prove all the theories you rely on to understand the observation, and then you would have to prove all the theories those rely on and so on. This process would never end and so you would never create any knowledge. In reality, scientists create knowledge as discussed by Karl Popper. The scientist notices problems, proposes ideas about how to solve those problems, criticises the proposals until only one is left and it has no remaining problems (the solution), and then moves on to a new problem. The criticism can include experimental testing, but is not limited to such testing, and a theory may not be tested if it is inconsistent or has some other flaw. All knowledge is guesswork that has survived criticism and nothing else. If a theory has survived criticism, it might be true. If it has not been refuted, then the only guess available at that time is that it is false. Any theory, including a theory about what has been observed, can be reconsidered if you can some up with an alternative that has some implication that could be criticised independently of the problem is was intended to solve. See "Logic of Scientific Discovery" by Popper, Chapters 1-5, "Realism and the Aim of Science" by Popper, especially Chapter I. See also "The Fabric of Reality" by David Deutsch Chapters 1,3,7 and "The Beginning of Infinity" by David Deutsch, chapters 1,2,4,13.