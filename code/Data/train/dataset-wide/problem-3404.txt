Well, it's not perfect, but it's good enough for most needs, il particular if you use the latest version and mandate network-level authentication. You can make it even more secure by adding a gateway server that will tunnel connections through SSL and protect your internal machines. Further refinement would be to use certificate authentication for the session but that's probably unnecessary. As ever, though, the real question should be: what are you trying to protect and against what risk ? If you're going to RDP into the domain controller of your company, then you probably should invest into a few additional security measures (like the gateway server I mentioned) simply because causing a DOS on a DC can have real consequences fr your business. If you're just thinking of connecting to your home computer from the net (and already have descent security in place: good enough passwords and up-to-date OS), then it's probably not worth your while to secure it any further. 

To test your setup, you can use a tool like fiddler (or simply wireshark) from a client that is outside your network. 

I'm not sure exactly what your question is so feel free to update it if that is not the answer you're looking for. If you're asking whether logging into a compromised server with a public key will the private key on the client to be compromised then the answer is no: the private key always stay on your machine and is only used to sign the authentication token you sent to the server. That doesn't means it has no consequences, though: once you're logged on into a compromised machine, everything you do during this session is potentially compromised as well. for instance, if you use a private key stored on that server to access another server, that key will potentially be compromised . In fact, if you perform any other kind of login from within the SSH session, then the credentials used are potentially compromised. 

The correct question would be: why exactly does your service needs to be run as system ? Basically, unless you either build some kind of RPC proxy or change the program to be able to impersonate a domain user, you simply can't grant access to domain resources to the system account. So, please try the following: download and run process monitor (procmon), set it up to register the activity of all processes involved and run them as local user. After this, check the log for any access denied (error code 5) result and come back with a result. At that point, we can start thinking about how to fix it through ACL changes or work around it. 

Segregation of duties and change control. Segregation of duties means that you never have a single person responsible for a single task. The best way to implement it is to make sure no single person CAN perform a single task alone. As for change control, to properly implement it you have to make sure all changes to production systems are fully documented. The best way to achieve it is to have the dev perform a handout to the system administrators, giving them the code and the documentation and letting them perform the deployment. 

I have a Windows 2008R2 server that experience high CPU load for several minutes about once every couple of weeks. Unfortunately, when these events occurs, logging into the system through RDP or at the console takes so long that, when it's done, the problem is usually gone. So, I've prepared a perfmon data collector that will capture the relevant data (CPU time per process, details of IIS worker processes, etc.) but I have no idea how to trigger it automatically under high-CPU load condition (as defined in Nagios: that's >99% CPU usage for more than 300 seconds). Unfortunately, the low frequency of these events makes it difficult to let the data collector run on its own. I have currently worked around it by using circular logging and leaving it running but that's not really a satisfactory solution. So, is there a (simple) way to get that data collector started under these conditions ? 

Routing only happens between two (or more) different networks. If you have only one NIC, then either your have a local virtual segment or you're using another kind of interface to connect to another network. Your network diagram doesn't make a whole lot of sense either: you should have (at least) two interface per router, not one. Could it be that you actually have bridged your "internet router" and your "linux router" ? That would explain it, then. Also, what's the internal IP range looks like ? is it a routable IP range or do you need NAT ? 

No. IIS is a web server, not a proxy server. There is a module for it that can be used to create a reverse proxy, but that's about it. Check out ISA if you want a proxy server from Microsoft. 

If you see that the TLS channel has been successfully negociated, then you'll have to gring in the big guns: grab the server's private key and decode the TLS traffic. That will only be possible, however, if the TLS connection does not use DH key exchange so first, disable that in your server or client. Once DH has been deactivated and you have the private key, you can follow the instructions on the Wireshark's wiki page about SSL/TLS which explains the process in quite a bit of details or this blog post that is perhaps a bit easier to follow. 

It depends on how, exactly, the application interacts with the file and the user. The root question is: "Is there a way to use a different security principal that the one from the user login session to access the file" ? If the answer is "no", then you can't restrict it. More pragmatically: If the application is a traditional desktop application used either remotely (application running on the client's computer accessing the file through a SMB share) or locally (for instance through Terminal Services), then no, you usually cannot restrict the access to the file using permissions because the process uses the user's identity to access it. If the application is a n-tier application (for instance, a web app), then you usually can put extra restriction on data file access: you restrict the rights of the users and allow whatever level of access is necessary to the security principal that will be used (that's usually done by assigning a specific service service account to the relevant process). Be mindful, however, of the fact that it is possible for a n-tier application to impersonate the user access token. In such a case, you should make sure that the files you want to protect are, in fact, not available though SMB at all (for instance, by placing them into a drive or folder that is not accessible from any SMB share available to the users). Note that, regardless of what the answer to the above it, if the application supports UNC path, you could make it less likely for the user to access the file by hiding it under a hidden network share. All you need to do to hide the share is to add a dollar sign ($) at the end of it's name. It will not prevent anyone but the less tech-savvy users from acessing the file should they want it but it might prevent accidents. 

I swear this has been answered several times already. You need to add two pieces of software to IIS to use it as a reverse proxy: URL rewrite and application request routing (ARR). Both can be downloaded from the IIS web site. The documentation to get you started can be found there. 

For more information, I suggest you start with the wikipedia article about the DNS system and, if you're really interested, go on with reading the first few of the numerous RFCs related to DNS (listed in the the wikipedia article but the most interesting ones are probably 1034, 1035 and 1591. 

(Just pass the dump file as parameter to the script. You'll need to adjust the path to match your system). 

Dealing with these types of problems can be difficult because the logs usually tell you only part of what is happening. Your best bet at this point is to use a packet capture and analysis software (like the most excellent - and free - wireshark) to see what is really going on the wire. Make sure you capture all the traffic between your machine and the remote system and try to see what difference you can spot between a working connection and your code. That might give you an indication where the problem lies. Do pay special attention to the way the TLS connection is negociated because it might point out to the actual issue (there is a pretty nice and simple to understand document that can be found at IBM's). For instance: 

RFC 2821's "VRFY" SMTP command is supposed to do that but, due to spammers abusing this command for validating mail addresses lists it's often disabled on the servers and SMTP gateways. 

There are many tools available to help you track down issues like this. Start with the performance monitor which is provided with your OS. You can set it up to take periodic snapshot of pretty much any metric you can think of, including individual memory usage per process. In your case, that's probably the best tool for identifying rogue processes. Also process explorer, which you'll have to download, is a very good way if looking into all processes of your system. It will allow you explore your system state in a very detailed way. Finally, you can use process monitor, also a separate download, to watch a particular process activity in detail. 

Do you still have access to the old AD DCs ? I came across a tool that claim it can recover an AD from a non-bootable machine assuming your SYSVOL folder is still around: $URL$ Waring: I haven't tried that utility at all so I do not imply anything - one way or another - about its quality. 

If you have UAC enabled, you need to copy the files from a privileged process. Personally, I typically do not use the sysvol tree to store login scripts: a simple shared directory with properly configured ACL usually serves me much better. 

Simplest way to work around these kind of issues is to use a content delivery network (CDN). Just host your downloadable content with the CDN of your choice and let them handle the load. Otherwise, there are DDoS protection services that can protect your whole web site. 

Generally speaking, no, SSL performance is not linked to the price (or quality) of the certificate. One specific exception that needs to be made is the CRL/OCSP distribution point that should be included with the cert. This is the property that indicates to the client where it should go to check if the certificate has been revoked or not. If that option is enabled on the client (which is not the usual default), then in order to accept the certificate for the first time a separate request will have to be make to the OCSP server or CRL distribution point. Sine these are controlled by the CA, a poorly performing server will impact your users. However, since this is a) not generally enabled and 2) only happens for the first connection and until the cache expires (typically, 24 hours) it shouldn't have much of an impact on your own web site performance. One thing you should keep in mind, however, is that enabling SSL - no matter what CA you use - will have a rather large impact on performance: it will bypass many caching mechanism (including many CDNs), will break load balancing to some extends and require far more CPU and memory resource per users from your server. For a server with low user count like yours, however, it shouldn't have much of an impact unless your system is badly undersized. 

Find out the IP address of the machine you want to redirect to Edit the file (yes, there is no extension) with any text editor. You might need to start the editor with a high security token (run as administrator" if UAC is enabled Add a line to the host file that read: 

Note that both operation can be easily scripted so you can export a list of DBs from the first server and re-import it in the second one. However, you'll have to rebuild a number of element for the import to work perfectly: pretty much all DB logons will have to be rebuild.