Although you can bring it up on other builds by installing the relevant libraries - a process well known to people bringing Oracle up on Linux. 

The only significant reference to the notation I've ever read is the Elmasri and Navathe textbook - EER gets taught in database papers from time to time, but I don't think it's widely used in practice.. I'm not aware of any data modelling tools with support for the notation. 

Not out of the box as far as I am aware, and if you need to distribute reports to external parties this is a bad idea as they will all have to install third party encryption software (incurring licensing and support costs). It will also put your help desk people into the job of babysitting all the users as they forget their keys. You're much better off providing a secure portal (this can be done with Sharepoint if you have this) and emailing out links to the reports. 

So, as an opinion, use timestamping or versioning if you have the option - it's much simpler. You can do field comparisons or hashing if you don't have the option of changing the database, but it's more fiddly. 

It's OK for nested subqueries to use the same aliases as used in the parent query, although it might be a bit confusing for someone reading the code. The name space for aliases on a nested subquery is separate from the name space on the parent. For example the query below has a nested subquery that also has an alias used within it. This would be potentially confusing to the programmer but fine with the DBMS engine: 

This is not a technical problem, it's a contract management problem. Don't frig with the turnkey application. It gives the vendor a get-out-of-jail-free card that allows them to ignore their service level agreement. Either, 

2NF deals with 'the whole key' - if you have a composite key and some members of the relation are dependent on a part of that key then they should be split off into their own relations. In your case: 

. . . and so on. For a query using a small number of partitions you will at least get the benefit of spreading your data and index disk I/O. Depending on the nature of your I/O you may be better off having all of the partitions on a single large RAID-10 as mentioned elsewhere. Of course if your controller won't let you do this (e.g. an IBM Shark) or you have multiple controllers then you will have to use multiple volumes. If you have the option I'd suggest benchmarking it before the system gets to production. Another point to note is that some systems (DS8000s come to mind) put their physical volumes into a pool and abstract away a lot of control you have over physical disk layout. For 1B rows you might want to look into direct attach storage if this is an option. 

If your query is returning a lot of rows across a slow WAN link then you might be seeing a slowdown due to network traffic. I have that class of problem where I'm working now. If you report server is on the same fast LAN as the database server it would be quicker under these circumstances. This might explain the difference in performance. Also, if you have virus scanning software running on your PC it could interfere with the local processing in a variety of ways - scanning the .data file etc. Often real-time scanning also hooks into memory allocation and scans data segments of applications. This really slows down managed applications. Those are a couple of possibilities that come to mind. There may be other causes as well. 

If you have the following database schema with Date, Product and Reseller dimensions and a price fact table: 

The comments on the question show that the issue is that the test database the OP was using to develop the query had radically different data characteristics than the production database. It had much fewer rows and the field being used for filtering wasn't selective enough. When the number of distinct values in a column is too small the index may not be sufficiently selective. In this case a sequential table scan is cheaper than an index seek/row lookup operation. Typically a table scan makes exensive use of sequential I/O, which is much faster than random access reads. Often, if a query would return more than just a few percent of rows it will be cheaper just to do a table scan than an index seek/row lookup or similar operation that makes heavy use of random I/O. 

ksh or bash let you script items through SQL*Plus, and you can do quite complex stuff through this. However, shell scripting tends to be a bit 'write-only' and sh and its derivatives aren't really much good for developing complex program logic. For running automated tasks they're OK. For complex client-side data manipulation, not so good (sed/awk notwithstanding). Perl has various Oracle libraries such as DBD:Oracle available through CPAN, and is somewhat better for work that involves complex processing client-side as its type system is a bit more sophisticated than 'line of text'. Another option that's quite good for this is python and cx_Oracle. 

Row-level YTD calculations on a database table are not very useful unless you have a unit of analysis for your YTD figures that can be processed additively (i.e. you're not going to find your reports trying to aggregate two YTD figures). This essentially limits their usefulness to summary tables. This type of aggregate table is of limited use with a cube, as the cube gives you the aggregation for free. If you have a time dimension on the cube you can make calculated YTD measures that encapsulate the calculations. with an expression such as . You don't even have to specify the date. The calculations you describe can all be done relatively easily in MDX with hierarchy functions such as and , and this is more flexible and much less effort to implement than working off an aggregate table generated in the underlying database. You're better off doing al the YTD and period-period comparisons in the cube. 

Populate the dimensions with the appropriate list of locations, date ranges, time of day to the right grain and one data source record per file. This table will also allow you to put a cube over the top, or can be flattened with a view, which will help people using tools like Excel or stats packages to get and use the data. 

This is essentially a speed for space tradeoff, and temporal databases are really just abstractions on a data structure that is semantically equivalent to one of the schemes described above. If you want speed, calculate and persist periodic snapshots of your position. This will use more space, but if your DBMS platform supports partitioning then the snapshots can be managed and disposed quite efficiently. The snapshot date forms the partition key, so any queries that specify dates will give the optimiser enough information to ignore partitions that are not relevant to the query. Frequent snapshots will use lots of disk space, so you trade off granularity against disk space usage. If you want to save space then store changes and calculate running sums for your as-at positions. You can also use periodic (though less frequent) snapshots in combination with the deltas for a period to optimise the query by starting with balance figures from the appropriate snapshot and adding the deltas to your as-at date. This technique is commonly used with accounting systems such as Oracle Financials. Hybrid approaches Essentially you have a continuum from a pure running sum to a pure snapshot. Snapshotting is faster to query, and can be used to speed up running sum calculations by allowing you to start the running sum calculation at the snapshot date. A hybrid approach is more complex as you have to implement both mechanisms, but it does allow you to tune your speed/disk usage tradeoff to whatever is most appropriate to your application. Some thoughts on what might be applicable when It's far easier to retrofit snapshots to a base transactional model than to go the other way around. However, some models - things where you have time-dependent state - only make sense when viewed as a snapshot that shows the state at a given point in time. An example of this is aged debt reporting where you want to see the invoices with money that's been owing for more than (for example) 180 days. If you want to retain a reconcilable record of aged debt positions then you may wish to snapshot this periodically. In the first instance a transactional fact table with the base movements (deltas) gives you the raw data from which you can calculate your snapshotted positions. It's best to have that available if at all possible, unless recording the data in that format is made infeasible by some limitation of the upstream data source. I've done an aged debt reporting system and a claims reporting system based on snapshots before. The aged debt report was an operational report run at the beginning of each week, and the requirement for the claims snapshot table was for a monthly report where the business wished to see counts and outstanding values of claims that were open at the time. Some things (e.g. claim reserving) are done on a periodic basis, so (for example) ultimates and IBNR1 are only calculated on a monthly or quarterly basis and it only makes sense to report those on a periodic basis. In this case, snapshotting the figures makes sense due to the nature of the underlying process that generates the data. 1Ultimates are insurance-speak for forecasts of the total claims expected on policies incepting in a given year. IBNR (Incurred but not Reported) is an estimate of the figures on claims that have not occurred or been reported yet. 

SQL Server 2005+ comes with an ad-hoc reporting tool called Report Builder, which does much the same sort of thing as Discoverer. This will probably get you what you need. SQL Server It also comes with a reporting tool called Reporting Services (SSRS), a SQL based tool that does the same sort of thing as Crystal Reports or Oracle Reports. SSRS will consume a variety of data sources, including Oracle. One key point to note about SSRS is that you can use it as a medium for publishing reports built with Report Builder. 

If eventual consistency is acceptable and all your queries are aggregates then perhaps a low-latency OLAP system might work for you. Your requirement sounds a bit like an algorithmic trading platform. This type of architecture is often used in trading floor systems that have a requirement to carry out aggregate statistical analysis computations on up to date data. If you can partition your data by date and old rows don't get updated then you can build a hybrid OLAP system using a conventional OLAP server such as Microsoft Analysis services backed by an ordinary RDBMS platform. It should be possible to make this cope with ~4TB of data and both SQL Server and SSAS will do shared-disk clusters. Similar OLAP systems (e.g. Oracle/Hyperion Essbase) are available from other vendors. OLAP servers work by persisting data in a native store, along with aggregates. Most will support partitioned data. In addition, most will also work in a ROLAP mode, where they issue queries against the underlying database. The important thing to note is that the storage strategy can be managed on a per-partition basis, and you can switch a partition from one to the other programatically, In this model, historical data is stored in MOLAP partitions with aggregates of the data also persisted. If a query can be satisfied from the aggregates then the server will use them. Aggregates can be tuned to suit the queries, and correct aggregates will dramatically reduce the amount of computation needed to resolve the query. Very responsive aggregate queries are possible with this type of system. Realtime data can be implemented by maintaining a small leading partition - for the current month, day or even hour if necessary. The OLAP server will issue queries against the database; if this partition is small enough the DBMS will be able to respond quickly. A regular process creates new leading partitions and converts closed historical periods to MOLAP. Older partitions can be merged, allowing the historical data to be managed at any grain desired. The clients writing to the database just write straight out the the underlying RDBMS. If historical data remains static they will only be writing to the leading partition. 4TB is a practical volume to use SSDs for if you need extra DBMS performance. Even mainstream vendors have SSD based offerings with faster SLC units as an option. 

Not surprisingly, Oracle's B.I. strategy is viewed by industry pundits as something of a dog's breakfast. There are probably other things that I can't think of OTOH; I'll add them as I think of them. 

I believe that it can, and the API wraps a fairly well documented web service interface called XML/A. The SQL Server 2008/2008R2 management software will work with SSAS 2005 cubes as well. 

On the trials and tribulations of making generic B.I. products For the moment I'm assuming that the physical structure of the data model remains the same and you just want to change fields and their definitions to site-specific customisations. If you want to frig the database structure you're really into building bespoke systems. The other assumption that I'm going to make is that you have a star schema. This can be done with metadata for each of the dimensions and fact tables that has the attributes for each table. This metadata allows you to construct a customised view over a generic base structure. You will have to structure the ETL to map to the base data but you can configure a customised view over it. There are a couple of ways you can manifest the base tables: generic columns and an entity-attribute-value structure. In each case, the customer's view is customised by metadata stored in the system. Generic columns A table with generic columns will have columns with names like 'Code1', 'Code2' etc. and groups of columns of particular types (code, money, float, int etc.). You implement a table like this for every dimension in your system, and a table with some integer and money values for each of the fact tables. This is customised by creating a view that links the columns to a business facing name. Te metadata is used to generate the view. If you have cubes and/or a suite of reports sitting on top of this, you can also programatically annotate the dimensions and report fields based on the metadata, allowing them to be productised but customisable. The view is simply a view over the base table that renames the columns.