I would commend alternative 1 to you. Just as having cells as columns a01, a02 etc. is poor normalisation so, too, is having measurements as m01, m02 and so on. In your example you have disguised this by calling them foo, bar and baz, but that is what they are. To address this what you call whould more usefully be called : 

In this world, the new value is only added to the row when that row is re-written. The software knows where it has written the row and maintains any forward pointers to be able to return the complete row on demand. It may choose to delete the old version, freeing that disk space for reuse, and write the new row elsewhere in contiguous storage. There is no implication that making a schema change in one place automatically propagates to other, similar rows. For key-value stores the requirements are even more lax. What constitues a "column" is entirely up to the applicaiton. All the storage engine sees is a blob of bits, which it writes to disk and indexes. How or where on disk it holds these bits is neither here nor there to the application. Not all relational databases require all parts of all rows to be contiguous. Oracle allows a single row to span multiple pages. SQL Server has off-page pointers for long text columns, and Filestream allows for storage in the OS's filesystem outside of the DBMS. 

I've never heard of comments causing a problem. The code you captured from the running server and the posted source are for different SPs. It makes me think that maybe the version is still in your database and being executed (from a job perhaps?) whereas you are trying to debug the code. For testing you could add another INSERT to the SP, writing as single value to a dummy table. Then if the commented-out lines run there will be more rows in the dummy table than expected. 

Service Broker can provide this functionality. It uses a store-and-forward messaging model. Your local edition may not have this available, however. 

By JOINing to order_items that query returns one line for each row in the order_items table. The COUNT is therefore counting rows from order_items, not from table orders. This is why you think you are over counting. Solutions would be to remove the joins to tables which are not referenced directly in the resultset, or to use the syntax. 

I would be inclined to use call it CompanyPhone at this point. I know I'm mixing logical and physical arguments. I'm assuming your intention is to implement a system and not simply derive an abstract model. One thing I haven't covered, and isn't mentioned in your question, is whether a phone number can relate to a company or an employee or a supplier i.e. are the relationships mutually exclusive. 

A view can be materialised by creating an index on it. There are several preconditions which must be met. These may be tricky to impose on large, established applications with layered views. 

I happen to be using SQL Server 2008R2 but I am interested to learn how other dialects would handle this. 

Having all your data in fewer, common tables is obviously going to make your coding simpler. I would suggest you pursue this option. Partitioning is not that scary. Annualised data like yours is the archetypal usage for partitioning. It will help with your query performance and may help with your data management tasks (load / delete), depending on MonetDB's capabilities. You say some data is in different formats in different years - tinyint v/s character. This will be a problem for you whatever architecture you choose because you will eventually want to combine these in one resultset. Fix this sooner rather than later. Fixing in the data and table definition will be less work overall than fixing it in each query as you write it. In summary 1) Fairly, especially if you have to reconcile type differences 2) "A bit" if your queries are simple to "a lot" of you get it badly wrong 3) not a MonetDB expert but most RDBMS handle data many times the size of RAM without problem. 

To go from "cat" (English), find the corresponding row in Translations. From that find the concept_id. Then read the concept_id, language_name row in Spanish. Homonyms make this difficult, however. 

Nothing is free. Sometime not having something isn't free either. Both having and not having declared foreign keys come with costs and benefits. The point of a foreign key (FK) is to ensure that this column over here can only ever have values that come from that column over there1. This way we can be sure we only ever capture orders for customers that actually exist, for products we actually produce and sell. A lot of people think this is a good idea. The reason we declare them inside the DBMS is so it can take care of enforcing them. It will never ever allow in any data that breaks the rules. Also it will never allow you to get rid of data required to enforce the rules. By delegating this task to the machine we can have confidence in the integrity of the data, no matter what its source or when it was written or which application it came through. Of course this comes with a cost. The DBMS has to check the rules are being followed, for each and every row, for each and every query, all the time. This takes time and effort, which is a load on the server. It also requires the humans to submit DML in a sequence that respects the rules. No more slyly forcing in an Order, then catching up with the admin afterwards. Oh no naughty human, you must first create the Customer, then the Product (and all the prerequisite rows) and only then may you create an Order. Without foreign keys we are much freer with what we can do, and the order in which we can do it. Problematical rows can be removed ad hoc to allow critical processes to complete. The data can be patched up afterwards, when the panic is over. INSERTs are generally a little quicker (which adds up over time) because no FKs checks are done. Arbitrary subsets of data can be pulled from the DB as desired without having to ensure all supporting data is included. And so on. This can be absolutely fine if the people involved know the system, take careful notes, have good reconciliation routines, understand the consequences and have the time to tidy up after themselves. The cost is that something, somewhere is missed one time and the database deteriorates into barely creditable junk. Some teams put the checks in the application rather than in the database. Again, this can work. It seems to me, however, that the total server load will be much the same (or slightly higher) with this approach, but the risk of forgetting some check in a bit of code somewhere is much higher. With DRI the rule's communicated to the computer once and is enforced forever. In application code it has to be re-written with each new program. For my two cents' worth, I'm all for foreign keys. Let the computers do what they're good at doing, like routine repetitive checking that column values match. We humans can concentrate on dreaming up new and interesting stuff. Having taken responsibility for a system a few months back I'm adding FKs to most tables. There are a few, however, where I will not be adding them. These are where we collect data from external sources. The cost of rejecting these rows is greater than the cost of accepting bad data and fixing it up later. So, for these few tables, there will be no foreign keys. I do this open-eyed, knowing we have monitoring and corrective procedures in place. 1I acknowledge the existence of multi-column foreign key constraints. 

A table with several hundred columns is within the capabilities of any DBMS. It may be strange but it is not nesessarily wrong.