Perhaps the greatest success story of linguistic analysis was Kurylowicz's discovery that Hittite inscriptions provided evidence for the laryngeal theory for IE reconstruction, proposed by Saussure. But everyone knows about that. (See Larygeal theory - History). Another well known case is the comparison of reconstructed Romance with Classical Latin. As I understand it (not my area), there is considerable disparity, which is passed off as being due to differences between the spoken vernacular and what had become a scholarly language. 

It's done to make the plot resemble the traditional articulatory plot of vowels by height and backness of the tongue. 

Well, here's an idea. I don't have specific references for you, but Linda Shockey did some work at Ohio State comparing vowel formants (and maybe other acoustic measures) in careful versus casual speech. As you might expect, vowel positions in acoustic space tend to be more peripheral (though Shockey's results were much more specific). So what you might try is assessing the carefulness level by how far vowels are from schwa (other than schwa itself), using the first two vowel formant levels to measure. 

I don't know Mongolian, but I'll venture a guess. There is front/back vowel harmony in Mongolian, so a distinction in writing between word final "a" and "e" is not all that necessary. It will be /a/ in back harmonic words and /e/ otherwise. The vowel separator glyph seems to have been pressed into service to make this distinction for the sake of non-human readers. I found some discussion and illustrations here on pages 5-6. 

There is no mathematics of English for you to learn, unfortunately, but for someone with an analytic bent, I think a careful study of McCawley's The Syntactic Phenomena of English, doing the exercises, would be fun and useful. I second Lawler's recommendation of logic as the most relevant formal discipline for the study of language, and, specifically, I suggest Hans Reichenbach's text Elements of Synbolic Logic (available on line!). Reichenbach had lots of interest to say about English. In my opinion, the most relevant area of mathematics is that connected with the Chomsky Hierarchy and context free languages. 

I have one for you, but it's not frameworkless. I don't see how there could ever be a frameworkless formal definition (since "formal" means it is part of a formalized theory). A topic is a clause constituent bearing the grammatical relation 0. This is to be understood as part of an extended version of the theory Relational Grammar, due to Paul Postal and David Perlmutter (see Relational Grammar). The 0 is a term relation in the same series as 1 (subject), 2 (direct object), and 3 (indirect object). RG did not actually have a 0 -- I've added that. The 0 relation for topic is subject to the same laws and constraints of RG as the other term relations, the most interesting of which is the Stratal Uniqueness Law, which requires that each term relation appear no more than once in a stratum. (A stratum is something like a derivational level.) 

There have been several syntactic theories based on unordered constituents, among them Relational Grammar (with Arc Pair Grammar) and GPSG. The idea is that tree sisters are fundamentally without any intrinsic order, but that eventually, at a superficial level, for certain languages and certain constituents, the order gets fixed. I think that Gerald Sanders was the first to suggest such an explanation for the generalization in Ross's Gapping and the Order of Constituents. I know beans about "merge", but maybe it is that old idea about separating ordering principles from constituency principles cropping up again. 

I don't see any floating of a preposition in your examples. McCawley has some interesting remarks about reciprocal "each other". In the reference, click "next" until you get to occurrences 6-9, and especially 8, which mentions an analysis by Dougherty attempting to relate this "each" to the one that floats. 

Take a look at John Chadwick's The Decipherment of Linear B. It does include some relevant theory -- I've used it as a text in an elementary linguistics course. 

Fortunately, Sapir's classic The Psychological Reality of Phonemes is available on line for free. I'm sure you'll find it interesting. I don't think your teacher's characterization of phonology is very good. While many phonologists would probably agree with Sapir's contention that phonemes are psychologically real, on the other hand, they would be chagrined to learn that they are not studying physical reality. It's probably the idea of some phoneticians that anything mental is not physically real, but few others would subscribe to it. 

Poetic conventions and language games may provide some evidence. See this previous discussion here and my answer: psychological reality question. 

The reason for the grammatical kinship between antecedents and questions is that they play corresponding roles in human reasoning. Questions are the conversational counterparts to antecedents. Compare: 

To me, perhaps to some other phonologists, this question is uninterpretable, since it mixes fact and theory. Allophones are the realizations of phonemes and have acoustic properties. Phonemes are part of our theories about observable acoustic properties. Phonemes don't have acoustic properties, only allophones do. Consequently, any sort of acoustic variation is, of course, allophonic. 

Yes, the phonemic transcriptions should be the same. But they're not the same, so this is a problem for phonemic analysis. It has been much discussed. Generative phonologists give up phonemics entirely, other linguists have attempted to weaken the principles of phonemics somehow, to accommodate such difficult cases. 

The wording of your question seems to imply an equivalence between first order predicate logic and higher order logics. They are not equivalent. First order predicate logic was shown to be consistent and complete by Kurt Gödel, but "Stronger logics, such as second-order logic, are not complete." (Consistency.) Whether first order predicate logic is sufficient to describe the logic of natural languages is an open question. 

Your alphabet is completely phonetic? Are you sure? The Sound Pattern of English feature system has around 17 orthogonal features which, even if they are all binary valued, gives 2^17 distinct sound segments. That takes a pretty big alphabet. (Though not all sounds occur in all languages.) I don't know why you think that a phonetic alphabet doesn't need word spaces. Some writing systems have not marked word divisions, but I don't think it has to do with how phonetic the system is. Trying to answer your question, my impression is that different writing systems for a single language have been maintained by different religious groups of a community. 

Logicians make the distinction between "use" and "mention", following Quine in Word and Object (though I don't know that Quine was the first). You can mention language examples, without necessarily intending to convey what would ordinarily be conveyed by saying/writing them. In your example, "Trees are green" is both used (since it is asserted), then mentioned, when a comment is made made about the linguistic expression "trees are green". Quine gave a famous example: "Tiny was so-called because of his size", where "Tiny" is used to refer to an individual, and mentioned, when a linguistic comment is made about the appropriateness of this name. 

But how can it be? If it were, the "Who" form would be ungrammatical, because the full version breaks the Coordinate Structure Constraint. The importance of John Ross's paper on sluicing, "Guess who", is that it gives evidence against deriving the small form sentences from the full forms. Not for, against. Someone seems to have missed the point. 

Describing a /t/ as "glottalized" without including an alternative "glottal stop" may be unwise, because it invites confusion between between a /t/ pronounced like an ordinary [t] but with simmultaneous closure of the glottis (which I would write [tʼ]) and a glottal stop [ʔ], because both are possible. (Also, "glottalized t" in other languages can refer to an ejective t.) It's hard to give good examples for /t/, so please consider the difference between three kinds of /p/ in my speech: (1) plain [p] in "spore", (2) glottalized [pʼ] in "stopped", (3) glottal stop [ʔ] in "pop bottle". For (3), there is no lip closure for the second "p": [pʰɑʔbɑdl̩]. There are correspondingly three varieties of /t/ also, but for "pot bottle" the difference between glottalized t and glottal stop is just a matter of timing the alveolar closure. (You may not have these exact pronunciatiions in your own speech.) 

If we raise issues concerning the people who are communicating, that's doing pragmatics. If we don't need to raise such issues, we can stick to discussing semantics. In your example, if I were to ask if the "you" in what A says actually referred to person B, I would be raising a pragmatic concern. Reference concerns individuals, while sense goes beyond that concern to consider how we figure out things about individuals. If I tell you I saw a unicorn nibbling at some lettuce in my garden this morning, you might be inclined on a referential semantic analysis to say that was false, but if you want to take into account that in some imaginary worlds, such things might happen, you might explore the sense of the term "unicorn" for me. Did it have a single horn, and so on? Perhaps I am commencing a story for you about a world where unicorns do exist. The possibility that various worlds can be populated by various individuals prompts some semanticians to describe the senses of terms, i.e. concepts, as being functions telling you for various worlds what individuals the terms refer to in those worlds. 

So...does this mean that 'stress' refers to both timing and intonation, or are they different? Both, and to other things, but it would be better to change "refers to" to the passive "is referred to by". You recognize stress by its footprints, sort of like Bigfoot. Stressed monophthongal vowels in English don't reduce to schwa, so when I hear an unreduced monphthongal vowel, I know that vowel is stressed. Stressed vowels like to be accompanied by syllable onset and syllable offset consonants, while unstressed vowels reject them, so when I find an intervocalic consonant leniting in a way I associate with syllable offset consonants, that tells me the following vowel is unstressed. For instance "bacon" can have [x] substituted for the [k], so I know from that that it's syllabified "ˈbac.on" with unstressed second vowel. But the [k] in "Baconian" doesn't lenite, so that [k] must belong to the syllable of the following stressed vowel: "Ba.ˈconian". The pair mentioned in SPE, "Plato"/"motto" is like this. The t in the first is not flapped, but rather aspirated, so the second vowel must be stressed: "ˈPla.ˌto", while the t in the second is flapped, so it must precede an unstressed vowel: "ˈmott.o". 

It's an adverb, using McCawley's definition of adverb as "a modifier of something other than a noun". (I don't think your term "adjunct" conveys anything useful.) IIRC, Joseph Emonds characterized "right" as a PP (prepositional phrase) modifier, but also proposed that some bare prepositions were actually objectless PPs. 

I find "semantics is the input to syntax" to be rather obscure in meaning, but perhaps it means that everything about the syntax of a phrase can be predicted from the semantics of that phrase. If so, all you need to do to show the thesis is false is to give some fact about the syntax of a phrase which is due to its meaning rather than tradition. That sounds pretty easy. Can you you predict whether a language has prepositions or postpositions from the meaning of pre-/post-positional phrases, for instance? 

Lots of linguists have disliked phonemes. Who was the linguist you have in mind? It could be Noam Chomsky, if you go clear back to 1964 and his paper Current Issues in Linguistic Theory. Chomsky argued that phonemic theory could be characterized by certain assumptions about the relationships of allophones and phonemes, which assumptions were vulnerable to factual arguments. Defenders of phonemics naturally attacked the assumptions as not actually characterizing phonemics at all. Generative phonologists (followers, to at least some extent, of Halle and Chomsky's theories) who had a descriptive interest in phonology have tended to use the term phoneme to mean segment of underlying phonological representation, which is probably very, very different from what a phonemicist would have understood a phoneme to be like. 

In Syntactic Structures, Chomsky remarked that natural language needs a grammar more powerful than regular grammar (type 3), such as context free phrase structure (type 2), in order to keep track of the pairing of such phrase introducers as "if" and the corresponding "then" clauses which go with them. I don't recall C's exact example, but consider the language generated by this context free phrase structure grammar: 

Maybe the first stop lenites to a fricative because it is in the offset of a syllable, and the change is limited by the law of similarity, which allows changes to be limited to those sounds most similar to a neighboring sound. The "law" is due to James Hutcheson: Hutcheson. J. 1973. Remarks on the nature of complete consonant assimilation. CLS 9. As it applies here, the principle could be phrased, "Only the most similar dissimilate." I argued for this interpretation in "Natural phonological descriptions (Part II)." U. of Hawaii WPL 8:3.45-54. I know I didn't answer your question about Autosegmental Phonology. Sorry. 

"who was dismissed" is an embedded question. It's unembedded form would be "Who was dismissed?", so no complementizer was added to convert the independent form into the embedded form. I don't understand why "was" would ever be considered a complementizer. 

Linguists seem to love diagrams, but what kind of diagrams you give for language expressions bears only a distant relationship to any real differences among grammatical theories, if any relationship at all. Since you've been interested in binary branching as one way to distinguish formal grammatical theories, I thought you might be interested in the following account of just how trivial a matter this is, theoretically. You can find a way to write binary branching diagrams for context free phrase structure grammar (cfpsg) or any of its notational variants (including dependency grammar). A sentence is generated by a cfpsg when there is a correct derivation for the sentence, and a derivation is a list of strings of terminal and nonterminal symbols, the first in the list being an initial symbol (typically S) and the last in the list a string of terminal symbols (say, e.g., a string of phonemes giving the pronunciation of a sentence in the language being described). It's a very simple theory. Each line of the derivation after the first is produced from the preceding line by using a phrase structure rule to replace some non-terminal symbol with a string of symbols. The last line of the derivation has only terminal symbols (phonemes, let's say). A tree diagram of the usual sort giving the grammatical structure of a sentence can be produced from the cfpsg derivation in a mechanical fashion, so that the tree structure can be thought of as, essentially, an abbreviated cfpsg derivation. Trees are grammatical derivations. One way to construct such a tree diagram is to make a branch at each application of a phrase structure rule. There is a previous line of the derivation and a phrase structure rule, and from these two things, the next line of the derivation is derived by applying the phrase structure rule to some non-terminal in the previous line. Two things in the tree are combined to form the next node up. So there you have it -- binary branching. As you can see, it's a trivial formal maneuver. If you wanted more branches than just two, you could arrange for that by applying several phrase structure rules at once in the underlying phrase structure derivation. Could we ever find out anything interesting about language by playing games with diagrams? I don't see how. Diagrams are not theories. 

I suppose all human languages must have some way of conveying complete ideas, if that means saying something which, if translated into English, would have a subject and a predicate. But really, that seems to be more a fact about English than about other languages, since it's about how ideas are expressed in English. If you look specifically at the overt forms of sentences, there doesn't seem to be any real evidence that the subject-predicate form is universal among human languages. For one thing, there are plenty of sentences expressing a complete thought in English and in other languages that do not have the subject-predicate form. And for another, there are many languages which have VSO word order -- that is, a basic sentence form in which the subject comes between the verb and the object, so that there can't be a single predicate phrase like the verb phrase of English, which contains both the verb and the object. The Wikipedia has a short entry on VSO languages: Verb subject object. From that article: