Background: I have numerous databases with a large number of VIEW's, and an extremely large number of SYNONYM's. For example, one db has more than 10k VIEW's and 2+ million SYNONYM's. General Problem: Queries involving (and system tables in general) tend to be slow. Queries involving are glacial. I am wondering what I can do to improve performance. Specific Example This command is run by a third party tool. It is slow in both the app, and in SSMS: 

I have been able to update statistics on the underlying system tables. This has worked in my SQL 2008 R2 or newer environments: 

An Event Notification can be used to both monitor for DBCC command events and respond to them. The script below creates all the necessary objects, which I would recommend be created in a separate database. I've added comments here and there where appropriate. Most of the script can probably be left intact, except for the stored proc, which you will want to tailor for your needs. If this solution doesn't fit your needs, you can simply drop the database. 

But this force me to wrap it with another . Is there any other solution instead of wrapping in ? (cte is also like wrapping) 

Let's say I have a world wide website ( I mean it has uses from all around the world). I have 1 server which is in germany ( at some city). it will never be moved from germany - ever. My question is : When users saves data , I need to save the action date : Should I save ( and I think I do) it as ? Or since - the server is in one location - it is ok to save all the dates as so the point of relativity is the same point. Am I wrong ? If I do , can you please supply a simple scenario where it's gonna fail ? 

of course- the is calculated before the actual update. but does this filtering () is also inside the lock ? I mean where is the lock ? here : or here : 

The problem: The problem is that I Must(!) put values in the in order for it to work ! Otherwise , For example : If I put in all this is the result : 

2nd Attempt Here I opted for a variable to store the total number of users (instead of a sub-query). The scan count increased from 1 to 17 compared to the 1st attempt. Logical reads stayed the same. However, elapsed time dropped considerably. 

I'm a huge fan of the event-driven approach for a scenario like yours. Meaning, I don't want to sleuth who did it after the fact, I want SQL to tell me when it happens. For that reason, I'd suggest a DDL Trigger. Here's an example: 

From there, you could build a TSQL string of commands and execute it. (I hardcoded for the schema. Presumably this will never change.): 

I have inferred that the Windows OS does not actively "push" the OS level memory availability status to SQL Server. Rather, the SQLOS Resource Monitor must actively ask for it via the Win API function. This would seem to be an asynchronous operation. Am I interpreting this correctly? If my reasoning for #1 is sound, what causes the Resource Monitor to get the OS level memory availability status? Is is performed on a schedule? Event driven? How much time might pass from the moment there is low memory at the OS level to the moment the SQL OS Resource Monitor becomes aware of it? When SQL Server reduces memory usage to free memory back to the OS, just how bad is that? Is this an "expensive" operation that should be avoided at all costs? 

What you are talking about here is multi-tenant vs multi-instance architecture. I'm just bringing up these terms as you don't use them in your question but this is what you are discussing is called and if you just plug "multi-tenant architecture" into Google, you will find a wealth of resources and discussion about it, entire books have been written on it. Some good resources regarding SQL Server specifically here: $URL$ $URL$ I would be with other answers, in that I would lean strongly towards multi-tenant as a default, unless you have compelling reasons to favour multi-instance. You don't need to split into thousands of individual client databases to scale, there are many other ways of doing that, that are likely to be preferable. Like clustering, replication, sharding, partitioning etc. Don't reinvent the wheel. There's nothing inherent that says you need to split this yourself manually on an individual customer level and indeed doing so is likely to increase significantly the costs of adding every new customer. You are talking about "millions" of customers, think of any large-scale cloud-based software as a service, Gmail, whatever, you hardly think they create an entirely new database for each new signup, now do you? There can be reasons where you do want to facilitate this, for example, if you are selling your product to a customer that MUST have it hosted in-house on their own infrastructure. But as a general SAAS rule, lean as a default to a multi-tenant architecture. 

I have a few "gaps" in my understanding of external memory pressure. I've found quite a bit of helpful information online, including this info from SQLSkills.com. 

Here's a blog post about handling events with DDL triggers with some further explanation: SQL Server Event Handling: DDL Events 

This could easily be put into a job step for a SQL Agent job. I tested this on SQL Server 2012 w/ SP3. I've also run similar code in SQL 2008 R2 and SQL 2014. I don't know for sure if it works in SQL 2016--please let us know if it does not. Other Notes The commands worked when I was logged in as a [sysadmin]. It also worked if the login was not a member of [sysadmin], but was the database owner. Membership in [db_owner] alone was not sufficient. (Again, this was on SQL 2012 w/ SP3. YMMV.) 

This is a sad commentary on the state of software development and deployments that rely on an RDBMS such as SQL Server. The development and/or DevOps teams should know what level of authorization is needed by applications, both at runtime and at deployment time. It's very important to make the distinction between these two. Unfortunately, development and/or DevOps teams never figure this out because {reasons}. So here you are... You've mentioned that you are familiar with fixed server roles and fixed database roles. I don't want to insult your intelligence, but I think it's worth mentioning for those that are not familiar. At the server/instance level, the CONTROL SERVER permission is a much better option than membership in . At the database level, I would never let a non-dba own a database. They connect as -- DENY permissions have no effect. You can deny permissions to a database user that is a member of -- this is also a better option than membership in . But it is still usually overkill. I generally prefer to give a database user membership in , , and . Additionally, I'll GRANT EXECUTE, CREATE SCHEMA, and VIEW DEFINITION. Based on your needs, your mileage may vary. Another thing to note: you cannot DENY permission on DBCC commands. Members of can do a lot with DBCC, and to a lesser extent, members of can too. After all of that, I've still had to give authorization to non-DBA types on a regular basis for deployments. In those scenarios, the requestor had to submit their request to a Change Advisory Board for approval. Membership in was temporary and I always set up a SQL Agent job to remove group membership after an agreed upon amount of time (usually one week). I did this enough times that I wrote re-usable code to quickly create the necessary job, job steps, schedule, etc. Moving on... Whether it's temporary or permanent, you still have to deal with non-DBAs that have elevated permissions. SQL Server has some built-in features that can help you handle events that you might not want to happen: DDL Triggers: these are a great way to handle certain SQL Server events synchronously. Maybe you just want to know an event happened. Grab the and send an alert or log the info to a table for later inspection. If you're more hands on, you can inspect the as events occur and optionally ROLLBACK the bad stuff. It's really powerful. I kept an eye on ALTER_INSTANCE, ALTER_DATABASE, DROP_DATABASE, and a raft of authorization-related events. (Note: ROLLBACK doesn't work with the ALTER_DATABASE event.) EVENT NOTIFICATIONS: some events can't be handled synchronously with DDL triggers, but they can be handled with event notifications. ROLLBACK is not an option, but you can at least grab the for post-mortem analysis. This option involves Service Broker, and the learning curve to get started is much higher than working with DDL triggers. I think it's worth it, though. There's nothing to prevent a member of from "sidestepping" a DDL trigger or Event Notification (other than their own ignorance). But that's a severe offense IMO, worthy of disciplinary action. Another feature you may want to look at is Policy Management. I've not used it myself, but it appears to be quite powerful and flexible. It may also be a bit easier to use as compared to DDL triggers and Event Notifications. I'll let you be the judge of that. 

Question : why I can't use my instance name to connect to my sql server ? Related info : After connecting via one of the 3 who work , I run a helpful info : 

I want to monitor only sp's. Does SQL Server Profiler listen to all events and then filters the desired data (e.g. sp's)? Or rather the sql emits only Profiler pre-configured events ? 

(yes, now sorting by path - doesn't work...) I need that will always be under (The only reason I added the is to order siblings !) and I don't want to enforce adding when not needed Question : Is it possible to enhance the query so that will affect only to siblings ? I mean , If I don't care about the order of the siblings ( by putting ) , I still expect the to be under Sqlonline : with siblingOrder Sqlonline - without siblingOrder 

I'm using sql server 2008 r2. I logged in to ssms which in turn connects to a remote sql server machine. im writing a query which writes a file. I need to know - which windows permissions I should grant to is there any which can provide me the windows account who is actually finally writes the file ?