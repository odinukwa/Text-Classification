This would be the finishing of the discussing. But WAIT as I mentioned earlier I have solved this problem and that solution is not matched with this one!! WHY !! Finally I figured I out, this is the result of not reading the question properly. They asked for the number below , but what I have done here? for and it was okay, but for ? 1000 is divisible by 5! We can't include it in the set!! So the actual summation would be , less than what we got here. People might argue why I have posted the whole solution here. But thing is that, anyone with a little knowledge of programming and the basic understanding of modulo can solve this problem like the questioner. Is this the right way of solving these problems? Was it the intention behind developing this problem bank? Of course not, at least I believe. They certainly want us to look at problems in a different angle. Look things those are hidden in the plain sight. Sketch the idea before painting actually. This post is for future reference only, intended to them those are interested solving the problems in this website. But also for others to show there could be some other perspective to any problem. Thanks for reading. EDIT As per suggestion by @200_success I am rewording here. As per the original question I agree with @Simon André Forsberg there is certainly nothing wrong with the modulo operation. And also it can be broke down to a simpler version as he already mentioned. I want to add with it that the condition writing is very easy with the basic understanding of . I am just breaking down the idea. In digital logic design is represented as and is as (and there are some more). So the condition becomes . Now you can have as many times as you like as its just an operation, so we can rewrite 

We need to get all the numbers below 1000 those are divisible by 3 We need to get all the numbers below 1000 those are divisible by 5 There will be repetition of numbers as there are a lot of numbers which are divisible by both 3 and 5. 

Note 1: I've renamed the function to to keep consistency with other similar functions such as . I've also reordered the initial condition to take advantage of boolean short circuiting. Note 2: As Rakete1111 points out, if your compiler hasn't caught up to C++11 yet, accessing the first character of an empty string is undefined behaviour. The condition should be modified like so: 

This finds the number of doors that can be opened by key \$K = 1\$ in the range \$[10, 32800]\$. As a general note, please note that identifiers that end with are reserved by POSIX, so be careful if you're including POSIX headers (if you are, remove the part to be safe). 

Consider the rule of 5 when your class manages a resource. Whenever your class manages a resource, it probably needs to implement the destructor, copy and move operations. You definitely do not want pointers to be copied when a copy of is made. Possible issues: 

Replace long conditions with type traits. In order to promote from a smaller type to a larger type, you currently enable certain conversion operators based on on your type . These quickly become hard to maintain and are error-prone: you might forget to add a type, you might forget to update something, etc. In order to solve this problem, you can take a type traits approach. It will require some boiler-plate, but not much more than what you've already got with those long enable-if conditions. Traits based approach: 

Silent bug: Bad move constructor Your move constructor will perform a copy because your parameter is ; you cannot move from a value because moving implies modifying what you're moving from. Simply change the signature to: 

and both have the member function. The function takes the arguments to construct the key-value-pair internally (thus avoiding copy/move operations). You can instead write your return statement like so: 

Note the consistent use of the same construction approach (placement-new) which looks nice and prevents syntax issues with primitive types. Mark functions as when appropriate. Non-throwing functions of yours include , and a few more. This will help programmers know that certain functions do not throw, which can help with optimizations. For compile-time conditional marking, use type traits. Here's a sample: 

If the polyline is made up of great circle arcs (i.e. "straight" lines) then this isn't true (unless , because the equator is the only line of constant latitude which is a great circle). An extreme example to make the point: consider a great circle arc from 10N 90W to 10N 90E. Both endpoints are near the equator, but the line between them passes through the North Pole. I'm surprised that QGis doesn't have libraries which take care of this for you, but if you really have to do it yourself then it's easier to work with the embedding into 3D Euclidean geometry. To first order we model the Earth as a sphere, so a great circle is the intersection of a sphere with a plane through the centre, and a line of constant latitude is the intersection of a sphere with a normal through a pole. The intersection of two planes is a line (although note the special case that the planes don't intersect), and the intersection of a line with a sphere is 0, 1, or 2 points, (although obvious they might not be in the right range for the great circle arc). Ignoring issues of numerical analysis, if the endpoints are \$p_1 = (\theta_1, \phi_1) = (x_1, y_1, z_1)\$ (respectively in spherical and Euclidean coordinates), and \$p_2\$ similarly, the plane of their great circle is the locus \$q \cdot (p_1 \times p_2) = 0\$. The plane through latitude \$\lambda\$ is \$q \cdot (0, 0, 1) = \sin \lambda \$. If we take the route of saying that \$q = (\lambda, \mu) = (\cos \mu \cos \lambda, \sin \mu \cos\lambda, \sin\lambda)\$ and let \$p_1 \times p_2 = (x_N, y_N, z_N)\$ then we have \$x_N \cos \mu + y_N \sin \mu = - z_N \tan\lambda\$, which can be solved for \$\mu\$. Then to check whether the resulting values (plural!) are in the great circle arc we calculate the midpoint \$m = \frac{p_1 + p_2}{2}\$ (we don't even need to normalise) and verify that \$q \cdot m \ge p_1 \cdot m\$. 

Having made it explicit what the difference is, we can address the biggest problem: correctness. To correctly sum a list of floating point numbers, you should start with the smallest ones, not the largest ones. But since we now have a simple expression for as a function of , we can invert it to find the first value of for which the term is less than the desired error, using a . 

You can similarly apply this to all your other applicable conversion operators. It will greatly reduce the amount of code inside your class. 

We can see that this is much more concise and easy to read. I believe that it is worth the extra layer because it automatises the pattern and conforms to DRY; it is used in the remaining types. 

Performs the set intersection of two integer packs. The output set is sorted. The output set will have the integer type returned by applying to the integer types of the left and right integer packs. 

Extracts from the specified template argument integer pack the values from at indices in the range \$[src, dst]\$. If \$dst < src\$, a reverse extraction is performed. 

Two reductions are performed every loop to check whether we're stuck with an irreducible number. For reductions that would loop infinitely, we can deduce two properties: 

From that, we know that to reduce a number, we must simply subtract the concatenation of the largest digit x times, where x is the count of digits of your input number. Since we know that a digit is from 0 to 9, we can write the following function: 

This implementation is missing a range based erase operation. However, the building blocks to build such a function are already present. 

Do we really need and ? No. Here is an alternative solution: You currently allocate bytes. This is wasteful. You can use exactly as much space as you need by declaring your storage any of these two ways: 

Every two iterations, the digit length of the reduction must be smaller. Reduction 3 will give us the same value as reduction 1, so you could check against that as well. 

Introduction In template meta-programming, integer sequences and ranges are very useful. I've made a couple of utility classes that do various operations on compile-time integer packs. The implementation is non-recursive (except for \$log(n)\$ recursions when generating a pack), allowing faster compilation and a greater number of template arguments. Note 1: In every implementation section, at the top of every namespace, there's a comment indicating which feature is inside. Note 2: Include guards are omitted. Note 3: The following headers are used: 

Heslacher's review covers most of what I would say, but there are a couple of things to add about the main loop: 

If we consider the last one, because I think it will require the least adaptation: what symmetries can a solution have? The square has eight symmetries: 

(on the assumption that the use of buckets is documented at the class level, either directly or via literature reference).   

Is there any reason for using two variables here? I think it could be simplified by having one variable which tracks whether there's any increase over the previous digit (although the complete rewrite will eliminate this anyway). 

As far as performance goes, it seems to me that with online judge questions in general you should assume that it's not required to iterate over 10^9 cases. I'm not going to describe an algorithm in detail, because that would IMO be missing the point, but ask yourself this: if the problem statement said \$1 \le N \le 1\$, would you still want to iterate over \$Q - P\$ cases or would there be a constant-time solution? 

That's not necessarily a prime. Was it intended to say to allow the subsequent loop to just go through odd numbers? 

Factoring out common code All three of those methods have a loop which does the same thing: find the factors of . That should be pulled out into a single method, which should have a suitable return type (e.g. , although there's a good case for , or maybe some integer stream type, but I'm not up to date on Java 8). Then two of the methods sum the factors: that's already pulled out into a method, so the other method shouldn't duplicate it. Instead you should have a very simple method 

The question of finding the next permutation in lexicographic order is a classical one (going back 7 centuries) and well documented in the literature. E.g. Wikipedia gives the following algorithm: 

My first instinct when I saw the loop was that the code must be buggy because then would throw an exception. Then I figured it out: because is a , gives (which does not comparable equal to ). That is, I think, the only thing which depends on being a , since you're not actually using division anywhere. So the lack of comment in the loop () and the comment on are both misleading. Of course, the best fix is to ditch the floats and work in integers.