You're referring to "incremental backups". Assuming your looking for something open source checkout bacula or backuppc. Backuppc can have a bit of a learning curve but it works great. R1soft is awesome (bare metal restores) but it is a commercial product. 

With that many systems two factor authentication via LDAP would work well. With one factor being an RSA secureid. If a cracker were to discover a user password they would still need at least three RSA number generations in a row before being able to duplicate future generations. Strong password policies to boot. 

Sounds like the software was designed with a heavy dependence to apache. I would use nginx in front of apache for this site. Put apache on 8080 localhost or something similar. You still get the benefits of nginx for static files without having to rewrite all their rules. Otherwise I would bitch to thebuggenie and get them to translate their apache rewrites for you. That's Lame. 

If this rails info is in a mysql dbase you can configure pam_mysql. There are pam modules for just about everything. Here's one for ftp that I have with mysql: 

I would like to have ephemeral ec2 instances push logs to a central flat-file store for archiving and manual perusing, as well has have that data pushed to elastic search. Is there a single agent that can tail local log files and both push them to a central flat-file store as well as push them to elastic search? 

What is an open source load balancer that allows for hash based balancing? I would like to do hash based load balancing of a URL but first remove the user specific arguments from the URL. Basically want to increase Varnish's cache performance by adding URL to node persistence. Example: example.com/foo/usertoken/bar Where the hash would be based on: example.com/foo/bar. 

You could also run a script that periodically dumps the username:passwords into a file and use the pam_pwdfile. There are a slew of choices. $URL$ 

A job that runs only once and runs forever till you tell it to die is called a "daemon". They are normally started via init scripts in /etc/init.d/. If your job ends at some point and can be considered a one-off kind of job, then you may want to look at the at command. For instance if I wanted to run the find command at 10PM tonight and only this once I would do: 

If you're not presented with a auth prompt when requesting matching files then you can be certain that the block is not being called due to an earlier break or what have you. 

Not only could you still perform your job function, but probably better than if you were using a Linux destop manager(on the server that is). As a Linux SA for the past three years I have yet to use Linux as a workstation and have never once needed to use X or any window managers on a server. I have seen a few servers running a full blown desktop manager before; always assumed the tech who installed it was clueless. I use mac as my workstation and keep a few virtual centos instances in parallels for trial and error. I run macvim as my editor and the rest is standard. 

I've created two test VMs in VMWare, both Server 2012 R2 Standard. One was promoted to a DC, with the domain name being test2.local. I modified the DNS record on the other server, and joined it to the test domain. Then I did (from an administrative command prompt) and the . I looked in the gpresult.html file and saw a warning on the Default Domain Policy. It says: A fast link was detected (not worrying about now) and "AD / SYSVOL Version Mismatch" on Default Domain Policy. Opening up the Default Domain Policy under Applied GPOs shows that the SYSVOL number is 65535. From what I have been able to gather, this mismatch occurs when there is security filtering and/or WMI filtering in place. I don't think I'm using either one of those, unless they are applied by default. At this point, I just want a clean base that applies the unchanged Default Domain Policy to a machine without any errors. Then I can keep testing the GPO I'm building without wondering where the errors/warnings are coming from. 

NET1 is behind the pfsense with outbound traffic source-natted via 172.16.0.3. NET2 is just routed. The upstream router has a static route that sends traffic destined for 192.168.10.0/24 to the 172.16.0.3 interface. Then the pfsense just routes it on to the correct address. I'm not 100% sure (almost, but not quite), but I think the upstream router does the natting for NET2. What I want to do is redirect any traffic that comes in on the external interface to a specific IP and port on NET2. I'm playing with portspoof. I know I can do it with iptables on the portspoof box itself. This is more an exercise of "I wonder if I can do this..." So the end result will be: packet hits external interface. It's destined for 192.168.10.5:3765. I want to rewrite it to point to 192.168.10.9:4444. It should then go on its merry way to 192.168.10.9:4444. The return traffic should hit the pfsense box and get translated back. Is this possible? I think it is, but I just want to make sure I'm not crazy. 

I have a GPO I've exported from Group Policy Editor on a domain controller. Is there a way to apply this policy to a computer that isn't connected to a domain at all? Just a one-time application of all the settings. Thanks 

I’m a little confused on how containers handle networking. For our normal systems, we’re enforcing things like Disable Source Routed Packet Acceptance, Disable Send Packet Redirects, etc. We usually enforce these via /etc/sysctl.conf, although they are typically backed up in /proc/sys/net/. Does setting network settings like that (things that affect /proc/sys/net) has any effect on individual containers? Does it matter? Does changing it in one container change it in the others running on that host? 

Additionally use the debug option to munin-run. Check the plugin file for any hard coded paths and verify they are correct for your system: 

As warner mentioned (that's becoming familiar), zone transfers are othen denied for security reasons. If the name servers aren't something you have access to you can attempt to discover the most common subdomains of a given domain using one of the popular DNS bruteforce scripts. They work by performing DNS requests against a local nameserver using a user supplied dictionary list. Dictionary lists exist solely for this purpose. 

This means that you CAN just change the progname and it will look for a pam file of the new name. Not a good security practice and I am kinda surprised by this. Maybe someone knows something I don't..since the OpenBSD guys are a much smarter bunch than myself. :p UPDATE 2: Verified that PAM servicename is set to the basename by doing the following from the console: cp sshd to sshd2: 

Does the originating/source IP show up in that log output? If yes does that IP show any valid requests in the http logs? Perhaps a monitoring system of some kind is checking http on your server, since you said it was in consistent intervals. Just throwing stuff out there. 

The best method for reducing apache's memory usage is to move away from mod_php and to something like fastcgi. Each of your apache processes is 15mb or more because of mod_php overhead (most likely). Having php requests handed off to fastcgi will reduce the average apache process size to approx 1mb or so depending on the apache configuration. Since php is now centralized using fastcgi it's memory usage is more efficient and the total amount of memory used by the system should decrease slightly. Another approach would be to place an http server that's more memory efficient in front of apache and have it server static content directly and proxy non-static requests to apache. Nginx would be great for this. As a temporary fix you can also look into decreasing the MaxRequestsPerChild to 1000 or something more aggressive. Since apache processes tend to grow in size as they serve requests this will limit their size by killing them off and spawning new ones. 

In the iptables output make sure there are not any rules rejecting connections before the accept rule. Also, try verifying it's accessible from another computer on the same network before trying outside the network (through the router). 

The nginx plugin may rely on Nginx being compiled with certain modules or log output in a certain format. Is there any documentation page for the plugins? 

As far as I know that will just allow snmpget requests for this data. How do I enable traps based off this info? Thanks! 

If you want to also be able to resolve server.subzone.prod, you'll have to set the option to 3, etc. If anyone knows how to make this work in MacOS X, please let me know; changing is documented not to work (and doesn't) and I can't figure out the right incantations. (Note: I'm hedging my bets here more than is probably warranted. I believe that the option will work on 99% of (non-MacOSX) Unix systems.) 

As a Unix and Windows administrator who does a lot of Unix scripting and almost no Windows scripting, I'd say that it is in part due to the incredible awkwardness of Windows scripting utilities and APIs, and the difficulty (maybe non-obviousness would be a better word) of running things remotely on a Windows machine. I mean, WTF is this? 

just to be sure of the math. Oh, and make sure that your device isn't changing underneath you as you copy it. That would be bad. 

I've copied and pasted what I hope is the relevant config out of my ASA (5525) where this is working for both AnyConnect and MacOS-native clients. I have expurgated it of localized information, so I may have typoed something along the way. I hope I haven't left anything out. (Look out for comments.) 

Depending on how your program reads the file, you might be able to use a named pipe, but it's very likely that it won't work; named pipes are not random access. 

where the backslash ("") tells the shell to accept the following character literally and not do any expansion on it. But there are some instances where you end up having to do it the multi-string-concatenation way, not that I can actually come up with an example right now. 

Generally, your shell will prefer its own builtin over any external program. One notable gotcha in regards to this is when you've defined an alias for the command. 

freeSSHd sounded like a great idea until I tried to use it. It seems to call home frequently, and often errors out on user console login, generating a popup. In order to enable a number of useful features, it requires console interaction, which creates a tray icon that allows users to kill the service. The Windows authentication is a nice feature, but there's no way to allow a Windows group, only individual users. I eventually discovered that cmd.exe via PsExec works as well, if not better, and I quickly uninstalled freeSSHd. 

I have a need for a free rsync-like tool for Windows (very preferably with some sort of delta encoding) that supports synchronizing Windows ACLs and can copy open files, probably via VSS/Shadow Volumes. (I have zero budget for this, as it is a one-time project.) Many tools come close, but fail on one or more of those accounts. I don't have a problem with a multistep procedure, but I want to avoid multiple steps to the sync. That is, I don't want to have to sync files and ACLs separately, as I don't trust that this won't get out of sync. I found a tool, , that is standard under Windows 2008, that allows me to create and mount a VSS Shadow Copy, so a tool that can do delta updates of changed files while supporting Windows ACLs would be sufficient. The server I'm working on is Windows Server 2008 (not R2).