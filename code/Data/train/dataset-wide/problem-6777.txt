When a degree wh-word (e.g., E. how, G. wie, F. que, Sp. qué, It. come, Port. como, etc.) grades an adjective, in some languages (= the 'Pied-Piping Type') the adjective must accompany the degree word as the latter gets fronted into the Focus position of the CP layer), whereas in others (= the 'Non-Pied-Piping Type') only the degree wh-word gets fronted, whereas the adjective remains in situ. Thus, for example, in English we say How beautiful you are __, Rome, when it rains! not * How you are __ beautiful, Rome, when it rains!, whereas in Italian they would say Come/Quanto sei bella, Roma, quando piove!, not * Come/quanto bella sei, Roma, quando piove! Although I had occasionally come across such expressions in isolation in various languages and was aware that the non-Pied-Piping ones should not really occur, since they violate what used to pass as a would-be universal principle (‘the Left Branch Condition’), I had never stopped to think about how strange the geographic distribution of the two options is and how heterogeneous the languages having each type of construction are. Thus, to consider just some of the major languages spoken in Europe, to the ‘pied-piping’ group belong (at least) a) Germanic languages like English, German, Frisian, Dutch, Swedish (but not Danish!), Norwegian, Icelandic, and Yiddish, b) Latin, and Latin-derived Romance languages like Spanish, Catalan, Corsican, and Romanian, c) Slovenian, d) Modern Greek, e) Latvian and Estonian, and f) Finnish, if I am not misinformed. To the ‘non-pied-piping’ group, on the other hand, belong no less heterogeneous languages, including, from West to East, a) Romance ones like Portuguese, Galician, French and Italian, b) Danish, c) Lituanian, and d) both western and Eastern Slavic languages like Polish, Czech, Slovak, Serbian, Bosnian, Croatian, Bulgarian, Russian, Belarusian and Ukrainian. Neither list is exhaustive (I do not know what happens in this respect in Irish, or Hungarian, for example), but a simple glance at the map of Europe shows how messy that distribution is, particularly in its Western half. My question, then, is this: Does anybody here know of any coherent theoretical account of the nature of the apparent ‘parameter’ that explains the possibility of the ‘non-Pied-Piping' construction (contra the 'LBC' principle), the strange geographic distribution, and the internal typological heterogeneity of the resulting two groups of languages? Thank you! 

Under the traditional interpretation of the terms 'transitive' (= capable of taking a 'direct object') and 'intransitive' (= not allowing a 'direct object'), the answer is 'Yes'. Spanish is a case in point. In Spanish sentences parallel to It pleases me, such as (1) or (2) 

I assume you are interested only in 'existential' (unstressed) uses of there. If so, here is my answer to your question (which, by the way, starts from a wrong presupposition): Contrary to 'dummy it', which is inherently singular (and categorially a CP!), 'dummy there' (categorially a DP) has no inherent number attribute value. Of course, it is a 'nominal' and must have a number attribute, but it is an unvalued one until there is 'associated' with some other nominal in the same derivation that does have a properly valued number attribute. 'Association', by the way, is just chain-formation in this context, and, of course, members of chains must agree in number and other grammatical features (e.g., they must share just one theta role, just one case, a single referent, etc., etc.). Just this fact largely yields the explanation you are looking for. Hence, since, surprisingly, none of my - very competent! - predecessors has referred yet to a special type of cases that are crucial to your question, let me do so myself: in There is/are a student/several students waiting for you in your office, the 'chain' is formed by there and the DP a student/several students (subjects in Spec VP!), where there supplies the chain with [Case: nominative], whereas a student/several students, which have no valued Case feature in Spec VP position (= VP-Internal subject! position), supply it with [Number: singular/plural],[Theta-Role: Agent], etc. It follows that 'dummy there' cannot form a chain with any other DP in that sentence (i.e., you or your office), because, if it did, a Case conflict (not a number conflict!) would immediately arise: the hypothetical chains {There-you} or {There-your office} would be illegitimate because both would have two valued Case attributes (i.e., nominative from there, and objective/oblique from the prepositions for/in, respectively). Also,'dummy' there can never be associated with an object either (contrary to what you claim in your question!), because, if it were, the resulting chain would also immediately contain two incompatible valued Case attributes (i.e., nominative from there itself and accusative, dative or oblique from the DP functioning as direct, indirect, or prepositional object, as the case might be). In sum, 'dummy there' can only be 'associated' with a theta-marked DP containing no valued Case attribute. That includes subjects in Spec VP, as in my examples above, and complements of non-case-assigning verbs like 'existential' be, eventive arise, etc., but never objects of transitive, ditransitive, or prepositional verbs. As you can see, it is simply not true that with dummy there agreement 'comes from the object', as you assumed, and, so, properly, there is no fact to be explained, :-)! 

An obvious difference I see (or, rather, one Ruth Kempson saw in a 1977 textbook of hers), especially from a cross-theoretical perspective (i.e., leaving terminological issues aside as much as possible), is that 'anaphora' is an intra-linguistically determinable relation, whereas 'co-reference' necessarily requires access to 'extra-linguistic' information. By definition, co-reference always implies 'identity of reference', whereas anaphora does not (I ignore here how the term anaphor - not anaphora - is used in Principle A of Chomsky's Binding Theory). In other words, the relation called anaphora may (but need not) hold of expressions that are supposed to have the same 'sense' (either in themselves, or only via context-dependent synonymy) but may still have different referents (and usually do), whereas co-reference does not depend on 'identity of sense' at all, but, of course, does strictly require co-referential terms to refer to the same extra-linguistic entity. This is most obvious, perhaps, in the use of one in John has a BMW and his girlfriend has one, too, where two different BMW's are being referred to, although similar examples can be built with all the other pro-forms (i.e., so, do, do so, that/those, mine, yours, etc., cf. John finished his thesis in 1990 and Bill did so a year later, Your thesis is better than mine, etc.). On the contrary, co-referential terms, of course, may (but, again, need not) have completely different 'senses', and yet, by definition, they must refer to the same extralinguistic entity. Thus, in My wife is sad because she misses her father, My wife and she have different 'senses', but may still have the same 'referent' (and that's how the sentence is interpreted by default, in the absence of further context); on the contrary, in She is sad because she misses her father, both tokens of she cannot but have the same 'sense', but they still may (or may not) be co-referential, although, in the absence of further context, they will be interpreted as co-referential; in John lost his temper, finally, John and his must be co-referential. In view of that, you may expect huge differences in the resources and computational strategies respectively needed to identify 'anaphoric' and 'co-referential' expressions; typically, determining whether co-reference, an extra-linguistically determined property, holds or not between any two expressions will require much more 'pragmatic' information ('knowledge of the world') than identifying pairs of expressions in the strictly intra-linguistic relation of 'anaphora'. 

Given the kind of syntactic framework Carnie's textbook is written in, the NP/DP Fredrick cannot be an argument of (nor, consequently, receive a Theta role from) consider for two reasons: 1) because that would leave the one-place predicate foolish without the only (= subject) argument it must have, an obligatory Theta role of foolish would not be assigned to/discharged by anything, and the Theta Criterion would be violated, and 2) because that would also require consider to be a three-place predicate, whereas, of course, consider cannot take, and assign Theta roles to, three arguments, as 'arguments' are defined in both logic and P&P linguistics (note that the 'secondary predicate' or 'predicative' that alternative syntactic theories would claim to exist in Wilma considers Fredrick foolish would not be a third 'argument' of consider, but a 'predicate' of Fredrick). As to why Fredrick must become the subject of the passive clause (iii) Frederick is considered t to be foolish, the reason is that, otherwise, Fredrick would receive no Case and would violate the Case Filter. That, in its turn, follows from the fact that passive participles like considered cannot assign accusative Case (nor any other Case) to their complements. When the complement of consider is a clause, as in (iii), the clause itself causes no Case Filter violation, as clauses do not need Case, but an NP/DP object does need accusative Case. If consider is not made passive, as in (ii), Fredrick can, indeed, receive accusative Case from it, no Case problem arises, and, therefore, Fredrick need not move from its 'deep' position of subject of the infinitive to be foolish. What's more, in such circumstances, Fredrick cannot move at all unless it is into a position in which no Case is assigned, e.g., the position of Topic, as in Fredrick, Wilma considers __ to be rather foolish. However, if consider carries passive inflection, as in (iii), either Fredrick moves into a position in which it can receive some Case from another Case-assigner (here: nominative, from the Tense inflection of the higher verb is) or the sentence violates the Case Filter and is doomed to fail. Finally, the Case requirement for NP/DPs also explains why * Fredrick to be foolish is considered ___ (by Wilma) cannot be a sentence of English: if the whole infinitival clause is 'moved' and made the subject of is considered by Wilma, the clause itself, of course, raises no Case problem, as explained, but its subject, Fredrick, still remains Case-less and violates the Case Filter, since a non-finite Infl cannot assign Case to an overt subject like Fredrick. If the subject is phonetically null, i.e., PRO, of course, no such Case Filter violation arises - either because null subjects require/support no Case, which seems reasonable, or because non-finite Infl assigns Null Case to PRO, depending on the theory of Case each Chomskian linguist adopts - and the result is well-formed, as in PRO to be foolish is considered __ by Wilma a fatal flaw. 

In English, as in German, Spanish, French, or Italian, non-lexicalized noun pre-modifiers cannot be 'right-branching' (i.e., they cannot carry either complements or modifiers of their own placed between the modifier's own head and the modified nominal). For example, noun phrases like "*a full of people room", "*a containing private documents briefcase", "*a satisfied with his work teacher", "*a similar to mine academic career", "*a published in the UK book", "*A born in 2005 Japanese girl", "*a near Heathrow airport luxury hotel", etc. are all syntactically ill-formed (although perfectly transparent from a semantic point of view). That restriction can be shown to follow from Predication theory, Kayne's Antisymmetry Hypothesis and other would-be high-level principles of Language, and at one stage I suspected it could be a sort of 'universal'. However, I now know that apparently parallel constructions are the rule in Mandarin, somebody told me once, unfortunately without offering details, that they are normal in Russian and other Eastern Indo-European languages, and, so, I suspect they may well exist in still other languages totally unfamiliar to me. If somebody here could supply me with examples of well-formed parallel examples from other languages and add careful glosses to help me understand them and check that their structures are really parallel, I would be very, very grateful. Thank you all in advance. 

If the title of your question means to you what it literally means, the answer is, plainly, 'NO'. Just by observing written texts, without any access at all to the extralinguistic referents and contexts of use of the written expressions, you would be absolutely unable to determine either their 'sound' or their 'meaning' (/use), and you would have no grounds at all to call that set of inscriptions 'English' or anything else, or even less to say that you had learnt English. Of course, you could study the distribution of each of the symbols figuring in your necessarily finite collection of inscriptions and, with a lot of patience, eventually establish an 'alphabet' (an exhaustive list of the symbols in them) and a set of 'construction rules' for the observed subset of L, i.e., a syntax (in the broad sense) in which you specified the observed combinations of 'letters' (a sort of 'phonotactics' of L), an inventory of (observed) recurring 'morphs' (bits of 'words' observed to systematically recur in identifiable co-texts), an inventory of observed 'words' (assuming the inscriptions contained punctuation marks, e.g., spaces, something to separate sentences), and you could, on distributional grounds, establish a finite set of 'syntactic categories' of words, and, since your corpus would be, by definition, a closed one, a finite set of recurring patterns of combination of categories of 'words' into 'phrases' and 'sentences', i.e., you could surely reach a fairly detailed set of hypotheses about the 'syntax' of L in the narrow sense, too. But, without access to the extralinguistic correlates of such 'words', 'phrases' and 'sentences', your 'language' L would have no 'semantics', would thus not even be a 'semiotic' system (in Morris' sense), and, of course, you would not have learnt a human language at all, because what defines human languages is their capacity to 'recursively' correlate indefinitely complex 'forms' with their corresponding 'meanings' (= in Chomsky's terms: infinite use of finite means). On the contrary, by definition, all the written expressions of L you could collect, however large your 'corpus' were, would only constitute a tiny fraction of what the alphabet and the rules of construction would be able to 'generate'. But, above all, your L would not be a system of 'signs'. Hence, even if you learnt your 'alphabet' and your 'construction rules' to perfection, even if you subsequently generalized and extrapolated your 'grammar' in order to 'generate' a much bigger complementary set of non-observed, but grammar-compliant and hence predictable new possible 'inscriptions', you would not be entitled to say you had learnt English (or any other human language). The well-known cases in which such sets of inscriptions have been 'deciphered' could occur at all because the decipherers did eventually obtain at least indirect access to what the inscriptions and their parts 'referred to' and 'meant' (with all the caveats that philosophers of language since Quine have established in this respect). Without such direct or indirect access to the extralinguistic correlates of 'inscriptions' and their components, it would have been imposible to even decipher the observed expressions, let alone learn the underlying languages. 

The use of the symbol ‘S’, standing for ‘sentence’ - originally, since Roman times, ‘a coherent phrase expressing a complete ‘thought’ or ‘proposition’’ - has largely been discontinued in technical accounts of syntactic structure written within Chomskian generative grammar because, at one point in its development, 'S', and its associated rule, stopped satisfying the requirements of a new theory that Chomsky considered it necessary to adopt in order to explain, instead of merely describe, the syntactic constructions of English and Human Language in general. The long story is really long and complicated, but the gist thereof is about this: Whereas early Chomskian analyses of S were trivially transparent 'symbolizations' of the long established view of the 'sentence' as a combination of a 'subject' and a 'predicate' (categorially: an NP and a VP, respectively), by the late 1960's Chomsky had realized that (context-free/sensitive) 'phrase structure rules' of the form X > Y (for Y = any non-null sequence of symbols) could describe, and therefore predicted the existence of, just about any sequence of syntactic elements. As that included infinite sequences never attested in any human language, such rules were obviously too powerful and failed to offer satisfactory explanations of what was or was not possible in human syntax, and Chomsky decided to first constrain their Y side and then replace them altogether with a set of simple principles meant to predict and formally explain what was actually found in the syntax of human languages. As a result, in 'Remarks on Nominalization' (1970) he proposed the 'X-Bar Theory' of syntax, a set of ‘principles’ (not called ‘rules’ anymore) according to which all syntactic constructions must be 'headed' (= 'endocentric', in Bloomfield's jargon), with additional constraints on what the non-head could and could not be and on how headed structures can recur inside other headed structures that we can ignore for current purposes. According to the X-Bar 'principles', then, any phrase of category XP is ultimately a (possibly recursive) 'expansion' of a head category X - where X ranges over the set of minimal syntactic categories, which, at the time, largely reduced to the 'parts of speech' already established by traditional grammarians in the preceding two millennia, i.e., N, V, A, Adv, P, Art, etc. Thus, an NP was ultimately an 'expansion' of N, a VP an expansion of V, an AP an expansion of A, and so on. The ‘endocentricity’ idea, of course, was not new (Bloomfieldian doctrine aside, it is occasionally found much earlier, e.g. in the treatment of adjectival modification in John Wallis’ Grammatica Linguae Anglicanae, published in 1653), but X-Bar Theory as a whole and its ambitious intended scope were new, and very important insights bound to stay in Chomskian linguistics for more than thirty years. X-bar theory worked reasonably well for core categories like NP/N, VP/V, or AP/A, but ran into serious obstacles with coordinate constructions, and, above all, with the syntactic structures par excellence, i.e., 'sentences': obviously, under the early TGG analysis of 'S' expressed by the popular rule S > NP + VP, S was non-headed ('exocentric'), and a flagrant exception to the XP = [...X...] pattern that X-Bar Theory predicted all syntactic constructions should comply with. Therefore, if X-Bar Theory was to stand, as desired, it was necessary to find a way to reanalyse the 'sentence' as a 'headed' construction, and, since S consisted of only two 'immediate constituents', the NP (functioning as the 'subject') and the VP (functioning as the 'predicate'), only two possibilities seemed available: either S was an expansion of the NP (and ultimately of N, according to the X-Bar principles), or the head of S was the VP and S was an expansion thereof, and ultimately of the finite verb. The former view (although defended, in the 1920’s and 30’s by no less a grammarian than Jespersen) was considered absolutely untenable for many syntactic and semantic reasons I cannot review here. There, thus, remained the latter option, i.e., that S was simply an expansion of the VP (and, ultimately, of the finite verb). In spite of certain semantic obstacles (VP, the predicate, names a ‘function’ in Frege’s sense, whereas S, like NP, names an ‘object’ = 1/0, again in Frege’s terms), that the finite verb was the 'heart' of the sentence had been the preferred view in the preceding two and a half centuries of ‘traditional grammar’ and remained so among late 19th century German linguists and early 20th century pre-Chomskian European 'structuralist' and 'functionalist' grammarians like Guillaume, Tesniére, etc. Unsurprisingly, therefore, between 1970 and 1979, TG grammarians, too, seriously considered adopting the V-centered view of the sentence as the obvious way to make S comply with X-Bar Theory, and, as a matter of fact, one of the most important monographs on X-Bar Theory ever published, Jackendoff's X' Syntax (MIT Press, 1977), did analyse S as an extended VP phrase (V''', in his notation). The fact that in many languages the subject seemed dispensable, whereas the finite verb was not, seemed to support that traditional view, and until about 1979 several other influential Chomskian linguists proposed similar, V-centered, analyses of the sentence. Or rather, of the ‘core sentence’ (= the 'predication'), since, as early as 1972, Joan Bresnan had already offered compelling reasons to analyze real speakers’ sentences as structures containing S preceded by a ‘complementizer’ category C lodging certain subordinators and expressing the sentence’s crucial ‘illocutionary’ properties (assertion, interrogation, command, etc.), a view that, under X-Bar theory, first suggested treating sentences, rather sloppily, as ‘S-bar’ phrases with the internal structure [Comp + S] (e.g., in Chomsky’s Lectures on Government and Binding 1981), or as Comp + V’’’ structures (in Jackendoff’s earlier work), and eventually as CPs, i.e., as projections of the C category (e.g., in Chomsky’s later, and extremely influential book Barriers, 1986). That is the source of the sentential tree diagrams dominated by CP that figure in most introductory syntax textbooks even nowadays. However, as Comp is invisible in independent declarative sentences, and those are the obvious sentential structures to start explaining syntax with to beginners, such textbooks often omit representing the CP segment of the projection and offer informal sentence trees dominated by the old symbol S or by the categories that eventually replaced it under the pressure of the X-Bar principles, either an uncompromising IP, as in Chomsky’s Barriers, or the more informative, and controversial, symbols (TP, AGR-P, etc.) that eventually replaced IP. To justify the original IP idea a bit more, though, by 1980 the study of verbless predications in English and other languages had progressed enough to lead Chomsky to claim that, at the UG level, the grammatically key element of a 'predication' was neither the subject nor the predicate, but something more abstract connecting those two elements, a view that logicians and philosophers of language could not but sympathise with (due to the existence of the 'predication paradox', a problem we need not go into here). Interestingly, in the Port Royal Grammar, that 'linking' element had been claimed to be the 'copula' be, but that ‘logicist’ analysis was arbitrary, at bottom, and never caught on in mainstream grammatical theory, so, for a long time, no alternatives were proposed. When the study of ‘universal grammar’ was seriously resumed by Chomsky and his disciples, though, the problem of the ‘linking’ category re-emerged, and in certain cross-linguistic TGG analyses published between 1979 and 1981, the possibility was seriously contemplated that the crucial element of 'predication' might be the abstract 'Aux' node that Chomsky himself had introduced in his earliest work on TGG (notably, in Aspects, but, in a less structured way, already in his 1955 LSLT thesis). As, within 'Aux', (English) auxiliaries all seemed optional, the key ‘linking’ component could only be 'tense' ('c', in Chomsky's earliest work), which, at the time, pointed to tense as a likely candidate to sentence-head status. That hypothesis, however, raised the obvious problem that many predications were non-finite (or even non-verbal) and could hardly be analysed as 'projections' of a syntactic element they did not have. The solution that Chomsky eventually adopted from his 1979 Pisa lectures (later Lectures on Government and Binding, 1981) onwards within the framework of the then new 'GB' theory was to analyse sentences as 'projections' of an abstract 'functional' head INFL instantiated in different ways, i.e., mainly as a tense morpheme (in finite clauses) or as 'zero' or other 'verbal' inflections (in non-finite ones), but also as other 'functional' morphemes (e.g., certain preposition-like words, etc.) in other constructions attested in diverse languages. That much will hopefully suffice as to the origin of the IP analysis of the predicational core of sentences. Analysing S as a projection of Infl according to the 'rules' IP/I''> NP + I' and I'> I + VP (alternating with other 'predicates' like PP, AP, NP, etc.) solved the 'endocentricity' issue that sentential constructions had initially raised, kept and reinforced the X-Bar principles as one of the key 'modules' of the ensuing 'Principles and Parameters Theory' of the mid 1980's, and, at the same time, allowed for great flexibility in analysing predications, which is why many introductory textbooks of the 1980's and early 1990's adopted the IP analysis of the core sentence, if only as a pedagogical solution. However, as P&P theory developed during the mid and late 1980's and the number of languages analysed within that framework increased, INFL inevitably revealed itself as a simplification, a cover term for an aggregate of different inflectional features, in particular, 'tense' (or lack thereof) and 'agreement', and, as such features could be neatly distinguished in 'phonetic form' in many languages, and could even appear separately in some, it soon became inevitable to insert them in syntactic trees under different category nodes. By 1988, largely due to Mark Baker’s influential work on the correspondence between verb morphology and syntactic projection, the Tense and AGR components of Infl were already distinct 'nodes' in all serious work on P&P syntax, and, to the extent that, in view of the morphological structure of verbs, AGR-S arguably is structurally higher than Tense, later P&P work by Chomsky and others between 1989 and 1991 started including analyses of the predicational core of the sentence as an AGR-sP(hrase), although eventually Chomsky himself retrenched into a simpler view that reduced IP to TP, for technical reasons that need not concern us here. That was only the beginning of the ‘split-INFL’ analysis, though. At about the same time or soon afterwards, evidence was offered that certain languages had not only subject, but also object agreement, and, for parallel reasons, the Infl 'segment' of the sentential projection quickly gained an 'object-AGR(eement)' node. Also, from about 1989 on, the sheer internal logic of late P&P Theory gradually led to the establishment of many other such 'functional' heads inside the 'core sentence', both within the 'INFL'/AUX area (e.g., 'polarity' and several 'modality'-related heads) and above it, in the area traditionally associated with Joan Bresnan's (1972) Complementizer and the expression of a sentence's 'illocutionary' features. In Chomsky’s writings on GB and P&PT, e.g., in Barriers (1986), Comp was still a single head projecting a CP than contained IP as a complement, but, in work by Rizzi, Cinque and others about a decade later, the original Comp node had itself split into a C in the strict sense, plus, perhaps, an additional Force head (with its specifier), a Topic ‘shell’ (= a Top head plus a suitable specifier), a Focus ‘shell’ (again, a Foc head with its own specifier), and, in Cinque’s influential monograph on Adverbs and Functional Heads (1999), several attitudinal and epistemic adverbial heads and specifiers. [Parallel developments took place in the analysis of NP-structure starting with Abney’s (1987) influential 'DP Hypothesis' soon followed by very delicate DP internal structure developed in Cinque’s cartography programme, among others]. By the late 1990’s, therefore, CP was clearly just a convenient cover term for a number of hierarchically ordered syntactic projections - including ForceP, TopP, FocusP, etc. - that syntax textbooks chose to specify or not depending on their pedagogical policies and on their authors' expository strategy in each textbook section, which explains why the syntax beginner may still easily encounter apparently conflicting analyses of ‘the sentence’ - and, of course, different symbols, like CP, ForceP, IP, TP, etc. - at the top of sentence trees even in contiguous sections of the same textbook. The story of the structure assumed for what we must now informally call 'the sentence' since the early 1990’s, in sum, is so complicated that it is virtually impossible to summarise, particularly in non-technical terms, but, in response to the OP's specific question, I would like to add, by way of summary and conclusion, that the increasingly detailed hierarchical structure that research has revealed within what we used to call the ‘sentence' has converted that term into an informal, pre-theoretical, label, useful enough to the extent it invokes the traditional idea of a ‘sententia’ (= a ‘complete’ and coherent expression that ‘makes sense’) and the layman’s common sense view thereof, but with no real significance in contemporary syntactic analysis of a non-pedagogical nature. That, of course, is a problem for beginning syntax students, who must remain alert to detect when their textbook is just omitting irrelevant structure its author accepts and assumes to be there, if hidden, and when it is opting for an alternative analysis that denies the existence of that extra structure, but, in case the OP feels she is being irresponsibly played with, let me quickly add that the possible uncertainties this state of affairs may induce are far from mere ‘collateral damages’ caused by a teacher’s or a textbook author’s pedagogical strategy. The dire fact is that, at the time being, even professional syntacticians do not really agree as yet as to the fine details of ‘sentential’ structure, including such key aspects thereof as the number, labels, and hierarchical disposition of sub-sentential phrases, the syntactic and semantic properties associated to each of the by now innumerable heads and specifiers postulated, or crucial semantic issues like what level(s) within current ‘sentence’ structure are ‘propositional’ (the ‘saturated’ VP?, the higher vP node?, PolarityP?, TP?, Agr-S-P?, CP?...), to name but a few and avoid going into the movement-related issues that follow from such uncertainties. Assuming that a syntax textbook author is competent and well informed, in sum, what label he chooses to use at the root of a ‘sentential’ tree is often just a matter of pedagogical convenience and need not worry learners once they are aware of that trivial fact. What must trouble both learners and syntax teachers or professional syntacticians, but particularly the latter, is that too much of the highly articulated hierarchical structure currently assumed for sentential phrases has never yet been properly substantiated in either syntactic or semantic terms. That, of course, makes life very hard for those of us who teach the same students full-length courses (as opposed to mere lists of selected 'topics') in both syntax and semantics and like to start our teaching by promising them not to consider any structure or machinery that cannot be set in a transparent, compositional, relation with empirically testable aspects of the meaning of sentences and their parts. At present, such promissory notes are, to the best of my understanding, impossible to redeem unless one retreats into a much simpler view of sentence structure than is currently assumed in professional journals, and, to that extent, the decision of many textbook authors' to simplify things and analyse sentences in terms of just CPs and IPs, for example, is understandable.