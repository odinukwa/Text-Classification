We've got a customers table (Who doesn't?), containing many records that are, from a business perspective, duplicates. I've been able to create an SSIS package to perform fuzzy grouping, and report on potential duplicates. Now, suppose I want to do this kind of analysis just as somebody is entering a new customer. The idea would be to perform a fuzzy lookup on customer name (and possibly some other basic info like postal code), and show potential duplicates prior to proceeding to the customer creation form. The obvious problem here is that the fuzzy grouping and lookup components are part of SSIS. If I wanted to run those on-demand, I'd have to do something insane like putting the search terms in a staging table, running the SSIS package, waiting for it to complete, and fetching the results from an output table. It would be slow, painful, and have severe concurrency problems. So, the other idea was to use full-text indexing. In experimenting with it, it looks like it won't be suitable. It can't catch subtle misspellings of customer names, or names that differ in "Company" vs. "Corporation" vs. "Co.", or "Anderson" vs. "Andersen", and other such variations. Is there something that will allow for the flexibility of fuzzy grouping/matching from T-SQL? I can tell a fuzzy lookup to save the tokens, but it looks like I would still have to reimplement most of the matching algorithm to make use of them. 

If the trigger is on the table Products, and is writing to the table ProductAudit, then you should be okay, as long as a table named ProductAudit exists at the time the trigger runs. The trick is making sure that you don't have audit rows being written by the trigger after you think you've already moved all the good rows from ProductAudit to ProductAudit_TEMP. The simple way to ensure that doesn't happen would be to set the database to something like RESTRICTED_USER mode and make sure nobody else can connect. If you need to do it without taking the application down completely, then you might need to do the whole thing in a transaction involving some TABLOCK, XLOCK, HOLDLOCK hints, or temporarily modify the trigger to write directly to ProductAudit_TEMP. 

Summary: stick with , , or , and make sure you're using the one that returns what you actually need. 

So you just need to fetch the last auto-increment value that was inserted? There are a couple of ways to do this, but they're all pretty simple. Query: php mysql: $URL$ php mysqli: $URL$ Both of those links have some example code that looks pretty easy to follow. 

I've got an Analysis Services database that's used for reporting on email activity. It mainly counts incoming and outgoing messages, who they're to and from, etc. Fact table: 11,367,910 rows Address dimension table: 386,015 rows To-address fact table: 21,303,290 rows (used for many-to-many intermediate measure group) Date dimension table: 9,132 rows It's a simple structure, but there's a lot of data in it. The two measure groups have 6 yearly partitions with varying numbers of rows in them. The whole thing takes about 30 minutes to process fully. But that's not my problem (at least I don't think it is). Seemingly randomly, the scheduled processing task will fail with the error The line number can vary, but the error is the same. I've tried hunting around, but I can't find any info on what this indicates. Anybody else run into this error? 

I think the answer is, like with many things IT, "it depends". A massive ERP database with lots of sensitive company and customer information? Probably not (both for security and performance reasons). A departmental 5 MB database with an Access front-end that tracks contributions to the donut and pizza funds? Not going to make a whole lot of difference, at least for read-only access. Granted, the first example is much more common than the second, but these are differences you should be aware of if you're in charge of making these types of policy decisions. But on the flip-side, it's amazing how quickly a 5 MB donut-and-pizza-fund database can scope-creep its way to a 50 GB part-numbers/customer-credit-card-numbers/who-knows-what-else database if you let it. 

Alternatively, you could assume that values in which the match pattern appear earlier are "better" matches. 

Since all your matches have to match the LIKE pattern in order to be included, the simple thing to do is assume that shorter values for the column you're matching are "better" matches, as they're closer to the exact value of the pattern. 

This one's got me a bit perplexed. How might one determine what process/program is making file-level backups via VSS/VDI and the SQL Writer service? I've tried using Profiler to trace backup/restore events, and that just shows me SQL Writer itself creating the backup events. I don't see anything in the Windows event log indicating what's talking to SQL Writer, nor does SQL Server put anything meaningful in its own error log. I've got a situation where something is making snapshot backups and breaking the differential chain, but we're not sure what the culprit is. It's undoubtedly some kind of backup agent, or a virtualization assistant that talks to SQL Writer to allow for VM snapshots, but we need some way to pin it down. 

Honestly, the easiest way to do this will be to upgrade your local instance to 2008 R2, or install a second local instance of 2008 R2. Then it's just a matter of making a database backup from the server, copying it to your local machine, and restoring it on a local instance. If you really need to transfer the database to 2008, for development or troubleshooting on that version, you'll have to do all sorts of unpleasant things like scripting out all database objects (including indexes, security, etc), transferring the data (bcp or SSIS), and hoping that your database isn't using any features that are new to 2008 R2. It's much less hassle to just upgrade. 

I've had to resort to a bit of guesswork, but as far as I can tell, I haven't done anything overly permissive. I started with a non-privileged test account that's a member of the AD group I created, and I was able to navigate as far as the FileStream directory for the database (), but nothing was visible in there. So, I tried the most obvious approach first: