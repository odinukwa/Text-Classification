You already have two answers. One says yes, and the other says no. Both give you arguments to support their view. Maybe the problem is with the question. Your question is, regarding English grammar: 

Borrowing can also go back and forth. The word "flirt" is used in French, both as a noun (for the act or for the person) and as a verb ("flirter"). It was clearly borrowed, a few decades ago, from English. But in English, it is said (among other possible origins) to possibly derives from the French "conter fleurette", borrowed a long time ago (16th century). I will not get into the etymology of "conter fleurette", since a quick scan of the web shows many not so compatible views of it. 

This sounds like one of the series of papers by Kirby and/or Smith; e.g., Smith, Kirby, Brighton 2003. They just call it 'iterated learning'. 

For many languages, SUBTLEX is considered a good source for realistic frequencies. There is an Italian version, SUBTLEX-IT, available at $URL$ (Crepaldi, Keuleers, Mandera, & Brysbaert, 2013). 

In American dialects, the sound in 'hat' is generally /æ/, not /a/. The use of /i/ (meaning /ai/) in the 'fight' vowel would be too long/close; you probably do not pronounce 'might' exactly the same as the phrase 'ma eat'. The vowel in 'might' is shorter/opener, and so /ɪ/ (meaning /aɪ/) is appropriate. (See $URL$ 

In Appendix A and Appendix B of Rastle, Davis, Marslen-Wilson, Tyler (2000), there are listings of the stimuli used. You want the '-M-S+O’ condition (i.e., not related morphological or semantically, but similar orthographically). There are 48, I think, so maybe that will be helpful. They sometimes differ by more than 2 characters. 

I do not have the linguistic competence of the users who commented the question. So I only took the question as asked, and looked also at wikipedia, And the concept, as presented, looks very arbitrary to me. Here are examples involving the verbs open and float. 

I recently visited Jordan and noticed that many mosaic are commented with included text. The text seems mostly ancien greek alphabet, but it also contains non greek characters such as C, obviously standing for Σ, at least for some words. Given I forgot most I ever knew of greek, I mostly looked at city names. For example yous have the cities NϵAΠOΛIC and ΔIOCΠωΛIC on two mosaics from the 8th century in the Church of St. Stephen at Umm al Rasas. Note also the different spellings for city (POLIS). Similarly. on the 6th century mosaic map of the Holy Land in the Church of St George at Madaba, there is the city NIKOΠOΛIC. Look below Bethleem (number 2) on the large this map: This cannot be the cyrillic alphabet, which does not contain ω and was created in the ninth century anyway, its authors (saint Cyril and Methodius being born between 815 and 827). This seems to indicate and evolution in the use of greek characters in that part of the world. Unless the use of C was justified because the ending sigma ς is not easily rendered in mosaic. This does not seem too likely as mosaics are pretty good at that level of details (look at the λ for Bethleem). Can someone give some light on this. Was there an evolution in writing. How much of it was already done when the two brothers created the Cyrillic alphabet, and would it explain some of the new characters in it? Also worth noting on the third mosaic (map of the Holy Land) is the greek spelling of Bethleem, with two different epsilons: BHϑλϵEM. And there is also the frequent mixing of lower and upper case (if I may called them that way, given the time). Was that common? 

TextGrids are still just text files. The simplest thing for a non-scripter is probably to open it in your favorite editor and find-replace them there. Try TextWrangler or SublimeText 3 if you don’t have a good one you like.(I’m not sure that this is even the correct StackExchange for this, or if the tags ‘phonetics’ and ‘computational-linguistics' are relevant.) So, the tiers probably contain intervals like: 

While a dedicated script (sed, awk, Python, …) would be better if this is a common operation, you can indeed do this in e.g., SublimeText. One method is to first select all the ‘text' lines (e.g., Find All with '.+text.+' is fine); then, Replace All '\\n' with ‘ ‘. This method assumes that you can't simply do a global replace of '\\n' with ' '. I think this latter, simpler option should be tried first, though. 

Former answer It seems obvious to me that linguistics cannot be limited to natural languages spoken by humans, if only for one good reason: analysing extreme cases is a good way to test theories, and to understand the nature of a topic. Understanding why the bee communication system (to avoid the word language) is or is not a language is part of understanding what language is. Another interesting aspect is that human communities develop sublanguages that are specific of groups or activities. My own experience in Legalese writing, for example, was an interesting one, both regarding syntax and semantics. My impression is that such sublanguages nearly form a continuum from usual natural languages to such formal creations as programming languages, with scientific languages as intermediaries. Another topic worth studying is the evolution of scientific languages, as it is intimately linked with the scientific activity. The evolution of mathematical concepts and understanding has led to changes in mathematical notation and expression. And conversely, these changes lead to new understanding of mathematics as the language becomes more perspicuous with respect to the semantic domains adressed. This is also true of other hard sciences. An interesting aspect of these remarks is that they are more of a diachronic nature than of a synchronic one ... possibly because the synchronic wiew is linguistically poorer, or maybe too technical. I would suspect that there is much to be learned too from the diachronic analysis of Legalese (and this was probably done). The case of programming languages is a bit special. They have clearly a linguistic aspect as they are also intended for human communications. Programmers learn (often the hard way) that they should write programs so that they can read (and understand) them a month later. The problem with programming languages is that they are supposed to be also used by machines that can interpret correctly only very constrained expressions. Actually they could do a lot more, but we do not trust them (or ourselves) to use more complex means of expressions with machines. To take a simple example, people are reluctant to use ambiguous syntax to communicate with machines, even though they ae very good (much better than humans) at detecting ambiguity and asking for more precision. I would expect that the development of formal tools for checking proofs and programs, such as the Coq system, will ultimately help to develop programming in a less formal way, with a more natural linguistic expression, akin to the way scientists are now communicating. The formal part would be taken care of by the programming environment. This is only predictive, and not to be too much relied upon, but I think there may be much to be expected in the longer term from the combination of very powerful formal system combined with some AI to provide informal context. This said, I know of at least one work, actually quite old, that is an attempt to analyze programming languages from a linguistic point of view: 

It is often helpful to avoid voiceless sounds as much as possible in prosody stimuli, to get a clearer pitch track. For example, "Mary will win" has no voiceless sounds (from: Pierrehumbert, J. (2000). Tonal elements and their alignment. In Prosody: Theory and experiment (pp. 11-36). Springer Netherlands.). For demonstration purposes, you may also want to use a longer sentence. 

I agree with musicallinguist, but you can get this information for a given corpus if you really want to. One option is a tool called Phonetisaurus, which is one of a family of statistical graphone tools. Its intended function is grapheme-to-phoneme conversion, but one of the data files it builds along the way is a set of grapheme-to-phoneme mappings attested in the input corpus, weighted according to the computational model. It attempts to deal with the issues of context/cooccurrence that musicallinguist noted. This would be relatively tricky unless you're pretty familiar with the programming, and probably more effort than you'll want. 

Further thoughts on the issue. As I said, I do not expect to satisfy everyone. Not being very knowledgeable in either semantics or pragmatics, I gladly take in @jlawler's remark that "lots of generative grammars deal with semantics and pragmatics [and] only the ones who accept Chomsky's assumptions insist of segmenting them." This point of view can be vindicated in the following way. Ignoring the metaphysical issues on human thought and language (and I include here all forms of linguistic expression in the word language), the human brain and language system (including all senses) may be seen a physical device that receives input data, manipulates it in various ways, including memorization, and produces other data through various means (speech, writing, gesture, ...). In a nutshell, hoping not to hurt anyone's feelings, this is pretty much the description of a general purpose computing device. Note that I used the word "data" rather than "symbol". Can the working of such a machine be theorized formally with a formal grammar, viewed as a generative formal system? For example, could Chomsky's type 0 grammars be such a generative grammar formalizing the human language machine? The first point to note is that the human language machine is a physical computational device. Hence we may wonder whether physical computational devices can be fully described by a generative formal grammar. This raises two open questions, one in mathematics that may not be answerable, and one in physics. The first question is the Church-Turing thesis. It is a conjecture that any kind of algorithmic computability is equivalent to Turing computability. Turing computability is itself proved to be equivalent to "computation" by type 0 grammars. Note again the multiple readings of a grammar. This conjecture is generally considered not to be provable. I suspect it relies on an understanding of computability that assumes denumerability of all things computational. Computational devices are considered to be symbol manipulation devices, operating along an open time-line (there may be other assumptions). Whether that covers all possible physical devices remains an open problem in physics. There is some research work on these issues. Some is attempting to understand what kind of constraints in the physical world would imply the denumerability and time open-linearity hypothesis. Other research is trying to imagine more powerful computational models that could be allowed if some fundamental hypotheses were changed. For the time being, short of answering these questions, we have to assume the correctness of the Church-Turing thesis for all computational devices, including the human language machine. Hence, human language, together with the knowledge, memorization and interaction that goes with it, must necessarily have a description as a type 0 grammar, which can be read as a generative description. Thus, considering that the whole language process may be described by a generative grammar is in this sense an appropriate point of view. However, my vindication of it is not fully satisfactory for at least two somewhat related reasons: 

I’m not currently working with Praat, but I assume you’re calling something like string$() for the filename at some point. (Or, it’s being called implicitly to coerce your numerics into strings.) Can you use this function instead? 

For Google Ngram, here’s the example you mention: (most _ADV). More help is given on the info page: Google Ngram syntax guide. For more serious work and various corpora, I’d start with the BYU COCA site: BYU syntax guide. 

That can be a little trickier, and it's language specific. For French, I believe Lexique is a good choice ($URL$ Researchers have also had success using eSpeak ($URL$ to convert orthography to phonemes, for French and other languages (for eSpeak performance figures, see Marian, V., Bartolotti, J., Chabal, S., Shook, A. (2012). CLEARPOND: Cross-Linguistic Easy-Access Resource for Phonological and Orthographic Neighborhood Densities. PLoS ONE 7(8): e43230. doi:10.1371/journal.pone.0043230). 

The NS would be defined so that it derives only on negative sentences, and NCoord would derive only on conjunctions that "can be negated" and thus cause the observed ambiguity. However, I find this a bit awkward, and I wonder whether it would not be more natural with a Tree-Adjoining Grammar. Then, the question remains of when such an ambiguous analysis is acceptable. What are the conjunctions that allow the observed ambiguity. We have so far examples involving exclusively conjunctions with a causal undertone, though the direction of the causality may vary. I found another example, without causality, which may shed some light on this. Unfortunately, it is based a conjunctive form that is understandable, but rarely used (according to my search engine). 

It depends on the scale you're referring to. There is certainly active research in artificial language learning, agent-based and social network modeling of language evolution, novel language development by robot agents, social effects (micro and macro) on linguistic change, word formation constraints/factors, 'telephone game' experiments on language convergence, and so on. 

Neologism. Speaking for myself, though, ‘nonce’ is fine for things that are not strictly ‘one-off’. Everything starts with a first time, and 'accepted productive rules' are often precisely how a nonce word would arise, right? To the point that it seems odd to have a specific term for it. 

We study prosody because we observe that it is a meaningful, functional component of language. We transcribe it with the goal of abstractly understanding the contrastive elements that speakers seem to be perceiving and producing (and the system of interaction for those elements). This is analogous to other aspects of linguistics; e.g., we could roughly say that (segmental) phonology seeks to understand the system of sound elements in human languages. 

You see that this is a universally quantified statement, using the negation of the previous predicate. This is a general rule: if you want to negate a quantified statement, you change the quantification (from universal to existential, or the converse) and you use the negation of the predicate. The rule is simple to use in logic. It is a bit more complex in natural language in general, and English in particular. What may seem to be a negation in English is not necessarily a negation of the logical semantics of the sentence. This can be illustrated by your second example analyzed in the next section. Things can be even more complex when there is multiple quantification. About non-reversal of monotonicity in the second example Your inference "Driving is dangerous" -> "Fast driving is dangerous" shows that you understand "Driving is dangerous" as "All driving is dangerous". Here, "Driving" stands for the set of all forms of driving. Then, logically, the negation of "Driving is dangerous" is not "Driving is not dangerous" but "Some driving is not dangerous". Then this would not imply that "Fast driving is not dangerous". But if it were true that "Fast driving is not dangerous", that would imply that "some driving is not dangerous¨. i.e. you have the same reversal of monotonicity: "Some driving is not dangerous" <- "Fast driving is not dangerous". Your problem is with incorrect negation of quantifiers, not with monotonicity. "Driving is dangerous" means "all forme of driving are dangerous", which is a universally quantified assertion. When you negate universal quantification, you have to use existential quantification on the negation of the predicate. The negation of "for any x, P(x)" is "there is x, not P(x)". About the fact that the two examples have opposite monotonicity The second issue still has two do with quantification, and the respective properties of existential and universal quantification. To make it more visible I will use two other examples, where I make quantification explicit. Your examples are harder because they both have implicit quantification (which I already explicited above for the "driving"example). The two examples are chosen to be very similar.