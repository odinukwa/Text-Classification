prefix locations cannot be nested in regular expression one since the former have precedence over the latter during checks. Read the directive docs to understand how the block serving the request is chosen. You can (and you wish) doing the opposite activates FastCGI reverse-proxying for the current block, but is not inherited (what a mess it would be!) 

Note the use of the variable, reproducing the scheme used to connect to the frontend with connection to the backend. I am unsure about SSL configuration on the backend. I guess you need to use the same on each of them and the same SSL certificate as on the frontend. I do not know if you can use different certificates on backends, each with a different and change SSL parameters for connections with the proxy module. 

HTTP 303 is a redirection. There is nowhere in the configuration snippet you provide which creates any redirection whatsoever. It thus comes from a PHP file being processed in the block you are providing as I trust: you need to check the PHP application to fix this unwished redirection. 

Using , try to identify the master process (that is the one with PPID 1 and calling itself 'master in its description). Then send it a TERM, KILL or QUIT signal. Manually remove the PID file Check your service file (if you use init, it is most probably ) for the PID file name and compare it to the location you provided Ensure no PID file is there, no nginx process runs anymore and use your service normally to start/stop/reload/check status again. 

And so on. This is annoying, as our code needs to be maintained across multiple folders, and people often forget to do so; that's an issue that can be fixed, but I'd rather solve it in the solution rather than the process. The code in all the folders is identical, only the web.config files are different. The reason we have our sites split this way despite the identical .NET code is that we need different connection strings to different servers. This way, whichever site hosts the binding gets the traffic. This is how we handle load balancing, which is not the prettiest way, but it gets the job done. I'd rather MULTIPLE sites (with one or multiple app pools, doesn't matter) which work the same way, but function with ONE shared code folder. What I need is a solution to handle the connection strings based on the site in IIS. I didn't have much luck looking into virtual directories, which seemed straightforward but I don't think is possible, as it needs a web.config to even know WHERE to look for virtual directories. I'm sure I'm missing something simple here, but I need a little help to get to this: 

Depends on what you're comfortable doing; when starting to "professionalize" our office network for a similarly sized business, I set up a pfSense Firewall behind our modem, and assigned it routing tasks for the office. All you need is a machine to dedicate with a few NICs and you're on your way. It's pretty well documented online, and I haven't had an issue finding support when I've needed it. Once you have that installed, you can use the OpenVPN package to easily route the traffic you want to your network from anywhere in the world. The only steps besides setting this up on pfSense would be allowing traffic through your ISP's modem, which shouldn't be too difficult either. I'd say give it a shot! It's been rock-solid for us, and we run it in production now for our datacenter rack as well. It also never hurts to have a little more experience under your belt, and you could look to it as a cheap learning experience. The added benefit is when you want to do this, you can set up a VPN tunnel to your hosting provider, and then you'll have full access (on the terms you decide through firewall rules, etc) to your infrastructure more easily in the office. 

You made the usual mistake of people not really understand how nginx works. Remember the following: nginx always serves a request with a single block only. I suggest you (re)read the following: How nginx processes a request Now, looking at your configuration, backend requests would need to be served by 2 locations: 

To get advice on getting a clean and scalable configuration, listen to nginx inventor who hosted a talk at the nginx.conf user conference: Scalable configuration with nginx 

As you noticed, the use of seperate servers is not really necessary here, as a simple redirect would have been sufficient. However, I find it prettier if HTTP requests never made it to the right server. It would also help scale your configuration if you end up with services not (yet) supporting HTTPS. As for the arguments, I took your provided examples to the letter. If you wish to have some other arguments you do not want to touch and you want to redirect in a generic fashion, you may need to chain several (or use if you know what you are doing) to rewrite the variable, ending up with a rewritten construction ready to be sent to the redirection URL. 

That is worker recycling as the configuration directive sets. I (reasonably) assumed PHP-FPM would courrectly process any accepted connection/request before recycling the worker, as this task does not imply any kind of emergency. 

Avoid at all costs automated tools which lack, by definition, the brains to properly convert rules. There is no simple 1-to-1 match between Apache and nginx ways. They have 2 different mindset. You thus need to understand what you are doing and use the nginx way of configurating it. That is IMHO a complex problem out of the reach of an automated tool. Concerning rules: 

Here's a couple of links explaining what you'll want to be doing. Take a look through the rest of the documentation on the official site if you have any more issues. 10 minute tutorial. Part 5 shows what you need to be doing. Parsing old logfiles with correct timestamps. Might be useful as you get into the swing of things. 

Rather than beat yourself up trying to troubleshoot the sometimes tempermental web platform installer, you can always just install directly from the Microsoft provided ISO instead: here. Also, you might check that you're on Windows 7 SP1, as that's listed as the minimum requirement for this version, NOT standard Windows 7. 

Test-NetConnection will always return "False" for a localhost port. Use another tool (like canyouseeme.org) if you're checking to see if a port is open from a remote address. Your Powershell looks fine, and you should see the rule if you look in the Windows Firewall rules GUI, but you can't test it the way you are attempting locally. 

The flag will overwrite files. It is 'include same files', and should accomplish your task. I would also use to set the retry wait time to 1 second, rather than the default 30. Your delay might be from locking on your target side; have you checked that out? I don't know of a way to copy without checking, but that should get you where you need to be. Of course, you could also use another line of robocopy to just delete all the files from the target directory BEFORE running the mirror. That would certainly work as well. 

What you could do is uploading configuration files including directives to a specific directory. That directory would be included (ie through ) in your configuration. You will need to issue a signal to nginx master to reload the configuration though. @MichaelHampton is right about the fact you do no need to restart the server. Reloading the server configuration without downtime can be done by issuing the command or . There is no 'persistent directory watch + reload on filesystem change' feature in nginx. You will need to script it (ie with Lua), but that is not recommended anyway. You could create a cron task for regularly reloading nginx configuration... looks also dirty if you ask me. 

If a wrong certificate was to be presented (no SNI or wrong server selection), you would normally get a certificate warning since there would be a mismatch between the requested domain name and the one set for the certificate. This will display the protocol being used by the current connection: 

Following documentation, the first on is called a prefix location, while the second one is a regular expression (regex) one. nginx will check both but will eventually only select one, which in your case is the regex one. Now, when processing it, nginx will forward a request for to PHP, building the path from , since it is the only one defined. The backend then fails, unable to find the specified file. You might wish to try the following: 

This has nothing to do with domain names, but rather at what defines a network socket: a duple. In your case, it is implied all involved subdomains point to the same duple as the main one. You configured nginx to listen on this socket, thus handling any incoming connection there. nginx then selects the best virtual server for the request, defaulting to a default one if no best match is found. I suggest you read on how nginx serves requests. In your case: 

Time+ represents CPU time, or more specifically, "Cumulative CPU time which the process and children of the process have used". 

I'm running a basic Powershell script on a remote server, and while it's running, it ignores keyboard input completely. This is intended. When, however, you click anywhere in the Powershell window, it stops input until you clear the pause with a keypress. Is there some way I can force Powershell to ignore mouse input altogether while the script is running? I want this to be as difficult to close as possible, ie. you need to actually click X to close the window, or kill the process, or the like. Running in a Command Prompt window instead is acceptable, but I'd rather run it in the Powershell GUI instead if possible. 

Go into Task Scheduler (taskschd.msc). While there, there's a button on the right menu for Display All Running Tasks. I think this is what you want. While you're there, you should turn on Enable All Tasks History, so you could track something like this better in the future. 

I'm trying to find out how to measure the total bytes written (or a percentage of maximum expected, either is fine) for a few RAID arrays behind LSI controllers. The controllers are all LSI MegaRAID SAS 9271-8i controllers. I've tried using MegaRAID Storage Manager and MegaCLI, but neither seems to show the information that I need. I've found a couple solutions online, but they only seem to be for Linux, where you can patch the kernel or use smartctl in unconventional ways. That won't work for me on Windows. I'd really like to avoid pulling the drives out, putting them in another machine, testing with SMART, and then putting them back. Would be a real pain in the neck. If it's important, each controller has two virtual drive groups of 4 disks each, in RAID10, with SAS SSDs forming the groups. 

It is a general good practice to avoid filtering requests with regex locations, which are order-based, which is bad (remember Apache directives order sensitivity nightmare?). To filter, use prefix locations instead which are longest-match-based. You can embed locations withing each other if you ultimately need a regex one. Why not directly putting your directives into the location? Then, instead of using the variable (guessed from the match that would be variants of here), you could use . By the way, already contained the starting , no need to add one between the variables Avoid using redirections if you can avoid them. Avoid especially even more. Simple user redirections (URL rewriting done through redirection notifications sent to the client with HTTP status codes) can be done with . What you did here was an internal redirection (done locally on the server): its only use was to change the URI then used for the parameter. 

Start by deleting the stanza from your file since that triggers the now unwanted behavior of including content at level. Now you could use some locations such as: 

is not specific to any Process Manager () mode. Its benefits are to respawn worker processes after the specified amount of requests they handle individually. It could avoid memory leaks in extreme cases, but usually it just frees up memory allocations accumulated when memory-hungry scripts were executed. Memory allocations pile up and the total amount of memory allocated grows, but is only freed on exit (thus when respawn). As you put it, you seem not to have activated when using . You should have seen a difference, and certainly more than a flat line. has the added benefit of stopping workers depending on the number of idle processes amongst them (, ), which are a kind of a metric measuring instant load. Stopping useless processes frees the associated allocated memory at the cost of process handling (CPU), which counteracts ensuring a safety cushion in case of spikes, maintaining idle workers ready to process requests. Now, if you really are looking after your memory, is more aggressive, (de)spawning processes as (not) needed (anymore). This mode is the closest to the edge, since it won't absorb spikes as well as , but it is the leanest on memory consumption (with the counterpart of using more CPU for process management purposes). TL;DR Memory allocations stack up for any worker process. If a particular request was memory-hungry, it will top the processing worker memory allocation, and this won't be freed until the process stops. Use to recycle processes, whatever mode you use. will recycle processes on another criterion which is load, and will kill processes when too much of them are idling. Having a higher processes turnover is more likely to prevent them being too much memory-hungry, at the cost of more CPU cycles used for processes management.