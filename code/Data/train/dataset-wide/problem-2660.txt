There is some room for improvement. The memory access pattern through is not ideal, the inner loop iterates over the rows and only half of a cache line is used every time, for a large enough matrix that means half of every cache miss is wasted. It's probably better to unroll the middle loop by 2 (or 4) again. As a bonus, we get to re-use the broadcast from . Sort of like this (not tested, showing 2x) 

Which to be fair is super common and it is the most obvious way to do. But it has a problem: for a long enough vector, can overflow. That is undefined behaviour in C++, and it is also actually bad in practice. The most likely consequence is that results in a (very large) negative number, subsequently dividing it by 2 still results in a negative number that is only slightly less big. Indexing into the vector with a negative index will very likely crash. Here are some ways to avoid that: 

I've done some testing now, and made some improvement specifically for larger matrices (below N=64 it doesn't really help). Some results on Haswell, compiled with MSVC[1] 2017, measuring the time (in cycles) per element of the result matrix (so you can mentally compare it to how much time it should take). Time results were eyeballed and rounded to a "typical" value. 

One problem that immediately jumps out at me is the loop-carried dependency through , which has a latency of either 3 or 4 (depending on the processor) while there are not nearly enough instructions there to fill all that time, so it's lost throughput. The typical solution is unrolling and using multiple accumulators. There's too much stuff there for me to comfortably write the actual code, but the general idea is something like this: 

This loses the print-back of the input, if you want to preserve that you could use an . By the way, in Java 8 you can use to make a comma-separated string from your list, you don't have to do that manually. 

I think that's 16 pixels, but it's a good plan. The pack instructions were used inefficiently (in the linked question that was not really an issue) and that would be improved by processing more elements simultaneously. For example, something like this: 

There are some issues, such as not using obvious built-in solutions, an inefficient algorithm and what looks like a logic error possibly caused by having odd array-growing logic. I'll start with the error. In you assume that there are at least two numbers. That may not be true, if the user immediately enters 0 - which raises the question of what the GCD of the empty set is (0 or 1, depending on who you believe), but whatever it should be, it's bad form to crash with an out-of-bounds exception. The odd array-growing logic is probably part of that problem, it leaves a spurious 0 in the array which you thereafter actively ignore - it's simpler to not have it there to begin with. Anyway, you wanted an array that can change size - that's usually called which, conveniently, already exists. You can also avoid saving up the input entirely, by computing the GCD while reading the input. The GCD algorithm is strange and inefficient. It's not the familiar GCD algorithm, which is already a strike against it by itself since that makes it hard to tell what it's doing, and there is no redeeming quality. This is an algorithm with a very bad worst case, as much as O(min*N) (if the GCD turns out to be 1). I strongly recommend using the modulo-based version of the Euclidean algorithm, which is efficient and recognizable, or maybe Binary GCD if you want something different. In the unlikely case that you rejected it because it takes only two inputs, of course and so on, so you can simply loop over your input and call GCD on the gcd-so-far and the number from the input. Or, as mentioned, you could compute the GCD while reading the input, something like this: 

That doesn't really address the SSE2 side of the store directly, but you can do the same thing but with separate add/mul. Proper use of AVX would use the wider vectors. For this code that's a fairly trivial change, just make almost everything wider (except the matrix). If AVX is available, you should use it in this case - it doesn't help for everything, but for this type of code (almost purely vertical SIMD, apart from some broadcasts) it's great. FMA is also a free performance win here (especially on Haswell and Broadwell), well worth using even if it means writing two versions and doing runtime dispatch to also support the Bridges (which are perhaps not old enough yet to completely disregard). Supporting 32bit machines is annoying. They don't even have enough registers to load all of that matrix, so everything gets bogged down by the extra loads that are suddenly required. I don't see good fixes. Rearranging the multiplication so that the matrix can be held in 4 registers does work, but then a horizontal addition appears which is bad, and the multiplication wastes a lane on padding. I expect it would be worse than the extra loads, on the other hand if we're talking about old hardware like Core2 (which had x64 support, but at the time installing a 64bit OS was a rarity) then extra loads are extra bad since the load throughput used to be only 1/cycle. On the other hand, horizontal addition also used to be much worse than it is now. It just seems like all options are terrible. It depends on your audience of course, but frankly I don't think it's worth expending much energy on. 

But that is not great either. Ideally there should just be no horizontal operation in the inner loop. Even more ideally, not anywhere. And that can be arranged: a block of 8 results can be computed by rearranging the computation so that horizontal operations turn into broadcasts. Broadcasting from memory is pretty cheap (naturally it sort of "wastes" the load by loading only one thing, but it's not a slow operation), and there are much fewer of them, so that's probably better. Sort of like this (to show the general idea, not tested) 

Subtracting a smaller non-negative number from a bigger non-negative number is always safe. Adding half of it to the lower number is also safe because the average must be somewhere "between" the inputs so it cannot go out of range. 

This dominates not just horizontal screen space, but also the performance (or lack thereof) of the loop. Actual extracts (an extract with an index of 0 is turned into by a reasonable compiler, which is less of a problem) have a throughput of only 1 per cycle on typical CPUs. There are 7 here, so even with just this horizontal addition the loop body could only execute once every 7 cycles (but there is other stuff in there too so it's worse). There are slightly faster ways to do horizontal sums, for example (not really tuned or anything, just some simple "better than totally naive" hsum) 

I've also changed some thing to , because some useless sign-extension was going on sometimes (depending on how it was compiled). and are now calculated outside the inner loop, normally we can expect the compiler to be pretty clever about that too but compiling with OpenMP support seemed to make Clang less clever, it put the actual multiplication between and inside the inner loop. That's not good, because on Intel that would go to p1, and p1 is already completely packed with the float-to-int conversions and fp-multiplications (eg on Skylake both of those go to p01). That loop should be able to run at 1 iteration every 4 cycles in the best case on most architectures, but with that extra in there that wouldn't be possible. To convert to something similar can be used, but with and no packing to bytes. 

is based on the throughput of on Haswell, which is one every two cycles. So per cycle there can be 4 multiplications, we need N of them, so N/4 is the ideal time per element. For small sizes that's not so hard to get near, but for bigger matrices the memory access pattern messes up everything. v1 and v2 are the versions from above. v3 adds loop tiling to significantly improve the performance for bigger matrices. Understandably this causes some overhead, noticeable for smaller matrices. v4 unrolls the loop by 2x. Annoyingly, the best choice for the block size depends not only on cache size, but also on the size of the matrix. The times above are not all with the same parameters, but tuned a bit per N. I'm not sure where to go from here but it seems as though some improvement should still be possible, for bigger sizes it's still a decent factor away from the ideal. v3: 

Yet an other version, with even more tiling and with rearranging matrix 2. Added to the table of times above. Of course, some time can be saved if that matrix can be assumed to already be in that order, but I counted the rearranging in the benchmarks. That overhead scales as O(N²) while the meat of the algorithm scales as O(N³) so for a large matrix it does not represent a significant cost anyway. It seems to behave well now, staying around 110% of the theoretical optimum for any size I test. Perhaps some small tweaks are still possible. For example, unrolling the loop by 4 instead of 2 improved it slightly in my tests, but the difference is kind of small. v5: 

How much you should unroll by depends a lot on the precise code and the actual processor. For example on Haswell the initial target performance would be 2 FMAs per cycle (so unrolling by a factor of 10), but that would mean the kernel cannot come from memory since both available loads are needed for the image, limiting the performance to half the target performance. Getting to 50% of the goal isn't great, but there is hope: after unrolling, multiple iterations can share the same load from the kernel. That load can be a wide load, and instead of the appropriate element for the iteration can be shuffled into all lanes. This alone can reduce the number loads from the kernel to a quarter of the original (or an eighth, with AVX). And there is more: unrolling a different way. If multiple unrelated convolutions (different rows) are interleaved, the load from the kernel can be re-used for each of them. If you do 4 of these, you could try doing an in-register transpose (there is for that, which is more of macro than a proper intrinsic) and then just using 4 wide stores. That transpose is fairly ugly, weighing in at 4 unpacks and 4 movelh/hl's for a total of 8 µops to port 5, but it turns 16 scalar stores into 4 wide stores so that looks like a decent trade. With all this unrolling it's easy to go too far, running out of registers and spilling to the stack in the inner loop would just murder the performance so definitely avoid that - always check the assembly (you don't have to write it, just read). Unrolling by a total factor of 8 or 10 is probably OK, both in terms of being enough to reach (or get close to) the latency-throughput product and in not exceeding the number of available registers. So for example 4 rows and 2 iterations of the inner loop makes a factor of 8. Unrolling the inner loop depends on the kernel being a nice size, if it isn't then part of the last iteration is wasted work since it's partially multiplying data by the kernel-padding. So unrolling rows may work out better in general. 

So with conversion to , especially useful if you actually wanted a floating point division all along: 

Getting good performance out of matrix multiplication is tricky. Many concerns have been addressed already, but there are a couple of big ones that I have not seen mentioned. The main issue in medium-large matrix multiplication (for tiny matrices it's 100% micro-optimization) has historically been, still is, and will always be: reuse of data at several levels. The most obvious is at the cache level, but even at the register level you must economize. For example, on a modern Intel processor (similar principles are fairly universal, but that's where the exact numbers will come from) it is possible to execute two vector FMAs (fused multiply-add) in a cycle, and it is possible to load two vectors from memory provided the loads hit L1 cache. Two FMAs together have 6 inputs, but you can load only 2 things, so in order to get anywhere near a good performance there has to be significant reuse. In a dot-product, you can reuse the summation variable(s), but that is not enough, that still requires twice as many loads as can possibly be executed. So a perhaps surprising conclusion is that you cannot do just one dot-product at the time. There would have to be several in progress side by side, to enable sufficient reuse of data. Also you might think about different computational structures than dot products, such as small outer products (of rectangular, yet not too narrow, tiles). An other (but related) issue is that while many FMAs can be executed per unit time, any individual FMA is actually quite slow in terms of latency. Of course it is not surprising that it should take at least one cycle, and even that is already too long to be executing two FMAs every cycle with a naive dot-product. The problem there is that the sum variable takes a while to be updated, and this will delay the next FMA if the code is written the obvious way. Actually a latency of 4 or 5 cycles and a throughput of 2/cycle is common enough (Haswell through Skylake, Ryzen). That means a simple dot-product, even if vectorized (scalar is not even worth considering), can be 8 to 10 times as slow as a proper implementation, though "fast math" flags can alleviate this. Anyway this problem also disappears when performing a sufficient number of simultaneous dot-products - every individual one of them will still be about as slow as a naive implementation, but the total throughput can be increased to near the theoretical maximum. Once you have written a fast kernel like that (for reference, "fast" means close to 32 sp fp-ops per cycle on Haswell-Skylake and close to 16 on Ryzen), there are still more levels of data reuse that must be taken care of to get good results for large matrices. Loop blocking is often mentioned, but what's less commonly noted (and actually super important) is repacking the current tile from the matrix into a contiguous block. Without repacking, the tile may indeed fit in the cache, but may span more pages than the TLB can simultaneously cache address translations for. What the right tile size and shape ratio are, is also an interesting question that I frankly don't know the answer to, except to experiment. I don't really have great ready-made code to show you for this, it really is difficult and takes a lot of engineering and experimentation to get right, taking care of more (and weirder) details than I addressed. The good implementations that you can find elsewhere have had a lot of work put into them. I did give it a try, but the results were not as close to what they should be as I had hoped, reaching 28.5 sp fl-ops per cycle at best on as Haswell, and that was after writing a highly unrolled loop in assembly using all the vector registers and with various dirty tricks such as loading some data an iteration in advance and pre-computing addresses to avoid some complex memory operands with associated µop delamination, and despite my best efforts with tiling/repacking I still saw a performance drop-off for larger matrices.