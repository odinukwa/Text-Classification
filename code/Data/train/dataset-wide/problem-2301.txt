Here's the weird thing, the following query DOES work, the only difference being whether rating needs to be larger than or smaller than 1. Suffice it to say that I'm utterly perplexed. Is this a bug? I'm on 11.2.0.4.0. Could it be something to do with JARO_WINKLER? 

I'm using UTL_MATCH's Jaro Winkler similarity function and it seems to be performing well. However, I would like to adjust the prefix scale according to the situation. Is this possible? Is it possible to see what the default prefix scale is? I could not find any documentation on this, but it seems that in order to be a J-W distance, it must use a prefix scale. 

I've been using sysdate to time procedures, but a lot of my procedures are very small procedures that get called thousands of times. Is there a way to get milliseconds elapsed instead of seconds? I know it has something to do with timestamps, but I can't find any exact way to do it. 

Will this trigger propagate? Like, if it updates a ROOT_ID for another record, will that trigger its own trigger? Further, if it does trigger that, will it use the new ROOT_ID? I want the ROOT_ID to propagate down the tree I've built. Edit: How this works is that each record has a unique ID, a parent ID, and a root ID. I basically have a tree, each member of that tree has a root_ID pointing at the unique ID of the root and a parent ID pointing at the one above it. The root's root and parent IDs are its own unique ID. in the case that a user manually changes a record to point at a new root and parent, I want all the children of that node to have the new root ID. Is there a better way to do this? 

While Merge is not the best performing solution, based on your requirements this would handle what you requested (as it would insert rows that did not exist on the target). I put a quick schema together that may not match yours, but gives you the idea. 

You can break down the vendors using a CTE, prior to using the COALESCE function to get a list of emails, with one per vendor. 

There is no metadata view that is going to clearly give you this information. Your best bet would be to have a scheduled process that runs every few minutes and looks at and evaluates if the is less than the prior , in which case you would know that it had cycled. You can also compare the to the to see how close you are to hitting the cycle point. A very basic view of this would be the following with a small sequence, you can see the initial values remaining, what it looks like after using a few sequences, and then how it gets reset as the sequence cycles. 

CrystalDiskMark is not really a good measure of storage performance with arrays, like Pure Storage. They write frequently about different testing methodologies on their blog. You would be better off using DiskSpd for a synthetic workload right now, although you would want to use something like distributed relay to get a true test of how things perform. As a general rule Pure are also very supportive of their customers, you could always reach out to them to see if they have a test harness already built that might help you figure out the information you want. 

It sounds like you are using Availability Groups and either one of your replicas is not syncing correctly, or it is getting so far behind that it is causing the log on your primary to grow until is reaches its maximum size (or you run out of disk). 

So, I have an insert statement in a Java program I'm writing. Under some conditions, I want it to insert some values as null. However, before I can execute the statement, I have to set all of the tokens to values. How can I make it set something to null instead? Edit: Example: 

Very weird question: Is there any way to artificially inflate the amount of time a query will take? An infinite loop would be great. 

I partially solved it: PHONE is actually a VARCHAR2 column. However, this doesn't at all explain why comparing in one direction versus another works or doesn't work, when I make that mistake. So I'm still very curious. 

Turning the original select into a table is not an option. Ideally for readability, I'd like to not copy/paste the select; I'd rather alias it somehow but I feel like my syntax for that isn't quite right. Here's my actual code. It is a horrible mess (actually if you have any suggestions for making it less of a mess that'd be great). It gets on the top layer - the layer below that works fine (producing the first table above, essentially) 

I want to determine if a resultset exists very quickly. At the moment, I'm doing a count - this is taking roughly 55ms, which is unfeasable.The table has ~100k records - I don't care if it has 2, 5, 100k rows that fit a query; I care if it has 0 or 1. Maybe 2 in certain situations. Is there a way to do this? Would limiting a count using ROWCOUNT (so it only counts about the first 2 rows it finds) speed up the count at all? 

You'll need to put a trigger on the siteArea table so that whenever it's updated or inserted, it'll call a stored procedure with an argument representing the new site area and that procedure will calculate the needed values and insert them in a table. Your question is too ambiguous for me to describe further. 

I've been using AGs for 3 years now, with table compression, and have not seen any material impact. The data shunted between the servers is all log transport, and that itself is compressed, the compression of the pages themselves doesn't really factor in. Your only concern would be if you were under CPU pressure already and then added on that little extra for the table compression. 

It appears as though you are trying to get the number of rows returned from the view, and have the ability to print that value as an output. Your query looks like it's trying to write this data to a table. Try the following (replace the default view I've used here with whatever you would need) 

I'm not sure what "Microsoft and Microsoft MVP standards" are, but my recommendation would be to find yourself some extra storage and to grow those logs out, and let them be at the larger size. Shrinking the logs is going to significantly slow down your month end processing, as each growth of the log is going to cause a stall in the processing while the new VLFs are zeroed out. Depending on the performance of your storage a 1GB log file growth could take 1-20 seconds (or maybe even longer). I'm pretty sure everyone would be super happy if it meant that the processing was completed earlier than it is now. There's the additional concern that your log shrinking routine would make the log smaller than it needs to be for general (not month end) usage. That would mean that you would have impact during regular work at those times when the log needs to grow. 

If you are running this as a SQL Server Agent job step you will need to use the step type, not a type. Within the it would be best to create a file on the machine with all of the steps that you require and then call that. For example create a file which contains... 

In the case of backup software, you can't use anybody's bak file interchangeably. SQL Server's backup file format is proprietary to SQL Server. It doesn't work for other apps. You can also name your backups anything you want during the backup process - you could use the .docx extension, for example, but that doesn't mean they're readable in Microsoft Word. (Some folks default their differential backups to .diff, and log backups to .log - but other folks just name 'em all .bak. There's no real standard out there, and changing the extension doesn't affect the contents.) 

If you use a different database designer tool, or if you use different datatype changes, you can get different results. For example, if you change a field from a VARCHAR(MAX) to an INT, you'll lose full text indexes on the VARCHAR(MAX) field because you can't full text index a number. 

Typical causes: someone drops the login (thinking they're cleaning up bad logins) or restores one of the system databases. It's tough to guess what this was after the fact, though. In user databases, you can go through the transaction logs to reverse engineer it, but you can't do log backups for master, so you're out of luck. 

Because of those, running queries against a mirror isn't usually cost-effective. If you're willing to stomach the Enterprise Edition price tag, SQL 2012 added Always On Availability Groups, which let you read directly from the secondary with nearly real-time data at a much lower management cost. Still has the EE price tag, though - readable secondaries are an EE-only feature. Generally speaking, performance tuning is easier than licensing more EE cores ($7k/core USD.) 

Elijah. There's two separate questions here: 1. Is DTC supported with AlwaysOn Availability Groups? As Microsoft says in big letters, no. I totally understand that you want to try it anyway, but keep in mind that you're now putting something into production that Microsoft simply will not support, AND you're using two separate niche features together (AGs and DTC). If anything whatsoever goes wrong, you're going to be in a world of hurt. This just isn't something I'd ever even think about trying in production. Keep in mind that if your managers find out that you deployed something Microsoft specifically says in big letters, "YOU CAN'T DO THIS," and you have any kind of outage where you have to call Microsoft for support, you're going to have some ugly explaining to do. 2. How should DTC be configured in a multi-node, multi-subnet cluster? Read Allan Hirt's post on configuring DTC with multiple instances of SQL Server in a cluster, and make sure to read all of the links in the post as well.