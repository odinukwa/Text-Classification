If reducing mathematics to logic is the goal of logicism, what other conceptions of mathematics are there? 

So my question is why does Kant further say that Der arithmetische Satz ist also jederzeit synthetisch (Arithmetical propositions are therefore always synthetical): what is ganz außer in 5 + 7 = 12, if we see 12 as just a convenient short name for 5 + 7? 

Maybe we want meaning, but Nature doesn't give us meaning - it is arbitrary, uncontrollable... That would be very unsettling and anxiety provoking, and would compel us to create a meaning for ourselves in this world that doesn't offer any. So, through the experience of being in a meaningless world, we experience the need for meaning. 

I am aware of two major modes of reasoning used for justification of belief: deductive and inductive. Whereas physics relies on induction, mathematics seems to rely exclusively on deductive inference, which ensures the soundness of its conclusions. Is that correct? Are there other modes of inference used in mathematics? Also, to be precise, what I am after here are the admissible modes of inference used for justification in the final product of mathematics, not the types of inferences mathematicians use in their daily work to generate mathematics. For inductive reasoning, what I have in mind here is what is explaining in this entry of the SEP. (if there is nothing in mathematics but deduction, mathematics is an extension of logic, and there is no other way to see the situation) 

How should we understand what is going on here from an epistemological point of view? It feels a bit convenient to toss these 2 terms from the point of view of mathematics - well maybe even wrong, in the sense that the resulting Hamiltonian is no longer the result of a sound mathematical derivation? Would it be desirable for the physicist to work with maths that would not yield unphysical solutions? Isn't maths in fact unreasonably ineffective in physics? 

The statement Without countable additivity, it seems – for example – that we must lose the result that an arithmetic sum of an infinite series is the limit of the partial sums. is surely false, for the simple reason that it's a definition and not a result. Given an infinite sequence x_1, x_2, ... we define repeat define its sum as the limit, if it exists, of the corresponding sequence of partial sums. And the limit of a sequence only depends on the formal definition. The sequence x_1, x_2, ... converges to L if for all epsilon > 0 there exists N such that if n > N, then |x_n - L| < epsilon. [Why doesn't philosophy.stackexchange support LaTeX markup?] That definition in turn depends only on the construction of the real numbers (as equivalence classes of Cauchy sequences, or of Dedekind cuts) carried out within ZF set theory. Any competent undergrad math major could outline the steps involved. There is no need for countable additivity to define the sum of a convergent infinite series. Perhaps the OP can supply more context for this obviously incorrect quotation. Countable additivity is useful in measure theory so that we can say that the union of disjoint intervals of lengths 1/2, 1/4, 1/8, ... respectively has measure 1. But you don't need an assumption of countable additivity to define the limit of a sum of real numbers. That result only depends on the proper definition of the real numbers and the development of the theory of convergent sequences and series. In fact mathematicians can even define the sum of an uncountable collection of real numbers (although if the sum is finite, we can prove that all but countably many of the summands must be zero); yet, uncountable additivity is not required of a measure. Oppy is either confused, is ignorant of basic real analysis; or is being quoted out of context by the OP. 

Sometimes in physics, the mathematics leads to "un-physical solutions or terms", that are readily tossed by the physicist. For example, when deriving absorption and emission rates for via quantized light-atom interactions in quantum optics class, we toss out 2 terms from the Hamiltonian, on the grounds that they don't correspond to any observed physical process: 

Here is a basic working definition of a law of physics from the point of view of a physicist. It does lead to several interesting philosophical questions, which I completely ignore (but am very well aware of). A law of physics is a provisional statement of an observed regularity in Nature. We formulate laws of physics to understand the world around us, and also to control it or act in it (not always: a good deal of astronomy for example deals with things that we will never control). A law of physics summarizes many instances of a given phenomenon in a convenient manner. For example, the law of gravitation summarizes in one formula how objects fall towards the ground as well as how planets go around the Sun. An observed regularity is something that holds in many, most or all observed instances of a phenomenon. A law is always provisional in the sense that a more encompassing or more precise statement of a regularity may have to be made later, given more observations. The laws of physics are usually numerical, holding between observable quantities that can be measured, so that the laws take a precise, mathematical form. However, there can be individual instances of measurements that do not exactly conform to the mathematical law at hand: there can be errors due to the measurement equipment or procedure, or fundamental uncertainty as in the case of quantum mechanics. This gives a statistical character to the laws of physics, and it is usually required to estimate the uncertainty in the measurements. A very basic example, but one that is IMHO worthwhile keeping in mind as a good image of a law of physics, would be the following picture, where we have observed a number of data points, and plotted the line (model/law) that bests fits these observations. It shows what the law/model is: the line, i.e. 2 coefficients a and b in a formula y = ax + b. This line summarizes the data points, it is the regularity/invariant underlying the phenomenon. It also allows interpolation and prediction to calculate unmeasured values of y for values of x. Finally, it shows the notion of error in measurement, as many actual points lie close to the line, but not exactly on it. 

"Symbiosis (from Ancient Greek σύν "together" and βίωσις "living") is close and often long-term interaction between two or more different biological species." $URL$ Assuming these married people belong to the same species, their pair-bond is not an example of symbiosis, by definition. In order for your idea to work, you would need to change the accepted definition of symbiosis. You and your gut bacteria are in symbiosis. It's true that they don't cook you breakfast; but they do help you digest it. 

You are correct that until we know there are numbers, we have a formal theory but not yet any domain where the theory can be interpreted. There are two ways to go with this. 1) It's clear that we all know what the natural numbers are. Even if we don't have a formal theory about them, it's clear that they satisfy PA and are therefore a domain in which we can interpret the axioms and theorems of PA. 2) We can formalize the axioms of set theory, say Zermelo-Fraenkel (ZF). Then the concept "exists" means "can be shown to exist from the axioms of ZF." Then we can say that the empty set stands for zero, and the set containing the empty set is 1, and so forth. The objects of this construction are known as the Von Neumann ordinals. $URL$ Now we can verify that the numbers 0, 1, 2, ... as defined by Von Neumann, satisfy PA; and therefore we have a set in ZF that can serve as a domain of interpretation for PA. The same thing is often done pedagogically with the real numbers. The usual procedure is to list the properties of the real numbers such as the commutativity of addition, the distributivity of multiplication over addition, and so forth. That's good enough for calculus class and lets you do everything you need to. In a math major class called Real Analysis, they'll show that you can start with the rational numbers, and construct a model of the real numbers using Dedekind cuts or Cauchy sequences. They show that to the students once, and from then on everyone forgets about it and just uses the properties of real numbers. In practice it turns out that most of the time nobody cares about the ontology of mathematical objects. All you need is their properties. But ultimately you're right, if we're going to be logically comprehensive it's not enough to write down the axioms of PA; we have to show that we can formalize something that satisfies them; and that's where the Von Neumann construction comes in. 

If mathematics is concerned with deductive reasoning, and relies on logic to ensure the soundness of its derivations, if on the other hand, the derivations of mathematics, at least in a philosophical and modern view of the subject, start from arbitrary axioms, can mathematics be reduced to arbitrary axioms and logic? Note: arbitrary here means that mathematicians are a priori free to choose an initial set of axioms, as in e.g. Euclidean vs non-Euclidean geometries, that they are not constrained by observations of the natural world in the choice of axioms to derive theorems from. 

I'm not sure why 5 + 7 = 12 should say anything new: I take it to be a shorthand notation to give a name to 5 + 7, which anyway is nothing but 5 times the unit + 7 times the unit, so there is not really anything "new" here. Or is there? If we see it the other way around, as 12 = 5 + 7, maybe 5 + 7 is not contained in 12: we have a set of 12 objects, and we say that we can see it as a subset of 5 objects + a subset of 7 objects, so we say that that subdivision of the original set of cardinality 12 exists. But is that really "new"? Of course, 5 + 7 = 12 is cited as an example at the beginning of Kant's Critique of Pure Reason, when he explains his view of analytic and synthetic knowledge. The question here though is not meant to criticize analytic judgments, but rather to criticize synthetic a priori judgments. I believe (for the sake of argument) that mathematics is composed entirely of analytic judgments, not synthetic ones, and I am trying to understand why Kant could argue at all that mathematics was in the synthetic camp. More precisely. To define analytic and synthetic judgments, Kant writes at the beginning of the Critique of Pure reason, 2nd ed (emphasis mine): 

Your assumption that given a list of cities you can always find one more city, is false. In fact at some point you will have a list of cities, but you will not be able to name one more city. 

I love you and I hate you. That's a contradiction that's often true. I want this piece of candy and I want to be skinny. That's another. Life is full of contradictions. The very definition of sanity is that you can get up every day and function despite all of life's contradictions. If you were a complete sociopath you would be uninterested in and unmoved by the collective weight of human suffering. If you were totally empathic with every living thing, and personally experienced the suffering of every creature in the universe, you'd die of grief the moment you woke up. A sane person cares about human suffering; but not so much that it affects their own ability to care for themselves. A sane person must be part sociopath and part saint. That's another of life's contradictions. It's only in abstract realms of thought that contradictions are false. In everyday life, they're often true. If you're told person X is shy, then if you know anything about human nature you know there's an exhibitionist hiding inside. And conversely if you see an exhibitionist, you know that inside they consider themselves shy. Literally everything in life is the opposite of the way it appears. Everything is a contradiction. This is why computers can not be conscious. Computers regard contradictions as false. How are you going to program a computer to understand that I'm tired and I don't want to go to bed? That I want to go to heaven and I don't want to die? Logic is terribly limited in that it can only assign the value of FALSE to a contradiction. Life generally assigned contradictions a value of TRUE. Let's see anyone program that!