I created procedures to encrypt, decrypt the column and everything worked OK. I then took the database (SQL Server 2005, Mixed Mode) and restored it to a new PC with an SQL Server 2008 Express. WITHOUT restoring KEY and Certificate, I launched my application which use the stored procedures to encrypt/decrypt and SURPRISINGLY everything worked in the new database!! I could decrypt previously encrypted data successfully. Is this normal?? What is the use of backing-up the Key and the Certificates then? Thanx in advance 

Something tells me that it was not normal. Let me take it from the begining. In an SQL Server Express 2005 I needed to encrypt one column. I did the following. 

I have an issue with one database. The issue is as follows: The database is being running on SQL Server 2000 Standard for the last 6 years in Full Recovery mode. In the beginning of this summer the database was about 5GB. Since then, the only thing we did beyond normal usage is some extensive deletions. This week, there was a problem with the PC and I was forced to do a backup and restore it on an SQL Server 2008R2 Express and set the database in 2008 mode. The backup file was about 1GB. When I restored it, the MDF was 9GB of size! I checked the old MDF and it was of the same size. I checked the size of the tables and they cannot reach the 9GB reported! I did a shrink but the size did not change. Any clues or where to check? Is there a chance that the Full Recovery, can affect the size of the MDF files? I am thinking of setinng the recovery model to Simple, back-it-up and restore it. Is it going to make a difference? Can I do it on a live database? Thanx in advance! UPDATE:The initial size of the database is 1306 MB UPDATE2 sp_spaceused: Database Size=8646.88 MB Unallocated Space= 0.00 MB reserved=1336984 KB data=1020376 KB index_size=210408 KB unused=106200 KB 

If I would put this data in the orginal table I would follow the rules of database design but many it would make work harder. 

In my application stores both real and fictional persons (from movies, etc.) as well as stage names and alter egos. These entities have almost the same properties and are often referenced via foreign key regardless if the are a real or not. I'm still concerned if it is a good idea to store this information in one table even if otherwise the model would be more complex. The table : 

Limit does have an impact on how many rows are selected/affected. 543 is just the number of rows which match the joins and the where clause. 

I then tried to set the length of the VARCHAR column as high as possible without getting an error. What suprised me was that MySQL allowed me to change the length to 191 but that would mean that the index is 8+191*4=772 bytes long but the error told that only 767 bytes are allowed. This is the table: 

I know that value objects should be embedded in the table of the object they belong to but what if the value object is more than a simple object and contains many fields/columns or even sub-objects? I currently have a table named which in short stores rich text. Since I don't always parse the content I've added a caching system. The table also has a 1:n-relation to another table which contains a list of all the links in the table. 

is to distinguish between real persons, etc. is the name by which the person is mostly referenced. In the case of a real person it is the firstname and lastname. is the name inclusive all academic degrees (if any). A sample for a foreign key: 

I know that one-to-one relationships can be used to split data into multiple tables for performance or security and that it is used to create a is-a-relationsship. But aside from that things, what is the real-world prupose of an optional one-to-one relationship, expecially one where both sites are optional? 

What kind of machine is the data going to be stored on? Is it a shared storage devices? The ultimate factor that will dictate your query time is going to be your harddrives. Databases and their query optimizers are designed to reduce the number of disk I/Os as much as possible. Given that you only have 3 tables, this will be done pretty reliably. A harddrive's read/write speeds are going to be 200-300 times slower than memory speeds. Look for harddrives with very fast latency and fast read and write speeds. If all this data is on one 2-TB drive, you're probably going to be waiting a long long time for queries to finish. Harddrive latency is ~10-15milliseconds while the memory latency is less than 10nanoseconds. Harddrive latency can be 1000-2000x slower than memory latency. The moving of the mechanical arm on the harddrive the is SLOWEST thing in this entire system. How much RAM do you have? 16gb? Lets say that lets you hold 32 records. You have 16000 files. If you're going to linear scan all the datapoints, you could easily end up with 5-10 seconds in seek time alone. Then factor in the transfer rate 50mb/s? About 7 hours. Additionally, any temporarily saved data will have to be stored on the harddirve to make room for new data being read. If you're using a shared storage device that's being actively used by other users... your best bet is going to run everything at night. Reduce the number of nested queries helps also well. Nested queries result in temporary tables which will thrash your harddrive even more. I hope you have PLENTY of free space on your harddrive. The query optimization can only look at 1 query at a time. So nested select statements can't be optimized. HOWEVER, if you know a specific nested query is going to result in a small dataset to be returned, keep it. The query optimization uses histograms and rough assumptions, if you know something about the data and the query then go ahead and do it. The more you know about the way your data is stored on disk, the faster you'll be able to write your queries. If everything was stored sequentially on the primary key, it may be beneficial to sort the primaries keys returned from a nested query. Also, if you can reduce the set of datasets you need to analyze at all beforehand, do it. Depending on your system, you're look at around 1 second of data transfer per file. If you're going to modify the Name values(the varchars) I would change it to a datatype with a maximum size, it'll prevent fragmentation and the trade off is just a few more bytes of memory. Maybe an NVARCHAR with 100 maximum. As far as the comments about denormalizing the table. I think it may be best to just store the datapoints in larger groups(maybe as spectra) and then do the data analysis in python or a language that interacts with the database. Unless your a SQL-Wizard.