Normally this error has to do with a lack of resources. What else is running on this server that consumes memory? There is a good chance that due to lack of memory your server is swapping and doing so, it takes too long to start processes. Monitor your memory usage. If your application does bulk updates, without limits, this could easily fill your entire memory. See Bulk Processing with BULK COLLECT and FORALL for a good explanation. If your data set is growing over time, surprises are waiting to happen. Use limits in the pl/sql code. Shutting down a database because there are performance problems is not exactly a solution for a problem, it is the denial of a real existing problem. Normally a database is online and available, a solid piece of foundation for an application. If your database/data has any value for your company, hire a dba to help you evaluate your systems. 

I see no unusable index. What I do see is that I can drop a parent partition and this cascades to the child partition. does this help? 

Yes, you can. This is called tracing the Oracle optimizer. Doing so creates a trace file in which the optimizer dumps all reasoning done when composing a plan. An example of generating a trace for a specific SQL can be found here how to trace optimizer for specific SQL system wide - 10053 trace event You might need a little time to read it, given the amount of data found in the trace, it is amazing that the Oracle even finds time to do this and return in a decent time. 

If you are using the 11g duplicate from active database option, the only port that needs to be open is the listener port (usually 1521). If you are using any other options to copy your files to the standby host, the required ports for the protocol that you use (scp/ftp/nfs ...) Apart from the 11g 'from active database' option, RMAN does only read backupfiles that are present on the node on which the restore/clone is to be taking place. Present in this setting means that they thould be readable. The can be on [n]fs or tape, as long as your host can read them it is ok. I hope this helps 

No, the reading of your cats, dogs and humans tables wont block other processes that are inserting in them. 

The values of the PK comes from different sources: horrible. Using the sequence would be the preferred way. You can not tell what exactly is the next value for the sequence by lookin in the dictionary. There is caching in effect, obviously for performance reasons. If you look in the dictionary you will find the value that is valid after the cache ran out of numbers. What you don't know, is how far the cache is used. 

at this moment you have control_files that still refer to the original datafiles locations. They need to be redirected. 

Short answer: NO. The import does several things. One is creating tables and inserting data in it. Next come the indexes, constraints, triggers and other objects. There is no such thing as a prediction as how long it would take. For the reading of data, if the indexes, triggers and constraints don't play a role, you can assume this is a linear process, where normally 1GB takes 10 times the time of reading 100MB. For the other stuff this does not work but for the creation of a specific index - for example - you could check v$session_longops to find the time_remaining for that index creation. You still don't know what is next. To find what is next, you could create an sql file that contains the ddl that is to be executed. This could help in giving an idea what is still left to do. Don't forget to specify a large buffer for imp. 

If indexes are in place: updates in indexes are always logged. An other thing is: nologging is only done for statements that copy data. will always be logged, can be unlogged. 

This works. Be careful, I have seen ppl doing similar things without testing, effectively destroying databases, especially when using patterns and manipulating database objects with the results. If the output is OK, activate the execute immediate. 

1) copy the file to where you want 2) create a pfile.ora that references your control_files 3) startup mount your database 

Since no other sessions are accessing your table now, the lock shows as NOT BLOCKING. As soon as an other session tries to manage that table, it suddenly feels it is BLOCKED. The index is used so you can not manage it as long as other sessions are using it. Having indexes on havily loaded tables is for sure like putting a brake on the loader process. You could help yourself by checking out the docs. See Conventional and Direct Path Loads Maybe partitioning can help you. Than you can prepare a single table, load that, finish it's indexing and perform partition exchange. Just don't create global indexes on such a table. 

instead of looking further. He missed the point that the database was started during the cold backup. This is one of my reasons not to be a big fan of COLD backups. They make me shiver. Be smart and carefully read Oracle® Database Backup and Recovery User's Guide 11g Release 2 (11.2) It is worth every minute you spend studying it. Oracle backup and recovery is not hard, there are just a lot of options. 

In short I would say: NO. If you want to invest lot's of time ... you could generate SQL's and usage frequencies from awr sql history tables but the parameters will be the same for all occurences of an SQL since the bind variables are not recorded for every execution. I am not sure about hammerora but from swingbench I know you can create your own transactions and for that you could use awr sql history as a guide on how many tx/s to generate. It still remains a synthetic load. Smarter would be to capture the databases input from a proxy between the application server and the database. Oracle Replay has a price, for a reason. 

create a init.ora file containing the dbname, db_unique_name, and control_files parameter. The latter one points to at least one of your ctl files. With a little luck you can guess the dbname from the file names. install the correct oracle rdbms version in a location called ORACLE_HOME export ORACLE_HOME=/where/your/Oracle/product/version/home_1 is export PATH=$ORACLE_HOME/bin:$PATH sqlplus / as sysdba startup nomount pfile=your_full_name_of_init.ora from step 1 alter database mount - this accessed the ctl files and checks dbname. Fix dbname in init.ora if reported wrong and shutdown abort the instance to start over at step 5. now you are able to look into the original files locations. If your files are in exactly the same locations as the original database thi sis easy: alter database open read only; 

No need to fiddle with begin/end blocks. Can't handle sqlterminator inside command, no matter where it is, in a string or not, can't handle lines with forward slash alone on a line in a string. 

A residual predicate is one that has an hidden extra cost because the predicate has to be tested again on every combination of rows that is fetched. Probe Residual when you have a Hash Match – a hidden cost in execution plans 

I am not sure if this is what you mean but DbVisualizer does a very nice job displaying this kind of data in a graph. Look for the monitoring feature. What is does is collect data from a query in a grid and you can display the grid as a graph. In the grid you can address previous cells and use them to calculate a difference. Doing so allows you to show the time difference and the increment, effectively giving you a rate. It is multi platform and multi database in a very nice way. 

yes, you can, this in fact is default behaviour. See Database Administrator’s Guide chapter 29 Scheduling Jobs with Oracle Scheduler 

No, there is no simple query for this, if you did not setup auditing for this. You could use auditing to track ddl that changes tables. You could create a procedure that tracks the # of rows affected by dml operations by reading dba_tab_modifications, before the table is analyzed, and store that in an own table to keep a historical view on it. Not for the # of dml. If you have ASH available you could also find out the # of dml on a table and record those statistics in own tables. ASH tends to grow a lot so there is a purging policy on it. So, the simple answer is no but as is the case with many things in Oracle database, with a little creative thinking, it can be made available. 

Just save the ddl for the procedure. This can be done using sql-developer 'save to file'. Tell the client to start sql-developer, connect to the schema where the code should be written and have them run the script using 'start' or the equivalent for start '@' script_name The sql script should be located in the directory defined in the preferences for Database>Worksheet>'select default path to look for scripts' 

You don't want them to be running with the same parameters in the first place. Very likely that you want to enable the 11g features. Things that you might want to make sure to be at least equal size are the memory footprint, temp tablespaces and undo tablespaces. In Oracle 11g you might be tempted to use Autmatic Memory Management. There are some issues with it so it might be smarter to configure your shared pool and database buffer according to the 10g database. In the run check if you are not over or undersized. Best is to hire a dba to help you with the setup, next best is to dive into Oracle® Database 2 Day DBA 11g Release 2 (11.2) There is a lot to learn from.