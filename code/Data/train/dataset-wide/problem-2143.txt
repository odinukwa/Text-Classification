You did not get the point of SQL programming. You can have 1000 copies on multiple machines of a script to create/alter an object, but until one of those scripts is executed against a server, that object on that server will be unchanged. Yes, you can have multiple tabs/windows with the same procedure, but for SQL Server all of those are different/competing sessions and whoever is last - wins.On another hand: SSMS. You can have multiple copies of a script of the same object in different tabs, which is very handy - you can try different scenarios just by switching tabs. It is not job of SSMS to track your changes and keep records which script is a "Master Copy". That is YOUR responsibility. However, if you are in a project development, you and your boss can't completely rely only on your discipline. Project manager have to choose which way to do a source control. Here is a not full list of things, which can be done: - Implement Change Data Capture on the server; - Restrict permissions on Dev/Test and allow "playing" only on Dev or personal machine; - Use change tracking/deployment/source control tools, which will keep records of any changes. And do not be surprised with SQL. All developers are experiencing that issue with source control. See it here: $URL$ 

Although I don't have a local copy of the Stack Overflow database, I was able to try out a couple of queries. My thought was to get a count of users from a system catalog view (as opposed to directly getting a count of rows from the underlying table). Then get a count of rows that do (or maybe do not) match Erik's criteria, and do some simple math. I used the Stack Exchange Data Explorer (Along with and ) to test the queries. For a point of reference, here are some queries and the CPU/IO statistics: QUERY 1 

I have been able to update statistics on the underlying system tables. This has worked in my SQL 2008 R2 or newer environments: 

I'm a huge fan of the event-driven approach for a scenario like yours. Meaning, I don't want to sleuth who did it after the fact, I want SQL to tell me when it happens. For that reason, I'd suggest a DDL Trigger. Here's an example: 

Marcello, if you absolutely have to be able to achieve your goal with a single script, you could try iterating through the databases and running TSQL in the context of each database. An example is below with sp_executesql running sp_executesql from/in each database in the cursor loop. Note that @Tsql is static here, so it's declared outside of the cursor loop. However it could just as easily be built dynamically within the loop. This approach isn't easy. It might even seem a little convoluted. I'll let you be the judge. 

Finally found an answer: You can create database user, which won't have a SQL Login and won't be able to authenticate, but will have permissions and can be specified in clause for stored procedures and functions. The simple way to create such a user is just to use clause during user creation: 

That depends on a type of . If it is the impact is minimal, but it can be tremendous amount of I/O associated to a process and enormous amount of time. If you plan to do a then you need at least the same space in your data file and in a log file as the size of your index. In other words just triple size of your index. For instance: You have 1TB database with only one table which is also 1TB in size. If you want to build/rebuild clustered index on it you will need additional 1TB+ in data file and 1TB+ in a log file. Impact on TempDB is minimal. Be aware that there might be completely different results depending on current average page usage and on a new fillfactor. For instance if you have 60% average page usage and planning to fill pages 100% then for 1TB table you'll need only about 600GB of new space. Also, can have significantly lower impact on Log file when you use Simple logging with 0% fragmentation. If you don't like my answer, why wouldn't you test all your scenarios with at least 100GB table and post results here? 

You have to have AutoGrow enabled and do it on the primary replica. You can also do it in t-sql using the ALTER DATABASE command. 

I am trying to use the Generate and Publish Scripts wizard in SSMS 2012 against a 2012 SQL instance and am getting errors. Trying to generate a schema-only script of the tables and schemas. I allowed it to continue when there were errors. 50% of the tables fail, specifically in the Preparing table xxx and the result is error. The final result is failure. This is a db on a test instance, the db on the prod instance (same-ish) works great. I only have db_datareader and db_datawriter on the instances. I cannot view the error log as a result, I cannot view the event log either. I am running ssms on a third server. Where can I get more info on the errors, if they are even in either log, and would they be on the instance servers or on the sssms server? 

If the developers have individual logins, you might consider a DDL Trigger. Here's an example for the , , and events. 

From there, you could build a TSQL string of commands and execute it. (I hardcoded for the schema. Presumably this will never change.): 

Here's a blog post about handling events with DDL triggers with some further explanation: SQL Server Event Handling: DDL Events 

Here are two queries I have used to compare permissions between database users. The first shows the database roles (if any) to which the database user has membership. The second shows individual GRANT and DENY permissions. 

There's a lot of moving parts there. But you might be able to make it work for you. I wrote a few related blog posts about TRY...CATCH that may be helpful: The Unfulfilled Promise of TRY...CATCH Enhanced T-SQL Error Handling With Extended Events Part 2: Enhanced T-SQL Error Handling With Extended Events 

I'm passing in a non-NULL value for , so I fully understand why I'm getting the error on SQL 2012. Questions: 

The short answer is there is no way to configure SSIS for Always-On with SQL Server 2014 and below. SSIS, just like SSRS, SSAS, jobs, logins, alerts and other minor things that operate at the Instance level are not Always-On aware...Always On only keeps databases in sync. There are strategies for mitigating all of these items however. For SSIS and business-logic jobs (everything except backup/DBCC, etc...) we have a totally separate instance for running those jobs (generally referred to as a central job server). It does not have any high-availability built in but generally jobs and SSIS packages are periodic so they have more tolerance for down times and could even be run from a test server in a pinch with connection changes. SQL Server Failover Clustering operates at the Instance level and is the High-Availability solution that supports SSIS/SSRS/SSAS/Jobs,etc... Always-On High Availability is closer to Database Mirroring and can use local storage but there is a cost, keeping all these other things in sync. 

This is the very general outline, go to Brent Ozar's site and download the AlwaysOn Availability Groups Setup Checklist to really do things right! 

I have inferred that the Windows OS does not actively "push" the OS level memory availability status to SQL Server. Rather, the SQLOS Resource Monitor must actively ask for it via the Win API function. This would seem to be an asynchronous operation. Am I interpreting this correctly? If my reasoning for #1 is sound, what causes the Resource Monitor to get the OS level memory availability status? Is is performed on a schedule? Event driven? How much time might pass from the moment there is low memory at the OS level to the moment the SQL OS Resource Monitor becomes aware of it? When SQL Server reduces memory usage to free memory back to the OS, just how bad is that? Is this an "expensive" operation that should be avoided at all costs? 

Here's an example that can be run manually in a single batch from SSMS. Just make sure to replace all occurrences of "2016" in the script with your SPID. 

Create and start an Extended Events Session: it will capture events, filtered primarily by SPID. Execute a statement in a block. Within (or after) the block, read the XEvents session data. Use the available data to respond to the error(s) as appropriate. Stop and drop the XEvents session. 

Pick a category for all the jobs that you want to only run on the current primary replica, I use the ag name Set up a SQL Agent alert for error number 1480 (AG Role Change) that will call a stored procedure in master via a job passing in the category name Build the sp to check to see if it is the primary, if it is enable all the jobs categorized with the parameter. If it is not the primary, disable all the jobs categorized with the parameter Build some functions to help recognize if the AG is Primary or Secondary 

There is a very detailed SQL Magazine series of articles by Michael K. Campbell that give code examples for how to structure either solution, I started with that article and only had to make minor modifications. 

You have to actually create the Availability Groups first, then it will show up in FCM. You have basically finished the prerequisite steps to allow an Availability Group to be created...so let's go create one! 

You can enhance error handling with Extended Events to overcome the TRY...CATCH shortcoming you are experiencing. The design pattern involves these steps: 

I have a few "gaps" in my understanding of external memory pressure. I've found quite a bit of helpful information online, including this info from SQLSkills.com. 

This could easily be put into a job step for a SQL Agent job. I tested this on SQL Server 2012 w/ SP3. I've also run similar code in SQL 2008 R2 and SQL 2014. I don't know for sure if it works in SQL 2016--please let us know if it does not. Other Notes The commands worked when I was logged in as a [sysadmin]. It also worked if the login was not a member of [sysadmin], but was the database owner. Membership in [db_owner] alone was not sufficient. (Again, this was on SQL 2012 w/ SP3. YMMV.) 

2nd Attempt Here I opted for a variable to store the total number of users (instead of a sub-query). The scan count increased from 1 to 17 compared to the 1st attempt. Logical reads stayed the same. However, elapsed time dropped considerably. 

There does not seem to be an issue for me querying a Linked Server from a partially contained database when logged in as a contained user. I would always recommend using a synonym. I did this on a stand alone SQL 2014 instance, using 4 part naming or the synonym worked fine. When logged in as a contained user you cannot see the Linked Server, but it works fine to call it. Don't see why it would be a problem with Always On Availability Groups...just create the Linked server on each replica, and call it. Not sure what you mean by secondary replica, do you just mean in a 2-node AG? Give it a try on your instances and see what happens. 

You will need to create an identical linked server on each of your 3 Availability Group nodes (server1, server2, server3). Linked servers (like logins, alerts, and jobs) are instance level objects so do not sync between the Availability Group nodes. You need one on each node in case of a failover, and you need to keep them in sync manually if they ever change. Once you import the data, referencing the linked server in your script on your primary node (Production listener) and preferably with a synonym, it will sync to the secondary node DBs in the Availability Group automatically. Synonyms are a Best Practice when using linked servers. If your linked server name changes you just update the linked server, not the code that imports the data.