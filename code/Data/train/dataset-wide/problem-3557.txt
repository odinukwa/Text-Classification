Didn't really believe it was DOA first time. Definitely don't believe it's a DOA now. But if not, then what is it, and what can I be missing? I've tested everything (AFAIK) in the component chain, the HBA just doesn't have much OROM interface that can go wrong, or any options to recognise SATA/SAS/both, or anything like that, and the main PC/server in both cases just leave detection to the HBA/RAID card. I've tested on two completely different platforms, with two different models of controller cards, with SAS vs SATA drives, and I'm utterly stumped. (Note: I'm slightly limited as I'm starting to transition from SATA to SAS, with the intent being to replace SATA by SAS as they wear out, so at the moment I don't have any other SAS disks or cables to test with, which I would otherwise have done too. But I think I've probably covered that by testing the cards+cables while varying SATA/SAS) Updated for more accurate title to help others, now more info obtained. See answer. 

I've done a fair bit of troubleshooting but I'm completely at a loss what could be going on. Hardware / platform 

Can I move a Storage Spaces virtual disk from one pool to another? Or do I need to create new volumes and copy the files with the operating system? 

With the described configuration, I have a SQL Agent Job that includes a Powershell step. That step throws an error when it gets to the line $app = start-process -passthru $program $program_args saying - "The error information returned by PowerShell is: 'The term 'start-process' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again. '. Process Exit Code -1. The step failed." The lines preceding this seem to work ok. I suspect that the Powershell host that is running the script is crippled regarding that function but I am not able to verify this anywhere. 

I have two disks from an old server that I believe were a mirrored set. I placed them in a new server and tried to do a zpool import but nothing was found. Does that mean the data that was on them previously is lost or is there some way to reconstitute the pool? 

After reading the original email thread and @ewwhite's answer which clarified it, I think this question needs an updated answer, as the answer above only covers half of it. As an example, let's use the output on my pool. I used the command . On my system I needed the extra arg to locate the ZFS cache file for the pool, which FreeNAS stores in a different location from normal; you may or may not need to do that. Generally try without first, and if you get a cache file error, then use or similar to locate the file it needs. This was my actual output and I've interpreted it below: 

As the question says. There's a lot of threads around on NFS vs. Samba/SMB, but a lot of them are outdated or refer to old security models, or just give a one-line "use SMB with Windows". Both modern Windows clients and modern *nix file servers can handle both of NFS or SMB/Samba. Whichever protocol is chosen, one will be using a "native" protocol and the other won't. So in mixed environments (*nix server/Windows clients) it's not as simple as "for X use Y". So I'm interested in actual pros, cons and experience. The few threads I can find that cover modern incarnations of these protocols suggest as possible differences: 

As suggested, the PROPER way would be to change the name back temporarily, run the STSADM tools and change the name to the desired one. The reason the name change prevents the instance from starting is that there are several places in the registry that refer to the location of the Central Admin and Content Databases. Specifically, HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Shared Tools\Web Server Extensions\14.0\Secure\ConfigDB, allthough there are others. If you search for the old name with a backslash and the word sharepoint(server\sharepoint), you should find them. STSADM can't make changes to the Config DB unless those references are right. Between those two things, you sholud be able to get it running with re-installing. 

I have a situation where I need to make the best of a bad DSL situation. The CPE is a black box with no access to DSL diagnostics. My plan is to get some sort of DSL hardware that exposes link-layer state and gives me knobs to tweak. I'd like to be able to mitigate bufferbloat as much as I can while I'm at it. The obvious choice would seem to be a Sangoma card in a linux system. I have no way of knowing if that will do anything for me without testing it, however. I have no other access to WAN troubleshooting equipment. Are there any other options avail to me as a consumer? 

Problem / troubleshooting so far This is a new server being set up, so all components are new, although some have been tested already before now. On booting, the HBA didn't recognise or report the 6TB SAS drive (either via the main BIOS or via its own OROM -> SAS topology) and the 6TB drive was cold and not spinning up. No other drives were connected. The rest of the system works fine, so on the face of it the issue is limited to one or more of bad HBA, bad cable, or bad drive. Troubleshooting steps so far: 

As FreeNAS isn't a router or firewall it doesn't have much built in to do this, so I'm not sure how to go about doing these things. It can't be uncommon to have it directly connected to the general LAN, so I'm hoping there's a straightforward helpful answer to the above 3 approaches, so I can choose which works best for me and figure out how to combine them if useful. 

This is related to this question and extends it. The symptoms are the same - 2012R2 x64 with 64GB RAM (21GB used), and both work fine, but IE won't open web pages, returns an error 1450 "Insufficient system resources exist to complete the requested service" and PuTTY gives the error "Network Error, no buffer space available". I could follow the suggestions in that question or increase various TCP parameters, but I suspect the problem really lies elsewhere in some process or other and I'd like to solve the underlying issue if I can because it's recurrent. The problem is that most online solutions seem to refer to Windows XP and x86 architecture, and their solution is "use an x64 based system", so I'm not sure how to adapt them for x64 with ample memory. Alternatively if it is a single process leaking kernel buffers, how would one view the open buffer count for each process, so that the process responsible for leaking or holding buffers can be closed or avoided, without killing user and system processes by "trial and error"? Relevant registry settings: 

Consider a configuration with two identical WS2012R2 servers. Both have quad-port network boards. Each has one NIC connected to a subnet shared by other servers and ultimately accessible to client machines. The remaining three ports on each server are 'nic teamed' and connected to a different subnet shared only by the two servers in question. What I would like to see is SMB traffic preferentially going over the faster 3-nic path and only resorting to the more congested 1-nic path if necessary. Can I trust the load-balancing in SMB3 to do the right thing here? If not, can I apply some sort of weighting similar to tcp/ip route costing? 

There a a few questions related to using DFSR and Shadow Copies together, but none that indicates if Shadow Copies replicate or not. Meaning, if I have a a pair of DFS replicas with Shadow Copies on Server-A, can I revert that file to a previous version on Server-B? If so, will that reversion be replicated back to Server-A? I suspect not- that VSS is a local NTFS feature and outside the scope of replication, but I cannot verify that myself at the moment. 

I have an LSI 9211 HBA and an Intel 910 SSD. I updated the firmware on the HBA, using LSI's usual sas2flash utility. I removed all other LSI/HBA cards first. Unfortunately it now looks like the SSD also has the same controller internally, which I had no realistic way to know in advance, and now Intel's SSD toolkit won't recognise the SSD so I can't figure how to put it back as it was. Sas2flash -listall shows the SSD with P20 LSI bios. What do I do? 

I didn't get an answer here so I asked on VMware communities. The best answer I got was that there's a VMware "fling" called visualesxtop (on the VMware flings website, best found via Google in case URL changes), which produces graphical and tabular monitoring data for an esxi host. Works great! 

There are two points here - what works, and what you should (or should not) do. When I'm done with an old HDD, I open the top and heat it red-hot internally with a small DIY gas torch. It takes a few seconds from start to end. No magnetic data is going to survive the heat rise, which destroys/randomises the magnetic domains with absolute certainty, even if the plating on the platters wasn't oxidised/charred/burnt off and peeling. The case is easy to open too. Notice the emphasis above: it's what I do. Almost certainly it isn't what you should do as a business. Nor is drilling, acid, electrocution, thermite, or any other fun activity. There are serious issues to consider before letting staff loose on the disks. As an individual I'm fine doing what I prefer. As an employer your company is probably legally liable for staff safety and any accidents (in most if not all countries). I wouldn't allow my staff to do what I do personally. All it takes is one accident with a drill, due to exuberance or carelessness, some metal swarf to hit an eye, or anything else, and you can expect a visit from the lawyers who will ask you exactly what training and control your company gives, when it turns ugly. Most of the alternatives suggested in other answers are a lot of fun - until they go wrong. At which point one person is in the line of fire. You. Alternatives - top off case (ensures exposure as other answers state), and ideally some action that physically damages the platters (in any manner) but doesn't incite reckless conduct or risk an accident. Perhaps buy a hand-held demagnetiser (mains powered, produces a powerful local magnetic field designed to randomise data, has little or no harmful potential). Less exciting but a lot safer. 

If I administrate an on-site WSUS instance, I am responsible for releasing updates to domain-joined PCs according to some policy guidelines. Is there some sort of metadata or technique I can use to assess the impact to end users before deciding when to release an update. The specific scenario I am trying to address is the occurrence of apparently smaller updates that require restarts and take an absurd amount of time to complete. On several occasions I have seen PCs that attempt to 'shut down and apply updates' from the Start Menu at 5 PM and sit for over a half hour while they grind away at the hard drive. These are modern, fast machines with SSDs and 8Gb or more of ram. My only theory is that these updates must be scanning the whole disk for some reason. If these updates are flagged in some way, I would like to hold them back until a planned downtime day. Are there any other options beyond applying them individually and timing them with a stopwatch? 

The PERC 710P supports both SAS and SATA disks. I have a single RAID1 set and would like to add a RAID5 set built out of SATA disks for archival use, but I cannot confirm that a configuration like that is possible. Citations requested if possible. 

I spoke to LSI (now Avago) tech support for storage, in Germany. They considered that if 2 different kinds of "known good" controller in 2 different machines both recognised all sata but not this sas drive (on any port and connector) then it was very likely the drive. They also suggested a further test - to connect the power side of the drive only (NOT HBA/motherboard/data wires) and turn on the server. (He warned me it would "sound crazy"!) Apparently like SATA, SAS drives spin up when they are first powered if the data side isn't connected (I didn't know that, wonder how staggered start works then?), providing a very good test that relies only on the PSU and power feed to the drive, and nothing else. Sure enough, SATA drives all spin up, this drive doesn't. He felt that was enough to be "almost certain" its a 2nd bad drive, however unlikely, without spending cash. The serial number was also almost identical to the original dud drive (1 digit change); so he also suggested speaking to the manufacturer and raise the question if they have any other similar reports for this drive, as it could be a bad batch. Update April 2017: I thought for a while that the issue was the LSI 9211 BIOS needed to be disabled, based on an online thread. I disabled the bios and it did work... but later when I moved the box it stopped working and I couldn't figure why. I took this info back to LSI tech support and they said it wasn't possible that the BIOS could be the issue or disabling it could help. They felt that moving the box was likely to have undone a fortuitous cable working and turned it back to not working. They said to try a new "forward" or "fanout" cable, and specifically, Adaptec (on the side, as it's a competitor!!) which are more reliable than most for SAS. They said that it wasn't always clear or marked whether a cable was the right kind or not, and to check carefully. The exact SAS cable one would need will vary depending on what interface the HDD and card have. The 9211 has a SFF-8087 connection and my HDD has an SFF-8082 connection (looks a bit like SATA but power and data ports joined). I was dubious about it being the cable (since the cable did work fine on electrically similar SATA), but went ahead and contacted Adaptec who commented that getting the cable right can be quite challenging in the sense of being sure exactly which kind of cable is needed. They checked the card specs and HDD specs and recommended their 2275300-R on Amazon, and much to my amazement it worked first time, so I guess they must have known what they were talking about.