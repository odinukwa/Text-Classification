Use the tools that you are most comfortable with. Your situation seems simple and lasts only for a short time. If you have to keep this up and running for a long time, you might end up by using more advanced tooling, where you define rules for conversions and synchronizations to be performed for you. Doing so is easier to maintain and to document. The learning curve might be a little steep. On the other hand, you could use this case to train yourself. There is no urgent deadline and when it gets more urgent, you probably build the home cooked version within a day. 

If the files happen to be from a valid backup you could have some luck, but mostly because you could turn in into a running database again. The complete system tablespace should be among the files you have to make a chance. Only reading data from the bare dbf files .... there are many options that make this hard to do. Depending on what you know about what you are looking for makes this more or less possible. Start with: forget it and be happy with any word you can recover this way. dd and od might be your friends, compression and encryption not. Maybe Kurt can help you, see dude could be your best option. 

For inserts, you can use . This lets you update certain fields if primary key is already used. The syntax would be something like: 

I don't think there will be a noticeable difference. Query optimization has very little to do with the syntax of your query and a lot to do with the RDBMS query optimizer. The optimizer pulls apart your queries and optimizes it as it sees fit. 

is file in your directory. The meaning of tilde in unix is current user's home directory. The values from different configurations are overwritten. So will overwrite was what defined in , and will overwrite what was in previous files. 

When you are copying these files without detaching the database first, you are risking to corrupt your backup in the event that will be synchronized during the copy procedure thus rendering the backup broken. 

If you use , MySQL will use timezone which is configured in . If no value is set in , then MySQL will determine your operating systems' timezone and use that upon startup. You can see how your server is configured with this query: 

It really is to be the number of physical disks, in the old fashioned style, not using a SAN. The best article about this I found here $URL$ Most others blogs/docs I found were more like copies from manuals than original contents that explains something new. When using this on a SAN, the cache is going to play a big role, as is the inner working of those beasts. For example, they write in a buffer and give an acknowledgement to the rdbms that the write is complete, even when the data did not even come close to the intended disk. When continuously other blocks are written, that cache could be choked and then we get a better view of the real io performance. Reading and updating blocks from a regular sized datafile won't be sufficient to reach that. 

Most of the time the session tries to tell the client something nasty happened. If the client is gone, this will never be acknowledged so the dbms will wait forever. When you kill a session, the connection between v$session and v$process is gone. The process of the killed session remains in v$process. You can find the OS pid for these processes by checking the processes that are not PQ slaves without a session. Kill those processes from the OS. Oracle is very robust and if anything should be recovered, smon takes care of that, if needed. The state of the database does not depend on a client that gets killed. Would be a bit strange since this would mean that if a client crashes it would corrupt the rdbms... If a database gets - logically - corrupted it would mean that the client did not make use of the regular transaction mechanism. The following SQL can be used to identify the processes that have no session tied to it: 

Secondary indexes (non-primary keys) in MongoDB and MySQL are very similar. Secondary indexes declare fields or columns to be sorted separate from the rest of the data, and use row identifiers to reference the rest of the row for a query. 

You can try creating a new table according to old one's structure. Not sure if this will realy help as you say that your original table is corrupted: 

Every process under linux runs under specific user privileges. Services (like MySQL) usually need to open ports and access various system resources during startup, so they are required to be started as user. However, it is not safe to have all the processes run under as it is not required for continuous operation of services, thus it is recommended to create a special user, which will be used to run MySQL service. MySQL will only be able to access what special user can, and this is going to be limited to MySQL files on the system. This is usual practice in linux. If you, however, use you distributions built-in package manager to install MySQL, this will be done for you automatically (in most distributions at least). 

You have 32G real memory on your system, use amm and have almost 25G for memory_max_target. That does not leave a lot of memory available for other tasks. Chances are that you SGA is swapped out of memory. Also the connection pool is a bit big. Can your system handle 350 concurrent running sessions? I would start with a number that is close to the number of CPU's of your system. AMM is OK, as long as your application is a perfect Oracle citizen. This means, it does use bind variables where possible and only uses literals in SQL for class selections. If your application is not such a perfect Oracle citizen, start with setting a fixed db_buffer_cache size to protect the database cache by being pushed down in size by the shared pool. If your app uses many literals, it won't re-use cursors -but still tries to cache them- and waste valuable shared pool space. Check v$sql .... do your cursors have high 'executions' or do you have many app SQL that have executions = 1? To prevent swapping out the SGA, use LOCK_SGA=true and use larger memory pages, say like 16MB page size. This gives better use of the memory and reduces CPU usage. As mentioned before .... the swapping in itself does not need to be a problem, if your app keeps running as intended it could be OK. It could run quicker if it would not happen. best tip: hire a real DBA to do some serious diagnosis. It is a job that requires a little more than just being able to install software and create an empty database. 

MySQL queries are not case-sensitive by default. It is possible that you have created case sensitive tables when importing data. Check if you have collation, that makes it case-sensitive. Reimport your data then using . Also, if you have collation, it will make queries case sensitive. Your collations changed when re-importing data. 

Also, it is possible to set timestamp per session. is used to always get timestamp in UTC, no matter what MySQL server's timestamp is configured to. 

MySQL has "General Query Log". This logs everything that is going on MySQL server: users connecting, disconnecting, queries etc. This query log is a file on your filesystem or (from 5.1.6 versions) table Control the general query log at server startup as follows: 

The answer relies on what kind of data you store and how. I would never suggest making separate database. Depending on the data you can either: 

I would use for this. - $URL$ - The script imports only a small part of the huge dump and restarts itself. The next session starts where the last was stopped. 

The fiddling with controlfile is only needed when you want to rename the database, what not is what you want. In you case ASM does not change anything for you. 

No. The words Secure Files should be interpreted that files can safely be stored in the database. This is often safer than storing them on the FileSystem with a reference from the database. If the file is in the database it is also protected by many of the database features. SecureFiles is the modern implementation of LOB's. The new implementation has a great performance boost compared to the old inplementation. dbfs is a fileystem that can be created inside the database and that can be mounted on Linux using dbfs_client. Regular operating system processes see it as a regular POSIX filesystem. The LOB's in a dbfs should be implemented as SecureFiles to have the best performance. The performance of dbfs is better than that of NFS, if using filesystem like logging. With normal logging dbfs has about the same performance as NFS, with the added possibility of replicating the data to standby databases. 

Yes this is normal. When RAM is no longer needed it is not freed at the same time. It is kept as cached in case the server would decide that it needs to access it again. This would save you extra time that you would otherwise need for data to appear in RAM. Cached memory is freed only when new applications request more RAM. 

After starting query log, investigate the file (or table) for further information. MySQL Query Log Documentation: $URL$ 

Which way to go depends on your data, the way you plan to perform queries etc. and is too broad question to answer. You will likely have to apply different method for different tables. 

I don't see why would't you just store the coordinates for every user. You can keep them either in the same table as users or in some kind of user details table, depending on how your current schema is designed. There is no point in having all geo to postcode locations mapped. To get the locations you can use something like google maps API. 

To me it looks like you did not start the listener but it is started. Your problem seems to be the tcp connection and not the ipc. So the listener is running, you can not reach it using tcp. Test this using If the telnet does not give a connection, a firewall is blocking you. Stop/edit the firewall. 

The default location for the password file is $ORACLE_HOME/dbs but since Oracle v12 it can also be stored in ASM. In that case you find it using srvctl 

Yes, you can and it is quite easy too. In Oracle, the ORACLE_SID is just the name for the Oracle Instance and has not very much to do with the DBNAME. A database with the name PROD, can be served using Instances with any valid name. There is no direct connection between the SID and the DBNAME. This connection is made using the parameters. The parameter file is identified as init${ORACLE_SID}.ora or spfile${ORACLE_SID}.ora In the parameter file is the parameter db_name. This is where the connection between the Oracle Instance and the database is made. So, you don't need to re-create a controlfile, you don't need to use nid, just make sure that your parameterfile has the right name, bring down the old Oracle Instance and start the new Oracle Instance after having set ORACLE_SID to the new Oracle Instance name. The parameterfile and the password file are both found using the ${ORACLE_SID} as part of their name. Re-creating the controlfile is only needed when the DBNAME has to change. nid is needed after a clone operation where you need to change the DBID to prevent accidents that could hurt the backups of the source database.