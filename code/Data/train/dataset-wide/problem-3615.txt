I'd say this is untrue anyway. Print servers are about centralised management and distribution, not about offloading work. Have you considered simply adding a different basic 64bit postrscript driver on the print server? This would probably get you the same result as the CUPS solution, with less mess. 

I don't have it on a clean Server 2008 R2 build, either, which could be correct according to this Technet article: $URL$ I do, however, have: 

Confirm that the staff aren't local administrators by checking the local Administrators group on the machine. You should be able to remove these by creating a GPO which manipulates the local Administrators security group. This can be done using the "Restricted Groups" section in your GPO: Computer Configuration -> Policies -> Windows Settings -> Security Settings -> Restricted Groups (NB: Be aware that this will effectively overwrite the Local Admin group so ensure you include anyone that's required to be a local administrator) Or, if you've set up Group Policy Preferences, you could use that too for more flexibility. 

Wow, that's a lot of questions. You need to get back to basics - get an unmanaged switch, stay away from your corporate LAN and build a Server 2008 R2 Domain Controller with DHCP and DNS and go from there. 

Yes, batch files support IF statements, and you can use any of the system variables (Type "set" at the command prompt to get an idea). Combine that with "labels" and you can easily create a decent script. I'm going to deliberately not Google that for you, though. Personally, I'd set a custom Environmental Variable defining what 'type' of machine it is in reference to whatever it is you're doing. 

You can SSH to iDRAC - so if your UPS has some kind of management agent, then you could run this in a virtual machine with a trigger to run an iDRAC shutdown. HOWEVER, this will not do a clean power down of your VM's, which is the critical component and the whole point of a clean shutdown. An ESXi server is easy to rebuild - your VM's are not. Therefore you'll need to shutdown your virtual machines somehow. I have no idea if ESXi free can do this from the command line. If not, you'll need to remotely shutdown each of your VM's using whatever tools their operating systems provide. This, does, of course leave you in a sticky situation because your scripts will be running on a VM, too. So, you'd be better off running your scripts in their own dedicated VM which takes the risk with an unscheduled shutdown as the host goes down. Alternatively, you could purchase the proper tools for the job and use the VSphere suite with VCenter. 

The list really does go on and on and every company will have their own justifications for it. The one thing I do believe, however, is that the main winners are the IT Department. Now, don't get me wrong - this can be good news for users in an organisation because they get things fixed and rolled out quicker, but with the exception of remote access I suspect most users would prefer their own machine (when it's working properly) With regards to Cost - You're completely right. Anyone who attempts to use VDI as a cost saving measure is going to end up in a world of hurt. VDI done properly is expensive, there's no doubt about it - you need good people, good project management, good hardware and a good design. Done cheap, VDI will be nothing but a pain and your users will hate you. That said, VDI can be cheaper administratively in the medium to long term, so it can definitely be cost effective, but I never treat it as a money saving measure. So, it that good? It can be - but doing it properly is complicated. I've had excellent feedback from users - especially those moving from old XP clients to a new Windows 7 VDI deployment, but I've also seen some absolutely horrendous engineering. The problem being that if you lose the buy-in from the users, it's borderline impossible to recover from - they'll simply see it as "rubbish". But, I am sincere in my belief that VDI can provide an excellent user experience. You're not going to be able to virtualise every workload (We're kind of where servers were 8+ years ago) but most office workers make excellent candidates. Interestingly, the last organisation I worked in even put their IT team on VDI (Save some emergency Fat Clients for obvious reasons) As I say, this answer could go on for ever - but to answer your question. Is it a silver bullet? No, absolutely not - it won't fix everything and not everybody can work well on VDI. Is it an excellent option? Definitely. 

The only way round this that I could think of would be to run a local proxy which does rule based forwarding. Perhaps it would help if you told us a little more about the problem you're facing. 

You're being painfully vague, but they're saying that they'll create a service account for your service to log in as and run under. However, there is no actual concept of "service accounts" in Active Directory or locally*, so in every way this is just a standard user with whatever rights are required etc. They may choose to restrict it as much as is reasonably possible (NB: Or, they may not!) but ultimately it's going to need access to some things for your software to work. If somebody got hold of the username and password then yes, they could in theory use it to authenticate and access whatever your software accesses. However, this isn't a security risk in itself - no more than having administrative accounts etc. The answer is the same: Make the password complicated, random and release it to only those who genuinely need it. We'd need far more detail on your environment before we could recommend other specific ways to mitigate risk, but I'm not sure it would do your professional relationship any good to approach them with said answers. *The idea is, however, best practice, well documented and understood. But you must understand there is no "Service Account" user type or anything. It's just a descriptive term. 

What's your upload speed. I'm assuming this isn't a leased line, so it's almost certainly different to your download (I.e., quoted) speed. In short, I'm almost certain that this is a) Not an issue with your server b) Not a fault Also, I'm not understanding what the difference between "server speed" and "internet speed" is. Can you be cleared on your infrastructure? Finally, can you be clearer whether you're talking about Kilobits or Kilobytes and Megabits or Megabytes. A 20 Kilobyte per second upload is 160Kilobits, which is probably average for a 1Mb domestic broadband connections. 

Firstly, Citrix is immaterial here so you can use any standard resolutions. I think you need to read this page: $URL$ Specifically the last section titled "Applying Outlook user profiles by using a PRF file". You should be able to put the reference to the PRF file in the registry: 

This is essentially VDI (Virtual Desktop Infrastructure) and the two big commercial players would probably be VMWare View and Citrix XenDesktop. XenDesktop can certainly do as you require, though it's far from open or free! I'm pretty certain VMWare View also ticks all the boxes, but I've not deployed that beyond test environments. You may also look into VDI in a Box as another commercial option as this requires less core infrastructure. 

Surely the answer is reasonably obvious. If you have (and configure) 4x 1Gb NIC's then you have the potential to attain 4 Gigabits Per Second of data throughput if all 4 IP addresses are hit hard. If you configure all 4x IP's on one interface, then you can only ever get 1Gbps shared between them all. There's nothing inherently wrong with having multiple IP's on one NIC, but if you can spread them across multiple physical interfaces, then there's absolutely no reason not to. 

Falcon covers it pretty well, though it's also an RDS/RDP feature, rather than being Citrix specific. Your information is sparse, but the easiest way to overule this is to set the Disconnected Timeout to 1 minute. You can do this on individual servers or through Group Policy. This will have the same effect as your script, but will be more 'industry standard'. 

Define propagated, and define your other DNS servers. DNS records are valid for as long as the other server considers them valid - once that time's up, the other DNS servers will forward requests onwards. There is no "pushing" from your DNS server, it's purely a pull mechanism. So, to answer your questions explicitly: 

That's certainly how I would've done it - though I always avoid using local users for anything. In my mind, local logins are to get you out of trouble. Personally, I would have installed everything with a domain admin account and then just added the "Domain Admins" security group. Or, even better, create a service account. 

You should get site 2 a permanent, reliable internet connection and use a permanent site to site VPN. You'll need a router each side capable of doing so, though. 

The problem with this question is that the term cloud doesn't have any single, standardised meaning. This makes it very difficult to define things concisely across vendors etc. All we have are loosely defined terms that mean one thing to one group of people and quite another to others. That said, a "private cloud" generally means infrastructure which is owned, controlled and used by a single entity. I can appreciate your question, but honestly, it really depends on exactly what type of service your talking about and what the marketing person / vendor meant when they wrote the term. 

You're describing the Citrix Web Inteface which works with XenApp and XenDesktop. It's also compatible with the iPad etc. 

It would be trivial to capture this information using a variety of monitoring software or packet analysis. That said, I'm voting to close this question as off topic because it doesn't feel like a professional question. 

HVAC - Cooling, but also ensuring it doesn't get frosty or damp in the winter. Remember you'll need water for air con. Power - How will you get sufficient power to the container and what about redundancy and resilience? Connectivity - As above, how will you connect? Will you risk running a high cable, or pay to dig? Again, redundancy, future proofing etc Security - How will you physically secure the environment from unauthorised access? Access - Opposite to the above, will you have sufficient access to get in there and get things done Weatherproofing - How weatherproof are the containers and what will you need to do to make them truly weatherproof. This depends on your environment, but I know I'd be tense when it was raining heavily. This also means your utility connects all need to be seriously weatherproof and that you have room to open the door without splashing everything! 

My second thought was to find a relatively unique form name, such as and search the whole registry. Surprisingly, the entry crops in quite a number of places. My suggestion, therefore, would be to build a blank 32 bit machine (32 bit to avoid confusion with shadow keys) and go to town on some of those keys. If you get it working it should be easy to transpose to wherever you need to do this. However, it has to be said, that given how much Microsoft clearly don't want you to do this, you are opening yourself up to support and compatibility issues. 

I have an image of a Server 2008 R2 system which has been Sysprepped and was shutdown prior to its first boot. Now, it would seem I screwed the XML file up as it fails halfway through the Serialize section when booted. (To be clear, it fails on the boot after taking the image) What I'm wondering is, can I alter the Unattend.xml file at this stage using my ability to directly manipulate the saved image? If so, do I edit the original (In the sysprep directory) or is it transferred elsewhere? 

You've got the wrong hotfix - you've downloaded the one for Server 2008 R2 / Windows 7. Yours should be called Windows6.0-KB2600100-x64.msu This is the download link: 

To my knowledge this isn't possible - the "Applies to" description is merely a string embedded in the adm(x) file. There's no "IF" processing for this - regardless of what version of Windows processes the Group Policy it'll set that setting, whether it has any impact or not. Group Policy is exceptionally quick at processing individual registry based settings and I highly doubt this is the root of your problem, though. 

By the way, your script doesn't work because all you're getting is the Powershell return code, not the data it produces. There may be a way to get it to work, but it's ultimately pointless compared to just using a proper Powershell script. For completeness, here is a nice article from MS on Powershell and Environmental Variables: $URL$ Update: Having reviewed this solution with @syneticon-dj in chat, it appears the issue you face using this method is that a command prompt has to be reloaded before it will reflect changes in environmental variables that have happened externally. You haven't provided much detail about what it is you're doing, but if this is the only reason you're launching Powershell, than my actual suggestion would to review how you're doing things. Either do your whole process using PowerShell or have you considered using scheduled tasks instead? You can schedule tasks based on the day of the week. 

This is a classic case of trying to solve a people problem with a technical solution. Is it possible? Not as standard, no. Is there a workaround? Almost certainly, you could run a script or service to monitor the folder and rename things. Should you? In my opinion, no. Any attempt to come up with some hacky solution will only lead to excruciating pain down the line, probably won't ever work properly anyway, and you'll then become responsible for the entire naming scheme. Conclusion: It'a a management problem. The offenders need to be told to work properly and adhere to policy, just like any other workplace rules. That said, it seems like you need a proper document management system. Some of them are very powerful, though they still require the user to use it properly! 

This is usually defined in the user properties panel in Active Directory. Check there and change if necessary. 

From my testing, I believe it's got to be the NetScaler that's sending that spurious FIN (I've tested from a machine in the DMZ, via the same Firewall etc and it doesn't have it) and I don't think it's out of the realms of possibility that an embedded device may not like it. So, can anyone explain why NetScaler is doing this and how I can change it? I presume it's a TCP Profile configuration but I can't seem to figure it out. I've attached two screenshots of the capture - both captures are using the same Client <-> Server and the same method (Putty). The only difference is whether I connect through the SSL VPN or not. Update 1 In a moment of wondering, I changed the Web Server to listen on Port 81, instead of Port 80. With this configuration, the handshake is correct with no spurious messages. This leads me to believe it's going to be something to do with the HTTP optimisation on NetScaler