Another term that has been used is "unilateral" or "unilaterally connected". I don't have a particularly strong opinion in favor of this terminology, but I am slightly opposed to just calling it "connected". (I usually assume "connected" means "weakly connected" for digraphs.) However, I must admit a reference by Tutte is good. Some references for "unilateral": 

Shall we try teamwork? Please feel free to edit this post if you have simplifications. The original sum may be re-expressed as $$ \frac{1}{2^{2m+1}} \sum_{k=0}^m (-1)^k \binom{m}{k} \binom{2(k+m)}{k+m} \frac{1}{2^{2k}} \sum_{j=0}^{k+m-1} \frac{2^{k+m-j}}{(k+m-j) \binom{2(k+m-j)}{k+m-j}}. $$ If we're trying to prove this is 0, we may drop the fraction out front. Also, change variables from $j$ to $\ell=k+m-j$: $$ \sum_{k=0}^m \left( -\frac14 \right)^k \binom{m}{k} \binom{2(k+m)}{k+m} \sum_{\ell=1}^{k+m} \frac{2^\ell}{\ell \binom{2\ell}{\ell}}. $$ At this point, my idea was to change the order of summation based on $$ \sum_{k=0}^m \sum_{\ell=1}^{k+m} \Diamond = \sum_{\ell=1}^m \sum_{k=0}^m \Diamond + \sum_{\ell=m+1}^{2m} \sum_{k=\ell-m}^m \Diamond, $$ but I can't get quite it to work out. The first sum simplifies, but the second sum I can't do much with. Any ideas? 

Let $\mathscr P _0$ and $\mathscr P _1$ be two non-overlapping sets of probability distributions defined on $(\Omega,\mathcal{A})$. Consider the distance defined as $$D_u(P_0,P_1)=\int_\Omega \left(\frac{p_1}{p_0}\right)^u p_0 \mathrm{d}\mu<\infty.$$ Two distributions are chosen from each set $Q_0\in\mathscr P _0$ and $Q_1\in\mathscr P _1$ such that $$D_u(Q_0,Q_1)\geq D_u(P_0,P_1)\quad \forall (P_0,P_1)\in \mathscr P _0\times \mathscr P _1,\forall u\in[0,1]$$ 

I have the following convex optimization problem: $$\begin{array}{ll} \text{maximize}_{{f,g}} & \displaystyle\int_{\Omega} g^u{f}^{1-u}\mathrm{d}\mu\\ \text{subject to} & \displaystyle\int_{\Omega} f \mathrm{d}\mu= 1,\quad \displaystyle\int_{\Omega} g\mathrm{d}\mu =1 \\ & f_L \leq {f} \leq f_U\\ & g_L \leq g \leq g_U\end{array}$$ where $u\in(0,1) $ and $$\int_{\Omega}f_L \mathrm{d}\mu< 1,\quad\int_{\Omega}g_L \mathrm{d}\mu< 1$$ $$\int_{\Omega}f_U \mathrm{d}\mu> 1,\quad\int_{\Omega}g_U \mathrm{d}\mu> 1$$ Here, $f$ and $g$ are distinct density functions and $f_L,f_U,g_L,g_U$ are some known positive functions on $\Omega$. 

Here are some remarks: $1.$ It is known that the inequalities above hold for example if $$\mathscr P_0=\{P_0|\,|P_0(A)-F_0(A)|\leq\epsilon_0\,\,\forall A\in\mathscr A\}$$ $$\mathscr P_1=\{P_1|\,|P_1(A)-F_1(A)|\leq\epsilon_1\,\,\forall A\in\mathscr A\}$$ where $F_0$ and $F_1$ are some predefined reference measures. However, in this case all $P_0$ and $P_1$ are not necessarily absolutely continuous w.r.t. a common measure. $2.$ In the paper $\Omega$ is called as an infinite set. I wonder if there is something different when $\Omega$ is countable or uncountable. I am more interested in the case when it is uncountable, and for simplicity it can be chosen $\mathbb{R}$ or any interval of it. $3.$ If necessary one can consider the following sets: $$\mathscr P_0=\{P_0|D(P_0,F_0)\leq\epsilon_0\}\quad and\quad \mathscr P_1=\{P_1|D(P_1,F_1)\leq \epsilon_1\}$$ where $$D(P,F)=\int_\Omega p\log(p/f)\mathrm d\mu$$ and here every $P$ is absolutely continuous w.r.t. $F$, and $\epsilon_0$ and $\epsilon_1$ are some positive numbers such that $\mathscr P_0\cap \mathscr P_1=\emptyset$. 

If you are sometimes called upon directing a random walk in a directed graph, how should you direct it so as to maximize the probability it goes where you want? Formal statement More specifically, suppose you are given a directed graph $G$ with edge weights, two designated vertices $s$ and $t$, and a subset of the vertices $S$. The edges weights represent the transition probabilities of the random walk, the vertex $s$ the start, the vertex $t$ the target, and the set $S$ the set of switches. You are guaranteed that the weights on the out-edges of any node are non-negative and sum to one, that $t$ is absorbing (i.e., $t$ has one out-edge directed towards itself), and that the out-degree of any vertex in $S$ is exactly two. A random walk is taken on $G$, starting at $s$. For any given vertex not in $S$, the weight on an out-edge is the probability that the walk will travel in that direction. Every time that the walk reaches a switch (a vertex in $S$), you are allowed to choose which of the two edges the walk will travel along (and you are allowed probabilistic strategies). How should you direct the path if you want to maximize the probability that the walk ends up at your target $t$? Questions I am most interested in this as an algorithmic question. How fast can you find the optimal strategy with respect to the size of the graph? My specific application has about 100 switches among 200 vertices in a fairly sparse graph (say out-degree bounded above by 6). But we can also ask purely mathematical questions. For example, my intuition says (and I can hand-wave a proof) that there exists an optimal strategy that is deterministic in the sense that it always chooses the same direction for a given switch and this direction does not depend on the initial vertex $s$. Is this actually true? Also, is there a sense in which the optimal strategy needs to "coordinate" among the switches? That is, is there a local optimum that is not a global optimum? Notes A note on connectivity: we may assume that the graph is sufficiently connected. If not, we can identify all vertices that cannot be reached from the start node, as well as all of those that cannot reach the target node, into a single, absorbing fail state. We may assume the start node is not the fail node. 

Notes: $\bullet$ One can consider A or B since both conditions are equivalent. $\bullet$ $p_0$ and $p_1$ are densities of $P_0$ and $P_1$ and the same goes to $q_0$ and $q_1$ with $Q_0$ and $Q_1$. What I know: From Huber's paper (pages 260-261) Theorem 6.1 I know that if the distance is the $f$-divergence, i.e. $D_f$, then A and B are correct. Additionally, if A and B are correct, then $Q_0$ and $Q_1$ minimize $D_f$ (iff condition). Huber considers $$Q_{jt}=(1-t)Q_{0t}+t Q_{1t}\\q_{jt}=(1-t)q_{0t}+t q_{1t}$$ and finds the first and second derivatives of $D_f(Q_{0t},Q_{1t})$. He then shows that the second derivative is $\geq 0$ (convex) and hence $(Q_{00},Q_{10})$ minimizes $D_f$ if and only if the first derivative evaluated at $t=0$ is $\geq 0$ for all $(Q_{01},Q_{11})\in(\mathscr P _0\times\mathscr P _1)$. He shows that this is really the case, hence the claim is true. I think that this result can be strenghtened, i.e. if $(Q_0,Q_1)$ maximizes $D_u$ for all $u\in[0,1]$, then it should satisfy A or equivalently B. I dont know how to proceed. Addendum: It seems that the question eventually boils down to finding $(Q_0,Q_1)$ which maximizes $D_u$ for all $u\in[0,1]$ and fails to minimize $D_f$ for at least one $f$. This will be a counterexample to the claim (of course if there exists such a pair). 

For semiprimes, computing the Euler totient function is equivalent to factoring. Indeed, if n = pq for distinct primes p and q, then φ(n) = (p-1)(q-1) = pq - (p+q) + 1 = (n+1) - (p+q). Therefore, if you can compute φ(n), then you can compute p+q. However, it's then easy to solve for p and q because you know their sum and product (it's just a quadratic equation). If you believe factoring is hard for semi-primes, then so is computing the Euler totient function. Update! Factoring and computing the Euler totient function are known to be equivalent for arbitrary numbers, not just semiprimes. One reference is "Riemann's hypothesis and tests for primality" by Gary L. Miller. There, the equivalence is deterministic, but assumes a version of the Riemann hypothesis. See also section 10.4 of "A computational introduction to number theory and algebra" by Victor Shoup for a proof of probabilistic equivalence. 

I don't know if this deserves to be an answer or a comment, as it includes information you surely know. The terminology of matroid theory borrows heavily from graph theory, linear algebra, and other fields. A dependent set in a graphic matroid corresponds to a cycle in the underlying graph, so a general dependent set in a matroid is called a circuit. The length of the smallest cycle of a graph is its girth, so the same word is used for a general matroid. As you're well aware, the spark of a matrix is the girth of its corresponding vector matroid. You're correct that your notion appear to be dual to spark. The appropriate terminology here could have been "cutset", as that's the corresponding notion in a (connected) graph, but for consistency it's cogirth. The co-spark of a matrix is the cogirth of its corresponding vector matroid. A matroid is representable (ie, as a vector matroid) over a field if and only if its dual matroid is representable. Moreover, the transformation taking a representation of a vector matroid to a representation of its dual matroid is completely effective. It follows, in a sense, that studying the co-spark of a matrix is equivalent to studying the spark of another matrix. In particular, any computational hardness results for spark carry over to co-spark. It seems, then, that the notion of co-spark would be useful primary as terminology and as a way of getting a handle on things, but not actually for new ideas. Here's one paper that has explicitly looked into these sorts of things: On the (co)girth of a connected matroid. 

Here $\max_{\theta\in(0,0.5)}\max_{C_\theta\in\mathcal{C}_\theta}$ corresponds to all such convex curves. One can just put a single $\max$. 

My work on this problem is based on the Arzelà–Ascoli's theorem: $1.$ It is true that $|h_l^k(x)|\leq M=1$ for all $x$ and $l$, which implies uniform boundedness $2.$ But $\kappa$ can be chosen arbitrarily on $A$ as long as it is increasing. This means one can choose a sequence of $\kappa_n$ on $A$ such that $\kappa_n$, $n=1,2..$ is not an equicontinuous family. Therefore, $X$ is not compact. When I look back at Sion's theorem, It seems that I need the compactness argument such that the minimum exists. Namely, there must be an $h_l^k\in X$, which minimizes $f$. If the domain of $h_l^k$ would be $\mathbb{R}$, It would be easy to take a sequence of functions $h_{l_n}^k$, $n=1,2..$ each belonging to $X$ and the minimizing $h_l^k$ could be obtained for $\lim_{n\to\infty}h_{l_n}^k$, clearly implying that Sion's theorem would fail due to lack of compactness. But the domain is given as a closed interval; $I\subset \mathbb{R}$ and I have difficulties to understand why I must expect that for $I$, Sion's minimax theorem can fail? All functions are on $I$ and all are bounded by $1$ and at least one of them will be a minimizer and I can determine it. Thanks for reading and for any help. 

The following trick uses some relatively deep mathematics, namely cluster algebras. It will probably impress (some) mathematicians, but not very many laypeople. Draw a triangular grid and place 1s in some two rows, like the following except you may vary the distance between the 1s: 

Recall that a category C is small if the class of its morphisms is a set; otherwise, it is large. One of many examples of a large category is Set, for Russell's paradox reasons. A category C is locally small if the class of morphisms between any two of its objects is a set. Of course, a small category is necessarily locally small. The converse is not true, as Set is a counterexample. Now, I can construct categories that are not locally small. However, what's the most common or most reasonable such category? 

A random $k$-coloring of the vertices of a graph $G$ is more likely to be proper than a random $(k-1)$-coloring of the same graph. (A vertex coloring is proper if no two adjacent vertices are colored identically. In this case, random means uniform among all colorings, or equivalently, that each vertex is i.i.d. colored uniformly from the space of colors.) 

There are two sets defined: $$\mathcal{S}_0=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1(U_k=u_k)\leq \prod_{k=1}^n P_0(U_k=u_k)\right\}$$ $$\mathcal{S}_1=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1(U_k=u_k)> \prod_{k=1}^n P_0(U_k=u_k)\right\}$$ and the corresponding objective function: $$R(n,(p,q))=\frac{1}{2}\left(\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0} \prod_{k=1}^n P_1(U_k=u_k)+\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1} \prod_{k=1}^n P_0(U_k=u_k)\right)$$ $2.$ The random variables $U_1,U_2,\ldots,U_n$ are not necessarily identically distributed; 

I would like to attempt to solve the given problem below. I am sorry that I dont know whether such a question is solvable at all, even for just one specific example. If the solution set is empty, I am also helpless about how to show it. I asked this question two years ago at math.stackexchange with no answer or helpful comment. Given two distinct and continuous probability density functions on real numbers, $f_0$ and $f_1$ consider the following set of density functions: $$\mathscr{G}_0=\left\{g_0:\int_{\mathbb{R}}\log\left(\frac{g_0(y)}{f_0(y)}\right)g_0(y)\mathrm{d}y\leq \epsilon_0\right\} $$ and $$\mathscr{G}_1=\left\{g_1:\int_{\mathbb{R}}\log\left(\frac{g_1(y)}{f_1(y)}\right)g_1(y)\mathrm{d}y\leq \epsilon_1\right\} $$ for some sufficiently small $\epsilon_0$ and $\epsilon_1$ such that $\mathscr{G}_0$ and $\mathscr{G}_1$ are also distinct. In other words any density $g_0\in \mathscr{G}_0$ is not an element of $\mathscr{G}_1$ and vice verse. Note that $f_0$ and $f_1$ are known and assumed to be positive so that the terms $\log(g_i/f_i)$, $i\in\{0,1\}$, are well defined.