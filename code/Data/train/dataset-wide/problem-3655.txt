The zone for should contain the rest of the records. If you are doing this on the same DNS server, you can omit the glue and records. See How to Delegate Reverse Subnet Maps records for documentation and examples. RFC 2317, Classless IN-ADDR.ARPA delegation, has already been linked to. 

You may want to enable masquerading which will make all outgoing traffic look like it is coming from the Pi. A starting point would be these rules. 

If you have a static IP it should be the same as the other interface with the appropriate ip address and net mask. It should also be the only interface with a gateway line. See for detail on the file. 

Some of these actions are sensitive to slow DNS services, load on the system, or both. If you can find your configuration, the acl specified the directive will have most of the actions which can happen. The directive may be run after the data is received. The , , and directives will also run if specified. 

It is quite possible that the does not match the lines you have. The filter I have installed would not match those lines. There is a utility that can be used to test your regex. A multi-line string in the regex contains multiple regexes, one per line. A regex like the following may match better. I think the existing regex is not finding enough data. 

I run Shorewall as the firewall with a log drop policy. I use a dshield log parser to report port probes to dshield.org, and copy me with the log file. The logcheck tool also scans my logs once an hour and report any interesting data. I find port scans are not done that frequently now. It may be that use of tools like fail2ban makes port scans not that attractive any more. 

Your mail server is not configured to allow unauthenticated senders. This would be normal on port 587 (Submission) but is incorrect for an MX which must allow unauthenticated user to send mail (but not relay to other domains). 

As long a the server isn't down for too long more than (two or three days) almost all your mail will arrive. E-mail works on a store and forward basis and the upstream servers will retry periodically. I believe the standard states they should retry for four days or so. The good news is most spam is delivered by servers which try only once, so you won't have to filter a huge backlog of spam. Setup a good alerting system and a UPS and you should be good for most cases. My connection to my ISP went down during a storm while I was on vacation and I was able to resolve the situation in a couple of days. 

If he wants to handle 1000 concurrent users at 500ms, then I would expect a zero error rate. However, it is common that the specification allow for some slow requests. I am used to specification like "95% of the requests will be completed in with 500ms with 1000 concurrent users". Dropping connections is generally not acceptable. 

Avoid all of the above, and you will have a much better chance of getting your mail delivered. EDIT: I have found Port25 Solutions Inc. has a very good automated verification service listed on their E-mail Authentication page. Many thanks to them for their fine service. It is designed to verify DKIM signatures, but gives excellent feedback for most of the items listed above. Check in the Port25 resources section, and use the appropriate email address to get the results mailed to your desired e-mail address. Remember if you need to do DNS changes it can take a day or so to be reflected in all the caches. Worst case should be two times your Time to Live setting. 

The unused initialization code will be freed as soon as the memory is needed for other purposes. (It will be backed by files from which it is read.) The memory paging mechanisms on Linux are well designed and have been tested for years. It is rare you would want to swap any process out of memory. This would result in heavy paging activity any time the swapped process is scheduled for execution. If you truly need the memory from the other applications, you have too little memory. You can prevent the other programs from executing by sending them a signal with the command. Be careful which programs you stop or you could lock yourself out of the system. If you are experiencing large pauses during startup of your process, consider using to determine where the bottleneck is. You can also use to determine which process are being paged or swapped heavily. Don't be surprised if your process shows up as the problem. I've run servers which were severely starved for memory. To perform startups, it was essential to limit the number of processes starting at any one time. Process start almost instantaneously even if memory is far over-committed. If you really want to force everything possible out of memory you could write a program that allocates the desired amount of memory and continually writes to each page of the allocated memory for a few loops. It will experience all the issues you want to avoid. 

The backup DNS servers (one or more) will be slaves to your primary DNS server. Changes to the primary DNS server will be picked up by the slaves. This may be done on a periodic basis, or in response from a notification from the primary server. This is one cause of delays in changes to DNS being recognized across the Internet. Your primary and backup nameservers will be listed as the nameservers for your domain. Before DNS notify, slave nameservers would have a prior version of the DNS data for some period of time. (This is the one of purposes of the serial number.) Once all the nameservers have updated to the same version (same serial number) they should all have the same data. Editing a zone file without incrementing the serial number can cause inconsistent data. There is no switching to the backup DNS server(s). DNS requests are distributed across all your nameservers relatively evenly. (This is done by querying servers using a round robin schedule.) If one or more name servers are down, requests will be retried on another nameserver after a timeout. As long as one of your nameservers is up your domain will resolve (slowly at times). You want to have all your nameservers always up. In your case, you may find that it is simpler to use your ISP or domain registrar to host your domain. They will have one or more backup nameservers and will have resources dedicated to keeping them running. 

Use mtr to check the network. Try a command like . Leave this running in a window, or use screen so you can detach. It should catch any problems with the network. Packet loss is typically 0% on most of my systems. EDIT: What does the output of show for your IP address before and after ssh drops. You may want to try this on another server on the same subnet. There should be only one HW address for the IP address and it should not change. If it does you have an IP address conflict. 

Looks like it could be a memory leak. Try setting to something like 100 to see if that fixes it. If so start tracing your memory leak. It could also be a DOS attack. Do you have a spike in requests before the problems start. 

If you need to rewrite the content, then look at the module. If you have control of the backend content, it may be better to use relative links in the content. If a domain is required, then using the domain from the request eliminates the need for rewriting. Just pass the request back without modifying the Host header, and use that domain for links that include the domain. 

Try as a starting point. This will pick up connections which haven't hit the access logs, as well as connections which have recently be closed. Try to get counts by recent connections. Check the IP address I used. EDIT: Changing your accesslog configuration to record response time (%D or %T) instead of ident name (%l) will allow you to collect data on which URL patterns are slowest. Doing so when the whole site is slow may not provide useful statistics. Use top to ensure you are using a significant amount of swap space. If you have a memory leak then limiting MaxRequestsPerChild to 100 or 100 may help until you find and fix the problem. 

Check the DKIM signature in the header. The diagnostic messages seem to indicate that the domain field (d=) is empty or missing. Ensure you are supplying your domain when you sign the emails. The DKIM header should look something like the following: 

You can can use any values you want to discriminate between the DMZ and your host zone(s). You could use 0 for the DMZ as I did for site 2 above. Your ISP may provide a smaller block than a /48. The RFCs suggest that they may subdivide a /64 and allocate /56s. This would restrict the range you have available to allocate /64s. 

For an answer on the contents of the various folders check man hier. The /var/lock directory is intended to contain lock files. It is mainly used for devices and applications. File locks are usually placed in the directory being locked on next to the file being locked. /var/tmp is another /tmp directory. As /tmp is often on a tmpfs and is therefore cleared by a reboot, it useful if you need to create a temp file which will survive a reboot. This directory is rarely used. 

EDIT: I ran some queries on my database of email data. Of 2500 connections that used a local address in the MAIL FROM command, only 28 retried with a non-local address. This is about a 1% fail rate on just rejecting the use of local addresses in the MAIL FROM command. I found no cases where a server reconnected and tried a non-local address. Assuming the Envelope_from header is added before spamassassin filters the message both conditions can be combined in a single blacklist entry in local.cf. Again this assumes you do not run spamassassin on outgoing email. 

Starting with should log the connections. There are additional debug bits which will provide additional information. However it may be simpler to use and filter out the connections to the LDAP (389) or LDAPS (636) with grep. This should indicate where the connections are originating. If you have root access on the originating system, you can use to determine the program originating the requests. 

Dovecot supports several password encryption schemes. Some require plain text passwords to be exchanged. Dovecot 2.0 prefixes passwords with the scheme such as . You can specify a default scheme when you specify the file. I found I needed to specify the auth_username_format when I upgraded. Configurations like the following may help. 

This is a common setup. Large organizations often use subdomains for non-personal mail types. Servers sending you mail will look for the MX record for the fully qualified domain receiving the mail. Your domain and subdomain are different fully fully qualified domains. You can point your MX record(s) to any hosts that are configured to receive mail for your domain. Ensure your mail server is configured to accept mail for your subdomain. It is common for mail servers to be configured to send and receive as the organization's domain. You may need to reconfigure so that it is configured to send and receive mail for your subdomain. 

I would seriously consider using rsync to transfer the files. It will avoid copying the files it the haven't changed, and only pull changes if the file has been changed. This saved me lots of bandwidth and time pulling log files. rsync can be run as a server on the remote end, or run over ssh. I would suggest using key-based authentication which ever option you choose. 

By default will put the interface into promiscuous mode. For your purposes, I don't believe you need promiscuous mode for what you are doing. Specifying the inteface prevents from looking for it, and possibly getting the wrong interface. grabs the raw packets before they get firewalled by , so you will be able to see packets that get dropped or rejected by the firewall. 

I would expect there are not a lot of tools as the parse rules are likely to be application specific. However, I work with a number of tools that do accept requests from email. Most poll a mailbox and parse the messages they find. For simple processing tools like allow parsing, filtering and transformation of messages. sas the ability to pass the message off to a program for processing. I use this to handle DMARC reports and load them into a database. I've also used it to parse, log and remail requests for delivery to a pager. In my case I use , but postfix should be able to do the same.