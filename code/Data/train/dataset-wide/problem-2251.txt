The credentials for which are not stored in SQl Server database and managed by windows/AD. There would be entry for windows autheticated logins in master database with respective SID but password would be with Active directory. 

Rohit, If you would have searched a bit on web you would have found the answer that Buffer cache hit ratio should be as high as possible and 98.2 % value is quite good value please make a habit to search it will be helpful to you. Bufffer cache hit ratio definition could be eaisly obtained from perfmon. A simple definition is the percentage of sql server pages requested and retrieved from the buffer cache without reading from disk. If SQL Server would not have to read data from disk it will save I/O which is costly and perhaps on busy system it would reduce query time and increase its processing speed. Reading from memory is quite fast as compared to reading from disk. Below article will surely interest you $URL$ 

Personally I will try to post this dump to one of my MS friend if he is free he might have a look, but dont expect much. 

Please ask them and also, as max pointed out, test whether such tool can provide point in time recovery. Test various scenarios and see if it matches your RPO and RTO. I am sure its not free of cost only go if it REALLY provides some thing more than what native SQL Server Tsql backup provides. I have not found any tool more reliable than TSQL built in code for backup. Ask them,if possible, to do a POC (proof of concept) and show you how it would be better than native TSQL backup. You have to let them know that there is no point in choosing any thing blindly if it does same work as a tool which you have free( inbuilt). You can professionally ask them above questions. Plus if you use such tool you might need to hire a person who is expert in this tool so might be some more extra cost. Think all pros an cons and then move 

You just installed few updates for windows I don't think any update which actually made change to SQL Server version was installed. Current SQL Server version is which says its still SP2. Please don't install any SQL Server update via windows update feature it does not always functions correctly. If you want to install CU please download it and run the executable with admin privieges 

This is normal and happens quite a few times on a busy SQL Server OLTP system. I am not sure how to put alert from your side on internal flags set by windows. But you have ResourceNotificationHighandLow.exe which can tell you about the notifications. You have to download it. Complete details about it are on This Link 

Handy corruption demo script has been created by Paul Randal. These are really good and would help you a lot. Please browse below links 

The advantage with using domain account is that they are more secured and when you configure availability groups using SSMS GUI the endpoints are created by the SSMS GUI and also granted the connect permission(. As you can note from above that when using built in account you need to create endpoints using certificates manually and grant connect permission. You also have option of running SQL Server services with different domain account, if you do so you just need to make sure both the logins are created on remote machine in master database. More over before changing I would suggest you to read Troubleshooting Always ON Configuration( see the accounts section) 

it would be incorrect to say that its a memory pressure, its only that particular process is creating scenario which seems like memory pressure. 

If you really want to monitor SQL Server target memory I would suggest you to look at SQLServer:memory Manager--Target Server Memory: This is amount of memory SQL Server is trying to acquire. SQLServer:memory Manager--Total Server memory This is current memory SQL Server has acquired. There are other counters as well you can get from This Link. Few other interesting counters are Page reads/sec – Number of physical database page reads that are issued per second. This statistic displays the total number of physical page reads across all databases. Because physical I/O is expensive, you may be able to minimize the cost, either by using a larger data cache, intelligent indexes, and more efficient queries, or by changing the database design Free List Stalls/sec – Number of requests per second that had to wait for a free page PS: Please let me know if I have missed anything or if something is not clear Edit: Here is the Connect Item for your issue lets wait what MS folks have to say about it. Please vote it 

Stop there, this is not correct approach just because you did a full backup and log backup please don't be in idea that you can safely remove old log files and full backup. Do you have a plan to test old backup by restoring to actually see that in case of disaster this backup files would work. Remember only successful restore guarantees that your backup is totally consistent. Do you have a option to check backup integrity.If not please include it into your backup plan. At least keep 4 day old backup files( this is what I do on local disk) before if you delete backups This strategy is upto you keep backups for time where you are sure they wont be useless if disaster strikes. Sometimes business want only specific data change to which was made couple of days. I also have my database backup up on tape and that tape is stored for 6 months. 

You need to use all above methods speacially one suggested by Remus to see and get idea when was it last accessed. Suppose in a big firm you want to figure out that database A is being used or not and who is using it. First shoot a mail to all required stakeholders, managers, and application owners that you are going to make Database A offline as you feel this is not used anymore. Make sure you include everybody and then wait for a day so that people would revert to your mail. If nobody reverts and make database offline(dont drop it). Wait for another week or a month. You can do that as you already have made DB offline and it is not causing any load on server. Yes there might be monthly jobs running which needs that database once so I added to wait for a month. Even after a month if nobody reverts go ahead and do whatever you want to do with it. 

Second to what Aaron mentioned. You can use DBBCC shrinkfile command for database in logshipping but make sure you dont use TRUNCATEONLY option. If you use this option Logshipping would eventually break. Its better to use NOTRUNCATE option when shrinking Its quite common scenario for log file to grow because of some huge index rebuild or huge delete operation. In such case log file increased because it required space to log information. If you shrink it, again it would grow next weekend when the delete operation or index rebuild job runs. So whats the gain in shrinking ? Instant file initialization is not there for log files( it works little differently for Tempdb log file). When information on log file is being written space is zeroed out first and then information is written this could be a performance bottleneck because whole operation has to wait for information to be written in log file first. More about Instant File Initilization 

SQLServer:Buffer Manager--CheckpointPages/sec: SQLServer:Buffer Manager--Memory Grants Pending: SQLServer:Buffer Manager--Target Server Memory: SQLServer:Buffer Manager--Total Server memory SQLServer:Buffer Manager--Free Pages SQLServer:Buffer Manager--Free List Stall/sec SQLServer:Buffer Manager--Page Life expectancy