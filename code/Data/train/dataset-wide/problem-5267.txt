I think Daniel Dennett's conception of the process of "multiply drafting" competing interpretations is a theory of this sort, even though he is not a transcendentalist. (At some point, a materialist can now take genetics as a dodge around the a priori and get to parallel problems.) The basic description is that this is a uniform process that plays itself out in parallel ways on multiple levels, like a fractal with finite information. At the highest level, we as a society are comparing paradigms for basic sciences that will determine our technologies. At the lowest, various aspects of memory are writing their relevance into descriptive narratives that are then being edited for applicability and memorability to find a description for the scene directly before our eyes, which is the one that will be retained. The retained narratives become what you are modeling as intuition, as only retained material can incorporate parts of itself into the contending narratives currently in play. As I read it, your question is about the middle ground, where the conscious memory is asserting itself, and cultivating narratives. At this level, an individual, in my interpretation of this process, is focused consciously on coping how much of himself is embedded into thinking that may be transmitted consciously beyond himself. Too much, and it is merely an anecdote, a little less, and it becomes a theory with leverage, to much less, and it becomes stereotyped or vague maundering. As a psychoanalytic type, I think we also transmit the aspects of this process that are discarded to those around us via more mundane interactions, so that they have another chance for incorporation into a narrative. 

It depends what you mean by 'believe'. There are certainly Witches (Wiccans, if you must), who do believe in these things in a sense. And I would suggest it is the same sense in which many "bad" Christians believe in their God. And, yes, it even extends to Odin, the Legions of Asatru exists, revived by Edred Thorsson (q.v $URL$ Christian cults like Santeria, Condomble or Vodoun are really just the same thing, and it is just as weird, in just the same way. But you can still buy Santero herbs and Five African Powers candles in most suburbs of Chicago. These people believe there is some force that is part of human nature to which they can appeal to influence their own psychology, other people and some natural events. So their 'belief' is more of an aspect of psychology, on the order of Jungian Archetypes, than of genuine conviction of something independent of our reality that nonetheless affects it. (They would express it as the latter, but I think we all know better.) At the same time, I think most ordinary Christians alive now, who think for themselves, backed into a corner, would admit that their worship of Jesus of Nazareth is cultural and experiential, and they just think it maintains their culture, identity, and psychology in some way, and that it affects history through human action, and not by intervention. So weird is relative. 

I am reading "Ancient Philosophy" by Anthony Kenny (Vol 1 of his "A New History of Western Philosophy", OUP, ISBN 0–19–875273–3). I was intrigued by the following statement (p.176): 

For the sake of limiting the scope of discussion, my question is about the first fallacy. Here's how Kenny describes it: 

A substance can come into being as the result of matter combining with form -- this is "substantial change" or "generation of a substance" (e.g. the matter "bronze" combining with form to become the new substance "bronze statue"). A substance can also come into being as the result of another substance changing its form -- this is "accidental change" or "alteration of a substance" (e.g. the substance "unmusical man" changes to become the substance "musical man"). 

I think this is a rather difficult question to address directly/compactly, but here's my take: Firstly, "science" is a term that requires clarification. The existing practice of science involves both predictions of observable phenomena and also explanations (models, theories) of such phenomena. Now, every metaphysical theory essentially has to accept the "predictive" aspect of science. For example, if you have a metaphysical theory that asserts purely divine mechanisms of disease and cure, and you refuse to use the scientific fact that antibiotics cure bacterial infections, pretty soon you'll run out of adherents. So all viable metaphysical theories have to accept/adjust to the "strong" facts (i.e. predictable patterns) that science discovers about the manifest world. However, when we talk about the "explanatory" aspect of science, which pretty much always involves the use of certain entities or abstractions that are not directly observable, these explanations themselves are essentially (or at least heavily) metaphysics -- certainly in a positivist/empiricist sense. For example, science used to refer to something called "phlogiston" to explain the phenomena of burning; we know that it was a purely metaphysical entity, because it turned out not to exist! And nowadays we can talk about "gravity", which used to be considered a "force", but now is perhaps thought to be some characteristic of "curvature of spacetime", and in the future may get discarded in favor of another explanatory entity. Incidentally, there is nothing wrong with this -- science advances by testing various "metaphysical" models and keeping those that do not contradict observations. But it would be misleading, I think, to think of "science" and "metaphysics" as some completely unrelated intellectual pursuits, because at least the practice of science certainly involves metaphysics to a significant extent. To summarize so far: 1. When it comes to prediction of observed phenomena in the manifest world, any metaphysical theory has to yield to science (and indeed whatever can be predicted accurately immediately becomes part of science). 2. When it comes to explanations of either the manifest or some kind of other "ultimate" reality, one can't easily contrast metaphysical vs. "scientific" explanation, because any such explanation can be viewed as fundamentally metaphysical -- as soon as it involves something that's not directly observable. So, at best, we can perhaps ask whether a particular metaphysical theory A does (or should) accept another metaphysical theory B that is prevalent in the scientific community. And I can't think of any generic answer here. It may, but it doesn't have to, and often doesn't. Which is not particularly surprising, for even within the scientific community, at various times different groups of scientists also prefer different (explanatory) theories. IMO, it comes down to the criteria by which to judge the goodness of an explanatory (metaphysical) theory. Scientific theories tend to be more economical (in the use of unobservabe entities), designed to be falsifiable in some way, reuse existing terms from accepted theories, etc. "General" metaphysical theories tend to focus on a certain kind of simplicity, or aesthetic beauty, or ethical considerations, or emotional needs, etc. Whatever one thinks are the most important criteria (of explanation) will guide one's choice of theory. And then one will tend to view other theories, including scientific ones, also in terms of how well they satisfy one's preferred criteria. For example, if your preferred criteria is to minimize the number of unobservable entities in a theory, you might choose/build a metaphysical theory that mimics existing scientific theories, re-uses some of the same terms (e.g. "quantum entanglement"), etc. Or, you might minimize the unobservables by choosing a theory where "everything" is explainable in terms of a single unobservable Being. In both cases, you'd likely prefer to fly on a plane built using "pure" scientific facts (i.e. wings generate lift, etc); but your explanations might range from a (scientific) "this is simply how the universe works" to a (theistic) "this is how Being designed this universe to work". 

Without some kind of intervention, the state has no effects whatsoever. But Mill is ruling out 'maternalistic' intervention: intervention to improve someone's life by adding rules to it for the presumed common good, and allowing only 'paternalistic' intervention: intervention to protect one person from another's harm. Earlier 'maternalistic' intervention can easily prevent later 'paternalistic' intervention. But if we back off from allowing the former, we become rigidly and paranoiacally ready to apply the latter, or we do not feel safe at all. If we cannot pull you over to measure your BAC unless you are about to hit someone, but we can sentence you to death for killing someone if you drive drunk, we will be doing more of the latter. And if this kind of rigidity is the norm in the population's mind, we will have more and more draconian punishments for being dangerous. We will be more and more willing to overreact to slight dangers that predict greater ones, creating a profusion of paternalistic rules, and less and less willing to identify behaviors that are not dangerous but can easily become so and shape behavior more subtly. (Unlike the previous poster, I see the drug ban as nanny-statish, and not paternalistic, during more libertarian periods of U.S. history, it was assumed that any drug ban was automatically unconstitutional.) 

Your title allows for better answers than your example. And I will answer from the title question, rather than limiting myself to the plane. You know where the centerpoint of a parabola is, even though it extends infinitely far away from there in both directions -- it is the point closest to the focus, the point of maximal curvature. It is a center in a specially defined sense which recognises a sense of the symmetry a center should confer. Similarly a cubic has the opposite kind of center, a uniquely flat point. However you rotate the curves, these central references remain obvious, and they are historically relevant enough that the ordinary sense of center is often extended to include them. A hyperbolic paraboloid has a saddle point where its tangent space splits it into ascending and descending parts, and that makes it a centerpoint of the space in important ways, again related to symmetry rather than distance. So in the sense of the point that everything is intuitively situated in terms of, like a city center, certain infinite spaces have centers. If our own universe is "too light", it may have a hyperhyperboloid shape, with the third-dimensional equivalent of the saddle point, and in this sense a relative center. 

I think it's a big assumption, but not without some merit. Firstly, we don't know if there are truly random events at all. To say that an event is random means that we don't have sufficient information (and method of calculation) to determine the exact outcome. But we cannot be absolutely certain that such information doesn't exist (e.g. some yet-to-be-discovered hidden variables) or may not eventually become available to us. So, one can at least reasonably entertain the possibility that there are no truly random events at all -- and in this case, there wouldn't be any random processes in the brain (and decision-making, etc), either. Secondly, to quote George Musser (in reference to Butterfield, Dennett and List), "human cognition involves different structures than atomic physics and is governed by different laws, so determinism at micro level need not imply determinism at the agential level." As an analogy, if you consider a process like gas expansion, the behavior of individual particles may be random, but certain "important" aspects of the "overall" behavior of the system are quite deterministic. So, in this case as well, it may be possible to view the brain as a deterministic mechanism (perhaps to some extremely high degree of accuracy) on the level of decision-making, even if you allow random events on a small scale. Now, as to your last question, if we were to grant that the decision-making mechanism is "truly" random, then it's very difficult to reconcile it with (at-least) a common-sense meaning of free-will. In this (common) sense, free-will implies "control" over the choice being made. However, if the decision-making mechanism is random, then to talk of "control" is (nearly) as meaningless as in the case when your decisions are determined by material/physical factors. So, at the very least you'd have to redefine the meaning/scope of "free will". Of course, people are certainly trying -- and it's worth reading about (here's one overview: $URL$ 

It's not clear whether Aristotle's own account is entirely coherent, but that seems to be the basic idea. 

My question is, what exactly is Kenny saying about Plato's and Aristotle's epistemological views? I mean, it seems that he's talking about contigent vs. necessary truth, and that Plato & Aristotle thought of all truth as necessary -- but I don't understand what that really means in terms of their theories. Is it being claimed that if Aristotle saw a blue unicorn, he'd assert that all unicorns are necessarily blue? I.e. that he would not be able to conceive of (the truth of) the color as being contingent on the particular unicorn? Or is it being claimed that he'd consider the blue-ness of a specific unicorn to be something less than truth, precisely because this blue-ness is not necessary of other unicorns? Or something else? Kenny closes by making a strong claim about the "impossible ideal of Aristotelean science". I think here he ascribes to Aristotle the theory that science should be a deductive (via syllogisms) process, producing only "necessary" truths. But is calling it an "impossible ideal" really justified, especially in the sense of Aristotle committing some kind of clear fallacy..? After all, didn't the logical positivists take up exactly this line of thinking about truth, and haven't they come up with some interesting/useful/influential arguments about what science is, even if other approaches have become more popular..? 

The thing simply is not alone in space, and the model is not obligated to answer the thought experiment. If it were, it would eventually no longer be so, so it is the space that really has state, and not the particle. Until there is something else, the particle would have no state, since a particle's state is a function predicting responses to potential interactions, and here couldn't be any. 

(It is not quite freedom, but autonomy in which Kantians place value. The 'freedom' to be insane and run around making trouble is not autonomy, because you are actually enslaved by a disease. You might be more autonomous if you are captured and pressured to enter treatment. We might have been freer, but less autonomous, if none of us went to school. We have been given tools that make us more powerful, by being made to put up with a limitation on our freedom. I will stick with your vocabulary, but bear in mind that it is not quite literally what Kantians mean.) There are two questions here, and I will give separate answers: 

This notion is Popper's, and it is a good one, but is not universally held. During periods of normal science, or when total falsification can be justified without depending deeply on a theory, falsification gives the most statistically stable means of moving forward. The entire mechanism of modern statistics is built around the 'null hypothesis', and stating up front exactly what it would mean for a theory to fail to account for experimental results. This boils 'partial failure' down into measurable numbers and allows us to accumulated them and trust what they mean in terms of our real likelihood of being wrong. So when you can use this criterion and apply these methods, you can converge on the most reliable answer in a way that can be validated by one's peers, and can give them some notion of how much risk is involved in accepting the theory if it is marginal. Of course, where there are people involved, the math is not completely trustworthy, because practice involves interpretation, etc., and scientists still have judgment calls to make. But when Popper applies, it is gives decisions a lot of accountability. 

(I understand that this is not an answer to the question per se, since it is not particularly about Lewis. But it is to the point, because there is a good reason to reject the material conditional interpretation altogether. It is a toy model for teaching logic, and not a realistic model of actual language use.) It seems mathematically, by this account, they are true when some statistic applied to all the cases falls in the right range of values, where that statistic and range depend upon the actual word. Something is mostly the case if more than half of the cases make it true, no? Something never happens when the count of events is zero, no? And it always happens when it never fails to happen. If it never happens, then zero divided by two is zero and zero is not greater than zero. Something that never happens is both always and never whatever you want. In fact, these are the easy cases. The fact of the matter is that it is not this degenerate case that causes most of the problem with this notion. In a lot of situations where the referents are abstract or potential, there is no metric that works. So computing this sort of statistic is pointless. Most of a merely potential set that clearly allows for infinitely many elements simply cannot be computed on a case-by-case basis. We feel like the integers are mostly not prime. But I can put the primes in one-to-one correspondence with the remainder, so clearly just half of them are prime... At the same time, I can clearly produce two non-primes for every prime. And if imaginary kittens are mostly not purple, I can suddenly imagine a whole lot of purple ones and skew the odds. Even when the cases seem to be finite and bound to reality, there is generally not a metric. I can "usually wear clothes when I go out", but then I can live homeless, in the woods, naked for fifty years, but only once. Are the cases measured by instants of time, or potential instances of observation, or the actual number of times I exit the door? You need a metric space in order to do this kind of computation, and there is no reasonable metric on 'cases' in general. So this is not a workable theory for any realistic grammar. You can fall back on context, and claim that the context explicitly or implicity imposes a metric on cases by the framing. But even in the most circumscribed reality there is not enough context to imply one. This account of language simply tries to hard to move away from dynamics and falls back too hard on mathematics, probability and set theory, without really taking them seriously. From a framing like that of Wittgenstein's description of language as network of games, or Lacan's account of the dynamics of domains of reference, or even Lawvere's perspective in Category Theory, reference is an active event, not a pre-computed static object. In that context, such parts of language are about controlling assumptions within the context of a narrative, and not about describing some frozen image of reality. In context, "I mostly do something" is meant to convey that you are to assume when I talk about doing it, that the forward trajectory of the story will work best if you assume it took place. If I don't then involve doing it, it is pointless, and it simply does not have an effect on the presumed narrative, and therefore does not have a meaning, beyond embedding that pending trigger in the context for use later.