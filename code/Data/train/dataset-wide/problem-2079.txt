Something must be trying to use xp_cmdshell to execute a batch file outside of sql server. It looks like xp_cmdshell has been switched on but you need to run for it to be active. Then the messages will disappear as what ever is trying to use it will be able to. 

Yes, use the same cert on each instance. Your restore process would be clumsy if you had different certs on different instances. I would suggest you read the documentation (including the relevant links) as encryption and key hierarchy in SQL Server is a topic too large for a single answer on a Q&A site, but essentially you need to: 

I am looking to deploy log shipping in my production environment. This will mean I will have to manage backup chains which could be disrupted by people taking backups to refresh development environments. If I have a failure and want to use the log backups to restore to a point in time I will also need the backup taken by the developer. This wouldn't happen if the developers only used COPY ONLY backups. So my question is: Is there a way to restrict users to only be able to perform COPY ONLY backups? 

I have assumed the ID field that would be used to join the two tables. Let me know what field you would use to join these tables and I will update the answer. You should also specify the correct alias in your @columns variable. I also assumed the UserRegistration table schema to be dbo. 

I would only recommend this approach if you have not, can not or do not want to install SSIS. If you are planning on regularly importing the contents of this file you will be better off using SSIS as you have things like error handling and data transformations that may be easier for you to configure in SSIS instead of writting them from scratch in T-SQL. The OLEDB solution may be the quick solution and require no extra applications other than the Jet OLEDB driver, but SSIS will be a better long term solution. I would recommend you research SSIS and try to make a package to insert the data in a test environment. 

If you can group your data in a way that makes row 1-8 group 1 and the other rows group 2 then you can do a page break at the end of each group. 

I'm not sure how you want to handle the error so I just used to display the error for this example. You could log the error to the event log, a file, a table in SQL, etc. 

The default roles all have db_ and then something descriptive of the roles permissions. I would stick with that. 

The difficult part with SQL Agent jobs on Always On Availability Groups (AG) instances is that you usually want the job to run only if the node it is on is the PRIMARY. You want the jobs to exist on each node so that they are there in the event of a fail over. I've found making the Agent Jobs "AG aware" is the easiest approach. I talk about a similar situation here. The general idea is to add a piece of code to each Agent job that checks to see if it's node is currently the PRIMARY. If it is then run the job, if not stop executing. The code below (modified from my blog post) should help: 

Your SSRS deployment just needs access to two databases ReportServer and ReportServerTempDB. It doesn't matter where they are hosted as long as they are accessible. When you move these databases to the new server you will need to open Reporting Services Configuration Manager and redirect SSRS to the new SQL 2012 server. You can change the connection on the database tab. If the connection test goes through OK then you should be ready to go. One thing to make sure of is that all the logins from the old server are present on the new server before transferring the databases. If they are not the permissions for any missing users will be dropped. 

This schema would conform to 3rd normal form. must be unique across all accounts. If it's not then a surrogate column that is unique in the table or a composite key of and could be used. Currently your table violates 2nd normal form. It violates 2nd normal form because , as I have described above, is not required to be part of the key. If it is not part of the key that makes it a non-key attribute that does not rely on the key of . It will be repeated for every invoice created for an organisation. 

Powershell is your friend here. When working with cmd commands in Powershell you can use the variable to read the result of the command you executed. The code below passes a command into the Invoke-Expression cmdlet and captures it's output. 

To exclude certain tables from your refresh of the integration environment, with a backup from production, you will need to make use of a staging\data cleansing environment. The plan would be: 

All you need is two tables. 1 for Products and 1 for Attributes. You would then have a one to many relationship from the Products table to the Attributes table. This means that each product can have as many attributes as it needs. Table structure would look something like this. 

UPDATE I made a mistake in my code above. It is not possible to alter the encryption algorithm and the certificate used in one statement. This was not required anyway so the following code is sufficient: 

I know that Red Gate SQL compare has a command line interface so you could write and schedule a batch file to generate the scripts. Details here You could then use SQLcmd to execute the scripts. 

While you could schedule an JET OLEDB type query as a SQL Agent job with something like the code below: 

We are doing something very similar at my place and this very question was in my head for a while a few weeks back. After noticing that logs were consuming 98% of our DBs in SQL Server I did some research on how we should handle this problem. It all depends on the structure of your logs but our logs are not really relational so storing them in a RDBMS and consuming all the resources built for the RDBMS does not make sense. I started looking into NoSQL stores and found DB Engines Ranking to be very useful to compare the feature sets of each DB engine. Personally I found that an eventual consistency solution worked for me as it has good sharding, availability, and fast reads on huge data sets. Check the CAP Theorem wiki entry to learn about the trade offs on different types of NoSQL solution. We are currently testing a Cassandra deployment for our logs with MapReduce jobs. 

Swap the foreign key so that the table has . This way multiple drivers can be associated with the same truck. 

You mention every airport has a manager so the manager table should have AirportID as an FK. The Office table should have AirportId and AirlineID as FKs 

The script below will give you most of the information from the standard reports in SSMS when executed on your secondary server. 

Without the schema this is a little difficult to test but I think this will work. Even if I do make some syntax errors you will be able to see what I am getting at. 

This is a Gaps and Islands question. See here for more details on problems like this. This should do what you need: 

Possibly, but in my experience you don't partition data for performance reasons. Yes, partition elimination can reduce the IO that some queries consume but there are complications that partitioning will introduce, see here for my favourite. What partitioning does help with is maintenance. You can rebuild indexes, and stats (on SQL Server 2014+) at the partition level. This can be a huge win when you struggle to keep maintenance operations inside your maintenance window. Partitioning is also great when it comes to loading and archiving data. The statement is your friend here. Lastly partitioning can also be used to optimise columnstore indexes by introducing a level of ordering to the data. This helps with dictionary pressure. Each partition acts as it's own columnstore index which means you get more global dictionaries as well as delta stores and delta bitmaps. Niko's blog has much much more info here 

I think this does what you need but you may want to replace the by making the statement into a statement. 

I think you should have the Survey table as it will be much easier to add it now than later on when your app is live. The second model is much better as it provides the flexibility to have multiple possible answers and user answers for a single question in a normalised fashion. Having a column for each answer is not a good idea. 

Yes this is normal. SQL Server will only automatically reduce the size of data files if you have switched on and there is space available in the files. This setting is not recommended as it will introduce considerable fragmentation and consume considerable IO resources. You could do a one time shrink with . See here for more info. This will introduce fragmentation but it can be fixed by rebuilding your indexes. IMHO the main considerations would be: 

I couldn't include in my test query below but hopefully I have provided enough code for you to understand the syntax you require. 

I would suggest moving all clients to the domain and removing the SQL logins as the AD route is much more secure. If this is not an option then I would probably remove the AD logins and have RDP users use SQL logins as having two logins per user doesn't make much sense. If for some reason you decide you have to go the two login route I would write a stored procedure to replicate the settings from AD users to your SQL logins. This could be triggered with a . This way you would only have to update one set of users. I would also be careful in using AD groups if you are not the only person responsible for the administration of the domain. It's not clear from the database who has the permissions specified for the group as users be dropped in and out of the AD group without your knowledge. 

Yes. The first query has left an open transaction. SQL Server doesn't check if the current user has permissions at compile time. It will find out at run time. If you were writing a stored procedure you wouldn't want to check permissions at compile time as a different user may execute the procedure. Wrapping the UPDATEs, in both queries, in a TRY CATCH would avoid leaving an open transaction. 

UPDATE Andriy noted that my solution was a SQL Servre 2012+ solution. The following code should work for versions down to 2005. 

If your RTO is ~ 1 minute and your RPO is 1 day of data loss then you could use log shipping. This would not require AD, a Windows Cluster or Availability Groups. If you shipped your logs every 15 minutes the recovery time needed to bring the secondary should be well under a minute (Needs testing but I would be surprised if it wasn't from what you have explained above). The only thing you would need to deal with would be how the application manages the connection to the secondary when the primary is down. This could be handled in the application, a load balancer (with static routes that would need to be altered on failure of the primary) or a clustered name resource (would probably need to be manually failed over). A FCI would mean you wouldn't need to handle and monitor all the log backup/copy/restores of log shipping and your RTO would be lower. This is not possible without AD. Also you can't have Availability Groups without a Windows cluster which also requires AD. 

A restore failing on the secondary will have no impact on the primary. You want to monitor your secondary DBs to make sure this hasn't happened. 

Yes the performance will eventually be effected but you can counter this with the addition of suitable indexes. Indexes will be easily be sufficient for managing the performance of tables that are ~50GB. If you get a lot more data in the future columnstore indexes and partitioning may be worth a look but they do require the Enterprise Edition of SQL Server. Columnstore indexes would require and upgrade to SQL Server 2012 or later. 

Be careful with the term Always ON, This refers to Always On Failover Cluster Instances and Availability Groups (Also database mirroring but this is deprecated). FCIs and AGs are very different technologies. To answer your question AGs can be installed on stand alone and clustered instances. It all depends on your HA/DR requirements. 2nd question: Yes AGs can automatically failover their databases to other SQL instances (refered to as nodes). The servers running the SQL instances all need to be in the same windows failover cluster. This white paper from Microsoft might be a good place for you to start reading on AGs. 

The following SQL will show you which file groups your tables and indexes are in which will make it easy to see if there is data in any file group that shouldn't have data. 

waits do not necessarily indicate problems with the IO subsytem. These waits are logged when SQL Server is waiting for data page to be read from disk into memory. More information from Paul Randal can be found here as mentioned in the comments. So if there was a problem with the IO subsystem it could cause high levels of waits but they can also be caused by the buffer pool being full so SQL server has to wait for space to be made before the pages are read into memory. Any applications running on the same server as SQL Server could cause these waits by trying to use the same memory that SQL Server wants. Non optimised queries that are not supported with suitable indexes can also cause many waits as the query may perform index scans instead of seeks which will cause more IO and mean that more pages will need to be read from the IO subsystem into memory. Index scans could also be caused by out of date statistics which cause SQL Server to produce non optimal query plans. Obtaining a baseline for your wait stats is a great way to enable you (or your DBA team) to spot wait stats during moments of poor performance that may indicate the cause of the performance drop. Again Paul Randal talks about this here and also gives some explanations of common wait stats. I have these explanations saved in an offline cheat sheet that I always have open for reference. 

It's not quite as simple as re-enabling log shipping as you need to do some initial setup to be able to switch from one instance to another. This article helps with that You also need to perform some clean-up before switching back to the original primary as it will be holding databases that are now out-of-date. If your hardware on the primary and secondary are the same and there is no performance loss or maintenance headache in staying on the secondary server then I would recommend doing that until the next failure as the step of failing back is not needed.