I want to query spatial data for nearest neighbor. I am using this article and the following query works perfectly: 

The problem is that the above statement does not use the index. If I replace the with a number the index is used, but when a variable is used, the clustered index is used instead the spatial one. I have try a lot of options and the only one that works is: 

- constantly (in 99% of the cases only data for the past 6 months) - rearly (in 99% of the cases only data for the past month) 

I have the following issue - is updating particular table columns when such update is not allowed. I was not able to find the (searching in T-SQL and ASP code, server jobs), so I decided to create trigger on the given table and try to log everything that is possible and will help me to find what is updating the table. I am wondering what can I access in the trigger - for example: T-SQL statement performing the update? Also transaction log back up are made on each 15 minutes, so I guess it is possible to detect the change from the backups having particular information like time and transaction id? 

Need to know how PostgreSQL orders records on disk. In this case, I would like to take advantage of index combination as stated in the docs, which as I understand uses bitmaps to get matching rows and returns them according to their physical location. The table in question has been clustered by its primary key. As I understand it, PostgreSQL does not automatically continue doing clustering after a clustering is finished (although it does remember that it clustered according to a certain index). Now, since this is the primary key, I wonder if the physical storage order would be according to that (which if true I would like to use to our advantage for a specific query). In summary, how does PostgreSQL order its new records, especially after clustering? Thanks very much! 

Some queries on my database server seem to take a long time to respond, and I believe the CPU usage is high. When running , I see ~250 "idle" connections (which I believe to be too many). I haven't started doing a full diagnosis, but I wanted to know if this is a good place to start looking. I am also using PgBouncer with transaction-level pooling. I am suspecting that I can easily reduce the number of connections by tuning the pool size. However, I don't want to start doing too many changes unless there is a good reason to do it. Can many connections in PostgreSQL 9.2 affect performance? Thanks very much! 

I have a primary database in recovery mode which is part of group. Is there a way to minimally log insert operation under recovery model? I have a process that is executed each day and insert few millions of records in a table. While the operations continued the transaction log file size is increased dramatically ( from 1 GB to 40 GB). As I have read I can used some variations of which are not fully logging the operation but I am concern about the effect of switching the recovery model? 

The history table columns will be and I am going to save a lot of space versus ordinary implementation which is logging all data (this is due to my test and my bussness cases). As I the supports I am wondering are their any pros/cons/differences between using it and trigger-based logging? I have check few artciles (here and here) and cannot see what more can give me. 

with space used - maximum 5-10% of it (a backup of the transaction log file is made every 15 minutes). So, the above settings are specific for each database but in common, the log file size is enough and not growing. After is made, the size of the transaction log file is increased with 1 or 2 or 3 GB. Since I do not need such big file for regular use of the database, I am shrinking them back like this: 

I saw this example in a Quora question about sharding MySQL tables, where it is specifically mentioned as a good idea for sharding if and are to be queried together. How is the modulus calculated so that rows with the same are on the same shard? 

I would recommend the 3 column solution with area code optional. It will be able to handle cases of countries where mobile numbers do have area code (like the U. S.) and will be much easier to maintain and service. 

Trying to find a way to easily transfer all the rows from a Cassandra ColumnFamily/Table to another. The command, as I understand, is a good option. However, as it dumps all the data to on disk and then loads it back, I can't help but wonder if there is a better way to do it in-engine. A specific example of what I mean would be the available in many databases. Of course, I realize that Cassandra is NoSQL and therefore does not to work the same way - but it seems like something which might be available. What is a good way to accomplish this? Thanks very much! 

Nothing with the server setup has changed since the migration. We are running two servers, a PRIMARY and REPLICA within an AlwaysOn Availability Group. The VMs are housed on Azure and have 8 logical processors, 28GB of memory, and all SSD drives. The data file layout looks like: 

The execution plan (PasteThePlan) is vastly different with this small change. It is tossing a warning that there is within the predicate. It was my assumption that would implicitly satisfy this argument as it does with . However, given that the operator returns each row from the initial input when there is a matching row in the second input, and because the warning exists, query optimizer assumes that each row is a matching row. Therefore all subsequent batch operations are ran against all 2048 rows in the table. What can I look into to better interpret what the execution plan is describing so that I can understand how to properly resolve the issue? Or, alternatively, am I simply missing the purpose of the operator when filtering result sets based on a static input? 

I am attempting to update a query that utilizes the operator within a clause predicate with to compare potential performance improvements and better understand what is happening behind the scenes when the two are interchanged. It is my understanding that in practice, the query optimizer treats and the same way whenever it can. I'm noticing that when the query is ran with the operator, it returns the desired result set. However, when I replace it with the equivalent, it pulls in all values from the primary table I want to filter. It is ignoring the provided input values passed to and returning all possible distinct values. The use case is relatively straightforward: the query accepts a pipe delimited string that can contain up to 4 values, e.g. or . From here I the input into a table variable to determine the corresponding internal for the . The result set returned is then filtered to exclude the that were not passed. To illustrate, here is a similar table definition: 

Maybe, the checks that the engine is making is doing something like this, comparing its current date with my newer date and this is causing the error - I have change the script like this and executed 1 000 times last night - no errors were generated. So, I believe I have fixed this particular issue, but I can't be sure. 

I am going to use a cloud storage where the fast storage is too expensive to hold all the data. So, I want to store only the the new data in on fast storage, and the older data on slow/cheap storage. Is there a way to achieve this? Also, since the major part of the data is not () I want to perform only partial backups on the data stored into the fast storage only.Let's say a full backup each week and a partial backup each 12 hours. I am looking both for storage and recovery strategies (any links are welcome, too). 

When a symmetric key is created we have the option to encrypted it by using at least one of the following: certificate, password, symmetric key, asymmetric key or EKM. When a password is used, it it said that: 

I have a question about whether or not it is necessary to back the tail log up when performing a restore of a database in Full Recovery mode. I've sprawled for a good while now, and have gotten conflicting reports. Consider this scenario in SQL Server 2012: 

The data it stores is enumerated based on a JavaScript method that uses information from their browser (I don't know much more than that, but could find out if needed): 

I'll end on a capture from RedGate SQL Monitor over the past 24 hours. The primary point of note is the CPU utilization and number of waits - during our peak hours yesterday, we were experiencing heavy CPU use and wait contentions. After this simple fix, we have improved our performance tenfold. Even our disk I/O has reduced significantly. This is a seemingly easily overlooked setting that can improve virtual performance by an order of magnitude. At least, it was overlooked by our engineers and a complete d'oh moment. 

This returns: My clause clearly brings back only the rows that are explicitly , so it is not an issue of being an empty string and evaluating to . 

I want to see the size of a spacial index. I am usually using one of the following code blocks to get the indexes sizes of a given table, but none of them is working for a spatial index. 

I am renaming some unique constraints to match our database objects naming convention. Strangely, there are several multi-line table valued function which returned table has unique constraints as follows: 

I think I have found how to fix my issue, but I am not going to accept this as answer as I am not able to explain what is causing the problem and guarantee this will work anytime. It's fix found after a lot of testing and I will be glad if someone can bring more light here. 

If large amount of data is deleted from the first database and inserted into the second one, only the delete operation is logged in the transaction log file of the first database, right? If I have two linked servers with many databases like this: 

Let's say I have a T-SQL table with 20-30 columns and 2-3 millions rows (huge table). I want to allow a user to update this data directly (the solution must work for different tables). I generally have the following options: 

In MySQL Cluster, and generally in sharded databases, how is a compound primary key's modulus calculated? Say, for example, there are two tables that have primary keys like so: 

Run the query. Use tool on the query. Analyze the results of using a certain tool. Check indexes.... 

Trying to figure out how I should expect my database to perform. Basically, I need to determine when the server I have set up is reaching its performance limits - which would help me plan server scaling better. This question aims more towards ways that I can calculate or estimate (really, any idea of this would be good) expected performance. This should ideally help me come up with a formula I can run based on several factors (like record size, number of rows, etc), instead of a subjective assumption based on a particular server / DB. So, what is a good way to determine expected performance on a PostgreSQL server? Thanks very much! 

Using SQLAlchemy to query a PostgreSQL database behind PgBouncer, using transaction-level pooling. What is the best pattern to use for this kind of set up? Should I have one-engine-per-process, using a , or should I create an engine per-request, and use for each one of them? Is there a different pattern altogether that I should be using? Thanks very much! Let me know if more information is needed and I'll update ASAP. 

However, even after dropping the trigger, restarting MSSQLSERVER, and even the entire server, events are still being captured. I dug even deeper and I browsed to within the registry, and the was not explicitly specified. So, I manually added it with a value of for . Restarted MSSQLSERVER again to no avail. Set the value to for and restarted... still capturing all login events. What else could contribute to this behavior? 

We have been experiencing an odd behavior in our application where various modules will begin to timeout in SQL Server 2012. Each time we stumble across this issue, we find that the statistics require update and that running fixes the issue. After updating index statistics, the timeouts go away. However, the frightening issue is the frequency in which we have been experiencing the need to update the statistics, and the fact that nearly all of the statistics are showing a need to be updated across all of our tables related to order processing. Due to the frequency, we have setup a job to run prior to business hours at 7:30 AM. This seemed to have calmed the issue while we continued to investigate until it occurred again today. Not 10 minutes after business opening, they were receiving timeouts. I immediately ran and the timeouts disappeared and application function returned to normal. The order volume since business opening was low (less than 20 rows added to the primary order table), yet the statistics became so bad between 7:30 AM and 8:10 AM that we began experiencing time outs. A couple of notes: