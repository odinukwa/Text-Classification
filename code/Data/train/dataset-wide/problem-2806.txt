In both cases, the sane outcome would be to default to your "main" language, which would probably be hardcoded in your application. Another issue you may want to consider is that nobody knows that there is an Accept-Language header and how to use it, so using that as a source of the user's preferred locale is not recommended: 

You do not have to go to all this trouble to implement a caching wrapper. It can be done much more simply: 

It's much better to make the default controller name part of the application configuration and have the router take it from there, because it's reasonable to want your "home" page to be equivalent to the "/something" url instead of "/index". The same goes for 

On principle the code is sound except for one issue: you do not check if the language is "valid". This raises some questions: 

Here it is much more obvious what you method you should be looking for. It's true that you, as creator of the code, do not need this -- but others might, and it's a totally free lunch. Second: this line bakes an application configuration decision inside the router. You don't want to do this, as it's totally reasonable to want to change this configuration most of the time while the router would stay mostly untouched. 

If you really want to do it in excel, make sure to use at least the tools available to the fullest. Let's go through the design choices one by one now, to see where things went south: 

If this is actually supposed to be C#, the whole script you posted should actually have wrapped in a regular C# class: 

And the machine did not stop. Why? Because you are only handling multiples of 10 seconds, and everything in between is never matched by your string comparisons. 

What is this line supposed to do? That should have been directly computed into and , so this line is obsolete. 

That function shouldn't need to know how to traverse a tree. That functionality belongs to , not to the invoking site. A simple is much cleaner in this context. The complexity of building and traversing the tree should be hidden entirely hidden inside the class: 

Your approach of separating the actual algorithm from the input/output handling was almost correct. Except that you forgot to implement the latter part. You are neither capturing any input, nor are you creating any output. Except for the log spam resulting from placing your strange test cases into main file. Providing a user interface of any sort is usually considered part of these challenges, even if it's just a console application which reads from and outputs to . Or in the case of JavaScript best a small HTML application. 

In general it looks good to me. I'd clarify the comments a bit more. Q=1 is typically used for the extra strong Lucas test, P=1,Q=-1 is used by about half the cases for the standard or strong Lucas test (the rest have a different Q). Some info on the various tests can be seen on my Pseudoprime Statistics page. What happens if n is even? For example, Lucas_5(6,1) = (1189,6726). Mod 1000 should yield (189,726). But the routine gives us (689,726). This is an artifact of the halving method used. My solution was to use a different, slower, method in that case, since it's rarely used (never by a standard primality test, but there are more uses that just those). For D=0, we can return a solution. See the Wikipedia page. Check the cases where P and/or Q and/or D exceeds n. It looks from a quick test that the returned Q_k values are out of range for k={0,1}. Doing a check and mod up front may improve performance. Consider instead of using libmp. Up to you, but I like fewer dependencies. I don't know all the tradeoffs between the two however. For the Q=1 case (used by the extra strong Lucas test) there is yet another method that can be faster. If D can be inverted mod n, then we can calculate V_k with 2 mulmods per bit, then compute U_k using the inverse. I hate to add more cases, and of course you'd need to benchmark to see if it worked better for your infrastructure and use. 

Now let's have a look at your code. There are a couple of oddities which strike immediately: Using for comparisons You got to be careful with with in PHP, it's not typesafe. What this specifically means for you, in your case, it that is actually , while the typesafe operator correctly yields . This is an important difference for you, since you are using as a special value, but there might also exist an entry with the perfectly valid numerical ID . Having an variable This is a direct consequence of not having a single root node. Half of your code revolves around deciding whether a node you visited is a root node or not. Specify a root node explicitly prior to entering the recursion, and this problem is void. Scanning the result set multiple times instead of indexing it You don't actually need to iterate over the result set over and over again to find the rows you want. Index it once, and you are good to go: 

I'm sorry to burst the bubble, but achieving "constant time" in any environment where the compiler, a bytecode interpreter, or even the processor might optimize your code, is delusional. This only works on very simple architectures where you control both the machine code, and know the precise hardware characteristics. With a Java compiler + Java VM + unknown hardware arch, you are far off from these requirements. I'm serious about the hardware. Worst case would e.g. be the JVM inserting a NEQ test around the character XOR and addition, which could trigger the branch prediction in the processor to go onto the "fast path" as soon as it encounters the first 2-3 equal characters. That's not even in the code you've written, but it's a sensible optimization to be added by the JVM. This is not even just about the "Blackhole" potentially being eliminated by the compiler, but being unable to assume even constant time for the actual comparison. If you want constant time, there is only a single legit solution: Use a clock and then wait to enforce a constant execution time. Beware that in concurrent systems, getting the system into a state of starvation on CPU time will still allow an attacker to estimate the effective cost based on raw throughput. Adding a random wait on authentication failures as suggested by others is a good start, but also suffers from the aforementioned issue. This is only reliable as long as the authentication is additionally rate limited to avoid the possibility of being CPU limited. 

Once happy with what you have, you may find it entertaining to look at some other solutions, e.g. Haskell, Perl serial / parallel, PARI and Mathematica Additional info Dec 2016: Example times for fast implementation in serial, time for first n Fibonacci primes: 

Unless I'm misunderstanding what your iterators are doing. A simple SoE will have something that looks like: 

The problem is that for arbitrary 18 digit numbers this is still going to be slow and may or may not hit your target time (it's about 0.11s each for 1000 numbers on my computer with 10 other computations running). I can speed that up about 3x using primes to 2000 then a mod-30 wheel. Going to Pollard-Rho makes it run thousands of times faster. If you really need it fast, you're going to want something like Pollard's Rho, Brent's improvements to Rho, P-1, or SQUFOF. Trial division (even efficiently done) is just too slow for general numbers this size. The basic Pollard's Rho is pretty simple -- the only real trick is doing an efficient . Brent isn't too hard -- more or less just running the gcds in batches with backtracking when needed. For this size, P-1 is probably going to be slower than Rho/Brent. SQUFOF is more complicated to implement. An additional complication is that now you need a primality test to know when you're done (deterministic M-R with at most 7 selected bases, or BPSW). It's possible to skip that by just using Rho etc. opportunistically, but that doesn't fully utilize the idea. 

This already guaranteed you that there can only be a single holding media object attached to each post, by design. Drop either of the relations. In a relational database, you want either a forward, or a reverse reference, but never both. Double linkage in a relational database is in fact not even possible once you start enforcing constraints. Not to mention that it is very easy to create an incoherent state, by inserting sets where the reversed relation points to the wrong row. I get that you want to express ownership of in order to be able to collect orphans, but that's not the way to do that. Let's just assume that double linkage itself isn't horribly bad, even then remains a bad idea. Why would you enforce an in , when you only know that multiplicity constraint for and ? Just drop that , and model the multiplicity solely via . If you want to reliably clear out orphaned media, just write yourself a little garbage collector for that purpose, in that case you can even safely drop that all together. Finding media which isn't referenced by any other model should be quite easy. 

If were then the could be reduced to protecting just the statement, but as it stands it must protect everything inside . 

In your shoes I would totally scrap using (IMHO it's useless in practice). If you want to go the extra mile and auto-detect the user's locale, use an IP database (I have used MaxMind GeoIP in the past myself) or the upcoming W3C geolocation spec. 

This check with would allow any url that maps to a function to be successfully dispatched, even if that function does not map to a "real" action (it can simply be a helper method). That's not really a security issue (you can easily make sensitive functions ) but it's not very consistent: targeting will get you to the index page, while targeting e.g. will show a blank page as most likely does not directly produce content. By demanding that actions are implemented in functions with names starting with "action" you can make sure that this never happens. The default action is a special exception to this rule, as it allows the controller writer to effectively specify which url routes to the default action and what the default action should do at the same time. Third: It's not consistent to show a 404 for a missing controller but not do that for a missing action and use the default one instead. You should do the same in both cases, and the right choice would be the 404. Fourth: Parameter handling needs immediate attention. To begin with, you are sanitizing parameters to an unreasonably restricted set. What if there's a search action somewhere and the user wants to type in a character like or a non-alphanum string? Clearly parameter sanitization needs to simply be removed. Apart from that, you are passing GET parameters to the controller action positionally. The positional part is not a dealbreaker by itself (although it would be a dealbreaker in a framework, or if the URL format were configurable), but the GET restriction is a bit ugly. However, this cannot be fixed easily because there's no good way to know where parameters from $_POST should be inserted, or what their order relative to one another should be. To address this, you need to make the code reflect into the controller method and look for argument names and default values, pull these out from your list of GET parameters and $_POST, place them in an array ordered by the position of each named parameter in the function signature and call the function with that. You should also probably return an HTTP 400 if the action turns out to have a non-optional parameter the value of which was not provided. All in all not the end of the world, but a speed bump and a non-trivial amount of code. 

Now might even be an arbitrary of the model instead of a single post, and you get the run time down to what you originally wanted. Cutting down on the number of individual database queries is the key here. 

And now we halt the program for 2 seconds, and expect the user to know that he should press a key to continue. There is the instructions missing. Either that, or that delay is entirely pointless. 

Even worse - what you just implemented yourself can have unitended side effects if the row object has additional properties with special meaning, which coincidentally collide with (not actually ignored) keys in the passed data. So your logic can be written more compact and less error prone once again: 

I probably wouldn't use the old and classes, their interface predates the introduction of generics in 1.5, and as you noticed isn't even type safe yet. 

While at it, you should get used to using the optional 3rd parameter on to pass in a predicate in order to filter spurious wakes: