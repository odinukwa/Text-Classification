To my knowledge, the other problems are all merely kind of unexpected, not deeply troubling like lack-of-causality could be. But since we have a menu of ways out, there is no known logical barrier to an infinite past (of sorts). 

There exists a unique set of features that results in the lowest (best) score. There exists many sets of features that, when chosen appropriately, tie for the lowest score. 

If you make a bunch of observations about particle behavior and explain them with laws, and then observe gas behavior and come up with Boyle's Law, and then realize that Boyle's law is just a consequence of the particle laws, you are no longer assuming Boyle's law. You can still use it, but since it's a consequence of other laws (which you are assuming, and had to the whole time, because you needed to explain particles too, not just gases), it's not an independent assumption. As another example, you could have a "rainbow law" about when and how rainbows appear. But it turns out that our knowledge of optics (regarding how light behaves when it passes through different materials) tells us that light passing through small droplets of water of roughly the right size must generate a rainbow. So you don't need to assume the rainbow law(s). 

You need to explain what you mean by exists and without energy. There are many cases where the presence of energy is incidental to the existence of "something": take a DVD for example, where your movie "exists" as a series of little holes in the recording layer of the DVD. If you compare the total rest energy of this thing with the rest energy of a blank DVD, you'll find that the difference could go in any direction, including down, depending on the details of the pit-forming process. (So you very easily can have less energy and your movie; existence doesn't take any extra energy in comparison to the default configuration.) On the other hand, there is thought to be zero point energy even in "empty" space. So as long as you have space to have something, you also have energy (or at least fluctuations therein, which is enough, since energy is always relative). So, yes or no, depending on how you prefer to look at it. 

(emphasis mine). You have imported an assumption here that, I think, explains why all of your examples are problematic. What precisely do you mean by the world? If souls are real, and they (we) exist after death, does the world include the post-death environment of the soul, or not? If no, why do you privilege existence-as-soul-plus-corporeal-physical-form above existence-as-soul-in-post-death-environment? If yes, is it necessarily true that you have to find evidence pre-death, or can you wait until post-death to make the call? So I think these are sensible questions to ask, even if, after a long detour that requires one to rethink one's epistemology, one concludes that the answers are, "no zombies or the question is meaningless; the mind is the same as the body or the question is meaningless; the origin of the universe is an insensible question unless you're redefining universe". You don't just (or many people do not) end up there intuitively, so the questions are reasonable to ask even if at the end you conclude, "Well, that was all just confused, wasn't it?" 

I think you're right that computation is not fundamental, but to me the point of using computation is that universal computation lets you compute anything including any metric of information that you might like. Therefore, measuring information in terms of computation is merely availing yourself of infinite expressive capability. It's not a statement that computation is the key thing here, just that you can do whatever you want with it. Also, constant factors matter a lot. If I invent a programming language which contains the previous paragraph as the symbol ☆ then there is just a constant factor difference in program length to express the above paragraph (some 440x), and yet I have failed utterly to capture the intuition about information that Kolmogorov was presumably aiming for. If you do not self-impose the limitation to use compact general-purpose computing devices, Kolmogorov information measures return nonsensical results. This also illustrates that computation is not what is fundamental about information. Rather, it is a tool (that must be appropriately used) for analysis. That said, I do not believe you can "prove" that the bit is the fundamental unit of information. You can certainly adopt a set of axioms that include something logically equivalent to "the bit is the fundamental unit of information". Most texts on Shannon information do exactly this (in that they prove that the Shannon information of is the right one†, and the computationally most trivial representation* is with = ). But if you ask on a deeper level, I don't think you can show that the bit is fundamental rather than the action potential (or synaptic release) from a biological perspective; or than the standard deviation (or confidence interval) from an analog statistical perspective. At some level they're all equivalent (but you have to dig pretty hard to get from each to the other), and which you favor will depend on your perspective. † For a very good reason, as any introductory textbook will prove to you. * Not for any really good reason except that two states is the minimum possible number, and it just so happens that physics conspires to make two-state devices easier to build than others, and so our computers are binary. 

is that it makes it sound like either one of these rules is a true rule quantified over all locations, or the other rule is a true rule quantified over all locations. That's not what's going on at all. Only the whole statement is quantified over all locations--for any location you pick, at least one of these things is true. So, how do we manage that? For example, suppose we're not in Paris. We choose the "when in Paris" rule; the premise is false, so the rule tells us nothing. Now, suppose we enter Paris. We choose the "when in London" rule; again the premise is false, so the rule tells us nothing. In particular, what we cannot say is that 

Whether it can or not depends entirely on the physics of the world in which the simulation is being run, and the physics used to create the simulator. What other answer could there possibly be? In particular, we know that the answer can be no it cannot because we can't distinguish our reality from something Turing-computable, and a Turing machine as described needn't affect anything outside that machine. And we know that the answer can be yes it can because of course you can have arbitrarily much coupling between the contents of your computation and the rest of reality. (We mostly only couple heat production because of the architecture of the computers we use.) Now, if you mean "can we use results from a simulator to impact the real world", then the answer is yes of course, that's why we build them! For example, almost all aspects of aircraft design are done in simulators before any real component is built. If you mean "should we just simulate reality and then take the answers that simulated humans (or whatever) develop and use them", then the answer is no of course not, how wasteful and error-prone because you could put your computational resources towards actually generating the answers you want instead of simulating a bunch of people playing Angry Birds, and anyway, you've probably got your simulated physics wrong and so the answers there won't translate. (This is not really a philosophy question; it's a speculative physics question.) 

So, if Sleeping Beauty wants to maximize number of questions answered correctly, she'll choose tails, while if she wants to maintain accurate thought for the maximal duration, she's free to choose randomly until she gets information. If you ask her to calculate probabilities instead of give a single answer, then it matters what exactly you ask. If SIA vs. SSA boils down to anything other than asking different questions / scoring your rewards differently, then making that assumption is unjustified. 

Unfortunately, the rules for life are far from clear. You can do anything to a bacterium. People get quite upset if you torture chimps and dolphins. And even more unfortunately, the rules for when something is meaningfully human life are subjective. Development starts with a fertilized egg that, aside from developmental potential, is rather less aware than the typical coral polyp, and ends with a human, with gradual changes all along the way. Saying "conception is an abrupt transition, so that is when human life begins" is perhaps convenient for bookkeeping. (Birth is another abrupt transition.) Few systems of morality advocate for a stance merely because it simplifies your bookkeeping. So there simply isn't a broad (let alone universal) consensus about what precisely counts as human life and when it starts, and in any case it's biologically a gradual process which presumably means that moral weight should gradually accrue also. Presumably with gradual accrual, there could be some points at which the rights of the mother would be weighted equally or more strongly, which would make the mother's rights relevant (e.g. right to self-determination, i.e. "choice"). 

Isn't an epiphenomenon a thing? Is it really useful to say that--forgive the overly-used example--"air pressure doesn't exist" because it is an epiphenomenon of the statistical properties of air molecules? Indeed, if you follow this sort of logic to its most reductionistic extremes, you start to conclude that nothing is a thing except for those very most fundamental forces and logical constructs, the former of which, at least, we probably haven't fully discovered yet. In order to avoid a complete inability to converse, you then have to invent a new word for "thing" (entity, object, whatever) that means basically what the old "thing" meant before you destroyed it. This is perhaps an interesting exercise, but ultimately an unproductive one. You could ask instead: is there a difference in kind, in some important way, between grammar and the otherwise undirected interplay of syntax and semantics? Considerable difficulty in answering this question arises because grammar is not something that we defined mathematically and then sought to understand, but rather is something that we observed people using. One way to proceed is to ask whether our brains are specialized for grammar in a way that is separable from syntax and semantics. If yes, then in some sense it is at least an interesting enough epiphenomenon to be worth talking about in its own right. And, in fact, there is moderately good evidence that this is the case. Damage to Wernicke's area causes syntactically and grammatically mostly-correct production of semantic nonsense. Thus, our brains compute semantics separately from syntax and grammar. In contrast, modest damage to Broca's area can cause retention many elements of correct syntax, but with muddled grammar--strange choices of word order, difficulty in understanding certain grammatically valid word orders, and so on. Although this is less clear than the result for Wernicke's area (severe damage can result in the loss of the ability to speak, or the retention of the ability to only speak a handful of words, and the distinction between syntax and grammar is not all that crisply defined anyway), we can provisionally say that there are grammar specializations distinct from rudimentary syntax and the full richness of semantics. Thus, yes, grammar is a thing. Our brains think so, at least. 

The answer you get will depend on who you ask. There is no consensus within philosophy, so depending on which philosopher you ask the answer may be either yes or no or maybe or it's-impossible-to-tell. However, within the natural sciences there is pretty good consensus that we are just (analog, noisy, non-deterministic (due to quantum mechanics)) computers. Of course there are a few people, mostly who aren't really in the right area of science to be familiar with all the relevant evidence, who cling to the idea of something special and non-computable about consciousness. But as far as I can tell, the entirety of scientific evidence comes down on the other side. To start, we need to recognize the profundity of the Church-Turing thesis, which says that all of the ways in which we build up complex computations are fundamentally equivalent to each other. Although it's possible that such a thing exists, we've had a good seventy years to come up with a strictly more powerful framework, and not found anything that can't at least be approximated (given what we know about the physical laws of the universe). So just from fundamental physical laws plus fairly profound results in computability, we would suspect that if there is a difference in humans' consciousness or other mental capabilities, it would be a difference in quantity, not in kind, at least not fundamentally. (It might not be practical to simulate consciousness--getting one bit of matter to emulate another sufficiently accurately is not always an easy thing to do--but this wouldn't change the answer that we are fundamentally chemical computers.) On top of this theoretical result, there are oodles of connections between brain (the physical organ, composed mostly of neurons and glia) and mind (our subjective sense of self and mental capacity). Between drugs affecting consciousness, fMRI predicting our thoughts and choices before we are aware of them, various illusions that make sense given the properties of neurons thought to implement perception of that sense (e.g. vision), behavioral disorders correlated with damage in various brain regions, and so on, that our brains give rise to our minds is one of the best-supported scientific theories out there. We obviously can't test it directly, but the pile of incidental evidence is impressive. So the working hypothesis should be and is that we are "just" chemical computers. It does not follow that all the sorts of qualities that we value in humans--compassion, inspiration, consciousness, etc.--do not exist. We can plainly see that they exist. It just means that these things are implementable, somehow. A beach doesn't compute much; it doesn't follow that silicon cannot be used for computation, only that it's not organized the right way on a beach. Likewise, if our computers don't seem much like us in many ways, the simple explanation is that we haven't arranged them to (not without lack of trying, but building sand-castles isn't going to help much with calculating a square root either--if you don't know what to do, you can't do it, even if you have very good reason to believe that it could be done). Philosophy is valuable in that it keeps us for taking this answer for granted, as it's not completely definitive, but it's seemed to have lost track of whether the other possibilities are near-certain or merely formal possibilities that have not been (and in some cases can not be) ruled out. Regardless, our best-supported theory by far is that we're (electro)chemical computers. As to what the difference is--well, goodness, where to begin! The architecture is utterly different--not so different that it's not Turing-computable, but everything from clocks to error-handling to distributed processing etc. etc. etc. is not done the same way in our brains. For some tasks which we do well, emulating what we do gives far better results (e.g. image recognition these days pretty much has given up on trying to do it the classic computer-science way and is just mimicking what our retina and visual cortex do). So the bottom line is really that we don't know what the key differences are because there are so many, but we have every reason to believe it's just a matter of different engineering, not something utterly fundamental.