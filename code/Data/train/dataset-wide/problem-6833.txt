This is more a related thought than anything else, but I thought it might be useful for people looking at this question in the future. Herr K. should write a short answer and have it marked as accepted. Let $X$ be an $n \times k$ matrix with linearly independent columns. Let $S \equiv \text{span}(X)$. Regression can be thought of as the following problem: Given an n-dimensional vector $y \in \mathcal{R}^n$, find the vector in $\hat{y} \in S$ that is closest to $y$ -- i.e. $$\hat{y} = \arg\min_{z \in \mathcal{R}^n} ||y - z||$$ It can be shown that the solution to this problem is given by $$\hat{y} = Py = X (X' X)^{-1} X' y$$ where we refer to the matrix $P = X (X' X)^{-1} X'$ as the "projector matrix." Then the matrix $M = (I - P)$ allows you to get the residuals of your regression by $\hat{u} = (I - P)y = y - \hat{y}$. As you said, if $X$ is square then $P$ reduces to the identity and the residuals are 0. This is insightful for several reasons. 

The agent is worried about uncertainty (an interpretation of the max-min is that he thinks it is a magic urn that tries its best to pay the least amount as possible, so it will choose the value that leaves him worse off if given a choice). Then using this in his decision process when comparing $L_{A'}$ and $L_{B'}$ he thinks these lotteries will pay the following amounts 

Another somewhat canonical form is the value function for risk-sensitive preferences when consumption follows a random walk with drift (there are also versions including capital -- see Backus Ferriere Zin 2014). $$c_t = \mu + c_{t-1} + \sigma_c \varepsilon_{t}$$ Begin with preferences given as Epstein-Zin with a certainty equivalence function of the form $\mu_t(x) = E_t[x_{t+1}^\alpha]^{\frac{1}{\alpha}}$: $$V_t = \left( (1 - \beta) C_t^{\rho} + \beta \mu_t(V_{t+1}) \right)^{\frac{1}{\rho}}$$ then letting $\rho \rightarrow 0$ gives us $$V_t = C_t^{1 - \beta} \left[\mu_t(V_t) \right]^{\beta}$$ $$V_t = C_t^{1 - \beta} \left[E_t[V_t^{\alpha}]^{\frac{1}{\alpha}} \right]^{\beta}$$ Taking logs gives us risk-sensitive preferences as presented in Hansen Sargent 1995, Tallarini 2000, etc... Define $U_t = \log(V_t)/(1-\beta)$ and $\theta = \frac{-1}{(1-\beta) \alpha}$ then we see that: $$U_t = \log(C_t) - \beta \theta \log \left[ E_t \left[ \exp \left( \frac{-U_{t+1}}{\theta} \right) \right] \right]$$ The form of this value function can be guessed as: $$U_t = \gamma_0 + \gamma c_t$$ References: 

Let $\Omega$ be the set of all feasible allocations with an element $\omega \in \Omega$. Consider $I$ agents such that the utility of agent $i$ is described by $u_i(\omega)$. Definition 1: $\omega \in \Omega$ is weakly Pareto-optimal if $\nexists \omega' \in \Omega$ such that $\forall i$ $u_i(\omega') > u_i(\omega)$. Weak Pareto-optimality is basically just saying that for any other allocation, $\omega'$, in the set of feasible allocations that not everyone can strictly prefer the alternative. Definition 2: $\omega \in \Omega$ is strongly Pareto-optimal if $\nexists \omega' \in \Omega$ such that $\forall i$ $u_i(\omega') \geq u_i(\omega)$ with strict inequality for at least one $i$. Strong Pareto-optimality is a little stronger in the sense that your allocation must be strictly preferred by some to all other allocations and everyone else can be indifferent. I hope that is helpful. 

While many general equilibrium models do not need to model money to approach the questions that they would like to answer, there are many models that do include money to address questions that need money to be a relevant feature of the model. These models do it in a variety of ways -- some might be more relevant than others. I will try and describe two of the approaches briefly. Cash-in-advance When I think of a cash-in-advance (CIA) model, the first models that come to mind are Lucas (1982) and Lucas Stokey (1987). Models that take this approach have agents saving in assets and money, but they must finance any consumption solely through their money holdings (basically assets are less liquid than money which isn't the worst approach). There are many critiques of the CIA of which I find Wallace (1998) to be one of the more thoroughly presented. New Monetarist More recently there has been a push to use models of money that are less "ad-hoc" about how money works. Some foundational papers of this field are the Kiyotaki Wright papers (1989, 1991, 1993). These papers build a very basic model of money and show what role it plays in economics -- There is a good discussion on Randall Wright's Wikipedia page under the sub-title "Research contribution." While these models confirmed some of the reasons for money to exist in a model, they were quite simplistic and lacked many other features of the real economy. A monetary course I took labeled these types of models as "Generation 1 models of money." There was a group of papers that were known as "Generation 2 models of money, but these shared many of the simplifications made in Generation 1 (so I won't discuss them here). Since this time Lagos Wright (2004) introduced a more complete framework (we referred to these as "Generation 3 models of money." One of the issues faced by these new monetarist models is that in order to solve the model, one must keep track of the entire distribution of money -- This is computationally infeasible unless the authors introduce a simplification in a different area of the model (in the Lagos Wright model, they simplify the form of utility in order to be able to compute the distribution of money next period). There is on-going research in this area and, in my opinion, is an open area for improvement in the literature. References 

Dr. Sargent's contribution was related to the use of structural macroeconometrics in policy and analysis and Dr. Sims' contribution was to the use of vector autoregressions (VAR) to analyze temporary changes in policy and other factors. Source: Nobel Prize Website 

Another book that comes to mind (in keeping with the 1 book per answer guideline) is the Quant-Econ book that is being put together by Tom Sargent and John Stachurski. This book is a little more geared towards programming than John's Economic Dynamics book and can be found (for free) online at $URL$ All of the book's code is on github and I think it provides a pretty strong introduction to the two programming languages that it uses (Python and Julia). 

With no other information, it is not possible to determine with $X(P_X, P_y, I)$ is a homogenous function. As mentioned in the answer by @BB King, it is (very) likely that because it is a demand function it is homogenous of degree 0. Formally, a function, $f : X \rightarrow \mathbb{R}$, is homogenous (of degree $d$) if, for any constant $c$, $f(cx) = c^d f(x)$. Using this definition will allow you to check for yourself whether it is homogenous or not. 

This should help you figure out how to compute RSS for the different models. Let's begin with what we have: We know that 

We first take logs, $$\ln(f(X_t, Y_t)) = \ln(g(Z_t))$$ If we do a First order Taylor expansion around the steady state, then we can write: $$ \ln(f(X_t, Y_t)) \approx \ln(f(X, Y)) + \frac{f_x(X, Y)}{f(X, Y)} (X_t - X) + \frac{f_y(X, Y)}{f(X, Y)} (Y_t - Y)$$ $$ \ln(g(Z_t)) \approx \ln(g(Z)) + \frac{g_z(Z)}{g(Z)} (Z_t - Z)$$ Thus we can write: $$\ln(f(X, Y)) + \frac{f_x(X, Y)}{f(X, Y)} (X_t - X) + \frac{f_y(X, Y)}{f(X, Y)} (Y_t - Y) \approx \ln(g(Z)) + \frac{g_z(Z)}{g(Z)} (Z_t - Z)$$ Recall that in the steady state $f(X, Y) = g(Z)$ and I will also multiply by one in several places ($\frac{X}{X}$ etc...), so $$\frac{X f_x(X, Y)}{f(X, Y)} \frac{(X_t - X)}{X} + \frac{Y f_y(X, Y)}{f(X, Y)} \frac{(Y_t - Y)}{Y} \approx \frac{Z g_z(Z)}{g(Z)} \frac{(Z_t - Z)}{Z}$$ Now define $\hat{x_t} := \frac{(X_t - X)}{X}$, $\hat{y_t} = \frac{(Y_t - Y)}{Y}$, and $\hat{z_t} := \frac{(Z_t - Z)}{Z}$. This is the percentage deviation of $X_t$ from $X$ (and correspondingly for $Y_t$ and $Z_t$). Then you can write the log-linearized equation as: $$\frac{X f_x(X, Y)}{f(X, Y)} \hat{x_t} + \frac{Y f_y(X, Y)}{f(X, Y)} \hat{y_t} \approx \frac{Z g_z(Z)}{g(Z)} \hat{z_t}$$ Two final things. First, one subtlety that caught me off-guard the first time I was switching between percent deviation and true values and you might want to be aware of; values that aren't normally negative can be negative because it just means that it is that percentage below steady state. Secondly, functional forms usually make these simplify quite nicely as you have probably seen in the log-linearized equations presented. In this example, Gali is using $y_t := \log Y_t$ as seen in the other answer, so hopefully this provides some intuition for what is happening elsewhere. Hope this helped. 

One book that comes to mind is "Economic Dynamics" by John Stachurski found here. This book discusses programming methods that are applied to economics. All of its accompanying code is kept on this github repository. 

The short answer seems to be yes your example violates expected utility... It mostly seems to me like a simple transformation of the first example you gave (but you got rid of the red balls). As mentioned in other answers expected utility is not equipped to handle uncertainty because it deals with taking expectations and expectations cannot be computed when you don't know probabilities. For that reason, I think it is relevant to my answer to specify the distinction between what risk and uncertainty are. 

In 2011, Tom Sargent and Chris Sims were jointly awarded the Nobel Memorial prize for empirical work. 

Full disclosure: I haven't read through the lecture notes that your provided particularly carefully, but I think I can answer your question. Edit: Heads up, by not carefully reading the link provided by the question, I missed something. The standard New Keynesian models (such as the one Gali presented) are modeled without growth. If you write down the model then you can represent it as a difference equation: $$0 = E_t \left[ F(X_{t+1}, X_t, X_{t-1}, Z_{t}) \right]$$ where $X_t$ contains all relevant variables and $Z_t$ represent the shocks to the economy. The "steady state" typically refers to the state of the world where $X_t$ is constant (think stable solution to a difference/differential equation) and $Z_t = 0$, thus you could write it as the solution to: $$0 = F(X, X, X, 0)$$ in which case $X$ would be the steady state value (notice not time subscripts --sometimes also done by denoting steady state with overhead bars $\bar{X}$). This is what he is calling $Y$ and it is a constant value. For the second question, I haven't read carefully, so I can't be 100% sure, but typically when a variable is written as $X_t$ it references the actual value that is taken (aka if you solved the model and simulated it exactly, this is the value it would have). For the third question, I think a deeper understanding of log-linearization will answer it for you. Log-linearization at its heart is just a Taylor expansion around the steady state. Consider a generic equation $f(X_t, Y_t) = g(Z_t)$. There are 3 basic steps to log-linearization (refreshed my memory here). 

Additional Note Both Thomas Sargent and Chris Sims were at Minnesota when Lars Hansen was working on his PhD there. They both worked closely with Lars and contributed to his development. Sources