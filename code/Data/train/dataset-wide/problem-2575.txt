Algorithm You have three nested loops. This is inefficient. A slight nit-pick is that you also use an inefficient way to get the resulting String using the and then the . I believe if you take an 'additive' approach to the process you will have a faster system. By extracting the onle loop in to a function it is also more readable: 

A quoted field with a line-break at the end, should extend to the next quote on the following lines. The quoted line-breaks should be treated literally. 

I dislike any recursive function which rely on external (outside-the-method) values. In your case, the is external. This makes the target hard to 'see'. Additionally, if we do sort the data, there are some benefits we can have, and a way to restructure the recursion to make it do less work (since we can guarantee that all values after a point have certain properties...): consider the method (assuming sorted ): 

I cannot see anything wrong with the way you are using it. The example is a little contrived (given that you are not actually using cassandra), and the only things I can see that look odd are a direct result of that. Bottom line is that it appears to be a fine piece of code (which is perhaps why there have been no reviews so far). As for some nit-picky things: I don't like that you are hard-coding the size of the buffer (on the write-side): is 'ugly'. I tend to do th 'overkill' approach and use a buffer that is always too big. I find memory is cheap for the occasional buffer (even up to a couple of megs of memory...). I then make the buffer a thread-local so that I am not always creating new buffers. In your example it is trivial because it is all in the main method.... but for a 'real' program you may want to reconsider. If you know your data will always be less than, say 4K, I would instead do the following on the write side: 

This behaves in the exact same way (because JIT happens after the property is set)... but, if you start your program as: 

This is a great concept to work with. Creating an enum of comparators is a clever way to make it easy to manage the sort order of file collections. The name concerns me. I realize that is convenient, but enums are normally given a "noun" name. "Sort" is a verb - something you do, not a noun which is a "thing". I would call it , or even . Adding to it makes code more readable. Additionally, enum members are normally in all-uppercase and under-score separated.... so, instead of . Adding the "File" to be you no longer need to have as part of the enum names, so you will have: . I believe you have truncated some code from the enum. It does not compile without the override of the compare method. I'll just claim it is because of how you summarized things for this question. Now, about the functional aspects. I agree, the override is a mess, and can be sorted out quite easily. First up, what you want to do is pass a comparator in as the constructor for the enum, and then use that comparator as a tool in the compare function.... like this: 

Thanks for including the method. As I suspected, the purpose of the hashCode method is to fulfil the equals/hashCode contract, and is not suitable for being an ID. For example, by changing the number of wins, you change the hashCode, and by proxy you change the player's ID. If you want the ID to be a temporary thing that is only constant for the short time that the player is constant, then your solution is OK. If you want the ID to last through a persist/restore cycle, or even from one moment to the next when they pee .... and their weight changes, then you need a different system. In addition, while you claim your hashCode is unique, it is not technically true. The chances of their being a duplicate are remote, but it is possible that two players will have the same hashCode, and hence the same ID. In reality, your system is not reliable, and the best solution would be to create an externally managed ID for each player - a record number from a database, a unique counter, or something. 

Your concrete TwitterStatus class would, in a single-threaded application, be OK for the purpose of having a single-set ("lazy set", or "set and freeze") mechanism. Two concurrent threads each setting the value, or one thread setting, and another thread getting the value may cause problems though. The situation can be relatively easily resolved with an AtomicReference value though... Consider the following value that replaces both the and fields: 

This should always be safe.... but, your code throws if the list is empty. Also, why call it , it should match other standard conventions, like for removing items, and I would suggest the name , with an offset and a length, like everything else that does these things. Bug 2 The original title of the question was: "Delete N Nodes after M in a linked list" Apparently I got fixated on the after part of the title, when the actual description makes more sense and means 'from', not 'after'. The method name is still 'After', so, that, in a sense is all that's left of my Bug2: 

In your own words: "I totally understand that my method is not the most efficient solution." In a sense, that's the summary of a code review for this problem. The basic issue in your code is that you have ignored two of the clues in the question: 

This class is essentially useless. It is a container for a Predicate, which is accessed through it's protected nature anyway. Also, what about it is 'restricted'? Why does it have that name? The static methods on it could be anywhere too. They have no business being on this class specifically. Basically the class is irrelevant, and makes the class structure more complicated than it needs to be. Moving the functionality to is trivial. 

Since the getter/setter is the only place where you actually know the type of the value, there is actually zero benefit for your entire subclass... it is redundant. You can get the same behaviour with: 

Find forward until the last , then take whatever comes after it until the next , , or end-of-line, and call that group 1. Then replace everything with group 1.... As an ideone here: $URL$ 

All in all, that's a pretty good implementation in a number of key details. Using the and methods are good to see. The methods that handle the overflow conditions are good too. I am not sure if you intended it, but you also correctly handle an obscure edge-case in 32-bit signed integer systems (not just Java), where is not the same as ... and your code actually gets it right for an exact input of the text "-2147483648" So, you have good details in your code.... and I can't see any broken edge-cases. My only recommendation would be that a state-machine may make things simpler... with just one loop..... but the state-machine may be a bit messy too, although I think it works out better in the long run... 

If you need to do something more special than just the 'needs modification' test, for example, you need to distinguish inside the block between these conditions, then I would recommend an Enum with a static method, for example: 

So, about 40,000 exceptions per half-second, 45000 exceptions if you include the unnecessary . But, the actual printing of the stack trace? Well, that's 450,000 times... In other words, printing the stack trace is about 10 times slower than creating the exception. Creating the exception is not fast either. Bottom line is that, if you don't have an exception, then there's no real reason to create one. If you want to create a trace just to create a warning message, well, that's an indication that the warning should be more severe.... Once you know where the costly parts are you can make a better decision as to what's important. Finally, I have seen significant improvements in performance when people make better use of exceptions..... but my experience goes back to the Java 1.3 times, when things were different. 

The above function takes the low-bit off of the input value, and feeds them to the high bits of the output value. I have put this together in a simple ideone 

Let's go through a couple of things. Concept, then missing functionality (.... you have some). Concept An N-Gram is a sequence of N words that have been found in a span of text. You can have 1-grams, 2-grams, 3-grams, .... n-grams. You identify these n-grams by finding all possible n-wide spans of text, and storing them. In the sentence , there are three 2-grams 'the quick', 'quick brown' and 'brown fox'. There are two 3-grams 'the quick brown' and 'quick brown fox'. When processing natural languages, it is often statistically convenient to weigh the likelihood of a particular word happening 'next' after an existing sequence of words. That is what this problem is about. given a span of 'n', find all the (n+1)-grams. Then, taking any n-width words, look for all the (n+1)-grams that start with those words. Randomly chose one to select the 'next' word. Then repeat the process until you run out of (n+1)-grams that match, or you hit the 'sentence' limit. You have just built a sentence that is statistically 'likely'. A smarter system will 'weight' the next word based on the frequency of the (n+1)-grams that were found in the text. I.e. If the original text has 'the white house' 10 times, and 'the white swan' just once, then it will 'randomly' choose 'house' 10 times more than 'swan'. OK, that gives you some context for the problem. Functionality The challenge/requirement was to take the value as an input. You have hard-coded it as 2. In other words, you have 2-grams and 3-grams, when you are supposed to have n-grams and (n+1)-grams. 

Your generator needed to spin-loop almost 75,000 times in order to generate 50,000 values in the sequence. A quality-checking XOR function has some value 212738101 (this is a good thing, I will explain later), and the average latency on the 'next()' call was 5.2 microseconds, the time to calculate the XOR function was an average of 1.3 microseconds, and the whole sequence was completed, in many threads, in 91.5 milliseconds. Put another way, you ran 4 threads at 100% for 91.5 milliseconds, and you generated 125,000 fibonacci numbers (but only used 50,000). Note, when I ran your code, I did it 10 times (after warmups), with the following results: 

While your code is neat, and organized, the issue is with your algorithm. The simple fact is that you only need to loop through the array once, when you create the instance. The trick is to "memoize" the sum of all members to one side of the array, and then compute the difference when looking for the sum later. Consider the code: 

Your solution is actually quite decent, and the code style is good, etc. There are a couple of style changes I would recommend: 

There is once very significant issue, one significant, and a few run-of-the-mill issues that concern me. The most significant is the use of . interrupts on threads are not used by anyone, willingly. They are a PITA that few people understand, and everyone hates. First, though, let me talk about the poison pill. This is a good idea. It is well used. There's an implementation problem though.... Poisong pills are normally one specific instance of an object, and it is shared, typically through a public static field. I would do the pill as: 

I am not a great fan of fall-through logic ... if there is an argument issue (missing argument, whatever), you are falling through the work functions, and letting the program end. I would make this more explicit with an . Programs should always set an exit code unless they successfully complete. I don't consider argument errors to be a successful completion. aside - Bash has some reserved exit codes, which you should typically not use. is one of the reserved exit codes. As a result, try to use values other than 1 or 2 for exit codes from shell scripts.... I would recommend creating an error-handling function... something along the lines of: 

Return-Early Having suggested the RegEx option, I would also suggest a simplified lastIndexOf/indexOf using early-returns, instead of nested if-blocks: 

Your code is not thread-safe. Each of the threads will, in parallel, be accessing both the , and the variables. Your Lambda is, in essence, modifying external data from the stream, and this is an anti-pattern for streams. It has side-effects. You should change your code to use the collect mechanism. There are a few ways to do it, but, you should look at this example for guidance: Reduction Some Notes about my following suggestion: 

All in all, it looks like the code is relatively well structured, and it looks like you have some good ideas in there. 

Given the large size of the potential result set, the previously suggested iteration mechanism seems appropriate. I looked at this and thought 'nice challenge', and I looked at your code, and figured there were too many loops. The way I see you doing the work inside the loops also looks cumbersome... I figured it would be 'neat' to have a generator that created all the segments on an as-needed basis, and did not do the actual work.... then I played with it, and found there were some potential optimizations. So, the parts of your code that are potential performance problems: 

I can only see one 'bug' problem with this code, but there are a few other 'style' and 'simplicity' issues. reportsDaTestare What is this? It appears out of nowhere: 

You are jumping through a lot of hoops to get your queue, consumer, and observer strategies aligned. I look at your code, and I don't really follow it all. Part of the reason is that I don't know RxJava at all, but, normally, I can follow these things without much research. So, your code streams the tasks, for each one, it creates an Observable for it which is subscribed on the Reactive computation engine. The observer for the Observable is set to add the result to the queue. Now, for the queue, you create an Observer that registers one action for each expected result. You then loop over those events, and print the result. I can understand why you may want to use RxJava for some situations, but, that last part makes no sense to me... Let me show you your code: