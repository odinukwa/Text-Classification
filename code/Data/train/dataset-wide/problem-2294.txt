Trace flag 5004 is useful for starting/stopping TDE. Might be worth giving it a go in case it helps. DBCC TRACEON(5004) GO DBCC TRACEOFF(5004) GO 

So you've got 5 'stacked' instances on a single Windows server. You haven't said exactly how many sockets/CPUs are available and how much memory though. I like setting affinity for each instance in such cases, even if I decide to have CPUs overlapping in the struggle to balance the overall CPU load (depends on each instance's load). Any instance with more than 4 CPUs could use an explicit DOP setting in my experience - rarely over '4' in stacked cases like yours. Don't forget to set 'Cost threshold of Parallelism' for each instance to something reasonable (50?) to avoid excessive parallelism - in your case this is even more important. Remember that the memory left "for the OS" should be more now, since you have to account for the footprint of each instance (on top of SSIS etc). Check in SQL Config Mgr if SSAS is also running and adjust its 'max memory' accordingly, by default it goes for 80% of the whole server memory (!) Also maybe worth taking away and 'Lock pages in memory' rights of the SQL Service account(s) so that the OS can breath and do its job better (if it pages, everyone suffers!). Also good practice is to set some reasonable 'min memory' for each instance. I think running sp_blitz and sp_blitz_first on each instance would give you some quick pointers on more pressing issues. You may also want to monitor some windows permon counters like 'available memory' and 'working set' for each of the processes running there in case you find particular times of the day/night when the server is suffering. 

Alternatively, you could omit the alias altogether since in this case you are not joining the pivoted set to other datasets and so all columns available would be only those coming from . However, one other problem is that you are referencing a column that is no longer available after the pivot: your column is used to produce columns and itself does not exist in the result set. You could try selecting all columns () to see the output. I suspect that you will find that your output is not what you expect, because you likely have too many grouping columns coming from the result of the join between and . How you should resolve that may need to be asked as a separate question, though. 

I would start with the list of available drivers. As you appear to want them as a CSV string, it makes sense to use grouping and concatenate the names with GROUP_CONCAT(). Use an outer join of jobs to the cross-product of dates and drivers, then, to get the available drivers for each day, group-concatenate the driver name only if it has no matching job: 

This is not an attempt at a different implementation of the same logic. This is merely an attempt at simplifying (and, hopefully, speeding up) the existing query. What I have noticed is that the two correlated subqueries that are being compared have just one difference. The second subquery has an extra condition in its clause, namely this one: 

Using the above query as a derived table, you can filter the output further on and get the difference total: 

Long shot perhaps, but worth checking file \Program Files\Microsoft SQL Server\100\DTS\Binn\MsDtsSrvr.ini or the equivalent on your setup. You may have to manually edit it with the instance name. Otherwise SSIS connections might be looking for an msdb of a default SQL instance that doesn't exist. 

Just my 2cents from my own experiments on 1-2 year old hardware: Read-only operations (DW-style scans, sorts etc) on page-compressed tables (~80rows/page) I've found to break-even at compression size reduction of ~ 3x. I.e. if the tables fit into memory anyway, page compression only benefits performance if the data size has shrunk by over 3x. You scan fewer pages in memory, but it takes longer to scan each page. I guess your mileage may vary if your plans are nested-loop and seek-heavy. Among others, this would also be hardware-dependent (foreign NUMA node access penalties, memory speed etc). The above is just a rough rule-of-thumb that I follow, based on my own test runs using my own queries on my own hardware (Dell Poweredge 910 and younger). It is not gospel eh! Edit: Yesterday the excellent SQLBits XI presentation of Thomas Kejser was made available as a video. Quite relevant to this discussion, it shows the 'ugly' face of CPU cost for page compression - updates slowed down by 4x, locks held for quite a bit longer. However, Thomas is using FusionIO storage and he picked a table that is only 'just' eligible for page compression. If storage was on a typical SAN and the data used compressed 3x-4x then the picture might have been less dramatic. 

In a / query, a clause filters a single sub- (the one immediately preceding it) rather than the entire UNIONed set â€“ thus, each sub-select can have its own clause. (This is unlike e.g. , which you would be allowed to specify only once and it would apply to the combined set.) So, if you want products by the same maker B from each of the three category tables, you need to repeat the filter for each sub-select: 

Note absence of GROUP BY. The OVER clause makes SUM a window aggregate function. With a window function you can return aggregate data along with detail (non-aggregate) data. The query above returns output like this: 

You might also want to switch to , because, as SQL Server does not support unsigned types, you will have only 127 positive values using the type. Additionally, if you want to ensure the values cannot be negative or nil, you can add a check constraint: 

The CASE expression is the column of your query's derived table. It is used in the COUNT function as well as in the WHERE clause (the IN predicate). Usually such repetition of code is eliminated by nesting. But nesting cannot be used here because of the MySQL limitation mentioned at the beginning of this post. So, repetition of code is the price you have to pay to work around it. Luckily, there is not much of it in this specific case. 

The problem is that Element_Answer.Element_Answer column is referencing two distinct tables. The weird prefix in some of the rows is apparently there just to tell whether the value references one table or the other. Nevertheless, the fact that two different tables are referenced from a single column is a violation of any normalisation rules. So, I strongly believe that the best solution to this would include a redesign of the Element_Answer table, so as to split its Element_Answer column into two: one referencing Form_Element_Groups.sort_order and the other referencing Element_Answer_text.ID. Both columns would be numeric as they both are essentially just references to numeric IDs. For the purposes of the scripts below, I am calling the column referencing feg.sort_order by the same name, sort_order, for consistency, and the other reference is called Element_Answer_text_ID. Feel free to replace the names with whatever you consider more appropriate. So, first you create two new integer columns in Element_Answer: