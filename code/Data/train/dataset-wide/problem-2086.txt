SQL Fiddle Following discussion in the comments about how this could be extended to get the per partition. This would be possible 

So it might be provisionally assumed that the first ("unnested") syntax is potentially beneficial as it allows more potential join orders to be considered but I haven't done exhaustive enough testing to have much confidence in this as a general rule. It may well be entirely possible to come up with counter examples where Query 2 performs better. Try both and look at the execution plans. 

This will be a decision of the cost based optimizer. It estimates that it is cheaper to fully scan the narrow index. It seems that you were expecting a nested loops with seeks on the clustered index? The execution plan shows that the table has 3.9 million rows. This will take 195 batches to clear out. Assuming that all rows in are unique and initially exist in and that the rows output from are in a consistent order for each batch (I'm ignoring the parallelism) then each successive batch will end up seeking all rows that have already been deleted in previous batches before finally getting to a patch of rows that have not yet been processed then in total your process will do more than a third of a billion index seeks for already deleted rows. 

If the issue you are having is Truncation you can use this XML approach that gets around most of the issues with XML entitisation. 

As the local temp table is not accessible to other connections you get this behaviour without explicitly taking a table lock however the comment about size of table still applies which is why you see the difference in your two cases. If you need a specific order add an to get scan in key order (with ). 

First it creates the stats for column successfully (the initial / pair) . Then it starts creating the stats for column (The selected entry in the screen shot). This entry is followed by an event confirming that the statistics on were created then the timeout kicks in. It can be seen that the stats creation occurs on the same spid as the query and so this also aborts the creation of stats on column . At the end of the process only one set of stats exists on the table. Edit The above refers to stats creation, to test auto update of the stats I ran the above query without a timeout so both sets of stats were successfully created then updated all columns of all rows so that the stats would be out of date and re-ran the test. The trace for that is pretty similar 

The value for is stored on the database boot page. (page 9 in the primary data file). This is not written to every time a timestamp value is allocated. Instead SQL Server reserves a few thousand at a time. For example if is and the is also then SQL Server updates the value written in the boot page to the next time it needs a timestamp value. The values from are allocated in memory and "lost" if the database is set offline and then online again. The backup contains the copy of the boot page that has been updated to . So upon restoring it the timestamp values will start from this number. It knows nothing about any lost intermediate values. To see this 

is just a session option allowing you to insert identity values explicitly in a subsequent statement. It doesn't do anything to the transaction log. I presume your first transaction wasn't actually committed and now is rolling back. This could happen if you have on Or a possibly more likely scenario is that you were already in an open transaction unknowingly. This could happen if your original script contained an error between the and . Executing could then terminate execution leaving an open transaction (If you had used any error would have rollbacked the transaction rather than leaving it open.) Then after fixing the error and rerunning the script in the same session the would be called again and increment and the would just decrement it down to 1, leaving the transaction still open. Rather than killing the session and causing transaction rollback the correct thing to do in that case (assuming you were happy that everything was OK and the original error condition hadn't caused changes that you wouldn't want to commit) would have been to check and again as many times as needed (I.e. in the event that there were multiple executions before the error was fixed) until that reached zero. 

If I explicitly try and reduce the sample size of that index's statistics to that used by the others with 

The plan shows a sort operator with an estimated sub tree cost of nearly and erroneous estimated row counts and estimated data size. 

However since SQL Server 2005 compiling a plan with a local temporary table does actually create the temporary table behind the scenes. You can see this by getting the estimated plan for 

The above figures illustrate the point about merge join as it "only" scans just over half the larger table. It still read all the rows from 1 to 1,999,999 first though and discarded them. Repeating the experiment with a gave the following. 

The first query has reads and the second so it is reading an entire additional day then discarding it against a residual predicate. The plan shows the seek predicate is 

You can use trace flag to print out locking info and to print logging info. (Both undocumented AFAIK) For the following test set up 

They will of course be able to see the names of the columns by issuing a and be able to get the datatypes with a bit more effort by a new (possibly ) table. They will lose intellisense on that table. You also say 

The negative object id is the id of the object fleetingly created. Polling in a loop whilst generating the estimated plan and comparing with the TableID subsequently output can show this. The estimated plan you see for the statements referencing the temp table is of limited use however as even the addition of a single row to the temp table will trigger a statement level recompile and potentially a different plan when you actually execute the SQL. This behaviour does not apply to global temporary tables or permanent tables. I presume this is also the same reason as 

The only query that didn't benefit from the additional range predicate is the one where the larger table was on the inside of a nested loops join. This is of course not surprising as that plan (plan 1 below) is driven by repeated index seeks using the values from . 

Original Answer Some databases do already (kind of) create indexes automatically. In SQL Server the execution plan can sometimes include an Index Spool operator where the RDBMS dynamically creates an indexed copy of the data. However this spool is not a persistent part of the database kept in synch with the source data and it cannot be shared between query executions, meaning execution of such plans may end up repeatedly creating and dropping temporary indexes on the same data. Perhaps in the future RDBMSs will have the capacity to dynamically drop and create persistent indexes according to workload. The process of index optimisation is in the end just a cost benefit analysis. Whilst it is true that humans may have more information about relative importance of queries in a workload in principle there is no reason why this information could not be made available to the optimiser. SQL Server already has a resource governor that allows sessions to be classified into different workload groups with different resource allocations according to priority. The missing index DMVs mentioned by Kenneth are not intended to be implemented blindly as they only consider the benefits to a specific query and make no attempt to take account of the cost of the potential index to other queries. Nor does it consolidate similar missing indexes. e.g. the output of this DMV may report missing indexes on and Some current issues with the idea are 

Similarly tweaking the cost threshold for parallelism so that the query gets a parallel plan was sufficient in the 130 case to revert to the lower estimate. Adding also causes the lower estimate. It looks like the 10% estimate is only retained for trivial plans. Your proposed rewrite with 

There are two basic approaches that SQL Server can use. A stream aggregate requires the data to be sorted by the group by keys. This can be either supplied by an index or might need an explicit sort. A stream aggregate is order preserving in that the rows output from that operator are in the same order as the input. This does not imply any guarantee about the eventual output from the query as a whole however. You only get that if you add an . For a the input stream can be ordered by either or to be acceptable for the stream aggregate in this case. The addition of an index might change the decision of the optimiser as to which one to use. The other basic approach is that of a hash aggregate. In which the grouping keys are hashed. This is not at all order preserving and will likely output rows in seemingly random orders. Additionally for parallel plans there might be hybrid approaches, e.g. Each thread could have a local stream aggregate with the thread results then aggregated at global level with a hash aggregate. Adding means that SQL Server will ensure that the rows are delivered in the desired order. If you are currently observing an output order of then likely you are getting a stream aggregate ordered by those two columns. The addition of an explicit order by won't change the execution plan to add any additional sort as SQL Server will recognize that the output of this aggregate is already in the desired order. 

can change the execution plan. For example if you try the below you will see a different plan with and without the hint commented out (on SQL 2008 SP1 CU5 and later) . 

Finally just for completeness with the trace looks as follows. It can be seen that system spids are used to perform the operation and they are unaffected by the query timeout as would be expected. 

One caveat however is that this does not always work. Adding a check constraint with definition causes it to fail with 

The definitive article on the topic is Arrays and Lists in SQL Server 2005 and Beyond which contains code and performance test results for a variety of split functions. You say these are column names? You should probably read another of Erland's articles on The Curse and Blessings of Dynamic SQL where he discusses both SQL injection and when dynamic SQL is a good idea.