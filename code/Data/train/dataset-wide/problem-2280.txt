I am not sure how smart it is to run multiple copies in parallel. It all depends on what your hardware can handle. If you want to run in parallel, run the copies in the background but be a little careful, when running too much in parallel, it will definately cause a slower throughput than running in a sequential way. For windows it will be slightly different, the loop construct will be different. I hope this helps. 

When you have naming conflicts, there is a problem, combination not possible. You might want to take some downtime for maintenance/upgrades. If you can not get a downtime from all applications at the same time, you have a problem. Using Resource Manager you can give a certain performance guarantee for the specific services. Services are a smart thing to use, it is the easiest thing to see how resources are used, compared to one and other. Easiest is to run multiple instances on a single server, each serving it's own database. This is the easiest but not the smartest thing to do. Smartest is to have a single instance on a single server. This is because every instance considers itself as the master of the server. You can not very easy isolate their resource usage, as you can do in a single instance. If you want to give some performance guarantee in a multiple instance server, the setup will grow in complexity because in many cases you need to start multiple projects and users to run your databases under. A single database can easily support a few hundred applications, a lot cheaper than using a few hundred databases. This quickly saves BIG money on a yearly basis by making good use of Oracle features. 

yes, backup of your parameter files is a smart thing to do, like backing up your database is a smart thing to do. If you still have an alert log file of your database, you can extract the parameter settings from there and use them to create a pfile using good old vi or whatever simple text editor you like. Most important parameters are 

As long as you have your database organized in such a way that there is an account that is the owner of the tables and other objects, and your application connects using a separate account you can do what you want. If you have tables and procedures organized in separate owners, you fist have to grant the required tables to the procedure owner (directly granted). Next you can grant the execute to a role or a user. As soon as you allow connections to an object owner your security is broken since in that case you can always use all the objects of the owner (you are owner so you can use it). 

gives some clues about the commandline options. study the errors. If there is a complaint about the dmp file, read the first few blocks. From those you can find who exported it, what type export it is and even some version info. It is a bit binary but just readable enough to get this info. tip: If your data has a real value for your company, hire a real dba to make a decent setup and have someone as a backup. It is easy to build a system that work happily for years and due to hardware issues suddenly become less happy. It's a pity if that costs your company the head. This is regardless for what brand of database you setup. 

In Oracle if you have the diagnostics pack available, simply go to the Performance tab in Grid Control or DBConsole and see which sessions are currently playing with your database. If it happened longer ago, take a dive into the awr reports and find the top SQL based on various metrics. It would help, if you mentioned a bit more about the actual database you are using, for example, brand, sometimes platform and versions. 

The regular session background processes can normally be killed. The special database processes ... many will automatically be restarted but better is to leave them running. Often those client processes have a wait event, something like 

your database is running in archivelog mode. An initial backup + archives allow you to recover your database to any point in time between the backup and the last available archivelog file. If this is a big import you are most likely going to make a backup afterwards. If you have no need to recover the database to a state between now and the previous backup, you could use rman to delete the archived log files, freeing space so the archiver can continue. An other option is to increase the db_recovery_file_dest_size parameter that could be in place since you claim to have plenty of space available. 

Start monitoring memory usage. Does your application terminate old sessions? If not, there is a connection leak that eventually takes away all processes your database instance can use and prevent new connections. There also could be a memory leak, that is why monitoring memory and swap could be smart to do. Most likely to me seems to be the connection leaking. You can easily monitor this by making counts of v$session or v$process. These counts should not grow unlimited. Since you use jdbc (thin client I asume) Dead Client Detection could also be useful. This gives the rdbms the ability to clean up unused sessions. Unused sessions are those that do not react upon a ping from the server. 

The error message is very clear, you need to specify a procedure or a valid pl/sql block. If you really want to truncate the aud$ table from a piece of pl/sql, you will need to use dynamic SQL. 

This is a bit hard to tell, what is high-end hardware? Using Oracle on EMC storage 60MB/s is certainly possible. Using that same old Oracle on exadata can be slightly faster, 300MB/s and sometimes higher, again, depending on the configuration. A full rack is faster than a half rack ... 

The best way to find out will be to run some known code samples with and without using the profiler. The differences normally should not be much but indeed can vary. Even dbms_application_info can hurt when put inside a high frequent loop. 

No, Oracle won't add an extra datafile. Normally the SYSAUX tablespace is more or less stable so it would be smart to check what is eating the space in there. Connected as a DBA user, run the script to get the current usage of the SYSAUX tablespace and see how it will grow when you change certain parameters for which you are asked to enter values. See OracleÂ® Database Administrator's Guide 11g to get an idea of regular sizing. 

Since no platform or version is mentioned, I assume Unix/Linux and 11gR2. First of all, check Running ASMCMD in Noninteractive Mode. This explains that we can use ASMCMD in a script so whatever we can dream of, we can script. In this case in order to copy files we can use 

Use the full normalized database, scan which queries need more performance than this model can give them and create materialized views on them. If the data is very volatile, having them fast refreshing introduces some overhead but you know the value and validity of your data, that is protected by the database. Oracle handles this very nice. You queries remain using the normalized tables and Query Rewrite will take care of the usage of the Materialized Views, when appropriate. When the Materialized Views are not fresh, the queries fall back to the original tables. For docu see Materialized View Concepts and Architecture 

John, normally it does not make sense to use dbms_redefinition in your case since you are not redefining a table. You are creating an extra schema. dbms_redefinition is meant for situations where for example your table T1 gets an other layout and this has to be done online. Application can stil use the old form of the table while you are completing the new table. This is done by creating materialized view logs. I hope this helps. 

The problem is the locking in the SGA that effectively cause the the concurrency is going down. This kind of apps make the database a close to single user platform instead of a highly scalable and concurrent accessible database. The cause of this is that the connections have to be maintained in a shared memory table where inserting and deleting means locking and causes serialization. There is a nice video series about this kind of real world performance problems connection pooling Implementing connection pooling in the demo app in the video causes a 10 fold performance upgrade ... The spawning of the processes does cause a little overhead but that is handled well by the OS. Next to that, the listener log wil grow quickly, causing every next connection to be slower than the previous one. If session auditing is enabled - a default - the sys.aud$ table will explode and make your system tablespace grow very rapidly. I see many databases where this is the cause of an enormous system tablespace and sometimes even bigger than the rest of the database, because of this connection problem. 

It looks like your database experienced an incomplete recovery. This is why it complains about open resetlogs. After an incomplete recovery you need resetlogs. To be able to get this database up and running again, you need to know what caused the current situation and some skills to get it out of there. If the data is not important at all, just trash the database and create a new one. The actions needed for this depend a little on the platform where you are on. the documentation looks like a good starting point. 

This creates the spfile for this instance with the default name in the default location (${ORACLE_HOME}/dbs/spfile${ORACLE_SID}.ora). Shutdown your database and start it without specifying a pfile or spfile. It should select the just created spfile and give you back your database as it was before. 

If you get problems before or during mount, it is probably because of wrong dbname, db_unique_name. Set them as they were/are in prod. 

You could try to fiddle with optimizer_max_permutations (default 80000) to reduce the number of combinations that are considered to build a plan. To my opinion your customer needs good help, migrating to 11gR2 would be a step in the better direction. There is an awfull lot of automated help in query tuning that in the previous release had to be done by hand. My guess is that this old version is also the reason why it takes so long to respond on a problem for the supplier. 

Looking at the paths of the datafile, I assume/hope this is no production database. If this database is setup as a regular production database, it is running archivelog mode. In that case it is quite simple to recover, if you have a valid backup for the damaged datafile[s], including all archived logfiles created since the start of that backup. restore the datafile[s] effected recover the database open the database. Oracle still has the docs for this ancient release online. Check Oracle8i Backup and Recovery Guide for the details. After that, upgrade to a current release. My guess is that many dbas have never had training on this release .... I think having a good dba at hand could be helpful... 

For this you use pass through. The client connects to the TT instance and TT passes the query through to the Oracle rdbms, when needed. Why not just add a few more nodes to the grid and also cache the bigger tables? 

For all scenarios there are solutions. Normally I work for sites that won't allow the loss of any data. Most of the times they also want protection against unplanned downtime. This is the ultimate protection. As most things, it also has a price. Most protection has the highest cost. But if you need this, there will be a business case for it. That is why you should start with the question what dataloss costs your company. This will give you a budget. For documentation start with DBA Essentials A very nice third party solution for DR can be found here dbvisit standby Hiring support from a good dba can make a huge difference for your company. A good dba has your business in mind when he/she proposes solutions. Your question a asking a bit much. You need dba support. For an overview pick the Oracle concepts guide. Read it and use it to help you hire a good dba. 

If your scheduler database fails, this should raise an event in your monitoring application. This is about the similar situation as compared to cron, when the server fails. In that case cron also has no way to tell it failed. If you want to use dbms_scheduler as an enterprise scheduling tool, don't forget the reporting. Reporting should help explaining what ran and why not. 

which files do you have? which version of Oracle database did they belong to? how were they backed up? what kind of data would you like to read? (text or also numeric/date formats?) what do you already know about the data you are looking for? how did you get these files in your possession? what volume of data do you expect to have to read/recover? 

Chances are that your table contains user defined types or is created in a tablespace for which you have no quota in your local database. You can trace the session that runs the script in your local database to find where it crashes. An other option is to get the ddl for the problamatic table and apply that to your local database for better error messages. To get the ddl run: 

to list the databases that are defined on the current host. This comes very close to the Windows services. You could pipe this into the next lines to filter out the database name and request that status of the database, where the instances are listed: 

If the database is setup correct: no direct path load. If a standby database is depending on the changes captured on the primary database, the primary database should be setup with force logging. If for some reason the force logging setting is not in place, after such an operation the affected datafiles should be transferred to the standby site[s]. Failing to do so will make queries on the loaded parts of the table[s] in the standby database to fail. 

Since you upgrade your OS, at some point you need to relink your Oracle software. In that case, easiest is to make a new software installation on the new server. Unmount the luns where the database is located from the old server and mount that on the new server where the database files should be mounted on the same locations as on the old server. I hope that the ORACLE_HOME is not on the same lun[s] as where the database is located. If ORACLE_HOME does exist on the same LUN[s], make sure to create the new ORACLE_HOME on a different PATH and make sure that config files like /etc/oratab and listener.ora reflect this change. This should give the quickest migration. If there is to be done more than just re-mount the LUN[s], make sure to have a capable DBA involved. That will save you a lot of downtime and problems. 

I don't know about sqlserver but in an Oracle database, I would suggest making a trace of the sql execution, one that includes all waits and events that cause the query to spend time. This shows the exact circumstances where the sql is executing in and that might be very different than those in the environment where you did the explain plan. Sqlserver without doubt has a similar feature to show the real execution, including waits. In Oracle we have sql plan stability. Maybe sqlserver has something similar? In that case, try to use that. 

First start the instance, next mount the database and open the database to enable access to the data. This can all be done in one simple step: , this starts the instance, mounts the database and opens it to enable access for applications. 

The error you encounter can have many causes. One is an incorrect environment. For Oracle, the variable ORACLE_HOME is very important. Start with setting that to where you installed your Oracle software. Next make sure that PATH points to %ORACLE_HOME%\bin After that, set the variable ORACLE_SID to the sid of your database instance. (The sid is part of the service name as is shown in the services control panel) Once this is setup, you commands can be issued as you stated.