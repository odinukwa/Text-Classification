There is an overall critique of 'rights' as a moral category. A lot of it is captured by Glendon, who is discussed here: $URL$ It has many dimensions, but some of them are easy to capture as logical contradictions. First, they are categorical, they are either being violated or they aren't, and life is not that binary. So you can have contradictions like the notion of the right to life in a world where everyone dies, leading to insane expenditure to save an individual life that then depletes resources and causes others to die. So we don't really have the right, we have the wish we had the right, and the obligation to consider those wishes somewhat equally. Second, they are adversarially posed, so they lead to the construction of enforcement mechanisms that often automatically break them. The right to order and stability, maintained locally by the existence of states, more globally creates wars in which humans are swept into chaos and violated in unspeakable ways. So we don't really have the right, we simply have the opportunity to control how it gets expressed when it happens to exist. Third, they are individual. We can mandate a right to property, but some property is only useful if it is shared. Property rights to a waterway, for instance mean nothing if someone uses their property right to the part upstream from you to build a dam and divert the water. Then the waterway to which you have the property right does not exist. So neither of you really have that right, you have something more complex and negotiated that cannot really be framed as a right on either party's part. Fourth they automatically create responsibilities, but do not assign those to anyone. We may consider it a right to have a fair and impartial justice system. But that doesn't mean that our justice system won't make itself corrupt and racist. So again, we don't really have the right, we have the obligation to hold the intention to have the right, so we fix these problems when they arise. 

Wittgenstein has noted that all of logic is a set of tautologies. If you want something other than tautologies, you need an inductive base. From a basic logic point of view, statistics never support an argument. So all observations of frequency are not conclusive. But we generally live in a world that also acknowledges science, at least at some basic level shared by both Aristotelian and modern scientific standards. You can actually count observed instances and say 'there are N observed instances'. If you have enough resources, you can find that 'm +/- s% of the population with a certainty of d%' does whatever. To declare arguments like that illogical holds one far too close to a place where absolutely nothing that is not tautological can be proved. And their informal equivalents are not illogical, they are just informal. Whether N is 'enough', or whether it is 'large' are subjective, but declaring talk about them 'illogical' as the base of an argument because they lie outside the strict bounds of philosophical logic is simply evasive. One can agree to deal with likelihoods, or one can discard science as a whole. Since modern life depends upon science to a ridiculous degree -- agree to deal with likelihoods. At that point this argument has an obvious logical means of solution. Use the word on people to whom it applies, and see if insult results in a large enough part of the population to establish its effect with an acceptable p-value. 

As someone who basically accepts Kuhn, the problem I see with it is that modern physics really does all three kinds of science at once, so this notion of segregated revolutions fails. 'Interpretations' like 'many-worlds' or other workable mindsets that are not part of the model are really pre-science (adaptive mythology), normal science obviously proceeds, and alterations to groundwork are also not yet stable. So, what period would we be in? It is unlikely we will meet another revolution in science that is truly 'incommensurable' with Newtonian physics and does not actually rely upon the approximate truth of that physics for its justification. If you read a quantum dynamics text, it is full of Newtonian physics, whereas Newton was not full of Ptolemy (or even Galileo). This is not like the contrast we had with atomism and substance, or with the new notion of elements existing in large numbers. In those cases, the old paradigm was in fact not a necessary part of the support for the new paradigm. It conflicted, and adopting a new paradigm left known facts newly unexplained. We even have two different new, revolutionary paradigms in process, both of which rely almost completely upon the fact that Newton was almost completely right. And we are having a hard time choosing which should be primary, but we expect them to agree in the long run. A quiet, pedantic revolution with two outcomes at the same time, each fully accommodating the other. How revolutionary! We are shifting the paradigm. But it has not caused any massive crisis in 'normal science', which just got split up a bit, while the old parts kept chugging. And although the modern theories had some major detractors and led to some excessive declarations, they were seen as curmudgeonly or bombastic, not dangerous. So either there is a distinct reduction in the quality of revolutions over time, or his notion of revolution is simply the high tide of a force that runs continuously through science at various scales. To continue looking at it in terms of its most extreme cases will probably not serve the discipline well in the future. 

One point of sheer science weighs in here. The primary result of the theory of cognitive dissonance: If a fact is by design 'stubborn' and not easily modified, the harder you have to work in order to maintain belief in it, the more sure that belief becomes over time, especially if you put yourself at social risk by abandoning it. Science's content is not stubborn, so belief in it does not become truly sure, pretty much ever, if it did, science itself would be impeded. But as a direct application of Festinger's results, which are reproduced in basically every generation of Social or Clinical Psychology graduate students, religions survive to the extent they make you work hard to stay a believer, without making it too hard to manage. So disagreements with science help maintain religious belief right up until a given breaking point. As long as there is some cognitive manipulation that permits the fitting back together, however tortuous, there is a direct and immediate reward for finding it -- control over cognitive dissonance. In fact, the more tortuous, the better-off the religion is. You can see this in 'big system' theologies like the Scholastic version of Roman Catholicism, when it meets an equally insistent huge network of complex inferences and images. The two often marry into something that people can just barely manage to not to get vertigo out of attempting to make sense of, and become another very active religion. The more strained the triple interpretation of a Roman Catholic Afro-American Syncretic cult like Vodoun or Condomble, the more dedicated its believers, right up to the point where obvious fictions (or an even more strained big-system faith like Mormonism) exist that are more imaginative and the cord just snaps because it would be more fun to believe the more obvious nonsense. The versions of Catholicism that do not demand a constant stream of offerings, elaborate immersive rituals, or the gore of ritual sacrifice are not doing nearly as well as Santeria, in terms of the sheer strength of adherence. At the same time, cognitive dissonance is an avoidable discomfort, which motivates people not to analyze the conflict if they do not feel it can be resolved. So conflict with science works against science which does not involve the same kind of social risk, because science not only permits questioning, it encourages it. Turning science into the kind of thing that demands obeisance, just distorts science, and although it happens repeatedly in history (high Caliphate culture, the Enlightenment, the Victorian period, to some degree Scholasticism, ... for the Left, now.) To the extent this is true, no, incompatibility between science and religion both predicts leaving and staying with one's childhood faith, in different ways, and it predicts caring more about religion and less about science. Religions are chosen both to make sense and not to make sense. This weird balance is not about logic, it is about identity and social cohesion, which has a wholly different character. 

This is the principle used in classical mathematics that is created by presuming the Law of the Excluded Middle. Whatever does not lead into contradiction must already exist, and can be used as needed. Its existence does not need to be further defended or derived in any way. The primary use is to let us generalize more freely about things that we cannot enumerate or identify, so we can imagine what kinds of combinations those imaginary things might participate in. We can imagine different configurations of infinities or spaces by starting from what they would have to be like if they existed, without feeling silly about it, because we have already decided that they exist. This is very convenient -- until it isn't. It leads directly into traps like Russell's paradox and other confusing aspects of negation. Does 'nothing' exist? Well, it must, unless that would be impossible, and the impossibility seems unlikely. But what is it like, this absolute nothing? It verily seeths with internal contradictions, and we would like to be rid of it, except we have accepted that whatever is not impossible is already real. Lifting this principle from Platonic mathematics and transplanting it into other kinds of philosophy has the same effect. It broadens our horizon, but threatens to confuse us. 

Unfortunately the reference to y in 'Choose y' is not a concept of a given thing, it is a concept of a thing lacking a concept. This involves the concept of the thing having a concept, so we are close. But we can close the gap only if C(C(x)) = C(x). By some slippery English, you can make it seem like the concept of the concept of something implies there being a concept of it. But it is not so. It falls to the bicycles-and-cavemen example. The concept of that which I have not yet imagined does not provide the concept of a bicycle just because bicycles are something I have not yet imagined. The technical reason is that intensionally defined sets need not have extension (evaluation of an iterator can be lazy, so the constructors implicit in the enumeration may remain uncalled, for the functional programmers out there). 

every mother were treated respectfully, rather than some being stigmatized morally for 'illegitimacy', and others being dismissed as non-competitors because they enter the 'mommy track'; and she could bring the child to term and hand it over into a system where it could always reasonably expect a life fully comparable to that of other children, on average; and she were compensated for her lost time and pain (not necessarily fairly, but as soldiers have been -- on a scale that is minimal, but not insulting.); and if her other rights were not already selectively systematically degraded relative to those of men long-term by previous generations taking unfair advantage of women's willing participation in the process of pregnancy (or like in the case of men and war or crime, we acknowledge we are biased, but institutions attempt to balance this.) 

The existence of 'strange attractors' undermines the physical assumptions behind eternal return. The math does not work: previous centuries have just not understood how complex prediction really is. The standard cheezy example is that as you zoom in on a point on the boundary of the Mandelbrot set, the image really can be proved to continually produce motifs not present in earlier images. The fact it might look the same on many occasions does not mean it is repeating, because it can be moving through the same state in a different evolutionary direction. Even if there is infinite time, there are also infinitely many derivatives of any function, so infinite subtlety in its variation, and constant capacity to escape repetition. A simpler example that even the Pythagoreans knew about, is that digit expansions of pi do not repeat, so a circular object rolling infinitely around a square whose side is an even multiple of its radius will, in fact, not ever fall into a perfect, repeating pattern. The position on the rim where it starts down the next side will always be a tiny bit different from what it was on every previous occasion. If something that simple and clean never repeats, why would anything as complex as a universe? The classical answer here is that the difference is always shrinking and at some point the difference is small enough not to matter. But from parts of math like bifurcation theory, we see tiny differences can have huge effects over long periods, if they somehow eventually affect a point where the system is very touchy. Previous generations of mathematicians did not really take that to heart. There is strong human bias to presume convergent behavior. But given computers, we can see by observation that repeating systems of any high complexity almost always have points where some of the derivatives grow very, very large, so a very small difference can make a big bump. This was the ultimate death knell of high-powered analog computers, and the reason everything is digital now. From an entirely different direction, the level of determinacy presumed simply is not consistent with our observations of the world. We really do observe quantum indeterminacy. So if you have any faith in modern science, this is just not realistic or likely.