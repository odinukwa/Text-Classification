I always prefer to have error bounds for the CLT, so my favorite reference for your question is the paper "A Lyapunov type bound in $\mathbb{R}^d$" by Vidmantas Bentkus (Theory of Probability & Its Applications 49(2), 311--323, 2005). From the abstract: Let $X_1, \dots, X_n$ be independent, mean-zero, $\mathbb{R}^d$-valued random variables. Let $S = X_1 + \cdots + X_n$ and let $C^2$ be the covariance matrix of $S$, assumed invertible. Let $Z$ be a $d$-dimensional Gaussian with mean zero and covariance $C^2$. Then for any convex subset $A \subseteq \mathbb{R}^d$, $$|\Pr[S \in A] - \Pr[Z \in A]| \leq O(d^{1/4}) \cdot \beta,$$ where $\beta = \sum_{i} \mathbf{E}[|C^{-1}X_i|^3]$. This is a $d$-dimensional generalization of the Berry--Esseen Theorem. 

Elementary but still useful is the regular value theorem or the submersion theorem: Let $f \colon M \to N$ be smooth and $n \in N$. If $T(f)_m \colon T_m M \to T_{f(m)} N $ is onto for all $m \in f^{-1}(n)$ then $f^{-1}(n) \subset M$ is a submanifold of dimension $\text{dim}(N) - \text{dim}(M)$. 

Has anyone tried one of these Logitech conference cameras with Skype $URL$ as a collaborative tool. I saw one report on-line of someone trying to use it to look at a whiteboard and claiming it didn't work because of reflections of the shiny surface. Thanks - Michael 

In the case of differential geometry everything reduces to vector spaces. Let $x \in X$. Then at any point $(x, y) \in X \times X$ $$ T_{(x, y)} X \times X = T_x X \oplus T_y X . $$ Using this identification the tangent to the diagonal at a point $(x, x)$ is the subspace of $T_{(x, x)} X \times X$ given by $$ T_{(x, x)} (\Delta) = \lbrace (\xi, \xi ) \mid \xi \in T_x X \rbrace \subset T_{(x, x)} X \times X. $$ On the other hand the normal is the quotient $$ (T_{(x, x)} X \times X) / T_{(x, x)} \Delta $$ and we can identify this with $T_x X$ in at least two slightly different ways. Either $$ \iota_1 \colon \xi \mapsto (\xi , - \xi) + T_{(x, x)} \Delta $$ or $$ \iota_2 \colon \xi \mapsto (-\xi , \xi) + T_{(x, x)} \Delta . $$ We can of course also identify $T_x X$ and $T_{(x, x)} \Delta $ by $ \xi \mapsto (\xi, \xi)$. I guess one explanation for the two identifications of the normal bundle is that there is involution $\tau \colon X \times X \to X \times X $ given by $\tau(x, y) = (y, x)$ which fixes the diagonal pointwise and hence acts trivially on the tangent space to the diagonal. As a result it descends to an action on the normal bundle which interchanges the two identifications $\iota_1$ and $\iota_2$, that is $\tau \circ \iota_1 = \iota_2$ 

This does not fit the original poster's question but is vaguely related: I have heard it said that Carleson did not get the Fields Medal in '66 because his proof of Carleson's Theorem was too difficult to read and verify at the time. (Granted, the result was only published in '66.) And alas, he was too old to get it in '70. 

However in general the existence of this $\alpha_k$ is unknown. Friedgut's Theorem [Fri99] comes extremely close to showing that $\alpha_k$ exists. His result implies that for each $k$ the above statement is true for a sequence of real numbers $\alpha_k(n)$ depending on the number of variables. (It is also easy to show that $\alpha_k(n)$ is bounded in the range $[1,2^k \ln 2]$ for all $n$.) It's utterly inconceivable that this sequence could oscillate, rather than tend to a limit, but in general this hasn't been proven. For each $k$, the value of $\alpha_k$ is "known", via sophisticated heuristics from statistical physics [MPZ02, MMZ06]. In this sense, we sort of have an example answering Sylvain's question. In a recent major breakthrough, Ding, Sly, and Sun rigorously established the physics prediction for $\alpha_k$ for all sufficiently large $k$. It is worth mentioning that this does not obviate the need for Friedgut's Theorem; by virtue of that theorem, it was enough for Ding--Sly--Sun to show that 

Maybe R.O. Wells' book ? $URL$ Sorry I'm not in the office so I can't check. I don't think it's particularly difficult. Certainly if the bundle is holomorphic you can construct the Dolbeault operator by just acting on the components of a section written as a linear combination of holomorphic sections. This patches to give a global Dolbeault operator because the clutching functions are holomorphic. To go the other way you need an integrability theorem such as Newlander-Nirenberg to show there are enough sections in the kernel of the Dolbeault operator to span the fibre of the bundle and trivialise it locally. 

Surely you can draw the 2-D image in the XY plane so it consists of points of the form (x, y, 0) and then give each point in it a random non-zero Z co-ordinate. So it should look like a mess except viewed looking in along the Z-axis. 

Here is some general discussion of the level-1 weight of Boolean functions -- $URL$ -- including the theorems that: 

Max-Cut, at least, in cubic graphs is NP-hard even to approximate to some factor .997. This is due to Berman and Karpinski, 1999: On some tighter inapproximability results. In Proceedings of the 26th International Colloquium on Automata, Languages and Programming, Prague, Czech Republic, pages 200â€“209, 1999. I wouldn't doubt it if the optimal cut is a bisection in the "yes" case of the reduction. 

Noga Alon published half a dozen papers under the name "A. Nilli". Mathscinet links directly from this pseudonym to Noga's publications. 

Reminds me a bit of Talagrand's 2nd $1000 conjecture, a special case of which is the following: Let $f$ be a nonnegative function on the reals and let $g = U_t f$, where $U_t$ is the Ornstein-Uhlenbeck semigroup and $t$ is some fixed positive number; say, $t = 1$. Then Markov's inequality is not tight for $g$; i.e., $\Pr[g > c \mathrm{E}[g]] = o(c)$, where the probability is with respect to the Gaussian distribution. I'm pretty sure this special case is hard enough that Talagrand would give you a fraction of the $1000 for it. 

I'm not an algebraic geometer so let's call $X$ a real manifold. I don't think that really matters it could be a topological space or even a set for what I am about to say. Assume that $E \to X$ is a rank $n$ real vector bundle. For any $x \in X$ let $E_x$ be the fibre of $E$ over $x$. The natural structure you have in this situation is that if $f \colon E_{x} \to E_{y} $ and $g \colon E_{y} \to E_{z}$ are isomorphisms then you can compose to get $g \circ f \colon E_{x} \to E_{z}$ also an isomorphism. From this follows the fact that you have a groupoid whose objects are all $x \in X$ and whose morphisms from $x$ to $y$ are $Isom(E_x, E_y)$ (or $Isom(E_y, E_x)$ depending on how you like to compose morphisms.) (1) $Isom(E, E)$ is a perfectly reasonable object, it's a bundle of groups over $X$. But it doesn't capture all the information such as isomorphisms from $E_x$ to $E_y$ where $x \neq y$. (2) $Isom(p_1^*E, p_2^*E) $ is the union over all $x, y \in X$ of $Isom(E_x, E_y)$ and if $f \in Isom(E_x, E_y)$ then the two maps are $f \mapsto x$ and $f \mapsto y)$. (3) I am not sure of the answer to this but it seems reasonable to me that this groupoid captures information about the symmetries of $E \to X$. I don't think that $Isom(p_1^*E, p_2^*E) $ being a principal $GL(n, \mathbb{R})$ bundle is correct. I don't see any reason why $Isom(E_x, E_y)$, for example, is acted on by $GL(n, \mathbb{R})$. 

I think it might still be unknown whether the constant can be reduced below $e$. By the Central Limit Theorem, if it can be so reduced, then it can also be reduced below $e$ for functions on Gaussian space. In Remark 5.11 of Janson's book Gaussian Hilbert Spaces, he says that the best possible constant in the inequality $\|f\|_q \leq c(p,q)^k \|f\|_p$ (for $f$ of degree $k$ and $p \leq q$) is only known in case $p = 2$ (in which case it is $\sqrt{q-1}$). In particular, I guess that means the best possible value for $c(1,2)$ was unknown at the time of his writing, 1997. Note that he gives the argument for $c(1,2) = e$ in Remark 5.13. (It's the same argument that is reproduced in my book in the Boolean case.) Finally, as Janson notes in Remark 5.12, even in case $p = 2$, it's not true that $\sqrt{q-1}^k$ is the best constant that can be put on the right-hand side; it's merely the best constant of the form $C^k$. In particular, when $q$ is an even integer you can slightly sharpen the inequality, by a factor of roughly $k^{1/4}$. (The arguments for this are sketched in the exercises of my book.) 

Maybe try first with $G=SL(2, \mathbb{C})$, $K = SU(2)$ and $T = U(1)$, the diagonal matrices with determinant $1$. Then $K/T = S^2$. Edit: I was thinking of a discrete lattice so this answer which I thought was a counter example isn't. 

Rummikub ? It encourages some logical thought and analysis. It seems to have at least one mathematical paper on it $URL$ and it's popular and fun. 

If you are happy with Frechet bundles here is an alternative approach. Let $\pi \colon {\mathcal G} \times M \to M$ be the projection and consider $\pi^{-1}(TM) \to {\mathcal G} \times M$ a real vector bundle of rank $n$. Following the notation in the paper let $P_{GL^+} \to M$ be the $GL^+(n, {\mathbb R})$ bundle of oriented frames of $TM$. The bundle of oriented frames of $\pi^{-1}(TM)$ is $\pi^{-1}(P_{GL^+})$. As in the paper pick a lift $P_{\widetilde{GL}^+} \to M$ of $P_{GL^+}$ to $\widetilde{GL}^+(n, {\mathbb R}) \to M$ of bundles over $M$. This exists because we assume $M$ is spin. Then $\pi^{-1}(P_{\widetilde{GL}^+})$ is a lift of $\pi^{-1}(P_{GL^+})$ to $\widetilde{GL}^+(n, {\mathbb R})$. If $g \in {\mathcal G}$ and $m \in M$ then $\pi^{-1}(TM)_{(g, m)} = T_m M$ so has on it an inner product defined by $g(m)$. Denote this "universal" inner product on $\pi^{-1}(TM)$ by $g$. It will be smooth for the usual reason with Frechet manifolds which is because if $M$ and $N$ are finite-dimensional bundles then the evaluation map $$ M \times C^\infty(M, N) \to N $$ is a smooth map of Frechet manifolds [1]. Again following the approach in the paper we let $P_{SO} \subset \pi^{-1}(P_{GL^+}) $ be the subbundle of oriented orthonormal frames for the metric $g$. Taking the pre-image of this in $\pi^{-1}(P_{\widetilde{GL}^+})$ gives us a $Spin(r, s)$ bundle over $\mathcal{G} \times M$. The associated vector bundle to this using the spin representation gives us $E$ as a smooth, finite rank, Frechet vector bundle. Finally you want a theorem that says that when you "push-down" $E$ with $\pi$ the result is a smooth Frechet vector bundle on ${\mathcal G}$. This seems reasonably but I'm not sure where to find it. I can't see it in [1]. Sorry this is a bit sketchy but that reflects the sketchiness of my knowledge of Frechet manifolds. [1] Richard Hamilton -- The Inverse Function Theorem of Nash Moser. $URL$ 

The classic example, given in all complexity classes I've ever taken, is the following: Imagine your friend is color-blind. You have two billiard balls; one is red, one is green, but they are otherwise identical. To your friend they seem completely identical, and he is skeptical that they are actually distinguishable. You want to prove to him (I say "him" as most color-blind people are male) that they are in fact differently-colored. On the other hand, you do not want him to learn which is red and which is green. Here is the proof system. You give the two balls to your friend so that he is holding one in each hand. You can see the balls at this point, but you don't tell him which is which. Your friend then puts both hands behind his back. Next, he either switches the balls between his hands, or leaves them be, with probability 1/2 each. Finally, he brings them out from behind his back. You now have to "guess" whether or not he switched the balls. By looking at their colors, you can of course say with certainty whether or not he switched them. On the other hand, if they were the same color and hence indistinguishable, there is no way you could guess correctly with probability higher than 1/2. If you and your friend repeat this "proof" $t$ times (for large $t$), your friend should become convinced that the balls are indeed differently colored; otherwise, the probability that you would have succeeded at identifying all the switch/non-switches is at most $2^{-t}$. Furthermore, the proof is "zero-knowledge" because your friend never learns which ball is green and which is red; indeed, he gains no knowledge about how to distinguish the balls.