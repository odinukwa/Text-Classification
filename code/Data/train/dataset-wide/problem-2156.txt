One is to try downloading the ILS — International Language Supplement. This has many extra locales in it; it might have (but no promises). Another is to try copying an existing locale (maybe or to . You'd probably want to edit the LANGUAGE and TERRITORY lines. However, be aware that the French and German locales are different; it will matter which you choose. Report a problem to IBM Informix Tech Support, requesting the file for . 

Don't put quotes around things that aren't strings! You have (with some newlines and one space-after-comma added): 

— the key column. — the flag column, containing or (with a check constraint). — a proxy for the unidentified set of columns containing the other data. Primary key on and . 

You say the directory has permission; does it also have permission. You need permission to access the files in a directory (so you might set the permissions on the directory to ... gritted teeth ... 777, but you shouldn't. Is the directory owned by user and does it belong to group ? Ideally, it should be owned by , and the permissions should be 770 (or ). 

Unfortunately, that looks like you're not allowed to use tab either. (Test: Informix 12.10.FC5 on Mac OS X 10.11.6.) Incidentally, I tried some alternatives: (alert, aka Control-G) and (backspace, aka Control-H) are both accepted, but none of (formfeed, aka Control-L), (vertical tab, aka Control-K), (carriage return, aka Control-M) of (newline, aka Control-J) are allowed. Any other control character from through (octal numbers, all except 9-13 decimal, 011-015 octal) seems to be OK (and NUL or 0 being OK did surprise me!). Do you have any tabs in your main data? If not, you could export with one of the acceptable alternative characters and then map that to tab. If you have tabs, life is harder because you'd need to protect any tabs that are in a data field with a backslash. DB-Access allows without problem and produces an unload file with tab delimiters, so the problem is not in the shell code I'm using. If you're certain you must have tabs, then investigate Art Kagel's which is part of his package from the IIUG Software Repository, which also uses my SQLCMD, available from the same source. Using (or , or various other twists of the command line) produces output in a tab-delimited format. (On its own, SQLCMD does not handle a full export; it will cheerfully unload single tables, though. It does not yet handle NUL as a field separator or line separator — a design decision or artefact that I need to review.) 

And so on. By default, (you can also use for that, and for ) will batch the loading into sub-transactions of 1024 rows, but you can configure that, of course. 

You would need the 32-bit CSDK or Informix Connect (effectively the CSDK Runtime) on the 64-bit system (as well as a 64-bit CSDK or I-Connect for 64-bit applications). Apart from that, I believe that it should work. If it runs into problems, it is because of the Windows registry and the need for separate INFORMIXDIRs. On 64-bit Unix systems, it was common to have both 32-bit and 64-bit versions of Informix CSDK or I-Connect on a single machine in separate INFORMIXDIRs. These days, there is less 32-bit code — there are no longer any 32-bit servers on 64-bit Unix systems, and usually there isn't a 32-bit CSDK either. 

How familiar are you with Informix? MySQL? Unix tools (like Perl, Python, etc)? You can get the database schema and the data in Informix's UNLOAD format using Informix's DB-Export () command: 

It feels like there must be a smarter way around this, and I would appreciate any clarification and corrections to the statements made in this post. Like mentioned earlier, I'm just trying my best to understand why it has to be this way. Apologies for the lengthy post and thanks in advance. 

At this point, I would need to store the textual old/new value of the company name, as well as the id of the recently added user.You might see that I'm already headed off in the wrong direction, and this is where I ask for help. I have 2 questions: Should I just use -datatype, or if this is considered poor design then what would be a sensible way to store these log-events? Thanks in advance. 

For the two former events, I'd store the type of event, the old value and the new value. This works fine as long as I stop here. However, if I want to log the 2 former-events, I would need to be able to store different data-types in the same column. The changes would look something like this: 

Yet, the severity of the failed test is, as mentioned, considered high. Octopus Deploy While configuring the Octopus Deploy Server, the setup fails with a FATAL error during the initialization of the OctopusServer-instance. The article related to the error-message does not explain why this is a requirement, but simply states that it will be a requirement for future deployments, from and including Octopus version 3.8. As a side-note, RedGate's CI-tool package, the DLM Automation Suite, supports deployments with varying collations without complaints. The recommendation of keeping all column collations to the database default seems more like guidelines or best practices to me. Why is it considered such a serious error by some? 

I am attempting to construct TSQL queries to substitute various GUI tools provided by SQL Server Management Studio. One of these tools is the , accessible through the window. 

In Example #1, the SET-operation will update all columns, even if only 1 column has changed. In Example #2, the SET-operation will update all columns for all rows, falling back to the old value if the value is unchanged. In both examples, all columns are hit by the SET-operation, and, according to my seniors, this creates an unnecessary/problematic amount of transaction-logging when done frequently. The same applies for the -statement. Even if you check a matched row for changes, all columns are hit by the update. 

We have several tables which we "refresh" frequently by rebuilding them in a staging-table, then performing a metadata-switch to the production-table using the statement. These operations are causing a huge amount of logging, and we're not sure how to handle it. For now we just moved these tables and operations to a new database using the recovery model, but I would love to hear alternative solutions to this problem. How do you suggest we handle the massive log generation as a result of frequent table rebuilds? 

Now, I'm only a junior DBD and my understanding of how the transaction-log works is very limited. That being said, my seniors have concluded that we cannot use the MERGE or UPDATE statements where all columns are processed in the same statement since it creates excessive logging. The argument for this is that when you perform an -statement in SQL Server, when you set the a column-value and the new value equals the old value, it is still marked as an update in the transaction-log. This apparently becomes costly when you perform lots and lots of pointless SET-operations. In the following example, we update the and of the target-table using values from the source table, joined by . 

Contract — holding the single-valued data about a contract, excluding receipts. It might record the latest receipt number for the contract, but that would be an optimization, storing derivable data. Primary Key: Contract Number (aka Lot Number). Contract Items — holding the 1-6 items for the contract. Primary Key: Lot Number, Lot Sequence Number. Lot Number is a Foreign Key reference to Contract. Receipts — holding information about receipts. Primary Key: Receipt Number. Foreign Key: Contract Number reference to Contract again. 

There's no need to update statistics on a view in an Informix database. Indeed, AFAIK, there's no way to do so; statistics are not stored for views. 

This is a mismatch between the list of column names and the list of values in one or the other of the two insert statements. I've tracked that down using careful layout so that I could see that each variable matches each column name. In the first INSERT, you list column in the list of columns but don't pass a value for . In fact, you don't accept a as a parameter, so it is easiest to remove the name. This code 'works' — you'll have to experiment from here: 

For better or worse (and mostly worse), although some of the files necessary to support are present, the file needed in is not present. Thus, you can have or or or but you can't have . You have a few options at this point. 

My main concern, regardless of the migration mechanism used, would be for the consistency of the migrated data. How are you going to ensure that you don't miss changes while the data is transferred. One of the classic Informix tools could be DB-Export, which (by default) locks the database so it isn't being changed while the export occurs. It gives you the schematic and unloaded (text) data files. Another option would be to make an archive of the existing system and then restore that into a new (temporary) instance, and you could then run your migration against the temporary instance without affecting the working instance. But you'd still have to worry about later changes. If your existing machine is not too woefully under-powered, you should be OK running the export as you suggested, but you still face the issue of ensuring consistency of the migrated data if users are changing the source system while you are migrating. 

Server 2 is obliged by its duties under the 2PC to contact the coordinator for advice on the state of the transaction. Until it gets a Commit or Rollback from the coordinator, it must hold the transaction in the 'Ready to Commit' state, locks and all — even if it is restarted. This could go on indefinitely, of course, so in practice there are 'Presumed Fail' or 'Presumed Succeed' heuristics that can be applied, but it is not something to undertake lightly. If you get into a heuristic operation, then you probably do have an inconsistent set of databases — which is just one reason not to go 'heuristic' in a hurry. Of course, if the Coordinator or the network are out of commission for any extended period of time, you probably have other problems than just database consistency. You can find out a lot from reading Concurrency Control and Recovery in Database Systems by Philip A. Bernstein, Vassos Hadzilacos, Nathan Goodman (available for download). That is tough going, though. You could look in Date (Database Fundamentals, 8th Edn), but it is covered very briefly there, or Recovery Mechanisms in Database Systems by Kumar and Hsu, which has a substantial discussion of 2PC in chapter 13.