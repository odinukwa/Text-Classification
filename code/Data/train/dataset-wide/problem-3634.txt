We just got rid of three of those.. (finally upgraded some remote sites) and you know, I don't think they have any option for that (not that version even with up to date firmware).. however, we never had trouble connecting PPTP through them.. What sort of VPN are you attempting? (Cisco IPSec?) Where does it spit that out? (In the Netgear logs?) Have you got any SiteToSite VPN's configured on your Netgear device?, if so, and you are attempting an IPSec VPN, it may be incompatible.. (which is probably why my predecessor configured our clients to use PPTP). Is your work VPN configured with NAT-T? (ask) If not, you will find that it will not work. 

EDIT: Now its back: (And you can see there are 4 drives in there, 1 hot-spare, didn't notice Drive0 was missing before) 

This has a lot of topical information: $URL$ It seems due to the way IIS uses processes, its a bit hard to tell which "site" is actually using the resources (as they are matched to a process, not a thread within a process), unless you match the log entries with perf logs.. and that could be a bit of a ***.. annoyance. Depends on your process pool settings I suppose.. you might be able to use "Process Explorer from Sysinternals" to determine which site is using which process, or processes etc, it handily provides graphs of individual process usage, and its free. (I like being able to pause threads within a process.. damn handy that!) $URL$ 

Lot of people remove compilers, and compiler tools because they can theoretically be used for exploits. I think that's dated. My experience lately has been that, if they can get in, they can get their tools in as well, so removing things like compilers, and emacs, and junk like that doesn't add much to your overall security. I'd be extremely careful what services I ran on the machine: everything that connects to the outside is a potential vulnerability. Otherwise, I'd only remove the compilers to keep my programmers honest. I don't think it adds much to security these days. 

IMHO, it's a little odd to change your log file by SIZE rather than by date. Most system logs (in unix or linux) rotate on a weekly or monthly basis, and not based on size...This is something I like for various reasons, and also something which, if implemented, would solve your problem. Eight years later, I don't know what the hell I was talking about here: there are tons of places where you want to rotate by size, because daily/weekly/monthly rotations can yield MASSIVE files which can cause serious issues. From a more experienced perspective, the real question is why you'd want to sit and continuously tail a file that's growing so fast that you're rotating it more than daily...It'd be like watching the Matrix stream by. These days you'd be better looking into some big data log aggregation like Splunk or Sumologic, where it can filter log events into classes and trigger based on specific log values...No need for watching live logs at all. 

Apparently can be done with ICAP Server: $URL$ The client is in squid3: $URL$ Config from above: (squid.conf) 

How much work would it be to set it up again? How much effort/cost is involved in backing it up? If the work to set it up is going to cost less than the cost of backing it up, you have an answer. I just take a ghost image of ours once a quarter.. a few months of patches is small potatoes in the scheme of things.. but I also force it to use the proxy anyway, so in the event it does die, I can still load most of them locally through that.. (our proxy has a LARGE cache, 100GB) Cheers for reminding me, I'm adding a scheduled task so I don't forget next quarter and am backing it up now! 

If complicated "Hairpin_NAT" isn't your scene, solution for the lazy: simply add a static DNS entry in the MT device that points to the local server.. sorted. All local requests get correctly resolved, bypassing the router, all external stuff ignores your DNS entry so goes the dstnat route. 

You need to be more specific about what you need it to do. ICT means very different things for different types of institutions (Cloud, however, means nothing in the way you've used it: "intranet Cloud" is like saying "internal Internet". Cloud services are, by definition, not hosted locally. If it's local it's a "cluster".) Just in a very general case, I'd suggest going to Sourceforge, and looking under "Education". There is an "Administration" section that includes a number of OSS projects that will probably do some of the things you need done. From personal experience, you're going to find that they're not really supported, and that they're pretty customized. Still, it's a lot better than starting from scratch, and you may get lucky and find that there is a package that is being actively maintained (some kind of yearly class project? Who knows.) 

That's your "core" router. that's the machine all the machines look to to figure out where machines on other subnets are. That's where you need to add a route pointing to the B machine as the gateway router for the 192.168.0.0/24 subnet. If you can give me some information on your router, I can tell you how to add that route. 

POP3 is a protocol, Exchange is an MTA, which can deliver mail using many protocols, it defaults to MAPI for user mail retrieval, but you can enable IMAP and POP3 (and even X.400) And it includes a webmail interface. (Which allows mobiles/iphones & remote users to access email). The webmail interface (when configured) allows users with broken Outlook to stay productive anywhere in the world.. ;-) 

Then configure the server to determine whether to modify the header or not based on destination domain: 

You only require one. Special? Active Directory is REQUIRED, as is a decent server with gobs of RAM and adequate backups. IIS is required for the web interface, as is SSL (so, either an external certificate or an internal CA) If its behind a firewall, you should only need to expose certain functionality, like outbound SMTP from the exchange server, and inbound SMTP to the server, then if external users require access, I would make them use a VPN, or use HTTPS (Outlook can use RPC over HTTPS). To secure it more, pipe your messages through a filtering service, like MessageLabs. 

If you read the /etc/tripwire/twcfg.txt file, you might be forgiven for thinking you can just add your emailserver's details there.. however that doesn't work. One quick final step, might want to add your email address as an alias for root, so you get roots email! 

Well, Powershell has $?, so I'm assuming the problem is that it only populates on exit? When you're dealing with errors in running code, the best practice is to use a Try/Catch block. Code inside a "Try" block will automatically fail over to "Catch" in the event of problems, and, even better, you can have different catch blocks for different exceptions, so you can handle different problems on the fly. Very cool. 

The 0.0.0.0 block is reserved for the "default route", for routing purposes. Inversely 1.1.1.1 and 255.255.255.255 is used to signify broadcasting on all routes. The 169.254.0.0 -169.254.255.255 block is reserved for Automatic Private IP Addressing There are a number of other addresses that are reserved for INCAN, ARIN, etc. A partial list can be found here. 

I don't know. My gut says wipe it, and pretend it never existed. If they know you had it, they may make a stink over it, regardless of how often you say they have the only copy. As a freelancer you need to keep a very strict separation between your systems and their systems, especially where things like backups are concerned. In the future, make sure they know what you're going to do with their data before you actually touch any of it. Tell them your retention policy, and when your contract is up, dump it on 'em and delete the traces. Write it up, and make 'em sign it. You can buy cheap boilerplate NDA forms at various places online. It's worth it, and it tends to calm down the people you're working with. 

You could write something similar using vbs/batch for windows. Then, use a neat little program: squid-report.pl (davehope.co.uk) to monitor the day's or week's logs, find the most used sites, or largest downloads etc, add the ones you don't want to the block list. Doesn't take much effort. Eventually (with custom error messages per acl, squid is great for that, I have a link that sends an email to me if you click it, so users can report errors with one click) you can train your staff not to visit porn/game sites at work. And, blocking ad's saves so much bandwidth and increases response! I find sqstat handy for "Realtime" monitoring.. $URL$ Great for finding out who is listening to internet radio and wasting bandwidth! Then, for your "Reports", you can use SARG: $URL$ Fantastic program, shows When/Who/What/Where the users were doing online.. ;-) 

Process explorer might be better, allows you to identify which module is in error, where its stuck in the stack, memory/net/etc.. (doesn't stream past like a log-viewer, more like task-manager with better options!) $URL$ MS kb/555021 suggests your profile might be corrupt, try creating a new one. 

Got that from this thread in a Drupal forum. Ymmv. I'm going home now, since clearly my brain is shot for the day. Also, make sure you allow for override, or the webserver will ignore your .htaccess file. 

There is no magical number of servers that encompasses "high availablity." If your needs are modest, you can do it with a single box, though (obviously) as soon as you have to reboot it, there go your 4 9's. As long as your site isn't being heavily utilized, I'd say that two (decent) machines with a mirrored database set up (which is supported by MSSQL web, as you know), and NLB should be fine. Again though, this is a very general question. I run about 50 HUGE sites off of two "servers" but the servers are big honking Sun blade servers. My worry wouldn't be about the machines (I'm assuming you'd be running both IIS and MSSQL on each machine, because there is no other way you could go about attempting high availablity otherwise) but about what you have between them and the internet. Having IIS exposed to the internet isn't the problem it once was, but IIS and MSSQL? I wouldn't do it if I had to be PCI or HIPPA compliant. @Tom: This is your standard "defense in depth" build. Almost everyone will suggest separation of web and database servers, and on top of that, you should put heavy restrictions between them as well...Ideally there should be nothing on your webserver that you can't just restore from backup...Your code is all mirrored on your development machine, so just dump it back up to the website and you're fine. But database servers change all the time, and your setup needs to reflect that. If you're restricted to two machines, you're going to have to install web and database on both, and do your best to harden them. I'd completely lock the databases down so that they can only be accessed by each other and the local webserver. NLB takes the place of a hardware load balancer, and should automatically failover. Obviously YMMV: the hardware solutions have a lot to offer, but the price point is much higher. If you're not expecting a huge amount of traffic, NLB should be okay, but the hardware load balancers tend to have superiour protection against things like DDoS attacks. Again, it's all about what you're going to need. @Tom: 1 IP is fine. NLB uses a "virtual" ip address that resolves to all the local machines. When you turn on NLB, go to the connection properties, and you'll see a "Network Load Balancing" tab. That tab needs the address that applies to the whole cluster (your static ip). Then you go in to the regular TCP/IP tab, and set up the local ip, which is the address that is specific to the individual machine. 

How did you configure the basic auth? If you are using Directives, try changing to instead. (as the proxied requests don't actually touch an Apache directory.. Edit: I assume you have: 

You can use GPO's in a windows domain to configure proxy settings. Also, use the PAC suggestion, but with DNS autoconfig, setup an A record for WPAD.mydomain, pointing to a local webserver that contains a file called wpad.dat (a copy of proxy.pac) in the web root. Any browser configured with "Automatically configure my proxy settings" or "automatic" or whatever will find this and work. Then only allow your proxy at the gateway. Works a charm! (then laptop users can still connect at home/o'seas etc) It is also a DHCP option. More info: $URL$ (needed for mime settings) 

According to this site: $URL$ PEAR should already be installed, just shy. A few vhosts config changes and you're off. Unless PHP has been compiled without PEAR, then no, its probably never going to work unless you reinstall it, or get your hosting provider to do so. 

Which "previous question" would that be? I assume when you connect your 3G device, it automatically overrides your default gateway, which you can check by using the command line and typing: 

If by "unformat" you mean, "reformat as NTFS" the answer is yes. If by "unformat" you mean "revert it to the NTFS it was formatted as before I reformatted it" the answer is no. Formatting a disk completely destroys the partition information on the previous disk. It's possible that you could recover something with some partition recovery software (a la Partition Magic) but not certain. 

Aside from removing or disabling the default.aspx file? Not sure there is a way. Rendering the HTML as opposed to the index is the designed behaviour. 

That's pretty irritating for a number of reasons. You should never just automatically apply every update that comes to you, but if you ARE going to do it, you should set up a scheduled time to apply them and reboot...Leaving that to the user is absurd in a production environment. To set up a time, go to Control Panel->System->Automatic Updates(tab) and do "Automatic" and put in a time (defaults to 3:00am). To do it properly, set up WSUS, so you can deploy all updates more efficiently across the domain. 

Check the umask for user Proftpd and group nogroup. Since you're trying to set permissions higher than the system default, they may be restricted by their OWN umask values. A way to check it might be to change the umask in proftpd.conf to 777; if new files show up as 000, then you know that configuration line is working. 

Spot on!, $URL$ You just need to specify if you want to audit the Successful deletion, or the failure to delete, or both. Both shows lots of event logs, but is useful tracking down malicious users, or buggy utilities. Please note: Before setting up auditing for files and folders, you must enable object access auditing by defining auditing policy settings for the object access event category. If you do not enable object access auditing, you will receive an error message when you set up auditing for files and folders, and no files or folders will be audited. 

.NET version: $URL$ With the source: dotnetslackers.com/community/blogs/basharkokash/archive/2008/03/15/USB-Detection-source-code.aspx Relevant section: 

Send a message to support@messagelabs.com, you should include the email address of your recipient, maybe they will work with you, don't know. It will probably help them if you include the complete original message, (extract the .eml file and zip it) 

Is on by default, and is designed to prevent dynamic pages from being cached. You can create an acl like the following: 

If you have configured it for IMAP, you will have to specify which folders get synchronised, so its possible you have folders that are not visible.. HTTPS is the best for Outlook. If you are comfortable (have the bandwidth) to download your entire mailbox again, simply delete your profile (control panel -> mail -> profiles) and start outlook again, it will revert to virgin outlook, and you will have to configure it, however, when you get to the "Exchange Server Settings" part, type in the ACTUAL internal dns name of the server, select the cached mode again, enter your username and then press More Settings.