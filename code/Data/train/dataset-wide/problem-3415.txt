When creating an autoscaling group I can choose an ordered list of termination policies for its instances. Amazon's documentation states that 

Basically what it ways on the tin, how can I create individual per-instance alarms inside an auto-scaling group created with a CloudFormation template? I can reference the ASG itself in an alarm and create ASG-level alarms, but cannot seem to specify dimensions to be "any EC2 instance belonging to this ASG". Is it possible or is my only option user-data script? 

What would be the best way to pass sensitive data to EC2 instance (on boot or otherwise) that only root can access? 

I cannot use UserData, because anyone can read it. I cannot use private S3 buckets for the same reason (metadata and hence credentials can be accessed by anyone on the box). I'd strongly prefer not to bake my own AMI, as it's quite a hassle. 

It is based on an Allow rather than a Deny, and uses positive string matching rather than negative string matching. Is there any reason you want to use a Deny? Using a Deny, you are saying: If the user tries to create a volume, deny the action if (1) the tags "empname" and "team" are not included AND (2) if any tags specified are either "empname" or "team". When you specify random tags, the first condition is satisfied, but the second one is not, so the Deny will not occur. I think it would be much more logical and less confusing if you followed the AWS example and used Allow and positive string matching. Otherwise, try changing the second condition to: 

Another proposed solution here: $URL$ Use a generic certificate for all agents connecting to the puppet master 

For some reason the CPU usage is at 20% CPU while I'm doing absolutely nothing, exactly every 10 minutes spiking to 28-30%. I thought there was something wrong with the instance, so I've re-created it, same thing. What does this? Is this an RDS phenomenon in general or is this specific to the burst capable instance classes? 

I know I can use IAM roles, but I found it's just too many moving parts, and complicates any scripts that have to use the access key/secret key (e.g. rewriting /etc/apt/* lines when it changes). Not to mention there is no way to attach roles to existing instances, which makes it even more pain. It's also not possible to simply restrict access by using VPC subnet, because S3 bucket access goes via public EC2 interface. 

I am trying to use AWS autoscaling lifecycle hooks in a template that encapsulates the following things: 

To /root/.hgrc When mercurial is executed, its looks in $HOME/.hgrc for trust relationships. On my existing server, the puppet agent was being executed with cron, so cron would have seen $HOME as /root/.hgrc On the cloned server, I was running the puppet update interactively, having opened a root shell using 

This will revoke the agent cert on the master, remove it, remove the cert on the agent and re-generate a new one. Save this file locally and then pass it to the Amazon EC2 API when launching an new instance 

This is logged 32 times during the 16 minute outage I am trying to figure out what is going on here. My concern is that due to deficiencies in my balancer config, that the balancer is repeatedly trying to send requests to the rebooting server (and therefore returning errors to the user). Why is Apache repeatedly telling me that it is "disabling worker"? Does the balancer module intermittently send user requests to the failing node, to try and determine if it is back up, or does it have its own internal health checking mechanism that is invisible to the user? 

As I understand it, an instance needs to be granted access to resources in order to do anything with CloudFormation. But when I run this on a Beanstalk web server instance: 

But it's not ideal, as it doesn't handle restarts and remote endpoint downtime very well, because it doesn't have anything like 's pooling, so I'll get duplicate logs and/or drop logs. Given that CoreOS has no package management, is there a conventional way to solve this painlessly? 

I want to get an automated process for AMI creation going, and one piece remaining is automatically cleaning up the instance after image creation. The instance is booted with a user-data script that does the necessary setup, then kicks off image creation from self using AWS CLI. Then it shuts down. I could go with option and wait there until the image is ready, then terminate, but the docs state that "file system integrity on the created image can't be guaranteed", so I want to avoid using it. What's the best way to kill the instance from itself after image creation is completed? 

You apply this file on each of the 3 nodes at /etc/rabbitmq/rabbitmq.config, and then start rabbitmq on each and a cluster will apparently form. This doesn't appear to work too well eg If you start rabbit2, and rabbit3 hasn't already come up, the service will not start on rabbit2, as it is trying to create a cluster with rabbit3. Equally, if you only apply the config on rabbit1, and have pre-started rabbit2 and rabbit3, rabbit1 will form a cluster with rabbit2 only, as, according to the documentation (I don't understand why): 

I'm trying to centralise logging in an environment that using multiple application technologies (Java, Rails and various DBs). We want to developers to bring up stacks with Docker Compose, but we want to them to refer to a central log source (ELK) to debug issues, rather than trying to open shells into running Docker containers. The applications all write to the file system rather than to STDOUT/STDERR, which removes all of the options associated with the Docker logging driver, and logspout too. What we have done is configure the containers to have rsyslog include the application log files and forward those to logstash which has a syslog input. This works in terms of moving the logs from A to B, but managing multi-technology logs in ELK based on the syslog input is horrible (eg trying to capture multine Java stacktraces, or MySQL slow queries). Is there a better way to do this? Should I be running logstash in each container, so that I can apply filters and codecs directly to the log files, so that I don't have to rely on the syslog input? Of is there some way to use the Docker logging driver with application log files that are written to the file system? 

I want to aggregate CoreOS logs to Papertrail service, which basically provides a syslog endpoint for aggregate logging. Common advice for this setup seems to be starting a service that does something like this: 

Within plain EC2 environment, managing access to other AWS resources is fairly straightforward with IAM roles and credentials (automatically fetched from instance metadata). Even easier with CloudFormation, where you can create roles on the fly when you assign a particular application role to an instance. If I wanted to migrate to Docker and have a kind of M-to-N deployment, where I have M machines, and N applications running on it, how should I go about restricting access to per-application AWS resources? Instance metadata is accessible by anyone on the host, so I'd have every application being able to see/modify data of every other application in the same deployment environment. What are the best practices for supplying security credentials to application containers running in such environment? 

Your terminology isn't very accurate so its hard to tell what you are trying to do. Cloudformation creates "stacks". You can create a stack that contains an RDS instance. If you want to update a stack, you can upload an updated JSON template to that stack. The only changes that will be applied to the stack are the changes from the original JSON template that was used to create the stack. So, if you have an RDS instance in the stack, and your updated template does not include any changes to that RDS instance, applying an updated template (which changes other parts of your stack) will not impact your RDS instance. If your updated template contains changes to your RDS instance, then, depending on what those changes are, your existing RDS instance could be deleted. I would advise testing on a dummy stack first. 

How can I tell (in ) if I'm running in interactive mode, or, say, executing a command over ssh. I want to avoid printing of ANSI escape sequences in if it's the latter. 

It's single availability zone no backups not in a security group that's reachable from the outside world 

But it glosses over the specifics of how these policies are combined and when the "fall through" happens to the next policy in the list, i.e. under what conditions each policy fails and moves on to the next policy in the list. For example, I have a policy list in my group and yet after scaling up and then down, the scaling group proceeded to terminate by newest (and healthy) instance (newer by a large margin), and I can't figure out why. Additionally, according to the same doc, default policy is actually itself a combination of policies, and includes and as two of its steps. If I have a list that includes , does it evaluate and twice? Lastly, does the termination consider load balancer? For example, if my new instance failed to initialise properly and is not in-service with the load balancer, and is in effect, will scale-down action kill the unhealthy instance first even though it's newer? 

The config/config.yml file exists, and had the necessary config to match the RAILS_ENV variable that is being passed to it. My suspicion is that the Docker container is having difficultly reading a file into memory, perhaps due to the way bundle install is being run as root, and is not creating the necessary CONFIG object for Rails to read. I am not a Ruby developer (I'm Operations), so please forgive any absence of basic Ruby knowledge. 

When I change the config file to use "disc" instead of "ram", cluster creation was a lot more stable. 

Boto has a function, update_environment, that allows the user to update options in an AWS ElasticBeanstalk environment. Using the AWS CLI, this would typically be actioned as follows: 

Not sure if your understanding of how Skype works is correct. Skype calls aren't initiated and terminated between users in the same way as a normal TCP conversation would take place. Each Skype client registers its location with Skype's central servers. When one user calls another user, the initiating client asks where the target client is, and the initiating client then starts streaming UDP (or relayed UDP, or sometimes even TCP) to the terminating client. However, because all of this has to involve an independent public Skype host, all of this communication will happen over the public internet, rather than over your VPN. To get Skype to flow over a VPN is very difficult, because once Skype can find a way to the Internet, it will always flow that way, regardless of whether you configure it to use a proxy. You would need to source route Skype traffic on your network device so that it is blocked from accessing the Internet, and then proxy it to a server on the other side of your VPN, and vice versa, which clearly is a whole lotta pain. Bottom line is that Skype probably is never going to use your VPN. Most likely you performance issue is cause by Skype not being able to punch a whole in your firewall to do pure UDP, and reverts to Relayed UDP or TCP. One quick win is to allow all UDP in and out of your firewall, but that still doesn't guarantee pure UDP. That will depend on how congested your Port Address Translation is. $URL$