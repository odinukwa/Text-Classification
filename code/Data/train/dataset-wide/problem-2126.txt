Everything I am talking about relate to relational database, specific MySQL. I have a number of tables in a database and for a moderate number of them, I am going to want to store a history of the records values when it changes. I have seen this done in a couple of different ways: 

One Table/One Field - Basically there is one table that store the history of all the table that need history storage. All change are recorded in one field as a text data type. Table Per Table/One Field - Same as above except the each table has its own history table (ie. Projects/ProjectsHistory, Issues/IssuesHistory, etc...). Table Per Table/Field Per Field - This is like the above in the each table has it own histroy table but also the history table has pretty much the same definition as the regular table with an additional of additional history related fields (updateDatetime, updateUserId, etc...). 

I've never attempted to define roles and permissions in PG. Please bear with me here as this question has a few parts to it. This is my setup: 

I defined as 16 and therefore created 16 indexes. I also created an index that contained all rows (ignoring ). In my tests, I get an almost 13 times speed-up by having 16 indexes vs 1. Great! However, this is test data (1 million random entries), which is somewhat meaningless. Nonetheless, it proves smaller indexes provide tremendous speed-up. Close to linear in fact. My question is, is there a way to calculate an optimum number of partial indexes without going through a tedious trial-and-error process? I have searched, but can't find any best practices regarding the number of partial indexes. If there is no "formula" to determine the answer, can anyone share their experiences with multiple partial indexes? 

The results from these views do not overlap and together cover 100% of the table. When I select a union of all of them, and separately filter each view, the query takes about 9 ms. Selecting a union of all views and filtering the result of that takes about 500 ms. I have also tested this without views, inlining the queries they contain, which did not produce a measurable improvement. This is the fast query (explain): 

(edit: see end for a simpler example) I'm searching in a table named "cases" (135k rows, 29 columns). Some of the rows in this table have a type of parent-child relationship (of different types), which means that for these records a mix of parent/child fields must be used for filtering and display. I have identified four different parent-child relationships and created views for them: 

Am I doing anything non-optimum here? Are there any potential security threats with this approach? Is the DDL correct as per what I want to achieve? For example, I donâ€™t address triggers. 

I have a DB with multiple schemas and multiple roles where each role corresponds to a schema (I'm simulating SQL Server's ability to cross-reference multiple DBs where each is its own area of concern). There is an additional schema called 'core' that has no corresponding role. It contains common functions, C-function wrappers, and custom types, domains, and enumerations, etc. The 'public' schema contains only functions and, so far, nothing else. 

Each role has its own login and has access to its corresponding schema. Roles may have access to certain other roles (simple ). All objects within a schema are owned by the schema (except for the schema 'core' where the owner is 'postgres'). Any given role can do the following only: SELECT from relationships and EXECUTE functions. Obviously, access to sequences, types, triggers etc. is required. All schemas have access to 'core' and 'public' (since types, functions, etc. are therein). Certain relationships will be individually set to have specific priveledges (such as no ). 

I am build a web application (project management system) and I have been wondering about this when it come to performance. I have an Issues table an inside it there are 12 foreign keys linking to various other tables. of those, 8 of them I would need to join to get the title field from the other tables in order for the record to make any sense in a web application but then means doing 8 joins which seems really excessive especially since I am only pulling in 1 field for each of those joins. Now I have also been told to use a auto incrementing primary key (unless sharding is a concerns in which case I should use a GUID) for permanence reasons but how bad is it to use a varchar (max length 32) performance wise? I mean most of these table are probably not going to have at many records (most of them should be under 20). Also if I use the title as the primary key, I won't have to do joins 95% of the time so for 95% of the sql, I would even occur any performance hit (I think). The only downside I can think of is that I have is I will have higher disk space usage (but down a day is that really a big deal). The reason I am use lookup tables for a lot of this stuff instead of enums is because I need all of these values to be configurable by the end user through the application itself. What are the downsides of using a varchar as the primary key for a table not excepted to have many records? UPDATE - Some Tests So I decided to do some basic tests on this stuff. I have 100000 records and these are the base queries: Base VARCHAR FK Query 

In my experience, a recursive hierarchy is the most practical way of tackling this. It offers the following advantages: 

In order to try and solve this, I have rebooted PG, the machine, and vacuumed the DB. I believe there is an error in the CREATE OPERATOR code. If I can index an array of custom type of (int, int4range), that would be even better. I've spent quite some time (a full day) wading through documentation, forums, etc., but can find nothing that really helps me to understand how to solve this (i.e. create a working custom operator class). 

We are about to set up SQL Servers in different countries. We need to link them, but we don't have to have a direct link (as in linked server). In other words, they can be loosely coupled. Is it better to connect them via a VPN and have them as linked servers, or use loose coupling via web services? By "better", I am referring to stability. 

As you can see, the join and filter is duplicated for each view. Trying to avoid the duplication yielded this query, which takes a lot longer (explain): 

and got ERROR: there is no unique constraint matching given keys for referenced table "foo". I can easily add this constraint, but I don't understand why it is necessary, when one of the referenced columns () is already guaranteed to be unique? The way I see it, the new constraint would be redundant. 

With the added requirement that one of the other field values () must be identical between parent and child records, I thought a composite foreign key should do the trick. I changed the last line to 

This query (query plan) returns the correct result, but does not use the trigram index. It takes around 330ms. Removing either of the match conditions (query plan), or having them point at the same table (query plan), removes the performance problem. Both of these use the trigram index and are executed in under 1ms, but do not solve the given task. How can I get PostgreSQL to use my index? I have simplified this example to the minimum necessary to demonstrate the effect. The actual query is much more complex (and partially auto-generated), so using a UNION of two queries with only one text match each would be very hard, if it's even possible. I'm using PostgreSQL 9.5.5. The schema is still open for modifications (to some degree). 

Now I don't know what configuration I could make to make one or the other (or both) faster but it seems like the VARCHAR FK see faster in queries for data (sometimes a lot faster). I guess I have to choice whether that speed improvement is worth the extra data/index size. 

Lets say I have a number of tables that will have a type and status and I want the types/statues to be configurable by a end-user of the application (so I can't use enums here). There are 2 different ways I am think about doing this: 

Have Types and Statuses tables for each table that needs them. Have one Types and Statuses table that every table that needs them uses 

What are some of the advantages and disadvantages to these different methods of storing record history? Are there other methods that I have not thought of?