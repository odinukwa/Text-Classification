Alfred Marshall expresses a similar view with regard to mathematical language being a more efficient tool than words. He says: 

However, Marshall provides a more muted opinion about mathematics being a necessary output of economic modelling: 

The question about using mathematics to model economic phenomena is a rather old one dating back to at least the marginal revolution (19th Century). A couple of the key figures of this era were William Stanley Jevons and Leon Walras. I don't aim to provide a complete, or terribly accurate, history, but let's just focus on Walras. In the preface to his Elements of Pure Economics, Walras paints a picture that suggests mathematics is a necessary input (basic ingredient) of economic modelling. He says: 

Let's begin by fleshing out what the EViews code is saying. The EViews code says that the variable CLOSE depends on its first difference and there is also an ARMA(3,3) error term included in the model. Supposedly, the ARMA error process is to whiten the residuals, which can be, and has been, argued against as a modelling strategy. It's also worth noting that there is no constant term in the equation. The reference to BACKCAST indicates how the MA process is initialized - the details of which can be found in the EViews documentation. ESTSMPL typically refers to the estimation sample. Having understood the EViews code, the question is now: how does one estimate a linear regression model with ARMA errors using MS-Excel? The easiest solution would be to find an existing add-in that provides this feature. An alternative method would be to write the code yourself using the MS-Excel programming language (VBA). Beware though, what makes this application tricky is the MA term; AR processes are easier to program and more common in certain fields, e.g. VAR models appear more frequently in economics than VARMA models. Another approach, if you are willing to delve into some code, would be to look at some R source code (either base code or from some time-series package) to get a sense of the task ahead if you do decide to program from scratch. Programming this from scratch seems like overkill, though, as this sort of estimation has been around for decades. Why bother if all you're interested in are the estimated parameters and not the underlying routines? It doesn't answer your question, but my advice would be to use R if EViews is not available to you. Or, seek out an MS-Excel add-in that will do the job for you. A useful thread from the EViews forum is here: $URL$ Note: "Estimation, and forecasting of MA processes is complicated, especially when backcasting is used to obtain starting observations for the error terms." Good luck! 

The point is concluded pointing out that too much detail (i.e. advanced mathematics) can distract from what's really important in the wider context: 

Can anyone provide references for research that focuses on coordination between groups of decision makers rather than within group coordination with individual decision makers? \ Edits given the questions posted in comments: 1 - I am interested in behavioral/experimental work. 2 - Coordination/Cooperation between groups within any context. So...could be groups as groups, groups as nations, etc. 

No Free Lunch : Let Y be the production set and $y \in Y$ denote an element of the production set Y. If $y\geq 0$ then $y=0$. Recall that for all elements $y \in Y$ we have that y comprises inputs and outputs. The inputs should be negative values anytime there is a positive output. That is, you can never produce something from nothing. To say y is greater than or equal to zero means y has no non-negative elements. To rephrase - y has nothing acting as an input. So, it must be true that nothing in y can be strictly positive. To rephrase - we can't have produced a positive amount of something if we don't have any inputs with which to produce outputs. Production functions satisfying this property should pass through the origin. 

I think the interpretation here is one of correlation if we assume second-order stationarity. That is, the coefficient in your example is simply the correlation between a contemporaneous value of your dependent variable and its one-period lag. 

I think there might be an issue with this as currently constructed: $\delta(p)= \infty$, $\forall P \in \mathbb{R_+}$ and $P_e = \infty =\dot{p},$ $\forall P \in \mathbb{R_+}$ This is because $\delta_0 = \infty $. Further, $\sigma(p) = S_\sigma p+\sigma_0= S_\sigma p$ since $ \sigma_0 = 0$. In words - there should be infinite demand for any normal good whenever P is 0 and there should be zero supply of any good whenever P is 0. That said, your intuition here is solid but I do think you're working on reinventing the tatonement wheel. Tatonement, in a nutshell, adjusts prices up when there is excess demand and prices down when there is excess supply. If you are able to do this after watching a few lectures online, then I say keep going. You could make meaningful contributions once you have a better understanding of the field. edit: If you'd like to message me privately, I would be more than happy to pass along my graduate school notes/materials to help guide your studies. At minimum, I recommend you followup your current study with some more advanced micro theory courses. 

I intend this as only a comment - it is just too long for the comment section: As it stands, I think you answer your own question. Lucas quantifies the impact of recessionary cycles by calculating how much an agent with a smooth consumption path would need to be compensated in terms of average consumption to instead endure a more volatile consumption path. He expressed the result as a percentage of average annual consumption. Remember, in economics more volatility usually implies more risk. And so what Lucas really did was create a function involving an agent's level of risk aversion that answered the question "how much do I need to compensate this risk averse person to endure 'this much' more risk?". As for your ending bit - you're correct. The idea you're getting at is called stochastic discounting. Essentially - I will pay more to ensure myself against a negative outcome whenever I believe that negative outcome will occur with a high probability relative to when I believe it will occur with a low probability. 

Question Are there any simulated datasets that have been designed specifically to represent macroeconomic data? In particular, are there any such datasets that can be used in benchmark studies? Background To give an analogy, in optimization, one may be interested in assessing the quality of a given algorithm. To do this, there are a number of test functions for optimization, such as the well-known banana function, that can be used to evaluate the performance of an algorithm; for example, by analysing if and how fast the algorithm can find a global minimum. I am interested in working in this controlled experimental setting using simulated data that has a macroeconomic interpretation. With data simulated from a known data generating process (this would be my control), it would be possible to investigate, say, the performance of algorithms designed to detect structural breaks and to assess techniques/models used for forecasting. I would greatly appreciate if someone could help: either by pointing me to some simulated datasets that can be used for the above purposes or by providing some guidance on how to simulate data that carries a macroeconomic interpretation. It's possible for me to simulate a variety of ARMA processes (or VARMA model), but I am really interested in something that goes beyond that; the simulated data ought to have similar properties to observed macroeconomic data. Obviously, I am trying to avoid using actual data because my control (of knowing the data generating process) would be lost. Update For one of the purposes I had in mind, a quick read of Castle, Doornik, and Hendry (2013) suggests that it is, perhaps, not that difficult. Their "experimental design" is based on the following equations $$ y_{t} = \beta_{0} + \gamma y_{t-1} + \beta_{1}x_{1,t} + \cdots + \beta_{10} x_{10,t} + \epsilon_{t}\\ x_{i,t} = \rho x_{i,t-1} + v_{i,t}, v_{i,t} \sim IN[0,1], i=1,\ldots,10,\\ \epsilon_{t} \sim IN[0,1], t=1,\ldots ,10. $$ along with some further qualifications. So, it would appear that for one of my examples (the case of evaluating structural break algorithms), relatively simple (although by no means, "not tricky") DGPs are all that's required. Reference: Castle, Doornik, and Hendry (2013) Model Selection when there are Multiple Breaks 

I think the main alternative to the Johansen (statistical) approach is the methodology propounded by Pesaran and Shin in their so-called long-run structural modelling (economic) approach. The main formal reference is Pesaran and Shin (2002). The methodology is also presented to a wider audience in Garratt et al. (2012). Although referred to as the long-run structural modelling approach, you will also read about the type of models associated with the methodology, which are called VARX* models. The distinguishing feature is that the cointegrated models are estimated using reduced-rank regression and (over-)identifying restrictions are derived from economic a priori then tested. The sequencing is slightly different to Johansen since economic theory is given priority. In the same school of thought is the cointegrated VAR approach associated with Juselius (2006). Again, Juselius propounds an economic approach to cointegration as opposed to the statistical approach by Johansen (and others like Phillips). Pesaran's approach has been programmed and is available in the Microfit software. Juselius' approach has also been programmed and is available in the RATS (CATS) software. To my understanding, you can find MATLAB code in the GVAR toolbox, and this should give an indication of what's required for the Pesaran approach. In terms of programming, I haven't seen much difference when compared to Johansen (although I just quickly investigated this). Note that the GVAR approach can be thought of as an extension to the VARX approach (the difference lies in stacking individual country models, but estimation of the individual models is the same). References Pesaran, M and Shin, Yongcheol, (2002), LONG-RUN STRUCTURAL MODELLING, Econometric Reviews, 21, issue 1, p. 49-87. Garratt, Anthony, Lee, Kevin, Pesaran, M and Shin, Yongcheol, (2012), Global and National Macroeconometric Modelling: A Long-Run Structural Approach, Oxford University Press. Juselius, Katarina (2006) The Cointegrated VAR Model: Methodology and Applications (Advanced Texts in Econometrics). GVAR Toolbox 

For the given answer, part 1a is saying that, since the company cannot earn interest on investments, it is better to just take the \$12k. However, one ought to know something about how this company discounts to actually answer this question. That is, one should make this comparison by comparing \$8,525 to the discounted present value of the \$12k, taking into account that the \$12k is received in payments spread uniformly over 12 years. The real answer to this question as written is, ' IT DEPENDS'. Part 1b is saying that $8,525(1+.07)^{12} \equiv 19,199.93 > 12,000$ and so the company should take the lump-sum payment and invest it. HOWEVER, this is again the wrong way to answer this question. Instead, one should compare $8,525(1+.07)^{12}$ with the money the firm will have if it chooses the \$12k and subsequently invests each annual payment. This value is \$19,140.64 (link to calculate). The answer here is correct but for the wrong reason. 

Sources that will allow you to gain more depth: excess reserves discount window/discount rate federal funds rate open market operations 

This is a very poorly written question. The gist of the question is this: should a company take a lump-sum payment from a customer of \$8,525 or a \$12k payment that is spread uniformly over 12 years. The thing the question wants you to consider between 1a and 1b is that in 1a the company cannot earn interest (which the answer just assumes is compounding) and in 1b the company can earn 7% interest. 

Given a utility function of the form $U(a,b)=min\{a,b\}$. Suppose that currently $a<b$. To increase utility, you should allocate more of $a$ to this person until $a=b$ and then increase $a,b$ proportionally. 

My partner and I are both undergrads. trying to complete an extracurricular research project and need to update parameters for our model. Our advisor is out of country and we have minimal/spotty contact with him. We have constructed our own model and found most information (we are reproducing his work and adding a few new things) on our own but we aren't sure how to calculate. Here are the things we are looking for: Everything below is for use in a limited participation model: Beta Value for a discounted utility model - The current paper uses a beta value of .988. I know what this is and why we use it but I am not sure how to calculate this number. My assumption is that this is 1-real interest rate. Can anyone confirm this? I know his numbers are a bit outdated. Capital depreciation rates - Again, I know what this is but am unsure of how to calculate an accurate value. Current model uses .025. adjustment cost on capital - This is given as .01. I assume this is the cost associated with adjusting capital stock etc. Again...not sure how this was calculated. standard deviation of technology shock - This is given as .0081. no idea how to calculate this. Autocorrelation of technology shock - given as .95. No idea how to calculate this. These are the few things we are not sure how to calculate. We've read through his other papers also and found that he cites other work for the values he uses but are unable to find that paper to determine how that author calculated the values. Does anyone here have any good ideas about how we might calculate these values so we can calibrate this model to the American economy? Any help, suggestions, hints, tips, literature suggestions etc. are very very very much appreciated.