Looks like some sort of heart-beat detector to see if the database is still online. Certainly it's going to return junk data since it has a TOP without an ORDER BY. As far as not seeing it, I don't know that I'd suggest eliminating this. You want to know if something is being called every 15 seconds, especially since that query is probably causing scans on the data. However, if you really want to, just add a filter on the TEXT column and include that text in the filter. 

I would strongly recommend that you treat your database basically the same way as you treat your application code. You can script your database out to it's component parts and check those into source control and then use the same labels & versions there that you use for your apps. To get the objects into source control there are a number of tools you can use. Microsoft has a tool that is nicknamed Data Dude. It works with Visual Studio. They're also preparing to release a new tool called SQL Server Database Tools (SSDT), again, working with Visual Studio. My company, Red Gate Software, makes a tool that works with SSMS called SQL Source Control. In terms of process, I wrote several chapters for the book Red Gate Guide to Team Development. It's available as a free download (or if you want to kill a tree you can purcahse one from Amazon). I go into a lot more details about working with databases in development teams there. 

That will return information not just about when the stats where updated, but their size, density, how selective they are, and the histogram that shows the distribution of data. With all that, you can determine if those stats are up to date and effective. 

Two things, first, what does the execution plan look like on this? Are you seeing efficient use of the existing indexes and structure? If so, then that's likely to continue into the future. If it's problematic now, it's just going to get worse. So, you might want to look at mechanisms for querying the data like those I outline in this article. By and large, at least through my tests, using ROW_NUMBER to get that latest value is likely to work better for you. Also, you may not want to use a view. I know that it makes code re-use possible, but sometimes in T-SQL we just have to write the same query 100 (or more) times because the engine deals with it better. Test with & without a view to be sure of that though. 

Just as when you update a value in an index that's stored in leaf and non-leaf pages, all those pages get updated with the new value. Columns stored at the leaf level only through include are updated when you update values. It's possible for this to lead to spage splits too. 

It's exactly what you'd expect if you were running trace, but it's from extended events. Here's documentation to get you started using Extended Events in Azure SQL Database. 

The easiest way to do it is going to be to SSIS to migrate the data over. The wizards alone should be able to do something this simple. The most efficient way would be to export everything to Azure blob storage and then use BULK INSERT. I'd do it all using Powershell so that you can control error handling better. It's going to be a lot more work though. If it's not that big and it's a one-time thing, I'd go with SSIS. 

Microsoft stopped developing functionality for Trace back in 2008. Everything now is focused on Extended Events. The same is true for Azure SQL Database. Extended events capture the rpc_completed event statement text in a very similar manner. As an example, I'm capturing rpc_completed events from a PowerShell script that is using the SQlClient.SqlCommand object to execute a stored procedure like this: 

Assuming we're talking about a full or differential backup, a marker is placed at the start of the backup process. At the end of the backup process, any transactions that committed during the process are rolled forward into the backup. Any transactions that are not completed are marked as rolled back within the backup. So, the short answer to your question is, all completed transactions from the beginning to the end of the backup process. During a restore operation, the final step is the cleanup of these transactions. 

You can either build your own monitoring tool or look to a 3rd party solution that can provide one for you. If you're interested in building your own, it depends on what version of SQL Server you're working with. If it's 2005, you can use the Blocked Process Report trace event. If you're running 2008 or above, I'd suggest using the equivalent extended event, blocked_process_report. Jonathan Kehayias has a good write up on how to use it. If you're looking at 3rd party products, Red Gate software's SQL Monitor has blocked process and long running process alerts built in. 

I went through a pretty thorough 2008R2 training a year ago. It wasn't radically different from 2005, but it was different enough that you're going to face quite a bit of frustration when you can't do things in 2005, or the things you want to do are done differently. SSAS has been changing quite a lot over the last several releases. This is the list of changes between 2005 & 2008, and it's not small. The new designers are the things that are going to cause you the most pain. 

Not suggesting a better way to run your queries, but hopefully answering the question. You're asking if SQL SErver can ignore the view when you're running queries, but presumably, you're writing the query against the view. In which case, no, SQL Server can't ignore the view. However, there is a process within the optimizer called simplification. Given enough time (more on that in a moment), SQL Server can recognize which parts of a view you're using or not using and then eliminate tables from the execution plan that are not needed to satisfy the query. But, it has to have enough time to do that. Since you're working with nested views, a major coding issue, you're not generally going to have enough time for the optimizer to get a good execution plan, let alone perform simplification and eliminate unnecessary queries. Short answer, no. There's not short cut open to you. You need to rearchitect the queries to eliminate nested views. Any other tuning you do will be, at best, perephiral to the problems you're experiencing. 

The constant scans are a way for SQL Server to create a bucket into which it's going to place something later in the execution plan. I've posted a more thorough explanation of it here. To understand what the constant scan is for, you have to look further into the plan. In this case, it's the Compute Scalar operators that are being used to populate the space created by the constant scan. The Compute Scalar operators are being loaded up with NULL and the value 1045876, so they're clearly going to be used with the Loop Join in an effort to filter the data. The really cool part is that this plan is Trivial. It means that it went through a minimal optimization process. All the operations are leading up to the Merge Interval. This is used to create a minimal set of comparison operators for an index seek (details on that here). The whole idea is to get rid of overlapping values so that it can then pull the data out with minimal passes. Although it's still using a loop operation, you'll note that the loop executes exactly once, meaning, it's effectively a scan. ADDENDUM: That last sentence is off. There were two seeks. I misread the plan. The rest of the concepts are the same and the goal, minimal passes, is the same. 

You need to run DBCC CHECKDB on the database. That will identify what the issues might be that are causing the database to be marked suspect. If it's something simple, like a non-clustered index with mismatched pages, you can drop & recreate the index. If it's something like a data table, you'll probably have to go to your backups and run a restore operation. 

It looks like the statistics are off. From the plan you posted it's estimating that it's going to read 64,000 rows, but it's actually reading zero. That's a very wide disparity. I'd suggest a few things. First, update the statistics with a full scan. Any index rebuilds ought to have taken care of that, but with this disparity I'm wondering if something is up there. Next, make sure that the constraints are all in place (although, this plan isn't referencing constraints since it's a straight index seek with a TOP operation). Finally, capture the wait statistics for the system to see what's actually causing things to run slowly. You can use extended events to capture the wait metrics for just this query, so that's an even better approach. Also, a 64,000 row range scan from a seek is a little excessive unless you have millions of rows. However, this could still be a part of the statistics being off.