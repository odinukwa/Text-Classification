I have several production MySQL databases running on an RHCS cluster. I want to setup replication of one specific database, with only specific tables from that database, to another brand new mysql database server. Can this be done, if so, how? 

I would restart the vcenter services on your Windows box now that the DC is back up and running before going any further. 

How are you assigning a public IP to the server? Do you have 2 NIC's on the server? Is it a physical/virtual machine? I need more information to determine what might be happening. It could be possible that there's another machine on your network with the same static private IP address that's causing a conflict. You can ask someone in the NOC or network department to do an arp lookup on the mac address for your server to see what IP it's grabbing, and then lookup that same IP address to make sure no one else is using the same IP by mistake. 

You can add custom reg keys by creating a custom adm file and importing it as a template into the Administrative Templates section of a Group Policy Object. Then link that GPO to your OU. There are docs at MS about how to do this, or you can look at the adm files that already exist on the server (somewhere under Sysvol I think). This process is called "tattooing the registry" and it means you are outside the control of group policy removal i.e. the reg entries will remain even if the policy is removed. You need to create a "reversed" reg key and deploy it (or just delete it). 

Do you mean ESX or Vmware Server (the freebie that runs on Windows)? I assume you mean ESX...in which case there are various options. How are you doing it now? Also what speed is the link? LAN or WAN? 100Mbps or 1Gbps etc? If you are copying from Windows using WinSCP or similar then that will be slow. SCP does encryption on the fly and WinSCP is particularly slow I find. Another alternative is to enable FTP server on the ESX host. Then you can just ftp from Windows, which will be hugely quicker. If you are going from ESX to ESX you can use command line scp which isn't too bad (compared to the Windows version). 

The output md5 hash values should match. You can check your csr also to ensure that it matches your private key and cert. 

Determine the IP address that is assigned to your server and then go onto the DHCP and set a DHCP reservation for that server. 

It appears that you may be missing some settings and may have some incorrect. You might try these settings 

This is correct behaviour. SSH does not log commands that are issued (aside from use of to change user. If you wish to have the commands logged, then you should use sudo for all commands. BASH history for the user will capture the activity also. 

Office 365 will not be allowing you to send on behalf of a 3rd party address. To do what they want, you will need an email server. The problem is that many of the emails will not get delivered since they will not originate from the proper email server(s) for the client and will be flagged as spam. To make this work, you will need that email server, and the client IT department will have to make DNS changes to make your server an authorized source of email from their domain. Or just set up a quick ubuntu server with postfix configured as the MTA for the client and send the blast. Some (or most) won't get delivered though. 

Have you installed the VMware Tools into the guest? I seem to recall there being a shrink option in there. Hang on, I'll just check... ...hmmm apparently the Tools shrink option doesn't work for pre-allocated disk (or one with snapshots). Did you pre-allocate the disk? Or did you just let it grow to the current size? 

Snapshots are 100% reliable in this scenario. I would be inclined to believe your developer has made a mistake. The snapshot doesn't deal with "files" it deals with low level disk blocks. If there was any problem with the snapshot you would have more serious errors than missing files, more likely you would get partition corruption or disk failure errors (assuming the vmdk could even be mounted). 

I have a spreadsheet on a Windows 2003 R2 server. The clients are all Windows 7 using Excel 2010. If a remote user opens the spreadsheet first then any other "local" user gets access denied if they try to open the file afterwards. By "remote" I mean across a WAN link, whereas "local" means on a LAN to the server. Is it possible to allow the file to be opened "read-only" instead of getting access denied? The spreadsheet is not configured to be shared within Excel. This is not a permissions issue, everyone can access the file at the NTFS permissions level. I suspect it may be to do with op locks and the fact that the remote user is across a slow link (satellite). 

Here's the setup: I have a MySQL DB cluster in Rackspace that I'm already using for replication to multple slaves in multiple datacenters. I am trying to use AWS' Aurora with a MySQL front-end, which I've had working before, but now replication is breaking. This is the error that i'm seeing Last_Errno: 1146 Last_Error: Error 'Table 'SOME_DB_NAME2.segment' doesn't exist' on query. Default database: 'SOME_DB_NAME2'. Query: 'DELETE FROM WHERE 1' The problem is, is that I have only imported one database called SOME_DB_NAME1, NOT SOME_DB_NAME2. So the error that i'm getting for a table that doesn't exist in my database at all is really strange. I exported 1 database from my Rackspace DB cluster to setup replication, and when I import that 1 database into Aurora and kick off replication, it almost immediately failes and gives me the Errno: 1146 for a table that doesn't even exist in the 1 database that I brought over. I've done a SHOW TABLES; using the 1 database that I brought over, and i've confirmed that the problematic table doesn't even exist. I don't know if maybe I have to grab ALL databases from Rackspace and bring them over to initiate replication, even though I just wanted 1 of the databases... 

If the box is a member server check in the local Administrators group on the server. If it's a Domain Controller check in both Administrators and Domain Admins in Active Directory Users and Computers tool. 

Raid 0 is not mirroring - it is striping. There is no redundancy, if one disk fails you lose everything. Raid 1 is mirroring. You don't get anywhere near twice the speed in practice. 

Yes stsadm is fine, it will backup the database contents and site. You should also do a SQL backup of the backend db just in case. We have a scheduled task that runs stsadm daily and dumps the backup to a folder. From there our backup software slurps it up and puts it on tape. EDIT - it appears my advice above is only good for small (<15GB) sites according to Technet. For bigger sites they recommend not using stsadm. This is news to me too, so I'd better read the link! 

I have a requirement to have a users home workstation (mac) connect via ssh tunnel to a corporate bastion host (bastion.corpnetexternal.com) and through to a mac workstation (host.corpnetinternal.com) in the corporate network. Here is the connection string: 

It appears that you are running the script manually as a non-root user and cron is running it as root. If that is the case then you will need to define the path to the ssh private key. If you are connecting as root to the remote server (bad) then you will need the root authorized_keys to have the public key of the workstation 

If one cert works and the other does not, then my first suspect is that both certs did not originate from the same private key. Start by checking that both certs actually are the same 

Here's the scenario: (2) Windows servers in an office Server 1. Application Server Server 2. Backup Server, PDC, DNS (5) Laptops that backup to the backup server The Backup server has Symantec Backup Exec installed and backs up the other server, than throws the backups that it does for itself and the other server into a second partition. Also, all of the laptops are configured to backup to the same partition underneath a labeled folder. The Goal: I want to have an rsync script backup everything on this second partition on the Windows server to another -- remote -- NAS device running Linux. Does anyone know this can be accomplished from a Windows to Linux platform, or anything similiar to what I want to do? I enjoy rsync and it's awesomeness, but if there is another, easier, solution out there, that'll do as well. Also, I know that Windows doesn't use Rsync. Just the method that i'm trying to use. 

You can have all the whizzbang technology in the world. Customers still won't be happy. You are simply enabling them to do their job. Their job may make them happy, but most times IT is invisible to them and only rears it's ugly head when things go wrong. 

I need users to be able to copy a folder on a network share to another folder on the same share. They need to copy security too. They cannot use their normal logged in account as it doesn't have enough permissions (the folder is a "template" folder and is locked down with restrictive NTFS security). So I created a proxy account with full permissions and a batch file that calls runas like runas /noprofile /env /user:proxyaccount@mydomain.com z:\copy_folder.cmd copy_folder.cmd just contains a robocopy command to copy the folder. The runas fails with "cannot find file copy_folder.cmd". Z: is mapped correctly and I have tried runas without the /noprofile and /env switches but I get the same error. What syntax should I be using for runas to find copy_folder.cmd? 

I have MySQL servers A, B, C, and D. Server A is the MySQL master who receives the write traffic, and my applications look at B + C for reads (current slaves). I have replication forwarding ALL database events from A --> B and then from B ---> C. Server B is a slave to A, but a "master" to C. There are about 14 databases being tracked in the bin-logs. So, I would like to setup replication to AWS Aurora (the future D server), but I only want to replicate ONE database to it, from server C. So my setup would ideally look like this: A ----> B ----> C ----> D (aws aurora w/ 1 database) How do I tell server C to become a replication master and only replicate ONE database to server D (aws aurora)? I hope this makes sense. 

The public key to use is set in the authorized_keys file for the user account on the server to which you connect. It is not set in the client. $URL$ is a good read for how this works. 

This is known to work, so no worries there. The ssh tunnel needs to be automatically created and there must be monitoring ensure that it is functional end to end, and if not to be restarted. Additionally, this functionality may not be dependent on or interrupted by a VPN tunnel that may or may not be present from the user remote workstation into the corporate network. I would appreciate any suggestions. Please, no comments on the why of using ssh tunneling if a VPN is available. I have listed the requirements levied upon me.