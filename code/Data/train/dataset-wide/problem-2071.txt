RMAN allows you to either try to fix the file (recover) or to recreate the file (restore). Before you give up (if ever), you should be at least trying both of these options. 

Grouping a Primary Key is wasted time, since you get groups of one record only. Hence, the SUM function is unnecessary, you just need to get the column. But, the real problem you are trying to solve, is a bit unclear. If you ever have 2 tables in a database, with EXACTLY the same Primary Key, there is something wrong. Obviously, they should be 1 table only, and then you don't need elaborate UPDATE statements. You may need an UPDATE statement, but nothing like above. Explain us why you have identical Primary Keys in different tables. 

The size needed in a temporary tablespace depends on the data volume manipulated, but including potentially other actions also requiring temporary space. What you should be doing is this : monitor temporary tablespace usage, (temporarily) expand the temporary tablespace. Maybe, you need to recreate your temp tablespace. That is, if there is corruption of some kind. One way - but not the only one - is stopping the database, rename the file(s) of the temporary tablespace, tail the alert.log, and startup the database. Obviously, don't do without backup, and don't test this on production. 

I've stared myself blind at this. It started as a more complex procedure, but I've stripped it down to the bare bones, trying to make it run. This is the current code: 

For the two former events, I'd store the type of event, the old value and the new value. This works fine as long as I stop here. However, if I want to log the 2 former-events, I would need to be able to store different data-types in the same column. The changes would look something like this: 

It simply refuses. It's worth mentioning that if I run the code without the parameter-part ( being the sole parameter to the function) it runs fine. I've tried other syntax alternatives, such as 

It feels like there must be a smarter way around this, and I would appreciate any clarification and corrections to the statements made in this post. Like mentioned earlier, I'm just trying my best to understand why it has to be this way. Apologies for the lengthy post and thanks in advance. 

I am attempting to construct TSQL queries to substitute various GUI tools provided by SQL Server Management Studio. One of these tools is the , accessible through the window. 

There are two reasons that prompts me to ask this question: tSQLt The T-SQL testing framework tSQLt considers it an issue of "High Severity" when there exists columns with a non-default collation. The author of the test states the following: 

Definately, exporting that many rows to a tool like Excel, sounds like the worst solution to what sounds like a normal action. This being : updating data. In the opening post, I don't see any reason why the data has to leave the Oracle database. Certainly not to a tool like Excel. 

It COULD be simple, but then it's down to luck, more than to anything else. The way of working is very simple : get all processes listed of the main database, ignore the ones from the standby. Then, do the same with the standby database : collect all process info. What is important here, is that you capture this while all is normal. Then you compare both lists, in detail. Chances you find some process description active on one, but not on the other, is pretty high. There you go, differing both databases on OS level. It must be said, although you technically may be able to differ (as explained above), it's never a good sign if questions like that are asked. Either no permissions are granted, or knowledge is limited. Both are not good. The good way of differing both databases, is querying some V$ view. What I'm describing above, is an alternative method, but you should not rely too much on it. Why not ? Because when alternative situations emerge, the procedure may not work anymore. I'm thinking of; databases being started up and such, but you described already that things may not work if different database modes are used. 

Our current database environment includes a cluster with 3 nodes(one primary and two read-only replicas), as well as a single, independent server standing next to the cluster. For brevity, I'll call the nodes N1, N2 and N3, and the independent server S1. Recently, we configured for our servers, using S1 as the MSX (master) server, and N1, N2 and N3 as TSX (target) servers. This means that SQL Agent Jobs that operate on the cluster-nodes are created and managed from S1. TSX servers report their state, outcome etc to the MSX server, which can be accessed using said . From the , the job-history of each enlisted server (TSX) can be accessed by selecting : 

At this point, I would need to store the textual old/new value of the company name, as well as the id of the recently added user.You might see that I'm already headed off in the wrong direction, and this is where I ask for help. I have 2 questions: Should I just use -datatype, or if this is considered poor design then what would be a sensible way to store these log-events? Thanks in advance. 

Yet, the severity of the failed test is, as mentioned, considered high. Octopus Deploy While configuring the Octopus Deploy Server, the setup fails with a FATAL error during the initialization of the OctopusServer-instance. The article related to the error-message does not explain why this is a requirement, but simply states that it will be a requirement for future deployments, from and including Octopus version 3.8. As a side-note, RedGate's CI-tool package, the DLM Automation Suite, supports deployments with varying collations without complaints. The recommendation of keeping all column collations to the database default seems more like guidelines or best practices to me. Why is it considered such a serious error by some? 

Now, if you don't use the Binary Logs on Slaves (for point in time recovery or chained replication) you can disable it and remove them: 

Unfortunately MySQL doesn't support "Functioned Based" index (but you will have noticed that). It is therefore not recommended to use this trick, Peter Zeitsev wrote in a blog post on the ORDER BY thematic : 

If you want to do more complicated tests (directly in you applications for instance) you should setup a real "Test Environment" from a fresh backup of your production. Max. 

You'r lucky you have only one secondary index, the is a clustered index so it isn't included in the index statistics. You can find the answer with the field of the information_schema.tables view (result in bytes): 

If you are sure that your MySQL is a standalone instance, there is no know issues with the statement and I see that your binary log is 1.5Mb so i'm sure your system can manage that purge without pain. In a Master/Slave infrastructure, you can purge binary logs before they been "played" on slaves, the result is the slaves replication goes down (with impact on the apps). Max. 

For DDL (ALTER TABLE): I stop the replication, run my DDL commands, run a rollbacked script and restart replication: 

Operations and issues on slaves will not impact the Master. I dont't understand your concept of "the master -> slave replication running every 5 minutes". All transactions on Master will immediatelty be replicate on slaves. If your tables are InnoDB, your dump will not lock any tables (with the option --single-transaction) so you can have normal activity on master and slave too. To backup your slave you can also stop the replication, shutdown your MySQL (Slave) and copy your datafiles or use a hot backup solution like Percona XtraBackup. Max. 

Now, I'm only a junior DBD and my understanding of how the transaction-log works is very limited. That being said, my seniors have concluded that we cannot use the MERGE or UPDATE statements where all columns are processed in the same statement since it creates excessive logging. The argument for this is that when you perform an -statement in SQL Server, when you set the a column-value and the new value equals the old value, it is still marked as an update in the transaction-log. This apparently becomes costly when you perform lots and lots of pointless SET-operations. In the following example, we update the and of the target-table using values from the source table, joined by . 

This is the information I am trying to recreate, using a TSQL query on the MSX server. I want to see the outcome and history of the jobs by all enlisted servers, similar to what is displayed by the GUI window. I've tried digging through the job-related tables and views of the database, but with no luck. The table on the MSX server contains no history from enlisted servers, and I can't seem to find any good documentation on how else I would go about gathering it. Is this data even accessible through TSQL? Any relevant resource is greatly appreciated. 

This example does not check whether any values has actually changed. If we ignore -checking and sane fallbacks for a moment, this could be checked in one of the following ways: 

Your graph shows a binary log purge launched at approximatively 4am the 23th. You can search for a query like: 

Effectively, DELETE order doesn't release space... You should reorganize tables with OPTIMIZE TABLE order. If you can stop activity, you can also make a dump and restore it. Max. 

To switch of MySQL engine you could : 1 Make a ALTER TABLE myTable ENGINE=InnoDB 2 Make a mysqldump of your table, then edit the CREATE TABLE statement to replace MyISAM by InnoDB and restore the dump in the new table (i called it myTable_InnoDB): 

Note regarding log-slave-updates: by default if you enable Binary Logs on Slave, the Slave will only writes events executed on the Slave directly, none of the events coming from its Master will be written in the Slave's Binary Logs. If you want to setup a chained replication (M -> S/M -> S), you need to tell the Slave to logs the Master events on its Binary Logs to replicate them on its own Slaves. This options is log-slave-updates. If you need to enable Binary Logs on Slave the command to see the curent position of the Slave´s Binary Logs is you will see the position coresponding to your files on your directory (on slave). Note on Binary Logs managment: Do not forget to set a "purge strategy" for your Binary Logs if you don't want to saturate your disks. The simplest way is to use the expire_logs_days variable which tell to MySQL to purge its Binary Logs older than this variable. I hope I was clear... Best Regards 

I am trying to create a log-table for storing events to a -object, and I am afraid I might be taking the wrong route. I've arrived at the conclusion that I should log different data-types in the same column, and it doesn't feel right. I'll explain the basic use-case with 2 tables; and . 

In Example #1, the SET-operation will update all columns, even if only 1 column has changed. In Example #2, the SET-operation will update all columns for all rows, falling back to the old value if the value is unchanged. In both examples, all columns are hit by the SET-operation, and, according to my seniors, this creates an unnecessary/problematic amount of transaction-logging when done frequently. The same applies for the -statement. Even if you check a matched row for changes, all columns are hit by the update. 

Now, say I want to log different types of events on the -object. I'd make a log-table called and and store events there. These tables would look like this: 

We have several tables which we "refresh" frequently by rebuilding them in a staging-table, then performing a metadata-switch to the production-table using the statement. These operations are causing a huge amount of logging, and we're not sure how to handle it. For now we just moved these tables and operations to a new database using the recovery model, but I would love to hear alternative solutions to this problem. How do you suggest we handle the massive log generation as a result of frequent table rebuilds?