Will not handle out of SQL references, but you might want to check out Redgate's SQL Dependency Tracker. It's a nice visualization tool. 

$URL$ shows that -Output expects a path, not an integer as the error message says. Pub & Sub are both v9.0.4211, Dist is v10.0.2723 My Script (run at distributor): 

You can have your app impersonate a windows user and do the connection that way. Example for ASP.net app. 

I know LINQ queries are composable, but have you tried playing with the order of LINQ operations to see if it could affect the query generator? This question may be better served over on Stack Overflow. 

Composite keys as primary keys also run into index size issues that can affect disk usage, io speeds, and backups. You might want to review Kimberly Tripp's posts about primary keys and clustered indexes here: $URL$ I too would suggest a surrogate key in this case instead of a natural one. 

If either $FreeSpace or $Size -eq $null, then it won't properly complete the query string. Either use command parameters just as you would in .NET (best method) or check for $null before insert. 

The simplest solution is to buy something like Red-Gate's SQL Doc. There are a few people who have written scripts that you can find for free. 

We are in the process of removing a previous dba login and he owns all the endpoints and event notification objects. Endpoints were easy to change; Event notification objects not so much. I found this thread about changing the owner of an event notification object (you have to drop and recreate). I don't want to go through this process again if I can avoid it. I doubt it's possible, but outside of logging in as another user, can you create an event notification that runs as sa, etc.? 

I have a 2 node cluster (NODE-A & NODE-B) with 2 SQL instances spread between them. INST1 prefers NODE-A, INST2 prefers NODE-B. INST1 started generating errors then, failed over to NODE-B. Migrating INST1 back to NODE-A generates the connection errors after it logs a "Recovery is complete." message. Win 2008 R2 Ent. SQL 2008 R2 Ent. Errors from the Event Log after first failure: 

If your database is in FULL or BULK_LOGGED recovery mode, you need to backup your database and log files on a regular basis. If your database is in SIMPLE recovery mode, then you only need to backup your database on a regular basis. Please read the following articles for more info: 

It depends on your environment. I would setup a test using both methods and see which works best for you. Personally, I would page on the server. Less data over the wire and less data in the client's RAM, the better. If you can control the client machine specs, all traffic is over a nonsaturated LAN and clients always page through multiple pages quickly, then you might want to page at the client. 

You could also try at the beginning of the proc, setting isolation level to SNAPSHOT. More info available at: $URL$ You will incur some cost in tempdb for the row versioning. 

I think this really comes down to user preference as there's no real technological reason to do this. In fact, for simplicity sake, I say always use dbo unless your security requirements stipulate otherwise. Of course, you can always do it for just organizational purposes as well. 

I had written a process back in 2005 to use bcp to dump the data out then pg_import the data and do all the schema scripting and conversion necessary. Procs are slightly different because of syntax discrepancies. 

You could use a Document-oriented database for this. You could then create a program in your preferred language to import the existing documents into the db, parsing the folder structure for the metadata (customer, job#, etc). 

You can, but I wouldn't. You would always have to wrap the DB name with square brackets such as [MyApp.Sales]. So to recap: if you value your sanity, don't do it. 

I'm trying to add the -Output parameter to my log reader agent for transactional replication and getting this error: 

You could use parameters to solve this problem with the added benefit of execution plan reuse. Set the data types to match database. 

I wrote a TSQL script that will update the description based on the current version of the report. GitHub Gist 

I've got a SQL 2005 SP4 server that connects to a 2008 SP3 instance via linked servers using the SQL Server server type. Every once in a while, one of those linked servers will start throwing login timeouts. To rule out firewalls, I can RDP to the server and run sqlcmd and get in just fine, even making sure to use the same login. I'm thinking that SQL has somehow cached something that prevents it finding the right address. The remote servername is defined in that machine's host file. So far, only a reboot fixes the issue. 

Have you tried using Adam Machanic's sp_whoisactive? There's an option to get the outer command to see if it really is within a proc. It could be the application is holding open a transaction instead of committing it. Try looking at DBCC OPENTRAN as well. 

Using SSMS, you cannot chain a restore of the backups in one operation. You would have to do multiple restores. You'll want to use T-SQL in order to be more efficient. 

Using Oracle 11g I just trying to understand the concept here.... Why the command bellow not work? (executed at SQL Developer) 

I use this command to "lock" my database, flush logs , take a LVM snapshot and release it. Testing the recover + binlogs , appear it's not working. (I discovery this when during the recover, applying the binlogs give error of duplicate key where supose not should exists at that moment) The behave I get is the data files still be writting after the flush. So , what's wrong here? I miss something? 

The problem is the FLUSH running in standalone session, when this session close it's released. To solve I run all at unique session. This is how I rewrite it and work properly 

I think understand your problem. The dbaccess break lines at the middle of the statement and the table name goes to line bellow... Something like this (dbschema output) 

After some research I've discovered the view V$PROCESS_MEMORY_DETAIL and how use it. By default it's always empty and need some commands to "enable" it (oradebug or alter session). References I found: 

I consider two solutions 100% functional... First suggestion Parse the dbschema with awk to join the lines until found a semicolon. This will give to you long lines... Then you can work little more with awk/sed to get only the index and table name... not so hard. But this probably will become a shell script. 

We are using Oracle 11g standard, without any options (diag/tunning/perf). I can identify one session with the SQL bellow which having a high consumption (1.4 GB) of memory on the past hours... (pga and uga) There is someway to get details about this consumption? Whether it is the use of temporary tables or something else? 

Oracle 9.2.0.8 running at linux . I'm trying identify the dependencies of one object with the dbms_utility.get_dependency command. But unfortunately it always return the error ORA-30929... I already looked at Oracle Support, googling... found nothing . No matter the user, either if I run with DBA user, SYS user or the owner... always get into the same problem. Anyone have any ideas what is this problem? How to solve or workaround? 

Second suggestion Query the database. For this , they should be a script and your user should have access to database with dbaccess 

Unfortunately it doesn't return the destination host when the origin is the local database (where I run this query). At my environment I have lot of Oracle instances , isn't operative easily connect instance by instance to find the same 'GTXID' and then figure out which database is receiving the connection. I would to identify the destination server name, SID name or local database link used for each GTXID listed. I'm digging and just appear to be impossible.... I expected at least found a relation with the name of the local database link used, but either this information is missing. 

Here is a illustration how works.. This was executed with dbaccess at Informix 11.50 FC9. You can name your return parameters, but they are valid only when executed the procedure as procedure by self... not into select statement. There you need to name each column manually... check below. References at IBM Informix 11.50 manual: 

Found the reason my script failed. The lock is released when the connection is finished. Appear be quite obvious, but I don't remember read at any place about this behave and since few commands are able to execute from prompt with mysqladmin (not this case) induce to wrong interpretation of FLUSH behave. My first script, where fails to lock look like this : 

Oracle 11.2.0.3 . I need to identify all "alter session" changed for a specific session. There is some v$session_* where I able to get this information? My objective is part of investigation what I doing about a problem of sessions hanging at compile time... from Forms Builder. I already got a trace of the session and have some suspicious over a bug documented at Oracle Support... but I want more details about this session. 

Using Oracle Database Standard Edition 11r2 . I'm looking a way to backup the database with zero downtime , using a snapshot of datafiles (LVM snapshots) and without need backup de archive logs. All options found so far , we will need save fews archives logs. My reason for this requirement is today our databases are put offline to copy our datafiles. There are procedures and documentation about backup and the restore . I want to minimize any change here at backup and restore process , all looking for minimize point of failures , human error which could be invalidate all backup. So , I need to exclude the archive logs from the equation , do not want they involved. I think something like : 

Using Oracle , versions 8i, 9i , 11g and 12c. Today I use this query to identify all open sessions thru database link: 

You cannot just move the chunks/datafiles and expected the engine just re-adapt to new path. There internal pointers at theirs configurations where the path is saved and need some administration steps to change. Basically , you can do this only at a Backup / Restore, using ontape or onbar utilities. Copying the files can be considered like a and this way is possible to rename the path. For that you will need to use the command. More details, check the manual online . If your datafiles is from older version which is installed at the target server, I'm not sure if this restore will work because you are doing a upgrade at same time... Maybe you need install the same version of the engine used to copy the files, restore them with ontape , shutdown the engine, install the new version and upgrade ...