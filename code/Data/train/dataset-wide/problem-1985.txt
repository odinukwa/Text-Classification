The advantage of using an auto increment field is that new records are automatically given an unique primary key. 

You need to know the size in bytes of each field according to the data type (MySQL reference here), then sum up these values together. 

If you are dealing with an immutable set of operations (i.e. you're absolutely positive that will always be one and only one of the options you listed) then it's ok to use an enum. Otherwise, your ER model will be much more flexible if you create another table: 

It is a bad idea to use primary keys which contain meaningful data. What happens if the hotel changes its name? In this example, it is better to use a surrogate primary key. 

This solution uses the MySQL variable which stores SQL queries executed each time a user logs in. Now each time a user logs in, it will be logged into the table . Be sure to check the permissions of all of your users to ensure that they can insert a record into the table. Another solution, available only for MySQL Enterprise and that I include here for completeness' sake, is to install the MySQL Enterprise Audit plugin. 

Yes, if you can identify a natural primary key (e.g. the student ID in an University, the ISBN for a book, the S/N for a piece of hardware) it is much better to use that one. 

Clearly you have to define first the groups to which the different privacy settings will apply. You can then fine-grain these privacy settings by setting each of these fields (the Custom button in your picture), or set all fields in bulk to a specific value (the Public and Friends buttons). 

What is the best practice to run on a cluster? I've done some experiments in a test environment without users, and it appears that an on a node does not automatically propagate its effect to the other nodes. This is consistent with the fact that this command modifies the indexes and the table's storage space, not its contents or its definition. 

I've seen a sudden increase of binlog files, very large in number and size, in all three nodes of a Percona XtraDB cluster: 

A with a sequence number of -1 is fine on a running node. When the cluster stops cleanly, the sequence number will be set to a value on each node. You'll then have to bootstrap the cluster from the node which has the most advanced seqno. 

NOTE: question edited to take into account restore over LAN vs. local restore. Are there benchmarks available for the time spent doing a database restore from a mysqldump file? Based on my experience, a restore over LAN has a speed of 100-130 Mb/min, while if the dumpfile is on the local SSD drive we're around 1 Gb/min. We're talking about database size, not dumpfile size. Hardware capabilities seem pretty much irrelevant once one has a fairly decent CPU and amount of RAM. I have been wondering if that above is a reasonable time or if this can be speeded up -- especially in the case of a non-local restore (i.e. done over the network). 

On MySQL command line on the remote machine, do a , then copy&paste the relevant part of the output on MySQL command line on the local machine. 

There's no need to have one table Users and another table Persons. I've never seen a social network that allows a single user to create more than one profile. True, there are people that keep multiple profiles, but these are done by opening multiple accounts (sometimes against the TOS). You could use this model: 

as now you can create all kinds of new disposition types e.g. "Repaired", "Use as is", "Scrap", "Sell to China", or "Bury in landfill". In this case, you remove and replace it with a FK instead. The advantage of this approach is that you can set up authorization levels for Technicians to perform a specific disposal (it is not mentioned in your question but it shows in your model, if I'm not wrong). This is done via a join table Authorizations: 

It's your duty to take care that the same data is not entered again i.e. verify that the student exists and, if yes, edit his record instead of creating a new one. 

The Testing can as well be included as a field in Investigation instead of having its own table, unless the exercise requires it. Then there are the tables for Doctor and Patient which you already modeled. This is the description of the tables, from it you can easily create the ER diagram, remembering that a FK on a table connects to the PK on another table. 

and the same for the email address. This schema only modelizes the contacts. You'll have to implement a different table to store the data for the users of your application and bind the contacts (the first table above) included in a user's addressbook to the correct user. 

I've my hands on a 3-node Percona XtraDB Cluster where, according to , some tables are corrupted (some indexes contain the wrong number of entries): 

I'm managing a e-commerce site which uses a popular online shopping cart software running on MySQL 5.6. Yesterday I noticed that reports that 990 of 1000 queries are waiting for a query cache lock: 

Since the field was never created with a DEFAULT clause, it gets automatically assigned DEFAULT NULL. However, at a first sight this looks like contradicting the fact that it was also defined as NOT NULL. Then I realized that this schema aims to force to insert a value (and a non-NULL one) in when adding a new record. Is this an acceptable way to do so, or there are better ways? 

Personally I like to design databases that allow for flexibility, but the choice here it's up to you. Choice of the DB engine A MySQL database with InnoDB (which is the standard engine in MySQL) will work fine, so it's a good choice. 

MySQL allows the use of wildcards for database names, in order to allow an user to operate only on a subset of databases: 

The int values mapped to each field set the visibility of the field with respect to the different groups, e.g.: 

Never use a name (as it might be not unique in the DB) or a phone number (as it might change) as PK. A golden rule is to use non-meaningful fields such as an integer ID with autoincrement. Note that this simple schema allows you to enter only one phone number or email for contact. For more flexibility you should move these attributes to their own table: 

Sizes are printed in Mb, and only tables larger than 1Mb are considered. Tables are listed in alphabetical order and the output is saved into a CSV file. Running this query at different points in time showed - obviously - different results as data changed in the database. However, just in the last few weeks the query produced identical results. Does it mean that the database hasn't changed much in size (as you can see, rounding is done at the 10Kb boundary) or is there something I am really missing? The question might sound absurd, but do the views in always hold up-to-date metadata? Note: no, I am not reading each time the same CSV file by mistake. Edit: all tables are InnoDB, and . 

Is there a way to do the same for the CREATE grant, to allow (cf. the example above) to create only databases whose name starts with ? Otherwise said: is the CREATE grant global (i.e. an user with this privilege is allowed to create any database, without limitations) or it can be limited in some way? 

It is clear that the table is badly damaged. The application using the database still works (although with reduced performances), and so do statements. The mysqlcheck tool cannot be used to fix it, so the solution I'm aware of is to do a , drop the table, and do a during the next maintenance window. Are there drawbacks with this way of proceeding, and are there other options to fix the table? EDIT: I've restarted the MySQL server with increasing values of from 1 to 4, and the result is always the same: 

Which databases are included in the dump generated by ? By my experience, that should be all the user-created databases, plus the database. The database is backed up only if explicitly mentioned, and is never backed up. Is this correct? 

Your question isn't very detailed, anyway here's how you could implement what you want: The simplest way to model a privacy setting schema is probably to add a visibility field to each relevant property of the user profile: 

In a MySQL master-slave replication architecture, how do you detect if there have been (erroneously, since this should never happen!) direct writes on the slave? For now, I'm running this command on the slave: 

You can say that a database is normalized in 1NF (or 2NF, 3NF, etc.) You can also say that a database is in 1NF (or 2NF, 3NF, etc.). That's equivalent, even if I would prefer the first sentence. If you just say that a database is normalized, it means that it's in one of the Normal Forms. I would avoid saying that, because without specifying the NF you don't convey much information to the reader. Since normalization works through a series of stages, a database in 2NF is also in 1NF (plus no partial dependencies), a database in 3NF is also in 2NF (plus no transitive dependencies), and so on. The process of transforming the database structure in an higher form (e.g. from 1NF to 2NF) is called normalizing a database. The opposite process is called denormalizing a database. 

How do you get the size (in bytes) of the result of a SQL query, in MySQL 5.6? will return the number of fetched rows, but not their size. My aim is to evaluate common queries to know a lower bound value for so that they can be served by the Query Cache. 

However, the Time is always 0 and the process Ids change all the time. My understanding is that the query waits for a table lock but the lock is released after less than one second. Is this a normal/acceptable behaviour or could it be worth to do some fine tuning on the query cache, perhaps removing it completely? 

Why does the tool suggest this? Can't the original composite index be useful? As far as I understand, an index on would be worth of deletion given a PK , but it is not the case here. 

I manage a Percona XtraDB Cluster that uses a network storage with a flapping connection. Periodically we experience a high iowait with crashes and remount of the fs in read-only. Replacing the storage is, unfortunately, not an option for now. Recently I noticed that when mysqldump or mysqlcheck are run, they crash the MySQL server on the node, with an error Here's the content of during the crash: 

An user would login to the site via his username and main email address (user_emails.is_main = TRUE). 

Our Amazon RDS instance rotates slow logs every hour, and keeps only the last 20 of them. This is not optimal for monitoring as it prevents us from seeing any slow query that might have been running more than 24 hours ago. Is there a way to have the slow query log rotate once a day - instead of once an hour - or any other way to keep the slow logs for a longer time? Edit: Database is MySQL 5.6. 

This question is too broad, I'll try to give you some guidelines and tips. Your databases are quite small, MySQL can handle databases much larger than that. I don't see the need to switch to another database. You should also use a separate partition for the mysqldata directory so if the system partition fills up the database is still safe (and vice versa). Use tools such as Mysqltuner to check if your MySQL config is correct with respect to your hardware. You can also do periodic benchmarks with sysbench to ensure your server is working smoothly. By the way, I'd recommend that you switch to MySQL 5.6 which is the latest stable version. 

Because of the way the query cache works, setting it to a large value will have detrimental effects. Generally, acceptable values range from 20 to 100 Mb. You should start with a low value, and then increase it little by little to maximize query cache hits. From the MySQL manual: 

I manage a PXC 5.6 cluster and noticed that the variable in isn't set. I was thinking to set it to (which is tmpfs) to explicitly make the cluster store temporary files in RAM. Without this setting, MySQL could choose or (which, on my system, are on-disk) to store temporary files. 1) Is there any drawback to do this on PXC? I ask because on a MySQL replication slave one must not use a RAM disk to store temporary files, since the slave will need some of the files when the machine restarts. 2) Is this really going to improve performances, or it looks like a unnecessary twiddling? The InnoDB buffer size is already large enough, and 95% of temporary tables are stocked in memory. 

I manage a Percona XtraDB 3-node cluster. Recently the MySQL datadir of one of the nodes (not the one from which the cluster has been bootstrapped) started being filled up with files. This is not happening in the other two nodes. What causes the creation of such a large number of Galera cache files? I stopped the daemon, wiped out the datadir and forced a SST on the node, but that's not really a solution -- I want to find the cause for this behaviour. 

Model Here's a possible model. After I drew it I noticed that the difference with yours is that the product part number is now in the Product table; I think it should be this way as it's a characteristic of the product. Then the Job refers to a Product via its part number, and not viceversa. Each table uses a surrogate primary key for clarity. This is not really necessary in several cases e.g. in RepairComponent, so feel free to replace it with a composite primary key. The table stores a product. The field (part number) should identify your product since it is manufactured; the product is then assigned a DMR when it fails and then, if it is repaired, a S/N. 

You should remember that a MySQL dump is basically a big SQL script textfile that creates the database(s) schema and inserts all data into it. So once mysqldump exits correctly, you can be pretty sure that everything went well (assuming of course that you're backing up the correct databases!) I'm not aware of any way a mysqldump file could end up corrupted (unless there were filesystem issues, in which case any file could be mangled). MySQL is (rightfully) pretty squeamish about it; if there were some corrupted tables in the database, mysqld would crash and would not allow mysqldump to terminate, in order to protect database integrity.