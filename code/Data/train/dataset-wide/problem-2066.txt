Well, it seems like all your data is meaningful, since you don't have a lot of it, and it all seems to have a role as a key or useful attribute. If you have PK (by definition unique) on isbn in b_c, then this restricts a book to one class. Is that true? At that point you could argue the design that the class_id should simply then be an attribute of the book table and you don't even need the b_c table. Since you already have a PK on b_c, I don't see the need for a surrogate key. Even if you were to expand to compound primary key on isbn, class_id to be able to allow multiple classes for a book, I don't really see a need for an additional surrogate unique key. In any case, it would only be an alternative unique key, I probably wouldn't make it the primary key and probably wouldn't use it in joins (joining to link tables is not terribly common as a foreign key, since they are usually identified by their parent or child as being part of a collection based on that relationship) This is what I would do: 

We've done a lot of this, and (administrative) users were allowed to fix the translations live. (You still might want a caching layer, but I'm totally down with driving this with a real database and not resource files - it gives you a ton of power to query and find things which need to be translated, etc). I think your schema is probably fine, so I'll just pass on some stuff we learned in the hope that it's useful. One thing you have left out is phrases with insertion points. In the example below, the order is reversed and the language is still English, but this could very easily be two different languages - pretend this is just two languages who normally put things in a different order. 

In addition to JNK's answer, you probably should read this article which discusses aligning table partitions and index partitions. There are many types of scenarios where partitioning scheme does exactly follows the primary key's first column - for instance in a data warehouse scenario where the snapshot date of a fact table is usually the partition column as well as the first column in the primary key. But equally, in OLTP environments where the PK is an IDENTITY or other surrogate key, it makes little sense to use this for the partition, since partitioning on arbitrary numbers is not normally terribly useful. In OLTP systems, you also tend to partition by date the most (probably not in the PK), but potentially also regionally or by some kind of organizational division (maybe in the PK if you aren't using a surrogate). But it's not a requirement. 

In addition to the rather odd (and possibly over-restrictive modeling of the phone number components), the "data" concerns me. It basically looks like the EAV pattern restricted to DECIMAL data type, which can have it's place, however there appears to be NO OTHER data modeled at all. When you finally get round to querying these things, it's a lot easier and involves less pivoting if you can just do things like or whatever it is these different types of data are that are attached to each user each week. 

You are far better off simply inserting into a mail queue table in the trigger and having a separate email process handling the mail - either in SQL Server Agent or an external program. This gives you the benefit of using the trigger as you desire, relatively low latency as the other job can see the mail once it's committed, scalability of managing the mail load independently etc. You can wrap the mail queueing into the transaction properly. Then the queueing app is reliable if the mail server is temporarily unavailable or if you have email address issues. If mail needs to be resent, it's simply a matter of resetting the sent flag. You can make expiration on mail, so that mail which couldn't be sent to the SMTP won't be retried after a certain time. You can quickly route all mail to a test mailbox. You can do quite interesting things with attachments and other things based on data in the queue without tying up the trigger for extended periods - things like generating reports etc. So the app offloads the mail responsibility to another component and is free to complete the database transaction and return success to the users, while the mail subsystem gets on with its work ensuring all you want to do with the mail gets done. 

Not sure about your specific domain, but typically in most inventory management it's going to be a little more complicated than that. Most businesses will take a physical inventory count on some kind of schedule and this may be as infrequent as annually. However, that physical will then become the new stock value, generating some kind of adjustment for the variance. In addition, sometime there is a separate release and receipt procedure which ensures that the change in inventory is booked to the correct inventory period. You should keep that in mind in any design, regardless of whether using triggers or not. 

In this scenario when you add something else which needs to be tagged, you just add the entity table and the linkage table. Then your search results look like this (see there is still type selection going on and turning them into generics at the object results level if you want a single results list): 

You've tagged your question with "t-sql", so here goes (although I think/hope that this should be ANSI SQL compliant) : 

is a server-level role that allows its members to create, alter, drop, and restore databases on the instance. Intuitively - and without having seen your dacpac - I would grant the executing user in question membership in the database role, so it can view all objects, as well as , so it can create, alter and drop objects in the core schema as needed. From there on, I would monitor what types of permission errors you encounter running the dacpac. Judging from your question, I'm guessing that it wants to alter or drop/recreate the database - if that's the case, see if you can fix that when you're building the dacpac. 

... and repeat this process for each table. You may run into problems with foreign key constraints, so you may have to arrange the order of the tables carefully, but in the end you may be better off not having any foreign key constraints at all in the local database. If your existing rows can change, the SQL script looks similar, except you download the data to a temp table first, assign a unique clustered primary key to the temp table, then ("upsert") the contents of the temp table to your local table. If you don't like or trust , you can obviously use and the old fashion way. Note that you won't catch rows that have been deleted from the remote table this way. Using SSIS With SSIS (or if you decide to build your own local application), the work is the same, but you won't need linked servers, and instead of using , you'll dynamically build SQL statements and run them through an ADODB or similar database connection. I've included the "build your own application" option for completeness; I wouldn't go down that road. Indexing considerations For the remote query to be as efficient as possible, you may want to investigate if you can get the remote DBA to set up an index on the "last updated" column of each table. Backup option In the end, I would probably consider Aaron Bertrand's suggestion and try to set up a solution where you download a backup of the remote database (perhaps a full backup, once per night), restore that backup locally and perform the sync operation locally on your server. Real-time Returning to the real-time issue - the driving parameter here has to be the business. If management needs up-to-the-second accuracy in your database, somebody is going to have to accept the considerable increase in work and costs, not to mention maintenance. In the end, you may find that a high-speed leased line may be a lot cheaper, and that you can archive the data with a giant download once a month or so instead. 

All of this serves to simplify the understanding of, and accessibility to, the business data for people who aren't too comfortable writing SQL queries or using pivot table tools. Some of the greatest pitfalls in data warehousing and BI involve users not understanding the structure or meaning of the data, producing incorrect results and subsequently not trusting the BI solution. 

The following pattern will often render very good performance, compared to self-joins and correlating subqueries: 

What happens here is that produces a . According to the documentation, the output type of is of the same base datatype as the input, although the scale (the number of decimals) may change, which is what happens here. 

I also removed Status1 from your aggregate, because it would create a separate row for each occurrence of employee and Status1 - what you want is one row per employee only. 

The permission is assigned to the server-level principal (the login, i.e. not the user, which is the database-level principal), and therefore, you'll need to apply it to each availability group replica. I would recommend that you apply (deny) this permission on a server role rather than individual users to keep it manageable. More detailed reading on the subject: $URL$ 

The "last updated" columns in the remote tables will help you considerably, in that you won't have to download the entire database's contents every time you run the ETL process. Without being able to install or modify anything on the remote server, there's no practical way to synchronize in "real time". The frequency and speed that you can synchronize the databases will probably come down to: 

This query might return zero rows, a single row or millions of rows, depending on the value of . The first time you run this query (without ), SQL Server generates a query plan based on the value of the variable for that execution. Suppose that value of returns a single row, the query can afford an execution plan that takes 0.1 seconds for each row it returns, completing your query in 0.1 seconds total. Now, if you apply that same plan to another value of where the query returns 1000 rows, that query is going to run for minutes and you would probably be better off with another plan, perhaps with different joins, aggregates, etc. When you use , SQL Server will evaluate the query (and the variable values) every time it runs and recreate the execution plan every time. This will add a few milliseconds to each execution (to generate the plan) but the resulting plan will probably be better suited for your particular value of . If you're fine with an execution time of two seconds, this won't be a problem. However, if your query needs to complete in milliseconds, I would start tuning indexes, updating statistics, etc. In summary, yes, this sounds a lot like a parameter sniffing problem, and appears to be a good solution in your scenario. I can't make any more qualified guesses without looking at query plans, tables and indexes, statistics and the different parameter values. Hope that helps.