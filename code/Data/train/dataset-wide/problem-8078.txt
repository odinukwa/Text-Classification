I'll mention WL Burke's "Applied Differential Geometry." It's written for physicists, it will not be to the liking of the majority of mathematicians, but it changed this engineer's view of geometric methods forever. The book changed the direction of my research because it presented a point of view that is not readily accessible if you follow the control and optimization literature. Becoming familiar with the differential geometry literature is an investment that a controls person is unlikely to make without a general idea of where the complete set of tools leads to. In this sense, the mathematics literature can present an obstacle. Burke's exposition is intuitive, though quite informal, and led me to read Spivak, Milnor, and other books, some mentioned here, which I would not have read if I had started with the math literature. 

Let $N$ be the solution to a linear differential equation $\dot{N}(t) = A(t) N(t)$, with appropriate initial conditions. Then $N$ has the property you desire, but it cannot be represented as an exponential unless $A$ is independent of $t$. You can find this and more in books on linear system theory such as Jack Rugh's ($URL$ I am not sure but I believe it is true that the converse also holds - under suitable technical conditions, if $N$ has the one-parameter semigroup property then it can be represented as the solution to a linear differential equation. 

No easy answers. The paper "Guaranteed Margins for LQG Regulators" by John Doyle has the immortal abstract "There are none". J. C. Doyle, “Guaranteed Margins for Lqg Regulators,” IEEE Transactions on Automatic Control, vol. 23, no. 4, pp. 756–757, 1978. 

Probably not without extra assumptions. If the eigenvalues of $A$ are fast, the integral for $P_m$ (in whose expression I believe a factor 2 is missing) will be weighted towards its values when $t$ is small, and $e^{-\alpha t}$ is close to one. So you can't have an expression that depends on $\alpha$ only. 

I suppose you mean du/ds in the last equation. There is one problem with reparametrizing the characteristic curves: the vector field f is zero at the fixed point, so you are dividing by ||f(x)|| which tends to zero (assuming suitable smoothness of f). So either you consider only a region that doesn't include the fixed point, or you have to be careful with continuity to ensure that all constructions exist. Apart from that you may reparametrize the characteristic curves as you see fit. Notice that this doesn't solve the original 1st order partial differential equation, because to achieve that you still need to compute all solutions to the dynamical system determined by f. 

The randomized algorithm most familiar to newspaper readers is the electoral poll. Polls select a very small fraction of voters and manage to predict the result of an election within a few percent. The hard part is how to sample randomly. But f irst show an example such as finding a volume or computing $\pi$, to avoid getting caught in a political argument. 

Heaviside's operational calculus, used by electrical engineers to work with differential equations, predates its mathematically accepted justification by decades. The same can be said about Dirac's delta function, which is used together with it. Of course, to some extent the operational calculus is a repackaging of the Laplace transform, but that is not all there is to it. One might argue that in this case mathematicians' splendid isolation worked the in the opposite direction. 

Control theory contains a vast literature on the Kalman filter, which deals with estimation in linear systems of the form $x_{k+1} = A x_k + B u_k; y_k = C x_k + D v_k$. Here $x$ is a state vector, $u$ and $v$ are random variables, and $A, B, C$ and $D$ are matrices of appropriate dimensions. Take a standard reference on Kalman filters, pick the matrices 0 or the identity as needed, and you will most likely find all results that you need. There are almost too many good books on linear estimation to mention, at all levels of mathematical depth and engineering intuition (or lack thereof). 

Descriptive geometry used to be taught to engineers, not so often now that we have computer drawing software. The idea is to project 3D objects onto TWO half-planes, then flatten the half planes into a sheet of paper. There is a redundant dimension in the representation. This can be exploited to visualize 4 dimensions: simply project a 4D object onto 2 planes. Descriptive geometry with both half-planes independent. One can go up to 6 dimensions by projecting onto 3 planes, or onto 2 volumes. The trick does help somewhat. 

Control theory is full of examples where, to achieve a certain goal, it is necessary to apply controls which are continuous by parts, and that result in solutions of the type you want to the differential equations involved. A practical situation is parking a car. You can drive down the street by steering and accelerating continuously. In contrast, parallel parking requires a sequence of back-and-forth, stop-and-go, steer-and-straighten maneuvers that are essentially discontinuous. The position and orientation of the car are continuous, but their derivatives are not. (This requirement is a consequence of the nonholonomic nature of the nonlinear constraints on the car's dynamics. The proof is a beyond the usual calculus content, but the phenomenon is easier to grasp than my previous examples.) 

In control theory, simultaneous stabilization of 3 or more systems is undecidable (necessary and sufficient conditions for simultaneous stabilization of 2 systems are known). This and other stabilization problems are discussed in Blondel and Tsitsiklis, A survey of computational complexity results in systems and control, Automatica, 2000, vol36 n9 p1249--1274, and its references. 

You are interested in what is called direct, or derivative-free optimization. The algorithm can only use a "zero-order oracle" which, when asked, supplies the value of the function at a point. It used to be a not-very-popular subject with a few ad hoc algorithms. The title of a 1996 paper by Margaret Wright, "Direct Search Methods: Once Scorned, Now Respectable" says a lot. It is available at $URL$ A recent book which gives a readable overview of the field is $URL$ Another book that mentions direct optimization is $URL$ 

Coordinates on a manifold do not have an immediate metric meaning. Until becoming familiar with differential geometry one tends to think they do. (Einstein wrote that he took seven years to free himself from this idea.) For example, linear control theory is for the most part metric with variables in $R^n$. When moving away from linear control theory, variables are represented as coordinates on a manifold. Nevertheless, much of the literature tends to either abandon metric notions altogether, or to keep using an Euclidean metric though it is no longer very useful. 

Controllability is an algebraic yes-or-no question: if your linear time-invariant system is controllable, then it is controllable in any finite interval, no matter how small. Its study doesn't require any extra geometric structure of the state space besides what is needed to define differential equations on it. If you want to discus the compromise between convergence and size of the controls, you need extra structure: a metric on how far the states and the controls are from a desired value. This is done in optimal control theory. Linear-quadratic optimal control for example uses quadratic distances (costs). That is to say, it introduces an Euclidean geometry to the state space. Other geometries and costs are possible. Quadratic costs are simply the easiest ones to deal with. Pick you favorite cost, and search the extensive literature - someone is bound to have obtained useful results. 

A non-integer delay is infinite-dimensional. It can be implemented in an apparatus with a different sampling time. Also, continuous-time transfer functions can be implemented with analog devices, and may not have finite implementations with a fixed sample time. 

Recipe: Find an equilibrium point. Linearize the dynamical system around it. Find a quadratic Lyapunov function for the linearized system. Compute its rate of change along the trajectories of the original nonlinear system. A compact region where the rate of change is negative is inside the basin of attraction of the equilibrium point. Many things can go wrong. Maybe the linearization is not asymptotically stable. Maybe it is, but the basin of attraction computed is too small to be useful. Then you need to search for Lyapunov functions that are not quadratic. That is a harder task, and doesn't lend itself to simple recipes as above. But is is a beginning. This is called "Lyapunov's indirect method" to study stability, because it is based on linearization. It is described in many dynamical systems and nonlinear controls textbooks. 

A very interesting practical application is the problem of state estimation - for linear systems the answer is called the Kalman filter. Given a vector field $\dot{x} = a(x,v)$ and a measurement equation $y=c(x,w)$, compute the initial condition $x(t_0)$, the perturbation $v(t)$, and the measurement error $w(t)$ that minimize a cost function $J$. The cost is usually expressed as an integral over time of some function of $v$ and $w$. Using Pontryagin's maximum principle or Bellman's dynamic programming, one arrives at a HJ equation which is used to find $v$. The additional step needed is to determine $x(t_0)$. It is a static minimization problem, which however needs to be repeated at each instant $t$ in the interval of interest. This is not a very practical answer. For linear systems with quadratic costs, the Kalman filter provides a recursive solution to the complete problem. In more general cases, the problem is much less studied either by engineers or by mathematicians. This is unlike the optimal control problem which has been studied extensively. I think the geometry of the solutions is crucial. My understanding is that the filter equation is a particular symmetry of the Hamilton-Jacobi-Bellman partial differential equation - at least when everything is smooth. Meanwhile, the Hamiltonian vector field is a characteristic of the partial differential equation - also a particular symmetry, but not the one that gives a recursive solution to the estimation problem. 

If you want to learn the mathematical theory of sensor fusion I strongly recommend you invent it. There are good engineering books, including the ones cited in the answers, on several different aspects of sensor fusion. A mathematical theory doesn't exist yet. 

A smooth approximation is $f(x) = -\frac{1}{\rho}\log \sum_i e^{-\rho x_i} $. The larger $\rho>0$, the closer the approximation is to the minimum. 

The most convenient algebraic test for stability of a matrix $A$ known to humankind is to pick an arbitrary $Q>0$ and solve the matrix Lyapunov equation $A^T P + P A + Q = 0$. All eigenvalues of $A$ have negative real part if and only if $P>0$. 

Shock waves are discontinuous solutions to many partial differential equations. The literature is large. I was going to add Brownian motion as another ubiquitous example but I realize that you want functions that do have derivatives almost everywhere. 

One method to optimize the integral $$\int_{\mathcal T} L(t,x,\dot{x}) \; dt $$ of a functional over a curve is the calculus of variations, which leads to ordinary differential equations: the Euler-Lagrange equation $$-\frac{d}{dt} L_{\dot{x}}(t,x(t),\dot{x}(t)) + L_x(t,x(t),\dot{x}(t)) =0$$ or its generalization, Pontryagin's maximum principle. An alternative is Bellman's optimality principle, which leads to Hamilton-Jacobi-Bellman partial differential equations. Each of the methods has advantages and disadvantages depending on the application, and there are numerous technical differences between them, but in the cases when both are applicable the answers are broadly similar. The calculus of variations can also be used to optimize a functional $$\int_{\mathcal X} L(x,u,p) \; dx $$ integrated over a multidimensional space. The resulting Euler-Lagrange equations $$-\frac{\partial}{\partial x} L_{p}(x,u(x),p(x)) + L_u(x,u(x),p(x))$$ are partial differential equations with the space coordinates as independent variables. Is an alternative approach using value functions, leading to optimality conditions along the lines of Bellman's optimality principle, known? 

In "On the Gap Between Deterministic and Stochastic Ordinary Differential Equations," The Annals of Probability, Vol. 6, No. 1 (Feb., 1978), pp. 19-41, Hector J. Sussmann showed that a stochastic differential equation can be solved by simply solving, for each sample path of the process, the corresponding non stochastic ordinary differential equation, and that for the particular case of a Wiener process, the solution obtained turns out to be the solution in the sense of Stratonovich. The paper observes that the results are not valid for equations with several stochastic inputs, because of commutativity issues. My question is: has the problem been solved for the multi-input case? Do comparable results exist? 

Turing's limit is logic - it speaks about static data if you will. Shannon's has to do with data transmission - movement, so to speak. There is some thought of combining logic with communication in the context of control theory. Turing's and Shannon's theorems appear in hybrid control theory to be precise - the branch that studies dynamic controls together with logic and switching. Nevertheless, they seem to be fundamentally different facts. 

As far as I know a multidimensional version of Bellman's principle of optimality has not been found. The papers suggested in the answers above all refer to one-dimensional independent variables, or to cases that can be reduced, by introducing integrability assumptions, to optimization of a functional with a one-dimensional independent variable. Finding a multivariable version of the dynamic programming method may be an open problem. Until it is solved, the classical Euler-Lagrange equations are the method we have. 

Even more visual, even less formal, is "Dynamics, the Geometry of Behavior," by Ralph Abraham and Chris Shaw. I find the approach very useful for a difficult subject, however it needs to be supplemented with more rigorous material. A digital edition can be purchased through Aerial Press $URL$ 

No, it does not hold in general. The input $\tilde{x}$ may bring the state $x$ outside the domain of stability $\mathcal{D}$ before converging to zero. More generally, the system may be null-input stable but its gain may not be finite. You are looking into input-to-state stability questions. The usual treatment involves the use of Lyapunov and Lyapunov-like functions. The literature is extensive because there are many many different cases that have to be treated separately. The concept was studied extensively by Eduardo Sontag. You may want to look at his page $URL$ for references. Lars Grüne has notes online $URL$ 

In control theory we often wish to find a feedback control $u$ to stabilize a given linear system $\dot{x} = A x + B u, y= Cx$. The problem of linear adaptive control consists in constructing such a controller, using measurements of $y$ only, without precise a priori knowledge about the matrices $A$, $B$, and $C$. During the 1970s and 1980s several adaptive control algorithms appeared, under restrictive assumptions on the matrices. Notably the transfer function $c (sI - A)^{-1}b$, in the single-input, single-output case, was required to be minimum-phase (have stable zeroes). It was thought that some of those assumptions were indeed necessary. In 1986 Bengt Mårtensson in his Lund PhD thesis "Adaptive Stabilization" showed that essentially all one needs to know are the dimensions of the matrices. For effective practical algorithms of course more a priori information is crucial. This discovery of "universal stabilizers" came as a great surprise to the adaptive control community. The techniques used, involving switching and dense search, were also rather unexpected.