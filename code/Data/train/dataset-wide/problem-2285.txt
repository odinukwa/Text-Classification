Basically I'm looking for definitions of what ROWID and KEY mean within the context of Pstart and Pstop. 

There is an old database that was upgraded from Oracle 10 to 11G R1. In Oracle 10 we had disabled bind peeking. Since Oracle 11 has adaptive cursor sharing, we want to turn bind peeking back on. When we did this we found that many queries went much much faster. Unfortunately, one critical query got very slow. As soon as we turn off bind peeking, the one query gets fast again but everything else goes back to being sluggish. The question is: In Oracle 11, what would cause bind peeking to make a query slow? I thought adaptive cursor sharing was supposed to take care of the bad bind variable peeks problem. 

First of all you shouldn't provide instance level permissions to your SQL server service account and database level permission, this goes against all security best practises. when you assign the service account through SQL server configuration manager SQL will provide the user with the necessary permissions to run the SQL server service. so I would first of all remove all the permissions you have given, and to the issue you are having, the service account need to be given the local security policy "Log on as a service", you will find this under local security policy under user rights assignment. you can get to the local security policy by under windows run box write secpol.msc P.S , also you should realise that when 

I eventually got the answer from Oracle support. In a word: statistics. Short answer: giving the optimizer more information (with bind peeking) resulted in a poor execution plan because it based its estimates on out of date statistics. Long answer: assume a table with various columns including a date column. When bind peeking is turned off and I have a query like "select * where date >= :d1 and date <:d2", Oracle doesn't peek at the value of :d1 and :d2. It just makes its best guess and estimates the row count. In our case this led to estimates which were "good enough". If bind peeking is turned on, Oracle looks at the value of :d1 and d:2 and then examines the statistics including the low/high values for the index on the date column. If the values of :d1 and :d2 are outside of the range given in the low/high values then it estimates a row count of 1. In other words, the out-of-date statistics report that low value for the column might be "1/1/2010" and the high value was "10/1/2013", but the date range in the query was "10/20/2013-10/25/2013". Oracle estimates 1 row is returned rather than the actual 50000, and a poor plan is chosen. 

I see everyone has jumped away from the obvious, the query result only show 2 files, it should show 3 files, 2 .mdf files and the .ldf , what has occurred here I think is the OP has run the query in the context of master database or in another database context hence showing the incorrect results. this query need to be run under the context of the database you are looking to get the results from as the query refers to dbo.sysfiles. put use [database_name] where database_name is the database which you are interested in finding the space used and available. 

You can use SQLCMD Mode in SSMS with :CONNECT and servername\instance to connect to particular server and run your command. Navigate to SQLCOMD Mode:Open New Query > SQLCMD Mode just type in query in following format if you have multiple servers to connect and take backup. 

After reviewing your error. I tried to setup in my TEST environment and I have received an error. If you setup connection from Excel and after selecting specific tables to test go to connection properties. You will be able yo see connection string. Try to verify it with your query and see if it is changing any parameters in connection string from your query. 

Just on a quick glance with out doing much analysis I would change t_transaction_type to only have a credit and debit and convert the t_transction_type table to be a payment_type or some other meaningful name. I would also avoid using t_ , table name should just have a meaningful description and adding the prefix makes it hard to manage at the database level.(i.e. intellisense here would show all your tables since they all start with t_) 

you have given the user sysadmin, which basically has all rights under the sun in SQL server world. I would recommend you spend some time reading BOL Fixed server role sections to understand what each fixed server role does. 

Table 1 has 5 columns. Table 2 has the same 5 columns, plus an additional non-nullable column. I would like to use datapump to copy the data from Table 1 to Table 2. In Sql Server, I can use bcp and specify a query to be used on export, in which I can "SELECT *, 'New Data'" from Table 1. This has the effect of putting the string 'New Data' into my new column in Table 2 when I import. How would I accomplish this using Oracle's Datapump? From what I've read, I can only specify a WHERE clause, which means I can't add 'New Data' to my new column. So far the only idea I've come up with is to disable the not null constraint on the new column in Table 2, datapump the data over from Table 1, and then update all rows to add 'New Data' into the new column. Is that the only option I have? 

No cursors, No need to use a template, multiple DBs created provided you have the dbnames in a table 

I think your best option here is to pick the tables which are most important to you and look at rebuild/reorganize indexes on those tables first, you can put it into a separate job and run this regularly, and do the other tables on a different schedule. by the way why are you adding and removing .ndf files, can't you just drop the tables and re-create them ? and in that process remove/re-create the indexes ? more context around your process would be nice so we can understand and see if we could help you improve these processes. 

I am debugging poor query performance for my application and recently came across the SQLT tool. The XTRXEC method combines XTRACT and XECUTE and it seems like this provides more info then XTRACT alone. However, one of the statements that I want to investigate is an Insert. Is it safe to run XTRXEC in a live production environment or will the XECUTE try to actually do the insert? If that's the case is it safe to do the XTRACT in production? 

I'm attempting to tune a query and I want to be able to tell if partition pruning is happening. On one part of the query, the I'm getting a Pstart and Pstop value = "KEY": 

On your primary do you have a transaction log backup job running ? what would be happening is this db is larger by the time restores are completed the primary db would've had another transaction log backup taken, you just need to copy this transaction log backup to the DR and restore and mirroring should initialise. other option is if you have enough disk space on your log drive you can disable log backups on the primary until you complete mirroring setup and re-enable it. hope this help.