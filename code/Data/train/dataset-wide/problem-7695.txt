The answer is no. First, to understand the question, WLOG $f$ is symmetric and positive definite; a general $f$ has a polar decomposition $f = os$ and the orthogonal factor $o$ has no effect on any of the norms in question. Then, WLOG $f$ is a rank 1 projection. The second and subsequent eigenvalues of $f$ do not increase $||f||$, but they could increase $||f(x)||$ for some specific $x$. So in summary, we can assume that $f = vv^T$ for some vector $v$. The question is whether $v$ must always make a small angle with some binary vector. Let $$v = (1,\frac{1}{\sqrt{2}},\frac{1}{\sqrt{3}},\ldots,\frac{1}{\sqrt{m}}).$$ If $w$ is a binary vector of weight $k$, then $|v \cdot w|$ is maximized when the non-zero entries of $w$ are at the beginning. However, $$||w|| = \sqrt{k} \qquad ||v|| = \Theta(\sqrt{\log m}) \qquad |v \cdot w| = O(\sqrt{k}).$$ This means that the angle between $w$ and $v$ is large, and therefore $||f(w)|| = ||v (v \cdot w)||$ is small compared to $||f||\;||w|| = ||v||^2 ||w||$. The same proof works if $\{0,1\}$ is replaced by $\{-1,0,1\}$, or indeed by any finite subset of $\mathbb{R}$. On the other hand, there is a variation of the question with a positive answer. Similar to Pietro Majer's remark, you can interpret the question as a comparison between two norms on $\mathbb{R}^m$. One is the $\ell^2$ norm, and the other is the norm whose unit ball is a polytope whose vertices are at the points in $S = \{0,1\}^m$ and its negative. By the theory of spherical packings on a sphere, for any $c < 1$, there exists a set $S$ of exponential size in $m$ such that the two norms are equal up to a factor of $c$. This is then a positive answer for that sample set of vector, even for constants close to 1. But such a set (coming from the centers of a sphere covering of the sphere) has to be fairly complicated, and I don't know if there are explicit asymptotic examples. 

If I understand your notation correctly, then your question is a bit confused, because $g^N$ has to be a symmetric matrix, so that "$***$" = "$*$". The condition that $g^N$ is block diagonal does not have to hold; it says that the tangent vector of the last coordinate, $\partial/\partial u^{m+1}$, is perpendicular to the surface $M$. On the other hand, there always exist local coordinates with this property. If you take any local coordinates for $M$, you can evolve them for a short time with the normal surface flow. You can even get the condition $B = 1$ in a local chart. Also, there certainly is another way to get the covariant derivative on $M$ and its Christoffel symbol. Namely, if you apply the covariant derivative $\nabla^N$ to a tangent vector field $v$ on $M$ in some tangent direction $w$, you get a vector field $\nabla^N_w(v)$ on $M$ that does not have to be tangent. You should then just project this derivative $\nabla^N_w(v)$ orthogonally onto the tangent bundle $TM$. The orthogonal projection is a useful tensor field $P$ defined on the tangent bundle $TN$ restricted to $M$, and you can write an explicit expression for the covariant derivative $\nabla^M$, or the Christoffel symbol or even the curvature tensor, in terms of $\nabla^N$ and this tensor field $P$. Actually, I am not entirely sure that this method is algebraically all that different, but it is at least conceptually different. 

You can show that a cobordism class goes to a homotopy class with basically the same construction in the next dimension. So you get a well-defined homomorphism. It takes more work to show that it's an isomorphism, but I think that it can be done with similar geometric considerations. For instance, to show that it is surjective, consider a generic smooth map from $S^{k+n}$ to $S^k$. The inverse image of $0 \in \mathbb{R}^k \subset S^k$ is a manifold, a small neighborhood of $0$ pulls back to a framed normal bundle $N$, and (roughly speaking) everything outside of $N$ can be crammed into $\infty$ with a homotopy. 

On the contrary, there are two major results in complexity theory that rule out a wide class of methods to show that $P \ne NP$. The first is the theorem of Baker, Gill, and Solovay, that a proof that $P \ne NP$ (or a proof that they are equal) cannot relativize. In other words, they showed that there exists an oracle relative to which they are equal, and an oracle relative to which they are different. The second result is the theorem of Razborov and Rudich, that if a widely accepted refinement of the $P \ne NP$ conjecture is true, then there does not exist a "natural proof" that they are different. By a natural proof, they mean a proof from a large class of combinatorial constructions. In light of those two theorems, there actually aren't very many known promising techniques left, even though there is a lot of evidence by example that the conjecture seems to be true. As Razborov and Rudich explain, these two results rule out candidate approaches to P vs NP for sort-of opposite reasons. There is a CS professor named Ketan Mulmuley who has expressed some optimism that P vs NP can be solved with "geometric complexity theory". I can believe that Mulmuley is doing interesting work of some kind (which seems to involve quantum algebra and representation theory), but I haven't heard of many complexity theorists who are optimistic along with him that he can really solve P vs NP. (But hey, Perelman surprised everyone with a proof of the Poincare conjecture, so who knows.) 

Something is wrong in the statement of the question. If $X$ has four points, then a measure on $X$ is a list of four non-negative numbers that sum to 1. But if you take the plane subtended by $(1,0,1,0)$, $(1,0,0,1)$, and $(0,1,1,0)$, then it also contains $(0,1,0,1)$, and these four measures are the four extremal ones in this plane. So the assertion, as stated, does not hold in the finite case after all. The space of all measures is a simplex when $X$ is finite; all I'm then saying is that you can intersect a plane with a tetrahedron to get a square. The geometry also shows you that the counterexample is stable with respect to small variations of the numbers. The only condition that comes to mind to make it true is to say that the measures have disjoint support. Then it's easy to show that they subtend a simplex of measures in general. 

Thanks to HJRW2 for the flattering invitation here, and I will give an answer, but it might be not all that deep. In fact I haven't been on MO much lately; maybe I should visit it more. I don't see any basis to say that Lackenby's result proves the mixing property of the quantum mixing proposal. There are many graphs where the diameter is better understood than the mixing time. Lackenby's methods are based on Dynnikov monotonicity for grid diagrams, and for that reason and others there isn't any statement about getting from here to there with random moves. The picture is even worse for other knot types, since the exponent in Lackenby's polynomial bound depends on the knot type. Nonetheless you can fairly say that it lends some credence to the mixing time question, at least for the unknot. Lackenby's bounds are spectacular, but no one thinks that they are sharp. Moreover no one knows how to make examples of the unknotting problem that look hard, i.e., nothing like the RSA factoring challenges with money at stake. An algorithm with a rigorous bound on running time is hard, but that's not the same thing. The graph isomorphism problem was in the same state of deadlock before Babai's algorithm. On the other hand the quantum money from knots protocol would need more credibility than just the mixing time proposal for people to trust it. It's perfectly respectable as a provocative proposal, but I don't know that it's convicing, at least not yet. There is another quantum money proposal due to Aaronson and Christiano that could be more convincing; on the other hand it's less glamorous in that it uses linear algebra over finite fields rather than knot theory. 

Really the closest that you can get is Wikipedia or the right kind of search in Google Scholar. The community needs tools to establish or recognize consensus, which of course is an open-ended problem. These tools, while they are certainly far from perfect, are the best tools that exist. If you did embark on a project to document standards, that could be a great thing to do, but it would probably eventually be co-opted by Wikipedia. When I think about quantum algebra, a topic which is notorious for "notation sprawl", I use Wikipedia and Google Scholar. The more traditional method is follow a few respected papers and textbooks, and this is also still reasonable.