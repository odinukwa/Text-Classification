Where are these files likely to have have come/why weren't they eventually expired and deleted like the rest of the archivelogs? Why does crosscheck archivelog all not detect them and add them back? is it because there is a 4 month gap between them and the next most recent files? Is it safe to just delete these files off disk? 

My understanding from the documentation is that this is normal on the first run (when there is no datafile image copy), and normal on the 2nd run (when there is a datafile copy, but no incrementals), but on the 3rd and subsequent run there should always be a datafile copy and an incremental to apply to it. This has now been failing for the last 6 nights. From $URL$ 

You might have to explicitly cast the first row of the with the types of and if PostgreSQL doesn’t figure the correct ones out. E.g. 

The complete list of characters in need of a special treatment is at the above link. However, it is best to have the intermediate language deal with such problems. For instance, in PHP you would call ; other platforms and languages provide similar facilities. 

which outputs the SQL definition of the table or view, even with the original comments (exactly the same as , with less typing. works too.) 

However, the example you posted does not match the column names you gave later. This means you are not posting your actual code, and this makes everybody’s job harder. Maybe this is only a toy example to learn how to use MySQL triggers. As such, it’s fine. In production, the very idea to change the price in the main table upon insertion of a new promotion is questionable. A better design would be not to change the table, add and columns to the table, and read the current prices either from a view which applies the active promotions for the day, or, if you have millions of products and performance becomes an issue, from a temporary table regenerated each day (and possibly replicated to slave servers... you know your scale). 

If I run a crosscheck archivelog all, this picks up only the recent archivelog files that I expect to be there: 

I have an Oracle Reports server 10g (10.1.0.4.2) that for the last few years has been running okay with the rwclient.sh client application residing on the same host. Due to some performance issues caused by long running/heavy reports we are looking at trying to separate the reports server from the application server. From memory rwserver.sh should be able to accept connections from a client on the same LAN, but not on the rwservers localhost (but I'm going back many years since I saw that sort of setup and I was just a developer, not an admin, so I could be misremembering it). When I try and submit a report across the network I get: 

Well, it all depends on your scenario. If it is a one-off import, you create a junction table and use it for your joins. 

You should read about string literals in MySQL. There you’ll learn that a backslash character is used to escape some special strings, and to have literal backslashes in your strings (as in Windows pathnames), you have to double them: 

To learn all you want to know (and what you’d rather not know) about sequences in PostgreSQL, read the docs. 

The safest way is to define product_id's with the finest reasonable granularity, to the point that if you decide to sell half the stock at a discounted price, you should define a new product_id for the items in promotion, all things being equal but the price. In this way you will manage to balance sales and returns without too many corrections. So I basically agree with blobbles's answer. You will have many ways to group your products together, e.g. same name, same size, same producer, same provider, and so on, and you will do that in separate tables associating the product_id with those features. 

I have two 12.1 databases on the same server that I've set up with rman and have been running okay for the last year or so (with numerous restores to a seperate test server - so at least restore/recovery wise rman is set up okay.). These databases are both around 100G in size, with the recovery area (originally) sized at around 200G. (Currently bumped up to 300+G to give a little more lee-way time whilst I work out what's going wrong). Just recently one of the databases has started filling up the db_recovery_area with backup pieces - normally this has been fairly static increasing over the day and then dropping back to zero when I run the rman level 1 backup over night (or level 0 over the weekend). However, over the last couple of weeks, the recovery space has trended upwards eventually getting to the point where I need to delete all backups/archivelogs out of rman to avoid a production database freezing due to lack of space. There's been no configuration change or upgrades on this server for some time and I can't work why the behaviour has changed - but only for one of the databases. Looking at V$RECOVERY_AREA_USAGE the only thing consuming a substantial amount of space is the backup pieces. 

(Edited to be consistent with new version of question) You cannot if you want to single elements of the lists which make up the field. You have to disaggregate those lists in a subselect: 

Having such junction table might improve performance, and it allows creating indexes. If the junction data is dynamic in such a way that keeping a junction table in sync is too difficult (a very unusual situation, I’d say), we can avoid the junction table but, at the very least, using instead of arrays is way simpler: 

However, this is an interface question, not a DB engine question, and it is rather strange. I suspect the interface settings have been changed from the ones used when asking the first question, because now the strings which the interface thinks are binary are automatically displayed as . Giving the first query to a commandline interface does not produce the results shown here. To get them from a commandline interface I have to write the second field as ; similarly, the supposedly wrong result of the second query is simply instead of because the interface does not know it is actually readable text. Casting it to should work. Please be aware that all those conversions the interface is forcing you to perform are not needed when data is exchanged between different parts of your application. 

I've enabled tracing on the reports server and can see requests on the server coming from the external clients (i think), but I only get one line per report run attempt. (But consistently get this same line so I think the rwclient is at least seeing the rwserver): 

Is it actually even possible to connect to the reports server from external clients? and if so; Where abouts do I need to configure the reports server so it listens for external connections? 

As mentioned, this is the same oracle home, same server, same disks etc - the two databases should be performing at least roughly the same I think. Any know what could be causing this? Or where I should be looking to see what could be causing the slow down? Thanks 

To resume the situation for people who don’t want to follow the link to the original question on SO: the OP is querying a Mysql DB via an unspecified interface which (sensibly) refuses to show the values stored in fields with encrypted data, and (rather dumb-mindedly) still shows a label for the value returned by calling on them. The answer given on SO was to cast those results as , which from a DB perspective makes little sense, but in this way the interface shows the results as text and everybody is happy. The question here is: if I have to use , how can I handle longer values? My answer is: you don’t say which interface you are using (your screenshots are not enough for me to recognize it), but I’d bet that if you cast as instead of it still works and you have no size limits that you should care of. So: