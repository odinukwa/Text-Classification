How often you need to run index maintenance/rebuild stats depends on your database load, specifically how often your data is modified (i.e. //). If you're modifying data all over the show (i.e. a staging table for a weekly batch process), you probably want to update stats/reorganize indexes nightly. If your data is rather more static you can probably make it a weekly or fortnightly schedule. 

You can do this (assuming I'm understanding you correctly) via having foreign keys in your child tables referring to the parent table. 

For Aurora Postgres, there's two relevant cluster-level parameters (note they're not instance-level parameters): and . I haven't tested this myself but you should be able to modify them in the usual way using DB Parameter Groups. 

First: This isn't easy to implement and it's going to break very easily (by which I mean performance is going to be horrible, and it's a nice little route for SQL injection attacks if you're not very very careful). I strongly advise you to re-think what you're doing because there has to be a better way. Second: The question as it stands doesn't actually make sense - what condition are you checking on the columns? ? Also, do you want to or the conditions together? To actually answer the question, though, to do this you'll need to write some dynamic SQL: 

There's no server-level 'read any database' permission and server-level roles can't be granted database-level permissions So yes, you will have to map users to databases individually. If you're using Active Directory, you can create a windows group, then give that group a login to SQL Server, then apply db_datareader in all databases for that group (you'll need to create users in each database though). 

I know I just spent this entire post detailing why EAV is a terrible idea in most cases - but there are a few cases where it's needed/unavoidable. however, most of the time (including the example above), it's going to be far more hassle than it's worth. If you have a requirement for wide support of EAV-type data input, you should look at storing them in a key-value system, e.g. Hadoop/HBase, CouchDB, MongoDB, Cassandra, BerkeleyDB. 

You need three tables - , and an associative table for the many-to-many relation between them (a can have many s and an will have many s), call it or something. and will have autoincrement fields as a surrogate primary key, will have FKs to both and , like so: 

First things first: MS Access was not designed for multi-user access. Every version of Access I've used had a disturbing habit of corrupting tables at a vastly increased frequency if there were >1 users using it. If the two users are connected to the Internet all the time, I'd recommend shifting your table storage to SQL Server and having the users connect to that (use a VPN or some other form of security! If they're on a company LAN it's even better, you shouldn't need a VPN then). It's a fairly straightforward process to convert to SQL Server. The users will still use the Access front end, but instead of having the tables stored inside the .accdb file and having to merge them, the Access tables are converted to linked tables to the SQL Server tables. This is possibly a bit more up-front work, but it'll save you hassle down the road (how often do you need to merge? who's going to do the merging?). Also, if the application ever gets more widely used, you can easily build another front end (in C#, Java, ASP.NET, whatever) and connect it to your SQL Server back end. 

Can anyone tell me why Instance 2 is not using the memory it has been allocated, and how best to troubleshoot? 

It's really not THAT different to Service Pack patching whereby you'll need to do passive nodes first, then give cluster resource group ownership to one of the upgraded nodes, in order to finally upgrade the active node. Map out your upgrade sequence first and test it if you have the luxury of a dev environment. In place upgrades are supported and the information is already out there from Microsoft. See here for example. You'll probably want to perform what's called a rolling update, which can be best explained by the following paragraph from the article: 

I have a SQL Server (2014 SP2) with a linked server to an Oracle 11 database. I have a very simple select statement to an Oracle view which I know should return around 140k rows. But here's the thing, when I run it in SQL Server I immediately see records in the results window, but all of a sudden, the query hangs when it has so far returned only around 2000 rows and it just sits there forever doing nothing else. Sometimes, another few thousands rows appear before it again hangs. The wait state on the query is on OLEDB which is to be expected. I'm thinking some sort of Network bottleneck...? Sometimes I do get an error after a while: Cannot fetch a row from OLE DB provider "OraOLEDB.Oracle" Before anyone suggests it, 'allow in process' is ticked on the provider settings. I'm looking for ideas to troubleshoot this at either the SQL Server side or the Oracle side to check that it's not a database config issue. 

Firstly, forgive my poor question title. I couldn't think of the best way to summarise this. Essentially I'm putting together some scenario based DR documentation, with this specific scenario being a SAN issue where we have lost disks which hold either log or data files. I'm assuming that MASTER has been affected (ie. the drive where it's files exist isn't accessible) and SQL services won't therefore start. My specific questions are as follows: 

You can not perform a downgrade in the same way SQL Server allows you to do an in-place upgrade, so you're stuck with having to manually move your database(s) between two installs. If you've got a named instance, you'll need to back it up, perform an uninstall of the Enterprise edition, and a fresh install of Standard - restoring the databases thereafter. Of course that's a basic theory, and it's not always that simple. Are you using any Enterprise specific features, such as partitioning, snapshots, some aspects of Always On Availability Groups, Resource / IO governance, compression...? You can see the full feature list by edition here here You can also run the following query to check for any version specific features you are currently using: 

This comes from this MSDN link found almost instantly after a quick web search. Bear in mind there are a few obvious and not-so-obvious functional reasons why you would not want to failover from Enterprise to Standard as some features are not supported in both. The idea of a clustered environment should be that you maintain integrity of the database environment upon failover, so why compromise that in any way, whether talking about support or features? 

(change table names as appropriate for Hashtags and Words) Or, slightly more complicated, get the top 10 words for all tweets mentioning a specific hashtag: 

(* I'm aware this would probably fail any security audit going, but to my mind if we've let an intruder into the server room that knows to look in the third drawer down for the unlabelled 'sa' password post-it, then we're screwed anyway.) 

is a session-level command in Sybase ASE, it's not a server-level setting (if it was a server-level setting you'd be able to alter it via ). Can you run wireshark (or something similar) on the packets being sent from the JDBC client to see if it's setting showplan on as part of the session initialization? That said - showing the plan should not affect database CPU or memory usage, the plan is generated by the query optimiser anyway. However, showing the plan will increase network utilization. 

I didn't mention the PK on , for efficiency it would be a compound PK on , but there's also an argument to be made for a seperate surrogate primary key (which I personally think is a waste of space, unless you need to allow for multiple s between one and one ). For a purely associative table in a many-to-many relationship, though, the compound primary key should work fine. Edit: The complicated part of this isn't in the relational design, it'll be in the application code, because you'll need best-match/partial matching in order to show users the groups that most closely match their interests (rather than having accidental splinter groups all over the place). And if you want to out-Facebook Facebook, you'll need a smoother and better user experience than they offer. 

Re: Shrinking. I see so many people getting their claws out at the very mention of 'shrink' and 'database' in the same sentence, so time to clear things up a little. Shrinking data is baaaaad. It literally turns your indexes into quivering shells of their former glory. Don't do it. Shrinking log should not be done routinely, but if you have a ridiculously outsized log (i.e. in one case I saw a 40GB log for a 100MB database, due to whoever set it up putting recovery model to full then never dumping the transaction log), you can shrink the log (after ) to reclaim that space without any ill effects (although, obviously, the will chew up I/O while it's running). PS: Speaking of log files, check your log file size and autogrowth settings. Small autogrowth settings can lead to underperforming log I/O (due to a poorly-explained feature in SQL Server called Virtual Log Files), particularly on batch transactions. 

Access is a perfectly fine database system for small scale individual-user apps. Here are some criteria for shifting: 

That is going to be horribly messy, since you'll need to find the single pair that's the newest prior to the . What I'd do is add another field to go with , and then when you add a new target for a staff member, before you add it check and see if there are any current targets () and if there are then set their end date to the new target start date (optionally: throw an error if there's more than one current target). So, the table turns out looking like: 

I have a weird problem I am struggling to troubleshoot. I have a development server with 18GB RAM and two SQL Server 2012 SP3 instances with @@version output: 

I've just inherited about 20 instances of SQL Server, as part of a wider acquisition project. I'm in the process of assessing performance and I don't like the way maintenance plans have been implemented. I'm seeing daily blanket index rebuilds (I can deal with this one) and also daily manual updating of statistics. Around half of the databases have been set to Auto Update Statistics = False, for reasons which are not clear other than I am told it is to reduce 'Performance Issues'... I always thought, and worked to, best practice of setting this to True and felt the Manual Update was not necessary if this setting was True. Am I wrong? Can anyone explain what the benefit would be in having this set as False, but doing a daily manual update instead? I should mention that some of the databases are highly transactional (millions of Inserts, Deletes, Updates per day) Others are low in terms of transaction rates, and some are all but read-only. There is no rhyme or reason though as to which have the Auto Update setting set to False. It appears to be a lottery. 

I've never tried this but I'm reliably informed it is 100% not possible. It is certainly not supported by Microsoft, for good reason, so why would you do it? 

I understand why of course, and I never do, mainly because I never need to. Now though, I have a database with an .mdf file that is 800GB. It was a system designed to collect data over a certain amount of time, with no retention period. A retention period of 90 days was recently placed on this data, and as such the developer has cleared about 3 years worth of data from the tables. Data and Indexes now total roughly 70GB, so now I have an .mdf file which is grossly over-sized. I want to shrink it to reclaim some of that valuable disk space. I'm planning to perform a shrink prior to rebuilding indexes and updating statistics (i.e the weekly maint. plan) I'm not breaking any DBA laws here am I? I assume this is an acceptable scenario in which to perform a SHRINK as it is a true one off? Thanks 

I'm in the process of writing up a proposal for implementing TDE on some critical tier databases. The actual process of configuring the TDE hierarchy and enabling encryption is easy, I think. However I want to make some recommendations about backups and I'm a bit confused. The last contractor here left some notes saying only certificates (i.e. the master database Certificate) needed to backed up with a .pvk private key. That seems to be corroborated across a lot of different tutorials online. Do we need to backup the Master key, and the Database Encryption key too? Is it safer to keep all these various bits in different locations? The context of the backups is around changing the documented recovery plans for these database for the guys and girls in the systems team, i.e. What to backup and monitor, how often, how to restore and in what order etc.