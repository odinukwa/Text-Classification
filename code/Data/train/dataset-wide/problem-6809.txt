This really depends on how you think about grammar. Your question sort of implies that it's primary morphosyntactic features you're after. And here, it's more likely to be the other way around. Things like dual and evidentiality seem to be disappearing as Trudgill has argued in a recent book (useful video overview here $URL$ And it is not really a question of 'modern' vs. 'ancient' but rather related to size of community and amount of contact. At least, that's Trudgill's argument. I don't subscribe to it fully but there's enough data there for the answer to the general question. The whole question of 'modern' language is really problematic. The anthropologist in me would say that it's hiding a world of technological and developmental normativities. Meaning, that it thinks that somehow there's a directionality to language development and that direction is towards our language, the natural way of being. There's a world of argument and data in anthropology to show that we need to consider even supposedly isolated communities to be modern - and I think this applies to languages as well. If you step outside of morphosyntax, however, you could talk about a lot of language uses that require certain linguistic tools. One of these is writing which requires stylistic, cohesive and other para syntactic devices not likely to be found in languages without a written culture of a certain kind (ie. it's not just about writing spoken words down). So many arguments have been made about hypotaxis being a 'modern' invention, etc. I think there's good reason to be skeptical about any such claims as absolute because of the fundamental lack of evidence from the past. However, there's probably enough there to start thinking about some broad tendencies. I'm not aware of any critical synthetic account of this, though. 

Sentence is not a very useful construct for most things you'd want to study about language. It is only an artifact of a particular kind of written language and it is really hard to provide any definition more specific than 'it starts with a capital letter and ends with a period'. Even then, the beginnings and ends of 'sentences' are fairly arbitrary. The only analytical concept I can think of where sentence is directly useful is measures of readability which look at the number of words per sentence. This can give a fairly reliable indication of the difficulty of text (providing they are long enough). Sentences are also used when dealing with concepts like subordination and anaphora accessibility but they're not really necessary for that. The two concepts to look at as much more fundamental are: 

Any advice on learning an additional language is kind of like dieting advice. It has probably worked for a few people at some point. But it generally involved unrealistic expectations about behavior and resources. This particular advice is more than a little kooky because it completely ignores communication but it would certainly take you some of the way towards learning the language. But memorizing 1,000 words is not trivial. Also, completely unfiltered radio or TV is probably not very useful without at least some initial guidance. If you really want to devote that much time and effort to learning a language, Benny, the Irish polyglot offers more comprehensive advice in the same vein on his website here: $URL$ Language learning is hard and it takes a lot of time. So whichever way you slice it, you will have to put in the hours. The internet is certainly a helpful resource for that. As to the point about adults modelling their learning after that of children, that is completely unrealistic. Children have different motivations, life experiences and generally not as fully developed cognitive strategies as adults. Giving up on those (such as by ignoring any grammar at all) is foolish. However, the biggest obstacle will be inhibitions and trying to maintain your linguistic identity. So that's certainly a lesson, you can learn from children. Simple immersion without marshaling at least some of your adult cognitive advantages would be unwise. There is also a lot of individual variation in how well different adults learn without some sort of guidance. Perhaps an even better resource is therefore Rebecca Oxford's learning strategy inventory. 

Perhaps a better way to approach the difference between functionalist and formalist approaches would be to explore the history and personalities involved. In principle, there's no opposition between functionalism and formalism. Units of language are used for certain functions and these functions can be described using formal methods. The question really is what are those functions and what are the formalisms used. Historically, functionalism was closely associated with the Prague structuralist school of linguistics (known as the Prague Circle of Linguistics - full disclosure, I'm a member). The functionalist theme developed in the 1930s (later put into in opposition to the functionalism of the Copenhagen school approach of Hjelmslev who understood function more in the mathematical sense). The Prague school was revived in the 1960s and tried to blend functionalist methods into the then increasingly popular formalist approach (inspired for instance by formal semantics of people like Montague). Formalism, on the other hand, is associated with Chomsky (although there were others and even earlier schools) who developed mathematical theories of the combinatorics of grammar combined with the claim that they are facts of language without any reference to their function. But other formal approached to language were also being developed about the same time. I already mentioned Montague, but there was also Bar-Hillel with his categorial grammar. Both of these were much more compatible with functionalist approaches in that they are concerned with formalizing language units that are put to real uses. Today, functionalism is most closely associated with the British Firthian school of linguistics - it's most famous proponent being MAK Haliday (it is now particularly popular in Australia and New Zealand). This school had the most impact on wide areas of language description from text and discourse analysis to pedagogic grammar (most foreign language textbooks and grammars will be beholden to some version of functionalism). Most of the work on language corpora has come out of the functionalist tradition (in the broadest sense). I think Haliday's three meta functions of language: 1. Ideational, 2. Interpersonal and 3. Textual are still the best delineation of what dimensions a linguistic theory needs to account for. Ultimately, the difference between formalist and functionalist linguistics is not the irreconcilable rift between linguistic theories it is thought of as. I'd suggest any one linguistic theory needs to be judged on a range of issues and whether it is labelled or labels itself as functionalist or formalist, is probably not that important. 

The only way you can really determine the lack of an equivalent lexeme in a language is through the process of translation (by a competent, experienced translator). The answer by @user6726 is absolutely correct. Let me just offer an additional perspective. First, you can only ever really compare two (or possibly a small group) languages to get a picture of alignment. Many people start with the (implicit) assumption of a cultural or linguistic default from which languages diverge. In today's context, the default is usually described by English (an assumption shared even by many non-native speakers). So you get the typical newspaper articles using the 'no word for X' trope which the Language Log rails against. It is not clear what equivalence looks like. There are countless features of the real (and social) world that are unproblematically not represented lexically in a language - e.g. names for exclusively local fauna or local customs. But there are many more, where the social and physical worlds are the same but the lexical inventory is not. I can see at least three types of misalignment: 

I could find no studies looking at the closeness of British and American dialects of English. But I would say that the question is formulated too nebulously to make it possible to answer easily. There are studies that show dialect leveling (or reduction in regional variation) within British and American English. But this is always illustrated on specific features and harder to specify in aggregate. Also, while there seems to be less dialectal variety within broader 'English' regional varieties, this has been ascribed to geographic mobility and not necessarily to media influences. While there are fewer Northern or Southern Englishes, there are still huge differences between the varieties that remain. I don't think this at all translates into leveling between British and American English. There is virtually no regional mobility (to have an impact) and the influence of global media is incredibly shallow - just observe the struggles of transplants in either direction. If media had the sort of influence everyone imagines, you would expect a lot more of merging. There are a lot of examples of mutual impact but none of them systemic. I would propose that both American and British English have been and still are developing more or less independently and will continue to diverge over time. I would suggest that literacy and media provide more of an internal cohesion for each variety than a vehicle for mutual comprehension or a future merging. 

But these findings do not necessarily imply anything straightforwardly deterministic about race and the voice (even if we were to accept an unproblematic definition of race). The key quote from the paper is this (my emphasis): 

No, the ampersand was not a letter but rather developed from a ligature between e and t in the Latin word et (and - as in etc). It has its origins in 1st century BC and seems to have been in common use since 1st century AD. You can read more about its history on the excellent blog (or book) Shady Characters: $URL$ 

Case assignment is a big problem for formal frameworks that postulate some sort of deep agentive case-like structures like theta roles in GB because whenever the morphology clashes with the deep structure. It's not too difficult to deal with it as part of the morphology module but cases like this have to be dealt with as an exception. On the other hand, it is not a problem in the least for functionalist or construction-based approaches. They simply look at it as a pattern fulfilling a function. Of course, your example 

The problem with your question is that it is built on the assumption that English is somehow the default, a natural language, from which other languages diverge. That is absolutely not the case. English is just as exotic to other languages as other languages are to it. In fact, "mapping semantic distinctions" is going to bring up all sorts of mismatches for any given pair of languages (even fairly closely related ones). The vast majority of these will concern the lexical labelling of categories. Which means that they cannot be easily resolved by lexical borrowing because they concern conceptualisation. For instance, there's no easy way to map the different ways of saying bread between English and Czech. The English category is more general and it causes troubles for Czech speakers of English because they expect a separate words for their fine grained categories. See this list of Czech words that describe something not easy to express in English: $URL$ Prepositions are also going to be hard to map well. For instance, in English you have to conceptualise tree as a container to say, someone is sitting 'in a tree' whereas a language like Czech says 'on the tree'. So you can see, it's a mutual mapping issue. However, there are also many grammatical distinctions in English that will be difficult for speakers of other languages. They might have to do with definiteness (articles) and the complex system of tenses. For instance, there's no easy way to say in Czech (and many other languages) 'I will have been cooking for 2 hours by the time you arrive.' without significant circumlocution. Conversely, Czech's aspectual system makes it easy to distinguished someone was 'shot and killed' and 'shot and not killed' without ever leaving ambiguity, whereas English always has to specify the outcome or leave it ambiguous. The list is endless. The semantics of all languages maps onto other languages only approximately once you get out of the sterilised world of logical semantics into the world of real language use.