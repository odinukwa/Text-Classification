A favorite of mine is l'Hôpital's rule. l'Hôpital paid Johann Bernoulli a retainer to keep him updated on developments in calculus and to solve problems he had. Correspondence shows that Bernoulli stated and proved the rule, which l'Hôpital then published. Heine-Borel was first published by Borel, not Heine. In fact, Heine's name was attached because he was using similar methods to solve related problems. Too bad for both of them that it was actually Dirichlet who was the first recorded to have proved it, but his notes were published posthumously and after Borel's proof. Cramer's Rule was published first by MacLaurin, and some believe MacLaurin knew the proof some 20 years before Cramer's publication. 

I'll start with a motivating example and only then proceed to the question. Consider a list of total packages of milk that were purchased on 9 consecutive days on a given store, $z_1,\ldots,z_9 = 1,0,0,2,0,1,0,1,3$ Assume I have an algorithm that predicts $\hat{z}_{10}=2$. I'm interested in assessing the confidence associated to this prediction, however, for stock management purposes, I want to measure the confidence that the predicted value is an upper bound on the true value $z\leq\hat{z}$, instead of $z=\hat{z}$. This is because what I'm ultimately after is to be confident in not running out of stock. I guess an estimate of this confidence, based only on given examples, is $1-p_{\hat{z}_{10}}$ where $p_{\hat{z}_{10}}$ is the probability of finding a value more extreme than the prediction $\hat{z}_{10}$ on the empirical distribution of all 10 observations: 

I would add: Atiyah, Michael F. (1979), Geometry of Yang–Mills fields and then the book of the same author about gauge theories: Atiyah, Michael F. (1988e), Collected works. Vol. 5 Gauge theories 

I have currently started a new research line aiming to prove a mapping between a 2-symbol Turing machine and the one dimensional Ising model. The connection is seen by recognizing that a set of symbols on the tape of the machine is indeed a configuration of a one-dimensional Ising model. The relevance of this connection relies on the fact that a limit on the entropy for information has been conjectured by Rolf Landauer in the '60. A mapping between a Turing machine and the Ising model can make this conjecture a theorem. My question here is what are the main references on Turing machines, probabilistic Turing machine and so on? Does it exist any standard, research level, literature to consider? Especially for probabilistic Turing machines, does it exist some result making them equivalent to a deterministic Turing machine and, in any case, there exists a theorem stating they can compute anything a deterministic Turing machine can? Thanks. 

Now if you treat x, y, and z as constants and consider only the variables a, b, and c, then you will end up with 3*5*7 polynomial equations in the 3*5*7 monomials 

My question is about a method described in Dr.Math forum for simplifying equations involving sums of radical functions. (The following is a transcription of the example given by Dr. Vogler): --- begin example --- We want to convert the equation 

But the determinant is a polynomial function in the entries of your matrix, which means that the determinant of A is a polynomial in x, y, and z. --- end example --- I have tested this method with several examples and it seems to work, at least in the sense that the roots of the radical equation are roots of the resulting polynomial (it seems however that the resulting polynomial may have real roots that are not roots of the original radical equation). The only method I knew about in order to get rid of radical expressions in equations was to carefully manipulate the equation and then raise both sides of the equation to the same power. This is to say I have a very basic math background. So finally, my question is: what is the name of this technique and where can I read more about it, or at least what are the keywords I can use to search google. I have many other questions regarding this technique, but I believe I good book on the subject would answer them. Just for completion, some of the questions are: 

This is a stochastic differential system of equations. I think this can help. You can also check the full syllabus of the course here with the proper references and other material to download. 

In order to iterate, you have to substitute the second equation into the first one. So, $$ n_{11}(x,z)=1+\int_{-\infty}^xdye^{-izy}u(y)\int_{-\infty}^ydy_1e^{izy_1}{\bar u}(y_1)n_{11}(y_1,z). $$ This equation is generally the starting pointing for an iterative procedure, the main tool of perturbation techniques. E.g., you can choose for the first iterate $n_{11}(x,z)=1$ and you will get $$ n_{11}(x,z)=1+\int_{-\infty}^xdye^{-izy}u(y)\int_{-\infty}^ydy_1e^{izy_1}{\bar u}(y_1) $$ $$ +\int_{-\infty}^xdye^{-izy}u(y)\int_{-\infty}^ydy_1e^{izy_1}{\bar u}(y_1)\int_{-\infty}^{y_1}dy_2e^{-izy_2}u(y_2)\int_{-\infty}^{y_2}dy_3e^{izy_3}{\bar u}(y_3)+\ldots. $$ You can stop the procedure at any desired order to get an approximation to the solution of the integral equations. Then, you put this approximation to $n_{11}$ back into the equation for $n_{12}$ and you will get an approximate solution for it at the given order. 

where A is a square matrix of height and width 3*5*7 which contains the variables x, y, and z but none of a, b, and c, and v is a column vector that lists off the 3*5*7 monomials in a, b, and c. Unless the only solution to your equations is a = b = c = 0, that means that the matrix A is not invertible, and therefore 

In other words, if you consider each monomial as a variable, then you can write these in matrix form as 

and replace every occurrence of a^3 by x, of b^5 by y, of c^7 by z. For example, the first few equations would be 

The condition on $g$ gives a definite pde for $A$. This can be seen in the following way. Let us insert the solution $f=A(x,y)e^{ig(x,y)}$ into the Helmholtz equation. We get $$ \Delta A+2i(\partial_xg\partial_xA+\partial_yg\partial_yA)+\Phi(x,y)A=0 $$ being $$ \Phi(x,y)=i\Delta g-(\partial_xg)^2-(\partial_yg)^2+a. $$ Now, assuiming $L$ is a linear operator with the Green function $LG=\delta$, one can write $$ g(x,y)=g_0(x,y)+\int_\Omega dx'dy'G(x,x';y,y')h(x',y') $$ being $Lg_0=0$. By substituting this into $\Phi$ and the equation for $A$ we get a partial differential equation to solve. For some operator $L$, the final equation could be simple to manage but, for the general case, maybe some approximation techniques could help. 

which would be $1-p_{\hat{z}_{10}}=0.9$. Assuming the above reasoning is correct (enough :), now to the question: I recently read "A Tutorial on Conformal Prediction" by Shafer and Vovk, and am curious on how to frame this problem on the Conformal Prediction framework. It appears to me the paper exclusively focus the case of estimating confidence for $z=\hat{z}$, so the question how to adapt it for the asymmetric case. If we were interested in the $z=\hat{z}$ case, a natural nonconformity measure would be $A(B,\hat{z})=|\bar{z}_B-\hat{z}|$, where $\bar{z}_B$ is the mean of $B$ (section 4.1 in the paper). That would define the following prediction regions (from the algorithm of section 4.2): $\Gamma^{0\leq\varepsilon<0.5}=\{0,1,2,3\}$ $\Gamma^{0.5\leq\varepsilon<1}=\{1,2\}$ $\Gamma^{1}=\{\}$ given that, $\alpha_0=A(\{1,2,3\},0)=\alpha_3=A(\{0,1,2\},3)=2$ $\alpha_1=A(\{0,2,3\},1)=\alpha_2=A(\{0,1,3\},2)=2/3$ $p_0=p_3=0.5$ $p_1=p_2=1$ It feels wrong to transport this reasoning for the asymmetric case, as it considers both directions (e.g. 0) as "more extreme". In particularly would mean that we have a 0.5 confidence in the $z\leq\bar{z}_{10}$ prediction, much lower than 0.9 which was given above from the empirical distribution. It appears to me that for solving this problem we have to choose a nonconformity measure $A(B,\bar{z})$ that is monotonic with regard to $\bar{z}$, for example, $A(B,\bar{z})=\bar{z}$. That would make "more extreme" asymmetric, resulting in the following prediction regions for the above example: $\Gamma^{0\leq\varepsilon<0.1}=\{0,1,2,3\}$ $\Gamma^{0.1\leq\varepsilon<0.3}=\{0,1,2\}$ $\Gamma^{0.3\leq\varepsilon<0.6}=\{0,1\}$ $\Gamma^{0.6\leq\varepsilon<1}=\{0\}$ $\Gamma^{1}=\{\}$ given that, $\alpha_i=A(B,i)=i$ $p_0=1$ $p_1=0.6$ $p_2=0.3$ $p_3=0.1$ This means that we can be at least 0.7 confident in the the $z\leq\bar{z}_{10}$ prediction. Notably it still does not match the 0.9 confidence obtained from the empirical distribution. I wonder if I am on the right track... Thanks! Marco