Additionally, not having DBO role permissions may mean. . . (Since you have so many different versions of SQL Server from 2005 - 2014, it may be best to have a small set of users test this initially to see who screams to iron out any kinks, etc.) 

You can also run this adhoc this way without SQL Agent scheduling specifying the correct DateTime variable when you want it to stop. Once the process has run, you just go to the file and open it as usual. See script logic at bottom with comments on how to confirm this isn't running any longer, etc. -- any traces for that matter and how to stop them from running if they are. NOTE: In my case we scheduled this job start with SQL Agent job at 7 AM and then put the DateTime variable value for the @stoptime argument passed to the sp_trace_create object so it quit running at 8 AM on this day. Once in the office and reviewing this, we were able to sift through and determine the issue and correct. We filtered our trace criteria down as much as we could though beforehand when we built the script file that saves to disk to give the TSQL for scheduling. 

According to Enable Encrypted Connections to the Database Engine (SQL Server Configuration Manager), you do this as such: 

Create the [user account object] service account in AD () Create a SQL login i.e. the service account in #1 () Create a SQL credential with that same () account credentials, Create a SQL Server Agent Proxy account to use for the "Run As" from the SQL Agent job to execute PowerShell 

Some things to check, read up on, or try not knowing the Server OS version, etc. (Sounds like a Windows Update broke something with the OS and SQL error log is catching it (extended event notifcation) but perhaps not able to send a notification, etc.) 

I have to agree with Max's comments about running the stuff that would normally run when the issue occurs as a simple step to confirm whether or not those items/processes are causing the issue -- process of elimination should be simple enough. Since you pretty much have the day and timing down to a science when the issue occurs, you could schedule a SQL profiler trace to run via a SQL agent job and give it a stop time (@stoptime) to stop the trace to see what details the trace will provide. Since you asked about where else you could start to troubleshoot, I think a SQL profiler trace would give you a lot of detail to go by actually. I'll paste what I have on this when I did it on SQL Server 2008 R2 below. SETUP DETAILS Follow the below instructions for scheduling a trace with a specific start time and a specific end time. You'll go to 'SQL Server Profiler' and build your trace criteria as usual but be sure to save to a file somewhere valid on the server itself, check the 'enable file rollover' option, and specify a trace stop time. Once all the criteria is selected and filtered for what you need to capture, run it, stop it, and then navigate. . . FILE | EXPORT | SCRIPT TRACE DEFINITION | 'select' the SQL Server 2005 - 2008 R2 | and then save the file somewhere you can open later. Go to the file, open it and you should get something similar to the below. KEY POINTS 

Consider testing with the parameter too and see what results you get with your logic. This may work for your need as well. 

It would seem the explanation would be that your your committed transaction sizes with the operations are HUGE. Simply make the commit\batch size parameters of your logic in SSIS or your execute TSQL in the package of a smaller size. Try testing with 100, 1000, 10000, 100000, and so on to see what gives you the best result and prevents the issue from occurring. If the transactions are smaller, once the committed transactions are committed in recovery mode, then the log space of the committed transactions can be reused by subsequent (or other) transactions. 

SqlDependency was not really intended for real time type processing. 3 ms is still pretty good. You should only retrieve required columns and you should use a Reader. DataTable has a lot of overhead. And you should be using . 

I suspect this is a duplicate but I cannot find it Typically another table and even some definitions state another table Repeating values is the norm for a FK FK point to the PK of another (or the same table) is the common configuration. In MSSQL it can be a column(s) with a unique constraint (and not PK). The row must be uniquely identified. Here is an example of a single table Assume each employee has exactly one boss The person at the top works for them-self BossID would be a FK in the same table Yes BossID would repeat if more than one person worked for that boss 

I don't like to use plural for table names Most people would expect a table to have multiple rows If the properties (columns) of character are the same if they are in a movie of a show then just one 

A unique constraint on TEACHER_ID, STUDENT_ID would prevent duplicate STUDENT_ID for a teacher. In fact you could just make the the PK and drop ID. This is TSQL so it may be wrong for mysql 

Then same thing for last_name I know it seems long but it will minimize logging The multiple updates are typically more efficient than OR Uh, oh I just read same transaction One or the other is going to be different a lot What about a view Just use the view during the update - this view should be very efficient 

If #temp has indexes then sort on the one that makes the most sense If that does not improve response then just force a hash join on all Still use the select in my answer - add the HASH to the query above 

If this is taking 15 minutes then something is wrong Is ASSET_ID indexed? Have you fragmented indexes lately? 

In the restore you just give the db, mdb, and log different names. And there is an option to force - I think it is called replace. We do this all the time to create new test db. 

This is SQL but I suspect it holds up for mysql A FK saves space and memory Yes there is overhead to a join but if both sides are indexed then a small overhead Consider a FK that is repeated a lot in the referencing table - like an audit table may repeat each user 1000+ times. a1) 

That link does not indicate a table scan. That is an index scan. With a narrow date range the date is more selective than the join so it (properly) does the date first. Keep the PK clustered index on the two Id Have three separate indexs 

It is more efficient to check if data has changed in the web application Sending an update to myslq with the same value uses more resources If there are no changes then send nothing Better yet don't even have the button active until there are changes If there are changes only send the changes and do so in one statement 

First txn_status should be an tinyint with FK and index Do you need bigint for amount I trust there is an index on date And consider 4 different queries In the three below the first was 86% of the cost Case was an index scan Where is an index seek Assign current_time to a variable so current time is the same for all The older transactions are going to be in pages most likely not use by inserts You would only get contention the newest - make that short and sweet This was tested on MS SQL-Server 

For the security credential/context which the application uses to authenticate to the DB, set an EXPLICIT DENY to it on the table to not allow it to access it at all (e.g. SELECT, UDPATE, DELETE, etc.). You should really have the application credential locked down in the DB which it uses anyway to ONLY allow it to ONLY have the EXPLICIT level of access it needs to each DB object specifically in the name of security. So why the user table is a concern now, wonder what other tables, etc. it has access to due to poor security design. 

Can you point the 'full path' to the UNC path i.e. instead and see if that works? Just like when you map the "X" drive to just use that in the full path of one of your packages and run to see if it'll work. If one of your jobs work like that (assuming most all are setup the same and this way, etc), you can probably script out the SQL Agent jobs through SSMS by pressing F7 (once SQL Agent jobs is highlighted), selecting them all from the right pane window, right click, then create to new query window, then do a mass CTRL+H and do a find and replace to replace with , and then run that. Just be sure the SSIS proxy account or the SQL Server Agent account has appropriate NTFS and SHARE permissions where the SSIS packages reside to read them. 

Use double backslashes [] in the folder path to separate folders rather than just one to escape the first backslash since by default a single backslash is used as a special escape character and ignored when used alone 

Noting that the. . . 1. will allow access to all tables 2. will allow , , and access to all tables 3. will allow access to all executable objects 

Ensure you're on the 'publication' DB so and then execute the SP. Otherwise, try various combinations such as the below for the variable you're passing to the SP as an argument to the SP in case it's related to it not liking the preceding which indicates that the subsequent string is in Unicode, thereby passing an , or value, as opposed to , or . 

You can do this by using sp_delete_job and dynamically creating applicable syntax and controlled logic to execute this for the dynamically created job name when date wise it needs to be deleted. Since you are building the dynamic SQL Agent jobs and creating them with a start date and end date, you'd just put additional logic in to execute sp_delete_job if the date is equal to day eight at the end of the job . You could put the additional logic in at the beginning of the job to execute sp_delete_job if the date is equal to day nine or the day after the eighth day . 

A few more suggestionsâ€”adding as an answer to accept more characters so I can include more. . . BE SURE TO RUN EVERYTHING AS ADMINISTRATOR 

The above TSQL will grant explicit VIEW DEFINITION access to the specific DB object and to the specific security principal as specified in the applicable TSQL logic 

I also wanted to share a db_DDLAdmin_Restriction role you may want to consider to consider creating otherwise with explicit to restrict what give access to so you could at least create this on the DBs where you grant them this role and set the explicit for the actual object types, etc. you don't want them to have access to. For example, if you know they will definitely be creating stored procedures and functions, you can exclude , , . 

Yes, active transactions are included in transaction log backups and this is how the database restore option works. It's important to understand how database restore operations and options work in SQL Server that'll help you get a better understanding as well so I put another hyperlink at the bottom with nothing quoted for further reading on this topic. Below are some resources that give good explanations for this as well to help put all the pieces together. Some of the article may be a bit dated, but the explanations seems accurate and the concept is still the same for your inquiry. Transaction Log Backups 1