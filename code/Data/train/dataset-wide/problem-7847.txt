Papers in mathematics are generally written as if the major insights suddenly appeared, unbidden, in a notebook on the researcher's desk and then were fleshed out into the final paper. While this is great for finding out about results, it's terrible for finding out about how they were arrived at. What I want are papers, books or essays written by researchers about their work on problems, especially if they describe the evolution of their work on a specific problem (Polya's writings on problem-solving are great, but not what I'm interested in). I'd like to know how hundred-page treatises on problems unsolved for decades are born. 

I have Gross and Yellen for the course that I'm taking this semester. Haven't looked through it, but one of the authors is my teacher and I've taken a past course with him, so I'll assume it's solid. His other book, an introduction to combinatorics, is quite good with its definitions and explication, but the exercises leave a little to be desired. 

I have an application where I want to the eigenvalues of the graph to be involved in the objective and constraints in a flexible way (moreso than just the nuclear or frobenius norm). Whats a good survey or intro source to this sort of optimization? 

I'm wondering if compressed sensing can be applied to a problem I have in the way I describe, and also whether it should be applied to this problem (or whether it's simply the wrong tool). I have a large network and plenty of data on each node. I want to find a set of communities that explain most data similarity and track these communities over time. In a compressed sensing formulation this amounts to the following: -My graph's representation basis is a weighted set of communities, where each community is a subset of the set of all nodes (candidate communities can be narrowed down to a tractable number rather easily) -Different feature measures (e.g. bigrams, topic profiles) serve as my sensing basis, with correlations between community membership and features serving as the coefficients of my measurement matrix. The big assumptions, that my feature measurements have the Restricted Isometry Property, that similarity is incoherent with community, and that similarity is a linear combination of community, are all almost certainly incorrect, however they seem plausible approximations to within (possibly significant) noise. Ideally, I can use this strategy to describe my network as a collection of communities and to track over time the prominence of these communities. I wonder, however, if there isn't some straightforward bayesian method that I'm overlooking. Misc Questions about Compressed Sensing: i) If my measurements are not linear combinations of my representation basis, but at least convex, then can I still usefully use compressed sensing? Edit: In the above case, for instance, the generally accepted submodularity property of networks means that a node's membership in additional communities that correlate positively with a feature have reduced effect. In this particular case it might be best to transform everything to logs, but in general this option might not be viable. ii) What is the meaning of the dual of basis pursuit? iii) How does one avoid basis mismatch in general? You choose your representation basis elements beforehand, so how do you make sure they're capable of representation? Edit iv) If your measurements are naturally represented as vectors, rather than scalars, is there any way to represent this other than counting each component of the vector as a separate measurement (though I suppose this works fine in general, if you have enough information about each component and everything is linear). 

Many network optimization algorithms, including shortest path, push-relabel, augmenting path, etc, actually have an interpretation in terms of linear programming. A famous application of semidefinite programming is the max-cut approximation. Does this optimization algorithm, or any other on networks, have a network interpretation, a la augmenting path? 

Just because a problem is NP-complete doesn't mean it can't be usually solved quickly. The best example of this is probably the traveling salesman problem, for which extraordinarily large instances have been optimally solved using advanced heuristics, for instance sophisticated variations of branch-and-bound. The size of problems that can be solved exactly by these heuristics is mind-blowing, in comparison to the size one would naively predict from the fact that the problem is NP. For instance, a tour of all 25000 cities in Sweden has been solved, as has a VLSI of 85900 points (see here for info on both). Now I have a few questions: 1) There special cases of reasonably small size where these heuristics either cannot find the optimal tour at all, or where they are extremely slow to do so? 2) In the average case (of uniformly distributed points, let's say), is it known whether the time to find the optimal tour using these heuristics is asymptotically exponential n, despite success in solving surprisingly large cases? Or is it asymptotically polynomial, or is such an analysis too difficult to perform? 3) Is it correct to say that the existence of an average-case polynomial, worst-case exponential time algorithm to solve NP problems has no importance for P=NP? 4) What can be said about the structure of problems that allow suprisingly large cases to be solved exactly through heuristic methods versus ones that don't? 

What are some good sources for linear algebra for convex optimization and graph analysis? In Particular, is Gilbert Strang's MIT course suitable, or some other online course? I prefer online courses (video or lecture notes) to books because they're usually much better organized. I'm at the undergrad level, but interested in doing research in machine learning and network theory, which use these two things respectively. I've taken one (basic, computationally-oriented: i.e. we didn't learn anything about basis, vector space or the meanings of linear mappings, but instead learned a lot about iterative methods Householder reflections) course on linear algebra, and all the time I find myself stymied by matrix formulations of optimization problems and assertions about eigenvalues of graphs. I want to become fluent in it for these purposes - I don't plan to go further in the pure math direction, just engineering, so keep that in mind (though a few proofs and abstractions won't kill me, and are indeed welcome. Side Question: Can you have a useful matrix of quaternions (since you can have complex as well as real matrices) or is that just a silly idea, for whatever reason?