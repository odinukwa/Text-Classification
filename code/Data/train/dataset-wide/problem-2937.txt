when defining your function, you meant to name the first argument , not . That was a bug. and are the kind of arguments that could use an optional value so I made it so. And once an argument has an optional value, it is recommended to move it to the end of the argument list, hence . I renamed to for 1) it is more descriptive and more likely to suggest that this is the annealing's temperature variable, but mostly 2) R provides as a shortcut for , although you can overwrite it like you just did here. This is a horrible feature of the R language and you should avoid using both for or as your own variable. is probably a more appropriate tool for logging. I also transformed the code so it prints one row per iteration. I hope you'll agree the log is much easier to look at this way. Keeping track of the best state is an improvement over the "vanilla" version simulated annealing process which only reports the current state at the last iteration. If you want it that way, then you need to use three states: best, current, neighbor. Which I implemented here. In addition to keeping track of the current state and best state, it is recommended to keep track of their function values, so you do not have to make unnecessary function calls to re-compute them at each iteration. Like the matlab function you referred to, it is preferable to return a list so you can provide more outputs than just the best state. Always, it is important to comment your code to make it easier to understand and maintain. 

seems completely unnecessary since the two matrices are symmetric by construction. Next, your string manipulation for the output filenames seems a bit complicated. If I get it correctly, you just want to change the extension from to ? If so, I think you should use as follows: 

Rather than carry the weird assumptions about your data column names into the second part of your code, I took care of making uniform names in the data reading/cleaning section. For that, I converted all names to lowercase using . I added a to check that each sheet contains the four columns you need. Always try to gather in your head the assumptions that your code is relying on, then add code to check these assumptions and die if they are not met. Everywhere I needed a column from the data, I referred to it using its name (e.g. ) rather than its position (e.g. ). Doing so will always make your code more readable and more robust to data changes. I replaced with a base solution. It helps remove a dependency to a 3rd party package but if you prefer to keep dplyr, it's ok too. I replaced the calls with simpler (easier to read) calls. I note that your calls were probably returning the expected results but you are doing something weird. To use , your regex pattern should really use a set of parenthesis to catch something that will be stored inside . It would have made more sense to not use regex captures and just use the empty string as the replacement instead of . 

It is recommended you avoid variable names like and since they are very basic functions from R. Or you run the risk of shadowing them. and are not integers so using is a bit unpredictable. I feel it is better to use like I did, and with control over the grid density. I was not able to test my code because you did not mention the package name for so it is possible it will need some adjustment. In the future, please make sure to give us everything we need to run your code. 

Last, stay away from for writing your output to a file. It is a really old function; the fact you have to provide as an input tells me it was designed with on-screen printing in mind. Instead, you could use to be consistent with your using of at the beginning of your script: 

I feel the key to generalizing your code is to store your variables into a matrix. Then let vectorized functions (, , , etc.) do their magic: 

You can make a couple easy changes to improve readability. First, use or so you don't have to carry a lot of . Second, reorganize your logical tests so each nested appear in the "if-false" part of the previous , not in the "if-true" part. Hopefully, this will make sense: 

If you were to add a at the top of your function, you would find that your function is called 25 times, once for each combination (pair) of pathways. So yes, despite having used , it is still essentially a big old loop you have under the hood... Here is how I would write a vectorized function so the heavy-lifting function (, or in my case, ) is only called once or twice: 

I think you can simplify your code quite a bit if you compute all the summary metrics once for all in a single function. This way, you also avoid repetitive calls to read_excel: 

The best tool to diagnose slow code is the profiler. Here is how you could run it on a few function calls to see what is slowing down the execution of your code: 

Set the arguments to , i.e., , and uses for testing if an argument has been set or not. This approach has the advantage that if the user called the function as follows: , then it would work as expected although both arguments have been passed explicitly. Do not set the argument, i.e. and use for testing. Keeping instead of alone is not dramatic, though I've not seen much people do that. 

I am not a data.table expert so I can't tell if there is a faster approach, though $URL$ suggests you are already using something efficient at each step. The changes I have to suggest would however make your code a lot easier to read and maintain. First, define a function that can be applied at each step: 

I went from a script to a function. This way it is a lot easier to use and share. I have abstracted what I believe were all the right inputs and chosen sensible defaults. Of interest is the use of for the so the user can set it once for all by running . So now, all you have to do is source the file where this function will be stored, then call the function. A function, when executed, runs in its own environment so all the variables that are created at runtime are deleted as you exit the function. Your global environment is never polluted, which removes the need for your "clean up" section (which was otherwise pretty harmful as janos pointed out.) Like , forcing the install of packages via is a bit harmful. No, you just want to use and it will die right there, leaving the user with the decision of installing the package. There is a nice blog about why some people -me included- prefer over : $URL$ I made use of R's Date object wherever it made sense. I rewrote the way the api url is built, making use of a named vector for the arguments. With inline comments. I hope you'll agree the code is a bit more readable and easier to maintain, something you should always aim for. I replaced with . This way you don't get the prefix that comes with printing a vector. This has less to do with code reviewing but I thought that it would be more useful to see the total time spent on the website. Average time spent per day can be a little confusing: the user might be wondering if it is including all days or only those when the website was visited (you chose the latter.) While total time removes that ambiguity. Also, I thought it would be more useful to plot the data as a barplot with a real timescale, i.e. including holes for periods when the website was not visited. I used for that, see the picture below. The x-axis might look weird but you did spend six seconds on the website on July 1st so that's why it starts on that day, although you won't really notice there is a data point there. 

if your data has fewer than rows to start with (see where I used ) if your data has a single column (see where I used ) make sure that the last page will always show 

The main thing to note is that your profit function only uses vectorized functions ( and ) so it is vectorized with respect to its four inputs. This means that you do not have to create loops; instead you can just feed the functions with vector(s): one vector for the variable that you are shocking, and scalars for the other three fixed inputs. Here is my suggested rewrite, having put everything into a function: 

Then, using matrix multiplication between two such matrices, you get the number of gene matches for each possible combination of pathways. You then just have to compare that number of matches with the length of the pathway: . In the conversion of each input into a matrix of zero and ones, see this particular line of code: . This is what fills the ones in the matrix. It is completely vectorized, via a single call to . 

the matrix where each row contains the and coordinates of each vertex. the matrix where each row contains the value at a vertex. 

See how the function returns a one row data.frame. Then you can call the function on all files via and bind all the outputs together: 

I think it reads a lot easier this way, where each line contains a test and the output value if the test is passed. If the test fails, we move onto the next line. And at the end, the last line contains the value if all tests have failed. I also like this layout because it looks a lot like how you would nest Perl's ternary operator: 

where is a vector, so does not have to do any conversion. Note that you have a similar issue twice inside where you meant to use instead of . Second bottle neck With the iterative merge calls, you end up with pretty large data. What comes as pretty costly in that these calls, by default, also sort your data. That's where the second item in your profile () comes from. To get rid of it, which should not affect your results, add to all your calls. On my machine, these two changes cut the computation times by roughly two thirds. I hope this puts you on the right track. 

Under both designs, it is my humble opinion that you are making it confusing for the user when you make the default when the user passes nothing. I would not provide any default. A third approach might be: 

Instead, I would suggest you write a function to process a single sheet and only loop once using that function (disclaimer: code could not be tested!):