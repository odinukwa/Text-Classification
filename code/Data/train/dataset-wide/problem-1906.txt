Tropospheric ozone is a significant greenhouse gas (see e.g. IPCC AR4) and has well established negative effects on crop yields. For example, Van Dingen et al (2009) evaluated yield losses of up to about 15% globally, depending on the crop. 

It seems difficult to find a reasonable proxy species for $\mathrm{NH}_3$, since its sources (primarily agriculture) are quite distinct from the sources of more commonly observed pollutants. Some observations of aerosol-bound $\mathrm{NH}_4^+$ might (or might not) be available, but due to the reasons you explain, it is non-trivial to infer the gas-phase $\mathrm{NH}_3$ concentration from the $\mathrm{NH}_4^+$ in aerosols. There are papers about estimating atmospheric $\mathrm{NH}_3$ from satellite data, see for example here or here. These are for column-integrated and not surface $\mathrm{NH}_3$, however. 

There already exists at least one method that attempts to use electromagnetic precursors to earthquakes; it actually tries to detect electrical currents running up through active faults, explained by the piezoelectric phenomenon. Search for Varotsos if you want, perhaps there are other approaches as well. Even though the device has a very long "antenna" (ie it tries to avoid noise which usually has shorter wavelengths) it has had some false positives (power lines may produce signals as well, they are long and signals may be of low frequency. Personally I would never focus on short wavelengths such as TV frequencies. Varotsos correctly focused on low frequencies (there is a practical limit to the size of the "antenna", I think it is some 50-100 meters long). So in short, no, TV disturbances will never become a valid earthquake precursor. Too many false positives would ruin it. Keep in mind that in the aforementioned approach, the first signal comes perhaps 4 months before the earthquake and further signals and seismic data are used in order to further narrow the time window, which right now is in the order of days to weeks. However the location and depth are pretty inaccurate (100 km radius or more) while the magnitude may be almost 1 unit lower (the most recent "success" had a magnitude 0.8 lower but it was a doublet). Currently there is an active "prediction" for an earthquake 100 km around Athens, of magnitude near 6. If it is to come true, it should happen until June or early July, but there are dozens of capable faults within the radius so it is practically useless at this point. 

3) Now edit again, set the and to the values that you calculated in step 3, set , and re-run . The child domain file should be created. 4) Look at the file. Repeat the procedure until happy with the domain location. About the parameter. This is an integer factor of child grid refinement relative to the parent grid. For example, if set to 3, and parent grid resolution is 12 km, the child grid resolution will be 4 km. Odd values for (3, 5, etc.) are recommended because for even values, interpolation errors arise due to the nature of Arakawa C-grid staggering. is the most commonly used value, and recommended by myself. 

I guess that it is possible. As mentioned by others, on top of static stress triggering, which is the effect of an earthquake in its vicinity (typically up to 2-3 rupture lengths away), another way by which an earthquake can trigger others is by dynamic stress triggering, which may have effects farther away but are hard to identify and measure. So far, the only earthquake that seems to have demonstrated how dynamic triggering over long distances can take place is the 2012 Indian Ocean earthquakes. This earthquake had an unusual combination of characteristics: it was a very large strike-slip earthquake, it took place in oceanic crust (which allows surface waves to travel longer) and had a very large concentrated burst of Mw 8.4 - 8.5 with large slip over limited distance (I do not post sources since there are at least 3 apparently good studies with very different results, for example one study says that slip exceeded 80m locally (!)). These characteristics suggest that this earthquake was very efficient at radiating energy through the crust rather than towards the interior or through very fragmented crust. So, what this earthquake showed, is that it is possible to speed up significant earthquakes (earthquakes that were almost ready to happen, but they were sped up up to 90 days). Still, it is only one case, the mechanism is not well understood, we are not sure why this earthquake seemed to have caused this phenomenon while other earthquakes with higher magnitudes did not show the same effects. Concluding, while static stress triggering has only a fairly local effect, dynamic stress triggering may have a wider effect but we only have detected one such case, which could very well have been a coincidence or due to an unusual mechanism. So the answer to your question seems to be yes, but with doubts and in any case for very special earthquakes. 

Opinions vary. In my experience, few scientists believe that satellite-based emissions could replace traditional emission inventories. On the other hand, for many areas and pollutants, the regular emission inventories also involve a lot of extrapolation. Ultimately, the question is whether the inverse modeling can deliver some useful added value compared to the existing inventories. 

In fact, there are many chemical transport models which solve the radical chemistry without steady-state approximations. The equations are very stiff but can be integrated with suitable methods, usually implicit. Some approaches are discussed in this review. The Kinetic Preprocessor, mentioned in the comment, includes a set solvers for stiff systems and has become somewhat popular in atmospheric chemistry. Some models that use code generated by it (either by default or as an option) include GEOS-Chem and WRF-Chem. I used KPP to implement the CB4 chemical mechanism in this air quality model. An approach like this is possible because of operator splitting, which means that the processes included in the model are solved as a sequence of uncoupled operations. For example, a model time step could consist of calling an explicit scheme for solving advection, then Crank-Nicolson for vertical diffusion, and then a Rosenbrock solver for chemical kinetics. 

The answer to this question depends on what process one is interested in simulating well. As you know, short/mid-range weather prediction models and climate models have very different applications and goals. Because the short range weather prediction model is typically of much higher resolution than climate models (~1-10 km versus ~50-200 km), it is almost always more skilled at simulating clouds forming at a specific location and time. Cloud and rain prediction skill tends to be greater in synoptic scale fronts and near topographic features, and smaller for sporadic, small-scale, tropical thunderstorms. In very high-resolution NWP models (2-3 km or smaller), convection and clouds may be simulated reasonably well without using any cloud parameterization scheme. Nevertheless, clouds and rainfall are still hard to simulate or forecast very accurately. On the other hand, due to very low grid resolution, climate models can really aim only for correctly simulating the occurrence frequency and the amount of clouds and rain in a larger geographical region over a longer period of time. In climate, clouds play a significant role in radiative feedbacks. Climate models still rely on cloud parameterization schemes. 

To my understanding, the question refers to studies which aim to estimate pollutant emission fluxes using satellite retrievals of atmospheric constituents. Mathematically, this means estimating the forcing term in an advection-diffusion-reaction system given a set of tracer observations. How well you can localize the emissions depends on the pollutant: for something shortlived, like NO2, a given observation will be influenced by a relatively small area, while for a long-lived pollutant (say CO) the effective resolution would be much lower. For long-lived pollutants, the problem is not well posed and needs to be regularized in some way. Typically, instead of reconstructing the emission from a scratch, you try to just refine an existing emission inventory. The issues you mention regarding model errors are real and certainly do introduce biases in any emission estimates based on satellite products. This is quite well understood by people working on the field, but the difficulty is in quantifying the errors that are due to model uncertainty. 

The only open and ongoing data source for in-situ ocean wave measurements I am aware of is the National Data Buoy Center. Though NDBC manages data service from plenty of moored buoys in the Gulf of Mexico and the Atlantic and Pacific coasts of North America, unfortunately there isn't much in your region of interest. The only buoys I have found that are somewhat near the Southeast Asian region are 2 buoys on the east and west side of Guam, and one right by Northern Mariana Islands: 

I've been a WRF user for almost 5 years now, and contributed code to a recent public release. I am not aware that WPS (WRF Preprocessing System) has such a tool that takes in the grid and point coordinates and returns the appropriate index. However, it is very straightforward to do so yourself. Some suggest using an external library, I think that may be an overkill for such a simple task. Here is what you need to do: 1) Run to generate the parent grid. Since you don't know the exact location of the nest yet, set in . This will generate a file called . 2) Look at the file to find the right indices for your child domain. and refer to the x and y indices on the parent grid at which the southwest corner of the child grid will be positioned. and are the longitude and latitude grids of the mass (pressure) points. Using a programming language of choice, find the grid cell that is closest to your desired location for the child nest corner. This is typically done by looking for the minimum value of distance between desired location and all the points on the grid. For example, in Fortran, you can do something like: 

It can happen but not like you may guess. I believe you have seen rupture maps for large earthquakes, such as the 2011 Japan earthquake. In that earthquake, a foreshock of 7.3 took place at the perimeter of the area of largest slip during the main earthquake. This could be interpreted like this (using the asperity model): the foreshock hit at the "slopes" of the asperity but failed to "climb" up immediately; aftershocks were slowly migrating towards the "top" of the asperity, then the main shock happened. Furthermore, once the whole fault was activated, slip at the shallowest parts went beyond charts (40, 50, even 80m of slip were reported locally). The main (deep) asperity had the fewest aftershocks (it slipped completely and without restraints), but there were enough of them in the shallowest parts of the fault. This is probably because the shallower slip was not due to an asperity but rather due to diffused slip of unconsolidated sediments (the "bookshelf" model was used, plus there were aftershocks that appeared to actually... pull back the pieces closer to their original locations). Anyways let's focus on the deeper asperity. An earthquake can take place at the "slopes" of a large asperity but fail to rupture towards the "top" It can however rupture towards the perimeter and achieve respectable sizes. Then another earthquake can take place nearby, but still low at the "slopes". Finally an earthquake can climb over the top and force even the previously ruptured areas to re-rupture. All 3 earthquakes can very well have epicenters pretty close to one another (such as the two eastern Nepal aftershocks recently). It seems that the 1960 huge Valdivia earthquake demonstrated this behavior but I have yet to find a good, detailed, open-access study on it. Concerning Fred's answer, unfortunately Bilham (he mentions Jones and Molnar, 1986) (search for "preceded") says that 10% of strong earthquakes in the history of Himalaya have been preceded by strong shocks. Other sources support Fred's answer, but I would choose to believe Bilham. Concerning the "lots of aftershocks" notice that the author of the question pointed, for a given earthquake with a normal aftershock sequence it is expected that its strongest aftershock is 1.2 magnitude smaller than the mainshock. For this case, this would yield 6.6. In fact, the strongest ones were the 7.3, an 6.7 and a 6.6, so yes this is anomalous, but the empirical 1.2 value I mentioned is a bit smaller for Himalayas. Still, an 7.3 is anomalously large but before that time we had no reason to suspect an anomalous aftershock sequence (to my knowledge of course, I searched for b-values but did not find anything during the first days of aftershock activity).