Ignoring the error First of all, you should be able to ignore string truncation by going to your Flat File Source, Error Output and then changing "Fail Component" under "Truncation" to "Ignore Error". Better solution The real issue could be that the string length inside the SSIS pipeline is still wrong because it got initialized at some earlier point. You can determine if that is the case by double-clicking the green arrow from your Flat File Source (or after Derived Column/Data Conversion) and choosing "Meta Data". There you can see the length of the field inside the pipeline. If have often seen that meta data doesn't match the source component anymore, mostly if creating the Flat File Source AND its corrsponding Connection via the Wizard. My suggestion would be to delete the connection and the Flat File Source and recreate them as I haven't found a way to re-sychronize pipeline meta data with source components. 

EDIT as op stated that Enterprise is not an option: When using Standard Edition, you can fall back to using Partitioned Views. The main difference from the usage side is that you have to maintain the partitioned tables for yourself and you also have to make sure that every new member tables gets added into the view. The idea builds on your scenario 2 but you don't have to care about where to insert the data, as this is determined by the Partitioned View based on CHECK-Constraints in the member tables. For a good example, see 2 As for all partitioning scenarios you have to make sure that the partitioning columns you choose fit your query-and-insert behaviour. 

Looks like you are at the beginning of your investigation. Probably you should dive deeper in below areas to nail the issue. 

Table will be partitioned by RANGE weekly. On Master keep latest week's partition. And drop earlier partitions in below way. 

Reference Issue : Mariadb - Variable 'innodb_log_file_size' is a read only variable Reference Chat: $URL$ 

Doing by this way the SELECT statements are performed in a nonlocking fashion, but a possible earlier version of a row might be used. Thus, using this isolation level, such reads are not consistent.When you say not-consistent it means recently changing records i.e.. DML transactions that are currently in process will not be read. I assume which is in your case it is acceptable. This is also called a “dirty read.” Otherwise, this isolation level works like READ COMMITTED. If I were to be you, the below order is what I follow. 

You may switch the table from innodb to memory, but ensure the size of the table isn't that huge so that it doesn't eat up entire of its memory. Yes, you may safe guard or recover the table during system crash. Have master/slave replication setup and once the table is created on master with memory as engine type and change the table engine type to innodb (on slave only). So that even at the instance of master crash/restart you will get the data from slave. 

Please read about the security implications this has: $URL$ In short, don't use ownership chaining when you can't trust the db_owner of the source database. For a good in-detail explanation including examples see $URL$ 

You didn't do anything wrong. Unfortunately, referencing calculated columns in the field list is just not possible. The usual workaround is to use self joins, joins to subselects or anything like that. But that would only qualify for more complex calculations. In your case, the solution would be to just write it the way Ryan mentioned, even if that turns the stomach of every programmer. 

Database ownership chaining will achieve what you are looking for as long as your SP and the tables in the destination DB are owned by users of the same name. Just alter all participating databases so they are allowed to do ownership chaining: 

My first question to your situation would be if the Standard Edition is a given or if you had the possibility to upgrade to Enterprise. Within the Enterprise Edition of SQL Server 2008 R2 you would have the possibility to use Partitioned Tables. Essentially, they do what you described as a manual process in the background. Quote from 1: 

Here is my opinion. As long as you are going to use innodb engine it doesn't matter which one are you in. Provided Maria gives you the option of multiple writes to master-master architecture using Galera-cache. But as your TPS goes beyond a particular point, you might face issues in getting writes paused due to flow-control. Hence you may use innodb engine let there be 2Nodes using MariaDB using GaleraCache Cluster or simple Master&Slave. But have your applications writing to one node only either case. For my.cnf you may use percona tool wizard to generate as per your requirement. 

I guess you are under a wrong impression that it is a OS related issue. Check your server bit if it is 32 bit and if you try to install 64bit package percona's xtrabackup . It would end up throwing such error messages. 

You may give it a try using percona backup alpha version for Windows - Download_Link . Below are the steps after installation in Windows Bash. 

Why do you think you need to compare the data? Anyways it has to go to DB I guess. You can have a current_timestamp as one of the columns to DB and value can be derived when the Device generates its local data using device NTP. 

So instead of foo | bar taking a's position, it follows b's position, which almost always changes the meaning of the text and might make it nonsense. Are there options to control this? 

What is the difference between the search in search and replace, (the replace function) and full text search (tsvector) in Postgres? Or, put another way, can tsvector be used in search and replace (and would there be any advantage in doing so)? UPDATE There must be something about trying to formulate a question that helps clarify it in your own mind. Yes, there is ts_rewrite. The docs say: "In essence this operation is a tsquery-specific version of substring replacement". 2nd Update Ok, now I do have a question. In the docs examples, the replacement comes after the original string, even if the part being replaced was at the front of the original string: 

Ok, but how do you do either the drop and re-apply or the split without losing the relationship which is the whole point of a foreign key constraint? There must be some other mechanism which maintains the link somehow. What is it? How does it work? etc. I have seen this post, so I know it CAN be done: Recreate all foreign keys in all tables as deferrable (batch) answer by @a_horse_with_no_name. My question is, how? I can copy and paste the code given, but that doesn't mean I understand what it is doing. Most of the tutorials I've seen online are far more basic than this. Can someone refer me to a text that covers this issue (preferably available free online or in a library)? Or give an explanation here if it wouldn't be too long or too much trouble? Thanks. 

From Mysql 5.6 onwards you can explicitly mention the partition to be queried and it can go on further querying the data based index within the partition. I.E. 

You may set a cron every minute to trigger below command and pipe it to a logfile and rotate it every day. 

Folks, Is it required to turn off huge pages in Mysql 5.6 server under /sys/kernel/mm/transparent_hugepage/enabled -> To never? We have a heavy transactional database and calling the data segments quite often for a few particular tables by its index. How does a memory foot-print work here? Would it help or degrade the performance. In AnonHugePages: 109242368 kB ( I see entire innodb buffer pool size is occupied under AnonHugepages). No specific issues found so far due to memory. But need an advise on when to keep it on and when to turn it off. 

Any best recommendation variables for my.cnf on percona server on a high 256GB RAM and 32 CPU with 4000 IOPS? How about below ones? 

[difference this time is you are granting with the password for REPLICATION SLAVE GRANT] Show Grants: If you do show grants; after you give GRANT REPLICATION command. You should get below grants from same slave host via CLI.