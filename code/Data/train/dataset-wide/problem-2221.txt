Since SQL Server 2008 this has been supported by the two views mentioned above. If the overhead of collecting the needed words is not too expensive, it does open up new possibilities for Full Text searching. 

require . Perhaps you need to bill them so as to be paid. have no . Perhaps subscriptions are automatically paid through a credit card, PayPal, etc. 

If I understand you correctly, your major problem is the log files during the several hours of backup. From your opening statement I understand that the 1.5 TB database normally runs in SIMPLE recovery, and thus no log backups to do. Disclaimer: I have never done log shipping on this scale. Of course, you should ask whether you can get more space allocated for your log files. If you can, then great. However, I think a small modification of your plan, providing that you already run in SIMPLE recovery model and/or the risk of SIMPLE recovery model for a few hours is worth it, would ease some of your worries. 

For MySQL in addition to the command, there is also a command. However, has limitations. If you you cannot remove one table by running . This is because will only remove rights as they are granted. That is, you could: 

You can try to address the problem by increasing the timeout periods. Increase the standard settings on your server for and . Or in your connection string you could include "Connect Timeout=120;General Timeout=120;" to increase the time allowed for the connections. 

If relatively few of the XML columns are actually needed in your query, then you could join the candidate XML columns to the Unique Clustered Index, perhaps greatly reducing the total amount of data being read. This may well result in a faster execution plan. Of course, if you actually need every XML column then there would be little benefit. But that depends on the factors described in the linked article. 

The database owner is the login in sys.server_principals that owns the database, as defined by its SID. After a restore it may easily be the case that the login that was used to restore the database is not the login that was the previous owner of the database. (This is even more likely to happen when the database if moved between servers.) So, there could be three settings that you are having trouble with 

[EDIT] OK, so the query is more complex that can be embodied in a single query. This means that you will likely need to use a pattern much like the one you first posted. Note that EXISTS implies a DISTINCT and is often faster than a JOIN from a SELECT DISTINCT. But you can try different approaches and compare the behavior, timing, etc. Then choose the one that you like the best. 

The simplest way to work on this is to follow Martin Fowler's (and many others as well) technique: $URL$ Note: SQL Server syntax 

For Microsoft SQL Server, MySQL, PostgreSQL, etc the wildcard is not * but is %. If you search for "wildcards" in your SQL documentation you should find your particular server's supported wildcards. EDIT: Post comment: Sorry, I have never written MS Access queries. 

You can create a script to add a column for a table. If all of the tables are identical it is not too hard. 

There is a TechNet blog that discusses some merge replication issues with blobs on a SQL Server 2008 or higher server. $URL$ Note that the author cautions about which settings to use when there are SQL Server 2005 clients, such as you have. 

You must be using Enterprise Edition. The primary filegroup must be the first restore. Filegroups that are not yet restored remain offline. 

But of course. I suspect that you want to upgrade to the SQL Server 2014 Business Intelligence edition, since that is more frugal than Enterprise. But the BI edition does include the SQL Server database also in the license. Are you wanting to upgrade the BI server in order to get the latest BI features? SQL Server 2014 can certainly pull information from earlier editions of SQL Server for processing locally. Or the tools can read directly from the 2008 server. 

When you use DBCC SHRINKFILE(Logfile, size) it only truncates from the end of the log file back as far as it can go. When it reaches the highest virtual log still in use, it cannot shrink further. This is described in the SQL Server Books Online at: $URL$ So, once the high end of the log is clear, it can be shrunk down in size. Again, that will depend on how much of the log is still in use. The log can be cleared by backups, but the backups will not clear incomplete transactions, so the log can remain in a high-end VLF even after repeated backups. With regard to the increase and decrease of VLFs, how big was the log file created to be originally and what is the setting for log file growth? If it grows by only a small amount it will create more VLFs than anyone desires. A common pattern for shrinking a log file is CHECKPOINT, BACKUP, SHRINKFILE, CHECKPOINT, BACKUP, SHRINKFILE, etc until you get results. There are many reasons that the log may not be shrinkable, including a very large rollback. (Not your current problem, of course.) 

If you choose to index every column in a table that is involved in a query, you have chosen to store considerably more data. This has an effect on the storage required and in the overhead of maintaining all those indexes. (Or maintaining all columns in a very wide index.) In past experience, having many indexes, especially several indexes with similar distribution statistics, can make it easier for the server to select a poor index and thus a poor plan. 

You are correct that the BEGIN DIALOG CONVERSATION does not specify using a TRANSACTION for starting a Service Broker dialog. But many developers use them as a standard practice. For example, the 2008 R2 TechNet page Beginning a Conversation and Transmitting Messages seems to recommend and (or ) as a good practice. This topic Configuring Service Broker for Asynchronous Processing also shows transactions being used. These multi-step examples that use transactions do so for the same reason transactions are used in other code. Namely to ensure either a complete transaction or else a rollback to a consistent state. Of course, you can decide if your Service Broker dialog is simple enough that you do not need a transaction. But that is up to you to decide. 

What fits well with your current skills? (That might make it 'easier' to use.) Is your focus on getting something working soon, or on developing new skills? What is the cost of your choices (money, time, effort, unfamiliarity, etc)? 

The basic answer is: No. Do you have a record of the 18-20 digit value somewhere? In another location in your database, on a piece of paper on you desk, etc? If so then use the source information to correct your error. You have not indicated which database software you are using, which can make a difference for many issues. 

If I understand correctly you want to totally remove replication from the server. There are a number of tools you can use, as outlined here for limited dismantling of replication. Removing Replication If you just want to strip all replication from the server, you can apparently use on each database that needs to have replication dropped. sp_removedbreplication (Transact-SQL) on the appropriate database The code is simple. Here is the MSDN example code: 

Your specification does raise some questions, but should be easily possible. Since % is a wildcard character, and * is not, then you might want to use % as your marker. 

Robert Sheldon in the following post from 2015 discusses NULL behaviors and why they sometimes (but not always) fail $URL$ He describes 13 NULL failures that a programmer can easily trip over. Failure #1: Not knowing what NULL means Explanation: NULL is a non-value, a nonexistent value. It is not zero. It is not an empty string. A value cannot equal NULL. No two NULL values are equal. That is the basic problem, but be sure to read about the other failures. Yes, earlier versions (pre-SQL Server 7 I believe) behaved differently, more like what you are wanting. However, if you search for the issue on Stack Overflow and Stack Exchange you will find many long threads discussing the issues. 

Consider creating a series of filegroups for the documents as the data grows. At the proper time, set the older filegroups to read only. This protects the data and gives you the option of backing them up less frequently. Ask your business to reconsider the value of storing documents in the file system. 

There is a tool to help you test your queries: $URL$ You can use the to test your full text results. For example, three similar searches using this function. 

However, you would be better just making a backup (which from SQL Server 2008 R2 supports compressed backups) and you will get a faster transfer than detaching and copying files. Not to mention the risks of detaching a file and not being able to reattach. Recommendation: Use and . 

The stored procedure is not examining the total culmulative size of the rows in the database. It is reporting the size of space allocated to hold that data in the cumulative size of the extents allocated for the data. If there is significant freespace available, such as from many deleted rows, then a rebuild of the clustered index would compact the space in pages and extents to be more efficient (i.e. smaller) for performance reasons. So, no data should have been discarded, but the rebuild process made that free space which was embedded in the data pages available again. 

RESULTS: Default This specifically queries for each of the states that you care about. Since there are two requirements, then the UNION will produce two rows when both selects are find a row. 

Table partitioning may improve performance if you are able to work within the limits of how the partition works. See the description at: $URL$ However, partitioning can also make your server run slower if your partitions are not set up "just so" and your queries cannot remain within a single partition. Gail Shaw has written an article on this: $URL$ A couple of quotes: "Partitioning can enhance query performance, but there is no guarantee." And, "In summary, partitioning is mostly for improved maintenance, fast loads, fast deletes and the ability to spread a table across multiple filegroups; it is not primarily for query performance."