The answer to your question is given in Feller's book (Introduction to Probability Theory and its applications), Volume 2, Chapter 6, Section on persistent and transient random walks. In my edition, this is Chapter 6.X, Theorems 3 and 4. Theorem 4 says that a random walk with zero mean is persistent. Theorem 3 says that a persistent random walk visits every interval infinitely often in the non-arithmetic case and gives the corresponding statement for arithmetic random walk. 

1) I believe it is possible to obtain $P(M_n<a\ | \tau>n)$, where $\tau:=\inf\{n\ge 1: S_n\le 0\}$ in more or less explicit way only for the case of the simple random walk. These computations can be found in Billingsley, Convergence of Probability measures, Chapter 2.11. For large $n$ these computations become universal as you can apply FCLT, see below. 2) Generally, one can use Brownian motion approximations as was suggested by fedja. Mathematic justification is given in the references below. Before going into this I will say a few words about conditioning of random walks to stay positive. There are two ways to condition random walk to be positive. First way is to condition on $\{\tau>n\}$, where $\tau:=\inf\{n\ge 1: S_n\le 0\}$. Second way, is to ''condition'' on $\{\tau=\infty\}$. Here you cannot use direct conditioning as $\mathbf P(\tau=\infty)=0$ (for the zero-mean finite variance random walk). So, the way to proceed is to use Doob's $h$ transform. As a result one obtains a Markov chain (Lamperti Markov chain, in fact) which is a discrete-time analogue of Bessel process. Now I can move on to approximations as $n\to\infty$. a) When $a$ is of order $\sqrt n$, you need to use corresponding Functional Central Limit Theorem (FCLT). You need functional central limit theorem as the maximum depends on the whole trajectory of the random walk. The way to use functional limit theorems is described in the above Billinsley's book, you can see an example for unconditioned random walk in Chapter 2.11. For the first type of conditioning FCLT is proved in Iglehart, 1974, Functional Central Limit Theorems for Random Walks Conditioned to Stay Positive (doi:10.1214/aop/1176996607), see also Bolthausen, 1976, On a Functional Central Limit Theorem for Random Walks Conditioned to Stay Positive. Conditioning of the second type was considered in Bertoin and Doney (1994), On Conditioning a Random Walk to Stay Nonnegative (doi:10.1214/aop/1176988497). FCLT for the second type of conditioning can be found in Caravenna, Chaumont (2008), Invariance principles for random walks conditioned to stay positive (doi:10.1214/07-AIHP119). Finally, in higher dimensions you can use FCLT from $URL$ b) When $a$ is much larger then $\sqrt n,$ then $\mathbf P(M_n<a\ |\ \tau>n)\to 1$. c) When $a<<\sqrt n$ you are dealing with small deviations probability. If you write $\mathbf P(M_n<a | \tau>n)=\mathbf P(M_n<a , \tau>n)/\mathbf P(\tau>n)$, then approximation for $\mathbf P(\tau>n)\sim Cn^{-1/2}, n\to\infty$, and $$ \mathbf P(\tau>n, M_n<a)=\mathbf P(\mbox{random walk $S_k$ stays in (0,n) for all } 0<k\le n) $$ Now, if $a\sim An^{1/2-\delta}$ for $\delta\in(0,1/2)$ you can split this trajectory in $n/a^2$ parts of length $a^2$ and use independence to obtain estimate $$ \mathbf P(\tau>n, M_n<a)\le \left(\mathbf P(\mbox{random walk $S_k$ stays in (0,2a) for all } 0<k\le a^2)\right)^{n/a^2} $$ Now, the probability inside the parenthesis is on the right scale and you can apply the FCLT to get $$ \mathbf P(\mbox{random walk $S_k$ stays in (0,2a) for all } 0<k\le a^2)\to C(A)\in(0,1). $$ This results in the following upper bound $$ \mathbf P(\tau>n, M_n<a) \le C(A)^{n/a^2}\approx C(A)^{n^{2\delta}}. $$ As $C(A)$ is in $(0,1)$ this probability is quite small. . 

I am using the notations, definitions, and results of the Section X of [1] on generalized Orlicz spaces. We say that $\varphi : \mathbb{R} \rightarrow \mathbb{R}^+$ is a $\varphi$-function if it is symmetric, nondecreasing, continuous, and satisfies $\varphi(0) = 0$. Then, for a measure space $(\Omega, \Sigma, \mu)$ and $\varphi$ a $\varphi$-function, $L^{\varphi}(\mu)$ is the set of measurable functions $f : \Omega \rightarrow \mathbb{R}$ such that \begin{equation} \rho_\varphi (\alpha f) := \int_{\Omega} \varphi( \alpha f ) \mathrm{d}\mu < \infty \end{equation} for some $\alpha > 0$. We call $L^\varphi(\mu)$ a generalized Orlicz space. We set \begin{align} \lVert f \rVert_\varphi = \inf \{ k > 0, \int_{\Omega} \varphi \left( \frac{\alpha f }{k}\right) \mathrm{d}\mu \leq k\}. \end{align} Then, $(L^{\varphi}(\mu),\lVert \cdot \rVert_\varphi)$ is a complete linear metric space (after identifying $f_1$ and $f_2$ when $\lVert f_1 - f_2 \rVert_\varphi = 0$), cf. [1], Section X, Theorem 2. Question: Consider a continuous and linear operator $\mathrm{L}$ between two generalized Orlicz spaces $L^\varphi(\mu)$ and $L^\psi(\mu)$. When is it true that there exists a constant $C>0$ such that \begin{equation} \rho_\psi(\mathrm{L} f) \leq C \rho_\varphi(f) \end{equation} for every $f \in L^\varphi(\mu)$? Some motivations: When the function $\varphi$ is convex, $\rho_\varphi$ defines a norm on $L^\varphi(\mu)$ that is therefore a Banach space. We talk in that case of Orlicz spaces. Here, we do not assume that $\varphi$ is convex, and not even that $\varphi$ goes to infinity at infinity. This situation occurs in the study of infinitely divisible random variables taking values in spaces of infinite dimension, see for instance [2]. Still, $\mathrm{L}$ being continuous and linear, it is bounded for the metrics $\lVert \cdot \rVert_{\varphi}$ and $\lVert \cdot \rVert_{\psi}$ but it is not so clear to me what can we say for the quantities $\rho_{\varphi}$ and $\rho_{\psi}$ in the general case. [1] M.M. Rao and Z.D. Ren (1991), Theory of Orlicz spaces [2] B.S. Rajput and J. Rosinski (1989), Spectral representation of infinitely divisible processes. Probability theory and related fields, 82(3), 451-487 

For fixed $t<1/\lambda$ it seems that intuition B is correct as $N \to \infty $. In this case, using the large deviations, one can see that it is highly unlikely that more than $(N+\varepsilon)\lambda t$ customers have arrived in time interval [0,t]. As there are $N$ servers and customers join the shortest queue it means that there will be empty servers and new customers will simply join them. Therefore, for $t<1/\lambda$ there will be a proportion of $\lambda t$ servers with 1 customer and $(1-\lambda t)$ servers with 0 customers. It might be good to have a look at N. D. Vvedenskaya, R. L. Dobrushin, F. I. Karpelevich, “Queueing System with Selection of the Shortest of Two Queues: An Asymptotic Approach”, Probl. Peredachi Inf., 32:1 (1996), 20–34, who studied a related system. 

Just an addition to Pablo Lessa's comment. If probability to stay at a point is zero, then you have a simple random walk and the Reflection Principle is valid. So, your question will follow if you find the joint distribution of the maximum and minimum, as the simple random walk visits all points between them. The exact formula for the joint distribution of the maximum and minimum can be found in e.g. book of Billingsley, Convergence of probability measures, Chapter 2.11. The question can also be reformulated as the distribution of the first exit time from an interval [-a,b]. Then, another reference (for the zero-mean random walk) is this paper $URL$ of Kemperman, Asymptotic expansions for the Smirnov test for the range of cumulative sums. Ann. Math. Statist. 30 1959 448–462. The more general case (when probability to stay at 0 is greater than 0) can be treated similarly to the above references. 

Integrate with respect to x. The LHS is one. The RHS consists of the product two terms, one being the inverse of the exponential of the cumulant generating function of h, the other being $e^{-A(\theta)}$. (Also plausible that I am asleep, in which case I apologize) 

For n an even integer, $0 \leq i \leq$ ${n} \choose{j}$, $1 \leq j \leq n$ let $X_{i,j}$ be a random variable taking values $\frac{n}{2}-j,0,j - \frac{n}{2}$ with equal probability. Let $S_{n}$ be the sum of these $2^{n}$ random variables. My question is, what is the 'correct' local limit theorem for this sum as n goes to infinity? That is, what is a local limit theorem that is in some sense sharp? To those who have not heard of the term: A local limit theorem is one that describes probabilities of the sum being a specific number rather than being in a region of size roughly proportional to the square root of the variance. To those who might think this is a little specialized: You're right, of course. I wanted to post a question with a concrete answer, and this is the simplest one that seems to be 'on the edge' of where local CLTs hold. The versions you see in textbooks fail for this (those that I'm familiar with fail in a few places), but 'just barely'. Also, a local CLT does at least hold here. I'm interested in other borderline cases as well. To those who might think this is trivial: It is true that a CLT for this sum follows from standard textbook theorems (e.g. the Lindeberg CLT). It is even true that the martingale local CLT can be used to get rates here - unfortunately, they seem to be wrong (that is, quite far from sharp). 

The theory of generalized stochastic processes was introduced independently in the 50's by Ito* and Gel'fand in a short paper. The latter then developed his theory more extensively in the fourth tome of his work on Generalized functions**. I am looking for the initial short paper of Gel'fand, in Russian, with the following reference: I. M. Gelfand,Generalized random processes, Doklady Akademii Nauk SSSR100 (1955), no. 5, 853–856, in Russian. I couln't find it on the web. According to WorldCat, it is in the MIT, too far for me. Does anybody have another solution to obtain this historical paper? *K. Itô, Stationary random distributions, Kyoto Journal of Mathematics 28 (1954), no. 3, 209–223. **I. M. Gelfand and N. Ya. Vilenkin, Generalized functions. Vol. 4. Applications of harmonic analysis, Academic press, New York, USA, 1964. 

Let $S(\mathbb{N})$ be the space of rapidly decreasing sequences and $S'(\mathbb{N})$ its topological dual, the space of sequences bounded by a polynomial. For $m\in \mathbb{Z}$, we also define $\ell_2^m (\mathbb{N})$ as Hilbert spaces of sequences such that $(u_n (n+1)^m)_{n\in \mathbb{N}} \in \ell_2 (\mathbb{N})$. It is known than $S(\mathbb{N})$ is the projective limit of the spaces $\ell_2^m (\mathbb{N})$. As such, $S(\mathbb{N})$ is a Frechet space with a nuclear topology. Its dual $S'(\mathbb{N})$ is hence the inductive limit of the spaces $(\ell_2^m)' (\mathbb{N}) = \ell_2^{-m} (\mathbb{N})$. So: Fact 1: $S'(\mathbb{N})$ has a natural complete nuclear topology defined as a countable inductive limit of Hilbert spaces. It is also known that any complete nuclear space is isomorphic with the projective limit of a suitable family of Hilbert spaces. See for instance Corollary 3, Section 7.2 of Topological Vector Spaces. Fact 2: $S'(\mathbb{N})$ has an abstract complete nuclear topology as a projective limit of Hilbert spaces. Question: Is it possible to describe the topology of $S'(\mathbb{N})$ as a countable projective limit of Hilbert spaces $H_m$, meaning that $S'(\mathbb{N}) =\bigcap_{m\in \mathbb{N}} H_m$, such that the $H_m$ are described as sequence spaces (bigger than $S'(\mathbb{N})$ of course)? 

This is just a bound from below. The derivation is a bit long for a comment. Clearly, a positive solution to the equation exists (although you need to specify in more details what happens when $X_1=0$ as $\lambda_1=0$ in this case). Now, we have \begin{align*} \sum_{j=2}^nX_{j} &= \sum_{j=2}^n\lambda_j+\sum_{j=2}^n\left(\sum_{i=1}^{j-1} \lambda_i\right)\ln (\lambda_j/\lambda_{j-1})\\ &=\sum_{j=2}^n\lambda_j+\sum_{i=1}^{n-1}\lambda_i\left(\sum_{j=i+1}^n \ln(\lambda_j/\lambda_{j-1})\right) \\ &=\sum_{j=2}^n\lambda_j+\sum_{i=1}^{n-1}\lambda_i \ln(\lambda_n/\lambda_i) \end{align*} Now since $\lambda_i>0$ we can make use of the inequality $\ln(1+x)\le x$ to obtain \begin{align*} \sum_{j=2}^nX_{j}&\le \sum_{j=2}^n\lambda_j+\sum_{i=1}^{n-1}\lambda_i (\lambda_n/\lambda_i-1)\\ &=\lambda_n-\lambda_1+(n-1)\lambda_n \end{align*} Dividing both sides by $n$ and letting $n \to \infty$ we obtain by the Law of Large Numbers, $$ \lambda \le \liminf_{n\to\infty}\lambda_n, \quad \mbox{a.s.} $$ 

You can find the asymptotics for this probability. For that let $$ \tau:=\inf\{i\ge 1: S_i\le i^\alpha\}. $$ Then, if $Var(S_1)<\infty$ then the asymptotics for $$ p_n=\mathbf P(\tau>n)\sim \frac{C}{\sqrt n},\quad n\to \infty, $$ see $URL$ for random walks with i.i.d. increments, and $URL$ for random walks whose increments are independent but not necessarily identical. In the latter case the answer is $$ \mathbf P(\tau >n) \sim \frac{C}{\sqrt{Var(S_n)} },\quad n\to \infty. $$ I have doubts about existence of a simple (and accurate) lower bound. 

In the 60's, I. Gel'fand introduced the concept of generalized stochastic processes (Ch. III, Vol. 4 of his work on Generalized functions). For a generalized stochastic process $\Phi$, he defines the concepts of stationarity ($\Phi(\varphi)$ and $\Phi(\varphi(\cdot - t_0))$ have the same law) and of independence at every point (the random variable $\Phi(\varphi_1)$ and $\Phi(\varphi_2)$ are independent if $\varphi_1$ and $\varphi_2$ have disjoint supports). Gel'fand especially introduces the complete class of Lévy white noises as generalized stochastic processes with characteristic functional of the form $$L(\varphi) =\exp\left( \int f(\varphi(t)) \mathrm{d}t \right),$$ with $f$ a function that has a L\'evy-Khintchine representation. Obviously, white noises are not the only stationary and independent at every point processes (ex: the weak derivative of a white noise). I am interested by a characterization of stationary and independent at every point processes. Especially, Gel'fand conjectured the following result. Conjecture: If a generalized stochastic process $\Phi$ is stationary and independent at every point, then the characteristic function $L$ of $\Phi$ has the form $$L(\varphi) =\exp\left( \int f(\varphi(t),\varphi^{(1)}(t),\cdots, \varphi^{(n)}(t)) \mathrm{d}t \right),$$ with $f$ a continuous function from $\mathbb{R}^{n+1}$ to $\mathbb{C}$ with $f(0)=0$. Is that result true? In order to express a kind of reciprocal result, can we characterize the functions $f$ such that the previous functional is a characteristic functional (main problem: its positive-definiteness)? Some people are extensively studied the positive-definiteness of functionals but wasn't able to find references that are clearly answering my question. Thanks for your attention. 

First, the (simple!) setup: I have a Markov chain X t on some finite state space Ω with stationary distribution π, and a function f from Ω to R. I'd like to estimate the integral of f with respect to π, which I'll write E π (f). There are theorems which say that $\frac{1}{n} \Sigma_{t=1}^{n} f(X_{t})$ converges to E π (f) as n goes to infinity. Now, if the $X_{t}$ were iid, then the Berry-Esseen theorem would give error rates in terms of n and (say) the maximum value of f. Are there similar theorems which give error rates in terms of n, the maximum value of f, and one (or several) of the frequently computed statistics of finite state Markov chains, like relaxation time, mixing time, covering time, etc? I'm vaguely aware of Sanov-type theorems for Markov chains, which give large-deviation results, but not in terms of these sorts of quantities, and I don't see how to convert the bounds immediately. Alternatively, I'd be very happy if anyone can give a reference for places that people have actually computed the sorts of error terms that do show up in statements of Sanov's theorem for some simple random walks. EDIT: Added Mark's comments, so that the question might actually make some sense now. In particular, fixed a missing f, and the rather more important mistake that in fact the CLT doesn't give any sort of quantitative bounds by itself. FURTHER EDIT: I accepted D. Zare's answer, since it certainly works. If anybody is interested in this question, I have since seen a bunch of articles, the latest of which is 'Optimal Hoeffding Bounds for Discrete Reversible Markov Chains' by C. Leon, which are a bit more specialized to the Markov chain case. I have also been told that Brad Mann's thesis is worth reading on the subject, but haven't yet picked up a copy myself. 

If the $X_i$ are i.i.d. Gaussian with variance $1$, then you have $$ c_p := \mathbb{E} |X_k|^p = \frac{2^{p/2} \Gamma(\frac{p+1}{2})}{\sqrt{\pi}}.$$ The variable $S_n$ is also Gaussian with variance $n$, therefore you have $$\mathbb{E} |S_n|^p = c_p n^{p/2}.$$ Hence, $\frac{\sum_{k=1}^n \mathbb{E} |X_k|^p}{\mathbb{E} |S_n|^p} = n^{1-p/2} \rightarrow \infty$ for $1<p<2$. At least, it means that you cannot hope for a constant $C$ as you expected. 

Numerous papers are referring to the following one R. M. Blumenthal and R. K. Getoor, Sample functions of stochastic processes with stationary independent increments, J. Math. Mech. 10 (1961), 493–516 in the literature, where they authors define what is now referred as the Blumenthal-Getoor index. However, I couldn't find this reference in the traditional databases, either on the website of this journal (does it still exist?). Does anyone have some information to find this paper? Thank you. 

Consider the Sobolev spaces with $p=2$, defined for $s \in \mathbb{R}$ as \begin{equation} W^{s} = \left\{ u \in \mathcal{S}', \ (1 + \lvert \cdot \rvert^2)^{{s}/{2}} \widehat{u} \in L_2 \right\}. \end{equation} It is a Hilbert space for the norm $\lVert u \rVert_{W^s} = \left(\int_{\mathbb{R}} (1 + \lvert \xi \rvert^2)^s \lvert \widehat{u} (\xi) \rvert^2 \mathrm{d} \xi \right)^{1/2}$. The weighted Sobolev space $W^{s,r}$ is now defined for $s,r \in \mathbb{R}$ as \begin{equation} W^{s,r} = \left\{ u \in \mathcal{S}', \ (1 + \lvert \cdot \rvert^2)^{{r}/{2}} u \in W^s \right\}. \end{equation} Again, it is a Hilbert space for the norm $\lVert u \rVert_{W^{s,r}} = \lVert (1 + \lvert \cdot \rvert^2)^{{r}/{2}} u \rVert_{W^s}$. We have the obvious embeddings, for $s_1 \leq s_2$ and $r_1 \leq r_2$, \begin{align} W^{s_2,r} \subseteq W^{s_1,r}, \\ W^{s,r_2} \subseteq W^{s,r_1}. \end{align} Now, is the following result true? Conjecture: Fix $s_1, s_2, r_1, r_2 \in \mathbb{R}$. Then, \begin{equation} W^{s_1,r_1} \cap W^{s_2,r_2} = W^{\max(s_1,s_2),\max(r_1,r_2)}. \end{equation} Of course, due to the embeddings above, $W^{\max(s_1,s_2),\max(r_1,r_2)}$ is included in $W^{s_1,r_1}$ and $W^{s_2,r_2}$ and therefore in their intersection. Is the other inclusion also valid? 

If there is no coupling s.t. the distance goes to 0 in $L^{1}$ (which I agree seems likely), you might want to look up an introduction to the Wasserstein-1 distance (which is exactly expected $L^{1}$ distance after an optimal coupling). This is the language that I've seen this type of problem most often discussed in. The field of finding optimal couplings for given metrics is 'optimal transport'. I vaguely recall that there are theorems about optimal couplings (in only certain $L^{p}$ only, of course!) never giving rise to crossing lines. In a 1-dimensional problem, such as the one you have, this would tell you what the optimal coupling is explicitly (in this case, if you construct your Brownian motion via Donsker's theorem, it says: whenever $W^{0}$ takes a move in the $\alpha$ percentile, make $W^{x}$ also move in the $\alpha$ percentile... in other words, my probably-misremembered theorem would imply coupling doesn't help your $L^{1}$ distance at all in this case). Cedric Villani has two excellent books on the subject, at least one of which was available for free download the last time I checked, and you should be able to find 'this sort' of theorem. Please don't take my word for the statements. Edit: Here is (I believe) a proof... though it might fit in the category of so-simple-its-wrong. First of all, we have starting points x,0 and add two normal (0,a) random variables X and Y to them. Plugging in the obvious cost functions, the "Kantorovitch Duality" formula tells us that the $L^{1}$ distance between x+X and 0+Y is at least x (while plugging in the independent coupling to the standard way of writing this metric tells us it is at most x). So, at time a, the $L^{1}$ distance between the brownian motions must be at least x (since at time a they have the same distribution as x+X and 0+Y, and we have found this lower bound for ALL couplings, and in particular all couplings that come from them both being brownian motions). In particular, the $L^{1}$ distance can't go to 0.