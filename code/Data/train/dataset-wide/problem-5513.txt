Quantum physics has several interpretations. One of them is the "many worlds interpretation," where every yes or no choice creates two universes, one where the answer was yes, one where it was no. Even in this case, which very clearly admits multiple valid paths forward from the past through the present, we refer to "the future." However, "the future" now refers to "the set of all possible futures." English users switch to "a future" when thinking about only a small portion of possible outcomes. It becomes "the future" once we no longer have a reason to believe there are any futures outside of it. 

Consider the game of "blackjack." Your goal is to get as close to 21 as you can, but not go over. Morals and taboos like these are very cultural. They shift from culture to culture. In fact, they can even shift from sub-culture to sub-culture (see: swingers). The takeaway may be that some cultures see some facets of society as similar to the blackjack game. The goal for a facet defined as such would be to get as much as you can, but not to go so far as to go past a line. If the culture chooses to express such goals in its handling of relationships, it is reasonable to assume "cheating" will get defined as such. One thing to consider is the power of the "nuclear family." In cultures which have the nuclear family, the relationship between husband and wife is typically given an elevated level of importance. The line between "being friendly" and "cheating" may exist because it was found to be a biologically meaningful line in the sand between behaviors that could be shared by all and behaviors which "should" be reserved for husband and wife. You also see this in many culture's dealings with death. There are a lot of things that are acceptable, but because death is so permanent, it is often given a special standing. It often becomes a line that one "should" not cross. 

If I may reword your question slightly, I would feel comfortable saying "Believing in uncaused events can be indistinguishable from believing in magic." I word this not from an ancient philosopher, but from a definition from Arthur C. Clarke, "Any sufficiently advanced technology is indistinguishable from magic." If you consider his quote to be a valid argument, then you should be able to see why my wording is valid. However, I will point out a detail that you might want to pull on before calling the topic closed: there is a difference between "uncaused event" and "event with no identifiable cause." The latter includes the individual's interpretation of events, while the former is a universal statement. I point this out because many English speakers will use "uncaused event" to mean "event with no identifiable cause" in some situations because it provides closure. An uncaused event needs no further analysis and can be filed away. An event with no identifiable cause must be remembered and kept open in case a cause reveals itself. 

(Emphasis mine). Such wording is a very powerful claim when speaking in the philosophical world. And, generally speaking, powerful claims are very hard to justify in philosophy. And, in fact, you will find that the scientific method actually does not always appear in every philosophy. I once attended a great lecture on what Traditional Chinese Medicine is, and the speaker explained a difference in how their tradition is developed as compared to Western medicine: 

I think it may be fair to say that you are wise when you find yourself clever enough to no longer need to be clever in life. 

Its tricky to square them. Hegel makes quite a strong declaration of what a man can do, which is in direct contrast with the story Orwell gave. There's a few approaches I could see to use: The most obvious is found in one of the the final phrases in Orwell's book: 

These are traditionally seen as strengths. However, they can also be seen as weaknesses (like all good superheros, their strength is their weakness.. that's what makes them interesting). The scientific method is completely and utterly useless without statistics. This means any singular event is completely beyond its reasoning. It cannot provide answers to topics such as "the purpose of your life" because there is only 1 you, and N=1 means there is no statistics. Related to this, the scientific method strives to be objective. It always tries to remove the observer from the picture. This is very valuable, because it ensures that your discoveries are applicable to others. However, it also proves to be tricky in many situations. Social studies in particular have great difficulties with the scientific method because it is so very difficult to make good tests that keep the observer out of the loop. As an example, TCM claims that acupuncture works. Those who have tried it, claim it works with uncanny success. However, science has had fits trying to find any effect of acupuncture beyond the infamous cop-out "the placebo effect." The issue is that it is almost impossible to develop an effective control to measure against because the acupuncture practitioner knows if they are doing it right or not. Whether you believe acupuncture works or not depends heavily on whether you accept results which lack a solid control to ensure objectivity. Finally, science tests its theories. This sounds absurd, because it seems so obvious that you should test them. However, a theory is not accepted at all until it is tested. The result is that anyone with a theory must expend the resources to do the testing before science will do anything with it. Other approaches get away with a different style: you use a theory once you have it, and you test it when you get an opportunity to do so. The tests can also be dangerous. (Edit: I had a reference to the LHC and potential to create black holes here, but it was too contentious. Instead, it has been replaced with a hypothetical example) Consider a hypothetical particle physics experiment. The scientist is rather confident that their theory is correct. They begin experimenting, after calculating that they would like 100 samples to do statistics on. Generally speaking, they are finding their theory holds out for test after test. However, on tests which disagree with their hypothesis (which happens in the scientific method due to noise), the observer notices a burst of energy from the test apparatus. That burst becomes stronger and more dangerous with every data point that disagrees with their hypothesis. At some point, the scientist decides to cut the experiment short, because they are uncomfortable putting their life at risk to finish the test. By the strictest reading of the scientific method, that data cannot be analyzed because it is tainted with the scientist's choice to cut the tests off early. This might induce biases because the scientist is more likely to cut them off faster if the results look good for their theory. Other methodologies are capable of using this data (including the intuition of that scientist, who will not try the exact same experiment again). Seeing that the strengths and weaknesses of science are so confounded, it is up to each individual to decide if those are ideal for them. There are many others, none so visibly different from the scientific method than that of TCM. As described to me in a lecture, the difference is in the approach towards healing the human body: 

I have noticed a pattern in philosophies where the successful ones I find all believe that everything, when viewed as one whole, is "good." This may be reached by assuming everything in the universe is good. It may be reached by assuming there is something outside of our world (like a deity) which is more good than all the evil in the world. It may be reached by assuming you have one dollop of good in you which can overcome the world. But they all seem to view the world as "good." I'm playing with the idea that "Everything, as a whole, is good (or at least not bad)" is a requirement for a successful philosophy that defines "good," and haven't come up with any counter examples. Nihilism gets close, by arguing that the world is not good, but not evil either. Are there any philosophies which view the whole of everything as evil (or bad, if that's an easier word)? The wording here has given me trouble, and from the comments I can see that I did not convey what I sought very well. In the first paragraph, I used "universe" and "world" differently, though I don't believe I was clear enough that I sought a distinction. The intended distinction was that "universe" would include some deity which is outside of our physical "world." To use Christianity as an example, its followers overwhelmingly believe that God's good is so great that it outshines all the evil in the world (and in hell), such that the sum total of everything is "good." 

"God of the Gaps" is not really a single argument, but a collection of related arguments which all use similar terminology. Some of them simply combat teleology, others seek to directly discredit theists. As such, whether any of them are "correct" depends greatly on the specific argument being made. However, all of them have a common attribute: the assumption that there is some part of the universe that is knowable via some non-theist means, typically empirical means. Thus I think that general statements of correctness can be made about a "God of the Gaps" argument in any case where we assume we can know something. Skeptics such as Aggripa the Skeptic made a living challenging such assumptions, and we have not really shaken those challenges, even over twenty centuries later. The question of how we can say we "know" something is at the heart of all empirical philosophy (indeed, all of epistemology). And in every theory that I know of which claims we know something, we run into something akin to the questions of what we can truly "observe" that show up in the philosophy of science, or the idea of the "first mover" in the metaphysics of causality. It appears we have a gap in the concept of what "knowledge" really is. And this is a powerful gap indeed. And whether you call it God, Brahman, the Dao, or simply call it "the unknown," it doesn't seem to be retreating from this particular gap all that rapidly. So while the "God of the Gaps" arguments tend to rely on the idea that these gaps become less important as other approaches (such as science) mature, they are built on those gaps. The validity or "correctness" of any such argument depends mostly on how comfortable you are with that. 

A key concept in science is the ability to consider any hypothesis, no matter how radical. If you wished to suggest the hypothesis that the moon is made of cheese, you are absolutely within your right to do so. However, they also need something which can describe a product of their scientific endeavors which has some meaningful value. These products are typically more abstract that the raw hypotheses that got tested, and they have substantial evidence suggesting that they may be true. These get called "theories." If you wanted to claim that you had a theory that the moon was made of cheese, I'd call you on it, demanding to see your evidence. In between is a very murky ground. There's definitely no clear dividing line between a hypothesis and a theory. A hypothesis may be too narrow to earn the title of a theory, even if it is heavily tested. An abstract idea may be insufficiently tested to call it a theory. As such, I would say the definitions you provided are simplifications for every day life, but they aren't far from the truth. They just miss out on the exciting process of actually turning a hypothesis or hypotheses into a theory. If you are willing to consider subjective differences, one difference you can rely on is that if you have a "theory," at least someone believes enough evidence has been collected to believe the theory is true. Typically it implies that the speaker believes it to be true, though that is not always the case (e.g. creationists may talk about "evolutionary theory," but from their word choice, you'll know they don't actually believe it themselves)