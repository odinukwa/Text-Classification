The key is in the names: high safety is safe (but slow), and high performance is faster (but less safe.) You've noticed that when you suspend the mirror, things get faster - that's because your mirroring is set up with high safety. If your business is willing to lose data when the primary fails, then you can keep doing mirroring, but switch it from high safety to high performance. (You can change this at the database level at any time.) If your business is NOT willing to lose data, then it's time to start troubleshooting which layer of mirroring is causing you problems: 

Why? Because SQL Server by default chooses the victim by whichever transaction is the easiest to roll back. In our case, Window #1 had done a lot of work already, updating thousands of rows, whereas Window #2 had done just a tiny amount of work. This is a contrived example in order to teach the lesson quickly - your scenario probably doesn't involve an ALTER TABLE command. Still, the same theory applies. 

Just because if you're getting started with transaction troubleshooting, I'd rather give you something easier to use than DBCC commands. 

There's three separate parts to this: Q: Do I use SQL Server's built-in log shipping or roll my own? A: I'd roll your own. What you're doing is beyond what the built-in tools are normally used for. Q: If I roll my own, where do I start? A: I'd use normal SQL Server full and transaction log backups on the primary. On the secondary, start with MSSQLTips' script to automatically restore backup files from a folder. That example will restore all of the files, though, not just some of them - you'll need to adapt it to only restore log backups in there. In a perfect world, you'd only attempt to restore log files that haven't been restored yet, but if you're in a rush, you could restore all of the log files every time - SQL Server will automatically skip logs that have already been applied. Q: How do I alert when files aren't being restored? Rather than focus on the files, I'd focus on the data. Do restores with standby when you restore the logs so that you can run queries against them, and after each restore job finishes, query your most transactional table looking for the newest record. If it's older than X minutes, your data may not be coming across. (Granted, this only works for databases with decent change rates, like adding records every few minutes - if you don't have that, then some DBAs add a dbo.LastUpdated table with a datestamp in it, and have a SQL Agent job that updates the only record in there every 5 minutes to set the datestamp to current.) 

Oh goodness, you have a lot of questions in here. I'll try to unpack them all. Q: What is "SQL Server in the cloud"? The term "the cloud" is too generic - you might as well be saying "SQL Server on the network." There are lots of different ways to deploy SQL Server in the cloud, some of which function exactly like on-premises servers (it's just that someone else is managing the server for you.) If you can, try to avoid generic sweeping statements like "SQL Server in the cloud" because you won't get good answers. The more specific your question can be, the more relevant advice you can get. (Not to mention the less downvotes, ha ha ho ho.) Q: Is there a universal rule for clustering keys? Generally, you want your clustering keys to follow the SUN-E principles: 

Generally, running compression/decompression on the SQL Server itself is a bad idea. SQL Server is licensed by the CPU core, so it's the most expensive place in the shop to do CPU-intensive things like compression & decompression. Instead, consider offloading that to an application server. Now, having said that: SQL Server doesn't support the installation of programs and doesn't have a way to bundle 3rd party apps with the install. You'll want to manage the installation of other apps with a software deployment product or an installer script. 

You didn't include the exact query to reproduce the issue (it doesn't happen every time), so here's a few different ways it'll happen: Using WITH (NOLOCK) - this hint in your query can cause records to be read twice, to be skipped altogether, and for your query to fail outright. (The same thing can happen with the SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED hint.) An error in the SELECT query - for example, if you're joining multiple tables together to get the data for the INSERT, and you have an error in the join (or a legitimate join that produces multiple rows of results), you can get duplicates. Concurrency - say that while you're working, someone else is inserting and removing records from the source table. They might have temporarily had duplicates in the source table, and when you go to look later, they're gone. (I've hit issues where people would insert a new version of a row, then delete the old version of a row, so some selects out of that table would see duplicate rows. They couldn't implement unique constraints in the database because of that code pattern.) To get an exact answer for your exact situation, post your exact code, or code that reproduces the issue you're having, and we'd be glad to dig deeper. And welcome to Stack! 

Depends on your client libraries, the way your app handles memory, and what you're doing inside the fetch. Pulling back one row at a time is a very dangerous design pattern for database servers to handle. They have to materialize all your results first, then spoon feed them to you one row at a time while you slice & dice results. You might be holding locks or slowing down the database server. Instead, pull all of the data into memory on your app and do the slicing and dicing after you've let go of your locks, OR do your slicing and dicing in batches on the database server. Batch processing is way more efficient than row-by-row processing. In the DBA community, this is known as ReBAR - Row By Agonizing Row. 

You've got two different questions in here that I'll answer separately. Q: Can the performance of one table suffer from other large tables in the same database? Yes, during insert/updates/deletes. SQL Server only has one log file per database. When you insert/update/delete, your transaction doesn't complete until your data is written into the log file. Let's say we have two tables: 

You have several different questions in here, so I'll knock 'em out individually: "I've read that it's a best practice to not let users use the sa login directly, instead using Windows Authentication" You're mixing two things here: the concept of SA, and the concept of SQL authentication and Windows authentication. SQL authentication is a list of usernames and passwords that are stored in each and every SQL Server. The fact that it's stored in SQL is the first problem. If you need to change a login's password, you have to change it on every server (or maintain different passwords on different servers). With Windows authentication, you can centrally disable logins, change passwords, set policies, etc. If you choose to use SQL authentication, then SA is just one SQL authentication login. It's the default administrator username, just like Administrator is in Windows authentication. It has local superpowers on that one instance, but not global superpowers across all instances. "...and allowing those accounts (or account groups) sysadmin privileges." Whatever authentication method you choose, ideally you want to follow the principle of least privilege: granting people the bare minimum rights they need to get their job done, and no more. Don't think of them as just logins - they're people who can get you fired. If they drop the database or disable your backup jobs accidentally, they're not going to get fired, because by default, SQL doesn't track who did what. You're the one who will get fired because it's going to happen, and you're not going to be able to say which person did it. "How does the best practice increase the security of my SQL Server instances?" You want to do two things: 

No, as of 2018/04/25, you can't choose the filegroup where Query Store data is placed. In the databases where you enable Query Store, the data goes into the default filegroup. It was a highly upvoted Connect item, so other folks share your pain - so I would expect that to be an enhancement added in the future. Thanks to @klitzkrieg, here are the current feedback items: 

Here's the line I like to use - it preserves everyone's dignity and lets everyone escape without finger-pointing: "Yes, that used to be a best practice, and..." It’s almost easier to explain that line in terms of what it DOESN’T do. It doesn’t dispute the speaker’s claim, because you don’t want to go down the rathole of arguing about whether or not the point was right at the time. You could spend the entire day arguing about why USB has the word Universal as part of the name, but that’s beside the point. You need to get to the right solution as fast as possible, and sometimes that just means ignoring something that’s wrong. It doesn’t chastise the other guy for not keeping up with the latest tips and tricks. It’s hard enough to just do our jobs, let alone keep up with blogs and training, and real life interferes. It lets the other person save face. It doesn’t divide you against the other guy. I try to use the word “we” whenever I use this line, because I want to take the other guy along with me on the journey to fix this problem. I've written more about it in my Consulting Lines series. 

SQL Server's STRING_SPLIT documentation does not specify an order guarantee. There is an open feedback request to add more features to STRING_SPLIT, including sorting, but as of this answer (mid-2018) the status is just "under review." For now, if you need to guarantee output order, try other string splitting techniques. 

When you perform a transaction - like deleting 600K records all at once - SQL Server has to log what it's doing in the transaction log file as it works. Say the process takes 5 minutes, and 4 minutes into it, the server reboots for some reason. SQL Server starts up, opens the log file, and then needs to undo all that work (because the transaction never committed. To avoid having a log file that large, you can: 

Nick Craver ran into performance issues with this too in Opserver. Here's the query he ended up with after working directly with MS folks on the query plan - the joins look kinda contorted, but as I recall, that was the only way he could get consistently good performance and avoid the timeout issues: 

There's a few gotchas in here: you've said that the queries will be short-lived, that you're going to have (relatively) high volume, that you're going to capture parts of the queries, and that you want to communicate back to the app tier. This means you want a non-blocking, high-performance solution, and there's nothing built-in that will make this easy. I'm going to just talk theoretically about something you could build, but it ain't gonna be easy. Part 1: capturing the query data as it happens. For this, SQL Server's Extended Events are probably the way to go. Think of it as debug points in the SQL Server engine code where you can insert your own code. As you learn about it, you'll find that you can gather all kinds of cool information about queries, but keep in mind that this is blocking code, and the more information you want to gather, the slower your queries go. You won't be able to push data outside of SQL Server in this part - you're going to have to either write the data to an XML file, or put it in the ring buffer. The ring buffer makes the most sense here since you'll need to query it in near-real-time. Part 2: getting the data from the SQL Server's ring buffer to the client. You could build something to push the data from SQL Server to the overlay tier, but this screams danger to me. It couldn't be single-threaded, because you could hang on trying to send a notification to a client that's running slow, and miss everybody else's notifications. Instead, what I think you want to do is have the overlay app detect on the client side when it needs to check the ring buffer, and then do that via a query. You don't want all the clients continuously polling the ring buffer - performance nightmare there.