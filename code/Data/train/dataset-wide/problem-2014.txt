I'd like to see how many transactions per second an instance of MySQL server is processing, on average (per hour and per day). Is there some query to determine this? I've looked through performance_schema as well as information_schema, but nothing seems obvious. I am running version 5.6.14 for x86_64 on debian6.0 

-- Reference Since you're using version 6.2.2.0, they're closed after three minutes by the client, which doesn't cause the server to treat it as a warning. Solution: Increase to five minutes or so. Note: watch the number of connections to the server to make sure that having connections able to SLEEP longer doesn't cause the total number of connections to approach . 

(These happen about 10 times per minute. The counter is up to 13,000 after a server uptime of only 24 hours.) We have perhaps twenty or thirty instances of a Windows client program that uses the .NET framework via Connector/NET (6.2.2.0) to connect to the database and I've made sure that every time it creates a connection (MySqlConnection.Open()) it is also properly closed (MySqlConnection.Close()). I've adjusted various timeouts, , and other settings based on research on the web, but these still seem to persist. Here are (what I believe are) the relevant settings: 

I installed MONyog recently to check for problems on a MySQL server that I admin. Most of the information was useful and helpful, but there's one warning I am not sure is worth fixing. It reports that commands are excessive (currently 9+ million or 58 per second), or unusually high. The connections to the database are strictly Windows .NET applications and they use Connector/Net as an API. I typically connect such that I specify which database to use when creating a object. I discovered in a test that doing this causes the to increment by one when the connection is established. I can "fix" it by not specifying a database, and instead qualifying the database within the query every time. (e.g. db.table.col instead of table.col) Is it worth going through the entire code base of the application to make this change? Is MONyog concerned with something that doesn't matter? 

Here, you can use sequence to increase value. Please noted if the sequence reach to its , you will face this error . For example 

After testing with 1 million row, here's the best way from @Julien Vavasseur combine with Index on "user_id, grade, grade_date" . We should consider using window function when having a lot of rows. 

Description: We have a table on PostgreSQL 9.3 and we migrate it (data & structure) to Oracle 11.0.2.4. Here is our table: 

We want to index and search "text" on log_info column, so we tried Oracle Text 11g . Problems: If we use "data_type" is "clob", we can use "context index" and it have to synchronize after DML . ( we can not use this way because of (**) ) If any, how can we index and search on "log_info" column (max_length < 10000 and data is changed every second) ? 

It takes 10 milliseconds to execute the query on the user_info_raw (remote server). But, It takes a lot of time when using theforeign table. When I remove , the query executes very fast. I think that my query on foreign table should send to the remote server for executing, but it's not, I don't know why, may be due to of this excerpt from postgres_fdw document 

Problem: I tested over 10 times, each of times when data is inserted about 2,000 rows (2,000 records is commited into machine 2) , with insert time about 3 minutes , I got errors: 

ask yourself if fragmentation is really causing you an issue? Fragmentation is only really a problem when reading from disk. Cache all your data in memory, and concentrate on statistics maintenance (link). Depending on the size of your database, if you can't cache it all in memory do you have a resource problem (e.g. RAM), as opposed to a fragmentation problem? Could your server be getting a high volume of hard page faults, and you don't even realise it?! How big are your indexes? If your indexes are less than 8 pages then they will be stored in mixed extents, and no amount of index rebuilding is going to solve fragmentation... move on! What keys are in your index? If any of the keys use something like 'uniqueidentifier', especially as the first key column, then I'm afraid you are unlikely going to solve fragmentation. Due to the nature of this data type, no sooner than you have rebuilt your index, it will be fragmented again after the first few inserts... that's the nature of the beast unfortunately. If you decide that fill factor is a route you need to go down, DO NOT set it globally for all indexes. Doing so could actually make performance worse. Fill factor increases the free space on index pages, ergo making the indexes larger in size. For an index that has an incremental key, reducing fill factor from 100 (or 0) will likely hinder performance, depending on the size of the index, because you will be causing SQL Server to read more pages to acquire the same amount of data. 

Question: Our solution: write 3 SQL queries search on "c1", "c2", "c3" are followed by conditions above. How can we do that with lowest performance ? 

Firstly, I have no idea about pgAdmin4 Secondly, It would take a lot of time when executing a large SQL file. Please consider using pg_dump (custom format) & pg_restore to enhance your performance. 

In EDB 9.3 , just one query no.4 can cast. And PG 9.3 , all queries can do. Are there still any ways to cast date in EDB ? EDIT: With EDB : 

2/ QUESTION: What is about "FATAL: terminating walreceiver due to timeout" problem ? How can I fix it ? 

Machine 1 (slave) and machine 2 (master) are in a cluster (streaming replication). Sometime, I see "FATAL: terminating walreceiver due to timeout" in slave log. Here is full detailed logs: Slave 

Format CSV, disable quote (replace multiple spaces to one space then space & to ). is result file. However, please check your input data if it is large because I tested on small data . 

Erwin said "You probably don't want to hear this, but the best option to speed up SELECT DISTINCT is to avoid DISTINCT to begin with. In many cases (not all!) it can be avoided with better database-design or better queries" . I think he's right, we should avoid using "distinct, group by, order by" (if any). I met a situation as Sam's case and I think Sam can use partition on event table by month. It'll reduce your data size when you query, but you need a function (pl/pgsql) to execute instead of query above. The function will find appropriate partitions (depend on conditions) to execute query . 

I believe it depends on what you set max memory to, what is available on the host, and what edition you have. Aaron Bertrand wrote a good blog post on exactly this: $URL$ To try and answer your question, "If the server has less than 128GB, you will see these technologies compete with buffer pool memory, and in fact be limited to a % of max server memory." 

In my mind there are a few ways you can achieve this, none of which are simple solutions. Which ever way you approach this, I think you will need to make two passes in to the database: 1) to get a list of current company names 2) to get all the contacts associated with chosen company. First up you need to decide where/how you are going to store your company names. You could either store them in a local table, and provide a lookup via the CompanyName column using a Data Validation list option. This will mean that only the names in the list can be selected. Good for data integrity. Or, if you don't want to store a local table then you will need to execute some vba code to connect to the database and download the company names. Iterate through the list of names and deliver the options to the user in whatever format you like. Alternatively, you could manually write a list of names, but this will require additional maintenance when new company names are added (not ideal). Or, if you are really brave/mad, let the user write the company name in (I don't recommend this!!!!!). Back to your original question, if you are using MS Query you should be able to parameterise your query in the query editor (if I remember correctly?!). Parameterise it such that the parameter value is taken from the cell where the user selects the company name. MS Query should do the rest for you... provided the query is parametrised client-side (i.e. in excel). Alternatively, you could once again go back to vba and dynamically create the query string in the code, passing in the CompanyName to replace @CompanyName and executing a full un-parametrised query. If your query is stored server-side then perhaps create a stored procedure to accept @CompanyName as a parameter (you will need to write your DECLARE in your usp, as previously mentioned). Then, again using vba code, you can dynamically create the EXEC statement to execute against the connection. These are just a couple of options, I'm sure there are others. But if you are looking for code as an answer, then I think you may be out of luck as there is more going on here than just "declare the scalar variable @CompanyName". 

Master ----> Slave : relpica from Master to Slave by asynchronous method (M send WAL, S receive WAL) 2/ Question: How can I monitor (catch) speed of WAL (ex: 1MB/s) is sent from Master to Slave ? 

On the other hand, I could not find the AccessExclusiveLock mode in the pg_locks at the same time. Does anyone know why it happen ? As Tom Lane's message, It can be a lock on Notify queue. 

I read on Cassandra Calculator, with those configurations, it said that Each node holds 50% of your data. Case: , (* ) OK when I stopped or Question : 1/ Why , (* ) stopped when I stopped 1 in 2 nodes ? 2/ Why "Each node holds 50% of your data" as those configurations? 

I am using nethogs command to tracking WAL send/receive between master and slave. Download: nethogs for centos link Install: Tracking: Ref: 18 commands to monitor network bandwidth on Linux server 

Question: After researching , I found 4 solutions. Are there any better solution (not using copy data from files) ? 

About errors above, because of the connection between pgpool and slave server, if I change slave's pg_hba.conf for pgpool host from md5 to trust, it work fine. Two ways to fix: 

Here our query and result we want, it means: when deleting id = 1 (parent row), table will automatically set parent_id = null in child rows (first level) . 

I want to maintain a clean environment, so when somebody looks at the model in future they don't see users who left years back. Ultimately, when somebody leaves their AD account will be deactivated so this is less of an issue and more of an OCD. However, new members will need to be added. I'm leaning towards option 3, but are the above options my only choices? Has anybody deployed a different model? Am I missing something obvious!?!?!? I hope not. Thanks in advance for any help/tips/advice. EDIT: I have also just thought of using PowerShell to add/remove users. Based on what David said, I could get one of my guys to maintain this instead. 

Index tuning is a science, there isn't a "one size fits all" solution, so when you ask "should I aim for a fill factor of 90%", this is a genuine time when "it depends". I already posted this link above, but I really recommend you read it, to understand the impact fill factor has. There are tools out there to help you on your quest (and I don't mean the Database Tuning Advisor!!!). sp_BlitzIndex is the one that I have stuck to over the years... hence my numerous links to their websites!!! :-) 

Do this manually... not dynamic! Create user groups in AD and add the user groups to the roles. This will be ok for "full read" users (e.g. Directors), or "manager" roles (e.g. sales manager for sales reports), but would not work for users that will be using row-level security (e.g. salesperson). Create a "global security" table to add to the model, which contains the 'variable' for read access permissions for "full read" access (e.g. Directors and Managers). I will also have another security table that will hold the data for row-level security for other users (e.g. salesperson filters). Then each role in the model will essentially use row-level security based on these security tables. Use TMSL to process each deployment in the SSAS instance and "createOrReplace" each role deployment where this user exists/needs to be added... unfortunately I don't know any TMSL so I don't know how hard this is to learn?