If you execute a use database statement, subsequent queries will only execute against that db. However: this is only for the current tab. In SSMS to see the connection context look in the bottom right corner of the current tab. You should see the current database name there. Or you could query the current db with: 

I found a workaround, but not one I'm especially happy with. We managed to use our Website monitoring software to hit the SSRS report server, and it seems to do just enough that SSRS web component starts up. The first user of the day now gets a response within 4 seconds. I have spent hours trying to get powershell to do this for me, and got very close. So I'll leave this here. If someone else can solve the last piece of the puzzle and make this work I'll credit them with the answer: 

It seems the problem is that SSRS somehow treats these connections differently than if a real user opens the report through a web browser. Even though the report is run it doesn't appear to be handled in the same way as if a user is hitting it via the portal. And so.. the first user experiences a big delay. Using the F12 debugging features in internet explorer, I managed to reproduce and capture what the browser is seeing. SSRS only took 4 seconds to produce and return the report, however in the browser it took 122 seconds from trying to open the URL to the screen being rendered. 2 minutes!!! No wonder users are complaining. Looking at my SSRS log files ( Located in: C:\Program Files\Microsoft SQL Server\MSRS13.MSSQLSERVER\Reporting Services\LogFiles) it appears that the SSRS appdomain is restarting or reloading. The Log excerpt below shows 

There are lots of articles and references available that explain how they work together such as this one. The keys words to search for are "sql server grant & deny". Permissions can be granted on the whole database, a schema or a specific object. (Table, view, stored proc etc) This can be applied per user or via a Role. Rather than modify the existing roles, you could create a new role and grant the permissions you want to the role, and the give users that. The most suitable method depends on how many tables they should/shouldnt have access to and how many users you would have to repeat this for. If there is 100 tables in the db, and the user should read 90 of them, id grant + deny. If you had to do the same thing for 5/10/50 users i'd create a role. 

And a similar error for the start service command. My SQL agent is running with local admin privileges. So I assume is has sufficient privs to do this. What am I missing? Is there some technical limitation in SQL Agent stopping this from working? 

I use SQL Agents tasks to execute my backup, maintenance & ETL tasks. IT would like to have a regular maintenance window to apply server patches. They're suggesting the 3rd Tuesday of every month outside work hours. Can I configure jobs to run on every day except that day? The other option I thought might be possible was to have multiple schedules configured to somehow exclude that day. 

I've recently installed SSRS PowerBI on premise CTP update v14.0.1.353. Since then we've noticed a few issues and decided to uninstall or rollback to the previous version. As we noticed SSRS was still installed we decided to try and revert to the old version, and learned some painful lessons in the process. With the benefit of hindsight i should have asked: Can I have both SSRS and PowerBI on-premise installed together, and how should i configure them? 

MDX & DAX are query languages for OLAP cubes. MDX is used on multidimensional OLAP cubes and is used by many vendors. DAX is Microsofts own query language and can only be used in a tabular OLAP cube or within Excel. Assuming you are using Microsoft SSAS you must choose which type of cube you want when you install SSAS. If you (or someone else) has already installed SSAS then you need to find out if its Multidimensional or Tabular, and then use the appropriate query language. 

But it provides no further details about how to install it. It mentions an ODBC Driver Manager but not much more than that. So.. How do I install RODBC? Do I unzip it to a specific path? Is there a utility or tool required to install it? Do I run a script? 

If we are talking about throughout and the scenario you describe i would consider that your requirement is 10 x 6kb/sec which would be 60kb/sec. So yes assuming you had 10 concurrent requests and you tried to move 10 x 100kb it would take 17 seconds. In theory 1000kb / 60kb per sec = 16.66sec. In the real world if the app required data from the db, the response time would vary depending on whether the db had to parse the query, create an execution plan and then read from disk. This all adds up to delays (latency) before the data starts being returned to the app. If this is a commonly executed query (for example in a stored proc) the query has already been parsed, execution plan created and data already held in memory this means the initial delay would be much lower. I would say it would be rare for an app to consistently consume data with 10 simultaneous connections. I guess a video streaming app could set a constant load for long durations. But more likely as other connections start and stop there would be more or less bandwidth available so transfer times could be lower. I would expect to see fluctuations in transfer rates. Also: watch out for kilobit vs kilobyte, And expect there will be some overhead. I.e It takes more than 6kb to move 6kb worth of data. 

Jynus's answer is very good, but i would also add that in some cases more than one table in a query can have the same field name which is ambiguous and must be resolved. Although I guess you could use a full table name as a prefix. In my experience when dealing with extremely long table names that can have very similar spelling, using aliases makes the code much more compact, tidier and more readable. 

If you dont have access to a sql instance you could install sql express or developer on your pc. If you are unfamiliar with sql server there will be a bit of a learning curve, but it will be well worth the effort. SQL comes with some powerful tools that open up a lot of options for you. Learning SSIS might be too much for this project but you can achieve a lot with the import wizard in SSMS and save the packages to be reused. 

Indexes don't have to be unique. Primary keys do. The purpose of a Primary Key is to uniquely identify a single row of data. If you don't have something that is naturally unique then add a column such as an identity column and define it as the primary key. It's standard practice, and will help you later if you need to update a row. If the table is being queried for analysis or reporting a single table is fine. Do some analysis on the types of queries you are performing and add indexes on relevant key columns. Good indexes will significantly improve query performance. For example if you are looking for results for a single store, by adding an index on the store id, you could exclude the other 299 stores from the select. This reduces IO and speeds up the query. If you have years worth of data but are only looking for things that happened in the last week then adding an index to a date column may be a big help. Look at your queries and see if there fields you are regularly filtering on. Start with those. Ensure the db has updated statistics for the table and see if performance improves. The results you get will vary depending upon what you are trying to do. If you are trying to aggregate records (i.e total sales for each store) then the query may still have to read the whole table. Below I've added a script with a basic test example. 

You could query before and after looking for the particular conditions you are wanting to update. For adhoc queries it's a good idea to run the query as a select statement using the same where clause before running as an update statement. The update should update exactly the same rows select returned unless the data has changed. 

Personally i have staging tables for extract, transform and persistent data storage. Whether you do full exports or incremental loads will depend on what tools you have, your strategy and whether your app schema and data support it. Sometimes you cant avoid full exports. Adding a column to a dimension isnt a big deal, but backfilling historic data could be very difficult or may not be possible at all. Trying to reconstruct how an app looked at a point in time retrospectively would be a major undertaking. You would need a very good case to justify that. All of the things you mention are possible, but only you can decide if the cost/benefit is worth it. 

You could use the sqllite date and time functions to convert your string to date and then you can make your comparisons. Probably using strftime('%d/%m/%Y',[your date]) 

As each of the various workarounds I have tried so far don't seem to work... How can I force SSRS into doing this before a real user attempts to run the report? 

All relational dbs should be "good at aggregations". Thats what they're designed for. Most db's follow similar design rules. To produce your result, they all have to do the same math. Im not familiar with other products but to consistently achieve response times like that you'll need to have the data in memory already (cached) and/or pre-calulated. Either using an aggregate table or an OLAP cube. The first time your query runs will be the slowest. If you can reuse the same query it will be faster. The more you can reuse the same query - the faster on average it will be. If you can pre-execute the query ahead of time your client may never experience the delay. Re-use and cacheing depend upon how often the data changes and how often the same query is executed. If the underlying data or the query changes, you go through the over head of calculating the execution plan and io again. If the data changes how quickly do you need the aggregate to reflect the change? If you are developing a proof of concept sql developer edition has column store indexes and memory optimised table features which may help, but if this goes into production you'd need Enterprise edition which isnt free. You could use an SSAS OLAP cube with SQL standard but there will be a lag while processing. However SQL Std isnt free either. It may be possible to achieve this with SQL express and standard query tuning and/or aggregate tables. Frequently executing the query will assist with caching which will help performance. Stored procs and parameterisation will also help. I doubt just picking a different product will magically make this query go faster without some other compromise. They all have to do the same math. They all have to write to and read from disk at some point. Based upon the volumes you have described and the desire to use 'free' dbs i would consider sql express and aggregate tables updated periodically. 

Your results may vary depending upon machine setup and load, But I think you should resonably expect to get your query down to ~1 sec or less. 

To answer your question directly: No. Using floats for a primary key is not a good idea. Why? Because floats can run into problems with rounding and precision (Is 1.20 the same as 1.2? Because mathematically it is.) Personally I would create a primary key on a single field. Preferably an integer. Integers are typically more efficient than floats and there is no ambiguity about interpretation as with floats and strings. The most important function of a primary key is to uniquely identify a single row in a table. It doesn't have to have any meaning or purpose beyond that. If it serves another purpose thats a bonus. In your case, (as your title implies) I wouldn't use the distance as the primary key (on its own) because it may be possible that 2 combinations of stores are the same distance apart. You could use it as part of a compound key. But in this example it doesn't make sense unless you wanted to store multiple distances between the same 2 stores. To go a bit further: This looks to me like a spatial query issue. If you store an XY coordinate with each store, you can then do things like a radius search. "Give me a list of pizza shops with 1 mile, 2, 5 , 10 miles" etc. Or even better from a customer address you could determine which shop is closest. If a new store is added it would automatically be included in the result set without having to manually calculate the distances. You could also calculate the distances between every shop. The only shortcoming of this approach i can think of is that the distances are straight line and may not accurately reflect the distance you may have to travel between shops by road. Depending upon what you are trying to achieve it may be useful to store the distance rather than recalculate it, but i personally prefer to derive values as needed. 

Technically we dont need 3 schemas. It's just a convention, and it allows us to keep things tidy. Read about Kimball methodology and Slowly Changing Dimensions. Type 1 or 2 etc. You will see examples of what fields you will need to store the meta data required. Talk to your business about what they want or need the DW to be capable of. Whether you need SCD's or not will have a significant impact on your schema, meta data required and ultimately what level of normalisation that is possible. 

Why? What am i missing? The inner query has filtered and converted the value I am evaluating. Is there a better way of filtering this list to be used in an update query? 

I believe we've found another workaround. I'm posting my answer as I think it may be useful, and it's different enough from wBob's suggestion. We've changed the insert part of the merge statement so that it inserts to a temp table rather than the original target. Once the merge statement executes, we then insert from the #table into the target. It's not ideal, but at least the merge still handles the complexity of the 'upsert' by marking rows that have been retired/expired. We found this to be an acceptable trade-off compared with completely re-writing the merge as separate inserts and updates. 

I've been given the task of producing a report which will extract data from a web service. The web service provides data from a list of sites including aggregates. Lets say total sales by month. Id like to plot these values on a map and locate them using geometry points. The developer is investigating how he can include the spatial reference in the xml reply. But before i get him change the Web service is there any limitation or restriction in ssrs that would stop this working? If so, what are my alternatives? If i can retrieve the easting and northing in the xml can i convert to a spatial datatype with an expression on the fly? 

Once you have a clear requirement, it will allow you to shortlist viable solutions. Within your shortlist you can start to express preferences and pros and cons for the evaluation process.