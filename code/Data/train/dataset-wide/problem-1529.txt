To see the difference between them, we have to plot the difference (plotting the residuals, in data analysis parlance). This next graph is the difference, zoomed in by a factor of 100 in the $y$-direction. 

The center of the galaxy is densely packed with stars and obscured by a whole lot of dust between us and it. For those reasons, groups that study the motion of stars around the super-massive black-hole in the center of the Milky way need to use big telescopes at near infrared wavelengths. Questions of dust obscuration aside, what resolution would a telescope need to have to pick out a background quasar looking through the center of the Milky Way? SDSS QSOs start at around 8th magnitude (Vega) in 2MASS photometry ($H$ and $K_s$ bands), and rise like a power law as flux goes down Here's a quick and dirty plot of SDSS QSO survey made from SDSS data release 10 made using topcat. I would convert the graph to counts per solid angle, but I'm too lazy to look up the survey area of the SDSS spectroscopic survey right now. 

Scientists cannot theorize clearly if minerals and elements are arranged in plumes and strata inside mars, and the diameters that make up the martian condensed/fluid inner/outer strata and plumes, they don't know if there is something fluid and metallic [mobile electrically charged plumes of (probably) heavy elements (perhaps including iron, the 4th most common element on our world's surface), and heat from radioactive decay]. They will know more if there is any thermally driven geological movement inside the planet, including molten iron, if they can find any small temporal fluctuations in the magnetic field. For the moment, a molten core only gives way to theories on "paleomagnetism, a latent magnetic crust of 10-125Km" deep stating that all the magnetism is left over from 4 billion years ago, and that certain zones have been de-magnetized by impact collision. Meteorites de-magnetize the crust and it may be possible to date the chronology of the weakening molten core of mars from billions of years ago, to know how long ago the molten core was magnetic, and how fast it ceased being as fluid as it was. Yt is not possible to determine detailed vertical variations of the magnetization of the Martian crust on the basis of magnetic data analysis alone, it is possible to estimate the thickness of the magnetic part of the Martian crust using other independent observations. The magnetic layer is bounded at the bottom by the depth to Curie isotherm of its major magnetic minerals, and at the top by the depth to the base of a near surface zone that has been demagnetized by impact-induced shock waves. Some studies have demonstrated that the secondary magnetization acquired by the lower crust in the absence of the core dynamo has little contribution to the observed magnetic anomalies. The measurements of the magnetic crust are the only witness of the inner core's activity, and of it's spherical geometry: 

No - the decreasing energy in the CMB is already well modeled in the Friedmann equations. The term in the density parameter that is proportional to $a^{-4}$ is the contribution of radiation energy density to the evolution of the universe, the term proportional to $a^{-3}$ is matter density (mostly dark, but includes ordinary matter), $a^{-2}$ is the contribution of the curvature of space-time itself, and the term without any factors of $a$ is the contribution of dark energy. The size of the radiation density, today, is already a small fraction of the matter density (about 0.03% of the matter density, 0.01% of the density of the universe overall - ordinary matter is about 5% overall). The last time the energy density in the radiation fields was the same size as what's in the matter fields was around $z=3,300$. I also disagree with @J.Chomel's answer - the energy stored in the radiation field is decreasing. Then energy density in the radiation field scales like $a^{-4}$, and the volume scales like $a^3$. Since the total energy is the energy density times the volume, the total energy scales like $a^{-1}$, just as you would expect with the number of photons being fixed, but the energy in each photon scaling as $a^{-1}$ as the wavelength increases. 

Pan-STARRS currently consists of two 1.8 m Ritchey–Chrétien telescopes located at Haleakala in Hawaii. Exoplanets were rare when they were detected with apparatus similar to PANSTARRS, and then they launched Kepler to find more, and they have as of 1 December 2017, detected 3,710 planets in 2,780 systems. The same will happen in the future with comets, especially for interstellar comets, which is like having material from many light years away arrive in the solar system. The difficult task is approaching a 25KM/s comet at landing speeds. Most of ESA's GAIA photography work is thrown away! When it could be used to detect comets! It discards most of it's CMOS data, including all the 1000ds of comets that it would have detected had it been designed differently. The bottleneck in GAIA is how much data it can transmit back to our planet. Here is a suggestion of a future mission which can possibly multiply our knowledge of comets and asteroids by 2 orders of magnitude, the suggested mission is called FAiRLiGHT based on our experience of processing large amounts of CMOS data for vectors. 100W of solar panels can supply a 10TB SSD type HDD in space. 10TB of SSD can keep a month's worth of space telescope observations on disk. 50W is also used for a serial CPU processor. The CMOS data is kept in a virtual 3D or Voxel array inside the 10TB memory, with simple memory indexing that represents Pixel X, Y , Time values. (it's cubic memory, representing space as a 2D plane with a time variant) That's mathematical simplicity. You keep super simple indexing for all the photos in the momory, so that the processor requires nearly zero instructions in order to read, write and recall memory very fast. A 50W processor can process about 10 million CMOS pixels every second searching for vector data on board the FAiRLiGHT space telescopes. To be precise it can process about 100 MFLOPs, floating point operations per second. It's a comet/asteroid reconnaissance mission that is essentially keeping all the GAIA photos in large memory banks and running a processor on 10 million pixels per second searching for shooting-star type signals across multiple photos. The clearest vectors/comets are then sent back to earth, which results in a thousand fold compression of the original CMOS files. They will be able to use Bi-focal and Tri-focal versions of GAIA telescope, which then triangulate exact positions for comets. The vogue has recently been about algorythms to study peaks in stallar brightness which describe exoplanets, and if material from other stars can be detected more efficiently like exoplanets were, for example by using satellite based bifocal vector processing engines, vector processing will perhaps become a well studied field and comets and interstellar meteorites will be seen frequently and with precision. 

My understanding of Type Ia supernovae is that they are expected, in most cases, to destroy the white dwarf(s) that went in to them, leaving behind no high density remnants (i.e. no white dwarf, neutron star, or black hole). Black hole/black hole collisions are expected to leave behind a black hole, of course, with less mass than the sum of the masses of the black holes that went in to the collision. Do we expect a kilonova to leave behind nothing but gas and radiation or some kind of stellar remnant? If it leaves behind a remnant, what class to we expect it to be (white dwarf, neutron star, or black hole) and what mass? The addition of mass seems redundant, but the dividing lines between the masses of these objects are based on upper limits on the mass of the less dense class (Chandrasekhar limit for white dwarfs, Tolman–Oppenheimer–Volkoff limit for neutron stars), and don't actually apply as lower limits for the mass of the high density class. For example, neutron stars are sometimes referred to as giant nuclei, which would put the lower limit of their mass at $1$ or $2$ atomic mass units, depending on whether the presence of a neutron and stability to radioactive decay are requirements. Yes, those times people are being poetic due to the difference in stabilization mechanisms (gravity vs nuclear forces), but the point remains that it may be possible for a neutron star to be theoretically stable at less than $1.4M_\odot$. The only lower limit to black hole mass I know of would be the lifetime limit from black hole evaporation. Put another way, do we think the density at any point in either of the inspiraling neutron stars gets high enough to form an event horizon? If that happens, this seems like a plausible way for producing black holes that are smaller than those produce by core collapse supernovae, kind of like how critical mass for atom bombs can be achieved by either bringing together enough fissile material or compressing the available material enough (e.g. the fat man detonation mechanism vs the little boy one). 

At a sunspot the movement of colder plasma flows downwards, and there is a hot spot trapped under the sunpot where the thermal energy escapes radially away from the sunspot. 

The stars do orbit the center every 250 Million years according to a galactic year, That's 20-25 times since the formation of the solar system. The complex shape of the galaxy, with a disk, bulge, bar and arms adds many non concentric gravity poles to the equation. Easiest to imagine is a wave of movement relative to the disk. It would be nice to see a gravity map of a galaxy in 3D to see that it is not a 1 body at the center kind of gravity map. Some (many? n%) stars can result from the pressure front of supernovae that happen inside star nurseries, which gives them erratic vectors compared to the disk and the arms. Perhaps if there wasn't the constant energy of supernovae affecting the birth and movement of objects in the galaxy, there would be rings as seen for Saturn. There's a parallel with mapping rocks in the solar system. If you measure the denser zones to try to figure out where every rock was moving, you'd find your predictions soon become chaos of spirals and local deflection, rather than simple trajectories of a predictable fashion. 

You can probably extract the data from Stellarium. Also, does it need to be all sky, or not? What Wavelength? If you want an all sky visible catalogue, the United States Navy produced the NOMAD catalog by combining 2MASS, UCAC, and the USNO-B catalog. You could also use the APASS all sky catalog, meant for helping astronomers with photometric calibration. If you don't care about whether the wavelength is in the visible, CalTech's IRSA has a bunch of infrared source catalogs (the smallest/shallowest one being the IRAS catalog). The deepest all sky infrared catalogs are the WISE sets. Other good databases to dig in to include the Hipparcos set of parallaxes, or some part of the Gaia catalog. 

Have we detected continuum optical emission from any rotating neutron stars that do not have an accretion disk dominating the light? I ask because I know we have observed Doppler broadening of spectral lines from the rotation of even ordinary stars rotating at ordinary rates, but it seems to me that only the rapid rotation of a neutron star would be fast enough to make the Doppler broadening of the thermal continuum peak to be observable. 

If we graph the variability of a small bright star which is lined up with the asteroid belt, is it less constant than stars that are far away from the asteroid belt? To what degree does variability change relative to distance away from the most dense areas of the solar system? I added the arrows to illustrate the question. 

A 2km comet at 40AU's can dim a sun sized star at 100 light years by 3%, and a 10km comet can cause an eclipse (is that right?) So would 2,3,10 km objects be easy to measure via star dimming if we had enough telescopic data from space? Can we measure lines of stars that are eclipsed/dimmed in sequence by transiting comets? Given 1000 frames of a small patch of space, where all the images are recorded into a 3D image stack (a 3D array, a voxel) X,Y and Time as dimensions, then an Intel processor can compare 20 million voxels per second of that collection of images to search for vector information of aligned patterns of star dimming, i.e. searching for vectors of comets. If that process is repeated for many patches of space, perhaps we can detect many comets, and I don't know the real maths involved, if a satellite of that design would realistically only find 1 comet, or 5000? It's for a theoretical comet apprehension system which uses 2/3 identical satellites each comprised 100Tb of SSD and a 20Mflops processors to search for vector information in the images. Some kind of fairy idea going around in my head, because of Oumuamua and because I work on voxels and I know that image collections from space can be stacked into 3D arrays and be searched for vector patterns at 2million voxels per second on a 200W PC, an idea that I am confusedly astrally-projecting as a basis to find comets. edit- A quantitative answer would be cool, say if the voids between stars on the milky way arm are 8 times the width of the comet, the comet would on average hit 2 stars every 8x8 frames. 

One of the tools being developed for the study of exoplanets is called transmission spectroscopy - basically looking at how passing through the exoplanet's atmosphere changes the spectrum of the star. The problem, though, is that spectroscopy is hard, and requires a big telescope, probably space-based. This makes the process slow. It seems that it would be very useful if we would triage the data using broad-band photometry to more quickly limit the presence of compounds like water, $\mathrm{CO}_2$, and $\mathrm{O}_3$. Looking at a plot of Earth's atmospheric transmission, with a breakdown by chemical compound, it seems like looking at the color of the transit depths in and between the windows in the wavelength range from 1 to 20 microns, might provide a plausible alternative to quickly get information on the atmospheres of planets and brown dwarf stars. For the exoplanets, specifically, the wavelengths blocked by an atmosphere make the planet, effectively, bigger at some wavelengths than at others, depending on the opacity at that wavelength, the thickness (density) of the atmosphere, and vertical extent of it. The question is: is this practical? Specifically, with a 1 to 2 meter class space telescope that simultaneously images in 2 to 8 filters? 

M.F.Loutre and A.Berger, 2000, Future Climate Changes: Are we entering an exceptionally long interglacial?, Climatic Change 46, 61-90 The effects of galactic cosmic rays on the atmosphere (via cloud nucleation) and those due to shifts in the solar spectrum towards the ultraviolet (UV) range, at times of high solar activity, are largely unknown. The latter may produce changes in tropospheric circulation via changes in static stability resulting from the interaction of the increased UV radiation with stratospheric ozone. More research to investigate the effects of solar behaviour on climate is needed before the magnitude of solar effects on climate can be stated with certainty. 

Impact craters are not always circular. They vary based on the angle of incidence. check Wetumpka crater and this research $URL$ 

It will be cold and dark in 10^(10^100) years. What else can be said of the state of things in the far distant future? Will everything have fallen into supermassive black holes and then radiated back into space so there is no matter left? Will the universe revert to an exotic state as it was in primordial times, or will it slow and stay unchanged indefinitely? What kind of structure will it have?