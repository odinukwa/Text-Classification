It's single availability zone no backups not in a security group that's reachable from the outside world 

Is it possible to buy an intermediate certificate to use it to sign subdomain certificates? It has to be recognised by browsers and I can't use a wildcard certificate. The search turned up nothing so far. Is anyone issuing such certificates? 

I am trying to use AWS autoscaling lifecycle hooks in a template that encapsulates the following things: 

I don't specify any access/secret keys in the command line. My instance role was manually created (by me) and definitely does NOT grant any permissions on resources. 

I want to forward TCP connections on a certain port of the machine A to another port on the machine B (which is actually the same that originated the connection to machine A) and simulate random or deterministic packet drops. How can I do it with iptables? 

with associated scale up/down policies, launch configuration, IAM role, etc. 2 of for EC2 launching/terminating events. (in a simplified example) where lifecycle notifications get posted. role for the autoscaling group to post notifications to the SQS queue. 

This configuration looks correct, provided your Puppet agent is running on a host in the 192.168.1.x subnet. The deny is coming from , since your is allowing all the resources. It would seem that your puppet agent is not connecting form an IP in this range -- try adding to the [files] section temporarily to confirm this. If your puppet agent host is multihomed, check which interface it is sending from. Also check the permissions and ownership on the fileserver.conf is correct. 

The command only retrieves data from the raw RRA; if you do not have an RRA of the requested granularity, then the nearest available will be used, but no additional calculations will be performed. Add a new RRA to your RRD file when you create your RRD file, with a 1cdp==5pdp rule. EG 

Note this is for a single user; you may want to have different options if you use a master user/password, or if you require SSL for the connections. Then call it with something like: 

to the Apache configuration (default for this setting is 131072). I have not been able to verify this fixes the issue but there is anecdotal evidence to indicate that it does. 

When an ASG is launched, the queue ends up with two test notifications from the creation of lifecycle hooks, but no notifications for instance launch. And here is the race condition. object references (and hence depends on it). This dictates the order in which CloudFormation creates resources (the group is created first). The problem is that the group starts launching instances before the hook creation is complete (instance launch is not a part of the template, so it starts executing in parallel). By the time the hook is created, there are no more events to post as the instances were already created. Is there any way to work around it and catch launch events at stack launch time? 

I want to aggregate CoreOS logs to Papertrail service, which basically provides a syslog endpoint for aggregate logging. Common advice for this setup seems to be starting a service that does something like this: 

How come I can still read any CF metadata? I noticed that in the client code, the script goes to use instance credentials ( is True) 

As I understand it, an instance needs to be granted access to resources in order to do anything with CloudFormation. But when I run this on a Beanstalk web server instance: 

It is not Dovecot recreating the Trash folder; it is your mail client (Outlook in this case). Some mail clients, when first setting up the definition, will probe the mail server to identify folders with the special use flags such as \Junk, and will then use these flagged folders for the special purpose. Others, such as Outlook, will just go ahead and do things their own way, and will create a folder with the name that they want to use regardless. What you can do, is to use the plugin to make both names valid. See here for an example, which makes both "Sent" and "Sent Items" equivalent: $URL$ 

Check your definition. You almost certainly do not have the autosubscribe option set, or else you added it later after Thunderbird had already probed the server. Make sure you have the option set on the relevant folders: 

If you place another gateway device between your Ironport and the Internet, then your only option is to disable SenderBase and any other IP-based authorisation in your Incoming Policy definitions in the HAT. You cannot tell the Ironport to obtain the previous-hop IP address from any other method than the incoing TCP connection itself (for obvious reasons - otherwise it would be far too easy to forge). One option would be to reverse the order of the devices -- IE, put the Ironport between the Internet and the Proofpoint box, and set the Ironport to have a fixed SMTP route to send all incoming email via the ProofPoint. Otherwise, you lose out on the Ironport's Senderbase rules, which are (IMHO) one of the primary benefits of the Ironport. 

Full description: I have two SSD drives set up with GPT + LDM (dynamic disk) in a state that seems to imply a corrupted LDM database. The problem is, everything works fine except for some weird behaviour when using or the . The GPT structure seems to be intact: 

The main disadvantage - it's client-side, so a special prompt is still your best bet if you're accessing servers from different locations. 

Purge and install should work, so you may have some more severe underlying problem. You may also try: 

Yes, or more precisely said - at least reload the config. And I see Zypher was first here while I was writing - you should see some individual config in 

seems to indicate quite verbosely that the current state of the LDM metadata does not conform to some Microsoft standard anymore. Is there any way to investigate this further and potentially fix this issue without recreating the whole disk partitioning scheme from scratch? It seems there's not much one can use to diagnose LDM issues. I will try to get a database dump attached in due course. I'm especially looking for some hints as to what to look for when analyzing the LDM database. 

The list is most likely configured to not allow self-unsubscription. This is why you cannot unsubscribe by email. The list of users is held in the database, and so - if you are unable to use the web interface to manage the subscribers - you would need to delete the appropriate record from the subscriber_table. If this list uses an external datasource for the users, then you may need to instead add the user to the exclusion_table as otherwise they will be re-added on the next synchronisation. Having said all of that, you are using a very old (4.x) version of Sympa, and it may predate some of this advice which is based from knowledge of 5.x and 6.x. 

Check your nagios.log and see if it is showing that NSCA has submitted commands, but for the wrong hostname/servicename Make sure NSCA has write permission to the Nagios command pipe file Make sure nagios.cfg specifies that external commands are processed Make sure Nagios has a reasonable command processing interval in nagios.cfg Check that NSCA is configured to accept commands from the remote sender - if you use Xinetd for NSCA then this will be in the file, and if you run NSCA as a daemon you should look in your file. Check your local host firewall to make sure it is not dropping the inbound NSCA connections. 

I want to get an automated process for AMI creation going, and one piece remaining is automatically cleaning up the instance after image creation. The instance is booted with a user-data script that does the necessary setup, then kicks off image creation from self using AWS CLI. Then it shuts down. I could go with option and wait there until the image is ready, then terminate, but the docs state that "file system integrity on the created image can't be guaranteed", so I want to avoid using it. What's the best way to kill the instance from itself after image creation is completed? 

Basically what it ways on the tin, how can I create individual per-instance alarms inside an auto-scaling group created with a CloudFormation template? I can reference the ASG itself in an alarm and create ASG-level alarms, but cannot seem to specify dimensions to be "any EC2 instance belonging to this ASG". Is it possible or is my only option user-data script? 

If a directory "foo" is owned by user A and contains a directory "bar", which is owned by root, user A can simply remove it with , which is logical, because "foo" is writable by user A. But if the directory "bar" contains another root-owned file, the directory can't be removed, because files in it must be removed first, so it becomes empty. But "bar" itself is not writable, so it's not possible to remove files in it. Is there a way around it? Or, convince me otherwise why it's necessary. 

You should migrate your mail using the utility from Dovecot. This will preserve the UIDs and even POP3 UIDLs if necessary. Run using the option, to 'reverse backup' from the remote IMAP server to the local Dovecot server. You need to have a special configuration file created, something like this: 

When you change any custom ruby code, such as a custom function, you must restart the Puppetmaster. If you are running puppet under passenger, this means restarting Apache. Otherwise, you'll get the old version. In addition, there is a rather nasty bug that kicks in if you are hosting multiple environments in your puppetmaster with an identically named function in the other environment. In this case, you have no way to be sure which environment's function is used when you call the function name! It uses the same function namespace across all environments... (this is confirmed to happen in Puppet 2.7.22, not sure about 3.x) 

RRDTool (as of version 1.4) does not allow you to have different scales on the Y-axis above and below. What it does allow you to do is to create a secondary Y-axis with a scale shift argument. Items are still plotted according to the primary Y axis, though, so you need to perform any necessary calculations yourself. The necessary parameter is --right-axis. $URL$ 

This way looping can be avoided. One problem remains though - the is still accessible externally. If that's not an option, I think your best bet is following David's advice and moving the configuration one level up. Some alternatives that came to my mind: 

Well, technically you're not redirecting anything here. Calling just makes save the whole typescript into which in practice means discarding the contents. See for detailed info and util-linux-ng package for implementation (). This has nothing to do with actually. Why this works is invoking has a side effect of creating a pseudo-terminal for you at . This way you don't have to do it yourself, and screen won't have permission issues - if you from user A to user B, by directly invoking you try to grab possession of user A's pseudo-terminal. This won't succeed unless you're root. That's why you see the error message. 

I think it's the other way round. It shouldn't be slower. The question is, "will running it shared make it faster"? As to that point - theoretically, yes. But the practical difference may be indiscernible. The point is - if there's one script only for all users (the idea of shared libraries) it's not only easier to maintain as pointed out earlier here, but the built-in caching mechanisms should be more efficient (at least from my understanding of the subject). One file to read, easy way to cache in memory. Multiple identical files (not sym-/hardlinked, different inodes) - this is more problematic. Or am I underestimating Linux capabilities here? Please correct me if I'm wrong - my knowledge of the internals is not that profound, but I think this would result in independent seeks/buffering. Another point - using opcode caches. I'm not sure about how it looks in different implementations, but in most cases using multiple identical scripts would result in unnecessary cache entries. However, there's one potential problem with shared scripts - when caching gets too aggressive (like caching external configuration files between different users). I had recently encountered this kind of behaviour with XCache and hardlinks. With proper configuration and non-buggy implementations it's not a problem though. So, my conclusion: If it's feasible, I would use shared installations both for convenience and performance. However, in some situations the difference in performance can be negligible, so it's probably "what fits your situation better". 

I am trying to avoid having to implement a UDP load balancing proxy, or write a new custom plugin to replace ec_dns_lookup. 

The cause of this turns out to be that when files are deleted from the ext4 filesystem living in the zram0 device, the memory is not freed back to the system. This means that, although the space is available for the filesystem to use (the output of ) it is nevertheless still allocated memory ( the stats in ). As a result, the memory usage heads up to 100% of allocation, though the filesystem still finds itself half-full due to deletions. This does mean that you can still fill the filesystem and new files will not use as much new memory space; however it does affect the compression ratio negatively. The solution is to mount the filesystem with the and options. The releases freed filespace back to the memory device and as a result the usage on the two matches again. 

Then, completely remove the definition from Thunderbird, and recreate it. Thunderbird will re-probe the IMAP, and will autosubscribe to the folders.