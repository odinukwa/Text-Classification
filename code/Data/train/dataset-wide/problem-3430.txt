I'm not sure I really buy that for safety reasons as above - it's processing user-supplied data. Another idea I got off-list was to not run checksetup.pl as root so it couldn't chown/chgrp permissions away. I'll probably go with that or stick to my old workflow (checksetup as root then run my fix permissions script). Follow-up: MKA replied to the second idea too: 

This couls be bug 3242 - clients make an OPTIONS request at the root of the repository which gets a 403. Is your access denied to /svnroot/ or /svnroot/project1/? (Check the apache logs if it's not in the message.) Solutions? 

which is the value in seconds. Google says you'll need to restart the w32time service (Windows Time) to pick up the changes. 

I want to get an automated process for AMI creation going, and one piece remaining is automatically cleaning up the instance after image creation. The instance is booted with a user-data script that does the necessary setup, then kicks off image creation from self using AWS CLI. Then it shuts down. I could go with option and wait there until the image is ready, then terminate, but the docs state that "file system integrity on the created image can't be guaranteed", so I want to avoid using it. What's the best way to kill the instance from itself after image creation is completed? 

How come I can still read any CF metadata? I noticed that in the client code, the script goes to use instance credentials ( is True) 

I have 1 server with windows 2003 which is our office's domain controller, and other server with windows server 2008 in another office. The domain names are different, and what i need to do, is interconnect both active directories, to authenticate (for example) with domain1 credentials on domain2. I have a sharepoint 2010 in server2 (outside office) so, it will work if i can login in sharepoint using credentials from Active directory1 Thanks in advance. 

Netflow only gives me data of traffic per protocol. I've used "ip accounting" command to solve this. conf t ip accounting interface output-packets exit then show ip accounting 

when you configure the account in outlook, you can specify the incoming server of that account. You need to point your exchange server configuration to rackspace's servers, and not to your domain. In the link below, in the rackspace support, you can find a guide to how to configure it in your outlook. $URL$ Regards 

I cannot use UserData, because anyone can read it. I cannot use private S3 buckets for the same reason (metadata and hence credentials can be accessed by anyone on the box). I'd strongly prefer not to bake my own AMI, as it's quite a hassle. 

Is it possible to buy an intermediate certificate to use it to sign subdomain certificates? It has to be recognised by browsers and I can't use a wildcard certificate. The search turned up nothing so far. Is anyone issuing such certificates? 

But it glosses over the specifics of how these policies are combined and when the "fall through" happens to the next policy in the list, i.e. under what conditions each policy fails and moves on to the next policy in the list. For example, I have a policy list in my group and yet after scaling up and then down, the scaling group proceeded to terminate by newest (and healthy) instance (newer by a large margin), and I can't figure out why. Additionally, according to the same doc, default policy is actually itself a combination of policies, and includes and as two of its steps. If I have a list that includes , does it evaluate and twice? Lastly, does the termination consider load balancer? For example, if my new instance failed to initialise properly and is not in-service with the load balancer, and is in effect, will scale-down action kill the unhealthy instance first even though it's newer? 

I have a Cisco 2901 with 2 internet WAN connections, one it have a static public ip, and the other one have a ftth dial connection. I have already both configured but what i need is the following: Currently: Internet_connection_1 (static ip) as a default route ip route 0.0.0.0 0.0.0.0 public_ip What i need: Use the ftth internet connection (connected in Gi0/0) to ONLY access some services, ex: clud_server_ip How i redirect the traffic, so any who want to access Cloud_server_ip, use the ftth connection and not the adsl? Note: i have already configured the dialer0 and getting the public ip from my isp and bonded the dialer with Gi0/0. 

This is the configuration that works, i hope that it can help someone else. Here is a redirection using protocols: ip access-list extended pbr_acl permit tcp 192.168.0.0 0.0.255.255 any eq pop3 permit tcp 192.168.0.0 0.0.255.255 any eq smtp permit tcp 192.168.0.0 0.0.255.255 any eq 465 permit tcp 192.168.0.0 0.0.255.255 any eq 993 permit tcp 192.168.0.0 0.0.255.255 any eq 995 permit tcp 192.168.0.0 0.0.255.255 any eq 587 permit tcp 192.168.0.0 0.0.255.255 any eq 143 route-map pbr_navigation_dial permit 10 match ip address pbr_acl set interface Dialer0 And here the configuration using ips: ip nat inside source list acl_dial interface Dialer0 overload ip access-list extended acl_dial permit ip 192.168.0.0 0.0.255.255 host x.x.x.1 permit ip 192.168.0.0 0.0.255.255 host x.x.x.2 ip route x.x.x.1 255.255.255.255 Dialer0 ip route x.x.x.2 255.255.255.255 Dialer0 

What would be the best way to pass sensitive data to EC2 instance (on boot or otherwise) that only root can access? 

Basically what it ways on the tin, how can I create individual per-instance alarms inside an auto-scaling group created with a CloudFormation template? I can reference the ASG itself in an alarm and create ASG-level alarms, but cannot seem to specify dimensions to be "any EC2 instance belonging to this ASG". Is it possible or is my only option user-data script? 

I want to aggregate CoreOS logs to Papertrail service, which basically provides a syslog endpoint for aggregate logging. Common advice for this setup seems to be starting a service that does something like this: