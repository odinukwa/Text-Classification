The initial purpose of this data is "fluffy", I'm generating a graph-map out of it to show our workflows so a limited depth is fine in the first instance at least. But better answers accepted if anyone has any 

At our organisation we have several non-Production environments where Developers run free and wild with TSQL code and databases. The DBA team do not normally monitor or maintain them. Recently several developers using one server have had to write procedures that throw around mountains of code such that their log files are growing to 10-20GB (on DBs approx 15-40GB) until they run out of space on the drive volume we have provided for the log files. The databases are all in Simple recovery mode and backups are (almost) never taken. As a bandaid I've created a SQL Agent Job that can be run by anyone to shrink all user log files on the server. What are some valid Log File Management strategies that might be used? For example it is my understanding that as the users generally do a blitz of intensive work that checkpoints are probably being throttled back and that thus issuing manual checkpoints would see no advantage. Is that the case or should we in fact investigate adding manual checkpoints to their code? And just to be clear it is log file space on disk we are interested in not space within the log file. 

Then it will work, the 0 will be ignored and an identity value generated. But as the question scenario states the insert statement is set in stone by the 3rd party executable then it is unworkable. 

In my current work position I've been given a desktop with SQL2014 tools installed including SQL Profiler. For some time we are still however supporting production SQL2000 machines and when using Profiler against these the following error is received: 

That way I can have the same SP installed on each server, any bug/feature fixes doesn't need me to manually type up 10 different SPs to install. In the real world I'm also limited by SQL2000 however I'd be interested to hear in ideas using SQL2000 and/or SQL2008R2. The servers are a mixture of both. As I understand it Synonyms wouldn't help as on the instance with 3 DBs I'd still end up with each DB having it's own copy of the SP with it's own hard coded definition of which named synonym to use. I also don't feel dynamic SQL statements would be a good fit. There's more to it than the example snippet above and I use table variables to marshal all the work to be done- so that would be out of scope for all the other statements I need to work with. 

Either approach above will require you to save the coefficient in the rentals table for historical reporting/auditing purposes and also maintain a note on what data was used for computation at that time 

The way to think about it is that there is one application with a single database which is being accessed through multiple channels, whether from the same server or different servers 

I would recommend that you have a Pages table that contains both Static, Blog and any new pages you may have. The Pages table would have the columns page_url, page_content and pagetype (a tiny integer) which will help you differentiate between different pages. Later u may end up with other page types which have content hence leaving page content as a field in the pages table and may also allow you to have custom PHP or markup content in a blog page for customization (thinking aloud) You can then have a BlogPages table which has the information for blog pages linking to the categories and posts. 

I follow the following rules for primary keys: a) Should not have any business meaning - they should be totally independent of the application you are developing, therefore I go for numeric auto generated integers. However if you need additional columns to be unique then create unique indexes to support that b) Should perform in joins - joining to varchars vs integers is about 2x to 3x slower as the length of the primary key grows, so you want to have your keys as integers. Since all computer systems are binary, I suspect its coz the string is changed to binary then compared with the others which is very slow c) Use the smallest data type possible - if you expect your table to have very few columns say 52 US states, then use the smallest type possible maybe a CHAR(2) for the 2 digit code, but I would still go for a tinyint (128) for the column vs a big int which can go up to 2billion Also you will have a challenge with cascading your changes from the primary keys to the other tables if for example the project name changes (which is not uncommon) Go for sequential auto incrementing integers for your primary keys and gain the inbuilt efficiencies that database systems provide with support for changes in the future 

If we have code in an SP Database1 then queries Database2 (on the same server) we want the same code to work on the databases Database1Dev and Database2Dev. But this currently means editing the full SP each time we push to Live. We want a single of line of code such as 

I'm dealing with a 3rd party app that I can't change or get support on. I've altered an underlying table in several ways to enhance it's functionality 1- I renamed the table PRE_CASE to PRE_TCASE 2- I created a view named PRE_CASE which is a select on to PRE_TCASE with a UNION ALL on to a linked server DB and table named OTHERSYSTEM_CASE This works to the extent the app is now fooled into displaying data from PRE_TCASE and OTHERSYSTEM_CASE to the end user when it believes it's looking at the table PRE_CASE. It fails however on inserting data. To try and fix this I created a trigger (I only require to insert to PRE_TCASE, never OTHERSYSTEM_CASE) 

My question is would this ever create two identical values for the Date? Does this answer change if parallelism is in use? (Assume value never specified, always comes from GetDate()) I believe I'm correct in assuming it wouldn't matter due to the behind-the-scenes uniqueifier being added, right? But I'm interested anyway. I'm asking from a SQL2008R2 perspective but would be interested if the answer differs for any version of SQL Server from 7.0 up. 

I would like to know if there are any dangers or relevant precautions before changing the IP address of a SQL Server 2008 R2 box. We have built a virtual PC with Windows Server 2008 R2 and SQL Server 2008 R2. The purpose of this machine is run a job that restores a backup file (copied by a different server), manipulates it, backs it up again and then copies it out to other servers. We've run it a few times in our test/dev DMZ and are now considering deployment. The simplest option would be to re-IP it. The server name would remain intact. My problem is a colleague has suggested this is unsafe (to re-ip a SQL Server). My question is a) Is this true for the OS and SQL I'm using? (Win 2008 R2 & SQL 2008 R2?) b) Is it true for any other combinations of OS and/or SQL? c) Is there anything special we should do in preparation? My research thus far indicates it will be fine but I trust the folks of DBA StackExchange more than those social MSDN people. $URL$ $URL$ 

Schedule a) and b) as cron jobs, but remember to removed files which have been loaded so that you do not get duplicated data These are just guidelines and will be made more complicated depending on the formats of the data and of the database into which they are loaded 

I would recommend modeling it as a sale with an adjustment for the trade-in, assuming that as a car dealer you accept trade-ins without a related sale of a car. Therefore within your system you re-use the purchase features, and can later report on metrics like how many sales are linked to trade-ins, what values of sales are linked to trade-ins, whether trade-ins boost sales, etc 

Remember these are just starting points you can fine tune specifics as you move along, but before you fine tune, measure tune then measure again 

Why not add two indexes: a) A two column index for (state, city) - handles queries for state only, then state and city b) An index on city - queries on cities only To tune for performance further you may be able to add lookup tables for state and city then use numeric keys to speed up query performance. 

In addition to the above changes to mysqldump, I would like to suggest that you try Percona Xtrabackup www.mysqlperformanceblog.com/2012/02/27/announcing-percona-xtrabackup-1-9-1/ which seems to offer more backup options, including parallel streaming which brings up to par with other mainstream databases. 

The order of columns in a primary key does not affect the insert performance, since the combination of the values in the two columns of the primary key are meant to be unique. However the order of index impacts the performance of SELECT queries. In the new order you will have a greater and increasing range of values (dates) across which the index is clustered (dates) as compared to clustering around 100 stores. If most of your queries are focused on a single store with the new ordering your queries will be slower. 

Yes this is possible, although you need to watch out for the following: a) Export using extended inserts so that the extended SQL syntax is used which will reduce the size of your SQL files (If you are exporting using PHPMyAdmin select both complete and extended insert options, as it creates complete inserts with extended insert syntax for large tables) b) Try using the GZip format which will compress the SQL files and make them smaller (depending on the hosting provider setup this may be an issue so you need to test it out with a small table) c) Export the tables in such an order that tables with foreign keys are imported last