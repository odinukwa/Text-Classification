I just noticed that you are only providing information on the differential and transaction log backup steps of the backup procedure. What is with the FULL backup? Ola has implemented a fail-safe mechanism which will delete transaction log backups only if a FULL and/or DIFF backup is available. 

You can not force the switching unless you shut down your database. If Oracle thinks it still needs the old UNDO TS then so be it. Your system will already be writing to the new UNDO TS so there is no need to force anything unless you have special requirements. 

I haven't found any other reasoning behind such a configuration yet. There are absolute no availability groups or transactional replication going on and transaction log shipping has not been implemented. The servers have been configured to have the default instance and multiple additional instances as explained below. SQL Server Environment The SQL Servers are configured to contain multiple instances. Server to instance relationship Each SQL Server can have 1 to n instances 

This is possibly due to the fact that the value that you are implementing is faulty. This can have multiple reasons, none of which seem to documented outside of Microsoft. Official Documentation The only useful reference to comes from the official Microsoft Documentation CREATE STATISTICS (Transact-SQL) in which Microsoft states: 

Ensure that the option is set and the IP address is correct and that the is set to you reserved port. Do the same for the other IP address. Locate the IP section (e.g. IP4 in this screen shot) and set accordingly: 

You have a misconception in your thoughts. A Windows login which is used to create a SQL Server login does not require a password. Solution When you create a SQL Login using a Windows account you have to use a different syntax. 

In some cases ...when you receive a database backup from a client, you might not have a corresponding SQL Server Login to link to the Database User. In that case you can create a SQL Server Login without assigning permissions to the restored database and then link the newly create SQL Server Login with the Database User with the above mentioned statements. 

It depends on the teacher's definition of security risk. IT Risk Management according to an article on Wikipedia has the following definition: 

That shows the source code of the new procedure . But where does the system store the definition of the view? This can be retrieved by issuing the following command: 

Alternate Solution (process node empty) After further investigation of some blocked_process_reports the following alternate explanation can be made. The Extended Events are capturing blocked_process_reports which are unrelated to any other processes at the time. Ergo: They must be blocked for a different reason I would suggest you capture a time frame of wait types from the sys.dm_os_wait_stats view on your SQL Server and correlate the numbers with the blocked_process_reports happening during your measurements. Paul Randall has a good script: Send me your wait stats and get my advice and 30 days of free Pluralsight in return The scripts captures the current counters, waits for 23hours (can be modified), recaptures the current counters again and compares them to give you the top 95% of wait types. You could try this out for say 1 hour and have the XEL file handy. You might find a wait type (e.g. LCK_M_SH, …) that is telling you that your storage is slow in writing. Or that you have some other overhead (e.g. CX_PACKET_WAITS, ….). Something is slowing down your Updates. You can then see if the sys.dm_os_wait_stats relate to the blocked_process_reports with the empty nodes. There are cases when a blocked SPID is being blocked by the same SPID: The blocked column in the sysprocesses table is populated for latch waits after you install SQL Server 2000 SP4 

See: dbfiddle for the example In your case, you must be using an older version of MySQL which will store the maximum allowed value which is . Fix If you create a table with or as pointed out in the accepted answer, then it should work. Table Definition 

Reference: CREATE LOGIN (Transact-SQL) If the SQL Server is a member of a Windows Domain, then it will retrieve the password policy from Active Directory. Otherwise the defaults are: 

In your example the should be the IP address of the application server or if the server has DNS resolution it can be the hostname ( record or ) of the application server. E.g. Given the following values: 

Individual File and Directory You might want to consider looking at the permissions of that one file. 

Back to Basics: Capturing Baselines on Production SQL Servers Once you know your baseline, you know what values for PLE are good for your DW SQL Server. 

Part I of II (had to split my answer) There are one or two things to think about here. In my opinion you may have taken a shortcut in the thought process. I'll explain while I'm making this assumption... You might want to have a quick look at my accepted answer here which was posted in reference to the question Need suggestion on Back up Strategy [closed] to give you some ideas about RTO and RPO. Why? Because setting might be a bad idea... First to answer your question. Is it a bug in Ola's script? - No. Ola designed his script to check for the last (Full Backup) and then to delete the transaction logs backups () which have occurred before that backup and only if the is lower than when the FULL Backup occurred. 

SQL Server Management Studio (SSMS) Change the setting in the security section of the properties window to SQL Server and Windows Authentication mode. 

Issue 1 This error can occur approximately after 2048 connections have been made via the listener when running on a non-English Windows installation. Fix for Issue 1 Create a Windows User Group named Administrators on the computer where the listener.exe resides. This can fix the issue of the listener dying. Reference: I'll post the link for the first issue as soon as I find it again 

I would like to point out that the join condition in the was still comparing with as can be seen in the EXPLAIN PLAN you provided: 

You can get at that information in the tempdb by following the following solution: reading temporary tables that aren't yours You can get some general information regarding tempdb by following the following post: DMV query to get at data stored in tempdb tables,... 

This will relink the (Native) SQL Server Login () on your target instance, with the Database User () of the restored database. Alternative Seeing as is deprecated, you could achieve the same with the statement: 

This undermines my conclusion that Simpana is using the native PostgreSQL commands to backup the database/instance and its WAL Archive Log files in the directory . According to the documentation a nnnnnnnnnnnnnnnnnnnnnnn.mmmmmmmm.backup file is a pointer to the earliest WAL file required for a roll forward recovery to succeed. Any older WAL files in the Archive Log directory could be deleted and are no longer required. What amazes me, is that there is a WAL file in the Archive Log directory without a corresponding *.mmmmmmmm.backup pointer file. Questions 

What you aren't doing is bringing back features. When you upgrade to SQL Server 2016 you are removing/altering SQL Server features, procedures, functions, etc. These features will no longer be available even if you set the database compatibility level to a "down level SQL Server compatibility level". Reference: ALTER DATABASE (Transact-SQL) Compatibility Level (This link also contains a listing of compatibility level settings compared between version 130 and 120, and another for 120 and down level differences.) So while you are retaining a partial backwards compatibility you might break your application because some features are no longer available (see example above) Answer As you correctly pointed out in your comment, compatibility modes in SQL Server 2016 are available from 140 through 130 and 120 and further down to 110 and 100, but compatibility modes do not magically bring back features that have been removed from a SQL Server version. Service Packs can have an impact on the compatibility mode of the version you are upgrading. E.g if the SQL Server you are upgrading is 2016 RTM and you are upgrading to 2016 Service Pack 1, then the internal workings for compatibility level 130 can change. However these changes should not affect the down level compatibility modes. This is pointed out in the following article: 

This should enable you to pinpoint the issue. As for the error message you are seeing: If the files are in use, then somebody or some system is using them. Use the above script to see if the database files are being used by a different database. 

When the FILESTREAM feature is activated on Microsoft SQL Server 2012 then SQL Server will create a "hidden" share on the system. The share is defined as follows: 

Ensure the Windows Account of the SQL Server Agent Service has access to the network drive. Check that the Run As setting in the job step is set to the SQL Server Agent Service Account. 

Forcing a TCP/IP Connection In the SSMS connection window perform the following steps to force a TCP/IP connection: Basic SSMS Login window 

If Enterprise Manager has been installed then setting off the following command will show the status of the Enterprise Manager: 

Consider running the or command in front of your statement. This will produce an explanation of how the query optimiser queries the database to retrieve the data you are asking it for. Example: 

The DMVs reference some system base tables, which can be accessed (but shouldn't): Reference: System Base Tables The policy you are looking for is built in to the code of SQL Server and is set per default for each new account. When creating a SQL Server Login you can decide to turn off the defaults: 

Does still have access to ? Switch user to with and then navigate through the directory structure . Can you navigate past into ? Verify the file permissions beneath with the following comamnds: 

The column in the sys.index_columns table is the order in which the columns are stored in the index. There isn't a column for the table. The column just replicates the column of the object it references. There is a slight difference in the wording of the article sys.stats_columns (Transact-SQL) for the column : 

(This is the default setting for a database in SQL Server 2012) Test Scripts The following scripts were executed using the standard SQL Server SSMS client settings and the standard SQL Server settings. Client connections settings The client has been set to use the Transaction Isolation Level as per the Query Options in SSMS. Query 1 The following query was executed in a Query window with the SPID 57 

The information provided in the answer is a generic approach when trouble-shooting issues with broken installations. Possible Issues 

The important bit is the option. Assuming you are using a new database name and you changed the file location then the restore dialog in the Files section will have displayed two (2) columns. One named 'Original File Name' and an additional 'Restore As' column where you can modify the database file path and file names for the new database. Because you are restoring a new database, there is no need to specify the option, as the database does not yet exist. However, before you execute the script or finish the dialog by clicking on OK, ensure that the files you listed in the Restore As portion do not exist on the file system. If the files do exist run the following statement to find out which database is using the files: 

Instead of just running the code and modifying the delay period, open up a Query window and execute the first portion of the SQL statement up until the part. Then execute whatever you have to do. After the query has executed run the second part of Paul's script to output the differences found. (The part after the ) 

Given the nature of a transaction log file (TLog has circular VLF usage) you could have a big increase in the snapshots, because all the data in the TLog has been changed. I recommend reading the following articles: Inside the Storage Engine: More on the circular nature of the log Transaction Log VLFs - too many or too few Initial VLF sequence numbers and default log file size and Brent's own Blitz Result: Multiple Log Files on the Same Drive This should give you an idea how a TLog file works and why you might have a sudden increase in your TLog snapshots in Simpana. At our shop, for example, we don't do any de-duplication on the TLog files, because of their nature. In some cases we saw an increase (yes, increase) in size after de-duplication. Summary: If you have large changes in the data, then you would generally have large changes in the TLog files. Even more so with the four times a day backup up strategy. However, if you are just backing up TLog dumps and not actually taking a Simpana TLog backup, then your issue is equally explained with the large amount of changes in the database which result in large amount of circular changes in the TLog. E.g. Your TLog with 4 small VLFs. 0 = no changed data ; 1 = changed data The last VLF has a modified page in second place 

Each of these options will install various components depending on the installation type selected. A list of items installed with each installation type can be found on Oracle's support site in the following document: What Are The Different Oracle Client Components Installed With Different Installation Type In 12.1.0.x (Doc ID 1610994.1) (Oracle Support) 

It can be normal for Page Life Expectancy (PLE) on SQL Servers in a Data Warehouse setup with frequent reloads to jump up and down. Why? Well which data will users normally query? It's the data that has been most recently inserted, because that is the data that is interesting. Unless you are performing analysis over long periods of time and conducting forecasts based on last years data. Page Handling in Memory (Buffer Cache) Microsoft SQL Server has an internal counter in each page that is stored in memory. This counter starts at four and is deducted by one, each time the SQL Server database engine looks at the page, when no reads occurred.