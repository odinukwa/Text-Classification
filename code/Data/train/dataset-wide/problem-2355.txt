That's an extremeley broad question. If your DB's tables use date in the primary keys you could run into duplicate problems. If you query by date range you may have duplicates or holes in your data. You may even have difficulty saying which row was written before another, depending on your design. Reproducing any previous state may become impossible e.g. "What was the customer's balance on the 1st January?" and so forth. Really, you need to understand your application and DB design deeply, and test, test, test. 

Edit: re-tested in light of Mikael's comments. I couldn't reproduce my earlier output and have removed the offending line. 

The purpose of normalisation is to structure the schema so that each piece of data is held exactly once. Given that the last order number can be found by referencing the order table at run time then I would say the current design is denormalised. I wouldn't necessarily conclude that this is a problem, however. A beautiful, normalised, logical data model has to be implemented on messy, imperfect, physical equipment. This has limitations which have to be dealt with. One way is to denormalise data. If this is done as a conscious act with an intended outcome, and has been tested, then I don't see that it is necessarily a problem. Denormalisation due to premature optimisation and poor analysis is a problem and should be avoided. As for alternatives, I can think of two. An index on Orders with customer_id and order_id as keys will give very quick access to the last order number: 

Enable Change Data Capture on the tables of interest. Copy the rows thus flagged to another table that matches the definition of the first, but with an additional datetime column to show when it was written. Essentially a slowly changing dimension, type 2 in the Kimball nomenclature. 

It would be really helpful if you said what your issues were. One thing I can see immediately is a scope problem. creates its own scope. Your statement to a database will use that database only until the returns. Then you are back into whatever database the above-posted code is running in. Let's say "MyDB" for the sake of argument. The statement will form its own scope again (I'm assumuning you this in your real code and there's a typo in the pasted example), inheriting MyDB as the context since you haven't told it to do anything else. Nett result is MyDB is really clean and no other DB gets touched! Fix: include a USE statement in every . Have a good read of Books Online. If took me a while to get my head around it but all the details are in there. 

This query will return all current bookings which overlap your proposed booking, or an empty set if none exist. 

Take it's output and run it on the new instance. Those AD accounts will now have logins on both instances. (This is a good opportunity to audit your access and ensure all those users really are needed.) Backup your databases (a proper SQL Server backup, not a copy of mdf and ldf files) and restore these to the new instance. You may have to get creative with the restores if the drive names have changed. If there are a lot of DBs some RESTORE FILELISTONLY and a script will save much time. Finally, on each DB, script an ALTER USER to reconcile LOGINS to USERS. This can be meta-scripted as above. Test connections to the new instance. Update the applications' connection strings. Take the old DBs offline and, after a few days, tidy away the old server. 

Your question is ambiguous. Do you wish for a user to see all rows in one table, but only one of the three tables? Should a user be limited to only certain rows within a table? Must this be enforced within the schema or is it OK to use other aspects of the application and infrastructure? Anyway, here are some ideas. Most DBMS have quite fine grained permission systems. It may be possible to GRANT just the permission required on each table. Create views that expose just the correct tables, columns or rows. Grant permissions on the views but not the underlying tables. Write stored procedures for each read & write. Grant access to these SPs but not the tables. Alternatively, let everyone execute all SPs but have the code reject unauthorised actions. Triggers can achieve a similar end. In the application have separate screens for each read or write action for each entity type. Limit access to the screens through the authorisation system. For a SQL-only solution for row-level access, add further tables, each keyed by userid and the store or business or corporation id. There will be one further column which is the permission granted. For each statement touching a base table join to this permission table and qualify on the userid and action. If the user does not have rights zero rows will be affected. These permission tables can get big. It may be useful to put users into groups and assign rights to the groups. If there are rules to control which, say, stores a user sees these can be modelled instead of the naive approach above. The precise table definitions will depend on how the rules normalise. This can get quite tricky and will require a lot of testing. 

No, there is no requirement for tables to be related to one-another. The fact that you do have two tables separate from the other thirteen would be a code smell and you should be sure that you have not forced two different business objectives into a single database. If the two really do support genuine use cases for the others then there's no problem. A typical example are "staging" tables. Here data from an external source is loaded into a stand-alone table. Then that data is validated. Only then is it copied across to inter-related tables. Another is to hold system configuration or user options. These have no foreign keys to the "business" data but support the operation of the system which uses that business data. Lastly I would point out that the logical and physical designs can be different. The data model (logical) may document foreign keys but the data base (physical) does not declare foreign key constraints for one reason or another. This would be a very specific choice for particular circumstances and relatively rare, however. 

For each Transfer there will be two rows in AccountTransaction - one each for the FromAccount and the ToAccount; one will be a debit and the other a credit. Over the years this table may get large and queries sluggish. At intervals (annual accounts?) it may be worth rolling it forward to a new starting balance and archiving previous transactions to a different table. To answer your questions: 

"The harmonized system uses a numbering" - they are not numbers, they are codes. They just happen to use digits instead of Latin letters for the code values. As you have to retain leading zeros anything numeric will not suit. You have to go with textual types. Suppose the codes were 

Most RDBMS mostly use BTrees for indexes. These hold no summary information internally about the fan-out or total number of descendents. So the only way to get to a desired offset within the data is to start at the beginning a step through. If you were able to explicitly define a sequencing column within your table the RDBMS may be able to use an index on it to more quickly get to your offset (no guarantees; which indexes the optimiser chooses to use is a complex topic). The query would then become 

SQL Server supports DDL triggers. This would fire when a table is created, for example. Constructing the table trigger code dynamically in the DDL trigger would, I believe, achieve the desired outcome. 

This will return a row for every row in the table, showing empty strings for employees and values for students. If the objective is to return only students a better solution is to put the filter in the WHERE clause: 

There are many good schema comparison tools out there. Redgate Schema Compare and SQL Server Data Tools for Visual Studio are two that come to mind (I'm not affiliated with either). In a development environment, create two databases which match the the current "live" schema. Deploy your changes to one database only. Use the tool to compare the updated DB to the current DB. Call the generated script "Deploy.sql" or something similar. Now use the tool to compare current to updated -- NB the order has switched. Call the generated script "Rollback.sql". An important aspect of both roll-forward and roll-back scripts is that they should be idempotent i.e. running the script once or multiple times against the same DB should produce the same output. Generally this requires a lot of "if exists .." statements. The tools can generate these for you. If space is tight you can use the actual production database as the "current" DB. I prefer not to because of the (small) extra load and the (small) risk of mistakes. 

You could approach it as double-entry book keeping. Each user has an "account" for each product, to which you credit and debit transfers. You act as the "bank" in the middle of the network. Each account's balance is the stock, either on-hand for sale or out for work. The tables would be something like: 

Transforming columns to rows is sometimes referred to as an unpivot. Declare indexes on the keys and other predicate columns. Effective indexing is a separate subject for which we'd need the precise table design and the query. It may be possible to pre-calculate aggregate values from these normalised tables. Running a batch job after loading and writing aggregates to a further set of tables is one approach. Materialized views is another. 1.75 billion values is large but not outrageous by current standards. Any of the usual RDBMSs on reasonable hardware should be able to handle it. Unfortunately the devil is often in the detail. It is possible to defeat the best software with contorted queries and convoluted schema. To avoid that we would need much more specific information. 

Change tracking offers the functionality you're looking for. When switched on, SQL Server creates additional internal tables. Changes to your data tables are noted in these internal tables. System functions allow you to pull changes out of these internal tables and transfer only changed rows. A related technology - Change Data Capture - allows you to see the "before" and "after" values of changes.