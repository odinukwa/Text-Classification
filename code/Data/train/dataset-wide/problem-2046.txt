The vendor does offer a forum and a knowledge base of sorts for the tool. Plus, there is also the manual. I don't really understand what you mean by 'everyday database admin concepts' with respect to a modelling tool, though. Could you elaborate on that? However, asking specific questions on data modelling tools is certainly on-topic for DBA.SE. If you get to 'How do I do xxx in Erwin?' type of questions then it's fairly likely someone here is familiar with Erwin and can answer the questions. 

For your application you might get a win from a date reference table (make a clustered PK on date for efficient lookup), which will probably be more efficient than de-normalising the week and month onto your large table. 

DB locks can exist on rows, pages or whole tables or indexes. When a transaction is in progress, the locks held by the transaction take up resources. Lock escalation is where the system consolidates multiple locks into a higher level one (for example consolidating multiple row locks to a page or multiple pages to a whole table) typically to recover resources taken up by large numbers of fine-grained locks. It will do this automatically, although you can set flags on the tables (see ALTER TABLE in the books on line) to control the policy for lock escalation on that particular table. In particular, premature or overly eager lock escalation used to be a problem on older versions of Sybase and SQL Server when you had two processes writing separate rows into the same page concurrently. If you go back far enough (IIRC SQL Server 6.5) SQL Server didn't actually have row locking but could only lock tables or pages. Where this happened, you could get contention between inserts of records in the same page; often you would put a clustered index on the table so new inserts went to different pages. 

The question is a bit vague, but I think this answer (a) may provide some useful guidance and (b) is too big to fit in a comment. Capacity planning a server is a bit more complex than that. You need to take some basic queuing theory into account. Your model would describe a server at 100% capacity, which is actually likely to cause performance issues. In practice you would want to aim for the server running perhaps 25-50% capacity at peak load if you want your site to have consistent response times. Queuing models state that the average wait time increases hyperbolically as the system approaches saturation. 

The point of zIIP processors is that you can't run z/OS code on them. Your COBOL code won't run on them. However, according to this article DB/2 for z/OS is an eligible workload to run on a zIIP processor. I presume your COBOL code has embedded SQL. Embedded SQL architectures are actually preprocessors that generate code that sends the query off to the database behind the scenes, so your SQL code is going to run on the DB server. If the COBOL programme is sending SQL to the DB/2 server via TCP/IP (DRDA) then it should run on the zIIP if the DB/2 server is configured to run on it. If the COBOL code is doing a lot of client-side processing or it's not using this then this will not be eligible. Chances are the COBOL app is not using DRDA, though. 

Don't bother tracking audit data on a per-field basis. It's much easier to have an audit table with the same structure as the main table and timestamp and user information tracked on the table. This architecture has a few advantages: 

Nonetheless, this is more common than might be expected. I think the main reason one encounters data warehouse projects implemented in this manner is that ETL tools are quite clumsy to work with if your requirements for tranformation logic are complex. ETL tools often have the effect of dumbing down the architecture and fobbing off logic into the reporting layer, which significantly degrades the effectiveness of the data warehouse initiative. The volume of work and presence of a central database gives the illusion of a data warehouse while providing little of the benefit. Another N-tier view of a data warehouse One could interpret 'Data', 'Functional Logic' and 'Presentation' as an ETL and reporting process in a more well organised data warehouse system. In this case, 'Data' could be interpreted as the staging layer, 'Functional Logic' implemented in the ETL, presenting a dimensional data store and/or suite of data marts, and 'Reporting' implemented through a reporting and ad-hoc query suite. Considered harmful For this reason, I think the concept of 'N-tier' is unhelpful and even a bit disingenuous. It sounds a lot like something a middleware company or consultancy might describe in a white paper - a flawed and even somewhat misleading theoretical notion that sounds good on paper. 

Best to avoid entity-attribute-value structures if possible as they complicate the model and are unnecessarily fiddly to work with. It sounds like the fields are going to be the same or substatnailly the same across all the houses, so you're probably just better adding them onto the property record. The exception being if you really have a genuine 1:M relationship. An example might be if you wanted to record (say) dimensions and room type (living room, bedroom, bathroom etc.) by room for a variable number of rooms. 

I presume your users are creating reports or data extracts. You can grant permissions to a particular schema, so you could create a separate schema for your users and have the stored procedures reside in that schema. Configure the security on the rest of the objects so the users only have read permission. You can also do something similar by creating a separate database and either require explicit reference to the tables or create synonyms within the reporting DB. 

Depending on your performance requirements, 100TB is a fairly aggressive data volume. If you want Oracle, you should check out their Exadata systems. Also, take a look at the offerings from Netezza or Teradata. With that volume of selects you might want to look at an OLAP based front end or at least fairly aggressive use of materialised views and query rewrite. You won't get 500 table scans/sec out of anything.For stuff with less stringent latency requirements you might want to consider a larger number of data marts to provide the reporting capacity to your user community. In this case, SQL Server and SSAS might be an option for the data marts as the licensing on a larger number of servers will be cheaper than trying to do the same with Oracle. See (1). Conventional hardware on a shared-disk architecture is likely to be slow on this size data set. NO! If anybody suggests NFS give them a good kicking. Either direct attach storage or a multiple controller SAN with lots of mid-range controllers. Think in terms of maybe a couple of dozen MD3000 series controllers or something similar - if you don't go for a purpose built 'big data' platform. Get a storage specialist with experience in PB range data warehouse platforms. You're probably up for a significant ETL development job, and a lot of testing work if you have to meet a stiff SLA. 24x7 on a data warehouse is ambitious at the best of times. Is this an operational reporting platform? Perhaps you might elaborate on your requirements a bit. Sphincter-puckeringly expensive, and dependent on your performance requirements. Last I saw (a couple of years ago) Netezza used to quote $20,000/TB for TwinFin systems, making your platform $2m for 100TB plus the cost of your redundant server and backup hardware. Exadata is, I believe, a bit cheaper, but I don't have any pricing to hand.Take a look at Netezza, Exadata and a Teradata platform for comparison, and costings for Ab Initio as an ETL tool. 

If you want to spread your partitions over different physical volumes, you can still stick with 8 volumes. You can stagger the index partitions so that an index partition resides on a different physical volume to its corresponding data partition, e.g. 

Unfortunately there isn't a cookie-cutter way to do this. You have to start with some base records, write queries that join against them and then repeat. As you go deeper the queries get more complex as they have to go right back to the root. If you have the foreign keys then you can automate the generation of the queries, as the joins will just be through the foreign key columns. It's not as hard as it sounds, but here are a few pointers if you take this approach: 

The columns are each individual item, sorted by the total Line Value over all months, i.e. from all-time best seller to all-time lowest seller, with items having no sales at all excluded. The rows are the months for all time where data exists. You can use a set definition to specify any date range you want. Note that this assumes the existence of a dimension called [Date]. This query will show the items in order of total sales across the columns and the sales for each month by item down the rows. I'm not 100% sure if this is what you want; feel free to clairfy your question if I'm not on the right track. 

You could put the original query using into a subquery and wrap it with a query that filters the results. 

Pros: Better database modelling and schema management than VS2010, better version control system, easier to customise the build and project work flow, better management of specs and documentation. Cons: More effort to integrate tools, limited DB integration into build process. Assumptions: Assumes that control of change/release process in more important than automated or tightly integrated release management for DB schemas. Also assumes that automated DB schema management is not 100% reliable. Not terribly high tech or slickly integrated, but for a complex project you're probably more interested in control than cute automated features. I'd argue that you're better off with a set of best of breed tools and whatever home brew build and test scripting is necessary to integrate them. Just try to keep the build (a) relatively simple to understand and (b) fully automated. 

Beyond that, you're probably up for a commercial repository tool like Powerdesigner, Erwin or Embarcadero. If you're actually working for an outfit that has 'hundreds of enterprise databases' on the books there's a chance they might have something like this on the books already. 

See the comment on the question about #1 and #2. I don't believe MySQL supports recursive CTEs, so a union is likely to be the best way to do this. The self joins are only going to work where the parent key is not null, because NULL will never evaluate as equal to anything (unless MySQL has weird NULL semantics). In order to capture the parent the is a reasonable approach. Left joins for this are barking up the wrong tree because you can only specify them as a join predicate for the child and you're trying to include data from a parent row that actually has no join against itself. Another approach would be to make equal to on the pop () row, so the self join would also pick up the parent. You can tell the parent because the and keys are the same value. 

Absolute numbers for months and quarters allow them to sort correctly when used in a query tool. A 'month of year' value is only useful for sorting within a year, but an absolute month value that is in order will sort a list of months correctly across years. This is very helpful to support (for example) a report over a rolling 12 month period. Keys for week, month or quarter (the year itself is inherently ordinal and suffices for this on years) also allow attributes such as day of month to be keyed to a logical key. Although the dimension table itself may be flat, many tools (OLAP servers, for example) will still impose a logical hierarchy within the data and require a key that is unique at its level to do this. 

It's almost a matter of semantics. A lot of hot air gets released in discussions about this but I'm not really convinced that there is any real philosophical depth to a distinction between the two. At some level you can view ETL as transforming data in a client-side tool before finally loading it, with ELT implying that the data is transferred to some sort of staging area with relatively little change to the format. 'Transformation' takes place afterwards. These are very fluffy definitions and could be applied to a wide variety of technical architectures, and there are many possible designs that either term could be used to describe. I'm very strongly in favour of an architecture where all the transformation and business logic can be built into a more or less homogeneous code base, and I've done a lot of systems where the transformation logic was quite complex. This tended to just use the ETL tool to land the data and then all of the transformation was done in stored procedures. Arguably this could be described as ETL or ELT with the difference merely being one of semantics. Some tools are very database centric, however (Oracle Data Integrator, for example, is often referred to as an ELT tool). If you subscribe to this view, then 'Extract' and 'Load' are happening before the data is transformed as they are being landed into a staging area and then crunched by SQL or PL/SQL code (which may be generated by the tool or hand written). Several people I've talked to seem to regard the principal merit of ODI as that it's not OWB. If you use a client-side tool such as Informatica Powercentre or MS SQL Server Integration Services then the tool can do extensive transformation to the data client-side. Some ETL tools, such as Ascential Datastage and Ab Initio are designed to do a lot of work with flat files and in-memory data structures for speed. In this sort of architecture the transformation has already been done before it's loaded. Perhaps this type of architecture could be definitely classified as 'ETL', although I've seen many tool-centric projects where all the real work is done by a bunch of stored procedure code. There are advantages to various tools and architectural approaches, but one can't make a blanket statement about the merits of 'ETL' vs. 'ELT' approaches because the terms are so broad that the difference is almost meaningless. Some tools and architectures may have specific advantages - for example, Ab Initio's heavy use of flat files gives it a significant performance advantage on large data volumes. In practice, making the distinction between 'ETL' and 'ELT' is pretty meaningless without going into a much deeper discussion of the system requirements, platform and technical architecture. 

Intuitively, if I was doing an OLAP solution for a retail chain, I'd say your infrastructure is really inappropriate for a system with substantial data volumes. This sort of kit has trouble with the data volumes you get in insurance, which is probably a couple of orders of magnitude smaller than I would expect to see in retail. As gbn states in the comments, SSRS is just a web application. You can set up a farm - start with one server with a few GB of RAM and a couple of virtual CPUs. Monitor it, and expand it if it's overloaded. The amount of disk space used by SSAS depends on the volume and the aggregations you put on the cubes. The storage format is quite compact - more so than SQL Server, but if you have large volumes of data it will start to get quite big. If it's getting into the 100+GB range then you should also look at partitioning. A surprisingly applicable generic solution Now, your client probably doesn't want to hear this, but VMs are not my recommended hardware configuration for any business intelligence solution. They work OK for transactional applications or anything that is not too computationally intensive. BI for a retail chain is probably a bit aggressive for this sort of infrastrucutre. As a generic 'starter for 10', My recommended configuration is a bare metal 2-4 socket server like a HP DL380 or DL580, and direct attach SAS storage. The principal reason for this is that a machine of this sort is by far the best bang for buck as a B.I. platform and has a relatively modest entry price. If you put a HBA on the machine then you can mount a LUN off the SAN for backups. IOPS for IOPS, this sort of kit is an order of magnitude cheaper than any SAN-based virutal solution, particularly on sequential workloads like B.I. The entry level for a setup of this configuration is peanuts - maybe Â£10-20,000 - and it's a lot cheaper and easier to get performance out of something like this than a VM based solution. For a first approximation, the only situation where this kit is inappropriate for B.I. work is when the data volumes get too large for it, and you need something like Teradata or Netezza. What can you do with your VMs, though?