Suppose someone offered you a billion dollars to believe that the moon was made out of cheese. It would be perfectly rational for you to start believing in the cheese-moon hypothesis, assuming you valued the billion dollars more than you valued the 'truth'. So what would it take for a rational agent to believe God wrote a book? Like everything else, the solution is large sums of money. 

Therefore, we can view free will as being "compatible" with "hard" determinism. To use the same quote of Blackburn's, but highlighting it differently: 

These thinkers were all born in Europe or North America within the last 500 years, which it sounds like you consider "mercantile" economies. With the exception of Kant (and the possible exception of Gauthier, whose biography I cannot find), they were all born to white collar fathers. 

If there is a non-zero probability that others exist, then you should give their preferences a non-zero weighting. Since you said you broadly embrace utilitarianism, I will point out that this will maximize expected utility. 

He's obviously not interested in "happiness maximization" in a straightforward sense; Mill believed in a "hierarchy of pleasures" - i.e. certain types of happiness are better than others. So to Mill, the argument that free speech might decrease certain types of happiness would not be a conclusive argument against it - we might value the happiness which comes from exercising our rationality above that which comes from other things, so the tradeoff is worth it. (This is, as you say in your comment, closer to what some deontologists believe than what some consequentialists believe.) 

To declare my conflict of interest: I tend to be on the Skinner side of things. If you want to be free come what may, more power to you. But if you'd like to be coddled and nannied, then I see no reason why we should worry about trivial things like "autonomy." So I probably am not doing Kant et al. justice. 

I think whoever told you that the goal was to not use axioms meant that the goal was to avoid "numerical" or "algebraic" axioms. 

Yes. You can reuse the first proof. hence the evidence suffices to both prove X's unlikelihood and ~X's likelihood. 

As stoicfury noted, it's hard to define "rational" in a non-question-begging way. But let's recall why we believe the expected utility hypothesis: because any person who follows some basic rules will act as though they are maximizing the expectation of some utility function. On your thought experiment: sure spending $1 will cost me very little, but it will almost certainly gain me nothing. Deciding I'd rather have the small gain for certain that an (almost zero) possibility of a huge gain does not seem "irrational". Pascal's mugging is a more extreme example of your thought experiment, which you might find interesting. 

I would add to Joesph Weissman's mention of Thrasymachus that Glaucon (who comes later in The Republic) presents the story about the Ring of Gyges and argues that ethics is just a sort of social convention. The SEP's entry on moral relativism says: 

If that's the case then there is no moral dilemma; clearly killing the captain is the optimal solution. The reason that Williams thinks his version is a true dilemma (and one which utilitarianism gets wrong) is because, in his version, you don't have this easy out. 

So, roughly, killing the fetus is "side effect" of saving your wife and therefore you aren't using it as a "means to an end." How exactly one defines a "side effect" versus a "means" is difficult, of course. As to what Kant would say: who knows? You're right that in the inquiring murderer case he held fast to his guns and claimed that you shouldn't lie, but if more extreme examples were brought to his attention would he have changed his mind? I guess we'll never know. 

There is no morally relevant difference between some non-humans and infants Infants have direct moral status Therefore, these non-human animals must have direct moral status 

the human species is very likely to go extinct before reaching a “posthuman” stage; Any posthuman civilization is extremely unlikely to run a significant number of simulations of their evolutionary history (or variations thereof); We are almost certainly living in a computer simulation 

And of course there's the nuclear option - denying the value of "truth". Consider the Knight of Faith. One of my favorie Kierkegaard lines: 

Husserl, and to a lesser extend Heidegger, are the philosophers most associated with phenomenology. You might also be interested in Sartre's The Emotions. I think a lot of this subject is considered the realm of psychology these days. 

Here is a simple counter-example: have you ever eaten until you were sick? Even knowing that having that extra piece of cake will make you feel miserable, you can't stop yourself. Drug users have more counter-examples. Caffeine makes many people more motivated, but doesn't appear to increase satisfaction once the task you were motivated to do is complete. At a more physical level, this can be seen as a result of the fact that multiple neurotransmitters are responsible for "motivation" and "happiness." The reason you want to have sex is probably due to your dopamine levels; the reason you feel happy after having sex is probably more related to oxytocin. As Wikipedia says: 

This is a place where we might need to look closely at Aristotle's definition of "eudaimonia." Eating until you're sick might increase short-term pleasure, but decrease long-run pleasure. Is that eudaimonic? I think the usual definition would say no, but Pascal might say yes - I'm not familiar with his philosophy. The difficulty with defining "happiness" in a reasonable way is one of the reasons why contemporary philosophers frequently focus on preference satisfaction instead. 

I think is not an objection to compatibilism, because the "if...then" statement remains perfectly true even if the antecedent is never met. Indeed, I think most people who call themselves compatibilists would argue that you cannot, in fact, have chosen differently, unless the circumstances were different. 

The SEP has an article on these sorts of proofs. I think it's fair to say that even people who endorse some sort of ontological argument don't find Spinoza convincing, for the reason you say. However, since you wanted criticism I will point out that any proof like this is a "tautology", so I don't really think that's a valid objection. The question is whether its premises seem more reasonable than its conclusion, and they don't appear to. You might find Godel's formulation more convincing. He basically replaces the idea of "nature" with that of "essential properties," and brings in modal logic. (Note that he never published any of these proofs, so there are various forms of "Godel's argument" floating around, some more reasonable than others.) 

You can read the full paper for her exact reasoning, but it roughly has to do with considering all the various duties one has and weighing them against each other. (i.e. certain duties are more "important" than others.) This is a very common way to approach it. Another way, frequently discussed with regards to euthanasia, is the doctrine of double effect: 

EDIT: It strikes me that the analysis of language is analogous to a method of proof in math. For example, someone might ask me: "Why is it useful to analyze the group structure of arithmetic?" I could tell them Lagrange's theorem, and how Euler's theorem follows, and even if they didn't understand the first thing about groups, I believe I could convince them that the idea could be useful. So what's an example of a problem that seemed difficult in philosophy, but the answer was illuminated by the analysis of language? 

Let me formalize your question and see where it gets us. Let's view time as a set of instants. In order for your question to make sense, this set must have an order (a notion of "comes before") and be infinite. Let us additionally take the standard assumption that time has a minimal element (the big bang) but no maximal element. Call the minimal element 0 and some other element 1. My father could be born at time 0 and I at time 1 and we both live forever. This doesn't seem to violate our intuitions of causality, and we have a father and child who are both infinitely old. You can modify this set up (what if time has a maximal element but no minimum? Neither? Both? Is time discrete?) or you can define "older" in a different way (must the sets be capable of being put into bijection? Should we care about the ratios of their ages or their absolute differences?). Depending on how you define such things, the answer might change. 

It's sort of circular, as you would define "evidence that supports a hypothesis" as "things which would increase our confidence in the hypothesis." To elaborate a bit: 

You are right that Wittgenstein wrote on the subject, saying e.g. "The limits of my language means the limits of my world." Wikipedia has a longer list of important persons. 

If you could choose differently, then you would act differently (thus keeping some notion of "free will") You cannot, however, choose differently (thus keeping with deterministic physics) 

If you want to leave no loopholes, I would recommend wishing for a logical system to be true. E.g. you could wish that everything you show in naive logic would come true, and then use Curry's Paradox to prove whatever you wanted. 

While we don't know for sure what his life was like, it seems likely that he was born relatively well off but retired and lived a life of poverty to focus on discussing philosophy. 

This is sometimes called the Sapir-Whorf Hypothesis. Wikipedia has an extensive article on the subject, which summarizes: 

In a traditional formulation of Peano arithmetic (due to von Neumann, I believe), we start by assuming that nothing exists: i.e. the empty set exists. We can then define the successor of zero ("one") to be the set which contains the empty set and zero. So if you mean "assume something exists" to be something like "assume the empty set exists and various set theory axioms are correct", your assumption will lead not just to the existence of "1" but of 2, 3, ... Conversely, if you deny the existence of some natural, then you must deny either that something exists or that these axioms are correct. The point being: it depends on what you mean by "number." If you take this basis that I've described here, and you can say "assume X exists", then you can let X be your "zero" element, and inductively prove the existence of the naturals. See Wikipedia for more. 

Having read that other question, I think I understand the issue. I'll do my best without LaTeX, but this might not be pretty. Suppose we consider everyone's utility to be of equal value. The total utility of a population would be the sum of each person's utility, i.e. By the definition of arithmetic mean, we can see that . So an equivalent definition of deals with the arithmetic mean. Here's where I think the confusion happens: does not deal only with the mean. It also includes the number of people. So if adding this person would decrease the mean utility, that would be OK so long as net utility was increased. If their utility was positive, then it is guaranteed to increase net utility, so essentially the question "should we bring person X into the world" is just "will person X have positive utility?" Which (in your scenario) is independent of others' utilities, and so there is no problem. EDIT: to be clear, this is what is sometimes known as "total utilitarianism". There does exist a variant of utilitarianism which does consider solely the average utility. In its naive form, your criticism is valid. I think proponents usually have some sort of "two-level" thinking whereby they switch between total and average utilitarianism as necessary. To the best of my knowledge, "utilitarianism" usually means "total utilitarianism", which is why I answered this question this way. If you were interested in a defense of average utilitarianism, let me know. 

As a side note, I don't think you meant to say "prescriptive" (unless I've greatly misunderstood you), but you might instead have meant "normative". 

In this paper of Aaronson's, a proof is given of Occam's razor by appealing to PAC learning. My understanding of Valiant's bounds for PAC learners is that it requires i.i.d. This is often a reasonable assumption to make. However, he used this proof of Occam's razor to justify the problem of induction (i.e. claim that the future will represent the past). If you assume that the future and past data are drawn from the same distribution (which is how I think i.i.d. is being used), this strikes me as essentially assuming that the future will be similar to the past, so using PAC learning here begs the question. Am I misunderstanding the use of i.i.d.? Is there a way around this problem? 

Suppse Sleeping beauty is told that there was some large number of coin flips, and that she is being woken on the nth flip as a result of it being heads or tails. Should that change her guess from if she was just told that the experiment was repeated once? What if she was told she was woken from one of the n flips, but not necessarily the most recent one? If we believe the Self-Indicating Assumption, I don't think the answer would change. But I'm not sure what would happen if we believed the Self-Sampling Assumption. 

Basic probability theory tells us that if these three things must add up to 100%, then we can decrease the probability of #3 by increasing the probability of #1 or #2. Why might we be likely to go extinct? Wikipedia has some answers: 

EDIT 2: There are many possible solutions to the repugnant conclusion. So I'll give my favorite: there is no such thing as a life worth living. I like it partly for shock value, but it also makes a good point: there is a fundamental difference between "this life is so good it requires you to be born" and "this life is so good it requires you to not die." I.e. once you've been born, moral laws apply to you which didn't apply before you were born.