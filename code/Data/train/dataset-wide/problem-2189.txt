Normally, I would recommend putting tempdb on its own disk(s) and size the database from the start, so you'll avoid the autogrow thing entirely. Autogrowing by 10 percent is often really bad practice for two reasons: 

Look for files on your drive called (database files) and (log files). The filenames don't neccessarily have to be the same as the name of the database. At a very minimum, there will be one of each type for any given database, but it's quite common to have multiple database files, often spread over different volumes. Move those files to their intended locations, then attach them by right-clicking "Databases" and choosing "Attach" and adding all the files. In my experience, adding the first file will often suggest the remaining files, but I don't know this for a fact. 

Because I've put the two conditions in a single clause, the two queries only scan the c_contact table once, making them much more efficient. In particular, an optimal index for this type of query would probably look something like this: 

The advantage of using the window function is that you eliminate all sorts of recursive solutions and improve performance dramatically. For best performance, you would use the following index on the table: 

If this query still gives you performance issues, try creating a temporary lookup table, populate it with the rows from "lkp", then join "t" and "lkp" as above. I would probably give the temp table an index like 

.. or even numbers (). And because the numbers are "ordered" with the in the join, will always be the lowest and the highest. Any more advanced features (or tuning for that matter) will come down to what platform and dialect of SQL you're using. 

I'm pretty sure that sp_BlitzCache will show you the cost of the actual plan used for your queries, whereas the server will considers a parallel plan when the initially estimated cost exceeds the threshold value. A noticable difference between estimated and actual query plan costs could happen if you have stale/bad statistics on your tables. If you've identified a specific query that you want to run serially, you can add the following at the end of the statement - this will ensure a serial plan: 

Here, the workload is split up into multiple elements. Some situations will benefit from an inline table-value function, whereas others (like yours) will suffer a performance penalty. It has to do with how many tables you're joining, the amount of data in each table, indexing, etc. 

I would agree with your findings. The pattern that your consultants wrote handles NULL values, and is also a really nice way to manage a very large number of comparison columns. But if the column cannot be NULL and performance suffers, I suppose there's no point in keeping it. Your test could be inaccurate in the following regards: 

Note: , , etc are deprecated and will probably stop working in a future version of SQL Server. I go into a bit more detail on SQL Server security in my blog post. 

As others have suggested, I would try batching the incoming transactions. One approach could be storing incoming rows in an in-memory table (requires upgrading to Enterprise Edition) and then move batches of rows into their final storage using something like a SQL Server Agent job every five minutes or whenever you are sure they won't change again. In-memory tables considerably reduce locking and latching issues, which make them ideal for the type of insert-update-update-update-delete workload you're describing. This is particularly efficient with natively compiled stored procedures. 

By default a login can see all the databases on a server. You can , after which a login can only see the databases where he has access. To verify what databases a login can see, try this when you're connected as an administrative user: 

Note the expressions: If a.StartDate is within the week, this is the weekStartDate, otherwise, use w.StartDate. Same thing for the end dates, but the other way around. 

Adding to the other answers, from a pure database performance perspective (and it seems the database is probably not your main bottleneck), this part of your query: 

The operator is equivalent to a series of , which doesn't help you at all here. Instead, I would build the query in a manner like this. 

If it's any use to you, I've written a script that compiles all of the permissions in a database. It's available on the Downloads page of my blog, and it's completely "as-is", without any warranty or guarantee. 

The error message is there for a reason. Using on thousands of tables in a single query is performance suicide. Yes, I understand that your reporting application is already in production, but I would still strongly recommend that you to go back to your development team and update the application or, even better, rebuild its logic in a stored procedure instead. As you suggest in your question, create a temp table, run each of those queries sequentially and dump the results into the temp table. Then, compile the report from the temp table. All of this can be done in a stored procedure, which will also cover other important aspects like security and performance. This carries a range of advantages: 

.. and see which works best. EDIT: When performing bulk-logged operations, make sure you make a backup (full or transaction log) before and after the operation if you need point-in-time restore capability and you suspect that other activity may be going on in the database at the same time that your ETL job is running. I wrote a blog post on minimally logged operations a while ago, there are links in there to other posts and documentation. 

Now all we have to do is calculate the number of days between weekStartDate and weekEndDate for each row. Divide this number of days by the total number of days for the Actual (=12), and you get a distribution key: 

Documentation on the database product is a bit thin, but it would seem that there is no such thing as fixed-length alphanumeric datatypes, but rather just a "string" datatype, which the ODBC provider perhaps translates to LOB. Is there a way to convert the database column to an alphanumeric datatype that SSIS will see as a fixed-length column? 

There are a few good DMVs (system views) where you can find this info: sys.partition_functions and sys.partition_schemes can be joined to sys.partition_range_values to get all existing ranges. The number of ranges should be the number of partitions. The data_space_id of the partition scheme can be joined to that of the index or heap in sys.indexes on the table (sys.tables) that you're interested in.