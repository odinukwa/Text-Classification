This is a "running total" type of problem: each row's total is calculated based on that row's value added to, or subtracted from, the previous row's total. Since you are using SQL Server 2014, Transact-SQL has built-in syntax available to you to help you with getting the results. Using the simplified model of "One deposit per month, many withdrawals per month", the SQL statement could go like this: 

Just group your @LatestFifteenMinTickets table by DomainId and take the difference between and . That will give you the number of sold tickets for each domain within the period collected in your table variable: 

The former way seems cleaner, the latter, however, is more flexible, as it will be easier to expand when we want to add a filter on the number of rented books per member. You would just need to add a HAVING clause: 

That is, you use the source row as a derived table and cross-join it to the target row and then just update necessary columns of the latter with those of the former. The target can also contain more than one row, if needed: 

The original row set is cross joined with two inline views, one that represents the required prefixes () and one that specifies how many spaces there may be between the prefix and the subsequent number (). This way the query will probe for all combinations of prefixes and numbers of delimiting spaces (, , , ). The string is searched for each combination, one after one, to calculate the starting position of the prefix. It uses PATINDEX, which, for each prefix and space number, builds its own search pattern. As per the assumptions above, PATINDEX expects a space before the prefix and at least one digit after the prefix and, possibly, a space. So, if, for instance, the current prefix is and the current expected number of spaces after it is 1, the pattern would be . To make sure there is a space before the prefix, a space is added at the beginning of . That trick accomplishes two things: it helps to find the prefix if it is at the beginning of the string and it makes the resulting starting position accurate. The latter is important because technically the returned position would match not the prefix itself but the preceding space. So, to get the actual position of the prefix, we would need to increment the result by 1. However, because we had added a space at the beginning of the source, all the positions were shifted forward already. So, in the end, the result points exactly at the beginning of the prefix. One final touch here is that if the returned position is 0, it is transformed into NULL by NULLIF. That helps to avoid errors about negative length values passed to SUBSTRING later. The found position is then used in CHARINDEX to find the first comma in found after that position. Again, to make sure there is a comma, the character is appended to . The result constitutes the ending position of the sought item. Using the obtained starting and ending positions, the item is extracted from using the SUBSTRING function. Finally, in the SELECT clause, before actually returning the item, the query additionally checks if there is a space after the number, just in case the item was not properly delimited by a comma. By appending a space to the item, it uses the same trick as when searching for the comma position, also taking into account the length of the prefix and the expected number of spaces after the prefix to make sure the found space character will be the one after the number. In cases where the prefix is not found in the string, returns NULL. Such rows, however, are excluded from the output by the WHERE clause, which specifies that the starting position be not NULL. 

Your own method of looking the value up in the table works as well, only it means an extra table hit, which may affect performance. 

So, in your case QUOTED_IDENTIFIER gets reset at parse time and is, therefore, OFF by the time CREATE INDEX is executed. The engine complains about it as expected. To resolve this, you don't have to resort to dynamic SQL, it is enough to put a line just after the CREATE INDEX statement to make it a different batch from the subsequent SETs, which would cause it to be parsed separately from them: 

This is called conditional aggregation: you are aggregating the results based on some condition. One of the charms of the method is that you can still get the total count in the same output, if necessary, by including into the resulting column set. 

Note that the above method assumes your table has no other columns apart from those shown in your example (UserID, Year, Mth, Value). If there are other columns, replace dbo.YourTable with a derived table pulling only those four columns: 

In this case the action needed is raising an exception, but the message returned with the exception needs to depend on what condition was checked first. The assignment statement uses a CASE expression to choose which message to store in the variable. You can also see that the error is raised only conditionally – only if the variable actually contains a message to show. If the value is an empty string or a null, the script will just continue without interruption. 

In order for the transformed set to follow the order of , you could include those columns in the output and use them for sorting: 

I started with an attempt at fixing Max Vernon's answer so that it would work for any year, but ended up with two different approaches. Approach 1. Being clever In this method I am calculating the Sunday-based week number directly from the offset from the year's first Sunday-based week's Sunday. A year's first Sunday-based week's Sunday can be obtained like this: 

If by distinct combinations you mean distinct sets of numbers, regardless of their order (in fact, sets are unordered, according to the algebra of sets), then the only fix your script requires is to replace each with either or , by which I mean that it should be either all or all . After you switch the comparison symbol, some comparisons will become redundant, and so your query can be simplified. Here is one way to rewrite it: 

The GetPermissionStatus inline table-valued function's result can be either an empty set or one single-column row. When the result set is empty, that means that there are no non-NULL entries for the specified page/user/permission combination. The corresponding Pages row is automatically filtered out. If the function does return a row, then its only column (IsAllowed) will contain either 1 (meaning true) or 0 (meaning false). The WHERE filter additionally checks that the value must be 1 for the row to be included in the output. What the function does: 

I am puzzled about how your query can run without errors if you just remove HAVING, but the semicolon is definitely out of place in the posted code. 

If Chris's assumption that four joins should be enough for most cases does not work for your particular case (and I don't mean the simple example in your question, of course), you can throw in more self-joins using the same pattern. 

The nesting, however, is superfluous too. If one pattern is not matched, SQL Server will continue testing the next one, until it reaches the ELSE clause. Therefore, your CASE expression could look simply like this: 

Use the above as a derived table to join it back to the original table in the UPDATE statement, so that you can populate the missing values from the derived table: 

*This particular implementation of the grouping method assumes there is always one row per attribute per person, so works correctly. If attributes of the same kind may, or will later be allowed to, repeat per person, use instead. 

On the other hand, if your procedure is more than a single INSERT statement and you would like to abort the entire procedure on an invalid insert at its beginning, then preliminarily checking the FK value may not be entirely pointless. However, valid FK values would still be causing double lookups, and that is neither good nor necessary. To achieve the same effect of aborting the entire procedure, you could just put your procedure's body into a TRY/CATCH block: 

The row constructor represents your pattern list as a table, additionally supplying each pattern with the number of rows to retrieve for that pattern. The CROSS APPLY operator applies your query to every row of the pattern list, i.e. to every pattern, limiting the number of rows for each pattern to the corresponding value from the pattern list. As a side note, please let me take this opportunity to suggest that you always qualify your columns with the table alias in a query that is reading from two or more tables. That makes your query easier to read/understand. You can always use short aliases to avoid repeating potentially long table names. For instance: 

So you are retrieving a column collected from either of two columns, and , depending on whether the other matches . After retrieval, you are checking that the retrieved or be among the connections of User 4. Finally, you are counting all distinct occurrences of the resulting column (which, I will repeat, is a mix of and ). This is how you could do that without the many nesting levels you have attempted and keeping all the correlations at the same level: 

By this method, all rows that have any of the specified attributes are retrieved and grouped by . In order to determine the groups (persons) having all three attributes, a HAVING filter is introduced to compare the number of rows* in each group to the total number of attributes in the list. The method can be slightly generalised if you can afford storing the attributes to search for in a (temporary) table. Here is what it would look like in that case: 

Yes, more concise ways of pivoting exist for your scenario. Option 1 For instance, you could use the PIVOT operator: 

I have rearranged the original joins slightly to make it more obvious that you need to join repeatedly both and . Note, however, that you can avoid repetition of the pattern with the help of a "local view", more widely known as common table expression (CTE), or WITH clause. You can implement the join in the CTE and then join repeatedly just the CTE: 

Just a join, no derived table, simple and elegant. However, in terms of performance, the latter solution could be worse or much worse, depending on the DBMS you are running it in. The former option aggregates only a single table, which is much easier to optimise, particularly when you have proper indices defined. Aggregation across multiple tables, on the other hand, is trickier. The RDBMS might take into account explicitly defined foreign key relationships, if any, but that still would be unlikely to achieve the same efficiency as aggregating one table individually and only then joining the resulting set to other tables. Perhaps, the last query could be slightly improved by replacing the column in the GROUP BY with , i.e. like this: 

No WHERE clause here – it is replaced by the join to the table of queried attributes, and the total number of attributes required to match is derived from the same table instead of being hard-coded. This kind of problem is commonly known as relational division. It is discussed in detail in this article by Joe Celko: 

Here's a SQL Fiddle demo* link for this query: $URL$ UPDATE To return only one set, you could add in one more round of ranking: 

What is happening there is you are matching the two column sets as rows using FROM-less SELECTs and the INTERSECT operator, rather than matching separately each column against its counterpart using the operator. The difference is that in this case two nulls are considered to be equal to each other. (When comparing them using , they are not.) So, as I said, if you consistently store the rows as in both tables, the simple rewrite above will let you match those rows when both tables have them. 

There is a way to implement this declaratively only without changing your current setup much, if you agree to introduce some redundancy to it. What follows can be considered a development on RDFozz's suggestion, although the idea fully formed in my mind before I read his answer (and it is different enough to warrant its own answer post anyway). Implementation Here is what you do, step by step: 

PIVOT and UNPIVOT are indeed not supported under compatibility level 80. However, you can unpivot rows using a nested VALUES constructor. The resulting query in my case looks slightly unwieldy because of the double nesting, but it works in SQL Server 2008 with any supported compatibility level: 

In this case it will not matter whether you use MIN or MAX over and later (in the main SELECT): each column will have exactly the same value (first or last accordingly) across all rows of the same group, and so and would return identical results in each case. In fact, you can get directly in the CTE and then, in the main query, just either aggregate it using MIN/MAX or add it to GROUP BY and reference it without aggregation, like this: