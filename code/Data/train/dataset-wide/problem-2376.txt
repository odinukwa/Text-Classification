Vikram, If your remote sites are named unique, then you can utilize HOST_NAME() as a FILTER while creating a Merge Publication, we are doing the same at our place. We have 2000 stores across USA, which we replicate (via Merge Replication) to our home office. All these stores have same schema across the board, so table ID is going to be duplicated across the board. To overcome the duplication issue, we utilized followings columns in our Replicated Articles: 

We will have a network pipe between our existing and new data-center, but it is not going to be that big. It is also going to get consumed by all the VM's our infrastructure team is going to transfer on daily basis. We ran a Full Backup (i.e. Ideara SQLSafe with iSpeed settings) on this ~7TB of database, it took ~9 hours and size was ~630 GB. We checked with the vendor who is managing our colo migration and network pipe between two data-center and they said, it won't be possible to Copy over 640 GB file faster to new data-center and we would have to do Full + Differential + TLOG to get up to date. Our issue is, data we are getting from all our store systems every 30 mins (via Merge Replication to Home Office), it generates lot of data change. If we try to do Full + Differential + TLog, we will be having same sizing issue and lot of downtime to copy over Differential and also we would have to deal with the Log file getting full, which we have limited space on our existing database server (as we are not getting XIO Storage anymore). We were thinking about SAN Replication, but, as our SAN is going to be different at the new data-center, that won't work for us. What approach you guys think we should take to migrate this ~7 TB of database from our existing data-center to new data-center with minimum downtime? **Note:Please let me know if I miss anything from the information side. Thank you, HP 

Recently we had an issue with a sql server 2008 r2 HA mirror. I have tried to reproduce this in a lab environment and i came to the conclusion that i am missing something OR something is not possible. I am using 2 sql server 2008 r2 servers. Both the servers have a iscsi storage connection to a storage server. Mirroring is done with a witness server in HS mode. i have followed a blog post by Glenn Berry about how to create a group failover, he told me on twitter it would also be usable in case of failing storage. However, somewhere i can't really get things to work. What am i doing. I have a sql-01 as princiapl and a sql-02 as mirror. 2 databases running in HA mirror mode. I have a failover script running on WMI 7 and 8. When i go to my storage server to disable the storage of sql-01 i would suspect that eventually i get a failover to sql-02. but it doesn't... Anyone can give me some hints on this? Edit : Failover with a shutdown-ed storage for sql-01 looks working, because of a tempdb that is getting corrupt which needs recovery (according to logs). Therefor the instance has to shutdown (also in the logs), so probably that is my reason i can failover from sql-01 to sql-02, by accident and apparently not by design. 

Well, the reason you don't see everything is quite easy, the buffer cache gets cleared when new resources are needed. You don't have unlimited memory available, so no unlimited cache to hold the plans. That's why you probably are seeing only a top level of queries... 

Table ID RowGUID (It will be unique per server at your remote location) SourceStore (this is filter for HOST_NAME(), which will make record unique at the home office (i.e. Central Location) 

EDW developers have created Views and StoredProcedures for end user (i.e. BI Analytics, Application...) to consume the data, this way, whatever changes or modification EDW group is doing to underlying objects on their end is not directly affecting end user. Application Developer (i.e. OLTP App) group who are consuming these Views/Storedprocedures, so they are not allowed to make changes to these EDW objects nor are allow to create adh-hoc queries from OLTP application, as it is also very critical for EDW group to get their load times done under certain hours. We are looking at these Views and Stored procedures, to see if we can tune them for performance. So far whatever I have read online I see that, it is not a good practice to utilized EDW for OLTP load as, OLTP application are chatty and they mostly need is recent data and not so much of historical data. Question: What are your all suggestions on, how can we utilize our current EDW data for these OLTP applications, so that, we are not sacrificing Load Times for Performance and vice-versa? 

Quick Information: Our EDW group has created On-Premise SQL Data Warehouse by collecting information from several systems. These SQL DW databases are made up of: 

We are going to migrate our data center from one state to another in next 3 months. I wanted to know, what is the best possible way to migrate one of our biggest database (i.e. ~7TB in size) as quickly as possible. Here is the information on the server and database: 

I was able to automate the process enough, keyword being enough. To help anyone else dealing with exporting from Microsoft Access and importing to PostgreSQL I'm providing details that helped us work through the errors we encountered. While I'm fairly competent with MySQL and new (though still reasonably well at PostgreSQL) the people who setup the various Microsoft Access databases didn't setup the databases/tables/columns well (understatement). First download "MS Access to PostgreSQL", it's freeware and I plan to make a donation to them. Currently on their download page the download link is on the right side of the table, threw me off for a little bit. $URL$ Secondly I needed to download the "2007 Office System Driver: Data Connectivity Components" which works fine with Office 2010... $URL$ Thirdly I opened up MS Access to PostgreSQL and followed the directions having the data imported to an existing empty database. Keep in mind that the program (at least by default) will spawn errors if there are tables already there with the same names (it won't automatically append/overwrite). Thankfully it provides a checkbox list of tables so when you've confirmed via psql or pgAdmin III (you must click on the left column before pressing F5 or it won't update the database/tables information) that at least some of the data has been imported it is time to deal with errors. There were two error types that I've encountered for the most part thus far with just two separate databases. For accounting the person who setup Access used a proprietary type for some of the columns, the program didn't like that so I was able to maintain the row values by simply converting it to the type. I'm correcting the database structure and all that after I've freed it from Microsoft's shenanigans as I'm trying to reduce the complexity of exporting it. You can change the column type by choosing the table, then clicking on the Fields menu in Access 2010, then under the secondary Formatting hierarchy you'll find "Display Type:". Apparently the program also does not like it when a column shares the same name as it's table so I simply added a prefix or suffix underscore and make note of it to correct after the dubious process of migrating the data is complete. In general a necessary practice in Access is to right-click on the table name under the Tables object list, at the bottom of the context menu should be a Check Web Compatibility menu item. If it finds errors it will create a whole new table in the database; in Access 2010 URLs to the Microsoft website were clickable. Another error I encountered was binary data, an image that exceeded the programs ability to import. I exported the data as XML and browsed to determine which row had the data, found the ID in Access and removed it. You can save it (in the Microsoft way at least) by copying the row itself and paste it in to Word...how to export it as a normal image I'm not sure offhand. The following are my notes that I've written down to quickly repeat the process to get Access data migrated to PostgreSQL. I originally was going to just export as XML and import to PostgreSQL though I was unable to find a working solution in that manner and I would have still have had to deal with some of the issues mentioned here.