libvirt is very lightweight. It's unlikely that it will have any noticeable performance impact, even when collecting metrics continuously via libvirt. Of course, if your hypervisor is already significantly overcommitted and you are having performance problems, then adding anything will make it worse. In this case you should fix the performance issues first. 

Unmount . This is necessary because we want to restore the file context on the mount point in the parent filesystem as well as the file contexts within the child filesystem. Restore file contexts for the mount point. 

The socket that gunicorn actually opened, as shown in the journal entries, differs from the socket configured in your systemd unit. The journal shows what gunicorn actually did: 

Make sure rabbitmq is actually listening on port 5672 and that the port is open in your Linode server's firewall. In your config, RABBITMQ_NODE_IP_ADDRESS should be blank, RABBITMQ_NODE_PORT should be 5672. 

haproxy is in the RHEL Server Load Balancer (v6 for 64-bit x86_64) channel. You'll need to add this channel to your server. 

I've selected all the files I want to look at, here in , but you can look elsewhere if you want. I suppress error output from because it's noisy. UsingÂ  causes to not output anything (because seeing a bunch of would be pointless). And by using I only run if found a match. So I get: 

Given the question in the documentation there, it would be wise to verify this applies to your specific (older) kernel before proceeding. 

You're right that nginx isn't apache, and that means that most of the places you might think of using rewrite, are inappropriate. Instead you should try to use . For instance: 

Forget this setup and buy a purpose-built device which performs the same function. These Ethernet-wireless LAN client bridge devices (which can be hard to search for since there's no real standard name for the device's function) range from as little as $20 USD on the low end to $1500 or more for a ruggedized industrial grade device. In an office, you probably don't need IP68 protection, though... 

It should be sufficient to encrypt the virtual machine swapfiles that ESXi creates. Try putting the swapfiles on a datastore that's encrypted, such as an encrypting SAN or self-encrypting disk. 

Your header has no value. This is a pretty blatant violation of RFC 5322. Fix your application so that it provides a valid value for the header. 

Why shouldn't you give up delivering email after one day? One good reason is weekends. Email is not now, and never was, particularly reliable. In the early days of the Internet, the 1980s, it was entirely possible for email to take a couple of days just to reach its destination, what with some network links not being 24x7, over expensive long distance dialup calls (back then it cost per minute to call two towns away, nevermind the cost of a call from Sydney to Los Angeles), or even over amateur radio. As a result, it could take a while to deliver email, and the protocols had to cope with unreliable and part-time connections. They do this very well, but even then, mail could get delayed or lost. Certainly today, email has an illusion of reliability, if only because the underlying transports are more reliable, and many uninformed people (like most of our users) have an expectation that it is reliable, but that expectation does not match reality. Without a significant change to email delivery protocols, which will probably never happen, email, like anything built by humans, will always be less than 100% perfect. Sometimes, we sysadmins take advantage of that. For instance, in an office where everyone is only there Monday-Friday, I can have an email outage lasting all weekend if necessary. Of course, it virtually never is necessary to be out that long, but I have had to have email down for over 24 hours in rare cases. In such a case, if you give up after 24 hours, email sent Friday afternoon may not reach its recipient. The sender won't find out until Monday morning, but if you had kept trying, the recipient would have had it by Monday morning. Further, it's very important to set user expectations appropriately. The fact that Internet email is not and never will be 100% reliable needs to be clearly understood, even as we like to think that it is. The RFC says you should keep trying, precisely because things go wrong, and it's intended that the mail be delivered eventually, if possible, but at some point you do have to give up. It might be OK to reduce this to three days. I've always thought five days was too long to wait for delivery for most messages on a 24x7 Internet. 

If you need it now, you can obtain Red Hat's 6.4 SRPM and rebuild it on your system (the same way CentOS will shortly). For example: 

The file is in the package, which is in the Red Hat system repositories. The fact that you can't find it most likely means that your Red Hat subscription for this machine is not active. Give the machine a subscription entitlement and try again. Alternately, convert the system to CentOS. 

To avoid caching specific paths, though, you need to combine this with a , which lists the URLs you do not want to cache. Note that must appear outside every block, and inside the block. 

Best guess is: Your files aren't actually on a UNIX or Linux filesystem, but on a Windows-based filesystem, either mounted locally (NTFS) or remotely via SMB/CIFS. These filesystems don't permit one process to remove a file being used by another process. On a normal UNIX or Linux filesystem, you could replace the file with no issues, and the process that had the file open would still use the old file until it closed it, at which point it would be permanently deleted. The solution here is to move your files off the NTFS or CIFS share and onto a native filesystem. 

OpenShift runs every file in the directories on the relevant schedule. Thus, we see that first is being executed by bash and throwing a syntax error. Then immediately afterward is being executed. To resolve the problem, get rid of and add this to the first line of : 

You need to add two custom domains for your website in Windows Azure. You already added , now you need to add the same way, except this time you will use the A record that Microsoft gives you in your DNS instead of the CNAME. You also have to set up the special record for it. 

You said that you wanted 3DES for IE8, but you have explicitly disabled it in the cipher spec! Just remove that: 

Your semantics are reversed. The rules you posted permit outgoing DNS connections to a remote DNS server, not incoming connections to a local DNS server. To permit connections to your local DNS server, reverse the INPUT and OUTPUT rules: 

The documentation covers the various SSL variables that nginx sets. First you have to actually set to or (depending on your requirements). Since this can only be used in an or block, if you only want part of your site protected by client certificates, you'll need to use and have the application check the verification result. Then the verification result will be stored in and the client certificate identity will be stored in . You then only need to pass these up to your application. If you need more than the subject identity, you can pass the entire client certificate with . 

Your site was compromised, but the malware site that it linked to has been taken down. You will need to clean up and reinstall WordPress, and most likely the entire server. 

You'll find in your pool configuration file in the directory. The default file is which configures the first and default pool. 

Since your block is listening on port 88 and you used a relative URL in your rewrite, nginx uses port 88 in the resulting URL. To fix this, specify the complete URL. For example: 

The first argument to must be an absolute path to an executable. Substitutions are not accepted. If you're just loading a bash script which then starts Node.js, you can do that explicitly: 

The only other place on the Internet where this particular error is referenced is in comment 9 of bug 1153727, which is marked as fixed. I would suggest upgrading the "functioning" nodes to the latest available version of Percona XtraDB Cluster. You should then be able to join new nodes of that latest version. 

By default is a tmpfs, and doesn't exist on boot anyway. To get into this situation, your server must already be non-standard in some way: Someone explicitly configured your system to not have emptied on boot. So it is best to fix the problem, by undoing whatever changes were made to cause to not mount as a tmpfs on boot: 

Cause Red Hat upgraded the version of OpenSSL in EL6 from 1.0.0 to 1.0.1 in the 6.5 update, in order to resolve a years-old feature request to add elliptic curve cryptograhpy support. This package is no longer binary compatible, and programs that were built against OpenSSL 1.0.0 must be rebuilt from source against 1.0.1. Unless you're installing third party applications, of course, which almost everyone does. Those have to be recompiled, too, and at this point most third parties have done so, and built new packages against 6.5. It is these packages that third parties are generally shipping today. Resolution Identify all of the impacted third party packages and contact the third-party package vendors for updates. Once updates are available for all packages, you can safely update your system to 6.5, installing the third party package updates at the same time, which will complete the resolution. For packages installed through the package manager and yum repositories, this is trivial; simply attempting to upgrade and being able to do so without dependency problems means that the packages are ready. For packages manually installed, you will need to check these yourself and apply whatever updates the vendors have provided. You should also pressure these vendors to supply proper RPM packages and yum repositories in these cases. Most users can update to 6.5 with a command such as: 

Use a Linux desktop. Every time I attempt to do something with PuTTY I am utterly frustrated by the constraints imposed on me, both by PuTTY and by Windows. If you have to manage Linux boxes, doing so from a Linux box gives you much more opportunity for better integration. The mouse works as expected, for instance, while it doesn't in Windows. Transferring files is more straightforward. And so on... 

You end up with duplicated packages when crashes during an installation. Since you apparently have a 512MB droplet, you should be aware that frequently needs more memory than that to complete an upgrade of any significant number of packages. It's common on 512MB virtual machines for to run out of memory during installation, and the solution is to set up swap space. 

The first problem is that (eventually) you need to identify everyone who has an old-style password and have them changed to a new-style password. Old passwords are not secure. Now, for the immediate error, this is happening because the client has secure_auth set, but the user has an old password. In order to login with the old password, the client must disable secure_auth on the client side. How exactly you do this varies by which client you're using. Some other workarounds can be found in the MySQL documentation: Client does not support authentication protocol 

A CNAME record won't work in this scenario. The web server you redirect to has to be aware of the redirect and set up to support it. Instead, set up an AAAA record with your IPv6 address. 

Books are all well and good, but very out of date books may not be so useful. Even IPv6 has had significant revisions in the last ten years. The best source of truth is the relevant RFCs, both the original ones and any that are marked as having updated or obsoleted them. RFCs are specified in sufficient detail to allow conforming implementations to be written. You can learn all the details of neighbor discovery by reading RFC 4861. 

Associating an Elastic IP with the instance is how this is done. Be aware, that unless you are using VPC, that your elastic IP will disassociate if you stop the instance, and you will have to manually reassociate it when you restart the instance. 

The search query is not there. For privacy Google redirects pages in such a way that the query is not passed on to web sites. 

Port 433 is not open on the remote host, which is why you got error 110 (Connection refused). Try using port 443 (the standard HTTPS port) instead. 

You tried to install MongoDB with the Ubuntu installation method, rather than the Debian installation method. Try again and this time use the correct installation method. 

The , and the entire location block, are probably combining to cause the problem. You can simplify this by getting rid of the block, and simplifying to: 

The first thing you're missing is a 404 error handler. WordPress Multisite pretty much requires this. 

Reading a very small file is much faster than reading a bunch of large files. This is generally true even if the CPU has to decompress it. 

Looks like you got a UTF-8 BOM at the beginning of your configuration file. Use a different editor (or editor setting in Notepad++) which doesn't add the BOM (and with which you can strip this one out).