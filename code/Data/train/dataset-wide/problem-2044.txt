The instructions for installing on OS X manually via currently use the non-SSL version as an example and do not actually link to the latest version (eg. 3.4.2 is included at the moment but 3.4.4 is the latest production release). I've raised DOCS-10176 to correct this information. In the interim, if you visit the MongoDB Download Center which is referenced directly above the step you can find links for of the available packages to download. The OS X downloads for MongoDB 3.0+ should all default to a version with SSL support, or explicitly highlight if a selected distribution does not include SSL encryption. 

MongoDB Ops Manager 3.4.0 (released November, 2016) includes monitoring for hardware metrics (cpu, disk usage, and I/O utilization) via the Automation agent. For more information, see: System and Disk Alerts. Prior versions of Ops Manager did not include hardware monitoring and required the use of an external tool () for CPU & disk metrics. If you are still using an older version of Ops Manager and have installed Munin-node for monitoring CPU & I/O stats you could look into using the Munin plugin to track free disk space. Munin supports sending alerts via email as well as through Nagios and external scripts. 

The production notes in the MongoDB manual are updated regularly based on issues and experiences reported by MongoDB users, so should be part of your preflight checklist for production O/S deployments or upgrades. 

I would suggest looking at the query performance in aggregate as there are should be other performance metrics that correlate. You should ideally track your MongoDB metrics over time using a monitoring system. For monitoring suggestions, see Monitoring for MongoDB in the manual. Asya's presentation on Diagnostics & Debugging also provides a good overview of performance troubleshooting tools and how they were used to solve some specific performance scenarios. 

Start . At this point all config servers should be online with the user information. Re-enable the balancer so normal balancing activity & chunk migration can resume. 

This is a more nuanced question as it depends on server resources as well as what those connections are doing and how/if your driver implements connection pooling. Ideally I would load test your actual application and determine how resource usage scales with increased user activity. If you are administering your own MongoDB deployment you should review the production notes and in particular make sure you have your ulimit settings increased appropriately to ensure you don't run out of file descriptors. If you're concerned about ease of scaling you could also consider a hosted database-as-a-service solution (for example, MongoDB Atlas). 

In order for data to be distributed across multiple shards, you need to have multiple chunks for a given sharded collection. A chunk is a contiguous range of shard key values representing approximately 64 megabytes of data by default. Chunk ranges are normally created automatically based on observation of the shard key values and size of documents being inserted through . When documents are added to a chunk range which is approaching the maximum chunk size, will trigger a chunk split which will try to create smaller logical chunks for balancing. Currently all data in your sharded collection is covered by a single chunk on (aka your server): 

If your shards are backed by replica sets you can make the balancer more aggressive by disabling , which reduces the write concern used for documents migrating between shards. By default, is which is equivalent to a write concern: each document move during chunk migration propagates to at least one secondary before the balancer proceeds with the next document. In MongoDB 3.0+, there is also an option to configure an explicit for the operation. For example, to disable from a shell: 

The "maintenance mode tasks" message is referring to a counter of successive calls to the command and (as at MongoDB 3.4) isn't associated with specific queued tasks. The command is used to keep a secondary in RECOVERING state while some maintenance work is done. A RECOVERING member remains online and potentially syncing, but is excluded from normal read operations (eg. using secondary read preferences with a driver). Each invocation of either increases the task counter (if ) or decreases it (if ). When the counter reaches 0 the member will transition from RECOVERING back into SECONDARY state assuming it is healthy. As at MongoDB 3.4, changes in maintenance mode are currently only noted in the MongoDB log. This command is generally only used internally by , but you can invoke it manually as well. Here's an annotated set of log lines and the associated shell commands showing the task counter changing: 

A snapshot is a full backup of your data captured at a specific interval. Restoring from a stored snapshot is the fastest option because minimal manipulation needs to be done by Ops Manager in order to provide the restore files. The (6 hours in your config) and (3 days in your config) are meant to provide more frequent restore points for recent data as compared to the archived daily/weekly/monthly snapshots. With your current configuration, restores would be possible to: stored snapshots taken on 6 hour intervals in the last 3 days, a point in time within the last 24 hours, or a daily/weekly/monthly snapshot. 

Replication handles recovery from failover in a properly configured deployment, but capacity and deployment planning are separate exercises. The defaults are generally chosen to be reasonable, so if you want to change them it is worth understanding possible implications. 

It depends what level of backup/restore granularity you want to offer your customers. Increasing snapshot frequency or retention will consume more server resources, but perhaps provide more comfort for your customers. Conversely, you could reduce the snapshot frequency or retention to save on resources. As a DBaaS provider I expect you would work standard backup & licensing costs into your service model and perhaps allow customers to vary backup options based on subscription plans or premiums (for example, as MongoDB Cloud Manager does). Note: MongoDB Ops Manager is not currently intended to be the base for a DBaaS offering and should only be used in production as part of a MongoDB Enterprise Advanced subscription (which includes commercial support for questions such as configuration, capacity planning, tuning, ...). 

This is the default provided in the service recipe for the Homebrew package. The official Linux packages generally use a convention of . 

As at MongoDB 3.4, no plans have been announced to make the In Memory Storage Engine available in the MongoDB Community Edition. I suspect this is likely to remain as an Enterprise-only feature for the foreseeable future given the more niche use cases for in-memory storage, however the definitive source will be the mongodb.com website and any future major release announcements. FYI, you can subscribe to driver/server release announcements via the mongodb-announce group. 

The database-level information was specific to the older MMAP storage engine and removed during the MongoDB 3.0 development cycle as part of the work toward collection-level locking and the new storage engine API. A more general performance indicator you can monitor is . This metric is available in MongoDB 3.2 as well as prior versions (2.2+, at least). Given there are multiple storage engines available in MongoDB 3.0, there are also more detailed per-engine metrics that might be of interest. For example, WiredTiger is the default storage engine as at MongoDB 3.2, and includes a comprehensive set of metrics in the section of output. 

This error message refers to processing of the gzip archive rather than the BSON data within. Your test with implies that the contents are likely OK and the error may be due to ignorable padding at the end of the archive (see: gzip complains with trailing garbage ignored). 

This limits your sharded collection to a maximum of 8 chunks (or the number of unique values in this collection). For effective sharding you need to choose a more appropriate shard key with higher cardinality. 

As at MongoDB 3.2, the only supported key for a sharded output collection for Map/Reduce is the field (non-hashed). There are known issues with Map/Reduce output to a sharded collection using a hashed shard index; the two features don't play well together yet and this isn't a supported combination. The documentation currently only suggests that can be used, however there could be an explicit note that hashed indexes are not supported yet. There's a relevant Jira which you can watch/upvote: SERVER-16605: Mapreduce into sharded collection with hashed index fails. 

To allow the command you need to grant either a built-in role that includes this privilege ( or in MongoDB 2.6) or create a user-defined role if you want a more narrow scope. For example, to create and grant a custom role to run serverStatus: 

One thing to note is that is a temporary change. By default the sync targets are automatically chosen by MongoDB, and will be reset if you restart instances or lose connectivity. There's a helpful open source tool called Edda which you can use to process MongoDB log files and visualise changes in replica set history. NOTE: The github repository has changed since that blog post was published; the current repo is 10gen-labs/edda. 

A configuration with three voting nodes in different data centres plus an additional non-voting delayed secondary is definitely supported. This is actually a reasonable configuration in terms of satisying core availability requirements using the distributed voting nodes with additional hidden/non-voting nodes for special use cases. A delayed secondary should be always configured as and priority 0, and may have 0 or 1 vote. However, there is one important caveat with your deployment: you have three data-bearing nodes but one of those is delayed. This will be problematic if you want to use any write concern higher than the usual default ( aka acknowledgement by the primary). In the event one of your non-delayed secondaries is unavailable, a higher write concern (eg or ) could still be satisfied but the writes would have to wait for acknowledgement from a second data-bearing node which would have to be the delayed secondary. A more robust configuration would be to replace the arbiter with a data-bearing secondary so that unavailability of any of the three voting nodes would still allow for acknowledgement of majority writes. 

Definitely! I've raised DOCS-10993 to improve this information in the MongoDB manual. I suspect there are also improvements to be made in the driver documentation so will review those as part of documenting the overall recommendation. 

WiredTiger's general strategy is to only compress pages on-disk where there is a storage benefit. The counter is incremented when compression was attempted but didn't result in an on-disk storage saving (so a page was stored uncompressed). Another metric for pages that are stored uncompressed is . In this case, the size of a page was considered too small to attempt compression. The commonly expected case is that pages can be stored compressed, which increments the metric.