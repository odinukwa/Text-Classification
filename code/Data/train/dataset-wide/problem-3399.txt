It might also be possible that there are external forces at work. Specifically something like Intel Node Manager which will adjust the power consumption of a host by adjusting processor P-states. To check if this is active see /proc/acpi/processor/*/info. See also $URL$ 

The number of machines you can get in a rack will usually be constrained by the power they consume (and the associated cooling) not the physical space. Too many people don't bother to check how much power their servers use then they get surprised when their rack is labelled 'full' when it doesn't look full. 

There might be processes in IOWait contributing to the higher load. One way of finding those is by running this command: 

It's possible that he setup sudo access for your account. Try . That will tell you if you have sudo access (and what commands you can run.) 

On the Dell R710 (and many other makes/models) you can monitor the power usage yourself with this command: 

I really like that you're trying to make this repeatable! My rule is this: Kickstart partitions the OS drive (if there are multiple) and installs enough to run the configuration management system of your choosing. Nothing more. Your config mgmt system takes care of adding the required packages. Start with and remove all packages except for your config mgmt system. This way it will install everything necessary for it to run, but nothing more (it may require some trial and error to get it perfect.) This way your 'repeatable step-by-step doc' is actually programmatic. 

It's up to you to write /usr/local/bin/apachebwmeter. Make sure to coalesce the domain names (www.example.com and example.com are the same user, you might have other variations depending on your environment.) Keep in mind that %B does NOT include HTTP headers, so it'll be approximate. -Dave 

Talk to your users. Is your wife OK with having the reduced availability in the case of a failure? Keep in mind that 'reduced availability' is both the server AND your time to recover it. Also keep in mind that any failure WILL happen at the worst possible time. Let's say that your server is the backend for your MythTV setup. Let's also say that your wife has come to rely on it (because it's been good to her so far.) How much cred will you lose long will you be in the doghouse when DWTS doesn't get recorded because the server failed? (Or worse, your wife is looking for a new job and she used her 'home' email address on that server that just failed.) On the other hand, if this is your own personal tinkering machine and you have no girlfriend or wife, then it doesn't matter so much and you can be as much of a cheapskate as you'd like. :) -Dave 

No. No matter how you setup the MX records, each email will only be delivered once (to the highest priority server that is available at the time of delivery.) 

This will move all of the files and subdirs from /mnt/disk2/home to /mnt/disk1/home. The result will be: 

You need to write your java app as a daemon. Even backgrounded processes can die when you close your terminal. 

I setup passive checks in nagios for various events. Then at the end of the event the passive check is sent to nagios (either via wrapper script or built into the event itself.) If the passive check hasn't been received in freshness_threshold seconds, it will run check_command locally. check_command is setup as a simple shell script which returns critical and the information of the service description. I don't have code examples handy, but if I could if interest is shown. EDIT ONE added code examples: This assumes that you have done the basic setup for NSCA and send_nsca (make sure password and encryption_method is the same in send_nsca.cfg on the client and nsca.cfg on the nagios server. Then start nsca daemon on the nagios server.) First we define a template that other passive checks can use. This goes into services.cfg. 

Email deliverability is a whole industry onto itself. When you start working with a company that helps you with these issues one of the first things they have you do is set yourself up with FBLs for all the major email providers. (Google doesn't participate in that one.) This can significantly improve your inbox percentage (assuming you act on the messages.) Speaking of, do you have a good bounce processing setup? 

Your maxclients setting is quite high. If you actually had 2000 clients hitting your site, what would you expect the RAM usage to be? Would that push you into swap? Try lowering your maxclients setting. 

What you are describing is a master master MySQL setup. It is well documented and supported. We've been using it for many years and we were not early adopters. The set ups that I've worked on have used the master/master active/passive setup, meaning that only one side takes writes at any given time. We setup a floating IP between the two to manage the write master. 

What's the difference between managing 10 servers and managing 1,000? Nothing, if you did it right. This is a job for configuration management. Look into Ansible/Cfengine/Chef/Puppet/etc. 

This sounds like a workable plan. I wouldn't use dns for the failure. I'd use something like LinuxHA or ucarp to manage a floating IP which will determine your writer DB. This is especially true if you have multiple clients using these DBs. 

The ganglia web frontend 2.0 was just announced at Velocity: $URL$ $URL$ That's a cleaner interface with some new features. 

You should review your find command, in particular this I'm not sure that is proper syntax. Are you redirecting a file named '20' into the find? 

There might be a utility, but I don't know what it is. You can just create a file at with contents similar to: 

If you've ever been on pager duty, you've probably needed to solve this problem. Maybe you should google 'pager duty'. Or you could setup your monitoring solution to follow dependencies. 

Boot from a rescue CD and mount the drive, then you can run your chown. I can't think of a way to do this without a reboot. 

Check the output of . That will tell you how much swap is active. It appears from the nagios info that you don't have any swap currently active. Use to activate it.