Any time the game is played, the host can choose a distribution that makes the contestant's advantage as close to 0 as desired (although not actually equal to 0). In your multi-round formulation, the host could, for example, behave as follows: in round $n$, choose $k$ uniformly at random from $\{1,2,\dots, n\}$ and declare $\{k, k+1\}$. If the revealed number is $1$ or $n+1$ the contestant can win for sure, and otherwise it's a 50-50 guess. So the contestant's optimal chance of winning in round $n$ (even knowing the host's strategy) is $\frac12+\frac1{2n}$. So indeed (a) no and (b) yes. 

You can certainly phrase this question more simply. Without loss of generality you can take $p_i\leq 1/2$, so that the most likely corner is at 0. Then you are looking for the distribution of $\sum_{i=1}^n X_i$ where $X_i$ are independent Bernoulli$(p_i)$. A simple way to calculate the probabilities you're after is recursively in $n$. Let $a_{r,m}$ be the probability that $\sum_{i=1}^m X_i=r$. Then $a_{0,0}=1$, and for $m\geq 1$ $a_{0,m}=(1-p_m)a_{0,m-1}$ $a_{r,m}=p_m a_{r-1,m-1} + (1-p_m) a_{r,m-1}$ for $1\leq r\leq m.$ This calculates the probabilities with order $n^2$ operations. Other things that might be relevant if you want to approximate rather than calculate the probabilites: approximation by a normal distribution of mean $\sum p_i$ and variance $\sum p_1(1-p_1)$ (if the mean is reasonably large) or by a Poisson distribution of mean $\sum p_i$ (if the mean is small and each of the $p_i$ is very small). Simulation could also give you a pretty decent answer. 

The condition on the form of $A$ is equivalent to having that $A(x)=1-G(x)$ where $G(x)=\sum_{k=1}^\infty p_k x^k$ is the probability generating function of a probability distribution on the positive integers. Here are three families of solutions: 

One can put all this together to get the full joint distribution for the ordering of $a$ and $b \mod p$ and $q$ (there are nine possibilities in total). Among other things you would obtain $P(a \mod p < b \mod p,~~a \mod q > b \mod q)=$ $\frac1g\frac12(1-\frac{g}{p})\frac12(1-\frac{g}{q}) ~+~ \frac12(1-\frac1g)\frac12(1-\frac{g}{p})\frac12(1+\frac{g}{q}) ~+~ \frac12(1-\frac1g)\frac12(1+\frac{g}{p})\frac12(1-\frac{g}{q})$ which of course you can simplify a little. In particular cases, eg where $g=1$ or where $g=p$, the expression simplifies a lot, of course. To get the conditional probability you originally asked for, divide through by $P(a\mod q > b\mod q)$ which is $\frac12(1-\frac1q)$. 

Let $B_t, t\geq 0$ be standard Brownian motion. Let $\big(\mathcal{G}_t, t\geq 0\big)$ be the natural filtration, defined by $\mathcal{G}_t=\sigma(B_s, 0\leq s\leq t)$. Define also a filtration $\big(\mathcal{F}_t, t\geq 0\big)$ by $\mathcal{F}_t=\bigcap_{\epsilon>0} \mathcal{G}_{t+\epsilon}$. 

Working with distance along the circumference (much easier to treat). Here's a nice way to get at least the asymptotics of the maximum. Another way to obtain the distribution of the interpoint distances is as follows. Let $W_1, W_2, \dots, W_n$ be i.i.d. exponential random variables with mean 1, and let $S_n$ be their sum. Then $(W_1, W_2,\dots, W_n)/S_n$ has the same distribution as the interpoint distances you describe. This is because, conditional on the sum $S_n=s$, the distribution of $W_1, W_2,\dots, W_n$ is uniform on the simplex $\{\sum w_i=s\}$. Let $M_n$ be the maximum of the $W_i$. Then we want to know about $M_n/S_n$. $M_n$ has mean $h_n=1+1/2+\dots+1/n$ and variance $1+1/2^2+\dots+1/n^2$. An easy way to get this is to note that the maximum of $n$ i.i.d. exponential mean 1 random variables has the same distribution as $E_1+E_2+\dots+E_n$ where the $E_i$ are independent exponentials and $E_i$ has mean $1/i$. (By the memoryless property, you can get the distribution of the maximum of $n$ i.i.d. exponential mean 1 random variables as follows: take the minimum, which is exponential with mean $1/n$, and then add it to the maximum of $n-1$ new i.i.d. exponentials with mean 1; the claim follows by induction). So $M_n$ behaves like $\log n+O(1)$ as $n$ grows. Meanwhile by the central limit theorem $S_n$ behaves like $n+O(\sqrt{n})$. So $M_n/S_n$ behaves like $\frac{\log n}{n}\left(1+O\left(\frac{1}{\sqrt{n}}\right)\right)$ as $n$ grows. In fact $\frac{n^{3/2}}{\log n}\left(\frac{M_n}{S_n}-\frac{\log n}{n}\right) \to {N}(0,1)$ in distribution as $n\to\infty$. 

I am not sure exactly what your question is. What do you mean by "the right filtration"? In each example, $\sigma(X_1,\dots, X_i)$ would do the job fine. Maybe you are looking for the smallest sigma-algebra in each case. But in fact the forms given in your examples are not the smallest possible: (1) for the case where you know the exact distribution, for each $i$ there is some threshold $c_i$ such that you take item $i$ iff it exceeds the threshold $c_i$. So you do not need to see all the values $X_i$; all you need is the information about whether each $X_i$ exceeds its threshold or not. (2) for the case where nothing is known about the distribution, famously you never take any of the early items; if $i<n/e$ you never take item $i$. So no information at all about the relative ranks of the first $n/e$ items is necessary. $\natural_i$ may as well be empty for $i<n/e$. Thereafter, you take any item which is the best so far. So you still don't need to know the exact ranks of the later items, just the information about whether they are the largest so far. So, this is not an "answer" but here is a formulation in the spirit of your examples, for the case where $X_i\sim N(\mu,\sigma^2)$ with unknown $\mu$ and $\sigma^2$. You can let $\natural_i$ be the sigma algebra generated by all events which depend on $X_1,\dots, X_i$ and which are invariant under the transformation $(X_1, \dots, X_i)\to(aX_1+b, \dots, aX_i+b)$ for all constants $a>0$ and $b$. Such transformations preserve both the $N(\mu, \sigma^2)$ family and the ordering of the random variables. 

Yes. To be precise about this you might have to specify more about what space you are working on, etc. For example, the number of customers in the queue is not Markov for a general service time distribution, so you have to be careful what you mean by "stationary distribution". You could look at the total amount of work in the queue, or alternatively at the number of customers in the queue along with the remaining service time of the customer currently in service. In any case, by coupling the arrival processes for different arrival rates, and using a coupling-from-the-past construction (i.e. Loynes construction) to define a stationary evolution of the queue, you can obtain convergence in distribution (indeed, convergence in total variation). Couple the Poisson arrival processes so that the arrival points in $N(\lambda_i)$ are included in those of $N(\lambda_j)$ for whenever $\lambda_i < \lambda_j$. Couple the service time processes so that a customer arriving at the same time in different systems has the same service time. Now consider the state of the queue at time 0. Fix some $\delta$ with $\lambda+\delta<\mathbb{E}[S_1]$. Let $\epsilon>0$. For $T$ large enough, with probability at least $1-\epsilon$ the queue with arrival rate $\lambda+\delta$ is empty at some point in $[-T, 0]$. From the coupling, all systems with arrival rate $\lambda_n <\lambda+\delta$ will also be empty whenever the $(\lambda+\delta)$-system is empty. Now, as $n\to\infty$ so that $\lambda_n\to\lambda$, the probability that the arrival process at rate $\lambda_n$ is identical to the arrival process at rate $\lambda$ on the interval $[-T,0]$ tends to 1. In particular, for $n$ large enough this even happens with probability at least $1-\epsilon$. But if two systems are empty at the same point in $[-T,0]$, and their arrival processes coincide on $[-T,0]$, then they are in the same state at time $0$. So we have that for $n$ large enough, the time-0 state in the $\lambda$ system and the time-$0$ state in the $\lambda_n$-system are the same with probability at least $1-2\epsilon$. So indeed the total variation distance between the time-0 state in the $\lambda_n$ system and the time-0 state in the $\lambda$ system tends to 0. But everything is stationary, so the time-0 state is just a sample from the stationary distribution. 

The crucial thing you need concerns exchageability properties of the Polya urn. The following two procedures give the same law: 

A Poisson approximation should be good here, for appropriate ranges of the parameters. Consider the events $A_{ij}=$"$i$ and $j$ are never put in the same class". Broadly speaking, if you have a large number of these events, and each individual one has low probability, and any pair of events are close to independent, and there are no anomalous higher-order dependencies (here in particular I am being completely vague) then you expect the total number of events that occur to be approximately Poisson (with mean given by the total expected number of events). Here $P(A_{ij})=\Big(\frac{(c-1)n}{cn-1}\Big)^y$ and the total number of events is $\left(\begin{smallmatrix} cn \\ 2 \end{smallmatrix}\right)$. If the first of those terms is small and the product is neither too small nor too large, then one would expect the distribution of the total number of events to occur to be approximately Poisson with mean $\left(\Big(\frac{(c-1)n}{cn-1}\Big)^y\left(\begin{smallmatrix} cn \\ 2 \end{smallmatrix}\right)\right)$ and hence the probability that none of them occur should be roughly $\exp\left(-\Big(\frac{(c-1)n}{cn-1}\Big)^y\left(\begin{smallmatrix} cn \\ 2 \end{smallmatrix}\right)\right)$. I tried a small simulation for c=3, n=25 and y=20, for which the approximation gives a probability of 0.33571. All students met each other on 33676 of 100000 trials, which is encouraging enough for the accuracy of the approximation in this case. (there are conditions on the dependency between the events which guarantee that this sort of approximation is (asymptotically) reliable. For example, if the events are all positively correlated in an appropriate sense. But that doesn't seem to apply here. So for the moment what is written above is merely heuristic. But maybe someone else will see how to be more precise). 

Assume $p_{i(i+1)}>0$ for all $i$. For any fixed $k$, you have $\mathbb{P}(\tau_1>n)\geq p_{12}p_{23}\dots p_{(k-1)k}p_{kk}^n$ for all $n$. So if $\sup_k p_{kk}=1$, then the exponential tail bound that you want for the return time can't hold. But you can still get exponential decay of the stationary probabilities by making $\limsup_{k\to\infty} \frac{p_{k(k+1)}}{p_{(k+1)k}}$ bounded away from $1$. (For a nearest-neighbour chain, $\frac{\pi_{k+1}}{\pi_k}=\frac{p_{k(k+1)}}{p_{(k+1)k}}$ by detailed balance.) For example try $p_{k(k+1)}=\frac{1}{3k}$, $p_{k(k-1)}=\frac{2}{3k}$, $p_{kk}=1-\frac{1}{k}$. (With, say, $p_{01}=1$.) 

To learn more about this, search for articles about Polya's urn mentioning "exchangeability" or "de Finetti's theorem". (In the simplest case $b=r=1$, $p$ has Uniform$[0,1]$ distribution.) Then to get the expected occurrence time of a particular word in the Polya's urn sequence, it's enough to understand the expected occurrence time in an i.i.d. sequence with probability $p$, and then average over $p$ from the Beta$(b,r)$ distribution. For example, given $p$, the first occurrence time of $B$ is geometric with parameter $p$, and so has mean $1/p$. Then $$ E(T_B)=\int_{p=0}^1 p^{-1} \frac{p^{b-1} (1-p)^{r-1}}{B(b,r)}dp =\frac{B(b-1,r)}{B(b,r)}=\frac{b+r-1}{b-1} $$ which agrees with your answer. For longer words, expected occurrence times in i.i.d. sequences can be obtained in various ways, for instance via martingales (if I remember right, there is a nice treatment of this in Williams' Probability with Martingales), or recursively by conditioning on early entries in the sequence. For example, given $p$, one gets that the expected occurrence time of $BB$ is $1/p + 1/p^2$. Then \begin{align*} E(T_{BB})&= \int_{p=0}^1 (p^{-1}+p^{-2}) \frac{p^{b-1} (1-p)^{r-1}}{B(b,r)}dp \\ &=\frac{B(b-1,r)}{B(b,r)}+\frac{B(b-2,r)}{B(b,r)} \\ &=\frac{b+r-1}{b-1}+\frac{(b+r-1)(b+r-2)}{(b-1)(b-2)} \end{align*} which agrees with your conjecture. 

EDIT: It was really remiss of me not to mention $URL$ by Patrik Ferrari and Herbert Spohn, where there are very nice results concerning scalings when the distance between the two directions varies with $n$. The case $\beta=\alpha+O(n^{-1/3})$ is the one where a very nice scaling picture should emerge; maybe $URL$ (Corwin and Quastel) on the "Airy sheet" is the beginning of this. 

Also, here's a heuristic argument to show that the probability behaves like $1/N$ as $N$ becomes large. The last ball to be chosen is chosen for the first time after roughly $N\log N$ steps. (It's the maximum of $N$ random variables which are geometric with mean $N$). By this stage, the number of times that any particular other ball has been selected is Poisson($\log N$). So the probability a particular other ball has been selected only once is roughly $\log N e^{-\log N}$ which is $\log N/N$. Hence the probability that no other ball has been selected only once is roughly $(1-\log N/N)^{N-1}$, which is $\exp[-\log N + O(\log N/N)] \sim 1/N$. Of course the previous paragraph ignores various dependencies, but as $N$ becomes large these will be negligible and no doubt one could formalise the argument if desired. 

As Lucia pointed out in a comment, by solving the hitting probability recursions for the Markov chain, you get that the distribution of the maximum is geometric; for $k=0,1,2,\dots$, $$ \mathbb{P}(M=k)=\left(\frac{p}{1-p}\right)^k\left(1-\frac{p}{1-p}\right), $$ or equivalently $$ \mathbb{P}(M\geq k)=\left(\frac{p}{1-p}\right)^k. $$ There's actually a simple intuition for why the answer must be geometric. The only way to reach site $k>0$ is by passing through sites $1,2,\dots,k-1$ on the way. So to hit $k$ starting from $0$, you first have to hit $1$ starting from $0$, then you have to hit $2$ starting from $1$, then $3$ starting from $2$, .... , and finally $k$ starting from $k-1$. Now use the Markov property (formally the strong Markov property) and the fact that the process is translation invariant (so that the probability of hitting $j+1$ starting from $j$ doesn't depend on $j$), to get that the probability of hitting $k$ from $0$ is just the $k$th power of the probability of hitting $1$ from $0$. 

Let $A$ be a function from $[0,1]$ to $[0,1]$. $A$ is an involution if $A(A(x))=x$ for all $x\in[0,1]$. 

You'll need some further assumption beyond just the bound on the maximum expected hitting time. for example, for $n$ even consider a graph on $n$ vertices arranged into $n/2$ pairs. If $u$ and $v$ are a pair of vertices, let the edge between them have weight $1$, otherwise let it have weight $n^{-2}$. now the probability of jumping from one vertex to the other in the same pair is roughly $1-1/n$. Otherwise, the walk chooses between all other vertices uniformly at random. so the process stays at some pair for time roughly exponential with mean $n$, before jumping to a new randomly chosen pair. this is essentially coupon collector. the maximum expected hitting time has order $n^2$ but the cover time has order $n^2 \log n$.