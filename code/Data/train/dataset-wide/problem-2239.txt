I'm answering this with hesitation as there isn't enough information in your description of the problem to be 100% sure this is the best advice. "Hangs or throws an exception" suggests the source of the issue isn't properly understood, so proceed with caution. The simplest solution to this is probably . determines whether SQL Server will rollback a transaction in the event of a run-time error. The default will rollback only the statement that caused an error, leaving any parent transaction open. The "gotcha" side-effect of the default setting is that a timeout can cause exactly the same problem, an open transaction that is the clients responsibility to handle and rollback. If the client doesn't try/catch/rollback, the transaction will remain open until attended to with (and I quote @gbn) the ultra-violence of . The oft quoted Erland Sommarskog's articles on error handling in SQL Server contain all the background and strategy you need for dealing with these scenarios and more. Edit (following comment): To identify open transactions, sp_whoisactive is probably the most feature complete. 

From what I remember about this database from your earlier questions, I'd suggest you look at these alternatives to your approach: 

You could also be running into good old fashioned parameter sniffing problems, for which there are myriad options to work around. WITH RECOMPILE might be an expensive option to specify with a query this large but it's worth investigating at both procedure and statement level. 

9 times out of 10 this is due to the database being in FULL recovery and no log backups being taken. See Paul Randall's article Running Out of Transaction Log Space. If you want to monitor the items you listed: 

The undo pass is required to revert any dirty pages from in-flight transactions that were flushed to disk by a checkpoint operation. A checkpoint writes ALL dirty pages from the buffer cache to disk, regardless of whether the transaction that generated them has been committed or not. Checkpoints and the Active Portion of the Log covers the gory details. Restore WITH STANDBY allows you to bring the database up in a read-only state between restores (as your quote mentions). For the database to be in a transactionally consistent state, the undo pass has to be run. To continue further restores (which is the purpose of standby), the pages that were modified by UNDO will need to be re-applied, hence the need to store them in the standby file. 

For the Compellent, storage is divided into 3 tiers with Tier1 being the fastest (SSD or RAID 10) through to sluggish Tier3 which comprises 7k SAS disks. I've not used one of these in a production environment as yet but do have access to one, which I hope to be able to run some tests against. I confess to being both intrigued and fearful of the auto-tier features, which on paper sound wonderful but in production may be problematic. The first example that occurred as potentially scuppering the auto-tier mechanism was database backups. There you have a repeating high write target which may fool the tiering mechanism into moving the blocks to a high performance pool, potentially pushing out data that should be there. To the original question: 

I'm left wondering why your staging table contains 12 million rows when you mention the daily increment being 1-2k. If you've previously processed the data why not truncate the table before loading the increment for processing. 

If recovery is taking a long time to complete and there doesn't appear to be any activity you may need to restore from backups. If you're feeling brave you should start reading everything you can find about repairing/recovering suspect databases. More information on what happened before the database ended up in this state would make for better answers e.g. what actions you carried out, what errors are in the SQL error log etc. 

I bookmarked Phil Factor's blog post Normalisation and 'Anima notitia copia' today as it neatly summarises the case for and against normalising certain types of data. Run the following query on a SQL instance and see if you agree. 

The Format function argument is a .net format string, where is minutes and is months. You specified minutes, so minutes is what you got. There are example format strings at Standard Date and Time Format Strings. As an alternative to you can also use just which is the Short Date Format specifier (detailed on the linked page). 

I've not tried fn_dblog on Express but if it is available the following will give you delete operations: 

When restoring a sequence of backups (differential & logs) you specify . This leaves the database in a state that will accept further restores, rather than recover it ready for use. 

Whatever is, it needs to feature in the index. Again, difficult to tell due to the obfuscation but probably as the sole indexed value, everything else . Can't deduce from the query why your doing it this way but the repeated joining to doesn't make much sense as it stands. Back to my earlier point, its all guesswork without the full picture. Original: The seek is on , you've included this rather than make it part of the index. Can't be sure this is optimal for the entire query without seeing the query and execution plan but to turn this particular lookup into the non-clustered index seek you're looking for: 

Short answer I'm afraid, no. Best bet is to do a partial restore to a new database then copy the tables and data to your target with: 

So you just need to execute the collection of schema scripts to recreate your database? You could execute them one by one or create a batch file and use SQLCMD to execute all script files in directory 

For SQL Server, you would SET IMPLICIT_TRANSACTIONS ON. In implicit transaction mode, any DML ( etc) or DDL ( etc) will start a transaction and you must explicitly or . Alternatively an explicit transaction can be started with . 

This approach requires accessing every record in BillDetails. A better approach would be to index BillDate and query as follows: