Ubuntu default guided partition layout, non LVM, completely unmodified. Confirmation of partitions, including ESP is present. 

I have a failed drive in a FreeNas server hosted at OVH. I need to get the drive swapped, but i'm extremely conscious of them pulling the wrong drive. FreeNas isn't reporting any serial numbers in it's GUI. I have done the below so far, I don't know how to either get the drive serial, or better yet blink the LED? 

Also, should i look to make the conntrack settings on the DNS server more aggressive to close connections faster, these are the current settings on the DNS server: 

So after a lot of testing and diagnosing I've found the solution, but I still don't understand why. If anyone can explain this to me I i'll award the answer. The problem is was with the /boot directory. As it's installing under UEFI, the ESP gets setup under /boot/efi, in Grub this worked fine, i could read this ok. However, some of the /boot directory, which contains the required kernel was not readable. When i attempted to manually load the kernel and boot from grub, the kernel loaded fine, but when attempting to load the initrd, i received error: error: attempt to read or write outside of disk hd0 From what I understand this is because /boot is on the main 4.5TB partition, the files can end up anywhere on the drive, and in this case, and my many test cases before this, the files in /boot are too far up the drive for Grub to read. Creating a dedicated /boot partition before the ESP partition has resolved this. This is the same issue as documented here: (according to $URL$ What I don't understand however, is that from my understanding, under UEFI the full 4.5TB should be readable. Ubuntu should boot fine under it's default partition layout with only and ESP partition? This is confirmed as I've managed to install Ubuntu without a /boot partition on 3 other identical hardware and bios settings servers. It's just this one server which couldn't read inside some of /boot.a I've ensured the disk was booting under UEFI in the BIOS. 

Add an A record for "autodiscover.domain.com" to your Exchange box public IP. Obtain and install an SSL certifcate for CN=autodiscover.domain.com. This could be a free SSL certificate as we only require one host name to be secured here. If you want to be fancy and allow for "mail.contoso.com" to work, then be prepared to pay Â£60/year to GoDaddy for a UC/SAN SSL certificate. Ensure $URL$ shows some xml after logging in, and pressing F5 shows a new time stamp each time you refresh the page. Ensure $URL$ loads as https. Both should have no certificate warnings. Check the URLs for autodiscover using these Exchange Shall commandlets:- Get-OABVirtualDirectory Get-ClientAccessServer Get-WebServicesVirtualDirectory Get-PowerShellVirtualDirectory Get-ECPVirtualDirectory Get-OWAVirtualDirectory Get-ActiveSyncVirtualDirectory Get-AutodiscoverVirtualDirectory 

To answer your direct question: Nothing would break if you don't configure SRV record on public domain. In fact, I don't even do it because even though the docs claim Outlook 2007 supports this, in my experience it plain doesn't. Only Outlook 2010 does. It's just Yet another strategy to get users to pay for the latest product. What I do, is this: 

I believe this is simply a case of a bad idea using RAID 5 in this situation, but I would like to see if I have missed something else. Hardware: 

To streamline the process we would like to install the driver while the machine is online. Unfortunately dism is requiring the disk to be offline, returning the error "This command can only be used with an offline image". We have tried pnputil to install the driver but this does not work, we believe this is because pnputil is for plugged in hardware with no driver currently. Is there a way to install a driver into an online image? 

Attempting to install Ubuntu 16.04 Server onto a newly initialised disk. Using UEFI, and a 4.5TB / partition. After installing without issue, on rebooting, the server will not get past Grub. Hardware: 

I have set up Exchange 2010 and Outlook 2007 in a test lab. Everything works (with SSL, OOO, etc). But there is one niggle: During autodiscover from an external machine on the Internet, the end user inputs their name, email, and password twice, clicks next.. Then, on my setup I get a windows login prompt, with the username as being "FredSmith@contoso.com" already filled in. Entering the password fails. Username set to "FredSmith" also fails... but "contoso\fredsmith" works perfectly! The IIS log shows error 401 for the first hit, which was contoso.co.uk\fredsmith. So Outlook is taking my email domain name and using that as a logon domain, which fails because only contoso.local works. Now, my philosophy is that the end user shouldn't have to ever know or type in the domain name. This is particularly because this is an SBS 2011 test site. So to recap, end user has to complete autoconfiguration wizard by effectively logging in twice, with different credentials. The whole idea, surely, is that any end user can set up outlook. Let's imagine 20% of end users don't even know the difference between a forward slash and a back slash when it comes to that username. 

I have a Windows Server 2012 running on Openstack which is hanging at the logo screen on boot. I'm trying to get the boot menu up (F8) however due to the speed the instance boots I can't get the F8 in quick enough. As the Spice client disconnects every time the instance is hard rebooted, by the time i refresh the console, it's already back at the hanging screen. Is there a way to boot it directly into the boot menu / safe mode, or delay the post so i can get the console up and start tapping F8? 

I've followed the Mitaka setup guide for my first OpenStack cloud. This all went ok (2nd time around!), however i'm now having issues with networking. The instance launches ok, and it's assigned an IP via DHCP, but it won't ping. I don't know if my network is setup right, so i've provided the appropriate outputs below. I setup Mitaka with option 2 - 'self service networks', but for simplicity i've only created a Flat DHCP 'provider' network just to get a feel for things. On my test setup I only have 1 working public IP address, x.x.x.111 

The certificate you purchased using using a root trust that has been introduced into the world since 2002. This is very normal. It just means an ancient computer running Windows 98 and IE3 will get a certificate warning, that's all. The real warning should tell you: Test this out on Android,iPhone,WindowsMobile.. because there is a slightly greater chance some of these handhelds won't like your certificate. 

You should now either disable port 443 on your public web server (unlikely..), or change the A record for "domain.com" to your Exchange box public IP. This is because the first url outlook tries is $URL$ Next: 

Add an A record for "autodiscover.domain.com" to your Exchange box public IP. Obtain and install an SSL certifcate for CN or SAN for autodiscover.domain.com. Ensure $URL$ shows some xml after logging in, and pressing F5 shows a new time stamp each time you refresh the page. Check the URLs for autodiscover using these Exchange Shall commandlets:- Get-OABVirtualDirectory Get-ClientAccessServer Get-WebServicesVirtualDirectory Get-PowerShellVirtualDirectory Get-ECPVirtualDirectory Get-OWAVirtualDirectory Get-ActiveSyncVirtualDirectory Get-AutodiscoverVirtualDirectory 

I'm looking at moving our XenServer cluster (150 VMs) to OpenStack on KVM. After extensive reading it looks like virt-v2v will do this. However I'm confused about it's usage. I was going to copy the VHD file and then run virt-v2v on this, then import into Glance, and start an instance. However it appears this isn't the process virt-v2v uses. Could someone explain the overall process, how to use virt-v2v or any other tool(s) that I will need to convert VMs from XenServer to KVM and import into OpenStack. The two 'clouds' are separate hardware, over the internet - so i would like to avoid shared storage between them if that's possible, however if it makes it too complex we can sling up a VPN between. 

Symptoms When high disk write access occurs (when a vm is being spawned for instance), the OS and subsequently virtual machines experience high i/o wait, to the point of becoming unresponsive, the OS becomes very laggy. Normal i/o is about 10MB/sec read or write (according to iotop). When simulating using dd: 

I am renting a Windows 2008 R2 VPS (Hyper-V). It has 1 network adapter which has a public IP statically assigned. I wish to allow incoming VPN connections, so that I can access SMB for a central file server. I have setup RRAS, and VPN'ing in works fine -- but when I uncheck the 'Use default gateway' on the client-side (as I do not wish for clients public internet traffic to flow through the VPN server), name resolution stops working.. so \servername\sharename stops working. Likewise, ping servername also says could not find the host. \10.0.0.50\sharename works still, but this is not ideal. I've tried setting the RRAS properties to give out addresses from DHCP and statically assigned pool. I tried to install DHCP server but it appears to only be able to bind to the physical adapter. Can anyone help, please? Apologies about my low accept-ratio, I will be wrapping up all my other questions at some point. 

The above triggered the mentioned symptoms almost immediately. RAID shows healthy, however I'm unsure if I have the optimum caching configuration enabled: 

I have some public facing DNS servers which came under attack from what appears to be a UDP based DDOS to port 53. The servers are CentOS 7 virtualised running on OpenStack Hypervisors (Ubuntu 16.04). The attack filled the conntrack table of the hypervisor causing connectivity disruption to all other neighbours also. I'm looking to tweak the conntrack settings, if possible, that an attack on one instance has less chance of affecting others, perhaps increasing net.netfilter.nf_conntrack_max to an extreme value? This was the specific error: These are the current conntrack settings on the hypervisor: 

In the end i couldn't find a way to map the mfiutil to a device. I'm sure there is a way but it escapes me. In the end i rebooted into the raid controller bios and luckily as the drive was completely dead it was showing in the controller. I think if i had studied megacli i could have realised this without rebooting, which would have been better. But in the end mfisyspd5 actually mapped to E1:S9, serial 1EJ49HWH But overall, if you want to run ZFS, don't use a RAID controller even in passthrough, just get a HBA. Will save you hassle in the long run. 

How can I trick SBS 2011 into allowing me to assign a UPN alias so users can logon as user@domain.com See $URL$ But on SBS I only have a 'General', 'Trusts', and 'Managed by' tab.. this is SBS 2011. 

If yes: You should now either disable port 443 on your public web server, or change the A record for "domain.com" to your Exchange box public IP. If no: You should now ensure the same certificate is used on your IIS box and covers autodiscover.domain.com without error. } Now: 

I have a normal Apache/MySQL serving a webapp that companies and users of those companies log in to. So everyone from all the companies are in the same Users table. Whilst my code does try to keep the companies isolated in PHP, such as , I do see the chances of an SQL injection to get private data from other customers as being quite likely as the code base is quite vast. We are reviewing the code, but at this rate we will be completed in years. In the meantime, I was looking at Docker to hopefully give each of my customers their own container with Apache server (with a tiny buffer, and 2 php workers), and MySQL with a measly 32 or 64 MB of buffer pool. This container would then allow just 1 TCP connection for HTTPS, which would be reverse-proxied based on hostname (eg customer-a.mycompany.com). No files get written to disk. And the PHP source code would be kept up-to-date using git, so perhaps we could even put selected customers on our Beta version, for example. I have been advised that Docker is not designed for the above scenario, and would not provide me with the security I seek. I get that a hacker could inject code into 1 container and would affect that one customer. But would Docker not prevent discovery of the virtual hosts used for other customers, reducing the chances slightly that a hacker would have affect on most of my customers. Just one we could financially handle, but not 12. Aside from security, I was excited about the possibility of the MySQL buffer pool being dedicated to each company was going to make the webapp generally faster because when Customer A does an insane SQL report, I am guessing the buffer pool gets overwritten with the insane report, so when Customer B does a simple query next, the data has to be fetched from disk all over again. In the future, I would love to see these "containers" on a GUI, and be able to drag-and-drop them onto another server (faster, with less containers).. eg to offer "faster" speeds to certain power customers. Does any of the above dreaming fit into reality for 2017? What platform/tool do you think I should consider?