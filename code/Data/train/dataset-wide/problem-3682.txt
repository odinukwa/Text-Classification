Long story short, there is no supported install of Windows Server onto CF or USB devices. You need to use regular old hard drives. 

I then found a few records that I expected to get delete with timstamps from a few years ago and ensured that that the and that time stamp was actually set: 

If what you want to test is your detection logic, I'm not sure exactly how we could accomplish this. The Model property is Read-Only so I cannot think of a way to modify it using PowerShell script within your Task Sequence prior to the conditional install of device drivers. 

If one of your laptop goes missing, and you either A) have confidential data on the machine, or B) can't confirm that there is NO confidential data on the laptop then you need some kind of encryption scheme. Of course, you (and your organization) need to decide what criteria you want to use to consider what is confidential and what isn't. I recommend you don't neglect this step; you don't want to go to all this work if it is not necessary, nor do you want to elevate your organization's equivalent of the office's cookie recipe to a level of secrecy that demands AES-256. If you've already been through the process, then you're good to go. 

My experience is mostly with Citrix's XenServer but the differences between their implementation and just building your own Xen host should be negligible. 

If I attempt the same process on our SCCM server (SCCMReportingPoint.ad.contoso.com), the Configuration Manager Administrative Console correctly starts ReportBuilder and I get the Create Report Wizard. I have done the following in an attempt to resolve this issue without success: 

EDIT: I checked the Software Update Group for this month's Software Updates and there is a deadline set for 03/10/15 at 20:53 but the deadline behavior for activities to be performed outside the maintenance window is disabled for both Software updates installation and System restart. As for the current time on the box, like it really does look OK but I'm just checking Date and Time in the Control Panel: 

Yes it should be technically possible. Remember, there's nothing magical about a SSL certificate. As far as Apache, OpenVPN or any other application is concerned your SSL certificate is just a bunch of pseudo-random bits. Whether or not this is a good idea depends upon your goals. More details would be helpful. It also occurs to me that this might violate the terms under which you purchased your signed SSL certificate from your 3rd party. Again, more details would be helpful. 

This is great example of how PowerShell can be far superior to some of the native batch scripting tools. You can use the New-PSDrive cmdlet to map a drive on Server B. The drive will only persist for this particular PowerShell session so if you execute a new script and/or PowerShell session the drive mapping will no longer be cached. You could also just remove the drive and recreated with Remove-PSDrive depending on your use case. 

We have a remote location that is connected to our central infrastructure via a 2.4 Mbps TLS connection. There is a Windows Server 2003 SP1 domain controller at that location that we would like to virtualize on a Hyper-V virtual host running on Server 2008 R2 Core (also at the same remote location) Both the source computer (the domain controller) and the virtual host are located on the same LAN which is capable of 1Gbit speeds. SCVMM lives on a virtual machine back at main office (at the other end of the TLS link). I would like to do an online P2V of the domain controller to the virtual host using the SCVMM installation we already have. This technet article seems to indicate that only the "meta-data" (such as machine.xml) required for the VMM host (the Hyper-V virtual host) is sent back and forth between the SCVMM machine (the VMM server). Our remote connection can obviously not sustain the traffic required for imaging the physical machine during the P2V process - I want all of the imaging traffic to stay on the remote location's LAN. 

Can anyone shed some light on (A) why this broke, I don't think you can just execute modprobe from . You need likely need to use the directive: From the interfaces(5) manpage: 

You will still be able to do port-based 802.1x authentication but only for the entire hub. As far as the 802.1x authenticator is concerned it is just able to allow or disallow (or assign to different VLANs) that one port that the hub is attached to. Imagine what will happen with a client authenticates this port to a trusted VLAN but then another client authenticates this port to an untrusted VLAN. From the perspective of the authenticator you will not be able to only the port that your hub is attached too (and hence everything that is attached to it). If you require port-based authentication on a switch or need to authenticate a device that doesn't support 802.1x you can rely on MAC Authentication Bypass, which is essentially just whitelisting MAC addresses or port as required. To really take advantage of 802.1x you need a switching infrastructure that fully supports 802.1x (luckily it's pretty common on mid-range enterprise grade switches). 

I know you have not been able to reproduce the problem if you remove your router from your network path but have you verified that this problem exists with other clients? I would ensure that you can reproduce this problem on more than one client (try your ping test on another machine) before you continue with the assumption that the router is underlying problem (although it likely is). DD-WRT has never really impressed me as a very stable platform. It's hard to tell whether that's a result of the COTS-nature of the hardware it gets run on, DD-WRT itself or a combination of the two. Regardless a quick google and a stroll through the forums finds lots of "connection dropped" issues. Packets are often dropped because there is not enough memory to keep the state of all TCP connections that are made by the clients. This is a common problem for COTS "routers" regardless of firmware (DD-WRT has a wiki about a related issue here). Try accessing your router via ssh (or telnet) and looking through for anything from ip_conntrack (the kernel module that is used to keep track of connection state). You'll likely find something like this: . I see you have already adjusted the setting to the maximum of 4096 but try verifying that using the command line (). If your WRT150N has a decent amount of memory (e.g., 32MB or greater) you can manually set to a number larger that 4096 (see here). How many clients do you have behind your WRT150N? Are you using an P2P protocols? It has unfortunately been my experience that Linksys hardware and DD-WRT kind of suck especially in situations where there is any real kind of network traffic. It might be time to graduate to more robust solution. 

You are probably seeing a more restrictive interpretation of (or ?) in Windows Server 2008 R2 which is then calculated against your less restrictive NTFS permissions. I like @Helge Klein could not find any documentation on what constitutes actual atomic permissions of the Share "meta-permission". As you have discovered, this process is complex and prone to errors. Make your life simple and use NTFS permissions exclusively for your access control. Set your Share permissions so they are wide open ( - ) and then set the NTFS permissions to provide you desired access control - something like the following is typical: 

This method uses the apachectl control program. In almost every occasion I would recommend the use of apachectl the to maniuplate the apache daemon. You have finer grained control over how the process restarts (see graceful vs. restart), configuration validation options and a way to get status information. The main difference between using the init scripts to restart apache and apachectl is that apachectl is specifically designed to allow administators to control apache whereas the init scripts are designed as a generalized way for Unixes to start processes after the kernel and init daeamon have been loaded. EDIT: Unfortunantly, I have no idea how to address your issue with Rails or Phusion Passenger. Try looking through your Apache logs for clues. 

The directive in your interfaces file is still the preferred way to configure these settings. You should make sure that an autoconfiguration tool such NetworkManager did not accidentally get installed during your upgrade process or that you are not using init scripts to add or remove extra static routes. 

First off. Sometimes when people talk about libraries they are talking about .deb packages that provide libraries to other packages. We'll deal with that case first. The other context you hear the term library used in is the traditional shared object sense. We'll deal with that second. will return a list of packages that are dependencies for . Packages are not necessarily congruous with libraries (i.e., libraries in the sense of linkable files), but in Debian and Ubuntu libraries are generally packaged as . If you do a you can find which package containing which libraries is installed. 

As you can see our good friend has quite a few libraries linked against it. handles file attributes if I remember correctly. Doing an against it shows that it was installed by two packages and (one for 32bit and one for 64bit). And on my system it looks like the package (at version 1:2.4.44-2) installed the libattr.so shared object file, which upon further investigation is at version 1.1.0. 

However it is preferable to add the name of the module to so it is loaded at boot time. (Notice that if the command command fails to execute your interface won't come up - even if the rest of the configuration is perfectly fine). and (B) how can I re-enable the ip_conntrack_ftp module? From the modules(5) manpage: 

You never want to be in a position where you are forced to learn on production systems. Learning involves making mistakes, and mistakes on production systems cost your company money. They may not care about anything else (which is clearly indicated by the very fact you are in this position to begin with) but they will care about this. 

You might also need to have local_enabled set to YES, depending on how you are setting up your accounts. 

Microsoft FrontPage has long been depreciated along with its successor Microsoft Expressions. Their functionality has been rolled into the Visual Studio product line starting with Visual Studio 2012. In my experience these FrontPage-managed websites are almost always internal facing "Intranet" web pages without significant complexity and FrontPage is there just to provide an editing interface for the administrative staff to update content. If this is your use case, consider moving to a Wiki, Content Management System or shudder SharePoint. 

I just spent the better part of this morning on a support call with a vendor where we eventually resolved our issue by manually adding the service account their application was using to the following policies which were being set by a Domain GPO: 

Acquire a wireless card that supports "Monitoring" or "rfmon" mode and use it in conjunction with Wireshark to view the 802.11 headers in your network traffic. This is wildly chipset, operating system and driver dependent but Wireshark has some nice documentation to point you in the right direction. What you are after is the actual 802.11 management headers and not just "translated" Ethernet layer-2 information (again, see the Wireshark documentation). It sounds like your network is primarily over 802.11 so the time spent getting this figured out will probably be worth it later - you'll need to look at the actual 802.11 headers eventually for troubleshooting purposes. Confirm that this actually is an issue with your access points (it probably is). Start Wireshark using '802.11' as your link layer type and then authenicate against an access point and purposely mistype the password. See what happens. You might need to also need to see what happens between the Radius server and the access point side of things as well. If you're having trouble interpreting the resulting data, you can always save it as a pcap and provide it here. You probably just want to confirm that it is an issue with the radius client before you spend a bunch of money on access points. Once you've confirmed that it is an issue with the access points, go purchase some nice "enterprise-y" ones. We use D-Link DWL3200s which are a pretty middle of the road access point as far as access points go. My only real complaint is that their command line interface sucks but on the other hand they are only about $300 each so I can't really expect too much. 

It is a terrible idea to have your SQL server directly exposed to unwashed multitudes of the Internet. Instead you want to invest it what's called a "3-Tier" architecture. 

VLAN tagging is supported in Linux using the 802.1Q module. NIC bonding is supported in Linux using the bonding module. Every major distribution should include a kernel new enough that it supports these features. 

You should push back against this decision. There is really no excuse for purchasing a new application that is tied to a depreciated product that no longer receives security updates. You're brand new application already comes with substantial technical debt instead of eliminating existing debt. Please do yourself and your organization a favor and lobby hard for another application or push the company for an update of the application that works with Java 7. If you must use this application you can find Java 6 SE in the Java Archive. Be FOREWARNED, Oracle will not provide any updates for Java 6 unless you pay for extended support. If your organization goes forward with this purchase, make sure you include the cost of Oracle's extended support in the capital item for the application as it is a dependency that needs security patching and support along with the application it supports. 

Zenmap (the GUI version the famous namp) has a nice topology mapping feature. Although, it will only generate a one time map and will not automatically update or give you threshold alerting. It will however, with five minutes worth of time, give you a quick and dirty topology map. 

Notice that the SCCM deployed version came from the local CCM cache. You can add these to your Detection Logic or Requirements as appropriate to correct detect this condition. 

Now that best practices recommends that we keep our targeting logic bundled with the Application what we need to do is create an appropriate WQL query Global Condition and then we can evaluate it using the Application's Requirements. Let's start with the WQL query. I used Scriptomatic to just dump everything in the WMI Class which is part of the namespace. I'm reasonably sure that SMS_InstalledSoftware is the best place to run queries against when trying to evaluate whether or not something is installed as Win32_Product is only for Windows Installer installed software. I find the following Firefox related object: 

Replace the word with and you'll see the security benefits because you have created a single point where all inter-VLAN traffic must transift without requiring physically discrete infrastructure. You can then filter, log, permit and deny to your heart's content. Compare this to a situation where you have one switch with your router, computer and server all connected to. Let's assume that your server and home computer are on separate IP subnets. Nothing would stop me from reassigning the address of your home computer to be on the same subnet as your server (or vice versa) and then sending malicous traffic to it. If we had VLANs configured as per your question I could still do this (presuming I already new the IP subnetting information) but I couldn't directly reach your server without transiting the router first (and its likely the router would not route for that traffic anyway). Herein lies the primary benefit of VLANs: The ability to treat one physical infrastructure like multiple "discrete" physical infrastructures. Instead of requiring physical separation you can achieve something similar by using seperate VLANs. Another way to put this is, you can take a single Layer-2 broadcast domain and treat it like multiple broadcast domains (which each VLAN "mapped" to a corresponding IP subnet).