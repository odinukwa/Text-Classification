I would like to create a trigger before inserts and updates to ignore any value given by these statements and instead use the next value in sequence - as if no value was defined in that statement. I don't want to use since this throws an error. I would rather ignore any value given and use the next possible value. What I am looking for is a way to identify the sequence used for this identity ans use in a before-insert/update trigger. In this article a SQL statement is given to get the name of the sequence that is used for the identity which I maybe could use within the trigger (???) but the statement doesn't work for me: 

But this goes well only for a short time. After couple of minutes it starts to throw ORA-01110 when I connect to the first instance. The data files are not missing. They are in the location in which Oracle reports them missing. 

The said file does in fact not exist. But such a file does also not exist for the other database. The directory contains a though for both databases. As far as I know oracle uses this pattern when starting the database. If cant find said file it goes on to use . I have created a INITDBNAME.ORA file and added the content: 

If the statement would work I would have the sequene name as a string but I guess I would need the object itself to request the nextval. Is a trigger the right way to go or should I look for something else? For Oracle 11g I have implemented this functionality using trigger and sequences. The trigger looked like this for inserts: 

error. Also confusing was that if the pfile was inside the Oracle Directory the command failed saying it cannot access the file. EDIT After I have deleted both instances, rebooted and created both instances again I can currently access them both. When I try to login to the first instance I get the following error: 

Version - SQL Server 2012 SP1 CU3. I have been using sys.dm_os_volume_stats for a while and it was working. Recently I have not been able to return any result from this DMF on one SQL Server 2012 instance. e.g. 

You could look into changing the database to contained database. Contained database user are authenticated by the database, not at instance level through login. It makes moving database to different instance simpler. If not, you could backup the login information using sp_help_revlogin scripts provided at this Microsoft support KB. And execute the output script on the new instance. 

Second test, rollback previous transaction, set READ_COMMITTED_SNAPSHOT ON but leave ALLOW_SNAPSHOT_ISOLATION OFF. 

When an index rebuilding operation is performed, it drops the index, recreates the index, and updates its statistic. If I read the question correctly, your question of dropping indexes, recreating then update the statistics is pretty much the same as simply rebuilding the index. Please note that index reorganization operation does not update the statistics. 

I don't know how your report table is currently being designed, but I would think the report table list all the rows from the query results (with predefined number of columns, in your case, 2 columns). As long as the query is correctly developed to return all required rows e.g. all enrolled courses, the report table designing part should be fairly straight forward. 

@matt you might want to adjust the tablix or row KeepTogether property. This affects how it tries to have all rows/table on the same page. 

Run query 1, and then query 2. DMV shows exclusive lock incurred by query 1. Query 2 appears to be waiting for query 1 to complete. Turning ALLOW_SNAPSHOT_ISOLATION ON doesn't appear to enable READ COMMITTED row versioning. Adding to both query 1 and query 2. Run query 1 and then query 2. While DMV shows query 1 incur exclusive lock, query 2 return details with 'Original'. Snapshot isolation appears to be in place. Observation from the test shows that itself enable/disable the READ COMMITTED row versioning regardless of setting, and vice versa. 

I'm building an inventory database that tracks computer equipment and other hardware devices. At some point in any device's life it is retired and gets archived. After it becomes archived it needs to be tracked as it is removed from service and properly disposed. I originally designed the archiving mechanism using an exact replica of the active database that would receive its data using a trigger on delete from the active database. The archive database includes replicas of all the related tables because as certain foreign related records are no longer pertinent, they should not be accessible to users to use with new devices, but are required for referential integrity and querying with the archive tables. Keep in mind that the concept of archive here is not just to keep a history or a log. The archive is a part of the business process, and users will need to query and update devices that are both active and archived. The ERD below uses the table as an example where all entries and updates are copied to the table. When users should no longer be able to enter inventory records of a certain device type, it is deleted from the table, but remains in the archive table. This pattern is used on all tables to ensure the archives refer to valid data, hence the replica of all tables. Active Table Example (Other related tables omitted) 

Problem I'm trying to figure out how I would query the database if I don't know if a device is active or archived? For example, if a user has a serial number and wants to find out information about the device, and they are unaware of whether it has been archived. Option 1: Create a view based on a union all? Option 2: Query the active database and then query the archive if the first query returns nothing? The saga continues... An associate suggested that I eliminate the archive database and use a soft delete scheme. I built a model using this idea, and then started running into a host of other problems. Here are the same tables using the soft delete scheme. Soft Delete Example 

The delete statement without the where clause delete all rows in the table without change of table structure. If the delete statement is within a transaction, then it can be rollback before the transaction is committed. If the delete transaction has been committed, the deleted transaction can't be rollback. Unless use of third party tool or restore the log backup to previous point if available. 

I have no issues with the same code on other SQL Server 2012 or SQL Server 2008 R2 instances. Does anyone know the reason behind and has fixed this problem? 

Most of the forum and example online always suggest to have both and set to ON whenever someone is asking snapshot, row versioning or similar question. I guess the word SNAPSHOT in both setting get a little confusing. I thought that, in order for database engine to use row versioning instead of locks for READ_COMMITTED default behavior, the database is set to ON regardless of what setting. The setting is set to ON only to allow snapshot isolation when starting a transaction (e.g. SET TRANSACTION ISOLATION LEVEL SNAPSHOT) regardless of setting. The only reason to have these two settings set to ON is when it needs to have READ COMMITTED row versioning AND snapshot isolation. My question is, is my understanding incorrect in some way? And that these two setting have to be always set to ON together (especially for READ COMMITTED row versioning)? 

I am trying to determine the duration of backup restore. I executed a few restore command to restore some backups located on network share drive. Here is command and summary, 

return ServerB under name and network_name column with id 0 So question is, am I missing some steps over changing servername or is this a bug? Version - SQL Server 2012 SP1 

After some tests, in order for the DMF to work, it appears that the service account needs to have at least READ access to the root volume of where the database files are located at, in addition to the SQL Server login security VIEW SERVER STATE permission. The READ permission does not have to be granted explicitly to the service account. It could be granted through other user/groups, 1) SQL Server service account 2) Everyone 3) Users 4) NT Authority\Authenticated Users The idea is SQL Server service account needs at least READ permission to the root volume. I have listed the test and details here. 

For the record, this design seems a bit absurd even to me, but this is my thought process. In this one, the presence of a in the table is equivalent to saying = true in Design 1. The has a foreign key constraint and is used to ensure only networkable devices are entered. Can do this using a CHECK constraint (see below), and by making a computed column that is equal to . 

This question regards the proper use of NULL and utilizing CHECK constraints for business logic vs stored procedures. I have the following tables setup. 

The issue I have with this design is I'm not sure if the relationship with and / is a good or bad design decision. Is propagating a non-key field like to other tables a bad design? I don't have enough experience with database design to make an informed decision. 

The table looks like this in this setup (Notice the addition of record with id #4, and the field which specifies that this status is for use with devices that can can connect to a network): 

I'm designing an asset management database that tracks IT hardware. I decided to use a supertype/subtype design. I'm at a point where I want to track history of changes for devices. I wanted to use a separate history table, but I can't decide how to track history for changes made to subtype tables. If I use separate history tables for each subtype table I can reconstruct records by joining them with the supertype history table, except in the case where subtype history tables change independently of the supertype history table. By independently, I mean there are x updates to data in the supertype table, creating x supertype history records, and y updates to a subtype table creating y subtype history records. If the changes are made on the same day, how would I reconstruct records? Is this a good use of supertype/subtype, or should I denormalize the tables? Otherwise, can anyone suggest any way to approach the history issue for this type of design? Using MS SQL Server 2008. Here is a very simplified ERD: