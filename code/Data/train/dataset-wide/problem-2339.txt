I admit that this solution is not pretty and presumably it won't work if another dynamic sql comes around with another resultset...however it works for me now. Martin 

as fas as I understand you would like to control the order of execution if x is not present search for y and so on. Therefore a simple IN clause or the case expressions probably won't work if you've got entries for x as well as for y and z....by result returning multiple rows. The best way I can think of would be to introduce a priority factor within a cte and use this to return only the relevant row. Here is an example. For simplicity I use a table with the following structure 

I had this exact same problem (although I'm on Mac OS X 10.5.8) with all the same error messages. It turned out the problem was that when the computer was turned on, MySQL was not started automatically. I solved it by manually starting MySQL: 

My understanding of InnoDB is that the rows are stored in order according to the primary key. Thus, they're out of order for any secondary indexes. 

Looking for anything such as known bugs in Sql Server 2016 SP1, CU4 and CU5 (both have same issue) on windows 2012 R2 on vmware. Esp. if where parallel threads wait on each other. Issue: After going production we have one specific query in an SSIS dataflow that was very poorly performing in 2016 (was fine in 2012). So I found some indexes needed to be added so added them which cut down on runtime by about 80%. Then it started randomly freezing (say once or twice a day) never returning any rows. Doesn't seem to correlate to how busy the server is, locks, or data volume. We have very high cxpacket waits but who doesn't? I'm running maxdop 8 with 12 virtual cores. Activity monitor very often shows session id is blocked by same session id on 2016. I never looked on 2012 to see if this happens. It also shows this when a query hangs. No deadlocks. I'm running with option recompile and default server maxdop 8 (database is maxdop 0). Any ideas greatly appreciated. 

First of all thank you guys for helping me to get on track with your comments. I have now worked through an example and have a better understanding what's going on. The problem arises with moving LOB-Data (such as VARCHAR(MAX), XML and so on) to another filegroup. When you rebuild a clustered index on another filegroup the LOB-Data stays at it's former place (set by the command in the CREATE TABLE statement). One classic way to move the LOB-Data is to create a second table with the same structure in the new filegroup, copy data over, drop the old table and rename the new one. However this brings in all sorts of possible issues like lost data, invalidated data (because of missing check constraints) and error handling is quite tough. I have done this for one table in the past but IMHO it doesn't scale well and consider the nightmare of having to transfer 100 tables and you got errors for table 15, 33, 88 and 99 to fix. Therefore I use a well-known trick regarding partitioning: As described by Kimberly Tripp LOB-Data does move to the new filegroup when you put partitioning on it. As I do not plan to use partitioning in the long run but just as a helper for moving that LOBs, the partition scheme is quite dull (throwing all partitions into one filegroup): I don't even care, which partition the data is on as I just want to get them moved. Actually this technique and the implementation is not invented by myself...I use a formidable script by Mark White. My mistake was to not fully understand what this script does and what the implications are....which I have now: For LOB-Data it is necessary to rebuild (or recreate) the table (mostly the clustered index) twice: first with putting partitioning on it and second with removing the partitioning. Whether you use or not this results in having to provide the space of the original data TWICE: if your original table has 100MB, you need to provide 200MB for the operation to succeed. At the beginning I was quite puzzled, ending up with my new data files which had a lot of free space after the operation was finished. Now I accepted that I can't cheat around avoiding the free space. However I could avoid the necessity to shrink files afterwards. Therefore my solution is to do the first rebuild on a temporary filegroup and the second rebuild (removing partioning) on the destination filegroup. The temporary filegroup can be removed afterwards (if hopefully I don't hit the error message "The filegroup cannot be removed" (have a look at my question here) anymore. Thanks for reading and your help Martin Here is a repro script for my problem which includes the solution I have come up for it: 

Query store is enabled on 2 of our production databases but I no longer have access to see the Query Store node under the database since my SA privileges were revoked (I had them temporarily for a new server). I can't find anywhere that describes what minimum permissions are needed for access to query store reports. 

Problem was due to use of the Vmware CPU Hot Plug feature in combination with Sql Server 2016. This feature disables vNUMA and is not recommended by Microsoft or Vmware (see document Architecting Microsoft Sql Server on Vmware vSphere). Switching this feature off and then rebooting the server resolved this issue and it resulted in average query performance of over 50% as well as showing improved utilization of CPUs. Since we were running this Vmware setting when using Sql Server 2012 and did not experience freezing/hanging queries, my guess is something in 2016 makes this environmental setup more problematic. It could be the new default "soft Numa" configuration in 2016. Btw...the query plan did not seem affected by this Vmware setting. You can however see a difference when you run select * from sys.dm_os_memory_nodes; With Numa switched off there is an added "Node 1" and the virtual address space reserved in node 0 goes from 100% of max mem with no Numa to 50% of total max mem when Numa awareness in vmware is turned on. 

It sounds like you should check out MySQL's . The docs claim that it is a very fast way to load rows. (I personally have never loaded millions of rows at once). Once the data is loaded, you may need/want to add appropriate indexes to your tables to help speed up query performance. For getting data out of MySQL, check out . 

This question is very general and I'd suggest clarifying it with more context, but I'll give it a shot. I would look at: 

The procedure "AUDIT_TO_OTHER_FG" is a database level trigger. Its purpose is to put audit tables (with history data) into another filegroup. Our Java Application running on top of the database is using Hibernate and doesn't bother specifying filegroups. However all of these audit tables follow a certain naming convention. Thus the trigger fires at a CREATE_TABLE event, rolls back the table creation and creates the table again on a different filegroup. Maybe this is not the most elegant version to put the tables onto a different than the default filegroup...however it has worked fine in the past and has never been a problem until now. I had Management Data Warehouse data collectors set up before for that environment as it was running on SQL Server 2008. There haven't been any problem regarding these triggers in that version. Recently we moved to SQL Server 2017 and now I am experiencing these issues. I dropped the trigger temporarily and the data collector worked fine. So somehow it appears to be interfering with the actions of the data collector and the problem is the dynamic SQL used. However I do not get why this causes a problem as the data collector seems not to create any table in my user databases and the trigger doesn't fire while the data collector is run. Workarounds tried I have read a bit into "WITH RESULT SETS" and tried to change my trigger as follows: 

I appreciate the thoughtful answers for this question by Erik and Mikael. I think the complete answer lies in the combination of both responses plus some additional information and results of testing I have conducted. Other blogs and another note on stackexchange found here Cardinality estimator issue are also helpful. I think in summary its really that the new cardinality is very smart but like Erik says, it relies on the estimated plan. And if that plan is wrong, then the result of course is wrong. In my case it was catastrophically wrong in one query causing it to periodically run for hours instead of couple of minutes. The plan seems to be most incorrect when (as we all know) if there are implicit data type conversions involved. So what I have found is the most common problem in my data warehouse loads has been when the plan in 2016 tells the query to run serially instead of parallel. Of course you can fix this by switching the database to the old cardinality estimator (not recommended) or as Brent's staff recently said in their podcast, hint the query to get rid of the problem on a case by case basis. You of course can use Option (querytraceon 9481) but that requires elevated permissions and I always hesitate before using querytrace, especially if a normal hint will resolve the issue. In my case I used OPTION(USE HINT('ENABLE_PARALLEL_PLAN_PREFERENCE')) and this was like a turbo button for my largest problem query and seems to eliminate the issue of sometimes running serially. 

So nothing of this really worked. Could you think of an alternative how to keep the trigger doing its job but also have the data collector work properly? Resources Here's the original sourcecode of the database level trigger: 

The problem I have got an issue setting up Management Data Warehouse on SQL Server 2017 CU 5. The job "collection_set_1_noncached_collect_and_upload" keeps failing. It is related to the "Disk Usage" collection set. Error Messages are the following (I highlighted the part which is most relevant IMHO): 

Note that you can also change the collation from within a query. For example, in the case-sensitive database, I can do 

Make sure that either is specified as the storage engine, or that your version of MySQL defaults to . Other storage engines will parse and silently ignore foreign key constraints. Here's the grammar for a MySQL foreign key: 

Why does MySQL choose that specific execution plan for the second query? I don't understand why it can use the index for the first query but not for the second query.