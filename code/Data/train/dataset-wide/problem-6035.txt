Edit: let me add a little bit more about what the is-ought distinction is (from my perspective anyway), and why the above is a sensible maneuver. Suppose I have some measure of value--let's call it V--and you can make factual statements about how V will change in various scenarios. Now I can without any problem construct statements of the form "if you want V to increase by vX then you should do X" assuming that I have correctly concluded that doing X will in fact increase V by vX. However, if I provide you with this information, you might simply say, "Oh, that's interesting, but I'm not going to do X." The question is whether there is a rational way for me to change your mind based on factual accounts alone. I could ask you what you value and use your metric U, but you could just say, "Well, I know that increases U, but I'm not doing it," which would not be irrational for you unless we knew that prediction of U is the only mechanism by which you select your actions and even then it's not clear that it would be wrong for you to decrease your U; rather we would only learn that it's impossible. So it's very hard to see a way out from stubbornness to rational compulsion (or, really, any impulse to act at all, if you think about it). If you envision defining value as to be that which is compelling to you, you've just shifted the problem: now value and compulsion are linked by definition, and you can read off value from states of affairs, but this still doesn't truly constrain your behavior because states of affairs are powerless to indicate to you that your values are wrong. The naturalistic counter to this is that this sort of elaborate value-land (or realm of oughts) living over factual affairs is logically possible but not at all a good model of how our subjective sense of value actually happens. Rather, the bare ought, compelled by detached value, is arguably not even a sensible construct; this simply isn't how it works in real humans. If you want to understand how motivation and value and "oughtness" actually works--why we have the intuitions of these things that we do, and so on--then you need to approach the problem differently. If you really want value in there, it's easy enough to inject it. Dopamine is pretty close to the universal currency of value in our brains. Restating oughts in terms of maximization of dopamine release (over evolutionary timescales, if that's what it takes) doesn't really lend any clarity, though; it just fulfills your wish to have value attached to ought. Hopefully this clarifies what I am trying to convey. 

Sometimes. Some branches of philosophy aren't inherently patriarchal or mono-racial, yet are key aspects of Western Philosophy. These include logic, philosophy of science, epistemology, much of ethics (including both consequentialist and non-consequentialist systems), and so on. Indeed, the whole point of much of this endeavor is to be as objective as possible, to depend as little as possible on gender or race or individual identity or anything aside from the bare commonality of being humans or a generalization thereof (conscious being). Thus, that all philosophy is inherently patriarchal or mono-racial would only mean that women or non-Caucasian races couldn't think straight. Logic is logic, no matter the number of X chromosomes per cell or the amount of skin melanin in the one who engages in it. On the other hand, social philosophy and all things relativistic (particularly common though not exclusive to continental philosophy) is very much a product of its culture. It is hard, for instance, to get very far through anything written by Nietzsche without thinking, "Hm, I don't think a Rwandan woman or a (hypothetical) seelie faerie or Lt. Commander Data would agree." So perhaps we could say: where the Western matters to the Philosophy as anything beyond a description of the historical source of the endeavor, then yes, philosophy is in many ways guilty as charged. However, very much of it is only Western in the incidental historical sense. (And, to "historical" I should add, "and in the makeup of the senior faculty of university departments".) 

I think that people who claim strong AI is impossible do tend (often unwittingly) to commit themselves to some manner of dualism. But there is an argument to be made that the process is inscrutable: We have no general theory for approximating arbitrary recurrent functions. Indeed, there are all sorts of recurrent functions with profoundly frustrating processes such as chaotic functions where long-term values cannot be predicted without infinitely accurate measurements of current states even when you know the form of the function. Furthermore, our brains are heavily activity-dependent (synaptic plasticity, adaptation, etc. etc.). Therefore it plausibly could make a difference to the qualitative operation of the system what the long-term chaotic behavior is. (It is not guaranteed that it will, but it might.) Thus, it is at least somewhat plausible that although our brains are in a universe which is nominally computable, in practice there is no way to find the computation that needs to be done. At every reduced level you run into exponentially hard problems (computational chemistry is too hard, protein folding is too hard, etc.), and at the highest level of abstraction there is no way to discover or verify the algorithm (and algorithmic space is enormous; you can't brute force the search). This would, nonetheless, allow us to monitor and replay the entire activity of someone's brain (given almost unimaginable heroics of instrumentation and genetic engineering), but that is not strong AI. (It is very weird, especially from a philosophical standpoint: here you have a consciousness-over-time arrayed as patterns in space instead.) Personally, I don't think this type of inscrutability is very likely, but the reasons for thinking so are more of a vague hunch about the evolvability of systems that rely upon details of chaotic behavior of self-recurrent systems than anything truly sound. 

The labor-theory of value presupposes that there is reasonable equality between the value of labor of different laborers. But this is clearly wrong: an Einstein or Picasso or Mozart can produce works of immense perceived benefit for very little labor (comparatively). Once you get into widely differing values of labor (due to talent, due to accessibility of natural resources, etc.), you lose most of the benefit of focusing exclusively on labor. How do you understand luxury items, for instance, which are in practice almost exclusively traded for small amounts of labor of the highly paid? How do you understand the appeal of limited resources like gold, diamonds, and rare earth metals, and the relative advantage of those who supply them? Of course you can call an average person-hour your standard unit of wealth instead of one dollar or the resources to raise an infant to an adult, but you're just picking another currency from among many possibilities. There's also the thought experiment of a world where labor accomplished nothing (e.g. due to extensive automation). People would still value things differentially. And there's the neurophysiological evidence that dopamine pathways are heavily involved in the assessment of value, but it seems unlikely at best that labor alone counts as a cost in that system. (Though there are rodent experiments where they do pit rewards against labor, so it certainly plays a role even in rodents.) So it's a useful idea but not the whole story. In a world of identically-capable individuals with uniformly distributed resources, maybe it would be the whole story. But that's not the world we live in. 

That we perceive the present as instantaneous is dependent not on special relativity or on events comparable in duration to the time it takes light to cross the width of our heads (nanoseconds), but on the relatively slow conduction of action potentials in our neurons (milliseconds). Given that everyone manages to maintain a percept of instantaneity in the face of the inability of their brain's to communicate with itself for such a long time, we can't rely upon intuition for much help here. What appears to us to be the instantaneous present is a collection of different events causally disconnected for milliseconds at least. Having dispensed with intuition, we then are left with a pure physics question: is a dynamic non-deterministic universe required to have an instantaneous present if it is to agree with experimental observations? And there the answer is a resounding sort of. Bell's inequality already argues that the universe is best modeled as non-deterministic (at the quantum mechanical level). Whether something is "dynamic" or "static" seems unlikely to have a bearing unless you make up rules that force it to be one thing or the other. (E.g. if by "dynamic" you mean that the rules change as a function of time, and you insist that time is a global variable instead of a point variable coupled to adjacent points in some way, then you have by your definition imposed a necessarily instantaneous present coupled to your postulated variable.) Experimental observations are consistent with a loose idea of "present", since state transitions occur in finite space and therefore in finite time, and since they have finite energy they are delocalized in time according to the uncertainty principle. 

You are basically arguing for Occam's Razor. It's a popular heuristic among philosphers and scientists, and much has been written on its general validity. You may be suggesting an added twist: if no simpler hypothesis explains the data as well, then you should mark the hypothesis as "doubtful", presumably because you predict that an as-yet-unfound hypothesis will supplant the existing one(s). This is a valid form of Bayesian reasoning, but you can't get very far because it depends critically on the as-yet-unfound hypothesis. But does such a hypothesis actually exist? If one doesn't exist, then of course the inference is wrong, and it is devilishly hard to come up with sound non-constructive existence proofs for a hypothesis. So as a practical matter, the observation might motivate you to look harder, but one probably ought not just conclude that a hypothesis with core idea but lots of tweaks and corrections is wrong unless it is actually logically inconsistent in ways that are not easily fixable. (One exception: if everything is a correction, then the hypothesis may not explain anything in which case you do have the simpler hypothesis that does equally well: "stuff happens, including (list of stuff)".) 

I think you and your friend are getting tripped up on language. The appropriate answer to Do you think that P? for some proposition P should reflect your knowledge of the truth status of P. If you have good reason to believe it true, you should say "Yes, P. P is true."; if false, "No. P is false."; if you have insufficient information you should say "I do not know whether P is true or false" (possibly with additional clarification or explanation of the degree of your uncertainty). The key is that the question is not about P but about your thought processes regarding P. One can always force a decision ("Will humans be immortal: yes or no?") but characterizing your thoughts with this forced choice is an impoverished view of your thoughts at best (or at least I hope it is). In contrast, the appropriate answer to What are the allowed truth values for P? is usually "true or false", if P is a statement for which a binary decision is pretty clear. Certain statements have insensible semantics ("Is somnolence more blue than yellow?"), while others cannot be crisply answered under the best of circumstances ("My friend Joe is happy") due to there being a continuum (or worse) of states that cannot in an elegant and principled way be divided into two groups. Still, for the questions you ask, the "I can't tell" option seems unlikely; "Will humans be immortal?" should have a pretty clear yes or no answer, at least after clarification about what counts as immortality (e.g. fully digital immortality). 

People are complex enough so that most of them probably would like something done unto them that is not generally desired, and would not like done unto them something that most people would like. (For instance, some people love surprises and others hate them.) Kant's more general formulation as you stated above falls victim to the same problem (albeit at a somewhat more general level), but the problem goes away with the second formulation (people are ends, so you have to pay attention to what they want). 

That we can very reliably make predictions on the basis of what we do know (or, rather that in the past we have been able to) is the best counterargument I know of against that argument. Although this appears in various guises everywhere from Popper to coherentism, the simple observation that we routinely do not walk into walls (and manage to build fairly solid ones) shows us that we have a keen grasp of a good number of phenomena on our temporal and spatial scale. We thus have something of a bound on the depths of our confusion: at least our universe has to be consistent, somehow, with the regularity we have managed to observe locally. (One should still be cautious about being too confident of proclamations about the universe, but with the can't-know-anything genie back in its bottle, we can at least entertain the notion of trying to know more than nothing.) 

Apparently rational behavior doesn't always yield optimal outcomes in social situations. The Prisoner's Dilemma is perhaps the best example of this: both actors acting rationally will yield worse outcomes than both acting "irrationally" (i.e. what would be irrational in the absence of some coordination mechanism). One could very well recommend that people act non-prejudicial if receiving prejudice causes the very problems that are at issue (e.g. assuming guilt may cause the unfairly accused to actually commit crimes, as they're already suffering the consequences without the benefits). This sort of issue (when do you recommend acting as-if-X, even if X isn't quite true) comes up all the time in rule utilitarianism. One could also deem that the impact of receiving prejudice (i.e. treatment that is unjustified) is, across society, worse than the impact of being unable to adjust to knowable changes in probability distribution (e.g. how likely are you to be mugged). Again, the utilitarian would not necessarily recommend the "rational" course of action. This isn't to say that you shouldn't notice the correlations. (Some people seem to go that far.) Just that you shouldn't act on (most of) them. The problem with prejudice is that you don't actually know that your suspicions are valid in any particular case (by definition), and acting on false suspicions can cause considerable harm. Many ethical systems take a dim view of causing harm on the basis of a falsehood. 

You're not doing empirical science if the objects postulated by your models are beyond hard to measure or observe, even indirectly. Given all the layers of indirection between e.g. a chair and your awareness of it (hi photons, photoreceptor cell voltage, action potentials in retinal ganglion cells, depolarization of simple cells in V1, ...), we shouldn't be the least shy of indirect observations. The reason why special and general relativity and quantum mechanics are so successful is precisely because they make oodles of predictions, some of them very weird, and measurements have confirmed those predictions over and over again. And inasmuch as there is a crisis in string theory (not really, but neither is it entirely comfortable right now), it is because it isn't really science at this point; people have had incredible difficulty narrowing down the elegant mathematics to testable theories. So it is certainly mathematics, and inspired by physics, but a lot of scientists (e.g. me) reject that it's science yet. Everyone is simply trying to do science when coming up with hypotheses and models, but you've got to see it through to finding telling agreement with observations before it actually meets that bar. It's just really hard to close that loop in some fields; when you fail, you've also failed to advance scientific knowledge.