No. You can't really disallow access to these files. What you can do, however, is hide and restrict the drive so they won't be able to navigate is through explorer. It's not bulletproof (because it relies on application supporting the limitation) though. 

This is typically bound to the hardware you're using. On most hardware (including intel) the MMU controls the whole process. When a program allocates memory, it will request it to the MMU and get back a virtual address. In turn, the MMU will register that page as being "in use" in the global address space map. When the program actually access that memory space, the MMU will lookup the page into the address map. If that page is in "live" memory, it will send back a "live" pointer to the OS which will handle the memory read/write in behalf of the program. If the memory isn't currently allocated, then it will trigger a page fault. This processor exception is then caught by the OS which is then responsible from figuring out where the data is in the swap file, load it into physical memory and give the page back to the MMU so that the initial process can continue. This means that, unless the memory page is accessed, it will never get back into "live" memory once put into swap. That is why there is usually an OS API that allows programs to specify that a particular memory block is NOT to be swapped to disk and should be kept in memory (I don't know about Linux, but in Windows, it's the VirtualLock function). 

xxx.xxx.xxx.120 is the initial server IP address, the new one being xxx.xxx.xxx.122. For good measure, I also ran (the new IP) and got nothing back, as expected. Unfortunately, when I try to start the web application, I got this error message: 

Using windows backup (or any backup solution that uses disk snapshot, really) is just fine for backing up a SVN repository. Be aware, however, that no matter what you use for backups, if you need to restore the repository, it is possible that all your SVN client will have to fix their local working copy if they have updated to a more recent version of the repository than the one that is available in the backups. This can result in lost work (or at least the need to manually merge the new code into the "old" repository). 

This is a fairly common problem with FTP when there is a NAT system between the client and server or a firewall that isn't configured for FTP. Your first sample uses so-called "passive mode" FTP while your other two samples uses "active mode". FTP works through two different TCP connection: a command channel (on port 21) that is used to transfer simple commands (login, list directory, etc) and a data channel which is used to send any data back (that is: a file but also the result of a directory listing request). In active mode, when transmitting a file or a directory listing, the client will specify an IP address and port number (PORT command) to use and the server will establish a new connection from port 20 to the specified connection. If the client is behind a firewall or a NAT device, it will prevent that connection from succeeding. In passive mode, the data channel is open in the opposite direction: The client will send the PASV command and the server will start listening on a random free port (typically in the dynamic range) and tell the client to connect to that port. Passive mode is much more commonly used because it is relatively easy for a firewall configured on or close to the server to detect the command and allow the new connection. On the other hand, an active mode connection requires the client to be able to accept a connection coming from the server and that is usually not working well at all when behind a NAT device or corporate firewall. the solution is typically to disable the active mode on the server altogether or make sure the clients all use passive mode. this is usually not a problem with modern client (which all default to active mode) but can be an issue with older ones or FTP scripts. 

You can enable user logon logging by following the instructions in KB221833. That will create a log file in which will contains timing information about each step of the logging process. 

You can setup roaming profiles on a different path and configure the server to delete the profile once the user logs off. The downside is that the profile will have to be copied to the machine every time the user logs off: if one of your admins managed to grow his profile to 2 gigabytes, that's going to be one hell of a long logon time. 

NSlookup is a DNS lookup tool. It can be used in interactive mode to query many different record types but, when used from the command line, it will either try to perform a standard forward-lookup of an IP based on the provided host name or a reverse lookup if the input is an IP address. So, if you enter a random IP address, here is what happens: 

I'm having a bit of a problem with LDAP search that should specify whether a user is a member of a given AD group or not (recursively). Basically, what I'm doing is issue a LDAP search with the following parameters: 

You most likely have a limitation in place for the source connection of the root user in your /root/.ssh/authorized_keys file. Specifically, your probably looks like this: 

100% safe ? no. No operation involving an array rebuild is fully safe, in particular with RAID 5. Will it work ? Probably. Your problem is that a single read failure during the rebuild will cause the whole volume to fail. And you're going to do it twice, including once with a drive that is already having trouble. Ideally, in such a situation you should take the system out of production, make a full backup, delete the RAID volume, change your disks, re-create the array and restore your backup. If you really can't take that much downtime on that array, then you should try to change each disk separately and wait each time for the rebuild to be done but do NOT do that without having a full backup first and, if you intend to keep that system active during the rebuild, make sure you warn the users first that it is possible that they will lose all data after the last backup date and that you got their approval (after all, it's their data and they should be the ones deciding what risk they'd rather take: safe but show with downtime or unsafe but potentially with uninterrupted service).