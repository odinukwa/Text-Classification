Williams, Patrick E., et al. "Entrainment to video displays in primary visual cortex of macaque and humans." The Journal of neuroscience 24.38 (2004): 8278-8288: Macaque monkeys: Of 92 cells in the primary visual cortex exposed to a 100 Hz refresh, 21 (23%) significantly phase locked to high-contrast stimuli. Humans: Responses measured using scalp EEG were seen at 72 Hz in some, but not all, observers. Krolak‐Salmon, Pierre, et al. "Human lateral geniculate nucleus and visual cortex respond to screen flicker." Annals of neurology 53.1 (2003): 73-80.: 

Yes, the eye can distinguish frame rates above 60 Hz. So can the brain. We are just not normally aware of it. Conscious perception of flicker is measured in laboratories using the critical flicker frequency (CFF) threshold, which is the lowest frequency of flickering light (Hz) that produces the appearance of steady light in an individual. It's a probabilistic statistic that is estimated by testing an individual -- it depends on the individual and the testing methods. And the same person will have a different CFF depending on factors including fatigue. Here is a pretty typical CFF plot (from Hartmann, E., B. Lachenmayr, and H. Brettel. "The peripheral critical flicker frequency." Vision research 19.9 (1979): 1019-1023.) showing how one person's CFF (the y-axis) varies at different visual eccentricities (the x-axis, greater eccentricities meaning looking at it less and less directly) and lighting levels (the different points and lines.) 

There is no conclusive evidence for or against amphetamine affecting creativity. Martha Farah's comments are liklely based on a small study (linked here) she and colleagues ran in 18 participants. The tasks were brief puzzles (rather than longer meaningful tasks one might expect to be sensitive to drugs that enable prolonged focus). Results were fairly inconclusive. Their conclusion begins with: 

As you can see, this person is around 60 Hz CFF in bright illumination when the flicker is around 15 visual degrees away from the center of their vision. This suggests that many people should be able to see the flicker of CRT monitors with 60 Hz refresh rates. They can. (As others have noted here, this is not possible with LCD monitors because LCD monitors do not flicker.) Why isn't CFF even higher at these peak conditions? Good question. The bottleneck in consciously seeing flicker is not necessarily the human eye or even the thalamus or the cortex. The eye can transmit flicker well above 60 Hz to the thalamus and cortex. Lots of neurons can fire well above 100 Hz. And we can even measure neural response to high frequency flicker. Here are a few papers doing this: Herrmann, Christoph S. "Human EEG responses to 1–100 Hz flicker: resonance phenomena in visual cortex and their potential correlation to cognitive phenomena." Experimental brain research 137.3 (2001): 346-353.: 

I would caution that when researchers find different patterns in subsets of the data that differ from the larger group, we should regard them as very preliminary (possible fishing around for results), especially if they didn't state they intended to do that sort of subset analysis from the beginning. Aderall is indeed made of different amphetamine salts, but when amphetamine dissolves in the body, the amphetamine ion becomes separated from the salt ions. As a result, there is no real difference in the biological effects of different amphetamine salts. Perhaps the most famous example of a high performing individual who used amphetamines and was extremely creative is mathematician Paul Erdos, who continued to coauthor papers in mathematical journals for over a decade after his death. We don't know if amphetamine made him more or less creative, but we do know that he believed they aided his productivity. 

So if the information is in the brain, why can't we perceive it? We don't really know. The simplest theory might be that these signals are just too weak. But it seems unlikely that we could record something with electrodes on the scalp that isn't strong in the brain. It is also worth noting that our ability to perceive flicker is a side effect of our ability to perceive motion. Most motion perception takes place in situations where we have other information about the moving objects. Flicker perception as measured with CFF threshold or as noticed with monitor refresh rates is a strange edge case. It likely made little evolutionary sense to optimize this ability when we could already, for example, see most fast moving natural objects well enough to catch them. So you might invert the argument and ask why should we humans have bothered to see fast flicker? Perhaps the most interesting possibility is that this may be a limitation of consciousness itself. An initial intuition is that conscious perception will likely be slower than the low-level processes it relies on. Because different types of perceptual processes have different computational demands, they take different durations to compute. Thus, combining perceptual processing in consciousness may be slowed by rate limiting steps. Alex Holcombe wrote a nice paper a few years ago summarizing the literature on temporal limits on visual perception of different kinds of stimuli. Moreover, Ruﬁn VanRullen and Christof Koch have argued that conscious perception occurs in discrete batches. They don't think it is perfectly regular, but rather quasi-periodic and determined by the task at hand. Still, they are essentially arguing that consciousness itself has a refresh rate. 

The paper is quite worth reading for other breakdowns (universities are responsible for starting roughly 1/3rd of the 'innovative' drugs that make it to market, which is a significant increase). TLDR: The $55M estimate is almost certainly too low. But the previous estimate of $800M is almost certainly too high. Nobody has the data we would like to have on this topic. (Note that my lack of rep here forbids me from using too many links, a somewhat ridiculous problem on a site that could basically be named "citationneeded.com") 

The title question isn't really answerable. But there are certainly other sources which bear on that estimate. I'm going to gloss over the different estimates here for a moment. The article claimed the W&L estimate was $55M, one of the authors (I'll quote below) claimed $59M, and I've heard $43M and $53M in reference to this article... let's just say "approximately $50M". To start with the obvious one, the study was primarily a counter to DiMassi's study that concluded the cost was around $800M per drug ($URL$ There are some important considerations to make on this study. Roughly half of this $800M was "cost of capital" associated with keeping the pipeline open for an average of 96months/8years (the average length of time it takes to get a drug through clinical trials). For those of us who don't work with millions of dollars spent over decades, I don't think it's unfair to characterize the cost in the DiMassi study as $400million. Another criticism of the DiMassi study is that the source data was somewhat opaque. A third criticism (made in the L&W paper) is that the companies take excessive tax credits on R&D, and that drives their real costs down from what DiMassi claims. Although I feel the $800M figure may be overstating things a bit, this newer study also has some severe problems. The most obvious issue is the cost of a phase III trial. Phase III trials usually have between 1,000 and 3,000 subjects, with an average cost of $26,000 per patient(“Clinical Operations: Accelerating Trials, Allocating Resources and Measuring Performance”, 2006). If you assumed no Phase III trial ever failed, that would still almost eat up the entire W&L cost estimate, just for a a phase III trial of 2,000 patients. Given that the failure rate is approximately 50% ($URL$ I don't see how the ~$50M figure is defensible. You still have to accommodate the costs of pre-clinical development, Phase I and II trials and manufacturing development. Some additional problems with the estimates come out when the original authors were questioned on it. Rebecca Warburton said in the comments section of that slate article ($URL$ 

This is in spite of the fact that they referred to "R&D" throughout the paper, and actually did factor in 'basic research' costs to their estimate, while claiming that companies hardly do any. (I'll deal with that at the end). Rebecca also claims that they did take failed drugs into account. The discussions with the authors attached to that article are worth reading. Donald Light seems to be saying that he took DiMassi's estimate, and then subtracted everything he thought was unreasonable... rather than approaching the task at hand and estimating costs/discounts/etc. For a direct confrontation between DiMassi and Donald Light, you can visit the abstract comment section of the L&W paper $URL$ . One important point to make (from that comment section) is that the R&D costs of a particular drug have no particular bearing on its price. Drug development is a sunk cost, after it is developed, the people who are buying it are paying for the development of the next generation of drugs. This is not a complex idea, and one that both DiMassi and Light understand (although there appears to have been a bit of a vocabulary misunderstanding early on) The claim was made both in the L&W paper and in the comments section of this question that "most drug discovery is not done by companies". There actually is some data out there in the form of this study: $URL$ . The sources of NMEs that eventually become drugs are: