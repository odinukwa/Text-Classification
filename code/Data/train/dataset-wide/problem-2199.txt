This is the default behaviour. As long as you don't have the default service in your tns_aliasses that are in use it should not be a problem. 

If you want to switch to a different database just issue . oraenv (if /usr/local/bin is in the PATH) and give it the ORACLE_SID of the database that you want to manage. Most databases will be accessed using a client from an other machine/VM. In that case you also might want to start the listener: 

Make sure that open_cursors is sufficient. What is the current value for open_cursors and session_cached_cursors? They should be high, start with 500. 

What needs to be done depends on where the connection initiative is done. If the initiative is done from the Oracle side, you could use dg4odbc and configure it with the odbc driver and connection details for the Postgress database. dg4odbc sets up a special kind of listener process that you refer to using a regular tns alias. This listener process has to be on the same machine is the ODBC driver but does not necessarily have to be on the database server. Ofcourse, the datbase server has a listener process but nothing -well maybe licensing- does prevent you from setting up a dedicated listener for dg4odbc on a separate macine. In your Oracle database you setup a database link that uses your tns alias that points to the special listener. 

I hope that you know what you are doing.... First find which segment would be a candidate to drop because it was not cleaned up by an Oracle crash: 

If you happen to be using the same platform (cpu/OS) it is fairly simple to get this up and running, if you know from which version of Oracle this backup is AND the backup is taken in the correct way. In your case, since all you have is ctl and dbf files, it should have been a COLD backup. If all of the above is not in place: forget it. If all of the above is in place: 

Using a job status from a scheduler is not reliable at all, often the backup scheduling has problems so it is important to also notice that backups are not started at all. For this you can find a zabbix Oracle module at github that does the backup monitoring based on v$rman_status. Zabbix does a real good job for this kind of monitoring. 

If the files happen to be from a valid backup you could have some luck, but mostly because you could turn in into a running database again. The complete system tablespace should be among the files you have to make a chance. Only reading data from the bare dbf files .... there are many options that make this hard to do. Depending on what you know about what you are looking for makes this more or less possible. Start with: forget it and be happy with any word you can recover this way. dd and od might be your friends, compression and encryption not. Maybe Kurt can help you, see dude could be your best option. 

I am looking to expand my single MySQL database into multiple instances to provide High Availability. As well, I am looking to be able to have read replicas and to shard in the future. I have looked for options andnoticed two external tools called MaxScale and Fabric. I was wondering what the difference is between them? 

I will be using Amazon Web Services RDS service to host a MySQL 5.7 server (Currently AWS do not support 5.7, but they have announced they are working on implementing this version and it will be out soon). I will only have one table, but I expect it to have MANY rows. It is a messaging table which will contain messages from users to other users. A Message can only be sent by one user to one other user. This message when read for the first time will need to be updated stating it has been read so it will require one update in it's life if it has been read. A User must be able to view all thier sent messages ordered by newest and all the messages they received ordered by newest. My initial design looks like this: Messages Table: 

This way I could store all the message (under 1mb) inside of this one entity, but I would have to keep track of which entity each user is at (if the user exceeds the 1mb limit I will have to create a new entity). This proved to also not be too efficient because if I send a message to perform 2 gets to see which message entity I am currently at and which message entity they are currently at. From there I must now use another 2 reads to get those entities and another 4 writes to update them both (considering I do not need to create another entity if it is full). I was looking for any ideas to accomplish what I need in a more efficient way. I have nothing implemented yet so I am open to ANY ideas. My main concern is for this to be efficient, Cheers! 

1) User would login using their username and password 2) User could get friends by getting their USER entity based on their username 3) User could add/remove friends by getting the entity of user1 and user2 and either adding or removing friend via transaction to make sure they are consistent. 4) User could get all the message they have sent by using indexing the the 'from' attribute (limit of 10 message per request). The same could be done to view all the messages they have received by using the 'to' attribute. When the message has been seen for the first time I would go to the message (1 get) and update the entity (1 write + (4 writes x 3) = 13 writes to update the entity). My major concern - If a user gets 10 messages, this will require 10 get requests plus if all 10 messages are new I will need to update each entity and if all of them are new that is (10 x 14) 140 writes. Then if I get another 10 message for this user the same process and this could all add up very quickly. I then thought of creating an entity to store all the sent/received messages in a string for a user inside of a single entity: 

Yes, you can and it is quite easy too. In Oracle, the ORACLE_SID is just the name for the Oracle Instance and has not very much to do with the DBNAME. A database with the name PROD, can be served using Instances with any valid name. There is no direct connection between the SID and the DBNAME. This connection is made using the parameters. The parameter file is identified as init${ORACLE_SID}.ora or spfile${ORACLE_SID}.ora In the parameter file is the parameter db_name. This is where the connection between the Oracle Instance and the database is made. So, you don't need to re-create a controlfile, you don't need to use nid, just make sure that your parameterfile has the right name, bring down the old Oracle Instance and start the new Oracle Instance after having set ORACLE_SID to the new Oracle Instance name. The parameterfile and the password file are both found using the ${ORACLE_SID} as part of their name. Re-creating the controlfile is only needed when the DBNAME has to change. nid is needed after a clone operation where you need to change the DBID to prevent accidents that could hurt the backups of the source database. 

Seems like a good reason to visit the Oracle Documentation site. The 2-day dba documents are very good. If your database is running in ARCHIVELOG mode, it copies all transactions to the archivelog destination. The transactions are always written to the redolog files but when they are full, they are only saved when running in archivelog mode. This enables you to restore your database to any point in time. This restore operation starts with restoring a full backup where you can apply the archives until you reach the point in time where you want to stop the recovery. For example, close before a table was dropped. Normally production backups are made with the database online. Again, a reason to run your databases in archivelog mode. IF not running archivelog mode, in a disaster scenario you might loose all transactions made since the last backup. If your transactions are really important it could be smart to copy the archivedlog files to a second DC on a very regularly basis. How regularly depends on the cost of loosing transactions. If you can handle loosing one day, you copy daily, most sites copy a few times/hour or even use standby databases that receive the transactions in a near sync way. In general you start restoring the database and perform a recovery until time. Normally we use rman to do this, something like 

Most of the time the session tries to tell the client something nasty happened. If the client is gone, this will never be acknowledged so the dbms will wait forever. When you kill a session, the connection between v$session and v$process is gone. The process of the killed session remains in v$process. You can find the OS pid for these processes by checking the processes that are not PQ slaves without a session. Kill those processes from the OS. Oracle is very robust and if anything should be recovered, smon takes care of that, if needed. The state of the database does not depend on a client that gets killed. Would be a bit strange since this would mean that if a client crashes it would corrupt the rdbms... If a database gets - logically - corrupted it would mean that the client did not make use of the regular transaction mechanism. The following SQL can be used to identify the processes that have no session tied to it: 

I have a very simple requirement. I have an application where users log in, add/remove/view friends, and can send/receive (also view the message you sent and received) a message (message is more like an email, not a real time chat) from a friend. When they receive the message, the application needs to mark the message as 'read'. The messages must also come ordered by newest to oldest. My thoughts: KINDS: 

Additional "would like to have feature, but can live without it requirement if it effects performance considerably" would be to have a user view their messages with another user: 

We believe this will work for, but we currently have an active community of users and have estimated when we roll out the feature it will be heavily used. Thus we would also like to plan for the future to be able to scale (premature now, but we think this is a very simple feature and are hoping to design it well now to save us time in the future). If in the future we need to scale horizontally we do not think our design will scale very well. We don't believe an auto-incremented message_id pk will work in a multi node environment. We looked into setting a UUID for this column ($URL$ but have read that this can really hurt performance since the indexes will be large. Reading this article we see paging can be an issue too. $URL$ In our current design we don't really see a great shard key which can be used for all our queries. We would like our queries to reach one shared server if possible. So my question is what would be a an efficient way to implement this basic messaging feature so that in the future it scales well with the queries we need. I have only ever worked with a single instance of MySQL so I am not an expert on scale out design with MySQL by any means and am open to ANY ideas (complete redesign too)! We believe sharding will be inevitable since our instance types are not very large. PS: We know some may say NoSQL is a great option for this scenario, but we looked into NoSQL options for this feature (Cassandra,DynmoDB,Google Datastore, Azure DocumentDB, FileSystem like AWS S3 or Azure storage) for a few months but due to costs for performance (indexes are very expensive in managed NoSQL environments), lack of ACID compliance (we have other ideas which will need true transactions), and more we decided on MySQL. 

Say I have a table with 3 columns: Say ID has an index on it, name has an index on it, and there is another index which combines id and name. Lets say I now have an update statement which looks like this: 

This did not work either. I then tried to change the password for root to make sure I was using the correct password: 

I am only updating the AGE column, so will MySQL still update the indexes even though non of the indexed columns were modified or will it leave the indexes alone?