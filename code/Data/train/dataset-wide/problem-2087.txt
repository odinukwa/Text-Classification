performs best to be unique, narrow, static and ever-increasing by itself. So in this case, the inclusion of DeletedDate actually result the clustered key becomes non-unique non-static (presuming the DeletedDate value could be changed). The in the non-clustered index is useful to cover the query without having to perform a key lookup to the table. However, as the INCLUDE columns only stored in the index leaf level, it does not help in searching for the values of the query predicate (in this case, NULL) A of DeletedDate allows a more effective search on the range of records that satisfy the predicate (NULL). A filtered non-clustered index could further narrow down the subset (only NULL records) and provide a better performance as well as storage for the non-clustered index. With your description showing that the query returns all columns from the table with a single predicate, You could create a single filtered non-clustered index key for DeletedDate with . Test it and examine the execution plan. 

Version - SQL Server 2012 SP1 CU3. I have been using sys.dm_os_volume_stats for a while and it was working. Recently I have not been able to return any result from this DMF on one SQL Server 2012 instance. e.g. 

I don't know Oracle but I will clarify what it means in SQL Server. On one Windows Server there can be multiple instances of SQL Server (independent installations). Each instance has their own databases (user databases and system databases). It is important to know that each instance has its own system databases (master, model, msdb, tempdb). Instances can use different SQL Server editions, versions and have different products installed. Some SQL Server components are shared between instances on one server (SQL Browser for example). To access instance A from instance B you have to create a linked server on instance B that maps to instance A. You can also connect SQL Server with Oracle using this mechanism. Each database can have multiple schemas. Schemas group tables, procedures, views and other database objects. 

I think it's a great news that Microsoft finally changed this default setting. The previous one was really bad - it caused problems with latch contention. Paul Randal's article on the topic 

You might have to change the SQL Server database engine service account (local account is default). It's simple. I would recommend using Ola Hallengren's Maintenance scripts though. They give you much more versatility. 

return the result in . Change the @HoursWorked data type in your scalar value function to instead of . 

But if I want to have to use with startup option limit to an application, for example sqlcmd (-m"SQLCMD"), how do I do that with NET START? 

After some tests, in order for the DMF to work, it appears that the service account needs to have at least READ access to the root volume of where the database files are located at, in addition to the SQL Server login security VIEW SERVER STATE permission. The READ permission does not have to be granted explicitly to the service account. It could be granted through other user/groups, 1) SQL Server service account 2) Everyone 3) Users 4) NT Authority\Authenticated Users The idea is SQL Server service account needs at least READ permission to the root volume. I have listed the test and details here. 

The delete statement without the where clause delete all rows in the table without change of table structure. If the delete statement is within a transaction, then it can be rollback before the transaction is committed. If the delete transaction has been committed, the deleted transaction can't be rollback. Unless use of third party tool or restore the log backup to previous point if available. 

You can use snapshot replication (it allows you to update the table every 5-10 minutes). If you would like instant changes, go for transactional replication. Note that you cannot publish a replication on SQL Server Express, you need at least Standard edition. 

Some people also suggest to install SQL Server on a dedicated partition instead of the C: drive but I think it is an overkill and does not bring any benefit. The installation itself cannot grow out of control. System databases other than tempdb can (theoretically) do that. I would also recommend putting them and their log files on a different partition. You should definetely not put data/tempdb/log files on the same partition as the OS, because if you run out of space for say tempdb, the OS might not be able to perform correctly when out of space. Also there is another big topic of data separation on different physical drives for security/performance reasons but I will not go into that. 

Share the execution plan, like @YperSillyCubeᵀᴹ said. I think you can improve the performance of this query by creating a nonclustered index on requestStatus, requestId. However: the fact that you are not specifying the column list (I'm guessing SELECT *) may change the plan by introducing Key Lookup or Clustered Index Scan (probably CIS on such a big table). If Clustered Index Scan is inserted to compensate for the lookup, the nonclustered index will not be helpful. I can answer the question more precisely if you update the question with the plan. 

@elijah, SMO does have function to shrink file. The PowerShell script below shows the log.shrink method is used to shrink ONLY the log file. The shrink with default or truncateonly option work for me with full recovery model. 

Run query 1, and then query 2. DMV shows exclusive lock incurred by query 1. Query 2 appears to be waiting for query 1 to complete. Turning ALLOW_SNAPSHOT_ISOLATION ON doesn't appear to enable READ COMMITTED row versioning. Adding to both query 1 and query 2. Run query 1 and then query 2. While DMV shows query 1 incur exclusive lock, query 2 return details with 'Original'. Snapshot isolation appears to be in place. Observation from the test shows that itself enable/disable the READ COMMITTED row versioning regardless of setting, and vice versa. 

Backup A 1st restore take about 5 min. Ok, after restore complete, I delete the database 2nd restore take about 1.8 min. Hm.. Let's delete the database and try again. 3rd restore take about 1.5 min. Hm.. 

Run Query 1, and run query 2. DMV shows query 1 incur exclusive lock, but query 2 returns details with 'Original' without query 1 commit the transaction. It appears that READ_COMMITTED row versioning is in place. Adding on query 1 and query 2, and run query 1 or query 2 returns error - Snapshot isolation transaction failed accessing database 'TEST' because snapshot isolation is not allowed in this database. Use ALTER DATABASE to allow snapshot isolation. Third test, rollback previous transaction. Set READ_COMMITTED_SNAPSHOT OFF and ALLOW_SNAPSHOT_ISOLATION ON. 

I am designing a data warehouse using Azure Databases (not Azure Data Warehouse though), I have 2 databases: the main data warehouse and a staging database. As in a traditional ETL process, the raw data is stored in the staging, then transformed and finally loaded to the data warehouse/data marts. Since it is Azure SQL, cross database queries don't work and the only workaround I know is using external data sources and external tables, which is pretty problematic. Can you recommend the best solution to access both databases at the same time? My ideal solution would be cross database queries like in SQL Server on premise. 

These two queries should produce the same execution plan and therefore the same results but this one is much clearer. 

This way the engine knows when you want to use Column from TableA and when from TableB. It is a good practice - you should always use aliases when writing SQL queries. The query you are looking for might be: 

It is true that the view doesn't exist in the subscription database. How can i create it without the base tables? 

From this article, I know to that slash (/) is used instead of a hyphen (-) when a net start is used with startup option, 

SQLFiddle Update: Martin's alternative is much efficient. Here is another similar approach to Martin's solution, 

@matt you might want to adjust the tablix or row KeepTogether property. This affects how it tries to have all rows/table on the same page. 

It is mostly referred as ETL process (extraction, transformation, and load). Here are a link of MSDN article on Transforming OLTP Data to OLAP Data Warehouses. It is an old article but the same concept applied. 

Technically you can assign any existing/new filegroup to any new partition scheme (for other table). However, you need to understand the existing partition function and scheme design, and more importantly the purpose of the existing partition. It could be partitioned for improved scalability and performance reason by having filegroup on different disks. Or it could be partitioned to archive older data on slower and cheaper disk. Or partitioned on a same/different disk for quicker subset access like during data loading process. So, the idea is to find out if the purpose of the new table partition inline with existing partition to determine if existing or new filegroup should be used. 

What I want to do is to change xmlns to a new value. I tried casting the xml as nvarchar(max), using replace and then casting back to xml but it didn't work (string truncation). I tried using XQuery but I kept failing. Can you recommend a solution? 

I created a user in SQL Server 2012 database and revoked all permissions given by the public role. Then i granted exec permission on a stored procedure. The user can execute the procedure but cannot get the data it returns. The procedure is in a schema1 and the tables from which it selects are in schema2. If I add the user to db_datareader role it can read all data from all the tables in the database. I tried WITH EXECUTE AS OWNER but it didn't work. How can I grant only the access to the given procedure and nothing else? 

A few notes about both of them. If a server crashes and you can't repair it what should you do? Set up a new server from scratch? In both physical and virtual environments it's gonna take a lot of time. When the database server is a VM should you recover it from a VM backup? This operation can last multiple hours for bigger machines. Or maybe one should keep a clean virtual machine with database software installed and in case of disaster: - restore databases on the clean VM - change the IP address and start using the new database server Any ideas? Or maybe you can submit some useful links or refer any books? 

No. Since the update does not update data in neither col 2 nor the clustered key (col 1), the index _idx_TableA with only col 2 does not get updated. @professionalAmateur, click on the execution plan button on the top. 

You can try using the maintenance plans under management folder. There are Execute SQL Server Agent Job task and other task that fit your need. you can design the plan in the way that after a step completion, it run multiple tasks like shown below. 

return ServerB under name and network_name column with id 0 So question is, am I missing some steps over changing servername or is this a bug? Version - SQL Server 2012 SP1 

The script show the log file was initially set at 1MB, change the size to 10MB, and shrink it back down to 1MB. Data file size remains at its initial size, 5MB. 

I was doing some test on server name change and encounter some errors. Here is the original setup: Server name - ServerA, SQL Server default instance - ServerA Changes: Server name - ServerB Before I change the SQL Server default instance 'servername' 

Note that it is the same table. This query causes serious blocking on one of my test databases (I wanted to drop it but couldn't and this made me investigate the issue). The lock type is LCK_M_X. I cannot kill the session - i get the following message: 

Using different users will not reduce or increase the performance. It is clearly a security issue. Usually you don't want users from company A to be able to access company B data. I would recommend to create separate users for each application and grant them only the necessary permissions. If you run the application using a user with admin rights you risk a lot when the application gets compromised. 

I have recently upgraded a few SSRS projects and solutions created in Visual Studio 2010 to run correctly on VS 2017. I changed the target SQL Server version to 2012 (since this is the version that is used). One of the problems I face is whenever I upload any of the RDL files to the Report Server I get the rsInvalidReportDefinition error. This problem is further discussed here: Error while uploading a report I used the recommended solution and uploaded the rdl file from the bin folder instead. This time it works fine. The problem is that the files in the main folder still do not work correctly on the report server. I am wondering if I should maybe replace the files in the main catalogue with the files from the bin folder? Can you help me understand this issue better? 

Second test, rollback previous transaction, set READ_COMMITTED_SNAPSHOT ON but leave ALLOW_SNAPSHOT_ISOLATION OFF. 

Backup B (almost same size as backup A, at the same network location) 1st restore take about 1.8 min. I deleted it and try again 2nd restore take about 1.5 min. Ok.. The service account is enabled for instant file initialization. My question is why the restore duration varied on different restore attempt on the same backup file to the same machine? Is it purely on network throughput (perhaps someone was doing something on the network share and stuff), or something else like cache or something in SQL internal? 

I don't know how your report table is currently being designed, but I would think the report table list all the rows from the query results (with predefined number of columns, in your case, 2 columns). As long as the query is correctly developed to return all required rows e.g. all enrolled courses, the report table designing part should be fairly straight forward. 

Most of the forum and example online always suggest to have both and set to ON whenever someone is asking snapshot, row versioning or similar question. I guess the word SNAPSHOT in both setting get a little confusing. I thought that, in order for database engine to use row versioning instead of locks for READ_COMMITTED default behavior, the database is set to ON regardless of what setting. The setting is set to ON only to allow snapshot isolation when starting a transaction (e.g. SET TRANSACTION ISOLATION LEVEL SNAPSHOT) regardless of setting. The only reason to have these two settings set to ON is when it needs to have READ COMMITTED row versioning AND snapshot isolation. My question is, is my understanding incorrect in some way? And that these two setting have to be always set to ON together (especially for READ COMMITTED row versioning)?