For each new copy forward all rows from the previous one. The and tables are just convenience sets which can be copied to for the first examination. I have separated as a special case of to make it clear that it is the tooth that is absent and not the row in the table! 

The database is an encoding of the business rules. Without understanding the rules there is no way of making sense of the database. Take it piece-by-piece, focusing on one area until you have a reasonable understanding of it. Even if documentation exists I like to draw ER diagrams from scratch as it helps me remember the connections. It also allows me to compare how I would have done it to how it is currently implemented. The differences are an opportunity for further learning. Reading the code will bring out further "virtual" FKs and alternate keys that are not declared explicitly. 

The FORMAT function accepts a wide variety of input types. The output can be formatted to the culture & locale of the user. It produces an nvarchar result. 

In logical modelling one tends to record the natural keys to entity types. These will often require the "parent" and hence weak entity types are documented. Consequently they can show the scope of re-use of particular values. Take the cannonical Customer-Order-Order Line-Product example. Although the Order will have Customer Number as a foreign key it is usual for Order Numbers to be globally unique so Order is a strong entity. Often each Order's line numbers will start again at 1. By making Order Line weak we document this possibility more transparently. In physical design we may choose to implement "weak" tables to reduce the number of joins in some queries. Continuing the example, if we make Customer Number part of the key to Order and also to Order Line then a query to count how many widgets the Jones Trading Company has ordered need only touch one table. Needless to say this involves costs in other areas and should be weighed carefully before implementation. 

There are several ways to get to this answer. I'll tackle it in two parts. The first is to find out how many groups there are. This is a simple summary query: 

If you rename the existing, underlying table and use the table's current name for the new view you shouldn't have to change any SQL in the application (except for references to the column, of course). Whether you should have the column at all depends on the application's needs and your operational constraints. If you don't need this history for audit/ reporting/ whatever then get rid of the column and rows when they are no longer useful. Smaller databases are better databases! If you must retain old records moving them to an archive table is a good solution. It keeps the operational table small and access fast (did I mention that small is good?). You may have foreign key constraints in place; that will complicate matters. A from the operational table and an to the archive table will be more work than a simple to the column and hence slower. If this is a problem, marking them inactive, then moving a short time later in a batch process could work. 

You will need to know from whom the sample was collected. This is the . You will need to know when it was collected. Since it is likely that several samples can be collected in one day this will have to be measured to the minute, or whatever is appropriate. I've added so you have a unique, anonymous values which can be used to track sample through the lab, used on bar codes and things like that. You don't say but I think you may want to have a column to distinguish between, say, blood and urine samples because some tests will be applicable to one but not the other. Now we have the sample in the lab we need to know what tests to perform. I will assume you do not perform every test on every sample. You need a table to hold this information. 

Yes, of course it's possible. Microsoft's done it in their datacentres, you can do it in yours. It may take some time and effort however. :-) There is not yet an Azure on-prem product. I've read rumours of one due in the next year or so. Either way, it will take a while to be fully functional and licensing not going to be cheap. Best bet would be to roll out the DevOps toolkit. Infrastructure as code; desired state configuration; pre-speced VM images produced from a pick list; automated on-demand app deployment. All the bits are around and open source. Linking them will take a few dedicated months. Getting management approval, and educating users to take responsibility for their usage may take longer. 

Yes, there is a cost to having lots of cold data in actively-queried tables. One obvious case is that bigger tables have deeper indexes (i.e. more pages between the root and the leaves) so require more IO to read and write. I'd suggest you use partitioning. Keep the active partitions in the current table and swap the others out. Ideally you'd then use a maintenance task to move the historical data to a separate DB for archive. Make it clear to your external users what the time cut-off is and allow them to query either accordingly. 

As new receipts arrive existing ranges can be extended, merged or inserted. Assuming receipts arrive in random order, the number of rows will increase (to at worst before reducing toward 1 as rows merge). I have a nasty feeling that this would be polynomial in IO, however, something like O(Nx). If UserIDs are allocated without gaps the checks are easier. In the real world there are likely to be gaps and there will be a lot of look-ups on the user table to see if two receipt ranges can be merged. The above is symmetrical for "user has read post x through y" and also read vs unread. 

In theory every relation should have one or more candidate keys. In implemented RDBMSs, however, there are no requirements to declare any keys at all. As the result of any query, such as your UNION example, is just another table it follows that the system need not create a key for the result-table. Take for an example the query 

Exposition The rules you give are all binary rules. They relate one entity type to another. If you have a rule which mentions three entity type then a ternary table would be appropriate, but you have not. For example the intersection entity type "TargetMarket" would be ternary - Red Bull (company) targets energy drink (product) to software (sector). I'm inferring from the many-to-many between company and product that the products are generic. For example "chocolate" and "spreadsheet". They cannot be "Toblerone" and "MS Excel" as they are trademarked and can be produced by only one company (ignoring licencing agreements). If all companies stoped producing chocolate (God forbid!) I imagine you would still like to record that "chocolate" was in the sector "food". With a ternary table this would not be possible. If all companies ceased producing chocolate (i.e. deleted corresponding rows from the ternary table) the chocolate <-> food association would disappear. Similarly a newly-formed company could not be recorded in this system until it was producing products. Should it ever cease producing products (e.g. become a shell company, go into administration) it would have to be removed from the ternary table and hence from the system entirely. As I mentioned in the parent question, the full answer depends on the meaning of the relationships between the entity types, as embedded in their names, and both binary and ternary tables may be required. Say a company produces 4 products, each of which is categorised in 3 sectors. That would produce 12 possible sector-product-company combinations. If your system requires to capture that only, say, 9 of these possibilities are valid or exist in fact, or some other well-named constraint then the ternary table would be the right way to do this. But this is a rule which is not mentioned in your set above. I would suggest you examine the constraints between a company's sector and those of the products it produces. There may be redundancies there which should be removed for the model. 

The identity column will keep things in chronological order. @tables' contents survive a rollback so are preferable over #tables or normal tables for this purposes. I haven't profiled this suggestion; I throw it out there for what it's worth. 

The rows just happen to be coming out in this sequence. There are many scenarios where the optimiser may choose to return them in a different sequence, even if the table remains the same. The only way to be sure the client receives results in a particular sequence is by writing an ORDER BY clause. 

Although the representation of the data differs between the old and new schema the data itself should be the same. This means a series of reconciliation queries will provide certainty that the migration was without error. Say you have an Orders table. The total number of Orders per year, month or day will be the same in the old and new databases. Similarly the total value of those Orders for each time period will match. You can expand this idea for orders-by-product, orders-by-customer etc. If any data was changed in the migration the corresponding reconciliation queries will not return the same value from old and new DBs. You will know immediately that the problem was with an order from customer "X" on day "Y" for product "Z" and can focus your debugging on that area. The same approach can be used for any other table. Take customers as and example. You can count customers by country, business area, first letter of name or any other differentiator you choose. 

Create two sequences, one for IBM and one for Airtel. Set the start points appropriately. Put all the logic in a stored procedure. Pick the right one for each INSERT. This is not a great design. Surrogate keys like this are best if they are completely meaningless. By tying a range of values to a customer you may have problems in the future. What will you do when a range is exhausted? What if you acquire another customer? Will you tolerate gaps in the sequence? For a short-lived, single-purpose application you may be OK. For an enterprise application it is a risk. 

I've included and so you can tell how a test has changed over time. If you are only interested in what a test looks like right now you can remove these columns. You mention PersonID, so we'll have a table to put that into. 

A simple approach would be to split Job A's step 8 (let's call them A8a and A8b). A8a checks B's status from the msdb tables and does a if B has not completed, or stops if B has errored. This gets a bit tricky in restart scenarios, or if history gets purged from the tables before A8a runs. You can avoid the busy wait by using a flag. This can be a simple one-row, one-column table. This flag is checked and set by both jobs like this: each job reads the flag. If it is set the other job has finished and step A8b can process. If it is not set, set it and end. Job A will need a new step at A7b to do this; job B will do this as its last step. If the flag is set job B can start A8b by using sp_start_job. You'll have to fiddle with A7's statements and "On success" / "on failure" actions to get it to work. You will need a new step at the start of the whole thing (A0) to ensure the flag's correctly set for each run. Effectively you've split your stream into 4 by doing this: A1-3, A4-7, B* and A8-10. It would be cleaner to set each up as a separate job, have A3 kick off both A4-7 and B* and then do the "is the flag set" thing at the tail of each of these parallel jobs before starting A8-10. Of course the real solution is to get a decent scheduler. Other posters have listed some 3rd party ones. 

If you want the longest substring that means there is no other which is longer. A predicate will give this. 

Dollars and cents are just different units for the same amount. Whether a charge is ten dollars or one thousand cents, it's the same cost. In the same way the distance from your house to the beach is the same irrespective of being measured in miles, kilometres or stadia. Changing the dollar-number must necessarily change the cent-number. Dimensions have the property of orthogonality. A measure in one dimension can change without affecting the measure in the other dimensions. One can move East or West without altering how far North you are. The latitude and longitude are independent variables. So yes, if your problem domain has the corresponding number of independent variables, and they are of the same primative type that your GIS software supports (float?), then you can use that software's capabilities to process your problem. Be careful of the geometry the software implements, however. Walking in a straight line on a Euclidian plane produces a very different result to walking on a straight line on the surface of a sphere!