If you are using load balancers anywhere in your site, you should be able to configure them to have DNS as a virtual service. My Kemp Loadmaster 1500s can be setup to do round-robin with failover. That would use their service checking to make sure that each DNS server is up every few seconds and divide the traffic between the two servers. If one dies, it drops out of the RR pool and only the "up" server gets queried. You'd just have to point your resolv.conf to the VIP on the loadbalancer. 

We have lots in common. One of the things we don't have in common is that you've got a 5Mb/s link, where as I'm using a 1.544Mb/s T1 between my various offices. Here's how I've coped. First, Robert Moir's suggestions are all spot on and his observations are correct. If I can flesh out one of his points, if you aren't monitoring the traffic on the various network links, do it. MRTG is not the best solution, but it's the simplest, and it makes it easy to see when your peak hours are. It may be (and in my case certainly is) that your slow traffic speeds are caused by your users. My wires run hot pretty much the whole time my users are doing anything. They know that when people are doing large DB queries, transferring files, and downloading content, then the network will be slow. Since it sounds like you might be using leased lines and not internet-pointed T1s or DSL links like I am, you can take advantage of a device known as a WAN accelerator which gets installed on both ends of a line (one each at your home office and your branch office, for example) and compresses the traffic, which yields an effective higher throughput than the bandwidth is capable of. Next, make any services local that you possibly can. The less you have to send over the link, the better. If the users have roaming profiles or home directories, keep those at the branch office, or at least sync them nightly so they can use the local copy. Decentralize your network security. I don't know how your network traffic works, but lots of places make their branch office only have one network connection, to the central office, and so internet bound traffic goes through the same pipe as internal traffic, in order to hit the corporate firewall. That setup is more secure, but it kills your bandwidth. Decentralizing that by getting internet access lines at the offices and installing smaller firewalls on those sounds like the opposite of what every security professional recommends (and it is), but business operations come before security in the IT version of Maslow's hierarchy. 

Argh! How can people answer this question without more information??? 1) Rough estimate of server load would be a great start 2) Uptime requirements. Maybe you don't want one server. Maybe you want two behind a load balancer (or cluster of load balancers) (or maybe co-located?) 3) Do you have any existing contracts with manufacturers? Update the question with these answers, and you'll get much better responses. 

My infrastructure has two data sites, the alpha (where operations takes place on a daily basis) and the beta (the backup site, in case things go horribly wrong at alpha). Although this is not currently the case, I am pushing to have scheduled downtime at the alpha site every 6 months, so that we can run all services from beta. This will accomplish two things. First, it will prove that our disaster recovery site is completely viable. Second, it will give me a week's worth of time to remove accumulated cruft at alpha. As it is, I don't reboot my servers as frequently as I should. I agree with the other posters who said that it's important to know that your servers will come back up when you need them to. You don't want to "think" that they will, only to find out that you've changed something and not done it correctly, or not documented it. 

which will then install php and the dependencies. At that point, you'll want to restart your web service: 

Have you considered using an email retention service like Global Relay? We use them to archive our email for compliance reasons, but that has also had the side effect of shrinking my users' email boxes, since they know that if they want a deleted message, they can log into the Global Relay site and retrieve it. 

From "Password Security: A Case History", by Robert Morris and Ken Thompson (1979) ( $URL$ ), quoted from the prologue: 

So in order to use it, either type "csh" and issue it from the command line, or write your script so that it uses #!/bin/csh as the interpreter at the top. Here are some csh basics to get you started. 

I believe that Womble's comment to Peter Schofield is the best observation here...these aren't true SAS disks. No doubt you're being sold "nearline SAS", which is where they take a SATA disk and put a SAS interface on it. The drive mechanics are identical to the SATA version; only the interconnect has changed. When you plug a nearline SAS drive into a SAS controller, it will be faster than the equivalent SATA drive because the protocols are different, and it takes a certain amount of time to convert between the SATA protocol and the SAS protocol. Wikipedia says that there can be a 30% increase in speed, but I've never delved into the protocols themselves, so I couldn't tell you. - As an aside, I really don't know what kind of faith I'd put into those numbers. Storage Review does a pretty decent job, I think, but I can't figure out why they didn't get another drive to test when the disk started performing like this: $URL$ That's a drive that should be physically identical and in terms of interface, should be performing much more in-line with the others. The fact that it shows a discrepancy like this indicates (to me, anyway), that there was something wrong with the device. They do say that it checks out with Seagate's test suite, but I wouldn't put stock in the results until they're checked against another drive of the same model. Those results are just too weird. Edit Since it was brought to my attention that I didn't actually answer the question, my guess is that the SAS drive will give you better performance because of the reasons I listed above. That is what I would go with, unless research bears out the odd results that Storage Review got. 

You probably need to call Dell and see if this is a supported system config. They've been getting much more stringent about what goes in their servers. If it's unsupported, they'll let you know, and if it's supported, they'll tell you how to fix it. They might even tell you how to turn off that warning even if the card itself is unsupported. 

Can I interest you in reverse DNS? Essentially, the client is doing the reverse DNS on the server, or vice versa. I propose a test: Disable DNS lookups on the server by editing /etc/ssh/sshd_config and making sure "UseDNS" is set to "no". Run "service ssh reload" (or whatever causes your ssh daemon to reread the config), then try again. Incidentially, it doesn't happeen to finally prompt you after a long period of time, does it? Another thing you might check is looking at the contents of /etc/hosts on the server to make sure that nothing is wrong there. 

I have never heard anyone complaining about performance on their NetApp. I've never been able to afford one to back up that claim, though ;-) 

I don't know the best answer, but here's how I've got it setup: I have a T1 and a DSL line. I'm routing all VPN traffic over the T1, and all internet traffic over the DSL. I also configured mine for failover, so if one dies, the other handles the traffic (albeit poorly) The key device in this setup is my Juniper Netscreen SSG5. It's a 5-interface router/firewall/vpn combo. One interface is internal, one is external DSL, and the other is external to the Cisco that has the T1 WIC. What kind of routers are you using? If you can get away with only using one router, then you'll be golden. If you can't, you may need to get a third routing device that decides which path to send the packets down. In that case, your VPN endpoint would need to be that central router. 

"High Availability" is always fun :-) On the "simple" side, you can specify multiple A records. This works in a (typically) round-robin style, and offers no ability to specify which server you want to serve by default. This means that you'll wind up with people in Britain getting served by the American servers, and vice versa. This is not what you want (least of all because we Americans get confused by all of the extra 'u's that show up in words like colour) ;-) A more sophisticated solution would be using a DNS service that uses a feature similar to the one that you suggested in the comments. A technique exists called Global Service Load Balancing as well, which accomplishes something similar. On the more expensive and sophisticated side, you could use BGP MultiSite MultiHoming, where you buy an Autonomous System Number (ASN), and advertise BGP routes to the same network from both sites. This theoretically ensures that everyone gets the "closest" (based on network accessibility) site. Or, of couse, you can hire Akamai to distribute your data and applications for you. For a lot of money. If you've got the money and expertise, I'd recommend the BGP multihomed solution. If not, you might want to look at the DNS solutions. 

I try not to comment in 'holy war' discussions, but I'll try to be completely neutral here. As a sysadmin, I'm a pragmatist. I use whatever works best for what I do. In my infrastructure, I have somewhere between 75-100 Linux machines and two Windows servers. They're the domain controllers for my active directory domain, which I use to authenticate the rest of my Linux machines. There are directory servers available for Linux that would do what the Windows machines do for me, but it costs the company less to buy licenses and use Active Directory than it would for me to learn to install, configure, and administer those services. There is probably Windows software to perform the other 98% of what I do in Linux, too. It's cheaper for the company to use what works and what we know, rather than what might work and what we don't know. 

It sounds like you really want to make an MSI, which is sort of the equivalent to a .rpm or .deb in Linux. There are lots of commercial softwares out there with which to do it. Here's the Microsoft KnowledgeBase article: $URL$ 

What I use for this (and it certainly isn't windows specific) is a hardware load balancer to stand in front of the cluster. I've got a pair of Kemps ($URL$ in a cluster configuration, and the services are pointed at them. They relay the request from the source to one of the members of the pool of servers behind them. The order of servers queried can be decided one of several ways. Kemp isn't the only provider that does this, and using this method. Most hardware-based load balancers will work the same way. It's the most reliable way to provide highly-available services, short of tens and hundreds of thousands of dollars in network infrastructure. 

Do you mean single sign-on or merely authentication? Authentication is probably pretty easy. Just point twiki to the OU that you keep your users in, if it's like every other LDAP authentication scheme out there. Single sign-on is much more complex, and I have no idea. Here's a HOWTO that might help: $URL$ 

OpenSSH supports 2 protocols, called creatively, "1" and "2". "1" is old, allows things like DES encryption and other insecure things. You're going to have to get your users putty certificates in order to connect now. The easiest way is to use PuttyGEN, which is available at the Putty site ($URL$ Hope it helps! 

I've had a couple, and I've seen some that are pretty doofily made. Whatever you buy, make sure it's got a female USB port (USB-B port) on one end and a female serial (DB-9) on the other end. I've actually seen them where the USB<->serial device had a USB cord. Stupid. The one I use most often is this keyspan: $URL$ 

That's not really a percent. That's the number of processes waiting on the processor. Unless you've got a ton of cores, that's not a good number. Have you tried killing the shell that invoked the 'ls'? 

There are a couple of pieces of hardware around like this: $URL$ But I haven't seen one wholely accepted solution. I've thought about this many, many times while cursing a crash cart that was locked in someone else's cage at my colo. 

IANADBA, but I'm writing a script that will take action as long as the oracle standby database hasn't been activated. What I'm looking for is two queries (or sets of queries, if necessary). 1 - Is the database mounted (as in, has someone done "alter database mount standby database") 2 - How can I tell if the database is activated (as in, "alter database activate standby database")? As I mentioned, I'm looking for queries, but if there's a way to tell in the system, I'm open to that, too. Thanks! Update I took the suggestion below (modified, slightly, because I'm dealing with Oracle 8i, and I get this: 

Use an unencrypted transfer method that doesn't do compression. I'd suggest FTP, given how simple it is to setup and the lack of chatty protocol, like Samba 

I suppose it depends on the volume and reliability you're looking for. Mom & Pop shop with 5u worth of machines? Might as well make them to save money. If you want reliability, then you want homogeny, and you need to buy your servers. I'd recommend against buying from non-primary vendors, unless it's some place like CDW. I buy my network equipment refurbished, not my servers. 

NFS always causes fun things to happen like this whenever UID/GIDs aren't lined up just right. Assuming that your webserver is running as user "apache", make sure that the permissions on the file are such that they're world-readable. su to the apache user and cd to the directory, and try cat'ing the files. It's most likely a permission issue. If apache isn't writing to the directory, it doesn't care if the files it's reading are on NFS or anything else. 

Different layers of abstraction. If you've got a large storage array, you probably don't want one server to use the whole thing, so you divide it into logical units (LUN is actually Logical Unit Number, but hey, it helps me remember). So you've got your storage sliced into usable chunks, and now you present it to the server. In a simple example, suppose it shows up as /dev/sdb. No partitions on it, it's just a disk, so far as the server cares. Why throw LVM on top of it? For me, it's because of growth. If I fill up that disk, I want to be able to add more space to it. LVM makes it easier and (in my mind) less risky, because I can resize that LUN on the storage array (or even create another LUN and present that to the server) and using LVM, I can grow the "virtual" disk without rebooting. I wrote an introduction to LVM here: $URL$ Which I referenced when talking about resizing LUNs here: $URL$ 

This is a stab in the dark, but have you checked that DNS is working ok on the web server? If it can't resolve all of the things it needs to on the first try, it'll time out and go to the second server listed in resolv.conf which may account for the success after first taking forever. 

This probably isn't going to be the prevailing opinion, but I use vanilla vim. I don't use any special features (other than syntax highlighting and line numbering), but it's great, because I can go to any Unix machine and feel right at home with whatever vi they happen to be running. 

Is /dev/sda local or on the SAN? If it's on the SAN, then there's hope. If it's local, and it's using all of the available space, you're going to have to free up some disk space by removing files. Update the question and let us know what the status is. Edit Ah, it's a VM! You're in luck (if there's free space on the host machine, anyway). Create another virtual disk in the VM manager that is as large as you want the freespace to be. Then present that disk image to the VM. Make sure the VM sees it (use dmesg to see if it shows up). Assuming it does, fdisk it, and create one partition. Issue 

I might be going out on a limb here, but I don't think that raw performance is the most important metric when it comes to technologies like this. I think that usability and interface is important, as well as tools to support a reliable infrastructure. It seems to me that Xen has a far more robust set of existing applications that support it than does KVM. That might not be the case, as I have no evidence to back it up. Whatever you go with, decide what the best solution is for you, and look at the entire package, not just the raw performance.