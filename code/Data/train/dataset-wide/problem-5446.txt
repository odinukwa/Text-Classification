It has been done quite a lot of times. Complex numbers for example, as you said yourself. But also every other number set. Once, people found it weird, that they couldn't express "1/2" as one single number, so they just invented 0.5 for that. And then they had debts, so they created -10 as a possible number and so on. The only rule is, that it cannot collide with any of the accepted axioms of math, I think. I, myself not being a mathematician, cannot imagine any problems in actual math that could be solved by creating another set of numbers, but there might be some and if so, maybe someone will someday create another set, conforming to the actual math rules. 

By request of , here are my comments unified as an answer — It seems likely to me that such a logic might not exist, or might be too limited. A possible obstacle.    Consider the classic proof by contradiction of the fact that the square-root of 2 is irrational. Let 

The parent can tell the child of a punishment or reward for their future behaviour (e.g. doing some chores); The parent claims that they are the reason why the child even exists (they are their creator); The child has ample opportunity to observe the physical and economic power of the parent, which to a child is enormous; The parent can express themselves in very clear handwriting with very few spelling errors and a large vocabulary which the child understands but could not easily write themselves; The child cares about the rewards or punishments. 

Imagine there is an allmighty god. Inspired by the question of "what would make you as a rational person believe, that book X was written by God Y", I thought of this: That book must present to me pi, as a fraction of 2 integers (in the decimal-system), which is, as far as I know, impossible. I certainly know that different persons will have different opinions on this, depending on what they define as "God", but the question is, as I think, quite interesting. 

Edited to add, as a preface: I think that I identified what you meant by "complexity" in this case, but it is worth noting that it is perhaps too broad a term without further qualification. I hope that Michael Dorfman doesn't mind me quoting him from the comments (with some added formatting): 

which is not a terribly convenient way to express the same thing, but it is still equivalent so long as we list all Scots, and no person (or thing) which is not a Scot. The ∀ symbol is more powerful in this way — it allows us to express a notion 

E.g. the borders of countries: They are formed by arbitrary "rules": Sometimes rivers and coasts, but mostly due wars and other weird, historical stuff that doesn't really exist in a reality outside of the ideological concept of our imagined self-importance (at least they seem so to me: Nothing in reality ever stops at a border from one country to another, let it be disease or just animals walking around or mountains going through or...). They do not have any basis in reality (contrair to concepts like "humid" and "arid"), they do not give any more value than a more-or-less-random guess, and still, people, all around the world, persist on using these "systems" to "measure" things (e.g. racism, xenophobia, but also "the [xyz-land] culture"). Why is that so? Why do humans create arbitrary systems with no basis in reality just to use these systems do to things with it that again have no better chance in succeeding than random chance (as you might have witnessed yourself if anyone talks about foreigners being stupid, lazy and so on or people belonging to the "race" x being ... and people belonging to the "race" y being totally different). I really don't quite get that. 

You observe, correctly, that just because a formal system S "asserts" its own consistency — by means of a proof which, in a meta-language M, is isomorphic to a proof of consistency of S — does not mean that you should therefore trust S to be consistent. Any inconsistent system which is rich enough to admit Gödel numbering (or an equivalent technique), and which has an explosive implication (so that everything follows from a falsehood), is able to prove its own consistency; although it would be interesting to know whether or not it allows you to derive "consistency claims" without passing through blatant contradictions of the form A & ¬A to do so. The big deal with Gödel's Second Incompleteness theorem is that the only formal systems which can "prove" their own consistency via encoding in Peano Arithmetic (or an equivalent system) — and which is also able to prove that addition and multiplication are total functions — are in fact inconsistent. Even if we knew from first principles that we could not rely on internally proven consistency claims, there is the ironic twist that in fact such "proofs of consistency" are in fact proofs for precisely the opposite. What this really means is that consistency is a bit of a chimeral property of a formal system to have. We are denied even the conceit of self-verifiability in totalizing formal systems. You can of course prove that a formal system S is consistent in another formal system M — but then why should you accept that M is consistent? Proving it so in another system M' is just pushing the problem away a further step. The consequence is that consistency of a formal system is a fundamentally negative property: a failure to be able to exhibit a contradiction, in which case you can never be sure if it is really consistent, or if you just haven't realized how to produce a contradiction in the system. In the end, Gödel's Second Incompleteness theorem says that unless (like Gödel himself) you believe that humans somehow have a sort of occult-ish access to timeless Platonic truth, mathematics is subject to the same epistemological limitations as the natural sciences, in which formal systems play the role of theories and the discovery of inconsistencies play the role of falsification. 

Recently I saw Heinz's dilemma and actually I found that none of the actions or purposes is really wrong or right to me. But I found that punishing (e.g. imprisoning) Heinz (in the case he steals the drug) would be wrong to me. Although my ethics do not answer whether he is wrong, various theories do (some can say it will lead to good consequences, therefore he does right action, and some can say it is violation of duty). But I hardly can understand what these theories suggest to do with those whith peoplr who were "right" or "wrong". Therefore I'm wondering now if the question of punishment (and reward) lies within ethics. If so, what is the ethics branch studying it? 

Is Q ⊃ P equivalent to P ⊃ Q, or does it allow you to infer P ⊃ Q ? The short answer is that they are not equivalent, and neither one allows you to infer the other. There are two easy ways to see this. is a constraint on when P can be true, while Q ⊃ P is a constraint on when Q can be true. In general, these are not comparable constraints; neither one allows you to infer the other. We can make reference to the truth-tables for each, using the table we've already computed for P ⊃ Q to find out the values for each row in Q ⊃ P: 

The statement would be false if he said "The outcome can only be 1 or 2." Would it be lying even in this case? Well, being wrong does not mean lie. Lying implies that the liar himself does not believe in his own words. The person who is wrong because of unawareness or unintended errors (fallacies, acceptance of wrong premises, etc.) is not lying. According to Cambridge Dictionary 'lie' is: 

There are many disasters that would take humanity "off-line" and machines surely can help mitigate or totally prevent them. 

Preamble I would like to make clear precisely what I'm asking. Apologies for the length of the question: please suggest a way that I might make this question more concise. This question is about formal logical systems, which I will take (somewhat fuzzily) to mean a formal system of manipulating symbols, which we suppose to have some sort of meaningful semantics in terms of "truth" and/or "falsehood", and where we have some plausible rules of inference involving &, &vee;, ¬, etc. in their familiar roles as logical connectives. That is: fragments of this system at least are clearly intelligible as representing logical reasoning — there is a transformation which would allow us to obtain a string similar to P &vee; Q from either P or Q, either of which we could obtain from P & Q, and so forth. For a typical paradox X of logic, we shall say that a logical system represents the paradox X if we judge that the formal system can capture the syntactical elements of the paradox; and we shall say that it suffers from the paradox X if the logical system is inconsistent (we may derive absolutely any well-formed formula) essentially as a result of the fact that it represents X. We will say that the formal system is (or seems) robust against X if we cannot demonstrate that it suffers from X. A simple formal model of self-reference I want to consider formal logical systems in which we may represent the Liar Paradox, specifically in order to specify a way in which to treat the Liar Paradox as a feature (or a bug) of a formal logical system. Consider a formal system in which propositions (i.e. strings of symbols) may be referred to by name. The names are labels which are allowed as propositions in well-formed formulae. The semantics of these labels being "names" arises from the fact that they are addressible in a straightforward way: the rules of inference allow for the name A of some proposition P to be substituted with the proposition P itself. (We might naively say that A ≡ P, though technically this would only be a tautology if we also allowed the substitution in reverse as well. We may consider systems in which this is allowed or not; I only assume that "expansion" is a valid transformation.) The Liar Paradox We consider a proposition ¬ L (which is well-formed in this system) which we then give the label L. This is then a simple and formalized realization of the Liar Paradox.* The classic question is what to make of L: is it true or false? In a formal system, the question is instead whether our formal system "suffers". What makes the Liar a "Paradox" is that classical logic does suffer from it. Consider a typical formalization of classical sentential logic. If we grant the Law of Excluded middle, we have