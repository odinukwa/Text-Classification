Interfaces show as down when you don't have any active devices connected to them. When testing you tend not to have things connected to all the interfaces right? 

Not sure exacrtly what you mean by "setting the share permissions removed the local folder permissions for the IIS user" (did the permission disappear, or just not get applied as expected?), but you should understand that Windows applies the most restrictive permission of the NTFS and Share permissions set on the object. Perhaps this explains what you experienced. The Microsoft Technet article on Share and NTFS permissions in Windows 2008 suggests: 

Use for each of your vhost directives. Then specify the for each vhost directive. If you do that then you don't need to set up for each vhost, although that will work too. I'd be interested to see if anyone has any info on why one method is better than the other. You can also use if you want to give an alternative hostname for the same vhost. See $URL$ for more. 

I don't have enough points to comment yet, but I'd like to add the tip of pulling some fishing lines through the conduits to help pull cables at a later date - in my experience, cables have a tendancy to get caught on the ridges in flexible conduit. Being able to tie a new cable to some fishing line, then go to the other end of the conduit and pull the cable through is a big time saver. When you pull a new cable, tie some fishing line to that, so it leaves a new line for the next cable, and so on. 

DHCP requests are limited to a broadcast domain - i.e. subnets delimited by routers, unless you have something like IP helper setup on the router. So using some kind of network level discovery method, you will never know for sure if you got them all, unless you connect to every subnet. You could use Wireshark or Network Monitor to view the network traffic in each of your broadcast domains - every DHCP server in a broadcast domain should respond to a DHCP request, even though the client will only handle the first response it recieves. 

Where is your web server? If your web server is on the same local network, sitting behind the same firewall then you don't have a problem. If your web server is on the internet, then you would be sending your password in plain text across the internet to the web server - there are plenty of places it could be intercepted in between. One special case here is if you have a VPN connection between you and the web server, in which case the password would be encrypted across the internet, but I imagine you don't have this. Any web host worth its salt should allow you to use SFTP or SCP to transfer files securely. Frankly, any web host who only offers FTP for file transfer is a liability to itself, its customers and the commmunity at large. 

Where is your server located? If it is the same location (network wise that is) as your testers so has a local IP address as well as a public IP address, then you could have the server listen on a local IP address in the same range as your testers, and use local DNS to resolve the live site to this IP address. You will still need to edit your vhosts for this, but frankly I don't see any way of avoiding that given what information you have given. 

I have an ESX 3.5 update 5 cluster of five host servers, all fully patched as of this Friday. Today I noticed that one of the servers has the Hardware Health status as unknown in Virtual Center Infrastructure Client. When I look at the Health Status view under configuration for that host, all the items are status Unknown. The server is exactly the same configuration as the others - same model (HP DL360 G5), memory, NICs etc. I have tried restarting the management service with but this has not resolved the issue. Asides from this, I am not seeing any issues with the cluster - however, I hate having a blind spot like this. Any ideas? 

It depends on how the alerts will be handled. You want someone to notice when a threshold event occurs. If they are generating emails that will not be ignored, or you know that someone is regularly checking the Nagios trends, then it is probably ok to just use the standard Nagios behaviour. If you don't think that will get anyone's attention, then you would want the service status to be "sticky". I'm not aware of any Nagios checks that have this behaviour, but I'd be interested to know about them too. 

However, what I want to do is be able to return a table, or csv file of all in the profile, So far I am only able to get specific properties (see answer below). I have tried piping the output to and but this just returns blanks. I feel like I am so close. I don't want to replace the call with lots of calls and it doesn't feel like I should have to. Any ideas? 

Do you have any experience with any of the distro's you list? If so, go with what you know already. If, on the other hand, this is your first foray into Linux servers, then Ubuntu or CentOS are probably better options in my opinion. Why? In my experience the documentation available for those distros was more consistantly approachable than Debian. I started out trying to learn Linux using Debian (about 15 years ago), and I went around in circles for a couple of weeks - I needed to understand x in order to understand y in order to understand z in order to understand x. Things may have changed but since Ubuntu and CentOS are both backed up by large businesses (CentOS being more or less the same as Red Hat Enterprise Linux) there are clear documentation paths, and books you can buy that take you through step by step. Once you've got either of these, you can delve into Debian with confidence. I doubt you'll get any benefit from 64bit unless you have more than 4Gb of RAM on your VPS. I wouldn't pay extra for this. The beauty of a VPS running Linux is that once you have it set up, you can upgrade, or migrate your config and data to a new VPS that is more appropriate. Start small and simple and work your way up. 

The file has probably been checked out by someone (particularly since the Doc library is set to force this), which in many circumstances hides the file from everyone else till the file is checked back in again. You can manage checked out files via the Document Library settings page - as an admin you can force the files back to checked in, although you might want to check with the person who has it checked out that this is appropriate. 

I am working on an SCCM project that requires powering on computers in the middle of the night to run updates. SCCM can send the WoL packets, but I am struggling to figure out how to pass this across routers. Our core router is a Dell PowerConnect 6248. I see from the docs that the 62xx switches support forwarding directed broadcast, which seems like a good start. However, having this on globally (which seems to be the only option) exposes us to some attacks. Would the best way of setting this up also include setting up ACLs on the egress interface to only allow UDP port N traffic from host IP to leave the interface? Is there another way of achieving this? NB - I already looked into IP helper, but this only passes broadcasts from a subnet to a specific IP address, wheras WoL is the other way around. Example: We want to pass WoL traffic between 10.0.0.0/24 and 10.0.1.0/24, which are separated only by a router. So the computer sending the WoL packet is 10.0.0.1, router is 10.0.0.254 and target could be any computer in 10.0.1.0/24 (specified in each WoL packet). 

Given it is a home server, perhaps you can find a way to shut everything that is writing to the disk off, then try and recover raw data directly from the disk. It's a long shot, but the ones and zeros making up your data may still be on the disk. I don't know off the top of my head any software that would help. OnTrack Data Recovery claim to be able to recover VMWare virtual disks, so perhaps they can help. I guess it depends how much they charge, and how much you want your data back. But to be in with any kind of chance, you need to stop using the physical hard disk the data was on immediately! 

The client can perform Path MTU Discovery, which is the smallest MTU on the complete path between two devices, and then reduce the MTU to match. This is particularly necessary for traffic that shouldn't be fragmented, such as IPSec. As Shane Madden mentions in the comments above dropped ICMP packets in the path will break this. I suspect there are some switches on the market that have all ICMP blocked by default, so I would check to see whether any new switches or routers were introduced and whether they need to have ICMP traffic enabled. 

Powershell can be used to manage DNS, via WMI (in which case you could also use VBScript if that is your bag). Microsoft Scripting Guy blog has a detailed entry on this. It includes an annotated script example that can be used for exactly what you describe. 

Maybe your server is compromised and is sending out spam. The CBL will tell you why you were listed from a link at the URL you posted - what do they say? Take a look at the headers of the last time a message made it through. This may give you an idea of how the mailserver you are receiving the mail through is treating it, and may give you some further hints as to what is marking it as spam. 

It depends on the CPU? If they are the same architecture (e.g. both Intel, both 64 bit) then you should be ok. I understand that some versions of Windows use a different kernel for single core than for multiple core although that may be less of an issue (I've seen a VM created for 4 cores have 3 cores removed and still run fine). So if you aren't making a radical change to CPU, and the rest of the server has the same hardware, I think you may be ok. But if the data is valuable at all, be sure to backup before trying this. If you only want to maintain the data between servers, you could install an additional disk, and then put the OS on that. Then just get the data from the other disk. Disks are cheap. Installing Win2k8 R2 doesn't take that long. Work out the costs against the cost of recovering your lost data! Update Missed the bit about RAID when answering this initially. Would be interested to know if this affects things in two cases: 

This gets me a bit closer. I'd still really like a script that iterates through all the profile properties and puts them into a CSV or XML file more gracefully. This will do for now. 

Restarting will achieve a lot of things, most of which can be achieved without restarting. One of the things that restarting will often do is clear the logs in memory, so you won't know anything about the problem after you restarted. So if you are seeing this behaviour frequently, rather than just blindly restarting, take a look at what is going on with the switch (assuming that is possible with the switch - it may not be if the switch is consumer grade, or the problem is really serious). Knowing what the problem actually is will help you fix the problem, rather than just working around it. 

This prompts for the password, then throws up the UAC dialog, and then opens the app. Is there a way to do this that bypasses the UAC dialog (without just turning off UAC altogether)? 

The PHP manual specifies where you can change each directive. As far as using ini_set or .htaccess you may be out of luck, as according to the docs: 

I have a number of VPN sites where the MTU is lower than standard (1500). I have had at least one site where fragmentation of packets has had an effect on the success of building an IPSEC tunnel. I am able to set the MTU on the equipment at the remote sites. However, at head office I wouldn't want to set the MTU to the lowest common denominator. Is there a way of setting an MTU lower for traffic destined to a specific IP address? Is fragmentation something I need to worry about for functioning VPN connections? Is it worth addressing this where I don't have problems? HQ equipment is an ASA 5510. Remote sites have ASA 5505. 

I can't find any documentation for what or , but I'm pretty sure it means that the IKE handshake has failed for some reason. Can anyone suggest any likely things that might be preventing IKE from succeeding, or specific details of what means? Update: We connected the ASA at the site of a customer of another ISP. The VPN connection came up immediately. This confirms that the problem is not configuration related. The ISP is now accepting responsibility and investigating further. Update: The connection suddenly came back online last week. I have notified the ISP to see if they changed anything, but not heard back yet. Frustratingly I am now seeing a similar issue on another site. I found a Cisco doc on the effects of fragmentation on VPN. I am starting to think that this may be the cause of the issues I am seeing. 

You need DNS, and probably DHCP running somewhere - preferably not on the same hardware as your Domino server. You could use Linux or Windows for that. It depends on what you are familiar with, and what your goals are. If you know how to do the essentials in Linux then fine. If you only know how on Windows, then you may make any cost saving using Linux as you will need to learn those aspects of it. But maybe you want to learn Linux :-) 

Your Apache config looks ok to me. The DNS record for mydomain.gov.br should point at the same IP address as the www. one, unless Apache is also listening on this IP address - but if it were then your site would probably work. You need to check your DNS service to figure out where the record for the www. free record is being defined. 

If you have console access to a Cisco Router you can use Cisco Password Recovery techniques to get in, then dump the cleartext config file via TFTP - you then have access to cleartext passwords, or passwords that are trivial to decrypt. So you do need to factory reset. If you haven't got the login details for the routers (in which case you can probably get in via SSH over a network connection), then you will need to use the above techniques to get in, then issue the command to drop the existing configs. 

This works fine in my own environment, but I am planning on releasing it for wider use, and want to make sure it validates correctly for others. 

RKhunter, Tripwire etc are great, but really only of benefit if they were installed before the incident - this is because they are great for detecting whether key files have been changed. If you install RKHunter now and run it, it will detect the inclusion of many rootkits, but it won't detect any backdoors an attacker opened up in the OS or the applications you use. For example, you could sneak onto a computer, create a new user, give them SSH and sudo permissions, and then clean up afterwards leaving a legitimate looking config in place, and no rootkits - then come back later and do your evil. Best thing to do is look at what ports have services listening on them, then look at the configuration of all those services and make sure they are all legitimate. Then look at your firewall configuration and lock down ports that you don't need, both in and outbound. Then install RKHunter etc to see if some script-kiddie dropped a root kit in there messily. To be frank, it is probably less work to do what JJ suggested and rebuild than making absolutely sure the computer hasn't been compromised. It's the data that is valuable, not the OS and config (apart from the man hours in setting that up). You'll never be sure it wasn't cracked by someone smarter than you. 

You have set up the DHCP server on the machine, but not the DHCP client. So your server can give DHCP addresses, but it isn't asking for an address. In most cases you wouldn't want your DHCP server to also be a client. Unless you know what you are doing, you probably don't want two DHCP servers on your network, so think about what you are trying to achieve here: