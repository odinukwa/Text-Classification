Run with header. This will tell you MAC address of the originator of the scan. Ping 10.51.1.15 at various times and see, if the arp antry corresponds to what link layer headers held in the tcpdump. If you have a managed switch, then you can log onto it and see from where do the frames come. This will tell you the socket, into which the scanner is plugged. This method will work also if the scanner really wants just to flood your ARP cache. 

I have no experience with snapshot limitations, but VMware document What Is New in VMware vSphereâ„¢ 4: Storage has the following entry: 

You de-thinprovision, so to say, the machine, when you copy it off datastore using VMware GUI. Take a look at this VMware Communities thread for a solution. 

Well, it says in your logs: USB disconnect. Could be a loose cable/connector or maybe ovecurrent on USB port if your backup script causes a lot of activity. If you want to be absolutely sure it's not filesystem-type related you could re-format the external drive and see how it behaves with ext4/3, NTFS or VFAT. 

Run run then ask each of the name servers above you what is their opinion about name servers for your.domain.com. To give an example with att.com: 

etc Can you log in to the NAS and monitor its performance too? One case I encountered was a NAS spawning multiple NFS daemons and dying under the load when a client connected. 

HDD transplantation will work. The only thing will be network cards. If you have more than one you'll have to identify which interface corresponds to which ethX device. If you identify them by MAC address, you'll have re-do network configuration. 

I think it may be OS-dependent. On AIX 6.1 you have ($URL$ "(cs is) Number of context switches per second observed in the interval. The physical CPU resource is subdivided into logical time slices of 10 milliseconds each. Assuming a thread is scheduled for execution, it will run until its time slice expires, until it is preempted, or until it voluntarily gives up control of the CPU. When another thread is given control of the CPU, the context or working environment of the previous thread must be saved and the context of the current thread must be loaded. The operating system has a very efficient context switching procedure, so each switch is inexpensive in terms of resources. Any significant increase in context switches, such as when cs is a lot higher than the disk I/O and network packet rate, should be cause for further investigation." If you have sources of the vmstat on your system you may look inside and try to find out what it does. 

Read the vendor documentation, from the reference architecture to design and planing, to installation guides. 

Here is a very quick-and-dirty description of how DNS system works, and here is a bit longer explanation. Before you start tinkering with DNS try to grasp how the system works. Depending on how your systems are set up you could just add Server 1's IP address (possibly as an alias) to Server 2's network card and be done. If it won't work for whatever reason (e.g. routing), you have at least 3 possible roads: 

mysql-4.1.20-2.RHEL4.1 in the output of rpm -qa means, that you have this package installed. You can read the description of the package by running yum search mysql on my RHEL6 gives following short descriptions: 

For monitoring you may try to use monit -- it should be able to restart a runaway server, if you put it under its control. As a fast-and-dirty solution you may put something like 

That's a bad idea. SSD drives give you insane iops, and that's why you pay for them: to hold data that's accessed most frequently in a random fashion. A database on a set of SSD drives can outperform (especially for read-intensive scenarios) a very expensive storage system. Operating system boots, then practically all executables that are used are kept in RAM, everything unused is put to swap. If you plan for heavy swap use (bad idea) you could put your swap on SSD, but I'd rather use a HDD and put the difference into RAM. 

Put the commands in a script, run the script from within the . Use subshell (i.e. put the commands inside ) Use as the command to be run by 

The "far" layout may give you better read performance (see $URL$ Apart of that RAID1 with md gives redundancy, but no speed boost. Edit: For read performance play with readahead (i.e. increase it. A lot.) . It can work wonders. 

Run iometer in your virtual machine. With just two 7.2k rpm drives random access is going to hurt you. You can get only so many iops from them. Try running two scenarios with iometer: 1) sequential read/write -- this should give nice, fat numbers. 2) random access to the drive -- here you should be in for a land of hurt. Setup a file for tests large enough to force it to be pushed out of cache of the virtual machine. 

At the end of your file so that values are overriden only for the group (you must use name, not numerical ID). If the client does not respond for more than one message sent every 30s, the client will be disconnected. You can fiddle with numerical values to your satisfaction. Seems to work on my RHEL 6. 

Looking at the answer, it looks like either you asked a non-recursive query. Or the name server does not do recursive queries. Both cases would be strange. What is a recursive query? When you ask for www.good-morning.se, then your browser must ask the name server of the domain good-morning.se for the address of the host www.good-morning.se. However, to know the name of the name server for good-morning.se domain, it must ask the name server for .se domain what is the name server for good-morning.se domain. A recursive query goes through all these levels in one go (you ask the server to just do all the work and give you the final answer). Non-recursive query does just one level at a time. The answer from your post contains addresses for name servers for .se domain, so it looks like either the question was not recursive, or the server didn't do all the fetching it ought. I have no idea why either of these could happen. You could use e.g. wireshark or tcpdump to see what kind of query is being sent, then you either have to figure out what is wrong and either correct configuration on your side or have a chat with support of your ISP. PS More on recursive and non-recursive queries see e.g. here or use Microsoft. 

Practically all modern Linux installers will detect presence of your data drives RAID and an LVM group on them. You'll need just to choose mount points for logical volumes and remember not to format them, which, at least for Fedora, is the default. If you decide to disconnect your data drives during installation and reconnect on OS boot, your RAID and LVM should be auto-detected too. You'll need to look up devices created in /dev/ and edit your fstab accordingly. 

To the best of my knowledge TCP/IP stack will short-circuit any communication between any local interfaces. Therefore no package will ever be sent or received over the link. 

For me the primary benefit would be capturing of oops/crash logs. With the likes of ILO you are loosing whatever rolls out of the screen. Serial console allows you to collect the whole thing and record it without using a camera. 

A stab in the dark: Your Samba server is in a Workgroup group. Your XP client is in an AD domain. This may cause problems. Do you have some entries in Samba logs when the client tries to access the share? Does it ever contact the server? At what stage the negotiations between the client and the server break? 

Performance characteristics I'm going to assume RAID 10, because in my opinion it has all advantages and no downsides in your scenario. You are going to be limited by a performance equivalent of a pair of HDDs. In other words you are going to be able to serve/write data at about 2x what a single HDD can do. For streaming you should be able to saturate 1Gbps link with no effort (reading or writing). For bursty data you are stuck with ~150 IOps (assuming 7.2krpm SATA drives). RAID 10 will guarantee that the load is spread among all drives for all I/O (unless you are unlucky enough to have application access data with a stride matching your RAID chunk size), and RAID 10 "far" layout should give you similar performance no matter which region of filesystem you are accessing. Lost drive means marginal loss in read access time (you loose the "far" layout benefit for the affected mirror pair). If you expand the storage with another pair of mirrored drives will not be able to reshape the data to re-stripe it over the new space. Effectively you have a RAID10 + RAID1 setup, unless you backup, re-create the array and restore. 

There's no such a thing as a Grand Unified Database Layout. If there are custom questionaries, there, really, need to be custom tables. Otherwise you are on a quick path to a single-table-of-200-columns of VARCHAR(128)-with-no-primary-keys monstrosity out of thedailywtf.com, which is inefficient, unsupportable and will hurt you in the future. Sharding, as recommended by toppledwagon may be a thing to consider, but first, double check, that your database is rationally designed. If it is not normalized, then have a very good, preferably backed by testing, reason, why it is not. If it has hundreds of tables, it's probably wrong. If it has single table, it is definitely wrong. Look at the ways you can divide your problem into independent sets. You will spend more effort up front, but the system will be better for it. Million rows, with, let's say, 2k of data per row (which seems a lot of characters for a survey), is 2GB of memory. If you can throw a bit more hardware onto your problem, maybe you'll be able to keep your data set in RAM? Which leads to the next question: What's your load in absolute numbers? Customer requests per second, translated to I/Os per second, divided into reads and writes per second, how many gigabytes of data, with what growth rate? How does your load scale with number of requests? Linearly? Exponentially? You don't have to publish your data, just write it down and think about it. What is it today, how do you think it is going to look in a year or two. Wikipedia says a 15k rpm SAS drive will give you 175-210 IOps. How many do you need in RAID 10 to satisfy your current and projected load? How big is your data set? How many drives do you need to fit your dataset (probably a lot less than to meet the IOs requirement). Would buying a pair (or a dozen) of SSD be justifiable? Is local storage going to be just OK, or are you going to saturate two 8Gb fiber links to a high-end storage subsystem? If currently you need 1k IOps, but have three 10k rpm HDDs in RAID 5, then there's no way your hardware will be able to satisfy your requirements. OTOH if your app has a user request per second and brings a 32 core 256 GB of RAM beast, backed by an enterprise-class storage to its knees, then chances are the problem lies not within hardware capabilities.