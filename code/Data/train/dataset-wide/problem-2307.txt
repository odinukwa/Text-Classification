Why is this happening? P.S.: I should note that Horde_groupware database have innoDB tables, when everything is messed up and I I get the error says bad information in .frm file. 

I have enabled SSL on with optional SSL connection: . I have used in order to obtain SSL certificates. SSLs works as expected and in file I can see that: 

Why it reports that ? Is there something in between that I have missed? Could someone shed some light on this? 

I have a unique compound key like fr(fromid,toid) in the table, when I run the query with explain I get the following result: 

I have used to generate SSL certification. All the private key, chain and certificate is generated by `let's encrypt. Now to use it in I first merged and into a file called : 

When we index a timestamp field like timetime. Can we use it for or is it just usable for OR ? Does mySQL use this index when doing vise verse ordering? 

I want to fetch those users that is not in a specific user's circle (Like G+). It means that user has not yet added those people to his/her circle. I tried the below query but it was empty: 

An easy php based web application to produce very realistic data to populate your table. Its online site has limitation, but if you download the source code and install it on your PC then you can generate every kind of data that you want. It supports csv,sql,html,etc file formats. 

I want to fill this table with lots of data. Let's say millions of records. How to do this? I need to produce unique emails. Different data for emails. I know I can use but don't know how to produce random data 

When operations are slow, and a user that has many records take so long it has domino effect on all the other operations after a few seconds. When I explain the query on the large collection I can see the result that it has used an index: 

But in the query I get the result I want, the part is not . Is there someone who could explain this and shed some light on the subject? 

Actually they don't do the same. TDE encrypts the data in the database, you need to configure the wallet and have it open before you can see/use the data after that. VPD is an Access Control Mechanism, it allows you to define sub-datasets that will be owned by different users, allowing them to see/use only the data that they actually own (even if the same set of tables are shared among different VPDs) To have more detailed information about each tool, please review the documentation on $URL$ 

you need to set the search string in ASM. Failing to do so, will result in the gv$asm_diskgroup view to be empty. Tomorrow morning with a clearer head and a desktop to give you exact commands I will edit and extend this answer. 

To complete some of the previous answers: A commit will end the current transaction in the current section. It will guarantee the consistency of the data that was "touched" during the transaction. A checkpoint writes all commited changes to disk up to some SCN that willl be kept in the control file and datafile headers. It guarantees the consistency of the database. 

In Oracle every result set have an implicit row number that you can use for limiting output. If, by any chance, you're using oracle 12c you can use the brand new feature for top n queries 

EDIT 1 As some of the comments point out that the concepts aren't completely clear I will provide some more information here. We have 3 major structures involved in the commit and checkpoint concepts. 

A final piece of advice: Set the db_create_file_dest to point to the ASM DATA disk group and remove the datafile name parameter from your command: alter tablespace CWSC add datafile size 100M AUTOEXTEND ON; As you may know, even if you give a name to the datafile, when it's created on ASM, that name becomes an alias and the real filename is set by the ASM instance itself. I hope this info helps you. 

So changes are made to blocks kept in the Database Buffer Cache (DBCache). Once commited, the changes are pushed to the Redo Log Buffer (RLB) which is dumped on a regular basis to the Redo Log Files (RLF) and, eventually to the database storage files (DF). Also on a regular basis and not completely unrelated to commit, the checkpoint process dumps the dirty blocks from the DBCache to permanent storage DF. During the checkpoint process the SCN associated with the latest DB block written to storage is written on the DF headers and the control file. That will be from that moment on the latest consistent estate of the database. 

As the official documentation says, unless you specify failure groups explicitly, ASM will place each disk into its own failure group (it doesn't have any knowledge about your HW infrastructure). Note that Oracle doesn't recommend using an OS LVM with ASM. 

So to go beyond the indicated scn, we search for archived logs that contain scn 2475373 + 1. If you want to verify manually, check the following MOS Note: How to determine minimum end point for recovery of an RMAN backup (Doc ID 1329415.1) 

Let's assume charater_set_system and character_set_server are different. My first question: I suppose that when I'm importing an export files generated by mysqldump the variable default-character-set has no effect as set names is always issued at the very beginning of the export file. Am I right or there could be corner cases? The second question: What about manually running sql scripts? The documentations states that some characters may be displayed incorrectly. How can I be sure that the data is imported correctly (a part from running an application test i.e. by trial and error)? 

I would suggest separate instances for your critical production databases and consolidation to a single instance only for the small and less important MySQL databases. In a consolidated MySQL environment, a single instance with n databases, any database could potentially impact on the others. Additionally, it would be better to configure appropriately at least the critical databases and that will be easier when you're using separate instances. Another important point is backup: different storage engines need different backup methods: if you use only transactional storage engines, you could avoid locking during the backup (xtrabackup, MEB, --single-transaction for mysqldump), non-transactional storage engines will require table lock during the backup. You could manage this by using flexible backup procedures, but if you need to generate a consistent backup for replication slave provisioning, for example, it would be more complicated if you have mixed engines across the databases. 

As always, I would start with the Oracle documentation. You'll also need a virtual test environment, check Tim Hall's articles, they are really helpful. 

I'm trying to understand when I'm supposed to set/force default-character-set for the client. The documentation states: 

In my recent project which is about social networking for an Asian country, I'm in doubt whether I use the below SQL statement for counting records: 

Problem solved: It should be an unknown option as I've put it below [mysql] not [mysqld]. I put all the parameters below and I went through the process again and now it works just fine. files are in their respective database folder. 

As option separates table files instead of putting all data and indexes of DBs into one ibdata file, is using this option improve speed of alter table? I have a table of 40M rows and when I alter a specific table it takes about 5 to 6 hours. Does this solution help? Is there other ways around to improve alter table speed on heavy tables? 

: Number of times the operation acquired the lock in the specified mode is so high compared to the fast query (on another collection) that has the below status: 

Now users in the name of who has been added by uid=60 should not have be shown! The result of this query is empty. I can't figure that out, what I'm doing wrong? 

It's good to note that I have done the same exact procedure on another server and it went all ok and MongoDB works as expected. Now my config is as below: 

I've read in an article about social networking database schema and saw that they've used varchar for those columns that I mentioned in the question title. It's not normalized! Isn't it better to have a table for sports and then put the foreign keys in user's profile table? In this case we'll have much less redundancy? Please correct me if I'm wrong. Which approach is better have another tables for fav_sports,fav_videos,fav_music, etc or put them all in user's table? 

I want to insert about 14000 records in a junction table, but the problem arise when there is a duplicate key for unique(iq_id,q_id)? what do do? 

I have DB with about 30M records in a collection with about 100GB collection size (total documents and indexes). I have a compound index that filters data based on user_id and some other fields like: , , etc. With I see slow queries of about 10s, 20s or even 40 seconds! I ran the exact same query and result is fetched less than 500ms (though it may get cached on second try). When I get that ongoing stat, I see the following lock status: 

The SCN (System Change Number) is kept in the control file and the headers of each datafile, uit allows the database to know which datafiles are in sync and where the database writer DBWR has to perform the next writes from the database buffer cache. Each backup also is "tagged" with a SCN (and thread-sequence) to allow the RMAN process know the exact "time" those were taken. Hope this helps. 

That will remove the fragmentation and shrink the associated datafiles plus it will set the HWM in the "lowest" possible position. CAUTION: This is a n I/O intensive operation, never do it during business hours or outside a maintenance window for production environments. Another option that can be used is the creation of a "backup as copy" of the fragmented datafiles and then switch the database to point those "image copies". Then drop the original and redo using the image copy as the original and backing them up "as copy" to the original location. It's a little longer and some more complex, but the "downtime" is just a few seconds while the switch is performed. Refer to the Oracle Database 11.2 RMAN Reference Manual for more details on this option. Hope this helps you. If not, please add more detail to the question. Like version of the software, maintenance window time, accepted downtime, etc. 

If there is no backup and no way to restoring the lost datafiles, what you can do is backup any other important datafile/tablespace and recreate the database. I think it will be the less painful way to get a fully working database. 

Prior to resizing, you have to remove fragmentation in the datafile. That means every segment (sets of blocks assigned to each object in that tablespace) needs to bring together all rows inside their db blocks. For this you have several options: If you're on 11gR2 you can use 

Due to the bad naming convention on your question, I will use ColumnA as A, ColumnB as B and ColumnDate as Date. I am assuming that you require the first item as it's fetched from storage, no particular order. And you have to know that if the row gets updated it can be migrated from the current block and so, the query result will eventually change. This queries are examples only and you have to edit them to get exactly what you want. You may try the following query: