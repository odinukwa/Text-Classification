For each hard disk attached to vm in hyper-v it's possible to set a maximum number of normalized iops. By default normalized iops size on Hyper-V is 8K. Is it possible to change this normalized iops size in some way ? Maybe using registry setting ? I am aware that it's possible to change a normalized iops size when using Storage QOS Policies with SOFS/CSV/S2D, however I am looking for a possibility to change a normalized iops size without using any of these. 

Login to your account and verify mobile phone number or other verification method required and try to send email again. 

You have not described what you mean by "neater way" however simply to get a list of block devices is to list them: 

S1 and S2 have network shares accessible from each other and desktop client. S1, S2 and desktop client have full network connectivity between each other. S1 and S2 have hard disks attached directly without any third party storage provider. Desktop client wants to copy a very large file (2-3TB) from network share on S1 to network share on S2. Is it possible to use some technology like ODX for the file to be directly copied from S1 to S2 with a command issued as a copy from desktop client. I am aware that since MS Windows Server 2012R2 an ODX is supported with a third party storage provider which supports this technology. However I'm looking for a solution which would provide same effect but with just normal MS Windows storage. 

First one (windows container) is a classic container which shares the same MS Windows Server kernel and not very secure. However second (hyper-v container) uses hypervisor to provide isolation and security. Sine release of Windows 10 Anniversary Update few months back Microsoft has included “Windows Subsystem for Linux”. Is it going to be possible to run Linux (Ubuntu, Debian, CoreOS, etc. ?) containers using the Hyper-V Container technology ? I have searched everywhere but cannot find an container image with Linux for Hyper-V Containers. Only two available which I'm able to find are Nano Server and Windows Server 2016 which are provided by Microsoft. Using Technical Preview 5 (TP5) of Windows Server 2016 now, the general availability (GA) is going to be any day soon as announced at Ignite conference few days back. Anybody got any news ? able to verify ? 

I cannot find a definitive answer anywhere (lots of contradicting information and very general). With the new version of MS Windows Server 2016 it's going to be possible to run containers in two modes: 

For Two-Way Mirror and Single Parity with write back cache you cannot use single drive in the SSD tier. $URL$ Simple answer: three times no :-) 

Older versions of MS Windows had BITS Service exposed on WMI in "root\Microsoft\BITS" namespace. However since MS Windows Server 2012 this namespace is gone. Is there any "new" way to control BITS using WMI ? I was not able to google or find any namespace which has BITS exposed. If not, is there any other way to listen to BITS events ? I would like get events on BITS session start, complete, error, etc. instead of polling this information using C++ app and .dll 

Next, add a bridge0a.conf and bridge0b.conf to /etc/openvpn/ together with a shared key. The files are the same for a and b, except for a different port (for example, use 3002 for b). Replace 11.22.33.44 by your server's public IP. Client: 

I am currently living in a country that blocks many websites and has unreliable network connections to the outside world. I have two OpenVPN endpoints (say: vpn1 and vpn2) on Linux servers that I use to circumvent the firewall. I have full access to these servers. This works quite well, except for the high package loss on my VPN connections. This packet loss varies between 1% and 30% depending on time and seems to have a low correlation, most of the time it seems random. I am thinking about setting up a home router (also on Linux) that maintains OpenVPN connections to both endpoints and sends all packets twice, to both endpoints. vpn2 would send all packets from home to vpn1. Return trafic would be send both directly from vpn1 to home, and also through vpn2. 

If everything goes well and the line is good, you will see four replies for every ICMP package: your packages are duplicated on the local side, and the replies to these two packages are duplicated again on the remote side. This will not be an issue for TCP connections, because TCP will simply ignore all duplicates. This is an issue for UDP packets, as it's up to the software to handle duplicates. For example, a DNS query will yield four replies instead of the expected two (and use four times the normal bandwidth for the response instead of two times): 

Create a /etc/openvpn/tap-up.sh script (and don't forget to mark it executable with chmod a+x tap-up.sh): 

For clarity: all packets between home and the HTTP proxy will be duplicated and sent over different paths, to increase the chances one of them will arrive. If both arrive, the first second one can be silently discarded. Bandwidth usage is not an issue, both on the home side and endpoint side. vpn1 and vpn2 are close to each other (3ms ping) and have a reliable connection. Any pointers on how this could be achieved using the advanced routing policies available in Linux? 

Don't forget to edit /etc/defaults/openvpn to make sure your new VPN configurations are started. Reboot you machines, or load rc.local and restart openvpn manually. Now you're ready to test your setup: 

I used the answer provided by @user48116 and it works like a charm. The setup is actually quite easy! NOTE: I implemented this with two connections to just one single server, as this already solved the problem for me. If you want to try a setup with two servers, the easiest way is probably to use port forwarding to forward the UDP port from the second server to the first, and use the same recipe as described here. I have not tested this myself though. First, make sure you have a 2.6 kernel with bonding support (default in all modern distributions) and you have ifenslave installed. Next, put this into your /etc/rc.local or any other place you prefer, but make sure it's run before openvpn is started (because it will try to bind to bond0): Client: