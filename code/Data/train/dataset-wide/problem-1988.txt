Backup the merge replicated database(s) at the publisher and subscriber(s) before proceeding. Script out merge replication from the publisher server and store the create/delete scripts safely in another location. Obtain the list of published articles and its existing triggers for reference in later step by running the query below on the publisher and subscriberâ€™s merge replicated database and storing the result somewhere for the time being (ie. In a text file) 

Also, you should monitor Wait Statistics on the server especially SOS_SCHEDULER_YIELD resulting in scheduler contention on Servers having multiple CPUs running concurrent Bulk load operations and competing for the same CPU Cycles. Refer to this excellent whitepaper The Data Loading Performance Guide which have everything that I mentioned with diagrams and examples. Also, to automate the sliding window technique - creating staging tables, loading data into it and then switching the partitions, you can fully automate it using SQL Server Partition Management -- it has command line option as well -- available at CodePlex. 

SQL Azure database does not support replication. See SQL Server Feature Limitations (Windows Azure SQL Database) 

There is no need for doing it, unless you are keeping track and history of all sql server restarts. You can easily find sql server start date using DMV (provided you are running sql server 2008 R2 SP1 and up) 

I would suggest you to take a look at Redgate's schema compare. Its worth the investment. Note: I have used Redgate's schema and data compare and they are life saver. Alternatively, you can use SQL Server Data Tools (SSDT) 

During Hurricane Sandy (natural disaster) and recently when moving our data center (managed failover), I was in your situation. I have implemented Logshipping and was able to efficiently failover during the above scenarios. The technique that I used was reverse logshipping (swapping roles between primary and secondary) - which allows you to swap the log backups. Below steps can be scripted with Powershell or TSQL script (I used sqlcmd and xp_cmdshell) FAILOVER Process: 

This example does not check whether any values has actually changed. If we ignore -checking and sane fallbacks for a moment, this could be checked in one of the following ways: 

Now, say I want to log different types of events on the -object. I'd make a log-table called and and store events there. These tables would look like this: 

At this point, I would need to store the textual old/new value of the company name, as well as the id of the recently added user.You might see that I'm already headed off in the wrong direction, and this is where I ask for help. I have 2 questions: Should I just use -datatype, or if this is considered poor design then what would be a sensible way to store these log-events? Thanks in advance. 

There are two reasons that prompts me to ask this question: tSQLt The T-SQL testing framework tSQLt considers it an issue of "High Severity" when there exists columns with a non-default collation. The author of the test states the following: 

In Example #1, the SET-operation will update all columns, even if only 1 column has changed. In Example #2, the SET-operation will update all columns for all rows, falling back to the old value if the value is unchanged. In both examples, all columns are hit by the SET-operation, and, according to my seniors, this creates an unnecessary/problematic amount of transaction-logging when done frequently. The same applies for the -statement. Even if you check a matched row for changes, all columns are hit by the update. 

It feels like there must be a smarter way around this, and I would appreciate any clarification and corrections to the statements made in this post. Like mentioned earlier, I'm just trying my best to understand why it has to be this way. Apologies for the lengthy post and thanks in advance. 

Is there any way in PostgreSQL to know how much the cpu cost was per a query and/or by session. It doesn't have to be the exact system cpu usage more of a relative cost like we get with disk I/O. For Windows or Linux. 

How about creating a test that uses your own data to answer your question? If you only learn one thing about database programming, it that there's often never a one size fits all solution to anything. There are best practices of course, but overall it's always case by case. The reason I say this is because with relation database queries there is often not a one size fits all approach to anything. Because there is a planner that decides how to go after your data the same query can be lighting fast or super slow depending on what the planner chooses to do with the information it has available at any given time. For example sometimes an index is your best friend one month and a month later your worst enemy depending if you are searching or inserting data and how others are using the system while you are running the query. So I always recommend creating a test that proves for your use case how this query will perform. And even then your results may vary over time and you will come back to optimize as data and usage changes. I think the answers you will get from stackoverflow is going to be text book answers, not the real world answer that only your environment can provide. And in the end it's your real world result that matters. Now your simple query that you are showing here should both be the same performance wise. But even then I say test it and prove it if you are asking the question. I would trust results before I would trust anyones answer including mine. 

I am trying to create a log-table for storing events to a -object, and I am afraid I might be taking the wrong route. I've arrived at the conclusion that I should log different data-types in the same column, and it doesn't feel right. I'll explain the basic use-case with 2 tables; and . 

Yet, the severity of the failed test is, as mentioned, considered high. Octopus Deploy While configuring the Octopus Deploy Server, the setup fails with a FATAL error during the initialization of the OctopusServer-instance. The article related to the error-message does not explain why this is a requirement, but simply states that it will be a requirement for future deployments, from and including Octopus version 3.8. As a side-note, RedGate's CI-tool package, the DLM Automation Suite, supports deployments with varying collations without complaints. The recommendation of keeping all column collations to the database default seems more like guidelines or best practices to me. Why is it considered such a serious error by some? 

I am attempting to construct TSQL queries to substitute various GUI tools provided by SQL Server Management Studio. One of these tools is the , accessible through the window. 

For the two former events, I'd store the type of event, the old value and the new value. This works fine as long as I stop here. However, if I want to log the 2 former-events, I would need to be able to store different data-types in the same column. The changes would look something like this: 

Question: In SQL Server 2016, does updating a column to the same value (e.g. updating a column from to ) produce the same amount of transaction-logging as when updating a column to a different value? Read below for more details. We have several SQL Agent jobs running on a schedule. These jobs select data from a source-table (replicated data, linked servers), transform it, then insert/update/deletes the rows of the local target-table accordingly. We have been through various strategies while trying to find the best way to achieve this. We've tried