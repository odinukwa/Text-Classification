These data are usually used to support or weaken the argument in favour of a particular model. In the case of the models discussed, they are designed to address predictions stemming from assumptions about the stages of speech production, and the order (if any) in which these stages are processed. For instance, WEAVER++ assumes that syntactic and semantic information is accessed prior to phonological information. Thus the TOT effect occurs when phonological information is difficult to access. Harely and Bown (1998), showed that words with an unusual phonological form are more susceptible to TOT, evidence that supports WEAVER++. As an aside, the 'priming' studies that Nathan refers to are in fact uses of the masked prime paradigm with a lexical decision task. These are used to investigate the processes involved in visual word recognition (i.e. reading), and are a great way to investigate the information we can access very quickly from written language. For instance, that we compute phonological information from brief (50ms) presentations of non-words (Kinoshita & Norris, 2012). What such information tells us about the mental lexicon is unclear. This isn't meant to be an exhaustive account of speech production. I've undoubtably presented some contentious information, and excluded some important information. Regardless, hopefully I've given you enough resources to make a start on your own research. Most of the articles I've mentioned are accessible through google, at the very least, Dell (1986) and Levelt et al (1999) definitely are. *Don't be put off by the length of Levelt et al (1999). Behavioural and Brain Sciences articles often include commentary by researchers in the same field, so the article itself is only about 35 pages long. The commentary is a great way to a get a feel for how a theory fits in with the rest of the field. Personally, I think it's a great idea. Dell, G. S. (1986). A spreading-activation theory of retrieval in sentence production. Psychological Review, 93(3), 283-321. doi:10.1037/0033-295X.93.3.283 Harley, T. A. & Bown, H. E. (1998). What causes a tip-of-the-tongue state? evidence for lexical neighbourhood effects in speech production. British Journal of Psychology, 89, 151. Kinoshita, S., & Norris, D. (2012). Pseudohomophone priming in lexical decision is not fragile in sparse lexical neighborhood. Journal of Experimental Psychology: Learning, Memory and Cognition, 38, 764-775. Levelt, W. J. M., Roelofs, A., & Meyer, A. S. (1999). A theory of lexical access in speech production. Behavioral and Brain Sciences, 22(1), 1-38. doi:10.1017/S0140525X99001776 Meyer, A. S., & Damian, M. F. (2007). Activation of distractor names in the picture-picture interference paradigm. Memory & Cognition, 35(3), 494-503. doi:10.3758/BF03193289 

However, is there any language that only distinguishes between paucal and plural? Or is it a linguistic universal that a language with only two number distinctions will always have singular and plural, and that the paucal number only occurs in languages with three or more number distinctions? 

The existence of a double-ð does not necessarily imply that the affricate was released twice, any more than other doubled stops are released twice. More likely the stop portion of the affricate was geminated, but the affricate was only released once. For comparison, consider modern Italian orthography, in which the letter z represents /ts/, but double zz is extremely common. This double letter does not indicate [t͡st͡s], but rather [t:s]. With that in mind, I see no reason to dispute the standard reconstruction of the tau gallicum. In particular, I find the suggestion that it could be [θ] to be very unlikely, given the available evidence. 

Japanese is considered a Japonic Language together with Ryukyuan, and the Japonic languages are not widely accepted to be related to any other language family. There have been, however, many different proposals, including a relationship to Altaic, Indo-European (!), and yes, Korean. However, all of these proposals have been heavily disputed, and none of them are considered to be proven. Even if Japanese is related to Korean, it's probably coincidence that these particular particles are related. The time depth for the supposed ancestor of Japanese and Korean would be so great that we wouldn't expect anything to have survived unchanged, and so two identical particles in both languages are probably the same due to coincidence. 

The alphabet has only been invented a few times, so this is easier to describe than you'd think. The first lineage of alphabets is the Phoenician lineage, which includes the modern Latin, Greek, Cyrillic, Hebrew, and Arabic alphabets, as well as a number of less prominent scripts both living and dead. While I haven't checked every one of these scripts, the Phoenicians themselves named their letters, and the tradition of naming letters was passed down to every derived script that I know of. Some of them kept or adapted the Phoenician names (Hebrew, Greek, Arabic), while others created a simplified naming scheme of their own (Latin, Cyrillic), but they all have names. The second major lineage of alphabets is the Brahmic lineage, which includes Devanagari as its most prominent member, but includes all of the myriad scripts of India and Southeast Asia in its family tree. Here, too, the original script had named letters, and the tradition of named letters seems to have been kept in all of the daughter scripts. That covers all of the unambiguous alphabets in the world. There is a third, somewhat contentious example, however. Korean hangul are usually presented as a syllabary, but unlike most syllabaries every hangul syllabic glyph can be fully decomposed into a set of strokes that consistently and predictably represent individual segments in the syllable, with well-defined rules for placement. Therefore hangul arguably represents an alphabet with an unusual, syllabic arrangement system, in contrast with the simple linear arrangement used in the other major alphabets. And to the best of my knowledge the individual strokes that make up a hangul glyph do not have names--or if they do, their names are somewhat specialized terms that aren't commonly used by people describing how to spell something. This is as close as I think you're going to get to an alphabet without names for its letters. Update: Hangul does have names for its letters, as pointed out in the comments. 

At each time step the learner receives one sentence from our language, and for each sentence in our language there is some finite time by which the learner will have been exposed to the sentence. Apart from this, there are no restrictions on the ordering or frequency of occurrence of the sentences (in fact, the environment might pick sentences in an adversarial order to make learning hard). There are absolutely no restrictions on the learner's learning strategy (in fact, it need not even be computable) The languages in our sets of languages are named (equivalently: numbered). A learner has successfully learned a language L if for any environment consistent with (1) there is some finite time after which the learner always guesses that the environment's language is L. A set of languages is learnable if there exists a single learner that can learn every language in the set. 

I was hoping @Mitch would expand on his comment about PAC-learning. For now, I will provide the only application of PAC-learning directly to linguistics that I know. It is in Ronald de Wolf's master's thesis: Philosophical Applications of Computational Learning Theory. de Wolf argues for the Chomskian stance on poverty of the stimulus in much the same way as Gold. Except, instead of showing that the context-free languages are not learnable in the limit from positive instances (as Gold), he instead shows that context-free languages are not effeciently PAC-learnable from positive and negative instances. Note that PAC-learning is much more reasonable model than Gold's since it allows errors of two kinds: (1) some very small fraction of children completely failing to learn the language, and (2) even the successful children making occasional (again small fraction of instances) mistakes on specific sentences, but getting the overwhelming majority of the language correct. These seem reasonable. The two restrictions placed on the learners (different from Gold) are: (1) that the algorithm must be polynomial time (i.e. efficient), and (2) the analysis is over worst-case distributions over inputs. The first restriction is perfectly reasonable if we are of the brain as computers camp. The second restriction is unreasonable (children don't get arbitrary distributions, they get very specific ones sometimes involving things like motherese), but still more reasonable than Gold's restriction of a completely adversarial presentation of sentences (that just lists them only in the limit). Unfortunately, I don't think de Wolf's thesis was unnoticed by lingusitics, since the only citations to it is from a recent non-linguistics paper by another theoretical computer scientist.