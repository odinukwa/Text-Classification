The problem is, however, with some hundreds of thousands of entries, this query runs for a couple of days, even though I have created indexes for the WHERE conditions. How can I optimize this query to run faster? 

The documentation states that without the GLOBAL or SESSION keyword, the isolation level will be valid for the next transaction and will be reverted to the session default after the transaction was commited. With autocommit enabled, what is the scope of an transaction isolation level for a SELECT statement? 

My goal is to go through all entries for a given date and assign the field "rank" for results. For example the player with the highest score should receive rank = 1, 2nd highest score rank = 2 and so on. If 2 players share the same score, they should receive the same rank (olympic scoring). Example: 

Turns out I have called accidentally on the live server. The slaves stopped replication now because they can not find the expected bin log files on the master. Is there a way to continue replication without data corruption and without reimporting the whole dump on the slave? Is it save to call with 'mysql-bin.000001' and position 1 on the slave now, because that is where they should continue, or do I risk data corruption in that case and it is generally preferred to reimport the whole dump and start over? 

With the help of Rick to look for pivot table I was able to create the final query as described in this post: $URL$ Query for a 6 months analysis: 

The part will return the first of a month to make sure the analysis is based on calendar months. I am not sure if it could be simplified. I was looking at but I was reading working on Strings is slower and should be avoided. If someone knows a better / cleaner way of doing that, please comment and I will include it in the answer. Here is some exemplary output of the query: 

I've been banging my head against the wall trying to solve this problem and I couldn't come up with a solution that absolutely guarantees two users can't book an appointment for the same time. The flow and scenario: 1 - The user enters the system and selects a service, say a haircut, and then clicks "Book". 2 - Upon clicking "Book", the server receives the date and time and the staff the user selected to perform the haircut. Based on this information, the available hours the selected staff has are calculated on the fly and sent to the user, who will see something like: 10:00, 12:00, 13:00, 15:00, for example. 3 - The user, then, selects one of the available hours and clicks "Confirm". The problem: Two (or more) users can select the same service and staff to perform that service, and when that happens, all the users will be presented the same available hours. Now, what happens is that when more than one user selects the same available hour, I need to be able to guarantee 100% that only the first will be able to schedule an appointment, and the other will be faced with an error. I could not come up with a mechanism that ensures no racing condition will occur. Note that this is a bit different from transport and theather domains, because I do not have a table with all the available hours to simply flag them. The closest I've come to a solution is to create an extra table, such as "AppointmentsInProgress", and add the starting date and time and the staff to the table, and make both columns unique. Then, before proceeding to process the user request, check this table to see if nobody else is currently scheduling for the same date, time, and staff. However, the performance of this solution is a big concern, and I read that the order of sql commands is not guaranteed, which means if two sqls hit the server with a microsecond of difference, there is no guarantee the first will have precedence. Furthermore, something tells me this doesn't really solve my problem. Any ideas? 

Note: I can't tell what date type is, but be careful to account for exact matches. Because there aren't any or in the ranges, exact matches may be unaccounted for if the date type isn't DATE. By creating a multiplier column, we can make just one trip to get the source data and do the work from there. Try using this as your view definition: 

To sum up, the odd treatment of AVG_RANGE_ROWS makes calculating row estimates more complex, but you can always reconcile what the CE is doing. What about Exponential Backoff? Exponential Backoff is the method the new (as of SQL Server 2014) Cardinality Estimator uses to get better estimates when using multiple single-column stats. Since this question was about one single-column stat, it doesn't involve the EB formula. 

The formula for estimating rows gets a little goofy when the filter is "greater than" or "less than", but it is a number you can arrive at. The Numbers Using step 193, here are the relevant numbers: RANGE_ROWS = 6624 EQ_ROWS = 16 AVG_RANGE_ROWS = 16.1956 RANGE_HI_KEY from the previous step = 1999-10-13 10:47:38.550 RANGE_HI_KEY from the current step = 1999-10-13 10:51:19.317 Value from the WHERE clause = 1999-10-13 10:48:38.550 The Formula 1) Find the ms between the two range hi keys The result is 220767 ms. 2) Adjust the number of rows We need to find the rows per millisecond, but before we do, we have to subtract the AVG_RANGE_ROWS from the RANGE_ROWS: 6624 - 16.1956 = 6607.8044 rows 3) Calculate the rows per ms with the adjusted number of rows: 6607.8044 rows/220767 ms = .0299311 rows per ms 4) Calculate the ms between the value from the WHERE clause and the current step RANGE_HI_KEY 

It looks like this query is making a lot more round trips than it needs to. Whenever you have multiple select statements bound together by UNION, you can look for ways to consolidate down to a single query. If I'm reading correctly, the date ranges in the WHERE clause are conveniently exclusive of each other yet inclusive of all possible dates. These date ranges are linked to a multiplier: 

Rationale: 1 - I will often query the table by only, which is why it is the leftmost index column; 2 - The column can be used for subsequent operations after the first query by (the id will be stored client side); 3 - I will never query by only; 4 - The unique index guarantees insert consistency, though it still slower than a single int; 

In my system I have temporary entities that are created based on rules stored in my database, and the entities are not persisted. Now, I need is to store information about these entities, and because they are created based on rules and are not stored, they have no ID. I came up with a formula to generate an ID for these temp entities based on the rule that was used to generate them: . This formula generates unique strings of the form My question is how should I build my table (regarding primary key and clustered index) when my keys have no relation or order? Keep in mind that I will only (99.9% of the time) query the table using the id mentioned above. Options I thought about after much reading, but don't have the knowledge to determine which is better: 1) primary key on a varchar column with clustered index. -According to various sources, this would be bad because of fragmentation and the wideness of the key. Also their format is pretty weird for sorting. 2) primary key on varchar column without clustered index (heap table). -Also a bad idea according to various sources due to indexing and fragmentation issues. 3) identity int column with clustered index, and a varchar column as primary key with unique index. -Can't really see the benefit of the surogate key here since it would mainly help with range queries and ordering and I would never query the table based on this key because it would be unknown at all times. 4) 2 columns composite key: rule id + rule index columns. Now I don't have strings but I have two columns that will be copied to FKs and non clustered indexes. Also I'm not sure what indexes I would use in this case. Can anybody shine a light here? Any help is appreciated. --Edit Here is what I think I'll be using after reading the comments, and the rationale: 

Note the steady decline in rows until we hit the RANGE_HI_KEY and then BOOM that last AVG_RANGE_ROWS chunk is suddenly subtracted. It's easy to spot in a graph, too. 

This gives us 160767 ms. 5) Calculate the rows in this step based on rows per second: .0299311 rows/ms * 160767 ms = 4811.9332 rows 6) Remember how we subtracted the AVG_RANGE_ROWS earlier? Time to add them back. Now that we're done calculating numbers related to rows per second, we can safely add the EQ_ROWS too: 4811.9332 + 16.1956 + 16 = 4844.1288 Rounded up, that's our 4844.13 estimate. Testing the formula I couldn't find any articles or blog posts on why the AVG_RANGE_ROWS gets subtracted out before the rows per ms are calculated. I was able to confirm they are accounted for in the estimate, but only at the last millisecond -- literally. Using the WideWorldImporters database, I did some incremental testing and found the decrease in row estimates to be linear until the end of the step, where 1x AVG_RANGE_ROWS is suddenly accounted for. Here's my sample query: 

To take the Reporting Services databases out of commission on this server without disrupting the service itself, you'll need to point the service to databases elsewhere. To do this, open Reporting Services Configuration Manager on the server running the RS service, and change the "Database Name" entry in the Database menu page: 

I expect with a single call to each source table (AdWordsData and WebSkuLookup), this will shave the execution time down quite a bit more. 

Window functions aren't permitted in UPDATE statements because UPDATE isn't compatible with SELECT or ORDER BY. Window functions are like scoped SELECT statements that re-examine the relevant rows and apply conditions like PARTITION BY and ORDER BY. In addition, many window functions require an ORDER BY clause (ROW_NUMBER, LAG, and FIRST_VALUE, for example). UPDATE statements use SET instead of SELECT, so SELECT is not allowed anywhere in the same query level. Any SELECT appearing with UPDATE must be contained in a subquery. Disallowing the ORDER BY makes sense considering an UPDATE statement is indifferent to the order in which it updates rows. There's no inherent downside to using a CTE or other subquery as a workaround to get an UPDATE to use a window function. That's the common practice advocated by T-SQL experts like Itzik Ben-Gan. (See page 29 of his book, Microsoft SQL Server 2012 High-Performance T-SQL Using Window Functions where he covers this exact scenario.)