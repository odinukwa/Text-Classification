I don't think invoke-sqlcmd can do thi. Regular sqlcmd.exe has the -e parameter to output the batches, although it will not output the "GO" . EG 

No. Some .NET data types map exactly to SQL Server data types, and int is one of them. And this method is not deprecated. See the docs here The issues caused by not specifying parameter types exactly are usually limited to bad plans. If you pass a parameter with a higher data type precedence than the column it maps to, then the column values must be converted to the parameter's type for comparison. And this will prevent index use, and require a conversion per row. Another issue with deriving the string parameter type from the value is that .NET's SqlClient will set the length of the parameter to the actual length of the string passed. This may minimize the network traffic and memory requirements, but can change the query plan caching and reuse behavior in SQL Server. For stored procedure invocations it won't matter, but for ad-hoc SQL batches, you can end up with a different cached plan for each string length. If you pass a numeric/decimal parameter with excessive precision this could potentially cause the loss of scale in the result, per the rules here, but parameters don't typically appear in calculations. 

You will have to call /pages/folder.aspx with an ItemPath parameter as absolute path /Test which url encoded will look like this. 

Take a log backup Put the database into simple recovery Upgrade When the database is back in full recovery mode make a differential backup and then start the log backups again. 

In both cases you end up with a CRT that you can import to the machine store of the SQL Server and then use the SQL server configuration manager to encrypt the connections. You will have to trust the root certificate in the latter case though and in the former case, if you are not running on a domain you will have to install the root certificate for the windows CSA on both the machines 

Yes you can. You open the FTP site in IIS and change the SSL Settings from Require to Allow. You have to check with the owners of the FTP site if this is allowed and I would recommend to use https if possible as this way you are transmitting the username and password using cleartext 

Also must mention that Azure SQL Database is ideal for a small shop with no DBA. And enables you to create readable replicas in the standard tier, and premium tier databases get an automatic readable replica. 

Given the constraints, I think the only solution is to use SQL Security to protect against SQL injection attacks. You only need to prevent SQL injection from untrusted users. So long as the procedures that accepts the WHERE clause can't be called by regular users, and you properly handle user-supplied strings, it should be safe. 

Migrating to in-memory tables always requires recreating the tables from scratch. SSMS will rename your old tables, create the new in-memory tables, load them from your old tables, and drop your old tables. 

Keep track of the row count in a variable, and return that to the client with an output parameter or a result set. 

Transactional Replication uses the TDS protocol and applies changes using SQL. The connections are established from the Distributor for Push Subscriptions, and from the Subscriber for Pull Subscriptions. AlwaysOn Availability groups use TCP/IP connection between the replicas using the Database Mirroring Endpoint. The connections are established from the instances hosting the Secondary replicas to the instance hosting the Primary replica. But as the Primary replica can move, every server hosting a replica needs to be able to connect to every other one. 

Use the SQL Server configuration manager to change the SQL Server Agent user account. Change it to run as local system, apply and restart and then back to the correct user, dont add any privileges to the user in the operating system (esp. not add it to the server local administrator group). The Configuration manager will set all the correct permissions for the account. 

For most parts I'm referencing Paul Randall's Inside the storage engine blog series. The only way to reclaim unused space from database files in SQLServer is using the DBCC SHRINK command which reallocates data within the database files freeing pages and after removing them from the Global Allcation map removes them from the database file. This operation is slow, creates fragmentation within the database and is even slower when dealing with LOB pages as those are stored as linked lists within the database files. Since you are dropping the NTEXT column you will have to wait for the ghost cleanup process to drop the data before shrinking. Now having lots of free space in the database files will actually do you no harm, if you have the disk space, backup compression will take care of the free space within the files. If you absolutely want to make the files smaller you can create a new filegroup with the database and make it the default and then move the tables into the new filegroup but this can take time and cause downtime. I have used the technique explained here by Bob Pusateri with good results. 

Also using three VLANs to manage your packet routing is a cheat, as it just moves the problem from the Windows Admin to the Network Admin. And it's like asking the city to build three separate roads to your house, just because you have multiple driveways. You can achieve traffic isolation with 3 IPs on the same VLAN if you configure the routing table appropriately. Inbound packets always arrive at the NIC hosting the destination IP, but outbound packets to the local VLAN will all go out over the NIC hosting the interface (IP Address) listed first in the routing table. See $URL$ for details. 

Define "little". The safest, simplest thing is to leave the database in Simple recovery, and use Full/Differential backups to move the data. You can also switch to Full recovery and use Log backups instead of Differential backups. In both cases you would 1) Take a Full backup 2) Restore the Full backup on the new server WITH NORECOVERY 3) Begin Downtime 4) Take a final Log Backup or Differential Backup 5) Restore the all the Log Backups or the Differential Backup WITH RECOVERY 6) Set the database OFFLINE on the old server 7) End Downtime You can perform steps 1, 2, 4, and 5 as many times as you want in testing, and to measure the required downtime. 

This is how DNS works with multiple servers the client will ask the servers in a round-robin fashion. if an entry is removed from one of the DNS servers the client will get as an answer from that server and will not query the other. Here you can get an overview but it boils down to this 

Yes it is possible to set one of the instances to run on port 1433. It is also safe as the dynamic ports will be set in the range of 49152 and 65535 as stated above. You use the configuration manager to set one of the instances to start at port 1433 (clearing the configuration for the dynamic port) and it will run on that port afterwards. 

The DMV's themselves are the same between same builds of SQLServer and give you a snapshot or a cumulative counter of the status of your server at any given time. You cannot restore the database to a new server with the metadata from the old one. You can make snapshots of some of the performance metrics into a table but you cant restore the state of the DMV's. You can however grab the query plans from your production server and use plan guides to have the same behavior in test, if you want to mimic specific behavior of a single query that is not behaving in production environment but runs ok in test. You can then of course use the Management Data Warehouse to collect information about performance of queries and the system to find which queries to optimize or third party tools that can collect query plans and wait statistics on the server. Here is a way to collect the query plans using sp_whoisactive 

If the content being versioned is not stored as a blob, but is in separate tables, the pattern still holds. All the tables that store the versioned data need the VERSION_ID. It would certainly be an interesting research project to explore all the different ways this can be done, and discuss options for conflict resolution, merging changes and sketch out what a general solution might look like. 

There's nothing in the deadlock graph or DMVs that will tell you that. That information is only available in a trace event or an XEvent, at the moment the lock is acquired. 

A database backup contains the allocated extents plus the parts of the log required to replay any activity that occurred during the backup, and restore the database to the point-in-time at which the backup completed. So your first backup file could contain more log records than your second backup. David 

If you have an MSDN license you can download and install SQL Server Standard Edition for Dev/Test. Otherwise you can use an Azure Pay-As-You-Go instance for testing. See eg SQL Server 2017 Standard on Windows Server 2016 

So if you are not on the stated service pack level you will need to upgrade to at least that. It is not neccessary, but start by running the upgrade advisor. Thomas LaRock has a checklist on his page both for the tasks needed before an inplace upgrade and what to do afterwards. 

The above statement create the table in the target database and bulk insert the data. If you want to copy the data to a different instance you are best off using SSIS. 

If your databases are in full recovery mode you will need to backup the transaction logs, when that is done you can shrink the log files and setup a regular log backup to minimize growth and keep good backups. I would reccomend that you read this: $URL$ but first things first. The first step is to find the largest log files which you can shrink without backups - So you have to find databases which are in simple recovery mode: 

Because the most restrictive lock currently held on the resource is the U lock held by the writer session, and which it has requested to "convert" from U to X. 

SQL Server uses Virtual Acconts by default, which are a security best practice. You should not use a domain account unless you have a specific reason to do so. The only two reasons I can think of are: 1) You have Failover Cluster Instance, or an Availability Group Listener that needs to support Kerberos Authentication. or 2) SQL Server service needs to access domain resources and there are other programs running on that server under Network Service or a virtual account that need to access domain resources. This is because Virtual Accounts access network resources using the Computer Account. If you have lots of different programs installed on the server accessing network resources using the Computer Account, that can lead to an accumulation of privileges to that account. So in the (paradigm) case of a dedicated SQL Server, not in a FCI or AG, you should not change the default service account. 

sp_depends will give you list of dependent objects in all current versions of SQL server but is listed as deprecated. Here is a MSDN article with a way to find objects referencing a function and with small modifications you can get the list of object referenced by a function: 

You can setup a test database and trace the application activity using the SQL Server profiler. By tracing the event "Security Audit\Audit Schema Object Access Event" you can effectively trace all the SQL the client is sending to the server. But that will not necessarily help you in discover the application logic. 

On a standard SQL Server license on SQL Server 2016 you can setup AlwaysOn Availability Groups on two hosts without the need for shared storage. On SQL Server 2014 and 2012 the only high availability option you have with a standard license is to create Alwayson Failover Cluster Instances which depend on you using shared storage. So using the standard edition the only option you have is to run SQL Server 2016 with AlwaysOn Availability Groups. Remember that a FCI or AG will only set you back a single SQL license if the secondary server is not queried. The setup is not that complicated but a bit out of scope, you can start here