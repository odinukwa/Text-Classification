I also would strongly suspect it holds with $c = 1/2$. Here's an argument which, if I haven't made a mistake, gives the upper bound $O(\epsilon^{1/2} \ln^{1/2}(1/\epsilon))$. Assume without loss of generality that $a_1 \geq a_2 \geq \cdots \geq a_n$ and that $a_1 + \cdots + a_n = 1$. Divide into two cases: Case 1: $a_1 \geq \frac{1}{100\log(1/\epsilon)}$. In this case it's sufficient to show that we already have $\Pr[a_1 X_1^2 \leq \epsilon] \leq O(\epsilon^{1/2} \ln^{1/2}(1/\epsilon))$. But this holds as you said because of the anticoncentration of a single Gaussian. Case 2: $a_1 \leq \frac{1}{100\log(1/\epsilon)}$. In this case, note that $\sigma^2 := \sum_i a_i^2 \leq a_1 \sum_i a_i \leq \frac{1}{100\ln(1/\epsilon)}$. Now the random variables $X_i^2$ are nice enough that one should be able to apply a Chernoff Bound to them. (I think I could provide a source for this if necessary.) Since $Y := \sum a_i X_i^2$ has mean $1$ and standard deviation $\sqrt{2}\sigma$ (I think, maybe the constant $\sqrt{2}$ is wrong), we should have a statement like $\Pr[|Y - 1| \geq t \cdot \sqrt{2}\sigma] \leq \exp(-t^2/c)$, where $c$ is a quite modestly small universal constant. So $\Pr[Y \leq \epsilon] \leq \Pr[|Y-1| \geq 1/2] \leq \exp(-\frac{1}{8c\sigma^2}) \leq \exp(-\frac{12.5}{c} \ln(1/\epsilon)) = \epsilon^{12.5/c}$, which is smaller than $\epsilon^{1/2}$ if $c$ is not too large, and even if $c$ is too large, one can make $100$ larger. 

I think a reasonably quick way to do it is something like this: Write $C = A \setminus B$. If we can show that \begin{equation} \tag{*} \Pr[|C \cap R| \geq .8 \lfloor n^d \rfloor] = o_n(1) \end{equation} then we're done because $.2 \lfloor n^d \rfloor > n^{d'}$ for sufficiently large $n$. For each $1 \leq i \leq \lfloor n^d \rfloor$, the probability that the $i$th element of $R$ falls into $C$ is at most $.76$ (for sufficiently large $n$). If the events were independent then we'd easily have $(*)$, by Chernoff bound. But in fact, the events are negatively associated, so the Chernoff bound still holds; see: 

EDIT: the strikethroughs are in response to the followup comments that ensued. I believe this is open. For simplicity, assume the $\phi$'s are the same function. It's a bit more standard to write $L(\phi)$ for the minimum formula size of $\phi$. The original question is whether $L(\oplus_m \circ f) \gtrapprox m L(f)$, where $\oplus_m$ denotes arity-$m$-XOR. Indeed, one could hope even for $L(\oplus_m \circ f) \gtrapprox m^2 L(f)$, since the formula complexity of $\oplus_m$ is $m^2$. This stronger (potential) lower bound is presented as a major conjecture (Conjecture 1.10) in the following recent work of Gavinsky, Meir, Weinstein, and Wigderson: $URL$ The question dates back to Karchmer-Raz-Wigderson'95. This leaves open the weaker statement $L(\oplus_m \circ f) \gtrapprox m L(f)$ -- basically, what was asked in the original question -- but my guess is that this is equally unknown. One more remark: as noted in the paper above, the desired conjecture is true if the XOR operation is replaced by the OR operation. EDIT: Wegener's observation (see coments) that the desired conjecture holds true for OR seems to apply equally well for XOR, as noted by Fedja. So it would seem that the answer to the poster's question is positive, even without the factor $1/2$, assuming the $\phi$'s are nonconstant. 

If it's not too gauche to plug my own course at CMU, 23 lectures on Analysis of Boolean Functions (one lecture by John Wright): $URL$ 

Say the $B_i$'s have success probability $x$, and let $P_x(A)$ denote the resulting probability of the increasing event $A$. Then the derivative of $P_x(A)$ at $x$ is equal to the "sum of the $x$-biased influences of $A$", which is a measure of the "edge-boundary" of $A$. Specifically, it equals $(1/x) \sum_{i} P_x(A_i)$, where $A_i$ is the event that $(B_1, \dots, B_n) \in A$ but $(B_1, \dots, -B_i, \dots, B_n) \notin A$. This is the "Russo-Margulis Lemma". For particular events $A$, one can sometimes argue that the sum of the $x$-biased influences of $A$ is large based on symmetries of $A$. In that case, $P_x(A)$ increases quickly as a function of $x$. It's hard to describe more in a short space; you could take a look at Ehud Friedgut's survey on his work on "sharp thresholds": "Hunting for Sharp Thresholds", Random Structures Algorithms 26 (2005), no. 1-2, 37--51; also at $URL$ 

into Maple and it'll give you an answer right away. But it seemed hard to find. After much searching, I hit upon the key phrase "unrestricted algorithm" which led me to the paper "An unrestricted algorithm for the exponential function", Clenshaw-Olver-1980. It's pretty hard to read, analyzing the time complexity for $e^x$ in terms of eight (??!) parameters, but its equation (4.55) seems to give some answers: perhaps $\tilde{O}(t^2)$ assuming $|x|$ is constant? And really, all that work for little old $e^x$? As for erf$(x)$, I found the paper "The functions erf and erfc computed with arbitrary precision" by Chevillard in 2009. It was easier to read, but it would still take me some time to extract the answer; my first impression was $\tilde{O}(t^{3/2})$. But again, surely this question was not first investigated in 2009, was it?! (By the way, question #5 is the one for which I really want to know the answer, but I can probably work it out from the answer to question #4.) 

Let $P(x)$ be a real polynomial of degree at most $d$. Assume $|P(x)| \leq 1$ for $|x| \leq 1$. I would like a bound saying that each coefficient of $P(x)$ is at most $C^d$ in magnitude, for some absolute constant $C$. This is surely a well-known, basic fact in approximation theory and I'm looking for a proper reference. I know one very recent paper which writes out a proof using the standard simple idea (Lagrange interpolation) -- Lemma 4.1 from a paper of Sherstov here: $URL$ Sherstov obtains $C = 4e$; I don't think either of particularly cares about getting the sharpest constant. In any case, Sherstov and I agree that this must have appeared somewhere long ago. Could anyone provide a reference? Thanks! 

My colleagues and I are working on a project related to an old paper of C. Borell and we have boiled it down to the following problem: Show, for all integers $1 \leq i \leq k$, that the univariate real polynomial $P(x) = \frac12 \binom{2k}{2i} (1+x)^{2k-2i} + \frac12 Q(x)$ is everywhere nonnegative, where $Q(x) = \sum_{j=i}^k \binom{2k}{2j}(1-x)^{2k-2j}\binom{j}{i}(-4x)^{j-i}$. (For what it's worth, Maple recognizes $Q(x)$ as $\binom{2k}{2i} (1-x)^{2k-2i} \text{hypergeom}([-(k-i), -(k-i)+\frac12], [i+\frac12], -4x/(1-x)^2)$. I'm not asking MO to prove this (although I suppose if someone saw how to do so immediately I wouldn't turn down the answer). Instead, I'm asking "Is this 'routine'?" in the sense of the word used in the Petkovsek-Wilf-Zeilberger A=B book? In other words, would Doron Zeilberger say, "Oh yes, just type the following into Maple and it will produce a proof of the claim"? In other other words, does this question fall into a class of problems known to be decidable? Of course, for any fixed $i$ and $k$ the problem is 'routine'. E.g., for $k = 4$, $i = 2$, we have $P(x) = 70x^4-168x^3+804x^2-168x+70$ and furthermore I can coax my computer into proving that's nonnegative. (Say, by obtaining a sum-of-squares representation like $P(x) = 42(x-1)^4 + 28x^4 + 552x^2+28$.) I don't know the "A=B technology" very well: my question is whether it, or any other techniques, can be used to automatically prove these inequalities. I would also be happy to accept an answer explaining why proving these inequalities is not obviously 'routine' and will require some ingenuity. UPDATE: Doron Zeilberger wrote me an email describing why at least 'half' of this problem is routine: namely, that for $i$ symbolic and $k-i$ numeric the nonnegativity can be proven by computer. He preferred not to post himself but said I could post his ideas here; I will do so once I get a chance to think about them. 

I think the "official" reference for this fact is Section 4 of the following paper of Chung, Furedi, Graham, and Seymour: $URL$ 

Half a year ago, John Wright and I were considering almost the same question, in connection with the quantum tomography problem; we even asked a few people, including Suvrit and fedja. The only very slight difference is we were hoping to show that $M$ is close to its own diagonal, rather than to any diagonal matrix. (Note that since we have the hypothesis that $M$'s diagonal is close to its spectrum, our question is equivalent to asking if $M$ is close to the diagonal matrix made from its spectrum.) Now if you insist on this, that $M$ be close to its own diagonal, then I think the answer to your question would be "no". I'm not 100% certain, but here's my reasoning. Let $A$ be any diagonal matrix of nonnegative reals (sorted, say) and let $H$ be any Hermitian matrix. Now suppose $M = \exp(-i t H) A \exp(i t H)$, so $M$ is PSD with spectrum given by $A$'s diagonal. Think of $t$ as a positive real tending to $0$. Now let's consider $\Delta := D(M) - A = D(M-A)$. By Taylor expansion, \begin{align*} \Delta &= D(A + i t (AH - HA) - \tfrac{t^2}{2}(AH - 2H^2 + HA) + O(t^3) - A) \\ &= \tfrac{t^2}{2} D(AH - 2H^2 + HA) + O(t^3), \end{align*} where we used that $AH - HA$ has $0$ diagonal. Thus, thinking of $t \to 0$, we will have $|D(M) - A|_{\mathrm{tr}}$ proportional to $t^2$ (or smaller). That means you're hoping that $M$ is $\Theta(t^2)$-close to diagonal. Now if, like us, you are further hoping $M$ is $\Theta(t^2)$-close to its own diagonal then you're out of luck: it's equivalent to showing that $|M - A|_{\mathrm{tr}} = \Theta(t^2)$, but as we saw, $M - A = it(AH - HA) + O(t^2)$, and hence $|M - A|_{\mathrm{tr}} = \Theta(t)$ (unless $AH- HA = 0$, but this need not be the case). If you restrict $M$ to have trace 1, then we felt the desired result was true but with a weaker conclusion of $O(\sqrt{\epsilon})$ rather than $2\epsilon$. -- In case this is related to our upcoming tomography papers, I'd be happy to continue the discussion over email :) Best, Ryan