I have 2 naming standards files from ERwin (.nsm extension), and I want to compare them to see if they contain the same rules. Is there some sort of automated way to check this? If there isn't a way to compare the two within ERwin itself, is there a way to convert the 2 files into a more 'standard' file type (i.e. an Excel file), and then compare the files that way? I'm coming from a programming perspective (mostly Java), and from what I've seen it wouldn't be that hard to write a program that compares 2 Excel files. 

Is there a way to limit the inputs into a DATE type in an Oracle database to only accept inputs formatted with only the day and month (not including the year)? 

Tom Li- MSFT described a solution to what sounds like your problem, but his solution was for SQL Server 2008. (But just how much has the installer been improved since then?) $URL$ Key points from his post include: 

Either of these models look reasonable. The intersection table is not strictly needed since you can get the information through either model. The question is: How do you intend to use the NoticeMasterFile information? Perhaps this change would support some future plans that you have. Of course, the problem is that looking at a data model does not really reveal the thinking behind the model. Nonetheless, separating the blob seems like a very good idea to me. If nothing else it protects you when someone runs on the table. As an aside and for what it is worth, if you use sp_tableoption to set the blobs will leave a 16-byte key that references the blob pages. The blob will still be considered part of the table. The separation would allow you to query the other columns of without the processing overhead of the blob data. (This is the default setting.) Questions: How many documents do you expect to accumulate in your database? How large is the average size? Et cetera? If your document load will be relatively light, then this should be fine. But remember that every document is included in the backups that you take. If you think it likely that you will eventually move into the Terabyte size range, you should do some further planning for how to store the documents. Perhaps: 

I am trying to develop a small business web application for my friends business. I do have some php,html experience so I have done the interface and all. Now I am trying to work on the database. The whole process is simple , the user creates a quote and once the quote is approved , he can use the quote to create an invoice. Here are the items in the quotation page ( I will call below items as variables/columns) 

Invoice number Date Requester Reference code 1 Reference code 2 Purchase order code purchase order date Credit terms(days) Bill to ( Company Name, City , Country , Zip code , Telephone ) Ship to ( Company Name, City , Country , Zip code , Telephone ) Items ( there will be more than one items ) [ Item Name, Item Description, Unit of Measurement , Quantity , Unit Price ] Total amount Terms and Conditions ( this will be a 3 line sentence, it can be anything , it will be a text .. example below ) [ stock available/ not available / 2 -3 weeks delivery period / cash on delivery / 35 days credit ] Created by 

The way I'm understanding your question is that since the arrays for a and b don't have a value for 'in location y', the loop 'skips over' it and displays the value 1 for d in the first open row. You could try pulling out locations for all the arrays (even the non-y values), and then using an if statement to display either 0 or the result you want. You can also try putting everything into one for loop, and the arrays that don't have a location of y output 0. Additionally, try looking into the isset(); function in php for those arrays that didn't get the location pulled. If you post the code that you're using to pull and display the results, we might be able to give better/more accurate advice. 

Or you might suffix the numbers so that all from Server1 end with a 1 and all from Server2 end with a 2. Or anything else that helps you avoid or limit the need for juggling values constantly or keeping the two machines aware of each other's state. 

We have inherited a corrupted database 'DBName', with no good backup available to us. The corruption apparently happened some months ago. We have tried a few thing: 

The whole point of an INDEX REBUILD is to make the processing of data more efficient. This means that when possible it reorganizes the data to reduce the number of pages (according to the fill factor and other limitations) used for that index (whether it is a PK, unique, or non-unique index)and to order the rows of data more efficiently within the used pages and extents. Therefore, it will reuse existing pages as the first choice. This should result in less storage being used. Of course if the indexes are modified, this could expand the amount of storage needed, depending on the modification. Using tempdb to rebuild indexes can be a performance boost, but is unlikely to dramatically affect the required storage. In this case you should definitely rebuild the indexes. Note that this will not reduce the physical size of the .mdf and .ndf files. It will free up space that can be used as tables in the database expand in size. 

I have a data model in ERwin (r7) that has about 1000 tables in it. I'm right now in the process of splitting it up into various different subject areas to make it a bit more manageable. Here's the issue: I create a subject area with approx. 50 tables. The problem is, they appear all over the subject area in seemingly random places. Even if I zoom out as much as possible, I can't see the entire subject area, and I need to scroll around to reach the tables. Additionally, there's an obscenely large amount of unnecessary blank space between the tables. Is there some sort of shortcut to condense all the tables in the subject area into one area (especially when there are tables that are also connected through relations)? Currently I'm moving every piece one or two at a time, but I'm hoping there's a quicker way to organize everything. Thanks in advance for your help! 

You should be able to wrap two statements in a transaction. Because contains the criteria, it last. E.g. 

And you can script out the msdb procedure to see the code in the procedure, in case you want to include some of that in your own script. 

For what it is worth, connections are usually representing a Client that has left a connection active, but is not doing anything. Therefore, the connection is . Usually pretty harmless. But a connection that is holding an OPEN transaction can (but not necessarily) lead to problems with other transactions or with the transaction log. Add to your list of problems: Network, Routing, and similar problems. 

How much memory does SSIS need? (The real answer is: It depends.) However, Jonathan Kehayias has a formula that is pretty good, or so I think, since it works great for me. $URL$ The simple formula for reserving space for needs other than SQL Server in Windows is: Start with 1 GB, plus 1 GB for every 4 GB in the machine between 4 and 16 GB, plus 1 GB for every 8 GB in the machine above 16 GB. After that you should do some monitoring to determine if you need more or less memory for your SSIS packages. That is the it depends portion of your planning for memory use. If you see problem with your SSIS processes then you need to determine if more memory is needed. Because you have two instances on the same server this will likely require further tuning. 

Quotation Id ( will be automatically generated) Date Requester ( alpha numeric with space ) Reference code ( alpha numeric with space ) Quote valid for ( alpha numeric with space ) Bill to ( Company Name, City , Country , Zip code , Telephone ) Ship to ( Company Name, City , Country , Zip code , Telephone ) Items ( there will be more than one items ) [ Item Name, Item Description, Unit of Measurement , Quantity , Unit Price ] Total amount Terms and Conditions ( this will be a 3 line sentence, it can be anything , it will be a text .. example below ) [ stock available/ not available / 2 -3 weeks delivery period / cash on delivery / 35 days credit ] Created by 

First, you need to load the .nsm file into the model. This is done through Tools > Names > Model Naming Options... > Use File: (select the appropriate file). Well, if you've got version R9 or higher, you're in luck. As per this post, the naming standards are an object in the model explorer just like every other entity, so you can simply find it in the model explorer, right click on it, select properties, and on the glossary tab click the save button, and save as .csv. For pre-R9 versions, you need to go to Tools > Names > Edit Naming Standards... The Naming Standards Editor window should pop up. There's a few tabs near the bottom, click the one that says 'glossary'. You'll now have an option to export the file as .csv. Took me way too long to figure this out myself, so hopefully this could help the next guy who has the same issue. ;) 

Materialized Views - A Different Approach to Sorting Joined Tables You alluded to Materialized Views with your question referring to using triggers. MySQL has no built in functionality to create a Materialized View but you do have the tools needed. By using triggers to spread the load you can maintain the Materialized View up to the moment. The Materialized View is actually a table which is populated through procedural code to build or rebuild the Materialized View and maintained by triggers to keep the data up-to-date. Since you are building a table which will have an index, then the Materialized View when queried can use the fastest sort method: Use index-based access method that produces ordered output Since uses triggers to maintain a Materialized View, you will also need a process, script, or stored procedure to build the initial Materialized View. But that is obviously too heavy a process to run after each update to the base tables where you manage the data. That is where the triggers come into play to keep the data up-to-date as changes are made. This way each , , and will propagate their changes, using your triggers, to the Materialized View. The FROMDUAL organization at $URL$ has sample code for maintaining a Materialized View. So, rather than write my own samples I will point you to their samples: $URL$ Example 1: Building a Materialized View