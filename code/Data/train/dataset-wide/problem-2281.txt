When two tables have the some column structure (as Location and Address do here) that is almost always an indication that they should be the same table (conceptually and logically, even if separated physically for performance reasons). So ask yourself, "What is the design assumption that leads me to think that Address and Location are separate entities rather than the same one?" Answer: The relationship between Employer and Address is 1-N; but the relationship between Post and Location is 1-1. Therefore, each Address must have a FK to the Employer, and each Post must have a FK to the Address, with a transitive relationship to the Employer via the Address table. 

which allows the easy conversion from Calendar YTD values to non-Calendar YTD values. However the table is a derived table, meaning that it must be maintained as new years are instantiated. I would like to replace it with an Indexed View to eliminate the need for this maintenance. I have proved the equivalence of the select above with that below, using a NUMBERS table, which is precise and deterministic and uses no sub-queries or APPLY operators, allowing it to be the definition of an Indexed View: 

No, it's not acceptable to have circular foreign key references. Not only because it would be impossible to insert data without constantly dropping and recreating the constraint. but because it is a fundamentally flawed model of any and every domain I can think of. In your example I cannot think of any domain in which the relationship between Account and Contact is not N-N, requiring a junction table with FK references back to both Account and Contact. 

Note that the single clustered index on the view is identical to the (one and only) clustered index on the original table. However, several queries running against the Indexed View run slower (averaging about 3*, ranging up to about 6*, slower) than against the original table. Does anyone know why this could happen? Is it a possible bug in the Engine to not treat two identical clustered indices identically? My test data currently covers only two periods, one year apart. I initially thought it might be due to the columns of the view being nullable, but using isnull to coalesce them simply makes the queries so slow I can't even measure the performance. I am on SQL Server 2014: 

First, you must ask the Controller (or CFO) what the appropriate costing model is for inventory of this sort - LIFO, FIFO, Average, or some less common pattern. This is very important as if you use a different calculation from that published in Financial Reports of the corporation, you will be engaged in fraud of the shareholders and be in violation of Sorbanes-Oxley. If the financial records are mailed to the shareholders, you might be in violation of RICO also. All of the methods I listed above are appropriate in various industries, at various times - you cannot make an a priori decision on one being better than the others. One absolute principle of GAAP is that when any change is made to the traditional accounting policies of a corporation, a number of prior years financial statements must be restated according to the new calculation in the current year's financial statements. This can only be done as authorized by the Controller and/or CFO, and possibly the corporate auditors as well. Never, ever program an accounting calculation of any sort without verifying the technique with an appropriate corporate authority. There are many aspects of GAAP (Generally Accepted Accounting Principles) that are non-intuitive to those without an accounting background. Update: Note that even when inventory items have serial numbers, and thus the actual cost may be known, it is not necessarily appropriate accounting practice to use the actual cost instead of a calculated cost based on all equivalent items in inventory. That is to say: The cost of selling a unit of product XYZ is a calculated accounting value that may not be the actual purchase price of the particular unit sold, even when that is known. I repeat: You must discuss this with your Controller and/or CFO. 

A Bond Purchase transaction will have to handle the Discount(Premium) on Face Value as either a Credit or a Debit depending on whether interest rates have risen or fallen since the bond was issued, so you won't even know in advance which side of the ledger your single-entry is on. Once you decide to engage in single-entry bookkeeping you lose all ability to handle these transactions, which as seen here are by no means either rare or complicated. 

If the primary key is a surrogate (ie an IDENTITY valued INT) then you need to remove it from the field list of the insert (if possible) otherwise (if you need to retain the surrogate key values to support foreign keys) to bracket the insert with and . 

In practice relations frequently have multiple candidate keys, and these rules must apply to all of them in turn. Also, when Normalizing one must determine the minimal key(s) for each relation, and use those rather than any Super Key. Your lecturer didn't remove the fifth column altogether because the functional dependency in that column still exists, and must still be accounted for in the Normalization process. Update: The FD AD->C doesn't disappear by virtue of recognizing ADC as a subset of CDA; neither is it sensible to have two relations ADC and CDA both in the model as this is a redundancy of exactly the sort Normalization is designed to eliminate. 

(Please forgive the SQL Server test case - the problem is common to all SQL implementations because that of common semantics mandated by the SQL Standard.) Even though you have used a LEFT OUTER JOIN, the semantics of SQL can convert this to an implied INNER JOIN if you improperly put constant-test conditions in a WHERE clause instead of the JOIN clause. The example below ilustrates this. Preliminaries to create test data: 

The Key - there must be a Primary key for every relation being normalized. The Whole Key - There must not be any functional dependencies of attributes on any proper subset of the Primary Key. And Nothing but The Key - There must not be any functional dependencies of attributes on non-key attributes. 

If the SQL server table is modified will it be automatically reflected in the [linked] table and when? Yes, on next refresh or requery. If I edit the linked Access table will it be reflected in the SQL server table? Yes; as before, these updates will be visible on the other end on next refresh or requery. Trusted Connection only controls how the connection is made to SQL Server, not behaviour after a connection is made. 

I believe these are more likely to give you incremental performance improvements than partitioning the data tables. 

I am with your teacher here. I cannot see any difference in degree of Normalization between the two solutions, but you are embedding aspects of a particular implementation presentation (Two views of what is often referred to as the External or Logical Schema.) into the core data structures. When performing the initial database design (of the Conceptual Schema) it is preferred to hide as much as possible both of the external presentations and of the physical table design. Once the Conceptual Schema has been designed (and fully normalized), then one would determine the best way to present the Logical views of that structure to external applications, and the optimal physical assignment of columns to tables (the Physical Schema) to maximize performance. Your proposal is pushing subtle choices of Physical and External design into the Conceptual Schema inappropriately. It appears to me that your teacher is having difficulty explaining that your proposal might be a valid physical database design, later, given certain assumptions, and accurately reflects the External views as currently implemented, but fails to meet the needs of a Conceptual Schema that maximizes normalization with minimal clutter. A key observations is that whenever two tables (entities) have the same Primary Key, then even if the normalization is the same the clutter has been increased. There will always be many ways to clutter a clean (Conceptual Schema) design, but this clutter can always be eliminated from it. In a real world database design with hundreds or even thousands of tables minimizing clutter is an essential design attribute. 

The key concept here is that linked SQL Server tables are truly linked to ACCESS, not copied. If you desire a private copy of the data, copy the tables down instead of linking them. 

Note how, despite the LEFT OUTER JOIN which one expects to ensure that all Person rows are returned, the placement of the condition in the WHERE clause instead of the JOIN clause coerces the join into an INNER JOIN; so that Ginny is dropped from the first result set. This is a specific example of how, more generally, the occurrence of a NULL value in a field being tested violates intuition. One loses the Excluded Middle, so that when a predicate A may be NULL it is no longer tautologically true that will give you all rows; all rows with a NULL value for A will be silently dropped. 

I have had success in general replacing lengthy IN operations with a join against a temporary table. This makes sense because RDBMS are optimized to perform JOIN's as efficiently as possible, and the handling of lengthy IN lists will most likely be done by repeating the query for every value in the IN list. The performance issue you see could easily be explained by the simple fact of the query being run 5,000 times. 

The SQL value null represents the absence of data, so when you pivot and a cell as no data a null will perforce be generated as the cell value. It is your responsibility as the programmer to coalesce the null to whatever domain value is appropriate for missing data, when that makes sense. 

t is not always accurate to the business requirements to unpivot the table as you propose. For example, in a tournament scheduling database the table for Game will always have two distinct FK's to Team, one labelled HomeTeam and the other labelled VisitingTeam. Unpivoting this eliminates the business requirement that a Game is always between exactly two teams. Another example is the case of a database for an online meeting scheduler, where each meeting typically has a single Host and any number of participants. It wold be appropriate to embed the Host FK to the Participant in the main meeting table, with all non-host participants listed in a detail table. If this were an appropriate model for your business requirements then all but one of your reference_n fields would be pivoted out to achieve proper normalization. So, in the absence of the relevant business requirements, we are left wondering whether it is appropriate from a business perspective to unpivot as described. However, let's assume unpivoting is accurate to the business requirements. Why are you worrying about performance so early in the design? If it actually turns out that once you have a few billion rows this one table is at the core of a key performance criteria, there are a dozen or more techniques for addressing that only one of which is the denormalization you inquire about. There is absolutely no possible way that you can determine at this early stage which of those performance enhancing techniques will be an appropriate solution. Meanwhile, if you continue to denormalize your design in this way you will vastly complicate the writing, reading, testing, and development time of the codebase using your application. Are you sure that cost is worthwhile for a very, very, early guess at performance needs? 

You are thinking about this incorrectly. An assignment submission from a student not enrolled in the corresponding course, if it happens to get marked, is not fundamentally different from a properly submitted and marked submission from a student who subsequently drops the course. Allow the entry of the mark to exist in the database, by noting that when the final report of assignment marks for each course is generated, the (inner) join will fail for all submissions for students not enrolled in the course at exam time. That is sufficient to properly implement the business logic. 

Index width would be degraded significantly with your proposal. Just how do you propose to manage all those random 2-digit integers to enforce uniqueness. Have you thought of how much code would have to be written and maintained to implement this scheme. Won't typing all those key fields in for every join in the implementation be a joy. 

None of these requirements for 1NF is violated by the design you propose. For instance a Junction Table created to normalize a many-to-many relationship will always have a composite Primary Key, with each component being also a Foreign Key into one of the joined tables. 

Note that while both sides of every transaction will balance in toto, in general it is not required for the two sides of a transaction to be the same number of lines with the same values. For instance a simple Retail Sales transaction (in most jurisdictions) will look something like this: 

The standard implementation of any criteria specified as is as Update: Using this CTE for sample data: 

It's a Multi-Level Marketing system! Jeff Moden has written a a pair of articles here and here on efficient implementation of Hierarchical Reporting against a SQL Server database. There are a number of ways to store the hierarchical information but the two main ones are Adjacency List (each child has a parent foreign key) and Nested Sets (each parent stores details of its child hierarchy). Adjacency List is more intuitive and faster to update, while Nested Sets provides faster reporting. Jeff has explained this topic far better than I can, and developed efficient SQL Server algorithms for converting an large Adjacency List tree into a Nested Set representation,