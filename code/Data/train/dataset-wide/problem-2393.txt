That way "Id" is just a row surrogate key, but CustomerNumber would be assigned once and would be a datum included in the versions of the data you are tracking. I suppose that the CustomerNumber could be the first Id assigned to this Customer, but I would suggest an independent numbering scheme, perhaps check-digited for a degree of protection from entry errors, should that ever be needed publicly. 

If it was possible to connect to a SQL Server without setting up a security context on that server, that would be a serious security hole. So, there must be a security context. If you can ride on some existing rights that are already defined, then fine, but likely any existing security context was not tailored for your need. And since you want a specific account it is further complicated. Perhaps you could have a SQL Agent job created on the that would periodically PUSH the data to a staging table on your . (This would also allow the administrators to control what happens, just in case that is an issue.) Then you would grant rights on your to allow the 's job to insert data into the staging table on your . Note: When SQL Agent runs a job using a credential it is not impersonating the account, but should be viewed as logging in using the credential's account name and password. 

Create and configure a single-node SQL Server failover cluster instance. ... On each node to be added to the SQL Server failover cluster, run Setup with Add Node functionality to add that node. 

Try making your user account a member of the database's SQLAgentReaderRole. The rights granted by that role membership are described here: $URL$ Look at the description of rights under SQLAgentReaderRole Permissions. You will see that it has rights to read the information on the jobs. This does not require being a member of the role. It does allow the user to enumerate jobs, view properties, and view job history. The user account can only make changes to a job if that account is the owner of the SQL Agent job. Otherwise, it is read-only. 

You can use to change the names of constraints such as Foreign Keys. The syntax is simple: From the SQL Server documentation: $URL$ 

Well, that is interesting. I created your 3 databases using different collations for each database. I tried starting from different databases to see the results. Naturally I do not know which collations you used on each database, but apparently the SQL Server is (in fact) aware of this. 

Create a column using to just generate an ascending integer (or a bigint) to have a small unique key per row. This will help improve performance in some areas. Create a which would automatically enforce the Non-NULL and unique requirements. (Thanks to srutzky for correcting my description.) 

Since you have determined that the two database approach is not right for you, you want to merge the two databases. (And I agree for all the little value that is) (1) One suggestion, the simplest, is to rename one of the tables and make the needed changes to the code to use the proper table instead of the proper database and table. (2) Another suggestion would be to merge the two tables into a single table. You would need to add a column to differentiate between the "Quotes" rows and the "Jobs" rows. Also, you would need to make sure that your primary keys are unique between the two tables. If these rows were generated with Access ascending keys it is likely that you have duplicate keys between the two tables. If this is the case, you would need to change the Primary Keys in, let us say "Jobs", to avoid any PK conflicts with "Quotes" rows. Then also change all the referencing tables to have Foreign Key values that reference the new Primary Key values. If there are many duplicate tables, then repeat as needed to bring everything into the new united database schema. This will be a bit of work to set up, but once completed you should not need to think about that any more. 

If I understand correctly, that is not possible as you described it. Kendra Little described the actions that are available to you. Please read her article at: $URL$ A couple of important points: 

Change to Compatibility Level = 100 and break your code. Set Compatibility Level = 80 and your code will live a little longer. Find some way to globally edit your code, dynamically or statically, so that you can switch to Compatibility Level = 100. 

So, for your requirement just set the key to 0 (no auditing) then back to 2 (failed only). But, alas, as Shawn Melton pointed out, it requires the server to be restarted before the change can be applied. Of course, you can program for restarting the server as well. Does allowing a restart (or 2 restarts) fit into your process? 

I have not used SQL Server Compact, but because of its design, there are warnings in the SQL Server 2012 documentation about concurrency issues. See: $URL$ for warnings on: 

I do not use the designer for making changes, so I have no particular experience with script problems. I did some years ago find that the designers were prone to errors, so I am not surprised that you have some problems. Nonetheless, you find the tools valuable. From your description, you wind up with two scripts, but they are not identical. So the problem is how to treat the scripts. Or how to avoid any need to deal with the issue. Database backups or database snapshots are a couple of tools that can help. If you are using SQL Server Enterprise or Developer editions, you have the ability to create database snapshots. So this solution depends on your SQL Server edition and you rights on that server and how many people are sharing that database. Scenario using a snapshot database and assuming that you have full rights to do whatever you want to with the design database. First, create a SNAPSHOT database (examples from the SQL Server documenation): 

With your list you do emphasize cost as a factor and the database is not expected to be huge. So, you can use the free version of some vendor product (such as SQL Server Express), or use open source offerings (such as MySQL, PostgreSQL, etc). But you will pay, of course, for any choice. Once you know where you want to spend your resources (which only you can answer) you should be able to discard some of the options. 

Yes, it is necessary to drop and recreate the linked server. This takes just moments, so the hit is very small. Likewise, there is no reason to believe that SQL Server would try to read the internal contents of a different relational database. 

MySQL - At $URL$ discusses how to use EXPLAIN to determine the usage of indexes. Postgresql - At $URL$ outlines the usage of statistics in views such as . Microsoft SQL Server - At $URL$ in the sys.dm_db_index_usage_stats view which reports statistics such as seeks, scans, updates, and latest usage. 

Kin made a couple of suggestions which should be helpful. First: Make sure that your statistics for the CallTime index are up to date. Your plan shows that the time filter is being made by seeks to the clustered index. So, for some reason the CallTime index is not being used. What is the definition of that index? If it is a multicolumn index, be sure that the most specific column is first. Example: 

The sad truth is that you cannot do anything computerwise about the 'sa' activities. This is because the 'sa' (or a member of the server role) are defined to be able to do pretty much anything. So, of course, you should only have that you can trust to do their best to adhere to the standards for the database. But even the best is imperfect. So you might set up some auditing to report on changes made in the database. (Or, more low tech, periodically check your views for a restricted table name in a view.) So it depends on what you are concerned about and how much effort you need to put into protecting the secure resources. In terms of views that expose secure information, you can DENY permissions on a view. Even if a created the view, the DENY of the view for those without approval will prevent their access. Perhaps any high security views could be named something like SecureViewXYZZY so as to emphasize the reason for those views and the restriction of who can use them. 

If there are more than two computers involved, then probably needs to be configured. Are you sure that SQL Server Agent installed correctly. Check for installation errors in the Error Log. Have you manually tried starting or reconfiguring SQL Server Agent? Or used the Services panel to start SQL Server? Or run . 

However, if you are interested in thinking about that possibility of skipping Primary Keys and Clustered Indexes altogether, you might read Markus Winand's post linked below. He demonstrates his reasons, with some code samples, to suggest that it might be a good idea at times to forego using those features. $URL$ But it all finally comes back to understanding your application and designing the code, the tables, the indexes, and so forth to fit the job you are doing. 

For merge replication issues, first stop is to read Chris Skorlinski's blog post at $URL$ He links you to several resources at the top of his blog post. The post is from 2010, but merge replication has not had any significant update as far as I know. Chris's bottom line is that there are too many variables: complexity of queries, power of the server, concurrent processes, et cetera, to be able to provide a recommended configuration. For example: "I (have) seen simple designs with little or no filtering supporting 1000 of users to very complex filtering/join design barely able to hand 10 subscribers." One link that he particularly recommends is: $URL$ Rob's focus is on merging replication to a SQL Server for later shipping to SQL Compact. But focus on the issues of merge replication and he has several suggestions, such as: 

Almost certainly it does trim the transaction log. (Otherwise the Amazon cloud will fill up with transaction logs.) Some details from the Amazon site include this topic and two sub-topics that you can investigate: $URL$ 

If this FinalTable gets really big, then there will be some serious overhead. That is why this time I created indexes on both StagingTable and FinalTable for columns A and B. This will help the joins run more efficiently. However, there are also more powerful tools, such as SSIS which could be a big help as the job gets bigger. The code above is just a simple picture of how to construct the logic. 

Review the options that you would like to use from that page. As Shanky described in his comments, shrinking files is a really bad choice. Using the or the options of will give you better results. However, the should not need to be run frequently. Analyze the degree of fragmentation in order to choose the frequency and the window of time you will use. An with either or will order the data into the update pages while reserving the space indicated by the . This means that if many of the data pages are fragmented, perhaps due to many deletions, the data will be moved around so as to put data in the order. While the data is being moved into logical order, it will empty pages and extents which will result in recovering space in the database. EDIT: $URL$ (for 2014) explicitly makes the following comments: "Rebuilding an index drops and re-creates the index. This removes fragmentation, reclaims disk space by compacting the pages based on the specified or existing fill factor setting, and reorders the index rows in contiguous pages. When ALL is specified, all indexes on the table are dropped and rebuilt in a single transaction." "Reorganizing an index uses minimal system resources. It defragments the leaf level of clustered and nonclustered indexes on tables and views by physically reordering the leaf-level pages to match the logical, left to right, order of the leaf nodes. Reorganizing also compacts the index pages. Compaction is based on the existing fill factor value." 

The database space is freed for the use of other expanding databases and/or for creating further new databases. If you are concerned about data loss you could archive the LegacyDatabases to some backup location, a file share, tape backups, whatever your choice may be. 

Have you looked into using SQL Server's support for Indexed Views? (Generically this is called a materialized view.) If your code really needs an assist in getting the answers this is a option, though not without its own difficulties. An Indexed View is a duplicate set of data taken from the main tables and stored within the Indexed View. There are rules and limitations, but it basically creates a specialized "table" that should be index-tuned to suite your purpose for using it. This MSDN topic points you to the details that you need: $URL$ The definition of an indexed view must be deterministic, needs certain settings to be adjusted, and there are actions such as OUTER JOIN and ORDER BY that are forbidden. EDIT: You should also use the NOEXPAND hint, as explained by Aaron Bertrand, to "ensure the uniqueness guarantee provided by the view indexes is used by the optimizer." See: $URL$ 

The LIKE pattern both controls the required characters and the 9-character length constraint. But the pattern should also include only numeric values, so then it would be: 

My experience shows that indexing just about everything will hurt your performance overall, for the reasons mentioned above. Both and should only contain the indexes that would be of best use for your queries. Of course, your first indexing decisions may need to be refined over time as you learn more about your database's behavior. Do not be afraid to add additional indexes when needed. And do not be afraid to drop an index that is not being useful.