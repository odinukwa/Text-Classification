From your description, you'll need the following tables ("..." means you might need additional fields in that entity): 

You need to talk to your network admin to see if Kerberos delegation is enabled, and whether it's constrained or not (if it's enabled and unconstrained that is a massive potential security risk, just by the by). If it is constrained then you need to register a SPN (Service Principal Name) for your SQL Server and bind your SQL Server account service to that SPN in your AD configuration. $URL$ has an explanation of it from the point of view of a web server that wants to pass user credentials to a SQL Server (whereas what we're doing here is passing user credentials from SQL Server to a second SQL Server, but the principle holds). If Kerberos delegation is not enabled and your network admin refuses to budge on that (and you can't create SQL logins), then you're out of luck and need to escalate via your management (or think of an alternative to using linked servers), since NTLM authentication (which is the fallback if Kerberos isn't enabled) doesn't work with linked servers. 

Firstly, the correct term for this is an audit table (or maybe audit history). Transaction logging is something entirely different (that's a core part of the DBMS which logs all active transactions to guarantee the ACID properties) - the DBMS transaction log (which is in a binary, very-hard-to-read format) is reused once it's no longer needed ("once it's no longer needed" is a bit vague, but a full dissertation on transaction log semantics is a bit beyond the scope of this answer), where an audit history stays forever, as long as you don't clear the table. To answer the actual question - it should be done inside the database (triggers being the easiest way). The basic reason for that is you want anything that's integral to the data (i.e. constraints, security rules, audit history etc.) to happen, no matter which application is using the database. Additionally: 

Edited after a couple of comments by the question asker!: Better idea for your design: (If you're not familar with these diagrams, that's seven tables, with foreign key links as pictured [the three tables all have two foreign keys referencing the data tables]. It probably looks like I'm overcomplicating things, but trust me, "tokenise once and store in the database" is so much more efficient than "tokenise data each time you retrieve it".) Sample data, using a simple tweet (ignoring Users for now, it's the same concept though): "test tweet! #howdoesthiswork #newbie" 

The SessionProductLink is a Junction table, which is the 'bit of the design' you were missing. The field names in italics are foreign keys (it should be obvious which fields they link to ...). the ID fields are the clustered key fields in the first three entities, for SessionProductLink you'd want a compound key of (ProductID, SessionID). (You can use a surrogate key there if you want, that's a style decision you can make) Other things you might want to think about: 

That said, based on what I've seen the query plan would look something like this: (this is vastly simplified, the actual output from sp_showplan is rather more detailed) 

If you need to redesign your database, chances are you'll need to rework some of the associated code too (that's entirely normal), but you should be able to redesign your database without losing any data. (Obligatory note: take a full backup first.) 

(This was originally a comment to @DaveE's answer, but I've put it into its own answer because it got long) is a logged operation. It has to be otherwise it's not ACID-compliant. However, differences between and : 

Put an expiry date into your user table, then every week shift all old expired users to an archive table. You don't have to break your data model for temporary users, and you keep a reasonably small users table. 

The reason it shows the after the is because without the there's no data to - I'm reasonably sure that you can effectively consider them as one operation in this case (i.e. the restriction conditions will be part of each index scan, rather than the index scan copying a whole lump of data and restrict throwing most of it out). 

The potential downside of that is data inconsistency (i.e. staff members having overlapping targets) - you can fairly easily write a script to check that though. I'd recommend making an stored procedure and wrapping all the checking/updating logic in that, also. (A couple of style notes - I rewrote your query slightly to seperate the join condition from the where clause. Also, I know it's only an example, but is a bad habit to get into - you land up transferring far more data than you need.) 

(I've used ANSI SQL syntax, I'm not 100% sure if MySQL supports all of ANSI SQL - but it should be close enough.) 

If a has multiple and an has multiple then yes you need a link table between the two (as per the answer in your comment). However, if, for example, you only ever want to attach a single to any given , then you could just have a field for genre inside the table. 

Essentially, the backup will be of the state of the database when it finishes the data-reading portion of the backup (so all of the data will be backed up), plus whatever amount of transaction log is required to ensure transactional consistency (the start time of the included log is ). Paul Randal covers this here (with aid of a diagram, which makes it all so much easier). In your example, would be committed (or rolled back if a was issued instead of a ) and would be rolled back (regardless of the end result of that transaction). (The other reason you try and do backups at a quiet time, aside from I/O contention, is that all of the transaction log generated during a backup normally has to be included with the backup.) The recovery phase of a database restore takes all the committed transactions from the log included in the backup and applies them to the database, and rolls back all the un-committed transactions. (This is why / is important. and you can use the database, but you can't apply any further log backups, you need to restore it in order to roll in log backups. Recovery breaks the log chain by rolling back uncommitted transactions.) Further reading: 

No it wouldn't (that is, it's not logically equivalent. Performance would probably be better though). Two reasons: 

My guess is that it's a legacy support thing (i.e back in version 1 of their software, they had date and time in seperate fields and they've been forced to support it because it always worked that way). That said, there's nothing brilliant about seperating date and time, so there's nothing you're missing. (The other option is that your ERP system developers didn't know that a datetime field can store a date and a time, but far more likely is that they had a business requirement to make it work the same as prior versions.) 

DBCC uses snapshots internally. Snapshots are then implemented as sparse files in Windows. So this is actually a problem with Windows' handling of sparse files, which causes the snapshots used by DBCC to occasionally break on large and very active databases. It's reasonably well documented with a few recommended fixes in $URL$ However, if you can DBCC on the dev server I'd recommend that instead. (I've personally only seen this problem with databases that were still serving traffic while being DBCC'd/snapshotted. Backup/restore to an idle server, DBCC there => no more 665 errors.) 

The last time I played with Access was when 2003 was the hot new thing, so this may not be entirely accurate to every detail. However, what you need to do is go to the query designer, change the view to "SQL" (i.e. raw text entry) and then you want to your two left-join queries together, e.g. 

Much simpler: You can find a full list of styles in Sybase BOL, although the explanation of each style isn't particularly clear (and is actually flat wrong for style 12 & 112). The Complete Sybase ASE Reference Guide includes a full list of styles with examples. (Please note that the complete guide is externally published, not from Sybase. Also I'm not affiliated with the book or the publisher, just mentioning it because everyone working with ASE should have a copy of it.) 

If you're the dbo (or aliased to dbo) or sa (which is implicit dbo on all databases), then you automatically have all permissions. Otherwise, a anywhere on the permissions chain overrides any . I'm not 100% certain on this, but I think if you have a user in two roles, it will apply all the granted permissions from the roles, then layer any over the top. 

First of all, if this is homework, please tag it as such. Secondly if it's not homework and you're doing this in a professional environment, get a professional to do it (or at least to thoroughly scrutinize your final design). Schema design underpins your application design, and flows on from clarity in business requirements and how well you understand those requirements. If you don't completely and clearly understand the requirements, your schema is going to be miles off the mark. And, to actually answer the question (at least as best I can with the information given), I'd keep the staff in one table, however you may want to create ancillary tables for each 'type' of staff member (doctor, nurse, janitor, admin, etc) to store data specific to that staff-member-type. To give an answer that's any more specific, the requirements need to be more specific. 

First point is that in any situation where data is changing rapidly and extended-duration table locks are taken, you're going to have induced latency in updates to those locked tables. Unless you use snapshot isolation that is. Caveat: snapshot isolation may not be appropriate for a reporting server (it increases memory usage and can hit tempdb hard), it depends on how hard you're pushing that server. Second point - have you looked into replication, specifically transactional replication? It should provide near-real-time updates without constantly pushing the database into recovery like log shipping does. Note that it may have a performance impact on your primary database, though. Third point - Are your indexes appropriate to the workload your database is undertaking? If you're not sure, profile it. The Database Tuning Advisor is a useful guideline, but don't believe what it says ipso facto (like carpentry's "measure twice, cut once", I'd like to propose a similar rule for database tuning - "profile twice, index once"). Use the Index, Luke! is a good site to start with if you thought an index went in the back of a reference book. Fourth point - if you've got reports that are doing massive rollups (i.e. months of data or whatever), is it okay to have some lag time in those reports? If so, you can schedule a task to create some summary tables (weekly, daily, hourly - depends on your requirements) and have your report hit those instead of summing ten million rows or whatever. Generally if you have a quarterly rolling income report (or whatever), then up-to-the-minute accuracy isn't necessary.