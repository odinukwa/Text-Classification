I was getting 403 & 404 errors on a Magento application I was working on I tracked down the issue to two blocks in the NGINX configuration, if I comment them out the issues are resolved but I would like to understand better what I'm commenting out. ISSUE 1 I was getting a 403 error on URLs such as 

I stopped and started our Dev EC2 instance, I just wanted to test something. Now when I try to access our web app via the new AWS Public DNS I get an error. 

Two things confused me about this, first off I don't understand why the server block would not be allowed, after checking online it seems that this can be caused if config files are not included in the block of my , but they are: 

We have a magento store which appends a query to our URL when you change Magento store views on the frontend. i.e. $URL$ Is there a way to remove the ?___store=uken with NGINX configuration? Say remove any query beginning with ?___store=. I'm very new to server config & nginx in particular. 

Can I just copy these to a location on the new server and reference them in the NGINX configuration file? Or do I need to generate a new ssl key, re-key the crt file(which one)? 2. NGINX CONFIGURATION The NGINX configuration only seems to need reference to two files Apache does? 

For people who may be unfamiliar with data deduplication, it is a technique whereby data is analyzed at the file (or block, I suppose) level, and where identical files/blocks throughout the file system are replaced with a smaller token. This has the effect of greatly shrinking the effective size on disk. It could be considered a form of copy-on-write. Read the wiki page on it. There is no filesystem that I have heard of in Linux to do dedup, file or block level. Such a beast would be handy, although pretty processor intensive. 

If not, check /var/log/samba/* and /var/log/messages to see why it didn't actually start Step 3 - Can we connect to it remotely 

I was looking for a good vCenter or even just ESX(i) check for Nagios a while back, and I didn't find anything. I ended up writing one-off scripts to cover the need, and I'm going to go back and improve them when I get a chance. There is this: $URL$ but I haven't tried it, as it's only compatible the commercial version of Nagios, Nagios XI. If you can script in perl, the VMware SDK is available here: $URL$ It should contain all of the functions necessary to do what you want (and then some). 

I hate to ask questions that don't really address the original request, but are you sure there's no way around keeping it in your office? Servers really weren't meant to be "quiet" or for use in habited areas. If the machine must be in your office, is there a reason it has to be rackable? Could it be a desktop on a rack shelf? There's got to be a better solution that won't drive you crazy. 

I'm trying to allow another developer to connect to a our ubuntu server and they are getting the below error in FileZilla. 

I'm moving a site to a new server running on NGINX. The old site's Apache2 VirtualHost has configured that I want to replicate in the NGINX configuration. From what I've read on the NGINX Docs this seems to be simply achieved with . I just want to make sure what I have is correct. From APACHE2 

Also I tried running the syntax test after removing the configuration file I am not using $ sudo rm default-staging $ sudo rm ../sites-enabled/default-staging $ sudo nginx -c /etc/nginx/sites-available/sendy-newsletter -t nginx: [emerg] "server" directive is not allowed here in /etc/nginx/sites-available/sendy:1 nginx: configuration file /etc/nginx/sites-available/sendy test failed 

Our webserver is on AWS running on Ubuntu 14.04.3 LTS. When I ssh into the server I see this message. 

I'm not really sure what this block was doing or what means, I'm sure I just took it off someones blog who recommended it. Any idea what it is doing? Below is my full NGINX configuration file, thanks in advance for any help and advice you may have :) server { # Listen on port 80 as well as post 443 for SSL connections. listen 8080; #listen 443 default ssl; 

We are running the site on Nginx & HHVM. I checked the /var/log/nginx/error.log and I see this error: 

You may need to do some research about DNS and it's records. The First line in the first example is the Start Of Authority. It provides basic details such as the master nameserver, the administrative email address, the zone serial number, and timer options. Second line is determining that the server is indeed an authoritative server for the zone. Meaning it has authority to respond for the domain. This is the purpose of NS records. The following is the glue record associated with the nameserver. Since the NS record is in the same zone as the domain you are looking up, it needs to determine the IP to hit in order to preform the look-up. In the second example, it follows the same path, except, since the nameservers are in a different zone, it does the look-up inside of that zone instead. With DNS, it is a systematic process to determine you got the right records for the right things. That is why you will often see multiple lines as it steps through the appropriate records. 

This one took me a bit to understand. When you run "free -m" you need to look at the "-/+ buffers/cache" line under the free column. This shows the available memory for use. Memory that has been used and released back to the system is just notated differently. As far as the system is concerned, it's not free, but it's also not in use and available for programs to pull from. 

On our Magento store when we try to we go to the product page in the admin section to upload images, we see that the images which we already have there on the front end are missing and there is no option to add images as per usual. 

I thought, maybe I added the public key to the on the server incorrectly. She sent me a public key like this called 

Also I notice that in the browser address bar the public DNS I entered changes back to the old Public DNS we had before, even on machines I never accessed the our app from before. I can access the site if I add a host entry to my local PC with the public IP address. Any idea what could be causing this? UPDATE I tried rebooting and got a 502 bad gateway error but once I restarted the NGINX server it went away but I still get the connection time out. 

FINAL SOLUTION Just to clarify, my mistake was that I set the password for the wrong user, I tried to set it for root whereas I should have set it for ubuntu with: 

Our site has been running slow recently and I've noticed that the RDS write IOPS can go as high as 80/sec and as low as 9/sec. Is this something that could be causing pages to load slowly? If so what could be causing it. Our RDS is a db.m3.large with 7.5 GiB memory and 100 GB SSD storage. On average we have about 2 people using our application at once, 5 max often just 1. Sorry if I'm not giving enough info, I'm very new to systems admin. 

I believe what you are seeing is based on the verbosity level in the console. I'd start with doing "core set verbose 1" and increment it until you get the details you want without the details you don't want. 

Things like this vary a lot. For instance, are you recording calls? If so, are you using Monitor or MixMonitor? Monitor is processed in the same thread as the call, MixMonitor in it's own thread. And if you are recording, you probably have a solid disk hit. I solve some of this by turning off atime in /etc/fstab. Something you can do to get a idea of what is going on in your system is to run vmstat. A simple vmstate 1 20 will you an optput to look at and you can see what is eating at the CPU. Another thing that you can do with asterisk is remove modules you don't need by adding "noload =>" lines to modules.conf. Often, there are a lot. You'll just have to take some time to learn what modules you do and don't use as all are autoloaded during startup. One more thing to consider is trans-coding. If you're accepting calls using the G.729A codec and your softphones/deskphones use G.711u, you're going to take a performance hit as as it has to trans-code those codecs and can't just preform packet-2-packet bridging. 

I'm afraid you assume incorrectly. Depending on the filesystem and underlying device, the bytes that once made up the file might still live on disk, but it's going to be inaccessible to anything short of full-on forensic recovery, which I'm certain you don't want to pay for. Have you checked your temp directory to make sure you don't have another copy laying around? Because you're probably not going to get that particular file back. Sorry for the bad news. 

Dia ($URL$ is the preeminent free opensource diagramming software. It's also pretty ugly. If you're on a mac, I recommend Omnigraffle ($URL$ It's my favorite diagramming software on any platform. It's neither free nor open source. There are also online tools that actually look pretty good. My favorite so far is Gliffy: $URL$ 

Yes, definitely talk to your internal IT people who are responsible for administering this It's possible that problem-domain.com has DNS that automagically responds to all DNS requests with a record pointing to their own IP block, typically to send traffic to a "domain not found" page with ads. This is called DNS hijacking. 

I'm using openfire and pidgin. For email, I've outsourced to an Exchange provider, so that we can use all of the niceties of Outlook and have corporate blackberry and active sync. 

Write the prompt first, read the variable, then use the variable to connect with scp echo -n "username:" read USERNM scp $USERNM@whatever