As you stated, as long as you do not do any arithmetic, sorting or use relational operators on the values, I would go with BIGINT. The documentation just mentioned storing the number as a string to bypass intermediate steps done through double precision representation. 

Note that both and are fields with values. From the Helpful Notes About values at the Bottom of that Page 

Now, load the mysqldump into the Slave. Line 23-25 of the dump file has the command, but it specifies log file and position only. This will set those values on the Slave. Finally, run these on the Slave 

Then restart mysql. As for the error messages, the InnoDB System Tablespace (ibdata1) and the Redo Log Files (ib_logfile0,ib_logfile1) are being created. From the above messages, my guess is that you did this something like this 

If you want that kind of access to other connections, you must upgrade to MySQL 5.7. Sorry, there is nothing else available in MySQL 5.6 and prior. 

Then restart mysql Perhaps does not understand server-id. Normally, options not understood by are supposed to passed to . That's probably broken in MySQL 5.1. 

I cannot make promises on the queries, but, at the very least, the index suggestion should help. Give it a Try !!! 

Every time you INSERT and hand mysqld a row where you set an auto_increment column to NULL, mysqld tries to reconcile the INSERT's Primary Key by finding the next auto_increment value and handling it internally. That's could be a little taxing on mysqld because handing a PRIMARY KEY (which is all defined as ) a NULL value forces this reconciliation exercise. That is really unnatural in terms of ANSI SQL. 

IDEA #2 : Poll the Processlist The problem with using either the slow log or the general log is that entry will not entry the log until the action is done. In other words, if a query takes 10 min to run, the entry will not land in the log file until the command is done executing 10 minutes after it started. Wouldn't it be nice to catch the bad queries in the act ??? You should use pt-query-digest. Even with queries that are not done, a histogram will be generated with the pattern of queries that have executed or are still in progress. Rather than write up an example, please see my posts where I discuss using pt-query-digest 

You do not need to consult the log. mysqld can tell you. For anyone running MySQL 5.6 and prior, or MySQL 5.7 in MySQL 5.6 compatibility mode 

50MB is very small. I would not worry about size at this time. The DB can grow if you do a lot of coding in the DB, updating form designs, create new stored queries, and things like these. If your queries are slow, it would not be due to DB size because Access tends to append data changes rather than handle data changes in place in many cases. I would look into the indexing of all your tables. Look for any queries you are making that use WHERE clauses whose columns are not indexed. You may need to consider compound indexes for WHERE clauses with multiple combinations. What concerns me is the number of columns in the HOUSES table. 60 columns? That is a lot of columns. Changes are the lenght of many of the rows are spanning longer than Access would like. Fetching just a single row from the HOUSES table is proabaly an expensive operation. You should look into denormalizing the HOUSES table. You could just go to the tab, and Click . In addition, you could just go to the tab, and Click and start measure other potential bottlenecks. If the database starts to grow too large (over 1GB) or if you DB is in the frequent of doing heavy updates of rows, you should run Compact/Repair periodically. You may be surprised how much space will be reclaimed in doing so. You can do it in one of two ways: 

under the group header before you restart mysql GIVE IT A TRY !!! I dug a little further and learned why 1400 comes up for table_definition_cache According to the Documentation for table_definition_cache 

There was a bug report about this issue concerning "freeing items" and the query cache. Although the bug is closed, there was no mention of innodb_thread_concurrency. Conincidently, I spoke with Ronald Bradford at Percona Live NYC back in May. I told him of a situation where I tweeked innodb_thread_concurrency because MySQL 5.5's multiple InnoDB Buffer Pool produced a tons of thread locking and I suspected that cached data I need most likely had spread amongst the multiple buffer pools. He plainly told me that I should never set values against innodb_thread_concurrency. Let it always be the default value, which is now zero(0). By doing so, you let the InnoDB storage decide how many innodb_concurrency_tickets to generate on its own. This is what infinite concurrency does. 'Freeing items' most likely occurs more often when we impose limits on innodb_thread_concurrency. That should always be zero(0). I would take a risk and raise innodb_concurrency_tickets and see if it helps or not. 

Then, schedule a MySQL Event on the Slave for the Stored Procedure to go off every 5 minutes starting 5 minutes after you create the event: 

Advaned concepts will come with experience, practice, and data mining all the MongoDB blogs going forward. 

Therefore, adding the PRIMARY KEY to a secondary index is definitely redundant. Your index entry would like . This would also needlessly bloat the secondary index by having 2 copies of the . For the index with , if you were to run a query like this 

max_binlog_size With regards to max_binlog_size, what you are asking is not possible. Why ? On , I answered the question MySQL - Binary Log File size growing more than the maximum limit. I mentioned the following from the MySQL Documentation on max_binlog_size: 

Now, no matter what order id1 and id2 are entered, id1 will always be less than or equal to id2 This may not be contextually what you want because id1 and id2 are unique identifiers. 

Instead of lots of INSERTs into your table, try putting all your new data into a CSV file and use LOAD DATA INFILE to mass populate your production table. You should disable keys before loading and enable keys after the new data is loaded. There is no need to optimize table doing these things. You may want to run analyze table instead. That will update the index statistics on the MyISAM table. I hope these suggestions help !!! 

You can shorten the running type of queries by indexing certain columns. Since you mentioned sorting DATETIME fields, one of the best ways to bypass sorting of temp table results is to use a covering index which includes the DATETIME as the last column of any given compound index you need. I know this is entirely possible because in a link about Covering Indexes By Ronald Bradford, he gave a list of future topics he was giving. Here is that list: 

If you know the exact IP of the incoming MySQL users, you can do the following Suppose the user is called and the incoming user's IP is . From what you are saying, there should be a MySQL user within called . You cansee that user by running this query 

The expression is based on . That matched. When you executed it, the change evidently failed because the column didn't change because does not equal To verify that run this: 

When you issue , a lot more happens than just cutting off DB Connectivity. The link in the comment from @ethrbunny already explains what things happens. I would like to focus on one particular aspect: The InnoDB Buffer Pool. InnoDB has to flush the InnoDB Buffer Pool's dirty pages. If you want to know how much, run this before shutdown: 

STEP03 : Load the mysqldump into the Slave Execute the mysql client loading the mysqldump into the Slave's mysql instance 

SUMMARY After reading these, you should develop a healthy respect for the binary executables and a responsible attitude towards running them. 

The main idea is to have be nonexistent just before the dump. The output file gets renamed to something you already use elsewhere. So, there is no need to change other processes or scripts. Give it a Try !!! 

Run the Column Query and Collect the Results in STEP 03) Perform Unique Sort ; Check For Duplicate Table Names 

Given these comments about partitions being processed in parallel on their wishlist, it would have to be true at this present time to say that all partitions have to be searched sequentially. UPDATE 2014-02-21 10:51 EST With regard to your original question, you stated 

instead of having a remote root. SUGGESTION #2 If you really want the remote root, here is what you do 

Both approved and sid should be together. IF they are not together, the MySQL Query Optimizer may decide to perform an internal index merge of the primary key and an index where approved is the first (or only) column. In fact, your new query should be refactored as follows: 

APPROACH #2 : Increase max_allowed_packet This may be the preferred approach because implementing this is just a mysql restart away. Understanding what the MySQL Packet is may clarify this. According to the page 99 of "Understanding MySQL Internals" (ISBN 0-596-00957-7), here are paragraphs 1-3 explaining it: 

Notice the column max_user_connections. It puts a moratorium on the number of connections a user may make per hour. Check and make sure the user '557574_prod' isn't connecting to mysql a ton of times within less than an hour. This is used in conjunction with the option max_user_connections that you can set in /etc/my.cnf Please change either max_user_connections in /etc/my.cnf or, if the individual user has this column set, that column mysql.user.max_user_connections. If you have no config control over my.cnf and you cannot do SQL against the mysql schema, then you beg and implore your hosting company to raise those limits for you. I believe you can set the max_user_connections as a session variable, but you may need certain privileges, probably SUPER. As for establish connections, you need to keep those connections open. Perhaps sending a heartbeat of some kind by doing a SELECT. But guess what ??? The db columns max_questions monitor SELECT queries per hour and max_updates monitors INSERTs, UPDATEs, DELETEs per hour. YOu may need to check these columns in mysql.user. You can quickly check those number with: 

ISSUE #2 : DNS If you are connecting to mysql using DNS, this could be one possible source of issues if: 

ALTERNATIVE What you need is to simulate the BENCHMARK function yourself. Here is some sample code for you to try 

Give it a Try !!! CAVEAT : Once you are satisfied with this operation, you can drop the old table at your earliest convenience: 

If you see multiple MySQL services defined in , you have multiple MySQL instances running. METHOD #2 : Check via the Task Manager 

From the looks of your numbers, you must have hit some bug or pre-GA behavior. Why? According to the MySQL Documentation, the default for table_open_cache is -1 for GA releases of MySQL 5.6. This tells mysqld pick a fair starting value give current OS conditions. What makes that value weird is the fact that 400 is the default for MySQL 5.6.7. The first GA release is 5.6.8. It may be a coincidence, but my guess would be that you set table_open_cache a little too high. Evidently, 5000 is a tolerable value for your DB Server. SUGGESTION Try experimenting with table_open_cache_instances setting it to 2 or 4. Then restart mysql and see. 

This trigger will do the job. If you are doing a bulk of rows into the table, this trigger will slow things down a bit. If you plan to do such bulk s, you are better off doing that in a single : 

You may need to downgrade PHP after all. If your hosting provider isn't in position or unwilling to adjust MySQL or give you a downgraded PHP, you will have to go with a hosting provider that will. As an alternative you may want to look into Amazon EC2 when you can have full control of all software upgrades after running . The links you mentioned before gave you what you needed to know. I gave you what you can do to accommodate the MySQL side. Do what you know has to be done to get this solved. 

OK Great. It has the data. To be honest, I think SQL Server can perform all of this in a single pivot query without a handmade temp table. I could have taken it to another level and concatenated all the queries into a single query, but the SQL would have been insanely long. If your actual query had 1000s of rows, a single MySQL would not have been practical. Instead of running the 3 INSERT queries by hand, you could echo the 3 INSERT queries to a text file and execute it as a script. Then, you have a table with the products and categories combinations individually written. 

Perhaps you can create a MySQL Event to call this Stored Procedure every minute. If Amazon does not let you have the EVENT privilege, you will have to write an external shell script on the EC2 server to connect to the DB and run the Stored Procedure. That shell script can be put into a crontab. If Amazon does not let you have the PROCESS and SUPER privileges, you may need to move the DB out of RDS and into another EC2 instance running MySQL to accomplish this. You could then create the MySQL Event without Amazon's hosting restrictions. 

Rollback Segments and Undo Space would know what changed data looked like before changes are applied. Redo Logs would know what changes are to be rolled forward to have data appear updated. You also asked 

If is less than 5% of , then the cardinality of can disqualify the from being of any use. Smae going with . For that index , run this query: 

That's why the binlogs start with a filename of Some feel disables binlogging. The MySQL Documentation on says the permitted value is a filename. Thus, it is not boolean. Simply removing the line or commenting it out disables binlogging. Suggestion #1 You may want to start mysqld manually using 

As a MySQL DBA, may I be so bold as to suggest my favorite tool? The mysql client program itself There are also a variety of startup options for the client program. Examples: 

Using will catch any error-based output (aka stderr). The mysqldump should still pipe normal console output (aka stdout) to the other mysql session and load the data as intended. EXAMPLE : I have a small database called sample on my PC. I ran this: 

Somone submitted this state as a bug, but the bug report claims it is not a bug. Both of these links describe DDL (Data Definition Language) against a table as a contributing factor. What is interesting to keep in mind is that doing something like is, in reality, DDL. We have to aware of any SQL we run. We may think it is SQL that is part of a transaction when, in fact, it is DDL. is an example of DDL we think is DML. This can cause current transactions to commit and may have unpredictable results when it comes to writing binary log events as the first link states: 

No need to have the myenum in any indexes. Leave it to the MySQL Query Optimizer to search the correct partition should any SELECT query have a clause that includes . If you ever have to increase the number of unique values, you will have to increase the number of partitions. Give it a Try !!! UPDATE 2013-10-24 17:57 As I said in the comments, you should partition by the enum with the highest cardinality. What about the other enums? DO NOT INDEX THE ENUM BY THEMSELVES !!! If your SELECT queries include WHERE AND enum4=...`, you should think about making compound indexes of enums. For example, if you have enum2, enum3, and enum4, you could make compound indexes like these: 

If after making these my.cnf file and restarting mysql you still cannot change innodb_buffer_pool_size, then I would suspect the mysql binary mysqld. It is most likely source compiled and probably has the values for InnoDB hardcoded. To verify if the mysqld is source compiled, check the processlist of the OS while mysqld is up and running. If you see dozens of mysqld connections, chances are the mysqld was source compied. From there, I would suggest uninstalling mysql and downloading an RPM or tar installltion. Then, see if my.cnf responds. 

represents the current binary log on M1 that was last read on M2. represents the current binary log on M1 that was last executed on M2. Let's suppose that M2 has this: