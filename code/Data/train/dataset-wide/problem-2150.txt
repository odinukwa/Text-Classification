I run a service where I deliver a lot of downloads that go over 1 extra hosts between the destination and origin host. They are represented by the integer interpretation of the 32 bit IP address. My system currently handles about 500 inserts/second during peak hours. I run a master-slave system. The master has an apache webserver with a PHP file that gets called from remote hosts and inserts a line into the log table. Then the changes get replicated to the slaves where queries happen. My queries are primarily aggregations over the mb_transferred field over a range in the time field filtered by client_id. 

Here we store the full username also in the adressdata table. To me this has the following advantages: 

The maser server runs an apache webserver with a simple php file that does the insert and is called by other servers. My server is now almost at the limit. I already upgraded to big hardware. I thought about using a GUID as primary key and using master master replication, that will for sure relieve something, but I think its short sighted, because it does not decrease the insert amount per server. I am expecting higher trough-puts in the future and I am also worried about database size. Also in future I plan to have a second table which defines "weights" for certain services. Something like: 

Then I want to run join queries against this table and multiply the mb_transferred field with a weight factor. I also want to add fields like "transfer_duration" to the logs table to calculate the speed of downloads and run queries to get statistical data how how well/bad the connection between certain networks, or certain servers for certain hosters is. The point is. The data structure is simple, its just a huge amount of rows. I have a lot of aggregation functions. This makes a light bulb in the "map reduce" section of my brain flashing. I thougth about doing vertical shards and use client_id as a breaking point. For example if I have 10 server send every user to its userid mod 10 server. This would be easy and relieve the load. But scaling will probably be awkward. So i think with the size of the project that I am expecting to reach soon with the current growth I cannot do anything but turn towards a distributed database system. I already tried to examine cassandra, project voldemort, amazon dynamodb and hbase but no matter how much I read I seem to run against walls. I think the long years of relational thinking are somehow blockading my mind. Can someone point me into the right direction on that? What database system would be suited for my use case and why? 

Replication across non-trusted domains or workgroups can be done using Push Subscriptions in conjunction with SQL Authentication for the replication agent process accounts. Alternatively, you can also use Windows Authentication by configuring pass-through authentication. To use pass-through authentication, create a local Windows account on both the Publisher and Subscriber that has the same username and password. Use this account for the replication agent process account and have the connections to the publisher, distributor, and/or subscriber impersonate this account. Ensure the account has the permissions required in Replication Agent Security Model. This approach is covered in the section Use Windows Authentication to Set Up Replication Between Two Computers Running SQL Server in Non-Trusted Domains in HOW TO: Replicate Between Computers Running SQL Server in Non-Trusted Domains or Across the Internet. If you have anymore questions, please let me know. I hope this helps. 

There isn't a built-in or easy way to do this with Web Synchronization. You're best bet is to programmatically monitor Merge Agent sessions at the Publisher/Distributor using sp_replmonitorhelpmergesession and sp_replmonitorhelpmergesessiondetail. You can script this out and poll on a schedule. 

To initialize a Transactional subscription from a backup you actually end up creating the publication first, enabling it to initialize subscriptions from a backup, then take a backup, and then initialize subscriptions with the backup. Changes that occur after taking the backup and before initializing the subscription(s) will be stored in the distribution database, which acts as a store-forward queue. Those changes will then be replicated to subscribers after they have been initialized from the backup. Note that by default the distribution cleanup jobs clears transactions older than 72 hours from the distribution database which is based on the publication retention period. You may want to temporarily disable this job before the backup and re-enable it after you initialize the subscriptions from the backup. Have a look at Initialize a Transactional Subscription Without a Snapshot and Initialize a Transactional Subscription from a Backup. 

Since your Subscriber is within two versions of your Publisher version, you are good here. However, this configuration is not supported if you are using Merge Replication since your Subscriber will be a later version than the Publisher: 

There is no hard limit but you will limited by hardware considering the agents will consume cpu, memory, and i/o. There is also a desktop heap issue that some have run into when running a large number of push subscriptions, ymmv. I personally have 1 topology I administer which currently has approximately 3,000 pull subscribers. 

Also, feel free to vent about your experiences with tasks like these. I'm sure I'm not the only one who has been handed down tasks like these. 

Create a new String table populated with a cleanly joined set of data from the original base tables Index the table. Create a replacement set of top-level views in the stack that include and columns for the and columns. Modify a handful of s that reference these views to avoid type conversions in some join predicates (our largest audit table is 500-2,000M rows and stores an in a column which is used to join against the column ().) Schemabind the views Add a few indexes to the views Rebuild the triggers on the views using set logic instead of cursors 

I have a table that stores version information in multi-dotted strings, ie '4.1.345.08', '5.0.1.100', etc. I am looking for the quickest way to find out which is larger. Is there a faster, or easier way on the eyes than what I have come up with below? One caveat is that I am 90% sure that we should ONLY find dotted numeric values here, but I can't guarantee that in the future. 

Say for example, if a defect is reported in this spaghetti mess, for example a column may not be accurate in certain specific circumstances, what would be the best practices to start troubleshooting? If debugging this was a zen art, how should I prepare my mind? :) Are there any preferred SQL Profilier filter settings that have been found useful in debugging this? Any good tactics, or is it even possible to set breakpoints in nested procs when using debug mode? Tips or suggestions on providing meaningful debug logging when dealing with nested string builders? 

This seems pretty rotten to me, but I only have a few years experience with TSQL. It gets better, too! It appears the developer who decided that this was a great idea, did all this so that the few hundred strings that are stored can have a translation based on a string returned from a that is schema-specific. Here's one of the views in the stack, but they are all equally bad: 

Somehow you are ending up with orphaned replication metadata after deleting the subscriptions and/or the publication. This can happen when using the GUI to drop subscriptions and publication. Instead, try using T-SQL replication stored procedures to delete the subscriptions and publication. How to: Delete a Push Subscription (Replication Transact-SQL Programming) How to: Delete a Pull Subscription (Replication Transact-SQL Programming) How to: Delete a Publication (Replication Transact-SQL Programming) After deleting the subscriptions and publication, verify the replication metadata was removed. If not, use sp_removedbreplication to remove all replication objects from the databases. 

You have probably read that you are able to deliver the initial snapshot via FTP. This is covered in Deliver a Snapshot through FTP. It is possible to replicate two SQL Servers across a VPN. This is covered in Publish Data over the Internet Using VPN. Once the VPN is setup, you should be able to connect and authenticate as though the servers are on a LAN or WAN. If you have anymore questions please let me know. I hope this helps. 

None of the above. If the 5 new subscriptions are initialized with a snapshot then by default all subscription database objects will be dropped and recreated from the snapshot bcp files. So the data from the week offline will appear to be deleted. 

There is no need to add an identity column for no reason. My guess is someone is issuing a SET IDENTITY_INSERT ON when there is no identity column in this table. You need to check the code to make sure it is correct. 

The biggest time saving option is to generate the snapshot and compress it with your favorite program, like 7-Zip, copy the compressed snapshot to the subscriber using file-copy or FTP, and then apply it locally using the -AltSnapshotFolder Merge Agent parameter. The existing Product database can be left in place while the snapshot is being copied over and client applications can operate normally. The way this works is open the SQL Agent job for the Subscriber you wish to deploy the snapshot to and get the Run Agent. command. It looks something like this: 

At the moment, replication is not supported on Linux. However, according to the release notes, replication support will be added in a future release. Release notes for SQL Server 2017 on Linux 

That seems like a complex plan for a view that has less than a thousand rows that may see a row or two change every few months. But it gets worse with the following other observances: 

While profiling a database I came across a view that is referencing some non-deterministic functions that get accessed 1000-2500 times per minute for each connection in this application's pool. A simple from the view yields the following execution plan: 

It sounds like you need to add another column in your data table to account for the quantity of ingredient. In the example you linked, it appears that the site calculates how many grams each ingredient adds to the total sum of the final product. But it will be up to you to decide how best to generate these. For example, if your site is all about home made juices, perhaps you can come up with a formula to determine how potent or how much flavor a specific ingredient adds to the final product. But I digress. If you have an additional column for grams you could take the weight of one ingredient, calculate the SUM(Weight) of all ingredients in your recipe and divide the two together, like in this Excel example. 

Is there a best practice method to handle localized strings via a view? Which alternatives exist for using a as a stub? (I can write a specific for each schema owner and hard-code the language instead of relying on a variety of stubs.) Can these views be simply made deterministic by fully qualifying the nested s and then schemabinding the view stacks? 

I recently inherited a MESS of a search. It's around 10,000 lines of code in the procs/functions alone. This search is aptly named the "Standard Search." It's a proc that calls about 6 other procs, which are entirely composed of string builders, in which each proc has between 109 and 130 parameters. These procs also call deeply nested functions which generate more strings to be assembled into a final query. Each proc can join up to 10 views, depending on the logged in user, which are abstracted from the table data by between 5 and 12 other views per primary view. So I am left with hundreds of views to comb through. On the plus side, it does have a parameter to PRINT out the final query, (unformatted of course!) It's a hot mess. The string builders also have no formatting, and they don't logically flow. Everything jumps around everywhere. Although I couid do an auto-format, to pretty print the code, that doesn't help with the formatting on the string builders, as that would involve refactoring the content of strings, which is a no-no.