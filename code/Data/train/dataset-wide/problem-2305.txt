So, even a few results with all sub-elements quickly make hundreds or thousands of rows, and if the parent table has tons of heavy NCLOB/BLOB/NVARCHAR(MAX) columns, there's a HUGE amount of data to be transmitted as duplicates. Some object-relational mappers work around this by SELECTing child and grandchild rows by their parents' IDs (the good ones loading them in groups/batches, not single SELECTs for each parent row!), but this is likely not transactional (subordinate/child rows may have been changed after parent rows were selected, not possible in a join - and of course you don't want to do this in "serializable" isolation level). A possible solution would be to transmit join results in a different format: instead of duplicating parent rows, they would transmit them only once, plus a reference between them and subordinate rows (likely the original PK/FK). In the most simple case, multiple result sets from all joined tables would be transmitted, with each row (or row projection) only once, to be easily transformed into objects by the client application: 

perhaps somebody can add the correct syntax how to remove shared cursors for a table or user this way, and tell if full admin rights are required for this. 

Multiple values in a column are against 1st normal form. It's also absolutely no speed gain, since the tables are to be linked in the database. You have to read and parse a string first, then select all categories for the "Deal". The correct implementation would be a junction table like "DealDealCategories", with DealId and DealCategoryId. Bad hierarchy implementation? Also, an FK in DealCategories to another DealCategory looks like a bad implementation of a hierarchy/tree of DealCategories. Working with trees through a Parent ID (so called adjacency list) relation is a pain! Check for Nested Sets (good to read, but hard to modify) and Closure Tables (best overall performance, but possibly high memory usage - probably not too much for your DealCategories) when implementing hierarchies! 

One remarkable thing I found out with a similar query on SQL Server 2008 R2: The query plan contained an index scan on a completely unrelated, nonclustered index, like one on the pr.RecommendedPrice column only. My idea is that the unrelated, nonclustered index contains references to the clustered index rows (pr.ID, pr.CategoryID), and it's cheaper to get these from a nonclustered index scan, rather than from the actual clustered index. Am I right in my assumptions? 

A minimum of automated data integrity, especially multi-object transactions, is still required. An in-memory relational database, or any focused on fast access without the need to access slow hard drives with every (write) operation, would help with speed, but still basically rely on a hard relational schema, which has yet been largely omitted and seems to be undesirable to the stakeholders. Can anybody with experience tell me if my assumptions are correct? 

Yes, they are compiled and stored on the database. In SQL Server, ad-hoc queries (without binding parameters) are compiled and stored, too, filling up the plan cache (except when ad-hoc optimization is enabled). But ad-hoc plans age out quickly and get removed, while frequently used, parameterized query plans will last longer in cache. However, I think prepared SQL is a bit dated and error prone. Both Oracle and SQL Server support parameterized queries for quite a while now, without the need for an extra roundtrip to prepare a statement. By using "sp_executesql" (MSSQL) or "execute immediate" (Oracle), and their support through database interfaces like ADO.NET, people can create parameterized queries identified by SQL string equality, without an extra database trip, and still use them when the original DB connection and command are long out of scope, even across applications - as long as the SQL string is equal (including capitalization, spaces etc.). This is also great for dynamic SQL built within stored procedures, where table or column names may vary. The scope of a prepared statement quickly gets lost in an application, requiring repeated prepare again and again. And I've seen faulty code calling "prepare" before every SQL query, forcing re-calculation or cursor creation every time, even when sending dozens of equivalent queries. There may be a minimum performance gain by sending only a handle to the database, instead of a full SQL string, but it seems no remarkable advantage, far outweighed by the extra trip and the scope/application limit of prepared statements. 

Do you want to perform the update once or on a regular basis? If the update on million rows is done once, then the best solution is to create a temp column "processed" on #testing2 table of bit (int,tinyint) which will serve as your null filter. The index on bit or int columns works a lot more optimal than on varchar. Also, having 2 type of values on index definition (0 for null 1 for not null) will be very fast. Keep your indexes and add the second index and you will have the following Plan if you change the filtering options in the query on "processed" column as it is marked in the picture below with the plan: 

In order to alter any column, first you have to drop the index or any other constraints that contain that column, and after create the index/constraint again. But this task must be done by a DBA because any drop on index or constraint will affect on one hand the data being queried - while the index is dropped, and on other hand put blockage on whole table for the time that the index is re-created. You need to make sure the users will be aware of this little or big (depends on the table size) maintenance. Good luck! However the Index create can be done with Online option which is less blocking. 

Considering the EventSubClass number you can find out what happened with the Query Plan and take specific measures. Additionally you can add other columns to Stored Procedures and TSQL Event Classes if you are interseted in HostName, WindowsUser or other info from Profiler trace. Also the trace can be stored in a SQL table making the analyse more easy and much more customizable. Here is a link describing more the Performance Statistics Event Class. 

If you still want to develop smth custom where you can log what info you want take these steps as start: Create tables with servers, databases linked to server id, backup info linked both to server id and db id. Create scripts that will be launched on destination servers using xp_cmdshell. The scripts will insert collected info to destination tempdb table and insert through linked server to your DBA server. This is very shortly. you can contact me on private if you need more details. I am using this method as it is the best way to track many stuff. 

first of all, the replication variables have effect on the slave only when activated on a replicated "slave" server. you need to understand that the filtering rules on master differ from the ones on slave. on master you can choose only to log a whole db or not. on slave you have more options. here is described $URL$ and here: $URL$ I think, you want to skip replicating a set of tables with a given pattern on slave. So, the variables must be configured on the slave. Change the configuration file on the salve and add the db_name instead of % for db part. 

Perhaps this problem has been solved meanwhile. Databases need dedicated knowledge and skills, any 10 year old kid can play around with SQL Server Management Studio, but creating and maintaining databases is not that easy. For SQL Server , make sure that Snapshot Isolation level is enabled. It's disabled by default, with the database in data locking mode, where long running queries block write acces, or even certain read accesses. I think Snapshot Isolation was introduced in SQL Server for analysis/OLAP. Also, the OLTP tables should have proper indices. There should be somebody in the company knowing at least the basics of all this. 

I have a query on Oracle 11g of the following type, which results in a series of highly inefficient full table scans: 

Tasks has indices on Id, ObjectId; ObjectAffectingTasks has index on both ObjectId and TaskId. All joined tables have proper indices, too. The ObjectAffectingTasks table contains task IDs affecting the object, but having another in the ObjectID, so all Tasks affecting object with ID 12345 shall be selected. When analyzing the query, it seemed to be the OR condition that spoiled the execution plan. Where clauses with only ObjectId or only the subquery were using all the indices. Another workaround was to create a Union subquery, which also used the indices: 

Imagine, you've tried to change this. In the beginning, you didn't know NHibernate and how it works, but then you came up with ideas how to adapt data access to it's real abilities, and avoid unneccessary database operations: map relations in NHibernate, keep sessions and transactions open for several object operations, do set/bulk operations, normalize the DB the way you've learned years ago, add foreign keys, views, maybe Materialized Views to it. But you keep being rejected, with arguments like: "nobody is going to pay for it", "the database can handle it, no matter how 'bad' the application is", and simply "it works". Disk space, memory, CPU power and network resources are cheap; refactoring data access would be much more expensive. Likely, standing by the code programmer's object oriented approach, rather than the DB programmer's set based approach, is preferred (including it's enforcement against the ORM implementation). What does it matter if the system could be 10 or even a 100 times faster, if it works sufficiently in the current way? Don't care about SELECT N+1 anyway, today's databases can handle it! That would only be gold plating! It might become different when databases grow into terabytes, but that's not for now. So, maybe, there is a solution in the "NoSQL" or "NewSQL" area. It might be possible to fetch objects from and store them in a database in a fast and efficient way. Even with many queries in a single object, rather than set approach, as long as it is a local DB without long distance latency. It looks like the current system uses the relational database just as an extended, persistent main memory, and all those "stone age relicts" of IT, like creating and maintaining tables and indices manually, or mapping objects to relational tables, just add a huge overhead. My idea is: A "NoSQL" document database is a good thing, because: 

Yes, you can manually script it. This is covered in the documentation here. In essence, you copy the archive log files to the correct filesystem location on the logical standby host, then register them using the command. 

Here's a list of supported platforms: $URL$ The MySQL yum repo is detailed here: $URL$ - That has binaries for MySQL 5.5, 5.6 and 5.7. 

So you don't need to specify the full path when you run : Set your variable to the full path of the relevant folder ( ?). Instructions for doing so are a google away. At the same time, make sure is set to and that your has added to it. The problem you're actually having is because of the parameters you're passing to sqlplus. You've been doing: 

I have a number of large tables, each with >300 columns. The application I am using creates "archives" of changed rows by making a copy of the current row in a secondary table. Consider a trivial example: 

... But in doing so, you're essentially admitting you're doing something wrong (plus it makes the name case-sensitive). Best practice would be to avoid case-sensitive object names, and avoid using reserved words. Documentation link here, with a list of reserved words, which backs up what I've stated: 

and can be the relevant columns cast to a (there are no or columns involved), as I do not need to do any post-processing of the values themselves. At the moment I can't think of a sane way of doing this for a large amount of columns, without resorting to generating the queries programmatically - I may have to do this. Open to lots of ideas, so I'll add a bounty to the question after 2 days. 

There aren't any special considerations that need to be taken into account when setting up a RAC->No RAC Primary->Standby configuration. In fact, there's a White Paper written by Oracle which explains the setup. The Oracle documentation is here. 

... make sure you substitute your listener name for above. You can also use ADR parameters instead - see the documentation. 

Compile it, and make sure it's placed in or (configuration can be changed to allow other directories) - compiler flags will differ for AIX too. 

It's utter madness and there's no justification for it. was created to represent the absence of a value & to use an actual value like -5000 is bonkers. Ordinarily I wouldn't write an answer this short, but the question deserves to be one of the most visible on dba.se & the more answers the better. 

.. to reload the kernel settings. That'll set the maximum shared memory segment size to 2Gb & should solve your issue. 

The character set of an Oracle database is set at the database level, not tablespace level. You have 3 options: 

Floating point numbers are not always stored as you would wish, due to the way CPUs deal with floating point numbers. If you're always storing numbers that have 2 decimal points, store it as an integer and add the decimal point in the presentation layer.