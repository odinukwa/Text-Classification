I had created table with engine BLACKHOLE basically the BLACKHOLE storage engine acts as a “black hole” that accepts data but throws it away and does not store it. Retrievals always return an empty result. I heard that we can retrieve the data by creating a new table same as the old table with storage engine as innodb or myisam. but i had tried that also but unable to get the result. Can any one pl help me on this issue to fix it. 

Try the above query in terminal or check the below link to repair table or databases via phpmyadmin $URL$ 

Check the insert query for teams field you have mentioned values as 1 and if you insert the same value for second record it will behave as below. 

Your BHCR is 98.92% its perfectly Good and Great. If the value of the Buffer Cache Hit Ratio (BCHR) counter is "high", then SQL Server is efficiently caching the data pages in memory, reads from disk are relatively low, and so there is no memory bottleneck. Conversely, if the BCHR value is "low", then this is a sure sign sign that SQL Server is under memory pressure and doing lots of physical disk reads to retrieve the required data. Prevailing wisdom suggests that "low" is less than 95% for OLTP systems, or less than 90% for OLAP or data warehouse systems. You can also test the true nature of the BCHR 

Error Cause: The control file change sequence number in the log file is greater than the number in the control file. This implies that the wrong control file is being used. Note that repeatedly causing this error can make it stop happening without correcting the real problem. Every attempt to open the database will advance the control file change sequence number until it is great enough. 

In create table you have set teams as primary key, and also you are aware that primary key does not allow duplicate values. 

I am getting the below error in mysql while trying to ping. i am using mysql version 5.5.37-0ubuntu0.12.04.1. when ever i use mysqladmin i am getting the below error. Please suggest how to clear this issue 

This error happens when Full back up is not restored before attempting to restore differential backup or full backup is restored with WITH RECOVERY option. Make sure database is not in operational conditional when differential backup is attempted to be restored. Please have a look on the below blog. $URL$ 

You can create a username and password also provide grant access to them. so that all clients can access the mysql from their local by using the username and password provided by you. 

I had some changes in that table and it got executed. mainly the primary key. pl go through the same, 

It will ask for password don't enter anything first time because it will use blank, n just press enter you are done. N later you can set password too...:) 

Full Backup started on 4/21 12 AM. While backing up one of the database, it is stuck. sp_WhoIsActive shows following information 

And I verified by only setting up 823 and ignoring 824 and 825, still the sp_Blitz does not report for other 2 missing (824 and 825) !! 

It turns out that the person migrated the jobs from old server to new server edited the jobs manually. One of the edit was to this was set to default log location on old server. On the migrated server it was edited to where the directory did not exist. (that person missed deleting the text in the path) On the other jobs (FULL and DIFF) the text had been removed so it was set like this and these both were working fine! 

I have setup a SQL Agent job to delete files older than 7 days. The script does it job when run through windows powershell window. However the same script does not work from SQL Agent job 

I just by chance added alert for Error 825 and ran sp_Blitz again. And the message for Finding "No Alert for Corruption" was not displayed for Error 823 and 824 !! After Alert setup: 

Server Config: RAM: 24GB - 21.5GB for SQL Server Processors: 8 Not much activity in this server - hardly 50 people access this SQL Server via Sharepoint. 

Since the command running indexoptimize was a deadlock victim, I am wondering the job succeeded instead of an error!! 

There is AD group on created on . The domains each other (per IT team). Now, we have a which is also trusted for both Domain1 and 2. We added AD group to . When either of Domain1\User1 or Domain2\User1 try to Login to SQL Server on Domain3 - we get Login Failed message. 

Also, I see another backup on the same database started 30mins later (Backup to VirtualDevice - I know this is AppAssure backup tool). From SQLSkills preemptive_os_waitforsingleobject, I see And this is blocked for 2 days - I don't think one of the session will terminate itself until I kill the other. However, LOG backups were happening without any issues. Only FULL backup and DIFF backup for that particular database was blocked. I killed the AppAssure session and all the long queue went away from AppAssure. I killed the DIFF backup as well. Now the only process left is this FULL backup with 100% complete with same and not willing to complete!! I had no other options but to kill the FULL db backup and restart backup jobs. And, the USER_DATABASES full refuses to start saying but showed nothing. Also, there was nothing in state. CurrentJobActivity in msdb showed that the job was active. Had to right-click-stop under jobs and then start the job again!! Any idea on how do we avoid this situation? (other than telling the IT-team to stop AppAssure?) [EDIT]: Adding the version 

WITH (NOLOCK) is the equivalent of using READ UNCOMMITTED as a transaction isolation level. So, you stand the risk of reading an uncommitted row that is subsequently rolled back, i.e. data that never made it into the database. So, while it can prevent reads being deadlocked by other operations, it comes with a risk. In any application with high transaction rates, it's probably not going to be the right solution to whatever problem you're trying to solve with it. SQL Server 2005,2008 and 2008 R2 will support Nolock. Pl look on the below link $URL$ 

If the tables were dropped, ApexSQL Recover can recover them even from databases in the simple recovery model. ApexSQL Recover can recover both table structure and table records On the other hand, ApexSQL Log cannot recover records lost when a table was dropped, it can only recover the table structure In case the records that were lost using DELETE (not DROP TABLE), both ApexSQL Log and ApexSQL Recover can help The advantages of ApexSQL tools over recovery to a point in time is that ApexSQL will recover just the tables you specify (creates CREATE TABLE and INSERT INTO scripts) , while a point-in-time recovery will roll back all the transactions that happened in the meantime A DROP TABLE statement marks the MDF file pages used by the dropped table for deallocation. These pages are actually still in the MDF file until overwritten by new operations. To prevent new operations overwrite the data necessary for successful recovery, we recommend creating a copy of the original MDF and LDF files immediately 

Hi I need to set auto increment for a table as S1. I tried by altering the table and also by creating by new table I am aware how to set auto_increment but when i try to set auto increment as S1 i am unable to do. since i need the primary key values should be S1,S2,S3 as following records. can any one please suggest the data type for that, 

The error (2003) Can't connect to MySQL server on 'server' (10061) indicates that the network connection has been refused. You should check that there is a MySQL server running, that it has network connections enabled, and that the network port you specified is the one configured on the server. Start by checking whether there is a process named mysqld running on your server host. (Use ps xa | grep mysqld on Unix or the Task Manager on Windows.) If there is no such process, you should start the server. If a mysqld process is running, you can check it by trying the following commands. The port number or Unix socket file name might be different in your setup. host_ip represents the IP address of the machine where the server is running. 

If you anticipate that your database file(s) will exceed more than 1TB in size you should configure the server to put the data files across multiple VHDs. How can spread my databases across multiple VHDs? Create one or more dynamic volumes consisting of multiple VHDs at the operating system level and put your database files on these volumes. Split your tables and indexes into separate database filegroups and then place those filegroups in different files (MDF + NDF) which are spread across separately attached VHDs. Note that this will not offer any benefit for transaction logs because SQL Server does not stripe across transaction log files but uses them sequentially. If you are migrated to Azure and want to compare the performance of your databases before and after the migration then take a SQL Server baseline(enter link description here) 

P.S. The Service Account was granted rights and the old files are being cleaned up. Version Info: SQL Server 2012 SP3 CU8 / Windows Server 2012 

However, if the Domain1/User1 and Domain2\User1 are added as individual accounts then we could Login without issues. 

I gave up resolving this issue for a while now. Could someone please assist me on this? I need to create this Linked server!! What am I doing wrong all these days? 

I am working with some old jobs and I found this code snippet. I am just wondering why would someone dump transaction log files from databases in to one file ? 

There is a SQL job scheduled with number of steps. Few steps are throwing Query timeout expired error but still the job step is being reported as Succeeded. I confirm that the steps are set to "Quit the job reporting failure" on failure. What could be the cause for the step to succeed? 

DatabaseBackup - USER_DATABASES - LOG: This job fails saying "Executed as user: Domain\XXXX-SVC. Unable to open Step output file. The step failed." The error is only with LOG backup job. The other DatabaseBackup jobs (FULL, DIFF) works just fine with same SVC account. So the service account have appropriate permissions. The Output File(Job Step properties-->Advanced) is F:\SQLAgentLog\ which is same for all jobs. Only problem is with LOG backup job. Has anyone else experienced this and is there any solution? Current environment: SQL Server: 2012 SP3 CU8 OS: Windows Server 2012 Note: This was working all good on a Windows Server 2008!! 

We recently migrated to a new SQL Server and the SQL Services are running under a Service Account. I did observe that the Service Account do not have rights on G drive and the old files are not being cleaned up. Question: Why doesn't the procedure when it cannot delete a file OR simply move on instead of waiting for around 9.5 minutes? Also, doesn't the job error if it cannot delete the old backups? We will not know until we receive alert !! Here is the log details. 

Is there a way to resolve this to make work on Domain3 SQL Server? Also, creating a group and adding the individual userID's from other domains worked too. We don't want a new group just for this. 

Here is the scenario which I am working with IT team to get it right with no luck. Following are the users who work on a SQL Server. 

I had a situation where the Native Backups were being made on a Server. I happened to see in that there was a third party backup tool () that was also taking VSS (kind-of) . At some interval, the AppAssure (backup being made to ) was doing a and at some other interval it was doing a breaking the log chain. Is there any way() to know when a backup log chain is broken? Here is a screenshot of the situation from February.