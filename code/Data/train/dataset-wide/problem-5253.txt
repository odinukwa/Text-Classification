Ok - let's say I tell you a number between 1 and 100 inclusively, which is random - how could you prove it? Of course, if I say 1, 100, 99, 42 or 13, most people will claim it wasn't random, because their first thoughts where about the same numbers, but if these numbers where excluded, then it wouldn't be a fair draw from 100 numbers, but from 95. So from a single number, you can't distinguish a biased number from a random number. I can tell you a number, and you can't prove it isn't random, but you can't prove it is as well. So I could tell you my technique, how to generate the number, and convince you. If most people could learn the technique, and you coulnd't tell before, what number the people are producing, and more: if the repeated usage of the technique would lead to an uniformly distribution over the numbers from 1 to 100, we would call this a 'Yes, we can'. Ok. So I take a sheet of paper, write the numbers 1 to 100 on small pieces of it, put them in a bag, and pull a number - mission accomplished! :) 

¹) very often, it isn't an equal distribution, but a normal distribution, when most cases are grouped around an average. Think: body size of persons of same age and culture: Most are around 1.8m, fewer 1.7m or 1.9m, far fewer 1.6 or 2.m. But rarely two childs from the same parents are of exactly the same length. If you enter a casino, you will hardly find anybody who can predict the next card from the deck, or the next roulette number. Some might believe they know a way to predict it, but they can't. Some people here might think, that if they knew enough parameters, they could predict the next roulette number, but think of it! The microscopic shape of the billiard ball will change with the temperature, and you can't shield a Casino from the outside world, to prevent any influence. The air is moved by the breathing of the visitors, and you can't calculate when whom of them will breath - it's not a problem which will be solved in 100 years with more computer power. It is systematic: The finer conditions of roulette are uncontrolled. So the outcome is random. My first impulse was, to write "true random", but as serious adults, we always tell the truth, of course, so if we say random, we mean random. You might construct a roulette canon in a laboratory, which generates predictable results, but you will not get a license for that. That much is predictable too. :) Now to the second, related questions. Some people believe that only equal distributions, like from throwing a dice, are random. But that's not the case. Consider a bowl with two black and one white ball, and you pull blindly from the bowl and put it back each time. You will pull a black ball with 2/3 chance and a white one with 1/3 chance. It is not predictable which color you will get, but the probability is not equal - it is 2:1. But knowing that, you could transform it into an equal distribution: If you pull a white, it is white, but if you pull a black, you have to pull a second black one. Generated by a PRNG: 

In mathematics, it seems that when we try to find relations about objects we are forced to set a unique object as a basis for the construction of each other object. For example: take one rectangle and define It's area as b•h, now if you take a line through Its center, it will allow you to cut the rectangle in two having its area divided in two pieces (b•h/2). Now if you rotate the line in a way that it passes through two vertices of the rectangle, then you have a triangle and it has the same area: (b•h/2). Now for the area of the circle, we need to make a comparison between what we made and we can do that by inscribing a regular polygon and taking the limit of its sides to infinity, that is: it's measure will depend on the sum of a infinity of triangles. Even in integral calculus, the area under the curves is defined as the sum of an infinity of rectangles, consequently: An infinity of triangles. And hence, the triangle seems to be the most basic object found when building these relations. Does this effect have a name? Has someone written about it? If yes, who? 

I've been thinking a bit about the core argument of social constructionism. Whenever I see the argument being employed, it seems that it means that if something is a social construction, then this thing is not too legitimate. This seems to agree a bit with Hacking here. I've noticed that I could take the argument itself and write: "The idea of social construction is a social construction. Then the idea of social construction is not too legitimate." At least for me, this seems to be a logical problem: An instance of the liar's paradox, with the argument eating itself. Is it actually a problem or the argument is built in an ideological framework unbeknownst to me in which this problem could be eliminated? I know that there are logics that deal with self-reference. But I don't know if the person who created the concept worked with these logics and conscientiously created the concept using it or if he/she didn't see that one could apply the argument to itself. 

I'm reading David Papineau's Philosophical Devices, and there's a section on numbers and set theory. But there's not deeper hint on why it's important to philosophy. I guess that in mathematics we can think of formulas for building the elements of some set, such as the set of even numbers, the set of odd numbers, the set of all the prime numbers (which I guess that until this date, have no formula) so perhaps it's important to make formulas for some objects in philosophy? I guess I can see the importance of the knowledge of the axiom of comprehension (and the Russell set), which states that for any condition C, there exists a set A such that (for any x)(x is an element of A iff x satisfies C). It is there to remind us that such axiom can't hold without some exceptions. Although I fail to see where one would create an object (in philosophy) and then notice a similarity of this object and Russell's set. Note: If you know better tags, please edit. I've tried to use the tag set theory, but It does not exist. 

Here we see a 5:6 relation, which is of course no proof, but the proof is more a case for Mathematics.SE or Statistics.SE. It's only to show how it works. Now: If you know the bias of any random distribution, you can transform it into an equal distribution, which means, that an equal distribution isn't such a magical thing. You could for example measure red and non-red cars which appear on the other side. If you counted a lot of them, and know the distribution, you can correct the unequalness by calculation. People with a deterministic viewpoint might argue, that, given enough data and big machines, and knowing all natural laws in greatest detail would make it possible, to calculate every minor event in the far future, but we know that there are far too many atoms in the universe, where we don't know where they are, and what their movement is. By all practible means it is impossible to predict the future, but that's what random is about: Can we predict it or not. It's not about "Could it be predicted, if ...". So the statement, that our future isn't predictable doesn't need a proof - it is a fact. A deterministic viewpoint is, what would have needed a proof, but from quantum mechanics we know, that there is uncertainty on the subatomic level (Heisenberg). 

Is it? Ok - we then need to see how to sharpen the rules, I guess? No tools allowed, like paper and bag, RNG and dices? Ok - I pull a lot of hairs out of my head - much more than 100, let's say 500 to 1000 hairs, and then I count them, and take the modulo. :) 

Is it really? Well, of course the technique doesn't scale. It can't be repeated very often; not often enough to check, whether an equal distribution is reached. And you could tell, that the rules prohibit any material as tool. Ok. Then I'm nearly to the end of my wisdom. For a random result, I need some unpredictable input, and of course, my brain isn't a good source for such an input. I could name a measurement, which would work for a small amount of small random numbers (1-10), and it works similar to the hair example: I think about a song I know, and the first song which comes to my mind, I count the characters. I can't predict from the first song - "I am the walrus" how many characters it has. The number is pretty big, compared to the number range (1-10), and I take the modulo, and have a random result. So this is a proof of concept, and the next time, I would, of course, have to choose a different song, and so the number of experiments is limited to the number of songs or poems I know, and it is a time consuming procedure. If the rules say 'name a number spontaneusly - in 2 seconds', I'm pretty sure nobody can produce repeatedly random numbers. But given enough time, you can. 

When I was a kid, I remind that I had a strange type of game - I was mostly concerned of annoying other persons. Whenever they assumed something, I asked why, and then when they aswered, I asked why again, and then I kept asking why until the person was annoyed. There was a point where the person could not provide an answer to the why's, I always wondered if our knowledge behaves this way: This way of having an infinite series of why's and at some point, a nullity of answers. So, is this infinity of questions a problem somehow? Have someone thought about it? 

I don't understand how the fail to settle all disputes implies that one couldn't settle many of them. The only possibility I see is using the meaning of "all" as "all disputes of a certain class" as such that this class does not contain all possible disputes. 

I'm not really specialized in the history of science. But it seems for me that as the time passed, the exact sciences tried to do that. For example: The second is measured in relation to the spinning of the electron along the nucleus of the atom and they tried to obtain mathematical certainty with methods that could be computed outside of mathematicians (this was part of the Hilbert's program). I believe that this happened because of the belief that we are bad observers and that there are some environments which are really not observable, for example: The set of all natural numbers. So, does this trend have a name? I'd like to read more about it - supposing it is actually something that happened and not just something I made up. 

I guess everyone knows Plato's allegory of the cave. He assumes people are in caves and then, he suggests that there is a possibility of "going out of the caves", gaining several nice properties with this. Do we have evidence that he noticed that we could "get out of a cave" and still be inside another "cave"? And that there could be an arbitrary number of nested caves? 

In the near past, I've read about the work of Tversky and Kahneman, the text said that they presented their work to a famous american philosopher (without mentioning names) and he said that he wasn't interested in a philosophy of stupidness. From what I know, Kahneman and Tversky wrote about biases, not specifically stupidness. So, is there someone else who wrote some philosophy of stupidness? 

Excuse me for such nonsensical question, but for some time, the idea of things that could not be discovered bothers me. I guess that this is an interesting object of investigation, and I also guess that someone already thought about it or wrote one thing or two. I know almost nothing about philosophy, but I guess that the hypothesis of undiscoverable things, could probalby pose what we know as something with less legitimacy. But again, this might be a meaningless sentence due to my lack of understanding. I'd be glad to be pointed to works that speak about this.