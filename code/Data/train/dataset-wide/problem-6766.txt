Diglossia typically refers to the situation you've described when the two forms are named, 'recognized' varieties. The distinction is a culturally reified one, while changing register is not usually considered to be a clear shift from e.g., 'Work English' to 'Street English'. 

There are three parts to this answer: why slice, how to slice, and ‘what does it mean for time to seem continuous’. You can also review first (wikipedia) if it’s easier. First, we may want to slice because a spectrogram (e.g., in Praat) is a 3D figure on a 2D display. Humans aren’t great at reading 3D figures. If you slice that 3D figure (intensity by frequency by time), you get a 2D figure. For a spectral slice, that’s (intensity by frequency); slicing lets us set aside time for the moment, and focus on the intensity-frequency distribution. Second, this kind of slice is made by ‘cutting’ the spectrogram at a particular point in time (i.e., on the x-axis). The metaphor may be clearer if you think of the spectrogram as a real 3D object, like little plastic mountain range. If you use a knife to cut a perfectly vertical slice of that mountain range, you can see the silhouette by looking at the end. That outline is the spectral slice: a simple plot of intensity (y-axis) vs. frequency (x-axis). Third, why even talk about time being continuous or not? The answer is that a spectrogram really is a big stack of slices put together. Mathematically, that’s where you get it (Fourier transform). In order to convert a single complex sound into a spectral representation, you’re forced to cut time into discontinuous pieces and analyze each little slice. The length of this slice in time is the window length (Praat manual). 

This pattern does show up a lot, and there seem to be multiple factors driving it. Rural areas (though not only such areas) tend contain small, immobile, tight-knit populations, in which local identity is socially important and outside communication is (historically) limited. On one level, simple isolation means there's less exposure to the mainstream forms, and to the 'leveling' effect of contact between speech communities. Such conditions instead seem to favor the maintenance of non-standard language features, and resistance to incoming changes from the mainstream; in the face of incoming changes, such conditions may even promote a reaction of 'exaggeration' of non-standard, local-identity features. Socioeconomic class may often play a role, and rural areas tend to be poorer. Note, though, that the conditions described have often applied equally well to dense, urban enclaves. In addition, as a general rule, isolation of regions has trended down over time (due to growth of transportation and communications technology). This breakdown of isolation seems to be the major factor in the loss of the distinctive local variety of e.g. Martha's Vineyard, or North Carolina's Outer Banks communities. 

Other issues: CMUDict has a funny doublespace-delimited format, so be aware when you're working with it in Python, R, etc. The Python code provided (and similar approaches) will get confused if COCA has a word that CMUDict doesn't; the join approach won't break, but obviously will not find a phonetic match for those items. 

You could simply divide by the size, so you’re comparing relative frequency. You could also express the log relative frequency, given how small the resulting decimal number is. It’s common to express relative corpus frequencies using a standard denominator (per million, for example), as suggested here. CELEX, for another example, provides both the per-million and the log frequency. 

I assume this is Optimality Theory, so that mark indicates when a constraint violation is the deciding 'fatal' violation for one of the possible forms. Specifically, asterisk (*) marks a violation of a constraint, and the addition of the exclamation mark (!) indicates that this violation was the deciding one. (See $URL$ 

The question is actually more nuanced, because phonotactic constraints are gradient and probabilistic; there is the question of ‘more likely’ and ‘less likely’ beneath the binary ‘legal/illegal’ concept. Looking at this level, the real words tend to be much more ‘likely’ than sequences sampled at random from the phonotactic space, which you’d expect if real people prefer likelier phonotactics. 

In the linked entry on ob-, all of the senses (including 'against') are spatial, not logical. Combining the senses listed, it seems that this is: 

It’s not practical or reasonable to scale sound intensity linearly. It’s impractical because the resulting visuals would be unreadable, and it’s unreasonable because (simplifying a little) humans perceive loudness on a logarithmic scale. To crib a common example, imagine you’re listening to a single violin. To double that loudness, you’d need 10 violins, not 2. 

I'm not sure your characterization of adult learning is accurate in general, but your basic intuition is right. It is not that case that the most efficient course for adults is infant-like, however; adults have entirely different mental resources, and "listen, listen, listen, and only later try to speak it" is probably too much passive exposure. Instead, adults benefit from communicative interaction in the language (however clumsy at first). There are SLA curricula that already focus on this pattern of 'listen/speak and then read/write'. The truly old-fashioned approaches did indeed reverse this, focusing on written translation as the goal. There is a broad literature addressing this question, and Wikipedia's entry for SLA has a number of references. 

The fundamental forms the basis for the harmonic series: if F0 is 100 Hz, then harmonics are present at 200, 300, and so on. Humans can 'recover' (perhaps 'hallucinate') the original fundamental from the presence of these higher-frequency harmonics. This is also true for non-voice sounds. Here is a related Wikipedia reference. 

Because of the simplifying constraints you describe, your problem is manageable. One approach is to use a forced aligner to automatically locate the segments in a given word; the output would be a TextGrid tier with intervals marked for each segment. I'm not personally experienced with this, but I've seen it done and I know it's a common practice. Maybe start with ($URL$ or ($URL$ for alignment. This works well with single-word, pre-scripted, lab-recorded input. Then, your system would select the appropriate intervals (e.g., 'segment labeled /i/', but also 'all intervals labeled with any of /aieou/', etc.), and extract formant values at some number of subintervals (this is @musicallinguist's question: 'how many subintervals?'). A common starting point is about 3 (25%, 50%, and 75% into the vowel), but you can always get more or fewer. This can all be done in Praat (scripted). 

I couldn’t find a cite quickly, but the phonotactic space in English is very sparsely filled. If you use a length interval of e.g. 3 to 6 phones, there are (many) millions of legal sequences, but only (as you say), some hundreds of thousands of used English words (possible 1 million, by some estimates). The ‘occupancy’ is greatest at the lowest end of the length scale, and decreases very quickly as length increases. I’m not prepared to say that the decrease is ‘exponential’, but the space of possible words gets huge as more combinations are possible. There are a couple ways to approach this problem empirically, in the absence of citations. One, you could use/make a phonotactic word generator to enumerate the phonotactic space (then use a dictionary search to subtract the real words). Two, you could use a measure like ‘lexical neighborhood density’ as a proxy for phonotactic occupancy. There exist dictionaries with neighborhood density measures calculated (or, you could calculate them for a new list, using e.g. CLEARPOND). The average neighborhood density across lengths might give a rough idea of the occupancy. This second option is a problem because average neighborhood density (as commonly implemented, with a phoneme edit distance of 1) drops to 0 very quickly as length increases. This happens precisely because the occupancy drops so fast, but it’s also annoying in terms of precision. 

From Praat Manual: "formats a number as a string with precision digits after the decimal point. Thus, fixed$ (72.65687, 3) becomes the string 72.657, and fixed$ (72.65001, 3) becomes the string 72.650. In these examples, we see that the result can be rounded up and that trailing zeroes are kept. At least one digit of precision is always given, e.g. fixed$ (0.0000157, 3) becomes the string 0.00002. The number 0 always becomes the string 0." 

The substantial discussion on ELU seems to address this: ‘continual’ came first ("without cessation"), then ‘continuous’ arose (“without gaps”), then the latter took over the meaning of the former (presumably, by spatial analogy), and as a result, ‘continual’ got a distinct sense. If this trajectory is correct, how could the original Latin sources of the suffixes be the cause? Glancing quickly at CELEX, it looks like there are very few similar pairs in current English, and ‘-al’ is (almost?) never otherwise used in a context where ‘possibility of interruption’ could make any sense. Without other evidence, I'd conclude that this distinction (continuous/al) is an isolated, arbitrary event, and not an example of a general sense of -al vs. -ous. 

Preface: I don't know of an online tool for this, and I agree that the real solution is to practice IPA. That said, there are a number of alternative phonetic alphabets (as James Grossman mentioned, though SAMPA is probably worse than IPA). Some of them might be easier to read: ARPAbet ($URL$ is relatively approachable, for example. However, it still requires some learning to be able to read quickly. You'd also need either a dictionary containing both transcriptions to search for words in (extant words only), or an IPA-to-ARPAbet converter; the latter is actually nontrivial because the phones in an IPA string aren't necessarily delimited, there's not an exact correspondence between most phonetic alphabets, and IPA may be used for either phonemic or phonetic transcriptions. Googling, I found this Praat script which appears to contain an IPA-to-ARPAbet conversion function ($URL$ and this Haskell file which seems to contain ARPAbet-to-IPA ($URL$ 

I've heard this referred to as 'linguistic insecurity'. This is neither '3-4 words', nor specifically incorporates the 'move to another region' aspect you mention, but people often talk about 'linguistic insecurity and migration'. 

There are some issues first, and then I’ll hazard an answer.I think you need definitions for what ‘how similar’ and ‘how close’ mean. For /p/ and /b/, if you’re going by the kinds of phonemic features that IPA broadly uses (plosive, bilabial), then they’re the same. Phonetically, there are major differences (e.g., voice onset time). Analogous issues apply for ‘close’ with non-shared phonemes: is a (voiceless) dental fricative similar to a voiced alveolar fricative? Answering these questions is probably best tackled through experimental research (e.g., confusability) with real French and English speakers.Given those concerns, ‘matching’ phonemes between these two languages generally are ‘similar’, though often not ‘the same’. Lay persons can recognize that some of these phonemes are equivalent; for example, an English speaker is likely to say that ‘baguette’ contains phonemes they recognize from English: /b/, /g/, etc. This is kind of cheating in answer to your question, though: the reason ‘shared’ phonemes are similar is because we’ve defined them as shared... because of their similarity. In other language matchups, speakers might have very different ideas about which sounds are similar to each other (i.e., different schema for featural closeness, etc.). 

You just need the correct Praat command: "Paint visible spectrogram...". Here is the relevant manual page: $URL$ 

In English, one counterexample is the very common '-ed’ (often /d/) ending: ‘filled’ is 1 syllable, and the morphemes are ‘fill’ + ‘-ed’ (/d/). 

Use the 'Pitch help' button in the object window when the Pitch object is selected. I'm going to simplify this answer a bit: Pitch estimation is inexact, so those numbers are basically the ranked candidates for pitch (y-axis, marked in Hz at intervals of 100 Hz) at that point in time (x-axis, marked in seconds). If you zoom in a little, you'll see they're stacked vertically, with the best candidate (highest integer) highlighted in pink. The 'pink lines' are actually patterns of the best candidates. Pitch cannot be estimated for unvoiced portions, so you'll see those intervals marked in blue across the bottom ('Unv'). 

A complex one like "contains a word-medial token of /b/ that is preceded by /s/, part of the onset of a stressed syllable, and followed by /a/" may be difficult to do in one step (though perhaps easier to do in several steps). There are many good sites for help with regex and testing ($URL$ Combining the information of COCA and CMUDict is fairly simple if you're comfortable with scripting/programming. This operation is called a 'join' or 'merge' for tabular data. In Python, for example, you can do this manually by reading in the CMUDict (as a dict) and your COCA-based orthographic word list (as a list). In a loop, you look up the CMU match for each word. Your output will be a new dict (I'll call it 'outputDict'). 

You should be able to do the phone environment searches you describe on the CMUdict. Instead of using a web browser, you need something with regex support (a good text editor). I would recommend something like Sublime Text as your text editor. Searches for word-medial vs. word-initial are simple for a regex beginner: 

While there are some context models that are unordered (Latent Semantic Analysis, some semantic word-vector approaches), that's not what you seem to be talking about. Instead, I think the critical point is that you're describing the n-grams incorrectly. An n-gram model is usually ordered, so that the whole sequence "three blind mice" is a trigram, and "blind three mice" is a totally different trigram. Your final formula seems to be trying to do both at once, which I'm not sure I've seen before. If you're really trying to answer the question 'What is P("mice") given that I know the last two words were "three" and "blind", but I don't know in what order?', then I think it would just be P("mice"|"three blind") + P("mice"|"blind three"). Obviously, this would get a lot more verbose at higher n-gram orders, but at that point I think you are indeed doing something closer to vector semantics. You might review Speech and Language Processing by Daniel Jurafsky & James H. Martin, Chapter 4 'N-grams' ($URL$ specifically Eq. 4.2; it's true that the focus on bigrams makes it slightly ambiguous that larger n-gram models are not ignoring word order. 

Broadly, you're describing the entire (not-fully-solved) problem of automatic speech recognition/automatic transcription. However, if you have the text of the sentences (e.g., if the recordings are scripted, or if you've manually transcribed their speech), then the problem is more tractable: you want 'forced alignment'. A popular software option for that is the Penn Phonetics Lab Forced Aligner (available at $URL$ There is documentation, but you might also do a web search for tutorials and guides. 

I don't think you've given enough detail about what you want to do with 'some statistical analysis', but I'll take a stab. I am not a stats expert, so these rough ideas will need to be checked by someone who is; I present them for you to know what to ask about. If you want to be able to say 'it's 85% likely that this person is from the longer VOT group', that sounds like a mixture model approach (there are many mixture model packages in your stats environment of choice). But you need enough data to figure out what the groups are, and you're collapsing what you described as a continuous variable into a 2-categorical variable. Maybe a better approach would be a mixed-effects linear model (using e.g., LME4). With VOT as your output variable, this approach would allow you to consider the relative contributions of various factors: speaker_age, but also gender, language background/dialect, control factors like vowel/phonetic context/word_frequency, etc., and (crucially) estimate the interaction of these features. Using the resulting model, you could make predictions and construct confidence intervals for those predictions. 

In R, you could simply read in both files as dataframes and do the appropriate join/merge operation. Using the plyr package, it looks like this (assuming wordlist has a column called 'orthographic': 

CELEX certainly has this information (at shallow and deep levels of morphological analysis). It is not free. I wasn't able to find a free alternative in a quick search. There are also morphological analysis tools for building this database yourself, though it's an area of active research. You might start by trying the resources here ($URL$ NLTK does not appear to do morph analysis out of the box, but further exploration there may also be useful. Incidentally, pro+tect+ed is unlikely to be the right parse, because 'tect' is not an English morph (though it is in the Latin source). 

I don't recognize the article, but you're describing generative orthotactics (possibly as implemented in a Finite State Machine). (See also 'phonotactics', which does the same thing on sequences of phonemes/phones.) You may also have noted various 'Markov' Twitter accounts, which perform this generation using sequences of words, again instead of letters. The concepts of this approach are still used for different applications in fields like automatic speech recognition, phoneme-grapheme conversion, psycholinguistics (see 'wordlikeness'), morphology/language innovation, etc.