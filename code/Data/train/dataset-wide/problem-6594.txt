Who introduced the notation e̯ a̯ o̯ (vowels with inverted breve below) for Proto-Indo-European laryngeals and when? Proto-Indo-European has been reconstructed with so-called "laryngeal" consonants, spelled *h1, *h2, and *h3. These were lost in branches other than Hittite but left traces on adjacent short *e as well as compensatory lengthening in descendant languages. Based on how they color *e, it's likely that these sounds were fricatives [x], [χ], and [xʷ], corresponding to the velar, uvular (formerly "plain"), and labiovelar plosives. Occasionally when these are syllabic, I've seen them spelled with the schwa indogermanicum: *ə1, *ə2, and *ə3. But lately I've been seeing reconstructed PIE with *e, *a, and *o with half-circles under them. I'm guessing that these correspond to syllabic allophones of laryngeals *ə1, *ə2, and *ə3. Am I right? What source introduced this orthography and why? Is it aesthetic, to make PIE look more like "words" and less like mathematical formulae, for the same reason that Pinyin uses accent marks instead of tone numbers? Or is there some new insight into the reconstruction of how the laryngeals were pronounced? And how do you type these, other than copying and pasting from elsewhere? 

Ditransitives of the English type are very rare. Most languages use either case marking on nouns to signify grammatical relations or polypersonal head-marking. By "English type" I mean that both objects can be passivized. Your example can be paraphrased as "A book was given to Mary by John" or "Mary was given a book by John". 

The term "agglutinative" refers to languages in which BOTH derivation and inflection are expressed by chained suffixes. The confusion might originate from the fact that both properties positively correlate in natural languages. Esperanto is an artificial language, and therefore atypical in this respect. 

One must be careful in making generalising statements, for example, languages without (morphological) case needn't be SVO or OVS, just look at Abkhaz. However there are typically some tendencies or predominant patterns. In the case of SOV, the languages tend to be agglutinative (which mostly implies rich morphology and therefore pro-drop) and non-configurational whereby SOV only applies to non-emotive (information-structurally unmarked) utterances. They also often exhibit head-marking predicates. If a SVO language has adpositions, they tend to be postpositions. BTW German is neither SOV not SVO but V2 in the standard typology. 

In the second example, I used a proper name to emphasize the individuality of the instance, which is multiplied. In the former case, if dog is understood as a class, then dogs could refer to multiple instances of that class as well as to multiple sub-classes of that same class. By the same logic, dog (singular) could mean a class or an instance of that class. 

So what rules adjective order in French? The question hasn't really been answered yet, and I don't see a single rule that can explain all possibilities. I don't claim to hold the entire solution to this problem, but in an effort to explain this feature to students of French languages, I have found several factors that can explain adjectival position: 

If more can be added to complement or challenge this explanation, I'd be glad to read it, as I am not fully satisfied with it myself. Meanwhile I hope it helps. 

There are no fundamental properties. Some/most (?) natural languages are mildly context-sensitive to allow for features such as cross-serial dependencies. Pure context-free grammars are too cumbersome to be used in linguistics, one needs to add a constraint system (in the form of a formal logic, typical an equational logic) which makes the whole system Turing-complete even if the backbone is a context-free grammar. 

It's valid insofar as linguistics studies both. The two issues are closely related. To express "I have seen him" one can use four words (as in English) or one word. Morphology, syntax, and lexical relations vary greatly across languages. One can study one language (synchronically or diachronically) or one can learn 10 or 20 languages and focus on comparative linguistics, typology, or formal grammar theories. 

Your intuition is correct, the parser is wrong in this case. Note that the argument structure for "lassen" taking an open complement is lassen⟨_,_,P⟨...⟩⟩ where P is the secondary predicator. This means that in a sentence like "er ließ sie ermorden" the causee depends on the finite verb. Reflexivisation unifies the subject with the argument in object position which in turn plays a role in the open complement's argument structure (this is referred to as argument fusion or coreference depending on the theory of dependency grammar used). For more on how argument structures interact with dependencies I'd recommend the PhD theses of Alsina and Manning. 

(Poor Lampwick.) This can be transformed using the definition of implication (if A then B means B or not A): 

The Japanese letter ん represents a moraic /n̩/ sound, which always follows a vowel. Finally, it sounds like [ɴ], the uvular nasal, but elsewhere, its pronunciation assimilates to the following consonant's place of articulation. Wikipedia's article about the letter states that it "is followed by an apostrophe in some systems of transliteration whenever it precedes a vowel", in which case it is realized usually like [ũ͍] or sometimes [ĩ]. That article gives no examples of words containing n'. But off the top of my head, I remembered the name of the failed Shin'en space probe, which is four morae /si-n̩-e-n̩/. A bit more digging turned up the boy's name けんいち (Ken'ichi), consisting of four morae /ke-n̩-i-ti/. Draconis pointed out another in a comment: 恋愛 (れんあい, ren'ai), meaning "romantic love", is also four morae /re-n̩-a-i/. In each case, /n̩/ is a full mora but may belong to the previous full syllable depending on how your linguistics professor defines a syllable. See also the Japanese Language Stack Exchange question Difference between んい (n'i) and に (ni). 

where e is an eventuality (also called situation, possible event, or state of affairs). Such logical forms can express everything one can encounter in language (such as quantification and logical connectives) so there's no reason not to use them if it helps elsewhere. And help it does a lot in pragmatically interpreting discourse. Meaning assembly that produces this kind of logical forms can be easily implemented (with or without lambda calculus) in both phrase-based and dependency-based grammar formalisms. 

Lambda calculus is a way of turning open expressions (that is, expressions with free variables) into functions. For example, λx.x+1 is a function that takes numbers to numbers. λx.x+y is a function from numbers to expressions with one free variable (if the domain of discourse are numbers). In natural language semantics, lambda calculus can be used to assemble meaning during parsing. The idea is that every word has a meaning (assigned to it in the lexicon) and syntax helps assign meaning to more complex syntactic units. In higher-order logic, the meaning of "Mary obviously loves John" is 

This also works with the slang interpretation of "beats it", with "x beats y" replaced with "x stimulates himself". Another user left a comment pointing out a third interpretation of the sentence, which I find less likely, that leaves at least some donkeys owned by richer men unabused: "every man who owns a donkey beats at least one donkey that he owns." This uses a third free variable: one for the man, a second to identify him as a donkey owner, and a third to identify a particular donkey as a beaten one. This third takes an existential quantifier. 

The English sentence "every man who owns a donkey beats it" can be interpreted as "every man beats every donkey that he owns", which makes no implication that anything exists. It is true even if there are no men, no donkeys, or no donkey slavery. This means a rigorous wording of this sentence uses only universal quantifiers ("for all" or "for each"): 

does dogs means multiple instances of a class dog? or does it mean the multiplicity of a same instance? 

Same adjective usage from homonymic adjective usage: The examples provided in the question (here in French): 

With regard to morphology a common example of a lexeme is [dog, dogs] where dogs is the plural inflexion of the lemma dog modified by the -s suffix, marking plurality. Although I can accept that dog and dogs are, morphologically speaking, the same word i.e. the same lexeme, it still bothers me because each has a different meaning. Semantically speaking, can we say dog and dogs are the same words? How does plurality relates to dog semantically: 

In dictionaries, however, what is never defined semantically. If I meditate on a word like car, I inevitably associate images, feelings and experience with the word. Even if I think of nothingness I somehow associate sensations and a mental image of emptiness (even if it may be wrong). When I contemplate the word who, I associate the word with a "person whose identity is unknown". When I think of what however, in terms of pragmatics, it seems to suggest a request for more information to the interlocutor, but semantically, I cannot find any meaning to it. Hence this odd question: What is the semantic meaning of what (or its equivalent in any language)? Could the meaning of what be defined as "a thing unknown to me"? 

In Guarani, the derivational suffix -kue means former, ex-. It's also used in Jopara, the mix of Guarani and Spanish. 

The idea is that deep syntax structures should be as language independent as possible. In your sentence "will" is only a tense carrier and "to", though being a semantic preposition, functions as a case marker (illative). From a graph-theoretic perspective, to translate a surface tree into a deep dag is to remove all the nodes that represent function words from the graph. Thus at the level of deep syntax a sentence has the same structure in all languages unless there's a lexico-semantic mismatch (in which case the edge labels will be different). In grammars with feature structures, deep structures are constraint-based. In LFG, for example, c-structures are phrase structure trees but f-structures are isomorphic to (deep) dependency dags (if one substitutes thematic roles for grammatical functions). From a logical perspectives, deep syntax dags are "hierarchical logical forms". Lambda calculus could be used to incrementally assemble the logical form and there's a one-to-one relationship between logical forms and dags. Also worth mentioning is information structure which is sometimes added to dags (though it's not part of syntax sensu stricto). It may be expressed as an ordering on nodes (as in FGD). Note that to fully account for syntax one needs a LP component (such as topological trees) which means that surface dependency trees are uninformative since they can be reconstructed from structures at the other levels. They can be view as a blend of topological trees and deep syntax dags. Surface trees tell us little but are structurally closer to the phonetic form of the sentece. Update: A few definitions might be useful: