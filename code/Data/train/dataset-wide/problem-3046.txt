Why do you need pandas at all? Are you using it just to store HDF5 files? If so, I'd recommend looking at h5py instead. You may have to learn a bit more about how HDF5 file formats work, but I think it would be more efficient than Pandas. In particular, in an application like this where the game board is naturally a matrix, Pandas' model that rows and columns need to have labels seems like annoying overhead instead of a feature. You could even store the iterations of the game as a single datacube instead of as separate HDF5 datasets. Probably defining your own chunking will improve HDF5 performance at the largest sizes, but it depends on how you want to view the data. Do you want to view the entire board for a single iteration easily? Or do you want to view a small region of the board across multiple iterations easily? Unless you have particular views in mind, a good starting place would be to just use 's autochunking, i.e., don't define your own. This is minor, but why are you using any interpolation at all in ? I'd recommend . 

BTW, the more common choice for the error model would be Gaussian noise instead of uniformly distributed noise. You might consider adding a comment to the code to explain your choice of the uniform model. 

Having separate functions separates the multiple algorithms you want to use more cleanly. This is valuable not only for your own thinking, but also if you want to profile your code to isolate specific time- or memory-intensive steps. A Python line-profiling tool that I use and recommend is called line_profiler. For bonus points, you could use instead of in both functions, which would lower memory requirements, although this could require modifications to the algorithms you use. 

I would personally find a name like or something to be far more illustrative and informative than . Also, the function itself could do with a slightly more informative name, such as or . The way you are generating feels very unnatural to me. Since you have already, you can just use numpy to define a without any for loops, like this: 

This was noted in the comments by @flodel. Identifying slow steps The best way to figure out what lines of the code are slowest is to use a line profiling tool. I like profvis. In general, the slow steps in R are element-wise assignment to vectors/matrices/data.frames, and unvectorized functions. In your case, 

Since you are working with binary data, you should tell NumPy that your is everywhere that you can. That will lower memory requirements and speed up certain operations. For example, in should become . The default NumPy is which is not what you want. The python convention for naming functions is i.e. all lowercase with underscores, not . So should be etc. The right approach for optimizing your currently too-slow function is to use line profiling. I really like the module. If you use IPython you can use it very easily as an inline magic function. Doing #3 will mean unpacking that formidable iterator comprehension you wrote! Instead of squeezing it all on one line, I'd define a generator function for the iterator, and try to be extremely explicit so that every line has only one function call, like this: 

I had my own solution to PE #2 lying around and in case it's interesting or useful to you here it is. (It uses and is wrapped into a function.) 

I renamed the function to eliminate the word from its name, which was misleading: you are multiplying the Earth's radius, so the result is not distances on a unit sphere but rather on an Earth-sized sphere. I added docstrings to explain the function. Rather than have the "magic number" in the bottom of the code, I moved it to the top, defined it as a variable with an all-caps name. This make it more apparent that the function relies on this value and is consistent with recommended Python PEP8 style best practices. The units (km) in particular may not be apparent to users of the code. It is obviously inefficient to compute every distance twice, as my version currently does. However, your version also double-counted, which would be difficult to avoid without storing a copy of the keys in your dictionary in a list or tuple, so you could them and then make the second loop go only from the index of the outer loop on up. For easy array-based trigonometry, I switched the calls to and to and . After that the only import I needed from was , but as Jaime pointed out that is available from numpy, so I just used that version and eliminated all imports of anything from . 

Storing the full array of Fibonacci numbers isn't necessary. You are taking the sum "as you go" and not storing every summand in the sum. You would do well to do the same for the Fibonacci sequence itself. Just store the last two values. I think your code could be nicer if it were wrapped into a function. You could use Python's keyword to make your function a "generator function", a technique that works well with "state machine"-like approach of storing only the terms necessary to compute the next term instead of storing the full sequence. You used the reserved keyword as a variable name. That's bad; try to avoid doing that if at all possible. 

Whether you regard my code as an improvement probably will depend on how familiar you are with numpy. I doubt it matters in this particular application, but if for some reason you wanted to do it with huge words, this version would likely be faster. Sample output: 

Why did you write your own functions for and ? It looks like you are doing fairly standard things with them, so I would use implementations from or instead. (And calling the median of a vector times a "weighted median" of vector makes some sense but it is really just the regular vanilla median of .) Your class seems pretty monolithic. I'd either remove it entirely and just use global-level functions and scripts, or make more classes. For example it looks like could be a class, and maybe even . I agree with all of Ethan Bierlein's comments too. You could make the binary logic statements in the big decison making loop a bit easier to read. For example, instead of duplicating and in every , you could pre-define a before the and then just use that variable instead of repeating the binary logic. You could do something similar with the other repeated logic. In a few spots you are mixing and native Python when you don't have to. For example, does really need or would the python built-in do the job? Or even better yet, if why not make the , , etc. variables numpy arrays? 

This converts the data frame to "long" format, with one record per row. So if the original data frame had six columns and rows, the new one will have rows, and three columns, one named , one named and our extra column not included in the call. 

Consider wrapping up your parameters into dictionaries. For your code, this is really a matter of style rather than best practices or anything. Your takes in 7 parameters, which is getting close to making it hard to use your function, mostly because remembering the order to pass those parameters is starting to get tough. 

So with this alternate , the sigmoid method is now only 2× slower, not 20× slower. I should emphasize that (a) I don't know enough about neural networks to know if is an appropriate expression or approximation to the actual derivative of , but if it is in fact OK, then (b) the reason that is so much faster is because of this approximation/error. The remaining 2× difference is because in your factored expression of , you are needlessly evaluating twice. I'd instead do this: 

Assignments to Pandas dataframes always work better when doing entire columns at once, instead of filling a single row or a small number of rows at a time. In the code below I first completely define a NumPy array of cluster numbers and only after it is completely defined do I pass it into the Pandas DataFrame. Even aside from the speed issue, your loop where you assign cluster numbers is very confusing because of a misuse of : you have but the idiom is always . You have the position of the index and the value switched. In your particular case this works and is a nice trick but you should document how this works in your code if anyone besides you is ever meant to read it. I changed variable names to be PEP8 compliant and to be easier to read. I also added some comments, docstrings, and formatting. Particular changes of note were defining and as variables, wrapping the code you wrote into a function (so that I could line profile it), making a variable defined in all-caps (which in Python suggests it is a constant), and defining the radius in the call to to be in terms of rather than just be another magic number hard-coded into the code. I think most of these changes improve your code's readability and make it more in line with style guidelines for Python. Minor point: coercing to boolean with works on SciPy sparse CSR matrices, and so doing the coercion before converting to a non-sparse NumPy array with should be slightly faster and use less memory than doing things the other way around -- no time is wasted on converting zeroes. 

Your function doesn't have docstrings. Docstrings are good because they give you an opportunity to tell users of your code how to use it. I had to read the whole code to determine that the binary search you were referring to was for element extraction on a (presumably sorted) list. You don't check for (or even document) that your function only works on sorted lists. For example gives which might not be what users expect. You could add an statement to guard against unsorted arrays being passed in, or at the very least mention the requirement for sortedness in the docstring. You could easily supply default arguments for and in the common use case that the search should start on the whole array. The case of is a bit tricky since the default value depends on the length of the whole array. But you could so something like: 

Speed comparisons are always a good thing, but it can be tricky to determine what is actually being compared. I think it's premature to decide that the comparison is "Python" vs. "R" without a lot of work to verify that all the libraries you use and functions you write are reasonably optimized for each language. One of the strengths of python is a pretty good set of line profiling tools. I like , because it works in IPython notebooks, a format I find convenient for "real time" coding and tooling around. I used line profiling to run a (Python 2 version of) your code, and I found that one single line in your script was responsible for 94.5% of the execution time. Can you guess what it is? It was this one: 

I'm not a veteran of pandas but I think it's safe to say that building a data frame row-by-row in pandas is not very efficient. How did I refactor your code to run in Python 2 and ran on it? Essentially I just wrapped everything you wrote into a dummy function called and called the line profiler on that function. For Python 2: 

By adding your code into its own function, I was able to compare timing between the version and your version: 

are likely to be slow. And the function in R is unvectorized as also a bottleneck. Possible solutions Vectorization of assignments I'm a big fan of the tidyverse system of packages. For this case, the , , and system of packages allows a very nice functional implementation of your problem. However, they won't automatically vectorize functions like . Vectorization of Googling around, it appears there are a few vectorized implementations of . One I found easy to use is from the genefilter package, . And although it isn't as much of a bottleneck, I also used a vectorized computation of by using the and functions from . Putting it together Below I implement some of what your original code does. I didn't worry about the particular flavor of t-test that you were using (i.e. ), and I didn't implement computing the correlation between p.values. 

This run, which covers about 1/5th of the effect sizes and 1/10th of the sample sizes you are interested in, takes about 5 minutes to run on my laptop. Since it uses , I think scaling up to your full system size would take about 50 times this number, or about 4 hours. The bottlenecks here are still the t-tests, but also simply calling , a sign that we got rid of the assignment-by-element bottlenecks and reduced the t-test bottlneck at least somewhat. 

The way you have broken down the code into three functions is reasonable and logical. Nice! The functions you wrote don't have docstrings, so it is hard to know how to use them. I couldn't get your function to run initially, for example, because I didn't know what was supposed to be. Adding docstrings would make this clear. For the specific case of the parameter in , why do you need it? When I used the function I did it like this: 

From then on, every operation on your data should be an operation on a NumPy array instead of on a Python list. The way I wrote it, it assumes your data are integers, and also that 0 can never occur as a real data point. You can modify the and offset of the call to accordingly to meet the requirements of your particular data. This approach will only be good if each user has a number of data points that is not too different from the number for the other users. Other wise representing your data as a full matrix will be memory inefficient. Use s. If your data are non-negative integers, for example, then will be much faster than , for example. Actually, if your data are integers, then you could probably just use to make your histograms in native Python, which could also save time.