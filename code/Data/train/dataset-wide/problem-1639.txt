There isn't one, because there cannot be one: which celestial object you see towards the east at 10 degrees above the horizon depends on: 

This is all very local, so we'll just assume a Hubble constant of $\textrm{H}_0 = 70\textrm{km}\,\textrm{s}^{-1}\,\textrm{MPc}^{-1}$ (Planck says 67.8 and Riess et al. say 73.8) and calculate redshift and hence apparent recession velocity at that distance using the Hubble law \begin{equation} v = \textrm{H}_0 \cdot d = 70\,\textrm{km}\,\textrm{s}^{-1}\,\textrm{MPc}^{-1} \cdot 4.5\, \textrm{MPc} = 315\,\textrm{km}\,\textrm{s}^{-1}\,. \end{equation} At constant acceleration of 1g you'd get to that speed in about 9 hours, but you might need more time to do your data analysis on the galaxy spectra you'll have to take. 

Have a look at SCAMP for astrometry and SWarp for stacking. Like the software mentioned in the other answer, both are open source, so you can check what algorithms they use. SCAMP documentation is here, with an explanation of the algorithm in chapter 6.7 (page 25). There's also a short paper, but the manual seems more thorough. Note that the software is written with wide-field multi-CCD mosaic detectors in mind, so what they do might be overkill for what you have in mind. 

Context Iron has the highest nuclear binding energy of all the elements (not completely true, but sufficiently accurate in an astronomical context). So, fusion of light elements into iron or something lighter is an exothermic process - you gain energy doing it, allowing the star to function. This is what happens in the last stages of a type II supernova. The core of a massive star in its last moments of life is hot and dense enough to fuse silicon into iron. Just before the supernova explosion, there is an iron ball of about 1.4 solar masses at the centre. The progenitor of a supernova type Ia is a binary system where a "normal" star loses mass to a compact stellar remnant (a white dwarf). Once the white dwarf Contexthas accreted enough mass to be above a limit of 1.4 solar massses, fusion starts again, completely disintegrating the compact object. Explosion A SN Ia completely destroys the white dwarf progenitor in a runaway fusion process. In a SN II, the pressure on the central iron ball exceeds the degeneracy pressure exerted by the electrons in the iron atoms' electron shell. The Fermi principle in quantum mechanics states that no Fermion (such as an electron) may occupy the same quantum mechanical state as another. The pressure exerted here is so large that the electrons of the iron atoms can no longer obey it and are forced into the nucleus, where they react with the protons to form neutrons. Iron abundance Why do SN Ia enrich their environment with more iron than SN II? This is not so much a matter of iron production, but about how much of that iron ends up in interstellar space where it can be part of a new generation of stars. In a SN Ia, the progenitor is completely destroyed, scattering all its constituent atoms into its host galaxy. A SN II forms a compact remnant, either a neutron star or a black hole. A lot of the later, heavier fusion products end up not being carried outward in the supernova explosion but become part of the compact remnant. Note that a lot of the heavy elements scattered from a supernova of the "exploding massive star" result from an abundance of neutrinos escaping the central explosion and reacting with the outer shell of lighter elements that is blasted away. 

No, there is no effect here. Why? Gravitational lensing magnification works by increasing the observed surface area of the lensed object while preserving the surface brightness. An exoplanet angular diameter is much smaller than that of its host star (even if we can't resolve either). Hence, all the magnification happens across a uniformly lit background, for which the measured increased brightness is zero. In effect, you stretch the image of part of the star into equally bright other parts of the star, resulting in zero magnification - the Einstein radius of the lens is much smaller than the radius of the star. You might get a negligible effect when the star moves across the edge of the stellar disk. Why does even that not matter? The geometry is very disadvantageous. Setting aside the small mass of the planet (which is another factor), gravitational lensing works best when the distance between source and lens is about the same as the distance between lens and observer. This is very much not the case in the scenario we look at here. Summary 

From the abstract of that paper, it seems to describe their method (they practised with mock GAIA data). Seems like they use the previous Tycho catalogue to improve astrometry and parallax measurements from GAIA data (they mention using Hipparcos parallaxes as a consistency check). So, figure 1 seems to be Cepheid distance vs. Parallax distance. 

Point Spread Function: The PSF of a single telescope, even a radio telescope, is much simpler than that of an interferometer. So the data analysis required to turn observations into an image should be much more straightforward for FAST. 

This is quite a broad question, since there are far more differences than similarities between the two. This answer will focus on the first part of the question and be about the technical aspects of the two telescopes. 

There are three main classes of galaxies: Irregulars, Ellipticals, and Spirals. Irregular galaxies, as their name suggests, do not fit into the "normal" classification scheme. So, how do we distinguish between elliptical and spiral galaxies? Brighness profile The radial brightness profile of an elliptical galaxy follows a deVaucouleur law ($r^{1/4}$). Spiral galaxies have an exponential radial brightness profile, although their central regions ("bulge") also follows a deVaucouleur law. Star formation Stars are formed in the spiral arms of spiral galaxies (and can be formed in irregulars), while elliptical galaxies tend to only have old, and consequently low mass, stars. Components As far as we can tell, all galaxies consist of a dark matter halo and stars. In addition, spiral galaxies also have clouds of dust and gas. If conditions are right, these can form new stars. (Some ellpticals have a very thin, very hot gas component as well, but there is a lot less of it than in a spiral galaxy). Kinematics Spiral galaxies are rotationally supported, while elliptical galaxies are mainly pressure-supported (i.e. they act like an ideal gas, with stars as gas molecules). There are some rotational features present in ellipticals, but they tend to be minor compared to the overall random motion. A graphical overview of the various galaxy types is usually shown in the Hubble tuning fork diagram. Note that this does not indicate an evolutionary progression from one type to the next. 

Searching for Exoplanets is an ongoing project, and there is no complete catalogue in any sense of the word yet for any radius. Most Exoplanets were found with the Kepler mission, but that one looked in a fairly small part of the sky. What exactly do you mean by hypothetical planets? 

Result is 412, consistent with yours. So, your integration and interpolation works. Right now, I think the problem might be in the definition of the half-light radius (missing pre-factor?). You should be able to find that in the astropy documentation. Quick check: Changing to 10, and integrating out to 100 gives a factor 10 larger half-light radius. 

That won't be easy. These catalogues have a magnitude (brightness) cutoff. Brighter, more massive stars can be seen further away than smaller, darker ones. 

Short answer, no, they don't. Longer answer, it's complicated. There are, in essence, five different measures for the centre of a galaxy cluster, based on different physical properties of the cluster, and occasionally not in agreement with each other. 

This is indeed the VLT, or rather one of the four 8.2m Unit Telescopes (UT in ESO parlance). Looking at the optical layout (found here along with a description of the telescope), the small mirror in the centre of your screenshot is M3, the tertiary mirror. M1 is the primary (in the background), and M2 is out of the frame towards the top- the white supports you see in the foreground hold it above M1. One advantage of this design is that you can have two large instrument platforms on the vertical tilt axis of the telescope - this way, you don't have to dangle several tons of instrument from the support structure (Nasmyth focus in the image). 

Best chance would be the Hipparcos catalogue. The first set of Gaia data will be released Mid-September 2016, but I don't know if it will be more accurate than Hipparcos already. 

That star is probably the planet Venus. It is always close to the Sun, and Stellarium shows it to be in the south and well above the horizon at around 5pm local time. To check, right above it should be Mars (also a planet). In general, if you see a very bright star, it is very likely that it is a planet, and most likely to be Venus, Mars, or Jupiter. The latter two can appear at any time during the night, but Venus is always close to the Sun, so it can be seen just after sunset or right before sunrise. All planets appear close to the track of the Sun across the sky. 

You may have gotten your reference frame wrong. The moon never stopped revolving around its own axis, it's just that one revolution takes as long as one orbit around the earth. 

So, there are other methods to find the centre of a galaxy cluster. This paper suggests that ususally the weak lensing centre is offset with respect to both X-ray and BCG. I am not aware of anyone using the mean position of the galaxies as a measure for the cluster centre. This paper evaluates the cluster centre offset between measurements in X-ray and the SZ. This is interesting because they both trace the hot intra-cluster gas. 

No, it just slowed down contiually until one side faced earth permanently (discounting libration, which, as you say, is due to the ellipticity of the orbit.). Tidal forces get weaker as tidal locking progresses, so the change in lunar spin would be greater at earlier times and then asymptotically approach zero as the moon becomes tidally locked. 

It goes on to explain how the observed field was chosen - no downtime due to it being too close to the sun, no obstruction by asteroids, etc. A ground-based telescope has the obvious problem that it can't observe during the day, so you need several telescopes in different parts of the world, and combine their measurements. Since the distortion patterns, viewing conditions, instrument characteristics etc. are all different, combining the data is anything but trivial if you want a precision measurement. In short, a change in the point spread function between telescopes might introduce both systematic and statistical errors in your brightness measurement. Kepler is only one instrument and optically highly stable. Is KIC 8462852 bright enough for a small ground-based telescope? Certainly. Its magnitude is 11.7, while Kepler wants to observe stars brighter than 14th magnitude (link above). This magnitude calculator estimates a limiting magnitude of 17 for a 1m mirror. It takes time to set up an observation. Large telescopes like the VLT accept requests for observation time twice a year. After your observing proposal is accepted, it will still take weeks or months until your observation is actually made. For smaller telescopes, it might be easier and quicker, but it will take time to set up a monitoring programme, even if telescope time is available. However, there are ground-based, long-term monitoring programmes. COSMOGRAIL is one that comes to mind. From their homepage: 

Because of Earth's rotation and motion around the sun, the positions of celestial objects change. Among themselves, stars keep (for our purpose here) their relative position, and astronomical coordinate systems make use of that fact. The celestial sphere on which we project the stars rotates roughly around the North Star. Angular distance from the North Star (or more correctly, from the celestial equator) is called declination. As you know from Earth's coordinate system, the position of the Null Meridian is arbitrary. On Earth, it's in London, in the sky, it's in the constellation pisces. This is called right ascension. More information: wikipedia. Note also that, depending on the application, other coordinate systems may be used in astronomy (such as galactic or heliocentric). 

You'll have to convert that yourself. If you know how to program, there are libraries for coordinate conversion. I would argue, though, that an Earth-centred spherical coordinate system is preferential for most applications. Distances are hard to measure. Hipparcos and Gaia use the parallax method. This gets less accurate with increasing distance. Also, distances are naturally measured in parsec, but the conversion to light years is trivial. The Gaia catalogue will have $10^9$ stars with positions and velocities. It will also be more accurate than Hipparcos. But the measurements are ongoing, and the final data release is planned for 2022. 

In addition to @PeterErwin's answer, there is the Himalayan Chandra Telescope (2m), located near the Indian Astronomical Observatory at Hanle at 4500m altitude but operated remotely from near Bangalore. If you want to leave the realm of optical astronomy, the ALMA telescope array is located at 5000m altitude in the Atacama desert in Chile and operated from a nearby support facility located at an altitude of 2900m. Access to the array itself is kept minimal due to the extreme altitude. 

The Hubble Space Telescope offers a parallel obervation mode. Observing programmes such as CANDELS and CLASH use WFC3 and ACS in parallel fields. These are also the only instruments of which I have heard that they are being used in parallel. Here are my questions: Is it possible to observe with more than two instruments in parallel? If so, what is the limit, and why? Why aren't all observations done with as many instruments as possible turned on? Granted, a lot of the data would be useless / difficult to analyze (like spectra of a random patch of sky), but on the other hand it might lead to serendipitous discoveries. HST won't last forever, so why not get the most use possible out of it? Note that this is about the science instruments - I assume the guidance sensors will be working alongside the others anyway. (Or is that wrong?)