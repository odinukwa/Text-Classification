I am trying to use puppet as follows in Amazon AWS: I deploy an instance which has hostname "server.example.com", which launches the puppet agent on boot and gets an initial software payload. I then terminate that instance. I then re-launch a new instance with the same hostname, which generates a new private key and csr for that instance with the same hostname "server.example.com" This time, the puppet agent update fails, because the private key on the new instance, doesn't match the key for that hostname on the puppet master. How do I overcome this? 

Check that the NRPE service in running on the destination server Check that you have configured allowed_hosts correctly on the destination server so that the Nagios server is allowed connect (nrpe.cfg) Check that no firewalls are blocking the connections 

It will only work for hosts for which that variable is defined For the 95% of hosts for which we don't define this variable, the task fails. Is there a way to only use the variable if it is defined? eg If remote_user is defined, use it, if not, use the environment variable set in .ansible.cfg Note: I know I can use: 

I want to use the kubernetes python client to change the name of ConfigMap from config-web-2-5 to config-web-3-0 

However, when I close the connection on the MAC, DPD doesn't seem to kick in. OpenSwan just keeps logging errors about the connection. Just looking for suggestions re. a fix. 

I have a server that relies on puppet vcsrepo to update code in a local mercurial repository based on a tag. When I change the required tag, using the vcsrepo "revision" parameter, vcsrepo should do a hg pull and hg update on the repo. This is all working fine. However, I have created a clone of this server to test something else, and now when I run the puppet update I get an error: 

The only way this seems to work is if you apply the config file on all 3 nodes, and have them start at exactly the same time. I am trying to deploy this with Ansible, which creates the nodes sequentially, so that doesn't work. What am I missing here? 

That figure is a total. It arises from your uniq -c command. Your command produces a large number of blank lines, due to the way you are using cut, and uniq is counting the number of blank lines it finds. It isn't the number of connections from a particular (unknown) ip address. 

This happens because puppet is running as root, while the hgrc file is owned by user The user parameter in vcsrepo is supposed to deal with this: 

ie the hg commands are supposed to run as user so that the Trust requirement in mercurial is satisfied. But its not working. The clone server is a bit for bit copy of the original. 

If elasticsearch isn't trying to distribute shards to other nodes, it won't have unassigned shards. I'm not sure why the default config on elasticsearch is to have 

I use a combination of Logstash and the AWS Elasticsearch service to index S3 access logs. The logs are collected in an S3 bucket, processed with the Logstash S3 input filter, renamed after they are processed and then archived in another bucket. I use this method so that the number of access log files that Logstash has to process in each rotation is as small as possible. However, the logs are not being processed in real time. When I look at Kibana or query Elasticseach, the most recent log entry that I see will be the latest log entry from the previous hour. I never see log entries that are < 1 hour old. I can't see anything in the s3 input configuration options to control this behaviour. There is an interval config option, which I have set to 120 secs. This is supposed to instruct Logstash to poll the S3 bucket which contains the logs every 2 mins. I also use this Logstash system to process syslog input from a variety of servers, which does process logs in next to real time. Is this something peculiar to the S3 input filter in Logstash? 

If downtime is not allowed, you could clone your server instance by making an image of it, then relaunching the image with the correct security group. You could then create an ElasticLoadBalancer, place both the original and the clone behind the ELB, switch DNS (with very low TTL) from the priginal server to the ELB endpoint and the wait until the load is distributed across both. You would then drop your original server out of the ELB, using connection draining, and reverse your DNS change so that it now pointed to the ip address of the new server. You can then stop/terminate your old server, leaving you with the new one in the correct Security Group. Of course, this solution won't work with a server running a writable database, but if you've got a writable DB on a single server instance in EC2 Classic, you've got other problems. 

I am deploying a docker-compose stack of 5 applications on a single AWS EC2 host with 32GB RAM. This includes: 2 x Java containers, 2 x Rails containers, 1 x Elasticsearch/Logstash/Kibana (ELK) container (from $URL$ When I bring the stack up for the first time, all containers start. The ELK container takes about 3 minutes to start. The others come up straight away. But the ELK containers exits after about 5 minutes. I can see from the logs that the elasticsearch service will not start. The log messages indicates a memory limitation error. However, when I then tear everything down, and bring it up again, all the containers start straight away, including the ELK container, and everything remains stable. The issue only occurs the first time I start the stack on a new EC2 instance. I can see from the docker stats that the ELK container is only using 2-3GB of the 32GB RAM available on the instance. The ELK container is configured as follows: 

ie Every minute (or whatever interval you have set) Logstash starts at the beginning of the bucket and makes an AWS API call for every object it finds. It seems to do this to find out what the last modified time of the object is, so that it can include relevant files for analysis. This obviously slows everything down, and doesn't give me real time analysis of the access logs. Other than constantly updating the prefix to match only recent files, is there some way to make Logstash skip reading older S3 Objects? There is a sincedb_path parameter for the plugin, but that only seems to relate to where the data about what file has last been analysed is written. 

A root volume in Amazon EC2 is not the same as a root volume anywhere else, so assuming that you can manipulate it in the same way as a root volume on server in a rack in your Comms Room will get you into trouble If you want to separate volumes in your server instance, add an EBS volume to it, create a partition on it using fdisk, create a file system on it using mkfs.ext4, then mount it on whatever mount point you wish. If you want this to persist through reboots, update the /etc/fstab file so that the volume is mounted at boot. 

In a task definition, but that will only apply to that task, and I don't want to have to update all task When I really need is for that condition to be available at the hosts definition, ie: 

I have 3 OpenSSH servers and I want to have only one. I have created a new server, and migrated all the accounts/data from the other 3 onto the new server. I now intend to update the DNS records for the 3 existing servers so that they point to the ip address of the new server. However, when clients who connected to any of the older servers now connect to the new server, they are going to get an error, as the server key in their known hosts file is going to differ from the server key issued by the new server. I can move the server key from one server to the new server, so that there are no issues with one of the three, but there are still going to be problems with the other two. I had thought about attached 2 extra ip addresses to the new server, and running 3 separate SSH daemons on it, with different server keys, but I'd really like to avoid this. Has any one got any other suggestions. 

I am trying to create a hot-standby replica for a PostgreSQL 9.5 master. The basic process I have used is to start a backup on the master: 

This issue seems to arise from the way S3 generates access logs rather than anything to do with Elasticsearch or Logstash. According to: $URL$ 

Make sure you use a private key that is in your keychain, or a key without a passphrase, so that you are not prompted for your passphrase for each host. 

Does anyone know what sort of data transfer speed we should be seeing on a VPC peering connection between 2 AWS regions (in this case us-west-2 and eu-west-2)? We tested this a couple of weeks ago, and saw speeds in excess of 200MB per second. Today, we are seeing speeds at ~10MB per sec. Obviously, we are using public transport networks here, so we would expect some variance based on time, but 10MB per sec seems very slow. Given that AWS facilitates cross-region replication of DBs in RDS, presumably they anticipate much faster speeds that this. 

This probably wasn't necessary, but it will reduce the size of the built image. Secondly, I added an extra run command to update the bundler gem: 

I'll work on the premise here that you are talking about the root EBS volume. ie You have launched an instance, and you now want to detach the EBS root volume, and attach that to another instance. Yes, you can do this. Stop the instance (1). Detach the root volume. Launch another instance (2), for which another root volume will be created. Stop that instance too, and detach the newly created volume. Attach the root volume from instance 1 to instance 2 as /dev/sda1 Start instance 2. No need to re-install anything. You can also accomplish the same effect by making an AMI from instance 1 and relaunching it as instance 2. 

I use elasticsearch as part of a Logstash stack, in which all of the components of the stack are installed on the same server. The purpose of this is to expose application logs to developers for debug purposes. I don't need to keep the indices created. I have a cron job that removes indices that are older than 7 days. The raw logs are preserved elsewhere in case we need historical analysis. The problem I have is that elasticsearch keeps entering a Red health state due to unassigned shards. I've researched various ways to recover this, but inevitably, I end up deleting the raw index files and restarting the service. This is a real pain, as its always the time when the developers need access that elasticsearch is borked. It seems add to me that there isn't an easier way to recover elasticsearch other than to delete the offending indices. I've configured elasticsearch to use a single node, no replicas and not to do any network discovery, but every couple of days, it keeps falling over. Am I wasting my time trying to run elasticsearch on single server? Is it always going to keep falling over due to unassigned shards? Given what I use it for, it would seem like overkill to actually have to deploy a cluster. Note: I am running this stack in Amazon EC2