EDIT: To keep things tightly constrained, I want to address this issue in terms of Utilitarianism. I can easily find texts exploring the ethical treatment of existing animals from that perspective, but I'm specifically interested in the ethics of (re)creating a "non-self-aware, but intelligent" animal. 

I don't want to get bogged down in whether those "facts" are irrefutably established in the specific case of Neanderthals - if not them, there will surely be other even more primitive hominids we could eventually recreate, who would most definitely lack those mental capabilities we define today as "quintessentially human". I'm also completely uninterested in the religious perspective here. I think religion and morality are at best orthogonal, and only "secular" morality can be expected to prevail in the long run. Finally, I don't think the morality of recreating Neanderthals strongly correlates with the morality of creating self-aware Artificial Intelligence (which may or may not be technically possible), since in that case by definition our "unnatural creation" would be able to discuss their circumstances with us, as rational thinking entities. In essence, what I want to focus on is... 

Watching this Debate on Extinctions, I was struck by the fact that the assembled panel seemed to have no clear consensus on the morality of recreating Neanderthals, even though most of them seemed to be in no doubt that this would be at least technically feasible within a matter of decades. That video is the second half of a fairly long debate, so I'll just summarize a couple of points I think are now generally accepted by researchers, as background to the ethical issue... 

I reason from the above "facts" that if we did recreate Neanderthals, our ability to communicate with them would forever be on a par with how we interact with, say, cats and dogs - they don't and never could have anything like human self-awareness, or a concept of "the meaning of [their] existence". 

The above is a 4D object represented on 2D. Quite the loss of visual information here. A 3D holographic projection could offer way more clues as to how a hypercube may look like. In that case. you would a 4D object represented on 3D. Much more visual info to get from that. But what humans lack in this case is the initial image of how the 4D hypercube looks in 4D. Due to that, there is no end point for the extrapolation to work, so in most cases humans will not be able to extrapolate at all a 4D object. 

It cannot be objective. It's not defined anywhere, it has no exact specs therefore you cannot make an universal rule for it. It will always be subjective, no matter what some completed in polls or not. Even in extreme situations (like mutilation) some may find that more attractive than the normal so beauty will always be subjective. The fact that so more (even a majority) agree on something does not make it objective. 

By choosing the orientation the humans think the cube may have, we can say that we generated a random sequence of 0s and 1s. But is it truly so ? If we do this test to thousands or more, we may find that a selection is way more preferred compared to the other. That means some other pre-determined factor (like spatial orientation capabilities, geometry knowledge, the distance between the eyes and object) influences the choice. So in order to actually have the random result we must consider all these factors when making the test. So basically, if we are able to account for everything involved, we may find a trick to generate something truly random. Otherwise...slim chances. 

One does not have to agree to a conclusion if he agrees to all the facts leading to it. Example: you can have 5 symptoms and a nurse can maybe formulate a diagnostic bases on that. But the doctor with many years of experience may independently check that all 5 symptoms exists and manifest as described by the nurse but he may set a totally different diagnostic. Hegel himself admitted that his dialectical method was part of a philosophical tradition dating back to Plato but he criticized Plato’s version of dialectics. Hegel argued that Plato’s dialectics deals only with limited philosophical claims and is unable to get beyond skepticism or nothingness. According to the logic of a classic reduction to absurd argument, if the premises of an argument lead to a contradiction, we must conclude that the premises are false. That leaves us with practically nothing. In turn, Karl Marx presented his own dialectic method, which he claims to be a direct opposite of Hegel's method. Direct quote from Marx in the paper "Afterword": 

In other words, Churchland would say you are wrong in claiming that mental states are 'useful'. He points out that there are many limitations and flaws to the theory, that from a scientific perspective render it not very useful. Churchland calls this approach 'functionalism', and compares it to those who argued that Alchemy was a useful abstraction and not an ontological theory, and so it shouldn't be replaced by chemistry. He summarizes: 

This is actually a paper and not a book, but one of the most influential in this area. Empiricism and the Philosophy of Mind (1956) by Wilfrid Sellars 

You describe with dazzling precision the exact difference between the two positions. And so you ask a further question (I take it to be the real question): Isn't this a word game?. I suppose that it is, in the same way that the statements "the Earth is round" and "the Earth is flat" differ only as part of a mere 'word-game', if that refers simply to the fact that 'round' and 'flat' are words. 

The quotes are taken from this paper: Elimanitve Materialism and the Propositional Attitudes By Paul Churchland. I recommend reading that for a more satisfying answer to your question. To answer the question directly, you ask: 

So to answer your question, Deleuze might say that the requirement for measuring (and he would be pleased that this question is indeed a Kantian question) is precisely the limit: 

For Deleuze, the universe consists of infinitely many forms appearing and dissappearing at infinite speeds ('chaos' or 'chaosmos'). Science uses measurement to help us deal with the utterly incomprehensible chaos (hence the 'Kantian aspec') by slowing it down: 

The act of 'measuring', limiting or slowing down is something that the philosopher Gilles Deleuze has written about quite extensively. He seems to have inherited this particular concept from Leibniz, who is in some ways the inventor of the limit (which, as you correctly note, is the ground for the possiblity of measurement), and who also wrote quite a bit on the subject. It also takes on a Kantian aspect in Deleuze (like most of his concepts), whose Critique of Pure Reason (in particular the chapter 'Transcendental Aesthetic') can also be seen as a meditation on the idea of measurement (for Kant, space [and therefore measurement] is no longer a relation [e.g. between bodies] as it is in Leibniz, but the form of appearances itself). According to Deleuze, measuring (although its not a term he uses himself, nor does it appear to have any significance in English translations of his work) is the defining gesture of science and scientific thought, and is related to the primary functives of science (functives are the things which a scientific function is made of, and functions are the objects of science): 

University research is not a guarantee for accuracy of theory. Any theory can get to be widely accepted and then something else comes in that invalidates the initial theory. It's how science evolves. Given this and how today's science and specially physics tends to work, I'd rather trust an independent research of a small group of scientists than that of an university's research. The logic is most of the time university projects are bound to comply by the very restrictive already established theories (there are exceptions, but they do not form a majority) while independent research can practically dig in any direction unrestricted. So far this approach worked very well for my research teams. 

Here's a example: We do an experiment and get valid results. We say X causes A to happen. We do another experiment and get valid results. We say Y causes A to happen. In both cases, we have math and observations backing up our claims. But X and Y are self-exclusive (a xor if you will). So what can we do in this case ? We can do experiments of another nature and see what was actually correct. But if that's not possible, we should select the cause that actually can explain more. I encountered this in physics many times, where theories were validated both by observation and math and in the math part we had a constant "c" and a variable "v" because in this manner the observations could be explained. But re-thinking everything, one could see that in the math formulas, the same valid result is obtained if "c" varies and "v" is declared a constant. Both cannot be variable or constant in the same time because it would invalidate the math part supporting the theory. So we got 2 options leading to the same result, confirming the same theory but we do not know which is a variable and which is a constant and we have no observable/experimental way to determine this. What did in such a case was choosing the option that can explain more. In the current example, let's say if we had a constant "c" and a variable "v" we can perfectly explain how a car engine works, but cannot determine anything related to the car wheels. If "c" varies and "v" is constant and we explain how the engine works just as in the 1st theory but we can also explain how the wheels work, we select the second option as the valid one. 

Your point about the usefulness of 'higher level' descriptions is one addresed by Paul Churchland very early on in the history of Eliminative Materialism, and constitutes a position known as functionalism, the origins of which probably lie with Wilfrid Sellars: 

Eliminativism (in particular, Eliminative materialsm) proposes that the theory of 'mental states' ('folk psychology', hereafter FP) is a theory. This is part of a wider position known as Theory-theory, as @Era points out. Moreover, it states that this theory is false and should be completely replaced (by neuroscience or something similar). Churchland describes the belief in mental states as follows: 

I think that the many 'Introduction to philosophy' texts suggested by other answers may not be the best place for you start. I think that an understanding of the context in which philosophy first came about (the Greek milieu) is indispensable in understanding what philosophy is and what it has meant throughout history. So, I would recommend: Plato's Symposium This is a very short and breezy read. I think its also one of the most well written works of philosophy that we have. 

Leaving aside the issue of the definition of 'philosophical logic', I think that the most famous book about it is the Tractatus Logico-Philosophicus of Wittgenstein. A touch-stone for Analytic philosophers and other kinds of pony-tailed pedants. 

Because your definition of philosophy ("philosophy is defined by its method, which is based on logical analysis.") is not widely (or at least not unanimously) held. So, existentialism is considered philosophy by those with a different definition of philosophy than yourself. An example of a concept of philosophy that is explicitly opposed to your own can be found, I believe, in Deleuze & Guattari's Qu'est-ce que la philosophie? ('what is philosophy?'). Here is a passage from the chapter 'Prospects and Concepts' taken from the English translation by Burchell and Tomlinson: