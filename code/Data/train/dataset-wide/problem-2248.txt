You'll either need to add a unique field (such as servername, etc) or configure some way of making each record unique. Generally, if not designed for at first, will require a schema change. Commons solutions are to use even/odd numbering in identity columns/id fields or using the source host name/database name. 

Judging by your other questions, I don't think SQL Audit would work for you. Most likely you're going to need to change your application and build in auditing to gather what you're looking for due to the complex requirements you are looking for. The application could be changed to generate its' own transaction id with the id of the web user, etc., and log that to an auditing area. That's, however, up to the application but would get you want you want. Attempting to continue to mine transaction log data will at best cause you throughput issues and at worse crash/suspect your database. 

SQL Server Errorlog Application Event Log System Event Log Cluster Log/Clustering Event Log Hypervisor Logs/Database Internal Lists of Downtimes/Upgrades/Patches/Etc. 

There is nothing stopping you from creating a 3-node cluster (all nodes in the same cluster) and installing two SQL Server FCIs. You'll get roughly the same results. 

Yes, it'll test your backups and the database but won't test the primary server. This means you might not have any issues on the test server but the production server may have intermittent IO issues - this won't show that. Still better than nothing and you're testing backups :) 

I'm assuming you're talking about the actual cluster service. If that's the case, then no do not change it. You could change it in WS2003 when domain accounts were used but since 2008 this is not possible. If you do this most likely your cluster will break. 

Yes in terms of what you can read at any given point as the log file can be thought of as a giant ring buffer that writes over the older inactive portions. It's circular and will continue re-using inactive portions unless it fills up and must grow because all of the current space is active. 

Generally speaking a differential works very similarly to a full backup, except that only the changed extents in the DCM are backed up. Differential backups still contain a portion of the log needed to be transactionally consistent. This will differ for every environment and is not "known" at the time of backup. You can use the of the differential to check LSNs that are included and covered. When you think the transaction finished and when SQL Server wrote the logs that may or may not be flushed to disk when the checkpoint ran before the differential backup may or may not be included. 

You cannot eliminate log backups for the most important reason of transaction log re-use. It's not possible to know when an issue will occur and thus having multiple ways to get to the RPO is useful and sometimes necessary. Differentials aren't a substitute for log backups, they will lower the RTO as they are much faster to apply than log backups but ultimately aren't worth much on their own (spanning broken lsn points, etc, is useful) How are you going to know which ones you need and don't need ahead of time? 

If all are disconnected because the network is exceeding sluggish or gone, then it won't be able to talk between the servers for a failover - the databases need to be synchronized, the primary is online and the session timeout for the mirror elapsed so it looks like a mirror disconnect rather than a primary issue. Even if something were to happen, after the disconnect the databases would not be in a state for automatically fail over. Reference of the required conditions for automatic failover for mirroring. Also, with Mirroring completely gone at this point, I'd suggest migrating away from using it... though you probably already know this :) 

Logon triggers are special. Not going too far into it, they must exist in the master database and not any other, thus the answer to your question is: "It can't." $URL$ -Sean 

Fairly certain it was stuck this way for a while because there is another entry later on with the same transaction timestamp. After that first logged message, we see... 

While this post won't be a complete answer due to lacking information, it should be able to point you in the proper direction or otherwise gain insight which you can later share with the community. 

It would restore ALL of the extents that were changed since the last full backup. These may or may not reflect actual changes. For example, a large insert statement that put 1000 rows into an empty table that was subsequently rolled back. The extents allocated to that table, while nothing would be in there since it was rolled back, will still be apart of the differential as they were changed. All the differential does (well, not all but mostly) is look a the DCM and backup those extents, figure out how much log it needs and presto you have a differential backup. So it won't apply the transactions (as in replaying them) but it will hold whatever their consequences are. 

AlwaysOn Availability Groups do not work at the transaction level. The unit of work is at the log block level. This is very different as we ship log blocks for all transactions, whether they are committed or not. The most likely "fastest" shipping mechanism would be the SAN replication given the parameters set forth and given the current versions of SQL Server generally available. This is only taking into account the literal question of "what method makes my data get to the other end the in the least amount of time?". 

Without going into a large amount of technical detail, you should choose whatever the storage behind the virtualization layer likes as it's preferred or most optimized transfer size, that's what I'd use for my allocation unit size. This is because it matches what the storage layer likes the most and will be the most optimized. If you want to get a little more technical, uniform extents are 8 contiguous pages which just happens to be 64k. This happens to nicely line up with a 64k allocation unit size. The largest possible log block can be 60k, which again fits nicely... but generally is much less, which results in space waste, and could not be the optimized size that the storage layer likes. The allocation unit size is just the smallest possible fragment given to a file. Thus an AU of 64k means a file, holding just a single bit would still have an on disk size of 64k. Normally, larger AU sizes result in lower on disk fragmentation which may or may not matter, depending on the storage array's physical storage medium. In an example of this, SSDs and NVME hold internal translation tables for pages (not the same as DB pages) for wear leveling and write amplification. In this case, there is no idea of where the physical data will be saved, and thus may or may not be physically sequential. Thus, again, I'd go with the value specified by the storage layer vendor. If you don't happen to know what that is, 4k and 64k are both well supported (though larger AU sizes may not support certain NTFS technologies, which shouldn't be used with SQL Server anyway - but for completeness I added it). Really, what will make the largest difference is whether the storage layer has mixed 512 and 512e/h storage representations (which is the physical block size). This will cause more issues that allocation unit size of the filesystem. 

There is absolutely no reason to. Once the database is restored, if over top a previous database or as a new one, there will be no information in cache (or very little). If you were to restart the services, the only thing accomplished is that SQL Server has to run recovery on all of the databases in the instance and re-acquire memory. This would actually cause a detrimental effect, assuming this isn't express edition and the buffer pool is actually being used (i.e. there is more than 1 databases on the system). Even if it were, there is still no good reason to do so. 

If you're using a CNAME, it'll just point to the actual host so you won't have any issues there. If you put in a new A record then it will cause an issue and you'd need to setup the SPN for the new A record. Otherwise, with a CNAME, the SPN should be the name of the host. 

There are some pages corrupt in the clustered index. This means there could (most likely will) be data loss without a recent backup. Object ID 1761739862 You don't have a recent backup. You already ran checkdb with repair_allow_data_loss so there is no way to know what you've already lost or what has already been done which takes you an extra step back. It looks like there is an issue with the disk subsystem. 

This can happen for a variety of reasons. There are some subtle nuances that make it seem like it isn't actually up. Let's take a look.