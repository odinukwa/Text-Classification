I'll focus on your Question 2 about the popularity of emotivism among contemporary meta-ethicists, and maybe the answer to 3 follows from it. Most contemporary anglophone meta-ethicists are moral realists and cognitivists (see below), but a specialist could speak better to the distribution of particular views in the subfield. As for popular morality (Question 1), that's a question psychologists are better positioned to reply to. I know Darcia Narvaez works on the empirical analysis of moral views in the United States public, for instance. But for Question 2: In the November 2009 PhilPapers survey, in which most of the 3226 respondents were professional philosophers including PhD students, 17% said they accepted or leaned towards some form of non-cognitivism about moral judgment. (Emotivism is one form of non-cognitivism.) Among professional philosophers with an AOS (Area of Specialization) in Meta-ethics, 15.6% (16/102) said they accepted or leaned towards some form of non-cognitivism about moral judgments. Let's keep in mind that being a minority position among philosophers in no way entails being wrong! 

It is at least logically possible that the universe be designed in such a way that the designer is undetectable. It's an internally consistent hypothesis, and it's consistent with everything we observe. However, it doesn't follow from that that it's a reasonable thing to believe. Other more radical hypotheses, like occasionalism are be internally consistent, too. The problems with admitting such beliefs to our storehouse of beliefs often lie not in that they contradict themselves or that they are entirely inconsistent with what we observe. Rather the problems often lie either: 

Try A Thing of This World: A History of Continental Anti-Realism (2007). Here is the Worldcat Link. It follows the tradition after Kant from Hegel through Derrida, and argues that it has involved a debate between realism and anti-realism paralleling that among analytic philosophers, but in different terms. Also, while I'm at it, you might look at several reviews by Ian Hacking (Canadian Philosopher of Science) of Michel Foucault. 

Perhaps more a physics question, but there is also a philosophical question why physics validate all measurements as the same. Of course one could ask why should one particular measurement be the only 'real' one. Of course this is all about special relativity. But a bit in line of Ockhams'razor one could also suggest that the measurement with the less variable parameters should be the most right one. But if one measure the length of something with a ruler just laying it besides it, it should be a more accurate measurement of the length of something than of the measurement of a observer in motion relative to that length. But in physics both measurement are equal in validity according to SR. Only light is absolute. So the question is, isn't a measurement with less variable parameters not more accurate than one that is depending on the observers'conditions? 

There is time as an interval between two events but you could also state that time itself has a certain duration (it passes). Now in physics it is not always very common to see time as something that has a duration. ' So to make that statement solid one should find a way to work it out that it is an illusion but how can you do that? 

Robots can become very intelligent. So suppose in future they are there and looks and behave a bit like humans. What question could be asked by which you can decide whether it is a robot or not? 

Some people like to live in the moment, or in the now. That is possible a more psychological way of interpreting a moment. But physically seen, does this contain a time lap? So is it actually possible to live in the now? Or is the Planck time (5.39 × 10 −44 s) the time lap which defines 'now'? Or when it is 17:00 h, does that moment has a time lap? Perhaps mathematically it could have a now, because 17:00 seems to be very exact. But in practice, regarding special relativity, a clock can't indicate the time precisely 17:00 because that would also imply that there is an infinite number of decimals which due to all motions can't stay zero. Or can they? 

There is a huge literature by contemporary Kantians, nearly all of whom reject specific parts of Kant's program. I think Philippa Foot's "Morality as a System of Hypothetical Imperatives" offers a nice revised understanding of how one might orient Ethics around the Categorical Imperatives Kant describes. But it rejects core ideas of Kant's about categorical imperatives, like that they all have the same meaning and that they have the force of necessity Kant seemingly thought they have (Kant's thinking about this being a matter of some debate). 

Sure, Matthew Kieran has written a recent paper in Philosophical Quarterly based on the problem that it is difficult to know whether one is a snob or not, and how that difficulty might pose a problem for aesthetic justification. The paper is The vice of snobbery: Aesthetic knowledge, justification and virtue in art appreciation. That's the one recent paper I know of. To search for other papers on any topic you might search the PhilPapers database. 

If it is the last of these you are interested in, I recommend starting with Thomas Nagel's essay “The Absurd” in the Journal of Philosophy 68: 716–27, and reprinted in his book The View from Nowhere. You can probably find it also online. Similarly introductory is Simon Blackburn's excellent little book Being Good, in which Chapter 10 and other chapters discuss the meaningfulness or meaninglessness of life, and what kind of reasons there might be that life is not meaningless. For a classic discussion of meaninglessness which is less straightforward to read and more tied to concerns about the supernatural, see Albert Camus's The Myth of Sisyphus. 

I will clear up the context of this question so that the potential answers can be effectively targeted towards what I'm looking for. Information (at least in the non-semantic, canonical sense of the word) can be defined using either the Shannon statistical approach or the Kolmogorov computational one. The former establishes it as a notion of average codeword length expressed in some arbitrary alphabet -which is deliberately chosen to be the binary alphabet in any textbook I can cite- needed to establish a bijective mapping to the outcomes of a random source; the Source Coding Theorem proves this definition to be equal to the entropy of the source in the statistical limit of an infinite number of outcomes. The second approach makes use of universal computers in some arbitrary model (almost always chosen to be an UTM), and then defines the information contained in a source as the length of the minimal self-delimited program in the language of the computer which outputs the outcomes of the source; as the choice of a reference UTM can only make the length of a minimal program a constant term different than in any other choice, the definition is quite robust. Indeed, the Kolmogorov approach subsumes the Shannon definition, because the probability distribution priors needed to encode the source and decode the messages are equivalent to a Turing Machine which computes an entropy encoding scheme (like Huffman or Arithmetic encodings). So, in essence, the modern definition of "information" is in turn based on the extremely flexible and unique (i.e. absolute) definition of computation brought about by the universality results of the first half of the twentieth century. Nevertheless, I find this story somewhat unsatisfactory. Using computation as a foundation for information seems like turning the world on its head, at least under an intuitive understanding of what the term denotes and in opposition to the canonical definitions of Shannon and Kolmogorov. Computers are procedures (or procedure-following objects, a meaningless difference under the light of Turing universality and mutual simulation of machines) and so, they are a particular kind of relation -recursion- between an input and an output. Even though this kind of relation is unique and absolute, it seems to me to be necessarily ex post to the notion of input and output; and those two are the "stuff" that intuitively matches the term "information". You can say a computation exists by showing a history of the steps (the stacked list of inputs-outputs), the same way you can explicitly show a logical deduction in sequent notation. The Church-Turing thesis then would just state that the class of stackings of inputs and outputs which can effectively be produced by any means, is exhausted by the Turing Machine or by any other equivalent model of computation. On the other hand, the "stuff" which makes up the inputs and outputs (intuitive information) in principle doesn't necessarily need an auxiliary reference to computation to be said to exist. A string of symbols doesn't need to be related to any other one; it can exist in isolation, so no compulsory reference to a procedure is required (again, I'm leaving the formalization of this intuition as an exercise to the reader; I just expect to be talking with enough sense to be understood). To end this section I leave this resume: 

With levels I mean different 'perspectives' on a subject/happening. For example, all things have color. When we see a green tree we say this tree is green. So from this level the tree is green. But for a physicist the tree has no color it only appears as a color but in fact they are waves reflected by the tree are 'mentioned' green. So for a phycisist only waves 'are'. Now there is a mathematical physicist and says, no that are no waves that are fields and vectors and a string-theorist says that the tree is a bundle of strings. And perhaps even a synesthesist would say the tree is color 6. Perhaps you could go on and on. The question is now whether these 'beings' are valid or not. Can 'beings' who are not really contradictory be all true in their own field/level or -is- there a real being what prohibit the other beings? See also min 34 $URL$ 

Because God wants us to be totally involved. So human have free will, can talk, behave and act; God wants our cooperation, because for a relation two persons are needed. Nevertheless God can give us something that we did't ask for, but this isn't always the case. 

Socrates was finally sentenced to death because his judges declared that he spoiled youth by his teachings and that he learned other Gods. But what precisely did he teach? 

According to his theory there is only one substance and that is God/Nature. But if God is only good can there be any evil or bad behavior etc.? 

I think it is a kind of idealism like that of Berkeley. But can this statement also belong to phenomenology of Husserl or of the philosophy of Kant or is there perhaps a better philosopher for this? 

Assuming that information is the fundamental stuff required for the concept of computation to make sense, one may wonder what is the minimal alphabet (i.e. the minimal amount of different symbols) which is needed to express any form of information in the sense I explained above. The obvious response might be that the binary alphabet is the answer to this question -after all, the bit is used as a universal measure of channel capacity. But it turns out, I haven't found in any textbook I've read a section dedicated to stating and formally proving this, let alone to elegantly and succinctly doing it! Again, it may sound obvius, but if the foundations of science are information and computation (a trend which has been going on for almost a century by now) I seriously think it should be established that the binary digit is the "most simple and fundamental possible unit of (generalized) stuff". I will now state my question properly: 

You already have the truth-table corresponding to the conjunction connective, so it should be transparent for you to recognize the answer to your question from the syntactic point of view of the calculus. Asserting ¬(P.Q) [Premise 1] is logically equivalent to assigning a binary truth-value of zero to (P.Q). So, looking at the table, you see there are three possible assignments to the truth-values of P and Q that yield that result. Then, by asserting ¬(Q) [Premise 2] and using the same argument, you assign a truth-value of zero to Q; looking again at the truth table, this restricts the possibilities to two cases (namely, one in which P's truth-value is zero, and another in which it is one). So, the assertions do not imply a definite binary value for P's truth state. From the semantical point of view, the interpretation of the conjunction connective in this case follows (informally) this way. You know that either P is false while Q is true, or Q is false while P is true, or P and Q are both false; and you also know that Q is false. This last statements eliminates the first possibility (Q being true while P was false), and you are left with the other two; so, what you seem to think to be justified (the second possibility, that Q being false then P must be true) excludes the third situation when both P and Q are false, and so is fallacious. 

For a lot of feelings it looks clear to me that a thought preceded it. Like when I think of the possibility of war I get afraid. But imagine the case when it is very cold, is the feeling of having cold preceded by the thought that it is cold or could you feel cold without thinking about it? 

Descartes teached that because he thinks therefor he is. But did he tell something about the question whether there are gaps between thoughts? So probably we could not exist between those gaps? Or wasn't it possible not to think or was thinking just a quality a human posesses? 

Hegel explains in his works how the Geist is evolving during history towards a kind of consciousfreedom. It seems to me initially that in the end our own minds has evolved to a rational God, but I'm not sure of this. But in what religion is the interpretation of a God similar to the definition of Geist according to Hegel? 

In the early days, atoms existed according to Democritos. I think Democrites just hypothesized this to build up his world view. Probably there would have been also philosophers who claimed that everything was divisible to infinity (was perhaps Aristotle such a person?) But when asking in nowadays physics it is said that elementary particles can't be divided anymore, because the just decay in certain given other particles. But is this view of nowadays physics the truth? There are muons electrons quarks and neutrino's which can't be divided anymore. But why should this be true? Why can't a particle be split into infinity? Ok, it is probably hard to do so, but is there a more profound 'law' which does state this view?