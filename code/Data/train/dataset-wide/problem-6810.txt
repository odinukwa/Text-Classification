Last year I watched on TV here (England) a Flemish drama series sub-titled in English - I was amazed how many French words were used in conversational Flemish - the characters even said 'oui' for 'yes'. 

Have you read this article by Kortlandt? $URL$ In it he explains how he understands the development of I-E stops from Indo-Uralic stops on the basis of stress patterns. 

I don't know about American English but in standard English there is a minimal pair: 'Bacup' (a town in Lancashire) /beikəp/ and 'Bake up' /beik ʌp/, however the stress issue may still apply. 

Possibly but we're delving here into the language that went before proto-Indo-European. The alternations are ones found in other languages. For example the l/n alternation is found in Ancient Egyptian and the k/gh is reminiscent (although different) to Verner's alternations in Germanic, so the theory is not far-fetched. There may be the relic of the l/n alternation in Germanic in the word for 'sun' (English 'sun' but Gothic 'sauil'), which indicates there may have still been an l/n grammatical alternation in proto-germanic. Given the number of pairs found by Bizzocchi it looks likely and I'm sure that many theories can be built around this evidence. 

ps sorry I haven't really answered your question - the best resource might be a standard English dictionary such as OED or Chambers (but note the online versions are often slanted towards Americans). John Wells (who taught me phonetics at UCL) is quite right - golf is in the 'cloth' set and 'doll' and 'solve' are in the 'cot' set (in England). Interestingly 'gas' is in the 'cat' set not the 'staff' set. The development is very idiosyncratic owing to words being borrowed at differing times after the change and some 'such' as 'golf' being used as social markers. It was probably originally a phonetic change lengthening vowels before fricatives in Southern English which then became phonemic after the loss of preconsonantal 'r'. In understanding a word like 'golf' you should be aware that in many southern accents the 'l' in the word is a vowel. 

It depends if you're ready to believe it means 'two hands' in which case the 't' is the last letter of the root of *komtos > PG *xamðaz > NE 'hand'. 

It's worth looking at Krzysztof Tomasz Witczak's article "A New Look at the Etymology of Germanic *lambaz". This can be found on www.academia.edu . He believes the word originally referred to a species of wild sheep, probably the mouflon, and probably the young of it. In areas where the wild sheep was wiped out (most areas) the name was transferred to another animal, either wild such as the deer, or domestic, such as the farmed sheep. 

From memory (from reading some book in the past - i'm not that old) I believe it was borrowed from Scythian after Grimm's law had happened and that it was the Scythian word for their royal roads. 

I'm actually a grad student on the team at Stanford that's been developing Stanford Dependencies (on which Universal Dependencies is based), so I think I can clarify things for you. You're assuming that SD and UD are theories, which they are not. They're frameworks for annotating strings of words, and they never claim to be predictive in any way. The goal of generative syntactic theory is to define formally the set of acceptable sentences in a language; that is, you feed the grammar a string, and if it can come up with an acceptable constituency structure for it then the grammar returns "true", and if not, it returns "false". That's it--it makes no claims about psychological plausibility or computational feasibility. SD (and UD) has a different goal--it doesn't try to make claims about psychological plausibility or about grammaticality, but it does try to be useful for computational research. Comparing SD to something like Minimalism is like comparing pens to pencils--it may look like they do the same thing, but there are fundamental differences between them that often render one totally unsuitable to the task at hand. Our framework actually does draw very heavily from theoretical linguistics, just not from Chomskyan linguistics, looking to LFG for answers instead. In LFG, constituent structure involves relations between tree nodes, like "specifier", "head", and "complement". But these trees don't get mapped directly to semantics--they first get mapped to functional structure, an intermediate level that involves grammatical function relations like "subject", "object", and "clausal complement" (compare these to the UD relations "nsubj", "dobj", and "ccomp"). This bipartite system of syntax makes tons of things easier from a theoretical perspective (Warlpiri is the go-to example). You're mistakenly assuming our framework is a representation of c-structure--where you're certainly right that empirically, functional material tends to be heads--when in fact it aims to be closer to f-structure--where LFG prefers to analyze lexical material as heads. I should also mention that we at Stanford have a parser that automatically produces UD-style dependency trees, and the way it does this is by first parsing the sentences into tree structures, and then running lots of complicated tregex expressions over those trees to pull out grammatical functions; this is highly reminiscent of how LFG operates. So it actually does make a lot of sense from a theoretical perspective, as long as you see it through the right theory. SD and UD have really been designed for NLP. NLP people don't care about whether the tools they're using are theoretically well-motivated--they care about whether their model does better than everyone else's, and our dependency representations help them do that. Relation extraction is one example of a computational task that's easy with SD and difficult with a traditional tree grammar. At first glance it seems simple--all you have to do is pull out triples of "predicate(agent,patient)" from a collection of sentences. But, in a tree structure, this gets very messy, because the agent can occur in a wide variety of different tree structure positions. In SD, we abstract away from a lot of this extra noise, which makes the job more manageable for our users (and we even have special relations for passive sentences to make this task even easier). So, to summarize: SD/USD doesn't make any claims about the existence of phrase structure categories (such as nonfinite VPs or PPs) because a) it's not meant to be theoretically predictive and b) it's meant to represent functional structure, not constituent structure. SD/USD is taken seriously in computational circles because it is easier to apply to practical applications, such as relation extraction, than theoretically motivated constituency structure trees. The USD approach to function words is not a matter of theoretical debate in computational circles, but it is sometimes noted when the choice of hierarchical vs flat structure affects performance. Our SD/USD parser actually does have the option to produce hierarchical structure if the user so desires, where copulas, auxiliaries, and prepositions are heads. But I don't know of any schemes where hierarchical structure is hard-coded in. I hope that answers the questions you had. 

Although all sub-fields in linguistics, including typology, might give you some insight into the languages that you learn, I think that the study of second language acquisition might be the most useful to you for obvious reasons. Wikipedia articles usually have external links to more in-depth resources, so you might start here: $URL$ 

From what I've read ($URL$ applicative voice occurs when an oblique noun phrase becomes an argument of the verb when the verb takes some applicative morpheme. As the article points out, something like this happens in English when a sentence like "Jack ran faster than the giant" becomes "Jack outran the giant." But in the many other languages that mark verbs with applicative morphemes, applicative voice is much more productive. The article states that "A language may have multiple applicatives, each corresponding to such different roles as comitative, locative, instrumental, and benefactive." So my question is, do applicative verbs marked for such roles ever govern different cases? For example, is there a language in which a verb marked with a benefactive applicative morpheme must have an argument or "object" in benefactive case? 

e.g. "The man whom we confined has escaped. But do any natural languages mark the noun as being modified by a clause subsequent to the one in which the noun occurs and having the form of an independent clause, like so... e.g. The man(suffix indicates that the next clause modifies this noun) has escaped; we had confined him. 

In natural languages that morphologically mark animacy vs. inanimacy on a largely semantic basis, (e.g. "hamster" is animate, "stone" is inanimate), which of the two noun-classes do terms for the following categories typically fall into? a) terms for dead organisms, and b) terms for parts of organisms that don't live independently of the whole, e.g. "fingers"? 

Sense relations are apparently semantic relationships between words and/or predicates. We learn more about this at Glottopedia.org. "A sense relation is a paradigmatic relation between words or predicates. Two major types of sense relations can be distinguished: Sense relations of inclusion, esp. hyponymy and synonymy Sense relations of exclusion, esp. complementarity and antonymy (both of which are instances of the relationship of incompatibility)." Is there a term for sets of words that have the same sense relation between them? For instance, how would one complete this paragraph? "'up' and 'down' are antonyms. 'animal' is a hypernym of 'cow' 'pal' is a synonym of 'friend.' Each pair of words just mentioned is a set of (term I'm looking for)." 

Suppose that you have a language, let's say it's SVO, has a clause pattern in which the subject typically stands for an agent or experiencer and the object typically stands for a patient or stimulus, and in which neither NP is overtly marked for case. Now suppose that the language has a different clause pattern in which the first argument in the clause takes an overtly marked case, let's call it "case-B," and the subject typically stands for a patient or stimulus while the object, which has no overt case-marking, typically stands for an agent or experiencer. This second clause pattern is like the passive, except that there's no valency reduction, and the second argument must be an indefinite pronoun if its referent is to be unspecified. My question is, is there a natural language with such a scheme?