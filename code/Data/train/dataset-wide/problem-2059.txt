Because this is AdventureWorks and obviously not a production server, you're probably not going to get great answers here. I'd just restart the server. (Is that the right answer? No, but we're obviously talking about a development playground box here, and I wanna get you across the finish line quickly.) If you really wanted to troubleshoot it, try sp_WhoIsActive: 

DBCC CHECKDB isn't a good storage test. It does logical tests too, not just reads from disk - for example, it compares data between multiple indexes on the same table to make sure they all have the same values. These checks consume CPU cycles. If you want a better pure storage test, consider setting an artificially low buffer pool number and running multiple simultaneous SELECT COUNT(*) queries across multiple large tables with no nonclustered indexes. 

2. Remove unused indexes. Use sp_BlitzIndex in the First Responder Kit (disclaimer: I'm one of the authors) to remove nonclustered indexes that aren't being used. Bonus points for removing duplicate and near-duplicate indexes. 3. Check the server's bottlenecks. From the First Responder Kit again, run sp_BlitzFirst @SinceStartup = 1 to get your wait stats since startup. If the top wait is PAGEIOLATCH, that means reading data pages from the data file. In that case, try maxing out your memory. 2008R2 Standard Edition takes 64GB just for the buffer pool, and can use more memory than that for other purposes. Make sure your server has at least 96GB RAM. (I'm not saying it's always a good idea to throw hardware at the problem, but my desktop has 64GB of RAM, and the rest of these changes are going to be more and more disruptive.) If your top wait stat is anything other than PAGEIOLATCH, then focus on the top wait type instead, because reducing the database size may not help performance. 4. Implement desperately needed indexes. When your database won't fit in RAM, the index designs become much more important. You'll see the list of desperately needed missing indexes from sp_BlitzIndex in the prior step, but also consider running sp_BlitzCache @SortOrder = 'reads' (again, from our First Responder Kit) to identify the queries doing the most logical reads. Often, their plans will have missing indexes right there, and again, you can implement those to reduce the amount of data you need to read from disk. 5. Double-check storage speeds. In an earlier step, I recommended running sp_BlitzFirst @SinceStartup = 1. Now that you've pruned indexes, tuned queries, and right-sized RAM, it's time to circle back to that same command again - only this time, instead of looking at your wait stats, look at the PHYSICAL READS section. That identifies which data files you've been reading from, and what their average latency is. If your average read latency is over, say, 100ms, it's time to start asking questions about storage performance. 

First, inserting data into a table variable is single-threaded (not parallel). You can learn more about that in Paul White's epic post Fording a Parallel Execution Plan. Second, when you're running multiple queries, you can't predict which queries will end up on which cores. An incoming query gets assigned to a worker, which is pinned to a scheduler, which runs on a CPU core. SQL Server doesn't know in advance how complex the query (or queries) will be, so it can't predictively load balance. If you truly want to get the CPU to 100%, you'll want to run a lot more than 3 queries, or you'll want to check DMVs to abort your queries if they end up on the same scheduler. Or do what I would do - stop using table variables and switch to a temp table instead. 

If this results from you passing data INTO SQL Server, log your parameters before you issue the query. If this results from you getting data OUT of SQL Server, remove your cast/convert functions and look at the data. Odds are you're casting/converting a field that isn't guaranteed to be a date. 

Your two queries likely have different execution plans. There's some hard-coding in the engine that says the top 100 will perform differently, but you might have stumbled into a problem like a spill into TempDB. To find out for sure and get custom advice based on the queries you're facing, you can post the execution plans for download. You can use SQL Sentry Plan Explorer to anonymize them if necessary. Post the actual plans, not the estimated plans. 

Because in the real world, tables have lots of columns. That means you're going to have to generate a lot of complex dynamic app logic to build dynamic strings, OR you're going to have to specify every field's before-and-after contents, every time. If you build these update statements dynamically for every table, only passing in the fields that are being updated, you can quickly run into a plan cache pollution problem similar to the NHibernate parameter sizes problem from a few years back. Even worse, if you build the update statements in SQL Server (like in stored procedures), then you'll burn precious CPU cycles because SQL Server isn't terribly efficient at concatenating strings together at scale. Because of those complexities, it doesn't usually make sense to do this kind of row-by-row, field-by-field comparison as you're doing the updates. Think set-based operations instead. 

You've got a bunch of different questions in here, so let's break 'em out individually. Q: If I join two tables in the same database with the above query, why is it slow? A: For starters, you're not using a WHERE clause, so SQL Server has to build the complete result set, merging both tables together. If you only need a subset of the data, consider using a WHERE clause to just get the data you need. Once you've done that, note that you're using a LEFT OUTER JOIN. This tells SQL Server, "Not all of the table1 records will have matching records in table2." That's totally fine if it's true - but if you know all t1 records will have at least one t2 record, use an INNER JOIN instead. Next, indexing starts to come into play - depending on the width of the tables and the numbers of fields, you may want to add indexes on the fields you're using for the join. To get good advice on that, it's best to post the actual execution plan you're working with. Q: If I the tables are in different databases on the same SQL Server, does that change anything? A: No. There's some interesting gotchas around things like default isolation levels in different databases, but for the most part, your queries should produce the same execution plans and speeds. Q: Should I use table partitioning to make this go faster? A: You mentioned database partitioning, but there's no such thing in SQL Server - I'm guessing you meant table partitioning. Generally speaking, no, I wouldn't jump to database design changes in order to make a join go faster. Start with the basics - understanding SQL Server execution plans - and only make database design changes to solve problems that you can't fix with things like indexes. 

In SQL Server 2005 and prior, you could specify the filegroup for full text. In SQL Server 2008 and newer, full text is completely different, and the ON FILEGROUP stuff doesn't matter. However, Microsoft doesn't just rip out syntax - they deprecate it, and a few versions later, it'll disappear. (Backup log with truncate_only is a good example of this.) This gives you time to clean up your scripts before the syntax disappears. 

Having done this type of project before, here's some of the practical gotchas to look out for: "2. This master then does one-way slave propagation to up to 200 read-only servers via VPN." Think carefully about the network traffic required here. In the simplest terms, if the insert/update/delete load on the database takes up 1/200th of a network cable's throughput, and if your master server only has one network connection, it's going to be saturated. (In practical terms, it'd be saturated long before that since you also need to accommodate query loads and backups.) Next, think about the logging requirements. Every transaction has to be logged, and the logged changes get sent to the read-only servers as they happen. If any one read-only server drops offline, then the master has to retain all of the logs until the read-only server comes back online and consumes the changes. In a high-volume change environment like you're describing, this can easily be a higher size than the original data set itself (since some records can get updated over and over, or get deleted.) You need the ability to recognize when a replica has gotten farther behind than it can easily catch up, and then the ability to reinitialize that replica from backup/restore rather than replication synchronization. Since you want to be able to fail over from one master to another, then every master would need to be aware of the replication synchronization state of every read-only replica. With these limitations, you can see why you usually want to separate the work of distribution out onto other servers. You want masters handling writes, but then just batching off the changes to a set of distributor servers that are responsible for tracking replication synchronization states across many read-only servers. (This is how Microsoft SQL Server's transactional replication works, and I would imagine most other DBMS's have similar distributor architectures available as well.) 

I hesitate to even say that last one, because the 32-bit issue is so bad, and it's really hard on the older versions of SQL Server. If you were on a current one, you could go through the plan cache and sort queries by memory grant, find the biggest grant recipients, and tune those. Not an option on this old antique, though. 

Things in the select are returned only if there are rows returned in the FROM statement. First, let's think of it conceptually. Query 1 is like: 

Yes, but only manually - Azure Blob Storage isn't integrated into the log shipping wizards. You would need to do your own backups to Azure Blob Storage, and then have the secondary server(s) restore from ABS using the same certificates. When other shops have done similar projects with AWS's S3 storage (writing their SQL Server backups there using 3rd party tools), I've seen them either log backups to a text file that the other SQL Servers check for backup files, or log entries into a centrally accessible table (like SQL Azure or Amazon RDS) and fetch the backup file names from there. 

Putting the comments into an answer so the question can be marked answered: @crummel4 says, "See the third section from the bottom of $URL$ titled How "initialize with backup" Works, and How to avoid Pitfalls" @Kin says, "For step by step instructions, please refer to $URL$ or $URL$ 

We've got a ton of learning resources on that Aggressive Indexes page (I just updated it with more, coincidentally) and I can't even begin to do justice to it overall. Having said that, here goes. Imagine that you've only got a clustered index on a table. Inserts will be pretty doggone fast. However, when you try to update a row in that table, and you don't specify the clustering key in your where clause, then you'll end up with a lot of table locks (aggression). In that case, the fix is to add indexes for the fields you frequently query on. On the other hand, imagine that you've got dozens of indexes. Whenever you want to insert or delete rows, you're going to need locks all over the place to get your work done. The high number of indexes will slow you down because nobody can get their insert or delete done quickly - and again, you'll see aggressive index warnings. In that case, the fix is to prune down the number of indexes to a more managable number. Generally, for transactional tables (as opposed to overnight data warehouse loads), I tell people to aim for: 

You can also right-click on the database in SSMS and click Properties, then look at the files from there. 

The easiest way is to buy an off-the-shelf monitoring tool. They all give you this kind of information - Idera SQL DM, Quest Spotlight, SentryOne SQL Sentry do these kinds of things with really low impact. The next easiest way is to build something yourself. If you're going that route, I'd start by logging sp_WhoIsActive to a table - especially with the @get_transaction_info = 1 switch. If you try the roll-your-own approach, you need to be aware that queries aren't the only thing that will cause the transaction log to grow. For example, if you're using replication, database mirroring, or Always On Availability Groups, SQL Server needs to retain history when one of those replicas is offline. To learn more about what's causing that, check log_reuse_wait_desc: 

Can be anywhere from 5 seconds to 15 minutes depending on your server, workloads, databases, etc. If you have thousands of databases, for example, you can check out the documentation and use switches like @CheckUserDatabaseObjects = 0 to go faster by skipping stuff. 

As discussed the last time you asked this question, your top wait is ASYNC_NETWORK_IO. SQL Server is sitting around waiting for the machine on the other end of the pipe to digest the next row of query results. I got this info from the waits stats results of sp_Blitz (thanks for pasting that in): 

And you'll capture a 30-second sample of your server's waits during the test. Look at the wait stats section to identify what your server's waiting on - and feel free to post a picture of that portion of the results to get additional help. The bottleneck may not be storage or CPU. Wait stats are the key to finding it. 

AWS Aurora's replication is more akin to Always On Availability Group. The primary pushes storage changes to other replicas. You don't get to make schema changes on the subscriber. More details are available in several re:Invent deep dive sessions.