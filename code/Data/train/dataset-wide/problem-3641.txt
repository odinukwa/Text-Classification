If you are using Apache for the webserver you can do a mod rewrite rule to forward requests on ts.v...com to the ts4.game....com server and include port numbers. Is the service http based or is it a program directly cOnnecting? 

Try that. You'll need to put it in each article folder in a .htaccess file, since each article folder has a different URL. 

Is it not like apache and just pick the default (first) configuration? If you don't want people to forward their domain to your server, make two or more virtual hosts, the first one pointing to an error page, and then any request for a domain not hosted by you will return an error. 

This is not a good idea. Your users wouldn't be able to trust you, in the sense that you can ADD CODE TO THEIR SITES. You could theoretically add keylogger scripts or something to the site, and so if you're going to be doing this, make it like a codeen server, where all new users are greeted with a welcome message. 

In a business sense, and if there a lot of users, first initial then last name. Not dots. jkerry or gbush or bobama or such. 

If your PDF files are mainly text based (few to no images) and your server has a lot of traffic accessing the same document and can handle the compressing , then yes, mod_deflate or mod_gzip is a good idea. If you're unsure, try it out first, and if the performance goes down, just undo it. I've been serving all content except images and video with mod_deflate for over a year now, and it's cut my bandwidth to under half (Lots of text documents and scripts). Also consider looking into a cache system if not already, as this will really reduce the workload on the server. 

In windows, I think if "domain.com" is mapped to localhost, it is independent from sub.domain.com. You need to specify every single subdomain in the hosts file for it to work. 

You are using SMTP right? Sending mail through a program using gmail servers should show any specified FROM address (easy to fake) but also the address of the account you used. Agreed, please post the "original" message with headers. 

I think you're better off running it under $URL$ emulation. If you've already tried compiling it for native running, then at least try this. It's less prone to cause errors with Squid. 

If you want it on a usb, try making a .img of it and using a free program online to burn it to a USB drive. ISO files don't like booting from USB. 

Mod rewrite. There are so many different configurations that I can't post any specific ones here. But you could use dreamweaver to update all links. Are you trying to get /test/app/images/stuff? What directory are the images in? And it is a good thing you are in relative path mode. That saves much time. Try putting the images folder inside test folder. 

The error log can display notifications as well. Does the intended task actually work as you want it to, even with this message? For example, Apache says stuff like "Apache started on port 80" and "starting child threads" and such. Not a big deal, since it's only notifying you. 

You should put the directory tag inside the vHost tag. Make sure your server will support execution of these scripts too. ALSO: You should just do "directory as /" as noted above. Otherwise you'd have to go to joebloggstest1.couk/joebloggstest.co.uk. 

Nameservers are set within the network adapter itself. 2 computers on the network can have 2 different name servers. One could even use the other as a name server. To point a domain to an IP, you need to A) have bought a domain name and be able to set name servers within, if you are hosting the server on the domain name its pointing to; or B) use a custom DNS service like "$URL$ and set up A-records to point the domain to an IP. Email me "Support/@/u4ik.us" for free DNS service. I run a few DNS servers. 

You can try this, just add it to the index page if it's PHP, or better yet, pop it in the "Header.html" file as a PHP function. Every time it loads it will give you the user's IP, referrer, user agent, and page viewed. Crudely, save this where it needs to go, for one hour, then cut and paste the file it generates (access-log-php.html) to a place where it won't be edited. You can look inside and count how many hits. Alternatively, set up a PHP SQL function where it increments by one, and stores to a table on a database, for one hour. PHP is the way to go, though. 

In fuzzyfox config, check you allow "ssl 3.0" and the likes. Otherwise post the "view source" of the page in firefox that doesn't load, and any error logs you have. Good luck. 

You let them get sent (just a few) then look at the email's "original text" if your server supports archiving mail. Look in the queue files if possible to see if you can actually read the mails before they get sent. I mean, depending on how important the mail server is, shut it down for a few minutes and note who connects to it when it comes online again. Even better, disallow use of it outside the domain. That will put an end to the issue for good. 

If centos 6 does not support legacy PHP, you are better off running v5 because if you have to mod the OS to trick it into running it, it could open up a security hole. Recommend you use the latest PHP and OS. If that's not possible, then make sure you have very tight security when using old PHP. That's why they update their software. I'm looking for a way to get this to work right for you. EDIT: I was beaten to it. Refer to the other poster's wget method. EDIT2: removed useless text. 

I've got a file called "stream.ogg" which is, well, an internet radio station. If a user tries to download the file, it always appears as "210mb" no matter what OS they try to save it from. I've got several other files for download, some are .zip, and some are .exe. When someone downloads a file, it says "unknown time remaining" or "unknown file size total" and I was wondering if I could get Apache to relay the total size to the client? Other servers on the net do have this set up somehow with Apache, for example when downloading a linux distro from any website, it always knows how big the file is. (ex. 19 minutes remaining). Can anyone help me with this please? Thanks. PS: Apache is the latest public release, PHP and scripts are enabled. 

It depends on the specs of the server and what type of content it serves. It may not be possible at all, (simultaneous right?) if it's serving active content and media. Database and small websites should do fine. In apache config, try making it start many worker/child threads. In the box itself, just test it to see if Apache will handle it. If apache can but not the box, maybe someone else can help with that. Sorry. Good luck. Yes, sorry for not reading it thoroughly. 

Did you install SQL secondhand, (with XAMPP or something?) And if not, and it was the official installer, make sure you GOT ALL OF THE files and not just program data. You really do need everything. I hope you still have the original files. 

Look under "Installed Programs" then "Show updates" and go side by side and compare, or print screen them and print it out to reference if they aren;t near eachother geographically. Here's a link to a program with really detailed information about the computers. This will help with everything you could want to know about a computer... LINK: $URL$ NOTE: The program takes a long time to scan everything because it is thorough (maybe 30 seconds). 

Well even if the site is in the trusted sites list, you need to either adjust the security setting: Tools>options>security settings> low/medium/high to allow scripts without warning. Click "custom" then look under the category of ActiveX controls. There's an option not to show warning for both previusly used and new scripts. That will resolve the "do you trust my script" error in IE. 

Gmail is tricky. Make sure you're using the right port. 990 I think. And for the username, it's your ENTIRE email address. 

You have to delete the entire "xampp/htdocs" folder (well, only what's inside it). Can you post your "error.log" file please? This is my configuration: 

Follow this pattern, and don't put an [OR] on the very last one. EDIT: New solution: If you want to block all (friendly) bots, make a file called "robots.txt" and put it in where your index.html is. Inside it, put this: