By all accounts this may be a bugged behavior in the sys.stats_columns DMV. This appears to be causing problems when a statistic is updated by way of the parent index. I believe this to be due to the mechanism with which the statistics are being updated in a constraint change. Should you create a statistic manually and then wish to change the columns you must first drop and re-create which forces the meta-data to be updated in the DMV in question. In the operation you have demonstrated there appears to be a situation where the metadata is not updated under any circumstances (DBCC *, CHECKPOINT, server restart, statistics update through parent index change, etc) once the change has been made. From my initial testing I can find only one case when the metadata is properly updated which is the drop and re-create scenario. You can take a look at the Connect item on the issue and up-vote as appropriate. There is a work around query posted there but its mechanism is based on matching the index name to the statistic name and utilizing the index meta-data. 

If the reports are always referring to the same index, then you might just have a corrupt nonclustered index (since the index ID is greater than 1). I'd definitely keep a close eye on hardware diagnostics, and also keep SQL Server up-to-date on patches to identify the root cause of it becoming corrupted. But you may be able to repair the problem by simply rebuilding the index: 

It sounds like a situation where you could potentially fall prey to - or benefit from - parameter sniffing. In a nutshell, SQL Server will use parameter/variable values when compiling an execution plan in order to determine optimal index usage (based on column statistics). Depending on the value that's passed in, you could get very different execution plans, and very poor performance if the "wrong" value is used later. (A nice article about it.). I'd personally calculate the date within the procedure using a scalar function, unless there's a need for whatever is calling this procedure to specify the date itself. Then you could always use the scalar function in the procedure to fall back on a default value. As long as parameter sniffing isn't giving you surprising performance changes, it's mostly just a matter of application design. 

From TechNet sys.dm_db_missing_index_details emphasis mine. Hopefully the above examples have provided some clarity around when and why you would have differences between the environments. 

Under the circumstances that you have indicated have you looked at VSS backups through a VSS provider that is either 3rd party or Microsoft based? You can perform a COPY_ONLY backup that will not break your production recovery chain and you should end up with a backup that of all of the databases that you can then recover elsewhere to within your reasonable margins. Keep in mind that a VSS backup has some of the same mechanisms and downfalls as database snapshots in that a very active database could cause a disk space issue due to the sparse files used. Take a look at the TechNet resources on the SQL Writer service here and VSS backups of SQL Server here. To do this through Windows Server Backup you will follow the wizard steps for a manual backup ensuring that you select VSS copy backup on the custom configuration settings under VSS Settings. This will allow your Windows Server backup to not interfere with any other backups taken on the server. See Windows Server Backup reference for details. 

SQL Server will default to restoring the files to their original physical locations. If you're restoring to a different server, or a different database name, these paths probably aren't correct. You will have to specify the correct new database name, and file locations (WITH MOVE...). If you're overwriting an existing database, you'll need to tell SQL Server that you specifically intend to do this (WITH REPLACE). This is a safety feature; don't get into the habit of blindly using this option unless you know you need it. 

For a few reasons, I'd like to have all permission denied errors (number 229) emailed to me. Checking master.sys.messages shows that is_event_logged for this error is 0. Thus that rules out using SQL Server Agent error alerts, which rely on the SQL Server error log (I've tested this - I get no alert). I figured I'd peek in mssqlsystemresource and look at the view definition for sys.messages, thinking maybe I could update is_event_logged for the message in question. But this view gets the system error message from , so that's a no-go. Is there a reasonably simple way I can get all 229 errors emailed to me immediately (or within maybe 30 seconds) without harming server performance? A 60-second cool-down between emails would probably be a good idea too. 

From a reliability standpoint of deadlocks per second the perfmon counter will be more accurate. The system_health monitor is based on ring buffers which can overflow over time or miss events if there is enough activity within a brief period in order to facilitate reliability. There is a file that backs this which you can look at which will give you more history but if you are experiencing as many deadlocks as you are indicating there is a distinct possibility that you may not have full history within these. This would explain your discrepancy between the two measurements, you aren't doing anything wrong from what I can see. The files will be located in your SQL Server install directory under %Program Files%\Microsoft SQL Server\MSSQL11.[Instance Name\MSSQLSERVER]\MSSQL\Log\system_health_*.xel. 

You could utilize a sequence object to facilitate the mechanism of generating the IDs if that is the sole purpose of the CommentParents table. Take a look at the TechNet documentation for the TechNet SQL Server 2014 Sequence Documentation for more detailed information. What this sequence object will do that your current table can't do is allow you to grab a value and assign it to a variable ahead of time without worrying about doing an INSERT/SELECT SCOPE_IDENTITY() process. The advantage here is at great scale the current design you have would break down due to the need to maintain metadata overhead where SQL Server has caching for Sequence objects. 

Assuming you're sending the messages from within a few stored procedures, I'd recommend checking the database name before sending the message, and skipping that step if it's not the production database. It usually looks something like this: 

As a point of clarification, you can bind forms/reports to stored procedures in Access. You do this by creating a pass-through query. The drawbacks are that this query will be read-only, and there's no simple way of passing parameter values to the procedure. You should be able to hard-code parameter values in the query text, though, and maybe even use VBA to rewrite the query text on the fly (never tried it, but you can do a lot of crazy things like that in Access). However, if you're just using this for reporting, these might not be problems. 

The simple way around this is to change some options in Management Studio. Go to Tools, Options, Designers, Table and Database Designers. Uncheck "Prevent saving changes that require table re-creation". Generally speaking, Management Studio will properly handle dependencies when you modify a table in a way that requires it to be recreated (it will do assorted tricks with temp tables behind the scenes, which you can see if you tell it to generate a change script instead of actually making the changes). However, this may involve modifications/changes to related tables or foreign keys. In other words, don't do this on a live system if you can help it, and make sure you've got backups before you do it. 

To answer the question of will you get the same results running an analysis such as Brent's sp_Blitz scripts in a non-production environment the answer will lie in what information you are trying to gather and what that information depends on. Take the two examples below as a means of showing the difference in results (or no difference) based on the type information sought. 1) Suppose I am interested in analyzing the indexes in the database to determine if I have duplicate indexes. To do this I would look at the database metadata which would be part of the database structure itself. In this case the results should be the same regardless of environment because the data that is being queried to draw the conclusion is part of the physical database structure. Brent's sp_IndexBlitz does this as one of the steps in its analysis, among others. 2) Suppose I am interested in analyzing an index to find out if the index is utilized. To do this I would examine the DMV (sys.dm_db_index_usage_stats) to determine if the index in question has any scans, seeks, lookups or updates. Using a combination of this data I could then determine if this index is making inserts run slower or is a benefit to select performance in a way that justifies its overhead. In this case though the data will be different in results between production and the non-production environment unless the exact same workload and configuration is running in both environments. Brent's sp_IndexBlitz also performs this same check and will provide the usage details based on the settings specified. To further clarify on the "why would data be different" which seems to be a sticking point here lets dig in to what DMVs are. A DMV is at high level just an abstraction layer that provides a view of the instrumentation that SQL Server has running. With this being said as mentioned by Tony when SQL Server restarts the data that is within these DMVs is not persisted. For this reason when you perform a backup and restore it is functionally equivalent to a server restart for the purposes of the discussion around DMVs. Once the database has been decoupled from the original host of this instrumentation data the data would be lost unless it was persisted elsewhere. This is mentioned in Microsoft's documentation as shown below: 

In this situation, it can be useful if you want to, say, compress historic data that doesn't change, while leaving the current data uncompressed to maximize insert/update performance. I've done this in a couple of places on our server. You also have the option of different backup schedules across the different filegroups. But if you're trying to improve overall performance, there will be much more effective things to try first. 

The most significant balancing act to consider is whether your need for real-time reporting outweighs the performance hit that both your OLAP and OLTP systems will take from using your OLTP data source directly as fact and dimension tables, and then bouncing ROLAP/HOLAP queries off of them. If the tables are all quite small, and the server isn't under heavy load already, then the penalty is probably negligible. If SSAS queries are going to kick off 500MB+ reads, then that's a problem. And unless you're doing some kind of high-frequency trading, you probably don't need to have your SSAS database that up to date. It seems like SSAS is more useful for big-picture kind of summaries, and if the past few hours of data aren't included yet, it's not going to make a big difference to those running the reports (ask around to be sure, obviously). We load our data warehouses and process the cubes each night, and that's typically plenty. We also have a few simple reports built against OLTP tables for viewing more targeted, up-to-the-minute data (stuff that's really more OLTP than OLAP in nature). 

Have you looked at the SQL Server Migration Assistant tool? This would probably assist you greatly in the migration as it maps source tables to destination tables despite possible naming irregularities. The tool is provided to my knowledge free of charge. $URL$ $URL$ 

Digging into the mechanics of this wait you have the log blocks being transmitted and hardened but recovery not completed on the remote servers. With this being the case and given that you added additional replicas it stands to reason that your HADR_SYNC_COMMIT may increase due to the increase in bandwidth requirements. In this case Aaron Bertrand is exactly correct in his comments on the question. Source: $URL$ Digging into the second part of your question about how this wait could be related to application slowdowns. This I believe is a causality issue. You are looking at your waits increasing and a recent user complaint and drawing the conclusion potentially incorrectly that the two have a relationship when this may not be the case at all. The fact that you added tempdb files and your application became more responsive to me indicates that you may have had some underlying contention issues that could have been exacerbated by the additional overhead of the implicit snapshot isolation level overhead when a database is in an availability group. This may have had little or nothing to do with your HADR_SYNC_COMMIT waits. If you wanted to test this you could utilize an extended event trace that looks at the hadr_db_commit_mgr_update_harden XEvent on your primary replica and get a baseline. Once you have your baseline you can then add your replicas back in one at a time and see how the trace changes. I would strongly encourage you to use a file that resides on a volume that does not contain any databases and set a rollover and maximum size. Please adjust the duration filter as needed to gather events that match up with your waits so that you can further troubleshoot and correlate this with any other teams that need to be involved.