There was a rare occasion where the logical filename of the database was truncated in SQL Server 2008 R2 You might find more information in the ERRORLOG of the server you are restoring to. Otherwise I would consider running the restore script outside of the scheduled task in Query Window to catch any other errors. 

These restrictions/limitations may have changed since the document was published. Other resources Brent Ozar wrote an article "SQL Server RDS" where he noted that... 

Your motherboard's disk controller (SATA interface) is having issues handling the large amount of data. 

There are various possibilities and I will try to answer them according to the numbered questions. Valid connection strings for the SSMS login box are: 1st instance 

It isn't just "the default behavior", it's the only behavior. Owners always have full control on their own objects (including grants and revokes on those objects). That's why separation of container schemas from accounts that login to the database is so key to database security. 

Moving even further forward (in the reference) the examined time (vertical axis) moves forward and reaches the Failure. 

Based on ypercubeâ„¢'s feedback and Aaron Bertrand's new feedback we have come to a mutual conclusion is the inverse of if the following conditions are met: where and 

You can search the command column of the sysjobsteps table for the path you have in your current jobs with the following statement: 

So if you read in the documentation that a parameter is only valid on server start, then you need a restart. Otherwise a reload should be sufficient. 

Select the last option View or change system parameters and click on Next > You are presented with the Configure System Parameters page. This page shows various mail settings. 

Start the program and you should be guided through the de-installation routine. If you don't see any option to de-install an instance, then you might not be running the correct program version, or you might not be on the server you are trying to de-install the SQL Server instance from. 

In the past, I have used ErWin from Computer Associates and PowerDesigner to move schema between RDBMSes. It's been many years since then, and I would evaluate their current feature set if I had to pick something today. 

Is there a particular reason that your PK is clustered? Many people do this because it defaults that way, or they think that PKs must be clustered. No so. Clustered indexes are usually best for range queries (like this one) or on the foreign key of a child table. An effect of a clustering index is that it bunches all of the data together because the data is stored on the leaf nodes of the cluster b tree. So, assuming that you are not asking for 'too wide' of a range, the optimizer will know exactly what part of the b tree contains the data and it won't have to find a row identifer and then hop over to where the data is (like it does when dealing with a NC index). What is 'too wide' of a range? A ridiculous example would be asking for 11 months of data from a table that only has a year's worth of records. Pulling one day of data should not be a problem, assuming that your statistics are up to date. (Though, the optimizer may get into trouble if you are looking for yesterday's data and you haven't updated stats for three days.) Since you are running a "SELECT *" query, the engine will need to return all of the columns in the table (even if someone adds a new one that your app doesn't need at that moment) so a covering index or an index with included columns won't help much, if at all. (If you are including every column from the table in an index, you are doing something wrong.) The optimizer will probably ignore those NC indexes. So, what to do? My suggestion would be to drop the NC index, change the clustered PK to nonclustered and create a clustered index on [DateEntered]. Simpler is better, until it is proved otherwise. 

So while an index seek is not expected, MySQL will still retrieve the data via the index rather than via a normal data row scan. 

Make the INITIAL extent size smaller Empty the recycle bin Add a datafile the tablespace to provide a larger contiguous chunk size. Reorganize the tables in the tablespace 

So you know have a configuration setting pointing you in the right direction. Caution Now the PostgreSQL instance could have manually been started with a different configuration file. Let's double-check with the command (beautiful isn't it?): 

This can be verified by querying the and tables and checking the column of the table. If the column contains a and the step finishes, then it will quit the job and report a failure (Quit the job reporting failure). Query Job History and Steps Here is a script for your convenience to query the job steps and history: 

When installing the Oracle Database Client 12c Release 2 software on a Windows client the user has the option to select an installation type. These are: 

You will receive a list of database users that have database privileges assigned to them. Note: These are the Database Users and not SQL Server Logins. Normal Behaviour When you create a database and assign a "user" to the database with certain privileges you are in fact doing the following (partly in the background): 

The PostgreSQL service will be normally running under the login, but lets go ahead and verify that. First we switch to the account: 

Keep your 3rd-party snapshots for disaster scenarios. They are consistent and bring the database back online fast. Have additional FULL, DIFF and LOG backups of your database according to your requirements. Store these backups in a safe place to ensure you can access them when you have to restore a database to either the FULL, DIFF, LOG or Point-in-Time. 

Possible Reasons TLOG Files Aren't Deleted Ola's Failsafe Mechanism If a DIFF backup is missing (for example the DIFF from WE and TH) then no TLOG backups will be deleted from disk up until the last DIFF or FULL backup. In this example you would have TLOG backup files up until the DIFF backup on TUesday at 6pm (1800). Solution 

Within reason (I mean "less than thousands", and you are no where near that) the number of databases should not affect the performance of the server. I have seen people claim to run thousands of databases on a single instance of SQL Server, and the performance of the user databases hasn't been a problem. (On the other hand, looking at those databases with SSMS is a problem, as is trying to manually manage all of them.) Very roughly speaking the performance of the server is governed by the amount of data that is accessed in those databases and how much memory the server has. Assuming that you have enough disk space to store both databases and that you never access the "test" database, the production database performance should be unaffected. If you "sometimes" access the test database, the performance of the server could be affected at those times. You can mitigate that by testing when the server isn't busy, like in the evenings or on weekends. All of the data in a database might not be regularly accessed. It is very common to have old records, "archives" and other sorts of stuff which remains in the production database by isn't regularly returned by queries on a day-to-day basis. In many databases, this "cold" data might be much larger than the amount of "hot" data that is regularly accessed. If the amount of hot data is very small, you might not notice any performance problems when using the "testing" database even when the production database is being used full-blast. To make things a bit more complex, poorly designed queries and database tables might be inefficient, and return data that really should be "cold", or cause table scans. In short, if the server can hold all of the hot data (from production and testing) in RAM, then you will probably be ok, or at least not much worse off than you are with only the production database in use. Back to your direct question: Will adding more load be a problem? The first thing to do would be to look at the performance of the server now. Do you have performance problems? Are those problems caused by a lack of RAM, a lack of disk I/O capacity or a lack of cpu power? There are many, many guides on the internet and StackExchange to help diagnose where your currently bottlenecks are. 

And now the , Scan is gone and it has to scan the entire table to get that value. Why do you do this SQL Server? 

But when I execute the same thing with the it does not return any results. Second case - If I replace the with and no also I dont get any results out. eg 

I have a transactional replication which was initially synced from backup. Now I need to add a new table which is really big so we have decided to backup and restore a fresh copy of the db to subscriber to re-intializing it. My question is, in this scenario should I be dropping the subscription, backup restore and then re-add the subscription? is that the correct way or is there any other way of going about it? Thanks 

I was trying to create the drive through Server manager, when I create the drive through Drive Management it worked fine. 

I am having a situation with linked server which I am not able to understand. So we have a linked server from a 2008R2 server to a 2014 server. The below sample query is executing from 2008R2 server and it works fine. 

But for the second query it looks like sql server can sniff the value with option(recompile). I thought SQL Server cannot sniff variables even if we use option recompile? Seek Predicates - Seek Keys1: Prefix: [DB].[dbo].[test].c1 = Scalar Operator((216)) 

and got around 1400MB/s I was also able to get comparable throughput using a T-SQL query as below and calculating the throughput from the number of reads and time. I got this from Glenn Berry SQL Course on PluralSight "Improving Storage Subsystem Performance ". 

So just trying to understand what the industry standard is around DBA's being able to see card number. As per PCI document 

Optimizing a Stored procedure is too broad of a topic. Firstly if you are getting a consistent run time from your stored procedure we may be able to rule our parameter sniffing issue I would suggest you start with querying the procedure cache to find out which query inside the stored procedure is taking up time and then start optimizing it Sample query 

Drivers for dBase (or Access or Excel) are not installed as part of the SQL Server install. It is likely that VS 2010 installed on your workstation is connecting through the old Jet drivers, which are installed on developer's machines. The problem with Jet is that it was never ported to 64 bit. I don't think that the old Visual FoxPro drivers were ported to 64 bit, either. Microsoft replaced Jet with "ACE", which is available in 32 and 64 bit packages. ACE drivers were first released with Office 2007 and supplant the older (and probably deprecated) Jet drivers. You can download the ACE 2010 drivers here. Since you are using a 64 bit server, you want the 64 bit drivers for linked servers. If you plan on running 32 bit packages on the server, you would need to install the 32 bit ACE as well. You might be able to find a similar package for 2013 by now. I have not used these more recent drivers, so I don't know if the the older formats (like dBase, Fox, etc) are still supported. After you install the drivers, they generally need additional configuration inside of SQL Server. IIRC, if you see errors that seem to be security-related, you need this additional configuration. In short: 

A small chunk should complete in a reasonable time. You may find that chunks of the same size can take wildly different times to complete. (Some might take a few seconds, others may take minutes.) Normally, until I get a good handle on how things actually behave, I will run a couple of these at a time. Once I have an understanding of how it behaves, I would wrap it up in a template script that I can reuse. As you know, SHRINKFILE can cause index fragmentation. You should plan for reindexing before letting the developers into the database. If you regularly move databases from prod to test, it should be possible to come up with a script that handles the shrinking and reindexing. I suggest you have a look at Ola Hallengren's reindexing stored procedure, which is very popular with DBAs.