It handles models that has the same rank, displaying more than 5 rows per device type just in case. In order to test it, create a table and fill it with data: 

I know by how these tables were populated that this result corresponds exactly to simple column binding, so that and can be omitted. The problem is, as far as I know, RDBMSs can operate only projections and row binding. Column binding isn't a typical operation. So the question is the following: is it possible to bind columns "as they are" without specifying any join criteria? For example, the following query, which is syntactically wrong, should explain what I mean: 

You should benefit from increasing to a resonable value. Being a test machine you can push hard on memory settings, so try the following: 

I know this topic is pretty complex and involves a lot of different factors. Let's say I have several similar queries running at the same time. These queries involves only read operation and several ordering and windowing. If I'm not wrong, with I can get the memory used by some operations like . I need to know how a single query impacts in order to calibrate (and kind of predict) the amount of queries I can run at the same time. Is it possible to get the maximum amount (not the total amount) of memory used by a single query? Should I get it from the output (which gives me only some memory usage infos) or there's a better way? I'm using Postgres 9.5 and 9.6 on different Unix-like environments like Red Hat 6.7, Ubuntu 16.0.4 and macOS Sierra. 

Anyone can help me to figure it out? Many thanks to all Pietro UPDATE I forgot to logout and then login after editing the bash profile. The suggestion made by Daniel Vérité was right. I just edited the env variable in in order to make it visible at a global level and not only from the interactive shell. I added the following line to : 

I have a software that generates transactions and executes them. I would like to asynchronously inspect about the transaction progress in term of the simple proportion of queries executed over total number of queries. I'm able to read both and generated by , so I was thinking that a solution could be printing to some custom messages like "Progress: 3/8" (or another custom text string). I've thought about executing a "logging query" which stores in a proper table the informations about query progress, but this table wouldn't be available until the transaction completes, making it unuseful for me to inspect transaction progress. At the moment I tried the following (suppose to have 3 queries in the transaction which do some stuff): 

I have also posted this question on my blog: $URL$ I’ve been working on a project over the past several months that will utilize Service Broker and AlwaysOn Availability Groups to meet some of the HA and DR goals of the company I work for (more info: ). Just recently, I was able to implement the full solution in my development lab and point an instance of our website at it. While we were working out some kinks in our database and website to get the two working well with my Service Broker Replication project, I began noticing some odd behavior in Service Broker when it’s used with AlwaysOn Availability Groups, and I wanted to blog about it in an attempt to see if anyone else has seen this issue and might have an idea how to address it. The Setup: I have a Hyper-V host running 6 Windows Server 2008 R2 VMs (BTDevSQLVM1-BTDevSQLVM6). The VMs are grouped into 2-node WSFCs with node and file share quorum. I’ve installed standalone SQL 2012 Developer Edition instances on each of the VMs, and created an Availability Group with a listener on each cluster (SBReplDistrib, SBRepl1, & SBRepl2). For the purpose of this blog post, I’ll be focusing on the communication between SBRepl1 and SBReplDistrib. The illustration below shows the Service Broker objects for each side of the conversation: (I'm new and can't post images yet, so please see my blog at the URL above for the image) The Service Broker endpoints and routes are setup per this MSDN article.The SBRepl_Receive route in MSDB is for the local server’s service (//SBReplDistrib/SBRepl on SBReplDistrib, and //SBRepl1/SBRepl on SBRepl1), and points to the local instance. The SBRepl_Send route on SBRepl1 maps service //SBReplDistrib/SBRepl to TCP://SBReplDistrib:4022, and the SBRepl_Send_SBRepl1 route on SBReplDistrib is a similar mapping for the service on SBRepl1. The Expected Behavior: My understanding of how Service Broker handles message sending and receiving is thus (This is pretty simplified. There is a lot more detail about this process in Klaus Aschenbrenner’s book “Pro SQL Server 2008 Service Broker”): 

PART2 - UPDATING In order to update you must specify values, so you have to know them a priori. This works perfectly, but probably there are other naive solutions: 

That time is spent by pgAdmin to pack and render data and is not the time spent by Postgres to complete the query execution. Why are you fetching 250.000 rows into pgAdmin? If you need to export the table to a plain-text file (like a CSV with header) you can execute this query: 

You can play with to obtain the merging concept you have in mind. Here's a link to the official documentation (PostgreSQL 9.4): UNION clause. I think that you would like to remove duplicates (if there are duplicate entries in both tables), so probably the is right for you. 

It's clear that the issue is related to . If you have control over the dump process, a possible solution is to do not execute the file from . To do so, you simply have to run the following two queries: 

What I get from a transaction like the one above using the previous bash command are three outputs: , and bash variable. I'll post some extract of them to make it clear what's their content. plog.log 

I'm currently running a OS X Lion Server system which ships with a built-in and not-upgradable PostgreSQL version. After years of usage I've finnaly decided to leave the built-in version and install an indipendent version. I disabled the built-in installation and downloaded the installer from EDB and followed the wizard. After many issues reguarding encoding and locales, I've finally managed how to setup a DB with no locale and UTF8 encoding. I issued the following command: 

The function is documented here: $URL$ ATTENTION Be aware that those two queries can give different results for periods that are exactly . The following returns : 

If I connect using pgAdminIII I get no problems. The command displays as the encoding used by pgAdminIII (the default installation gave me a encoding and that's why I run the command). The problem is that I'm not able to connect to PostgreSQL using . Whatever I pass to it, I get the following error: 

The initiator app creates a message (in this case, well formed XML) If there is an existing dialog conversation between the initiator service and the target service that is in the conversing status, the app can simply send the message on the existing conversation handle. Otherwise, the initiator app should begin a dialog conversation between the initiator service and the target service and send the message on that conversation handle. The message is placed in the sys.transmission_queue system table and Service Broker begins making attempts to deliver the message to the target service. Service Broker looks for an appropriate route and remote service binding and uses them to determine the address to connect to in order to deliver the message. Service Broker opens a connection to the target, authenticates, and delivers the message to the target service broker. The target Service Broker attempts to classify the message and determine what local service will handle the message (it uses route data in the msdb database for this). The target Service Broker delivers the message to the target service’s queue Once the message is successfully delivered to the target queue, the target Service Broker looks for route information back to the initiator and attempts to deliver an acknowledgement that the message was received. The initiator’s Service Broker receives the acknowledgement and uses routing information in MSDB to determine what local service the acknowledgement is for. Upon successful routing of the acknowledgement to the initiating service, the message is then removed from the sys.transmission_queue system table. If the initiator does not receive an acknowledgement that the message was received, it will periodically retry delivering the message to the target. If the target has already received the message, it will simply drop any additional delivery retries and send acknowledgements for them. 

The Odd Behavior: Step 11 is where I am seeing some very odd behavior with Service Broker and AlwaysOn. I see the message getting delivered to the target and processed successfully, and I also see the acknowledgement getting sent back to the initiator and received. However, the message remains in sys.transmission_queue as though no acknowledgement was received. To make things even more strange, Service Broker isn’t attempting to resend the message like I would expect it to if the acknowledgement wasn’t received. Instead, the message simply remain in the sys.transmission_queue, and as new messages are sent, they get delivered, acknowledged, and they too remain in the sys.transmission_queue. It seems to me like service broker is getting the acknowledgements and therefore stops trying to deliver the message, but doesn’t remove it from the sys.transmission_queue for some reason. The transmission_status for these messages remains blank, which should indicate that Service Broker hasn’t attempted to deliver them yet. I checked the retention setting on the service queue, and it is set to off, but that should only impact the service queue and not the sys.transmission_queue. I have also traced both sides of the conversation using SQL Profiler, and I am able to see the message getting sent and the acknowledgement being sent back to the initiator and getting received (see XML trace data at the end of this post). One odd thing did jump out at me in the traces though. I noticed that both sides seemed to be a bit confused about the TCP connections, because messages are sent from the IP address of the node itself while the service routes and the messages themselves point to the name/IP of the AG listener. This confusion appears to be causing each side to close the existing connection between the two services and create a new one in order to deliver a message or acknowledgement. I’m not sure if this is normal or not or if it has anything to do with why the acknowledgements aren’t being handled correctly, but it was the only thing I could see that could possibly explain the odd behavior. The Plea for Help: At this time, I don’t have a solution to this message retention issue other than to manually end the conversation with cleanup on both sides, and that’s not really something I want to do. If you have any ideas as to why this might be happening or what I can do about it, please leave me a comment and let me know. If there is any additional information that you would like me to provide about my setup or about the issue, please let me know in the comments as well. I will post a followup to this post if/when I find a solution to this issue. The Trace Data: Please see my blog post (the URL is at the beginning of the question). 

I hope to have fully understood your request. With this query you get a concatenation of tables (a UNION) with all records. 

It subtracts 12 months from and checks if the resulting date is greater or equal to . This query works too: 

where and are 50% and 75% of total RAM size respectively. should lay between and for your use case.Test it and give us a feedback. There are also two other moves you should make: 

As @a_horse_with_no_name said, without a plan made on the 100.000.000 rows it's difficult to give other advices. Try the previous query and tell us if it's faster (and correct too). 

This query will return the times a student retook an exam after more than 12 months from the first take: 

I've searched through the Internet but found nothing that solves my problem (for example issuing and adding to ). returns: 

PART1 - INSERTING You don't have to use but and add square brackets to the JSON data structure. I sugget you using a JSON validator website like JSONlint to test the correctness of your JSON data. This codes inserts 3 new records in : 

After asking on pgsql-performance list, Jeff Janes figured out that the cause was associated to the default collation used by Postgres (see this link for more informations). MacMini was using the much performing collation while Dell T420 was using the en/US collation. T420 (Postgres 9.4.1) 

I have many tables of the same length (number of rows) that were previously ordered by some columns as they were being written on disk. My is of the form: 

I hope this post will help other people in the future. After reading about a lot of digressions about kernel, virtual memory, RAID controllers, disk cache, WAL and other tech stuff I never found someone talking about collations. 

You can mess around with the various parameters available in . For example, you can export a gzipped file or a CSV with headers and custom separator: $URL$ . You can also issue a to export the entire original database and run to recreate it on the new database: $URL$ . This should work.. I've just dumped an output file of several tens of GB on OS X (linux-like) without any problem. Se also for more selective database dump options: $URL$