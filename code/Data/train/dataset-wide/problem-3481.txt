either do the both other answers (downgrading, or reducing security by disabling the check) Another option would be to actually fix the issue by having correct permissions for the root chroot folder. Qouting a nice blogpost, which Marek already linked 

the file is in /etc/update-manager/release-upgrades i assume this happens because 13.04 is already end-of-life and therefor the next supported version is 13.10 though not 100% sure, see wikipedia change 

Start the server Press Enter key at boot loader to get to the boot menu Press Space to pause the default boot loading Boot into single user mode (press 4) Define Shell : enter the full pathname for the shell (or press enter for default path /bin/sh) remount root file system (/) rw (read/write) 

There seems to be a somhow possibly related BUG from Mr Oetiker on github for rrdcache: $URL$ This actually could be my issue (concurrent writes) but it does not explain the cronjob to not fail. In the asumption i actually have 2 concurrent writes would return exit code 1 (per man page ,confirmed in testing) As i do not get an email with the output either and the observation that the cronjob do actually run fine all the other time i am somehow lost. Example output: 

I created the pt-stalk tool in Percona Toolkit to help diagnose problems that happen when you are not looking. It will not only help you catch the problematic query, it will gather enough information for you to determine whether the query is the problem -- and if not, what else might be. 

You might take a look at XtraBackup Manager, by Lachlan Mulcahy. (It's not a Percona product; it's a community-oriented manager for Percona XtraBackup). If it doesn't meet your needs now, it might meet some or all of them in the future. 

The problems you're seeing could be caused by dozens of different root causes. You definitely should not guess; you certainly need to measure and diagnose carefully. If you gather enough information, the true cause will be evident, and the solution will be obvious (provided that you also understand enough about the server's internals to know what you're looking at). If you guess and try to do things such as reconfiguring the server, my experience shows that you can make the problem much worse, or cause other problems, and you'll never really know whether any specific change helped or not. I would suggest using the pt-stalk tool from Percona Toolkit to capture a set of diagnostic data from the server when one of the spikes of slowness happens, and ditto for when it's running more quickly. There should almost certainly be enough information there to understand what's happening. If you are not comfortable making a diagnosis from it, any competent MySQL support provider should have no problem if you tarball the samples from pt-stalk and send them over. I don't mean to be too repetitive or insistent, but again please don't use trial-and-error on this one. 

No obvious change in the situation. $URL$ $URL$ Others suggest sata cable or even an incompatibility between board + drives. However as i seem to either have the issue on one drive and this populates to all 4, or having the issue directly on all 4 devices i am unable to pinpoint the issue further. As this is a production server putting this server down for maintenance (aka bios/kernel param changes) is possible, but i like to prevent that if possible. According to the hoster this might be power management related: $URL$ $URL$ 

ftp uses 2 ports, 20/TCP DATA Port, 21/TCP Control Port check your firewall,routing settings for that port aswell. Depending on your connection to the server you might need to port-forward the passive port rage. see Proftpd online documents set in the config file: 

For your setup this looks like the 2nd option. google for the exact error message "nss_getpwnam: name '0' does not map into domain" gives you plenty of results which might help you aswell (just pointed the most promising) 

if its not a copy past error that your "proxy access" is via https then i suggest you check your vhost configuration for SSL. for all i know you need your mod_proxy setup in both your normal *:80 and *:443 Virtualhost Directive. so either your Virtualhost Directive does not have the entry or you should show us the *:443 Entry to try to help you more specifically. 

I have applications running on tomcat, previously standalone. Now I have to implement high availability for all apps, so here I am. I have the option of a hardware load balancer or some other means. I will like to know if there are gotchas I have to worry about, especially since I will like to have a single log file location (a single log file between the two application servers I have is preferable). What do I need to look out for, will multiple file access from the two servers be an issue? Even on linux? 

I've been struggling with some practical limitations on Azure and some on-premise infrastructure. I setup a VNET on Azure and needed to connect via Site-to-Site VPN to 4 different on-premise locations. Naturally, I chose a route-based VPN which could support multi-site connections, but it turns out that one or more of these on-premise VPN devices support only static (policy based) IKEv1 setup. It won't be a problem if I can create a policy based VPN and connection to each client on the VNET but apparently I can only have one VPN per VNET. What options do I have to be able to connect to these various on-premise locations? Will I need to create multiple VNETs, connect each VNET to an on-premise location with a VPN and join the VNETs with VNET-VNET connection? Is this even possible even though it's becoming messy just typing it. Appreciate all the help I can get now. 

You can use the nc_cmd configuration option in the template, as stated in the comment just above your last comment to the bug report on the Google Code Cacti template project, where you linked to this question. In addition, you should know that you are using outdated templates. As stated on the Better Cacti Templates project, that project has been discontinued and is now part of the Percona Monitoring Plugins. See $URL$ and note that this software is fully covered by Percona support or consulting contracts, which is a great way to get help with installation. Or, in the unlikely event that there's a real incompatibility that won't let you get the templates to install as-is, you'll be able to get bug fixes to the software. 

The closest thing I'm aware of to what you are asking is the Securich project. It creates a set of stored procedures and helpers that you can use to create pseudo-roles and manage a lot of privileges in MySQL. Note that I haven't really used it myself; I've just watched the blog posts with interest. You can read more at $URL$ It was also presented last year at the MySQL conference: $URL$ although I can't find the slide deck, and Sheeri K. Cabral has mentioned it in her talk at IOUG Collaborate: $URL$ 

As Koen van der Rijt already wrote you should check SF for similiar questions and read the answers carefully. 

The latest available version for Centos (1.3.3g), as Spectre already said, seems to not work correctly with TLS 1.1/1.2 See the following link for having at least some notes about a potential fix for those issues beeing in Version 1.3.5 

This can have a few different reasons. For example it might be possible that the interface was created after Wireshark was started. In that case you should reload the interface list inside of Wireshark. To check that the wanted interface/Subinterface is listed check in the "Capture Options" that the interface you want to use has the IPs you want to listen on. Additionally this might be a permission error that the Wireshark user is not allowed to read the interface. 

it seems you have the vi or vim editor as default and this was used by dpgk. You can quit the merge window with: 

i do not know for additional csh files similiar to bash files (.bash_login .bash_logout) in the user folder, so you might need to check the existence with ls -a for showing the "hidden" . files. 

I assume this is straightforward, but I'm lost in the abstractions in Chef. I want to create a new Unix group on Red Hat Linux, called deployers, and then add some of my users to that group. What are the steps to achieve this? 

I would use the slow query log. It captures all queries, not just those that are slow, if you set long_query_time = 0. It also captures ALL queries, which is not true of the TCP-sniffing techniques mentioned here; those won't capture queries executed via a socket. Ditto for watching SHOW PROCESSLIST; you will miss fast-running queries. If you want to capture queries via the processlist or via TCP traffic, I would suggest using Percona Toolkit's pt-query-digest. It can poll the processlist for you (and make sense out of the results, which is very hard to do if you're capturing a bunch of samples of it yourself), and it can interpret MySQL's TCP protocol, so you can grab some TCP traffic and analyze it. Of course, it's also the best query aggregator / profiler / reporter ever written, but you didn't say what you want to do with the queries after you capture them. 

Did you actually start the virtual interfaces? I see in your interfaces file that you start eth0 automatically, but not the vlan10 device. 

Yes they can. Basically this is a "reverse DNS query" (see the comments for another link). There are search engines which output you the amount of domains on your IP See for example sameid.net as there is most of the time a reason to use SSL for viewing a site or entering data and, unless you do use SNI, you would need at least one IP for every domain anyway. 

As you already have written it seems you actually have a fallen or soon to fall SSD. Having an SQL Database and a nearly full SSD can result in fast "degrading" quality of the ssd. Best for something like this to at least have some values for anticipation is checking the SMART Values of the ssd. Some of the important values are "Wear-Leveling Count" and "uncorrectable error count" Depending on your SSD you can theoretically get a lot of (10000 or even more) repeated writes on one cell, but this might even happen fast than you think when all the data is still used and garbage collection can only recycle some of the cells. Sure enough the controller of the SSD usually takes care of that, but only during the last 1-2 years the controller did get significantly better. Basically summit: SSD broke. Advise: split OS + application on at least 2 seperate disks/SSD, get some raid to prevent downtime, and never forget backups. 

I would suggest the so-called user_statistics feature that Google originally developed. It is available in Percona Server and MariaDB, and there is a similar feature in the PERFORMANCE_SCHEMA in the upcoming MySQL 5.6 release. All of them make it rather trivial to determine what tables, indexes, etc are used. If this is not an option, then I would choose pt-query-digest as suggested by sreimer. 

There isn't a good read-made tool to do this. pt-table-sync doesn't work exactly as you've been told (I wrote it ;) but it's not the right solution. I have seen its bidirectional syncing functionality used to reconcile servers to a central source after being intentionally disconnected and updated, but this isn't the right solution for what you need. I don't normally give sales pitches on topics like this, but this is honestly a case where I'd engage Percona to develop a new tool for you. Some people have written little scripts that suit their personal scenarios, but a high-quality solution for general use doesn't exist yet, and isn't really that hard. The main thing is that you need formal testing, and Percona Toolkit's components are already 90% of the way to what you need -- it just needs a little bit of glue between them. You could do this yourself, of course, but why make a square wheel and end up maintaining it yourself and discovering all of its bugs when you least desire it? That said (end sales pitch, sorry) -- the solution I'd suggest should avoid blackhole tables and go with simpler, less troublesome techniques. (Yeah -- I wrote High Performance MySQL, too. I know. Back then I hadn't seen as many problems with blackhole tables as I have today.) It should do roughly what Rolando suggests, but there are some subtleties. For example, it shouldn't let the I/O thread stream a bunch of data from the master, getting far ahead of the SQL thread, then throw it all away when it round-robins to the next server. That would be really wasteful and cause a lot of impact on the master. There are a lot of little details like this that need to be taken care of -- another that comes to mind is not switching masters when a replication-caused temporary table is in use. 

I have several VMs within the same virtual network on Microsoft Azure and I plan to connect to a vendor's on-premise site. I have setup a site-to-site ipsec VPN with a public IP and about to send out the configuration to my vendor. The issue is, I plan to fire up multiple VMs depending on demand and they all need to connect through the VPN to the vendor. The issue is that the vendor wants a single public IP and host IP and won't accept a range of IPs. I'm thinking I should be able to NAT all the VM's internal IPs to the public IP so that the on-premise client sees only one IP and respond to the same. Is this the normal behavior or do I have to apply some more configuration? 

I'm using Google Cloud VPN gateway and attempting to connect to a CISCO ASA 5545 device on a 3rd party premise. It's a static route setup and the Cisco router is using IKE v1 only. I have this problem where from the logs, I could see connection is established, then it says scheduling rekeying immediately after, then received INVALID_ID_INFORMATION error notify, then received DELETE for IKE_SA vpn_[PEER IP] then deleting IKE_SA vpn_[PEER IP] between [VPN PUBLIC IP]...[PEER IP]. This continues to repeat in the log. Obviously there are configuration mismathces; the on-premise client wants me to change the encryption to AES-256 or 3des because "the device does not support AES 128". Is it even possible to change the encryption for Google Cloud VPN once you've chosen to use IKEv1? According to the documentation $URL$ IKEv1 uses aes-cbc-128 encryption, is it possible to change that to aes-256? Is it possible to make the on-premise device work with aes-128?