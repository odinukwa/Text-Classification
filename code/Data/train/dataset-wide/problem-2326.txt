Employees can have more than one address, you should have a join table to address for the many to many relationship. The phone table is designed incorrectly. You do not want to add a column when you get a new phone type. The whole leave thing makes no sense. Please explain if this is a system to manage leave requests? You should have individual records for leave accumulated and leave taken. Do you need an approvals table? The balance shoudl be figured out at the time of query. The pormotions table makes no sense at all. You want a table to store the organizational position (And it should be a history table so it should have start and end dates). This table should be updated everytime the postion title changes (they are not always promotions). Same with salary, you want a start and stop date. You seem to be missing data on who the person reports to. Generally reporting will need to be able to sort through the reporting hierarchy. YOu also seem to have designed this solely on the basis of the data entry GUI. This is HUGE mistake of epic proportions. With this kind of information, reporting is a far larger problem and you need to consider reporting and how you will need to see the data over time. For someone who works for the comany for ten years, what type of information do you need to call up ablout his history? This is a business critical database and should have been designed by a professional database programmer. There are legal implications to this data, there are security concerns. How are you planning to protect this information? Pretty much all of this should be unavaliable to most users and admins. It should be encrypted. This is critical privacy data. Employee records typically include benefits information. They also typically include information concenring awards and performance appraisals and performance warnings. The use of ID as the PK is a SQL Antipattern ($URL$ You should use tablenameId. 

You cannot just swap right and left joins. They mean two different things. You also need to understand the effect of having a where condition on the table on the left side of a right join or the right side of a left join. See this link to understand why this changes teh join to an inner join. $URL$ See if this code is what you want: 

One place besides recursion where I find CTEs incredibly useful is when creating complex reporting queries. I use a series of CTEs to get chunks of the data I need and then combine in the final select. I find they are easier to maintain than doing the same thing with a lot of derived tables or 20 joins and I find that I can be more assured that it returns the correct data with no effect of multiple records due to the one-many relationships in all the different joins. Let me give a quick example: 

Personally I would use SSIS to do this task. First I would bulk insert into staging tables or use a data flow, then do whatever clean up and transform tasks you need and load the transformed data into a final set of staging tables. Then use the data flow task to send the data to the production tables. The reason why I would have two separate sets of staging tables, one with the raw data and one with the transformed data is that it makes it easier to research data problems that come up with running imports over time and if the transform is done before the final load, then the final load affecting prod will be faster. Note SSIS in a dataflow does process one record at a time, but it does it much faster than the average cursor especially if you are not doing an data transforms at that point. Further in SSIS, you can send bad data (things like invalid values, missing required data etc.) to an exception table so you can inform the data provider of the problem but still process the rest of the data. 

It also depends to a very large extent on the purpose of the database. A database that collects data from sensors for instance that loads thousands of records per second likely has more inserts than anything else. An online store database likely has more selects becasue people look at far more stuff than they buy. A mature Enterprise business system may have more updates than anything, particularly if most of the new records are inserted through some type of bulk insert process. It would be a rare database that would have more deletes than anything else. Generally that would happen only on rare occasions such as losing a major client and removing theire data. If you have more DDL than DML statments and the database isn't in the process of setting up, then you probably are doing something drastically wrong becasue structure should not change as frequently as data. 

When you are buying new the choice is far different than when you are considering upgrading. Buying new it is my belief that you should always buy the newest version you can get. 2008 version will be no longer supported far earlier than the 2012 version. Better to start new with the lastest as you will be using this backend for a long time. As to the need to the first service pack, it will be out before you know it and since you are doing new development, the problems it fixes will likely not affect you as much a legacy database with millions of records would face. Now if you are just getting a new server but putting an old database on it, then the question becomes what are you upgrading from? If the database is already a 2008 database, it will be significantly less risky to use the same version. If you are upgrading, check to see if you can upgrade directly to 2012 from your version. 

First thing I would look at is updating statistics (not the same thing as indexing, look in BOL for more information, look up UPDATE STATISTICS) and possibly rebuilding indexes. Statistics should be updated regularly in amaintenance job and if you had a dba you would be doing that. Likely you are now getting a less useful execution plan for many things. Next thing I would look at is hiring a dba, there is no excuse for not having a dba when you havea system with millions of records. 

Data belongs in the child table. We keep it correct with a trigger on the table to ensure one and only record is marked as favorite (or in our case as the preferred address). However, @ypercube's idea of a separate table is a good one as well. 

Not sure why it needs to be recomplied but until you get to the bottom of this consider making the proc recompile every time it is run with the following code, at least that way it won't fail every morning: 

Do you really really need to match the id from the table you are merging after the load? Or do you only need it to get child records in and properly related? If it is only used to get child tbales properly related, then this a process we have used on occasion: Stage all the data in work tables. Add a column for the Id from the other table to the table you are loading to. Let the process create new records with identities for the parent table. Update the staged child table to use the new id by joining on the column you added. Have the process load the child tables. 

I like the idea of using a generic linked server name. However, in many environments this may not be possible, In this case, you can use dynamic SQl in your sp. 

You need to do log backups at least daily (we do them every 15 mintues). Read in books online about how to backup the log (not the daatbase, the log, these are two separate types of backups) and you should find directions for how to truncate the log without backing it up which you will likely need since you have let it get this size. The make sure you have a current backup of the database before you start. Then truncate the log and immediately set of regularly schedueld backups so this doesn't happen again. 

The main reason I support surrogate keys is that natural keys are often subject to change and that means all related tables must be updated which can put quite a load on the server. Further in the 30 years I have been using a variety of databases on many topics, the true natural key is often fairly rare. Things are supposedly unique (SSN) are not, things that are unique at a particular time can become non-unique later and some things like emails addresses and phone numbers may be unique, but they can be re-used for different people at a later date. Of course some things simply don't have a good unique identifier like names of people and corporations. As to avoiding joins by using a natural key. Yes that can speed up the select statements that don't need the joins, but it will cause the places where you still need the joins to be slower as int joins are generally faster. It will also probably slow down inserts and deletes and will cause performance problems on updates when the key changes. Complex queries (which are slower anyway) will be even slower. So simple queries are faster but reporting and complex queries and many actions against the database can be slower. It is a balancing act, that may tip one way or the other depending on how your database is queried. So there is not a one-size fits all answer. It depends on your database and how it will be queried and what type of information is stored in it. You may need to do some testing to find out what works best in your own environment. 

We require all database structure changes to be done with scripts (even on dev) and saved in subversion. Then on a set schedule we refresh dev from prod and they have to rerun their scripts to get back to where they were in the development cycle. This helps ensure that everything is done through scripts and that they have scripts ready when it's time for deployment. I know in 2008 you can set up DDL Triggers to track database structural changes, can you do this in 2005? This way at least you can find out when someone changes a setting who did it and find out why. 

Well first there is no way that all of this should ever be in one table. You need related tables for information such as Assignee and Inventor. Please read about normalization. But in reality are you sure that mysql is the way to go for this since you seem mostly to want to search on data that is not always in the same form. Personally, for this type of thing I would look at NoSQL databases for the text parts in conjuction with a relational database for the data that is easy to describe and determine the size of. 

I don't know for sure in mysql but I see a bunch of things that casue performance problems in SQL server. First, correlated subquereis are often performance killers as teh run row by agonizing row insted of in sets. Next, do not use a wildcard as the first character in a where clasue as it prevents index usage. Either your table is incorrectly designed that you need to do this (such as when you store a comma delimited list) or you need to start using some type of full text search instead. Frankly there is no reason why users can't put in at least the first charcters of the email or username, so the wildcard is probably not necessary at all. Look at your requirements and be sure you are not goldplating and harming the system inthe process. Next, no production code should ever use select * espcially when there is a join. You have repeated the join field twice by doing this which is wasteful of server reources and network resources. And if you are not actually using all of the other columns that is wasteful too. Further, I know in SQl server there is a performance hit while it looks up the column names and this may be true for myssql. At any rate it is a SQl antipattern just like using implicit joins is a SQl antipattern. It is also dangerous for maintenance as things which are added to the tables should not automactially be added to the query. You could end up showing things in the wrong place (if some person rearranges the columns) or returing data that you do not use or even showing some fields that you don't want the user to see. This is a very poor practice. 

First - you must be able to backup and restore a database. You must know how to set up a recurring schedule to backup both the database and the transaction logs. You should know what other maintenance is required periodically such as updating statistics. You should understanding indexing - how to create them and when to create them and when not to create them. You should understand how to read query plans - execution plans or explain plans depending on the db backend. You should understand datatypes and why using the correct one is important. You should also understand why every table needs a PK and how to set up PK/FK relationships and you should never allow application developers to think this stuff should be handled by the application and not the db. You should be familiar with database normalization. You shoud read about performance tuning and database internals for your particular database and be proficient in advanced SQL. You should know how to monitor performance of your database. You should know how to set up new users and use roles based security. You should be able to install the database on a new server. I'm sure there's more, but this is a starting point. 

Merge joins and sort are notoriously slow. Is it possible to write a query for the OLE Db source instead that has joins? 

We do something similar with client id. Yes it can improve performance if you don't need to go through all those intervening tables in every query. However, and it's an important however, this is best done only if you are using a surrogate key that never changes. Otherwise a change of the project id could require a cascade of updates that affect every table and lockup the system. I suspect project_id is as unlikely to change as client Id (client name, now that's another story) and so you might be fine. But please do consider if you will have updates to the field you denormalize, Also it is critical to set up a way to make sure the tables with the denormalized fields cannot get out of synch with the main table (PK/FK relationships are good for this and you might need cascading updates set (although I personally prefer not to use them if I can help it).)