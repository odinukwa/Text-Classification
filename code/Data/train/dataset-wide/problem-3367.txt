After reading the original email thread and @ewwhite's answer which clarified it, I think this question needs an updated answer, as the answer above only covers half of it. As an example, let's use the output on my pool. I used the command . On my system I needed the extra arg to locate the ZFS cache file for the pool, which FreeNAS stores in a different location from normal; you may or may not need to do that. Generally try without first, and if you get a cache file error, then use or similar to locate the file it needs. This was my actual output and I've interpreted it below: 

I've done a fair bit of troubleshooting but I'm completely at a loss what could be going on. Hardware / platform 

This is related to this question and extends it. The symptoms are the same - 2012R2 x64 with 64GB RAM (21GB used), and both work fine, but IE won't open web pages, returns an error 1450 "Insufficient system resources exist to complete the requested service" and PuTTY gives the error "Network Error, no buffer space available". I could follow the suggestions in that question or increase various TCP parameters, but I suspect the problem really lies elsewhere in some process or other and I'd like to solve the underlying issue if I can because it's recurrent. The problem is that most online solutions seem to refer to Windows XP and x86 architecture, and their solution is "use an x64 based system", so I'm not sure how to adapt them for x64 with ample memory. Alternatively if it is a single process leaking kernel buffers, how would one view the open buffer count for each process, so that the process responsible for leaking or holding buffers can be closed or avoided, without killing user and system processes by "trial and error"? Relevant registry settings: 

never use "more" in powershell. more uses the default formatter (which format-table also uses.) if you had used the Format-list commandlet, you would have found that get-help returns an object with a 'pssnapin' property or a 'modulename' property. 

Windows sends the WM shutdown message to all top-level windows WM_CLOSE etc - but does to in serial order to all the desktop programs. If you are the last one to get it, you have no time left. Windows has a process load/unload module or dll lock that means when apps are terminating or unloading, that apps cannot load at the exact same instant - its a single threading "shareing-of shared stuff" protection problem. Done to save RAM, but catches us here by slowing things down at a busy period for the O/S. Windows has a internal flag to say don't start any new processes or scripts we are terminating. This means whatever listens, can only write to a remote file or use a TCP connection or similar to let somebody know its closing. You can normally not run any new scripts during shutdown. 

Since the same script adds both scheduled jobs, the possibility the difference is in the session/current user credentials is not likely to be the case. I would swap the order in which you remove and add each job, to handle the daily job first, then the weekly job. If that changes nothing, then it is the job parameters and not something in the scheduler state engine that is perhaps uninitialized in error. 

I set up a VPN with pptpd on my Linux server. For many months it works fine. But suddenly it failed. The VPN can be successfully connected but no traffic can come in. Is there anything I can try to debug this? 

I am trying to setup a SVN on my linode server (Debian). $URL$ I found an article here but it only tells me how to setup with Apache. I am using Nginx instead, how do I set up? I hope to set up my own SVN server and check out the files via protocol, rather than Thanks, 

I am sending some files from Server A to Server B. And I created a key file so that I can use ssh from A to B very easy. And then I used scp and rsync to send the files. It was working quite well before but suddenly it fails to work. There's no error message. The machine just went silence after I execute the command (exact the same command as I entered because I keep it in a file). UPDATE2: On the receiving side, the log from is: 

I simply remember many years ago when I worked at a online video serving company like Youtube. We put the screenshots in one folder and then the server were always crashing. At that time a "rumor" saying people should not put many files in one folder, but we do not know the detailed reason. So how many files should I put in one folder? If there is a limitation, why? Any recommended ways to design this? My server information: 

Although the question is old, it might be looked at by other people. If so, remember, the output of and relate to all errors experienced. That includes errors due to your motherboard SATA ports (if used), the HBA card (if used), the SATA cables themselves..... not just the disks. Three quick diagnostic tests are - check the disk quickly using , check the card is correctly seated and not loose, and try a different port or SATA cable (the cable is a common cause of read/write errors). 

I understand that iSCSI doesn't 'know' about files, and that a file and its metadata which was in use wouldn't be guaranteed to be served correctly to a second 'interloper' read-only initiator if it was changed halfway through the request or something, but I'm thinking here about reading from old ZFS snapshots or when the main initiator isn't connected, or files known to be idle - situations where there may not be the usual risk of conflicting instructions and corruption, and for a small setup it would be helpful. 

I'm using ESXi 5.5 and vCenter 6.5 with vSphere Update Manager. I successfully staged about 20 updates (new host deploy) and set it to remediate. I know this can take a long time, but the percentages are static for many hours. Is there a way to check (via command line or otherwise) if it's actually hung or if it's doing anything? It would be nice to also get a more realistic idea about how much it's got to go, but that isn't essential, I mainly want to be sure that I can leave it be, and it's working properly on the patches, even if slowly. On Windows I can watch update-related files being accessed and update-related processes I/O counts, in task manager/perfmon, as they do their disk and process I/O, and on Linux or BSD I can use various well-known tools (but they aren't included in ESXi). What's the equivalent with command line or other tools, in vSphere? 

I assume you are wanting a shutdown event? It is basically impossible to run a script when a shutdown event occurs, logoff, yes, but shutdown no. I have a C# app that listens for the shutdown message, but when the message arrives, it is not possible to spawn any new processes, and the event is often missed. 

Installed service-pack 3? You want to remove/disable services and background apps one at a time to eliminate the one which is leaking connections. This is going to be an application that makes a connection but gets slow responses from the remote host. I would look at the internet connection (I assume LAN or WAN) external link speed and would not mess with the antivirus - its not there, its more likely another application that's at fault. I would start with internet browsers and other remote connection using programs. $URL$ 

The default formatter can mislead, but format-list or using get-member commandlet will list all the properties. I'll bet if you re-ran your command with a user format you would get: [PS2]> get-help remove-distributiongroup | select name,synopsis,modulename,pssnapin 

(above output is not real) But the nub of it is you would be able to see which module has the duplicate command. It's not a duplicate really, because you can call each command like so Asnapin\Remove-DistributionGroup or AnotherSnapin\Remove-DistributionGroup to use the other version :) 

Pool B has one dataset containing 1 TB data with dedup on. It's clear that the dedup functionality applies to the entirety of each pool. What isn't clear, is whether the RAM impact of dedup is based only on the deduped datasets? In other words, all other things being equal, will the dedup table size and RAM impact be similar for pool A and pool B, or far larger for pool A than pool B? I think the dedup table has to be similar for both (set poolwide but no impact on size from any non-deduped datasets), mainly because if it was much larger, it would be equivalent to forcing dedup on the whole pool not just specific datasets. However it isn't clear to me whether this is actually so. 

I don't know powershell specifics for regex or to scan a file line by line, but assuming powershell regex is like all other regex, the actual pattern to match your string with, looks something like this (the 3rd regex below matches your examples): 

What's the current state of play on this for a modern *nix file server with Windows 8.1/10 x64 clients, if NFS is also enabled on the clients? 

I'm guessing it's objecting to the "ada6" and I should be providing some other identifier, or a slice/partition ID instead. But I don't have these; zfs creates them itself when it attaches the disk. What is the correct command to use here, or what am I missing?