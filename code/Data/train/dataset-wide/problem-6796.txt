Edelberg shows that there are circumstances in which the sentence in (1) is true, but the sentences derived by substituting (2a) and (2b) for she in (1) are false. He gives the example in (3) (repeated with omissions and slight modifications from his EXAMPLE 2, p. 2): 

We can derive from (1) a true sentence if we replace the pronoun she by the definite description the witch reported on in the Gotham Star, but this definite description may not be available to any of the participants in the conversation (perhaps none of them reads the newspapers). The point is that the sentence in (1) may be true even if all the sentences derived from it by replacing she with a contextually available definite description are false. 

Assume someone utters this sentence to a group of people who are familiar with Hob, Bob, Nob and Cob and their livestock, but who, up until the moment the utterance of (1) is made, have been unaware of Hob’s and Nob’s belief that a witch has been harming farm animals. Assume further that all the participants in the conversation rightly believe that witches don’t exist and that Hob and Nob are wrong. Such a context provides the two definite descriptions in (2) as possible substitutes for the pronoun she in (1): 

I might also add to the discussion the question of grammatical function. In theories that include the specification of grammatical function (subject, object, etc.) in the syntactic representations they posit, the question arises what grammatical functions can be realized by non-finite VPs (or clauses). There is a discussion of this question by Rodney Huddleston in Chapter 14 of The Cambridge Grammar of the English Language (pp. 1206-1220). He argues that non-finite clause complements in English usually do not realize the object function (which is mainly realized by NPs), but a different function, which he dubs catenative complement. A discussion of the grammatical functions of finite clause complements across several languages can be found in the paper “The grammatical functions of complement clauses” by Mary Dalrymple and Helge Lødrup (which can be found on the second author’s webpage). In case you might find it useful, here is a tree for your sentence that follows by and large the syntactic framework in The Cambridge Grammar of the English Language. 

Compound words are really complex, even if we limit it to noun-noun compounds. It looks like you're concerned with "open" compound nouns, not "closed" or hyphenated or idiomatic compound nouns like these (what a list! right? corpus alert!). So - The syntactic definition of a compound noun is when the rule N -> N N can be recursively applied. Anything with more than two nouns is ambiguous in structure. ((coffee mug) holder) vs. (coffee (mug holder)) or whatever. Most languages are more predictable than English when it comes to "headedness" or left/right branching of closed noun compounds; Downing's conclusion to "On the Creation and Use of English Compound Nouns" is basically that the relationship is unpredictable from the semantics of the two words. My instinct (very scientific!) is that the concatenation vs. use of prepositions for noun-noun compounds is predictable by typology. My guess is that the form of compound nouns is correlated to the Adj/N word order tendency of a language. Of course, this isn't really an answer, but I think the answer you're looking for is too big. It's sort of like asking why a language is SVO or SOV. 

Agreed that the Jurafsky/Martin and the NLTK book are wonderful to start with. Next up would be Finite State Morphology by Beesley/Karttunen primarily focused on xfst applications. 

Graphemics/graphetics is more of a sibling field to linguistics than a child. Their parent would be semiotics: spoken language represents ideas, written language represents spoken language (usually). So maybe graphemics/graphetics is more of a niece or nephew since it's dependent on the existence of spoken language... 

Some of these forms survive as adjectives ("shorn," "stricken", "swollen"), but "have holpen"? Yeah, right. Interestingly, there are a few of verbs particularly in American English that have gone the other direction: where past participle form ("have __ed") was regular, but because there was a very common, phonologically similar word, it was "irregularized." 

The question your teacher gave you has already been discussed by Barbara Partee in her introductory paper ‘Lexical Semantics and Compositionality’ (see reference at the end) - only she used almost half empty and almost half full. If we assume that the constituent structure of both expressions is as in (1), and also make the assumption in (2), the question then arises how come the two expressions differ in meaning. 

According to (3) and (4), in deriving the meanings of the expressions almost half crazy/sane, the meaning of almost half combines each time with another meaning, and therefore it is not surprising that the two final expressions differ in meaning. A second way to solve the problem is to accept the constituent structure in (1), but to deny that half crazy and half sane have the same meaning. Maybe the meanings of half crazy and half sane always combine with some approximation/precision value, and when such value is not expressed it is taken to be the absolute precision value (the value expressed by exactly). We can now assume that the meanings of half crazy and half sane are such that they give the same meaning when combining with the meaning of exactly but not when combining with the meanings of other approximation/precision expressions (like almost). A third way to solve the problem is to accept the constituent structure in (1), but to deny the principle expressed by (2). We can now assume that in deriving the meanings of almost half crazy/sane, the meaning of almost is combined with the meaning of half, even though it doesn’t form a constituent with this element. Reference: Invitation to Cognitive Science, 2nd edition. Daniel Osherson, general editor; in Part I: Language, Lila Gleitman and Mark Liberman, eds. MIT Press, Cambridge 1995, pp. 311-360. 

As jlawler mentioned, the main "problem" is simply that Word doesn't recognize exactly the same things as (un)grammatical as actual people do. This isn't really the fault of the programmers - we're still learning things about English even today - but the result is that there are discrepancies between the program's verdict and a native speaker's. "Fragments" are also a particularly common area for these discrepancies; this is due to (possibly-outdated) conventions in formal writing, rather than conversational writing. 

The short version: no, but there is a useful perspective gained treating them as such. Or, perhaps more controversially: not until recently. That said, as was repeatedly emphasized to me in introductory linguistics courses, language is a fuzzier concept than our everyday use of it might suggest: dialects of "the same language" might not be mutually intelligible, as in the case of Chinese, or two countries might distinguish "their" languages from each others despite being perfectly able to carry on conversations. Note: the remainder of this answer focuses on English, as this is the language I have firsthand experience of; it may generalize to other languages, however. Specifically, the advent of the Internet and text communication has catalyzed the rate of change in written English, to the point where it may well be changing as rapidly (and diversely) as spoken English. Nevertheless, there is a fairly tight feedback loop between the two on account of the vast majority of English writers also being English speakers. For example, consider the term "LOL". I think it is reasonable to say that this is not a term that would ever appear in spoken English without its textual counterpart; pre-Internet, written English was exclusively for formal or private use. I've witnessed the term make its way into spoken conversations, where it was pronounced either letter-by-letter or (slightly more recently) as a single syllable. More recently still, the term has come back into written English as, variously, "lawl", "lulz", or "lel"; all express or refer to some form of amusement. I should probably note here that this sort of written English is not what you would find in a dictionary - it is conversational, informal, and an artifact of typed (or texted) interface limitations. While it is true that there are no "native" written language speakers, I wonder whether part of this absence might be due to the props required: the relative ease of learning a spoken or signed language means that the situation where a written language might become natively "spoken" never comes up (nor should it). It should also be noted that written English has impacted spoken English before, and vice versa - just ask anyone who's failed a spelling bee. Acronyms such as "radar" and "laser" are inventions dependent on the alphabet - that is, the phonemes, so to speak, of written language (the technical term I believe is "graphemes"). 

The finite clause (with or without the complementizer that) is a different syntactic category than the noun phrase, as can be seen from the fact that finite clauses have a different distribution than noun phrases. Some examples are given in your question and in @curiousdannii’s answer above; two more examples are given in (1) and (2). 

Subject and object are names of grammatical function categories. The grammatical function of a constituent in a sentence is determined by its relation to other constituents in the sentence. The subject, for example, is what the meaning of the predicate is predicated of, and it also agrees with the head of the predicate - the main verb, if there is one - in person, number and gender. Note that verb is not a grammatical function category but a syntactic category. The verb in a sentence realizes the grammatical function of head of the predicate (sometimes called the predicator), as can be seen in the syntactic structure tree below, where the grammatical function categories appears in blue. You can read a little more about this distinction here. 

(1) demonstrates that certain verbs that take finite clauses as semantic complements can (or must) be separated from them by non-referential it; but this is not an option for noun phrase complements of the same verbs. (2) demonstrates that noun phrases can appear between a verb that selects them as argument and a predicate (rude above) that is also an argument of the verb; but this is not an option for finite clauses. However, phrases of different syntactic categories may sometime overlap with respect to the grammatical functions they realize in the sentence. Both noun phrases and finite clauses (with the complementizer that) may be the subject of a sentence, for example. You can read a little bit more about the difference between syntactic category and grammatical function in this post. 

That is, the more infrequently they are used, the less likely it is that somebody's going to remember the irregular form so they just follow the rules they know. These are "early modern English" verbal forms (about 400 years ago): 

Some languages like Japanese have spatial deixis words that refer to both the speaker and the listener. Sometimes analyzed as proximal, medial and distant, it's more often analyzed as "close to me, the speaker", "close to you, the listener" and "far from the two of us." 

What those "childish errors" really show are that children are generating language, not just repeating it. Think "This is a wug. There are two __." (Sorry, kids, the answer is "wuggen." There's more than one way to pluralize a wug.) I would definitely argue that, particularly in verbs, the generative rules that children learn and then unlearn for irregular verbs is a factor in language change. It might not come directly from children, but the regularization of verbs happens: 

This is not very common, probably hypergrammaticalization, a very "grown up" occurrence. I mean, adults get things "wrong" all the time. (My peeve is phenomenon/phenomena.) English in particular is a confusing mess of different verbal paradigms and borrowed words with borrowed plurals from a half-dozen other languages. (Phenomenon/phenomena is Greek, but radius/radii is Latin, etc.) Point is: you still know exactly what the kid means when she says "gooses" or "thesises" or, as my niece is attributed, referring to a babydoll whose eyes would open and close: "She oped her eyes!" The key concepts for verb regularization are economy and analogy. Why remember forms when instead you could remember one simple rule? Think about programming a vocabulary into a computer: would you rather program every form of every word, or just tell it to concatenate the verb with "ed" to form the past tense and concatenate the noun with "s" to form the plural? Child (and adult) speakers' brains are just as lazy as the programmer-you is in this scenario. We'd all rather things be simple, neh?