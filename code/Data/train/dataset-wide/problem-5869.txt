Given the following assumptions: 1) When Wittgenstein speaks about meaningful propositions he does refer to propositions of natural sciences. 2) About the problem of the perfection of language, he believes that at least a part of ordinary language is already perfectly ordered, as well as propositions of logic, if we refer to that part which has been purified from methaphisical assumptions and other nonsensical propositions. I want to ask-> Are propositions of natural sciences to be considered propositions of natural language? 

_____________ edit _____________ By the way, I found this article about the discrepancy between Wittgenstein and Russell with regard the argument I mentioned; I'm looking for similar sources or other explanations and with regard to Ramsey's misurnderstanding as well. 

Is it correct to say that the problem Russell is focusing on is that of the impossibility, for logic, of representing the logical form? Could you explain to me more specifically in which sense the solution provided does solve the problem I quoted? 

There can be a zillion different unknown unknowns. E.g. suddenly a giant TV set might drop from the sky. One prepares by estimating probabilities for various kinds of unknowns, and concentrating the effort on the least unlikely. The dropping TV set is very unlikely, to the point that it would be counter-productive to waste time and resources on preparing for it. Unknowns of the kind "almost zero probability but almost infinite impact if occurs" are also best disregarded. Multiplying the near zero probability by the near infinite impact if occurs, to get an expected impact value, doesn't really work in this case. Very small differences in the numbers can produce very different expected values, so there's no real information about probability to be had. E.g. Blaise Pascal (contemporary of Descartes) used an expected impact value argument to apparently prove that one should better believe in a specific god. But it's in the nature of near-zero-probability things that there are zillions of in principle possible such things, so e.g. Pascal's argument failed when other possible gods were considered. For computer belief management dealing with unknowns involves representing both probability and support. For example, 50% chance of raining tomorrow with 100% support means that in half the cases, it will turn out to rain. But with 0% support it doesn't mean anything: no prediction is possible, and the probability must just be ignored. At least one theory that attempts to do this correctly, Dempster-Shafer evidence combination, suffers from combinatorial explosion, i.e. it's just impractical. Apparently one can not, in practice, get a completely correct picture of the world. Happily very rough approximations work well in practice. But that means that we in some cases will end up with doing the Wrong Thing, where if we but had infinite memory and processing capacity, would have enough info to do the Right Thing; hence the expression "20-20 hindsight". 

1 Williams, B. “The Analogy of City and Soul,” in R. Kraut, ed., Plato’s Republic: Critical Essays, pp. 49–60. 

As a software developer, I often face the problem of meaningless terms. It is very common in our industry that a certain term is first coined with a very specific and useful meaning. Then, it becomes fashionable to use that term, so everyone starts using it to mean more and more things until, in a couple of years, the term has become so diluted that its original meaning is lost altogether. This has been occurring with the term agile, for example. Which philosophers have addressed this probem of dilution of meaning and how to avoid it? I have illustrated the problem with a software development example, but it may occur in every professional field. When a term has reached that meaningless state, it becomes impossible to have a rational, useful and unambiguous discussion. 

1) I don't recall exactly those experiments, sorry; I only remember reading about it, but no doubt they involve exponentially increasing effects (i.e. chaos, in its mathematical meaning, more informally the butterfly effect) and inspecting ever more fine details. 2) I almost used the acronym SDS, Heinlein's Super Duper Snooper, with a footnote explaining it, but then I thought better of it. 3) Can't leave this without mentioning Charles Stross' “Accelerando” universe and “Glasshouse” novel. 

Well, the idea of a directly programmed intelligence, one where each main function of the mind was implemented by a programmer, was common in the 1970's. But while it's nice as a goal to gain better understanding of how e.g. vision processing works, it's wholly impractical as a way to create an intelligence. As already Alan Turing (1)noted in 1948 or so, the most likely way a machine intelligence is created, is the way that a human intelligence is created, namely by growing up and learning – with some basic instincts and abilities in place, of course. So a first answer is that the question as posed doesn't make much sense, because it's very unlikely that a machine intelligence will be of the directly programmed variety, that there will be any programming (except of basic functions such as edge detection in vision processing): I'd guesstimate that it's about as likely as a crocodile emerging up through the asphalt in the street, deftly stealing your wallet, only to be hit by a giant iron hippopotamus accidentally dropped from an airplane passing above. However, running on a digital platform means the possibility of making copies, partly or completely. It means the possibility of trying out things in simulated environments. Not the least it means that explorations of possibilities can be really, really fast. Currently the electronics is some millions times faster than our brain stuff, and that difference increases exponentially, which it's kept on doing since 1965 or so, roughly a doubling every 2 years. So we can expect some really fast evolution as soon as machine intelligences start creating new ones. 

One way to approach this is by Aristotle's metaphysics. According to Aristotle, everything there is can be classified in one of the following ten categories: (1) substance; (2) quantity; (3) quality; (4) relatives; (5) somewhere; (6) sometime; (7) being in a position; (8) having; (9) acting; and (10) being acted upon (1b25-2a4). According to his definitions, a substance is something that cannot "be said of" nor "be in" another thing. And substances come in degrees: primary substances are individuals, like this particular man or that particular horse. Next on the scale, we find species (e.g. "man", in the sense of human, encompassing the whole class of human beings) and genera (e.g. "animal"). Substance has a primacy over the other nine categories, since all of them exist "in a" substance. A quantity, for example, refers to how many items of a certain substance we are talking about. Similarly for all the other categories, known collectively as accidents.And, among substances, the primacy is reserved to the primary substances. Everything has to ultimately rely upon the existence of individuals. We can only talk about man – as a species – if there are particular men in the world, so we can apply the name "man" to them and say things like "Socrates is a man". In particular, the category "quality" is important to your question. Although Aristotle does not give us a precise definition of what quality is, we can grasp the concept with little difficulty. When we say that a certain horse is beautiful, it is clear that we are ascribing a quality to the horse. Consequently, we can also beautify a horse, that is, we can increase that quality in the horse. So, what about "beauty"? Is it also a substance? For linguistic reasons, we are sometimes led to think so. After all, "beauty" is a noun and therefore can be modified by an adjective, like "beautiful". Likewise, it can be the object of a verb, like "beautify". Semantically, of course, the phrases "beautiful beauty" or "beautify the beauty" are problematic (the very heart of your question, after all). But syntactically, they are ok. As in the famous example "Colorless green ideas sleep furiously", which is a perfectly grammatical sentence, but full of self-contradictions. But the fact that "beauty" is a noun doesn't change the fact that it still is quality. Moreover, there are no individual beauties, that exist by themselves, detached from substances and to which we can predicate things. A bit more formally: "beautify X" means "increase the quality beauty in X", where X is a substance. As we have seen, beauty can only be a quality, not a substance. So we cannot substitute beauty for X, in this case. In other words, "beautify the beauty" makes no sense at all. 

where is an identifier to be defined if it isn't already, denotes assignment (which creates an identifier if it doesn't exist), on the right hand side evaluates to logical if it doesn't yet exist, denotes logical OR, and denotes a new object, which is the result of the OR-expression if evaluates to . It's also the basis of boolean short-circuit evaluation in a great many programming languages. 

Not quite arbitrary association 1: PβQ≡Q can be viewed as a special case of an indexing operation choose(F,P,Q)≡Q and choose(T,P,Q)≡P. I.e., the first argument decides which of the two following arguments should be the result. All binary logical functions can be expressed in terms of such indexing, e.g. P∧Q≡choose(P,Q,F). And this connection is used in many scripting languages to represent the result of a logical expression as one of the argument objects. E.g., in Javascript, IIRC, 

In other news, … I find it noteworthy that the notion of practically irreducible complexity is not one of the "two positions" you list, namely (1) the supernatural and (2) the view that some things just can't be understood. If there ever was false dichotomy it must be this, with both branches just childish nonsense. Is it really true that modern adult philosophers in general think one has to choose between the childish nonsense options (1) and (2)?