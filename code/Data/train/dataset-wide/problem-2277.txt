You are making this needlessly complicated. You are trying to build a dynamic query where a static one would do just fine. Assuming the logic in the dynamic SQL is correct, the entire procedure could look like this: 

The performance will likely remain the same as with your syntax, but without making the query any faster this rewrite will at least make it more concise and arguably more readable. That being said, there is another method, fairly common as well, that you could employ, which might offer better performance as the number of the attributes increases. It uses grouping and aggregation: 

Your code does not work because of how the conditions are arranged in your nested CASEs. Take, for example, the first value in your output, . When passed through your set of conditions, it will match the very first one: 

The MAX function works in this situation because it returns the maximum value only across the non-null values in the specified set. In your case, there would be only one non-null value per partition of in each of the three cases, so the function would return that one value. This would be the result of the query for the example in your question: 

You can see that each product is listed only once. Joining that table to the other two will not produce duplicates. Therefore, just substitute the above query, as a derived table, for the in your query (also removing your GROUP BY from it, of course): 

The function allows you to delete a substring from and/or insert another one to a given string at a given position. So, in order to skip 18 characters using this function, you specify starting position 1, 18 characters to delete, and an empty string () to replace them with. Like , this solution will correctly work with trailing spaces, should you have the need to take them into account. 

The subquery is not correlated with the outer query. Your subquery needs to have a clause to limit the rows only to those where is the same as the outer query's . In order to be able to reference the outer query's columns, you need to assign a different alias to either instance of . It would probably be best to assign one to each – something like this: 

The recursive CTE extracts from the previous CTE's result set only the adjacent ranges starting with the row with the lowest ID. It also adds a "zeroth range", one that starts with 0 and ends with the lowest ID. This is the output: 

If you insist on storing the results in a table, you can just add an INTO clause to the query. A possibly better idea, though, might be to save the query as a view. 

You can see that the list only has the column names. To make this easier, I would further suggest you use short aliases for tables, like this: 

Then use a conditional expression to rank the rows based on whether the match is full or partial, using the DECODE function, for an example: 

The join to in the main SELECT would then be redundant. Alternatively, you could outer-join the result of and check for presence of a match in the WHERE clause, like this: 

The optimiser, however, might still internally rewrite your query to something very similar to a UNION query, so in the end the performance might not be much better, if at all, than that of query with an explicit UNION. 

First of all, after applying PIVOT, all columns of the pivoted set must be referenced using the alias specified for the PIVOT clause. As per the manual: 

However, if the table has no rows or if all values are null, this statement will not do any assignment. In this case, if PRINT still outputs nothing, it means was null even before the SELECT. Therefore, you may still need to apply the previous method. Besides, it is not a good idea to assign a variable this way when a table has many rows, because you have no control of which value will ultimately be assigned. Therefore. Alexei has a point there as well with regard to using TOP, as well as combining it with ORDER BY and, possibly, additional filters. 

The main SELECT essentially just takes the last row of each group using the filter, pulling only and and also renaming the latter to , so that the final output becomes what you want: 

Both columns specified in the SET clause of your UPDATE are going to be touched (written to) for all the rows matching the , no matter what or may evaluate to. There is no syntax to work around that while sticking to a single statement. So, either live with that or go for two statements: 

If I am not missing anything... Assuming you always specify the user name in such a way as to exclude possibility of multiple matches (if e.g. you decided to look the user up by a mask using LIKE rather than by a complete name), you could try inserting data into the record table using an INSERT...SELECT statement like below: 

For every row where is null, you are returning an empty string as . So without DISTINCT all such entries would be returned as 

Since you want to count differing values in a set that may contain identical values, I would say is the perfect tool to use. For instance, the following will count distinct levels per product: 

If id is defined as the primary key, you can omit grouping by all the foo columns you want for the output as long as you are grouping by the id. This special case of grouping is in accordance with the current SQL standard and has also been covered in the PostgreSQL manual, starting from version 9.1: 

As you may have noticed, the difference between the two ranking numbers is constant throughout the particular group of consecutive event occurrences of the same kind ("attended" or "missed") and is also unique for that group within its partition. Therefore, every such group can be identified by , and the just mentioned difference. And it now remains simply to filter the events leaving just the missed ones, group the rows and get the necessary aggregated data, like the number of rows and, possibly, as in the query above, the dates of the first and the last event occurrence in the group. The number of rows is also used in an additional filter condition to omit groups with fewer rows than required. 

The other, slightly less obvious, option would be to use a derived table to construct the final columns and thus avoid the renaming in the main SELECT: 

You are quoting from the wrong section of the manual. That passage is from the Table-level Locks section. The lock type you are interested in is specified to be . That means you need to refer to the manual's Row-level Locks section in order to find out when that kind of lock is acquired. And in the beginning of Row-level Locks it says (original emphasis preserved): 

This method will not work if you are trying to include non-existent names (in order to cover columns that might be added later, for instance) in the checkings. And, of course, this method will just be unsuitable for you if you are trying to stipulate for unknown names. But then, perhaps, what you are trying to do is not what you should be doing to begin with. Perhaps, you should revisit your schema and consider a different data model (at least for some portion of your data), like EAV, for instance. 

Now, as you can see, the query merely returns the flag indicating if a file has a single version or not. You, however, probably want to filter on that flag. To filter on it, use the above query as a derived table, e.g. as a CTE: 

I am assuming that a document that has not been modified will have a null in . Filtering out the entries with a null user ID is what the WHERE clause in the above query for. Now you could simply group the results by and count the rows in each group. That would give you the output values in this form: 

The syntax itself, however, is unambiguous enough as it is (to the parser anyway). It should be noted that the nested join syntax is not universally popular. If you like, there is an alternative that involves no nesting – a right join: 

Here's a demo for this one too: $URL$ If you ever need to make it one set per , change the call like this: 

This is almost like a cross join with a single row, except in this case the final output will always have at least one row. If the subquery does not return any rows, its columns will be null in the output. 

Now each row will have a group identifier, which is User 2's counterpart in each row. For your example the row set will look like this: 

Specific formatting, like e.g. , may be achieved using MySQL too, but normally it is better to use the presentation level for that. 

One workaround that comes to mind would be to use a "normal" table as a temporary storage. You would probably need to use a session identifier to make the approach work in a multi-user environment. 

This means you will need to calculate both the beginning and the end for each gap: take the latest between and as the beginning and the earliest between and as the end: 

If the columns you are talking about could be viewed as describing a single logical entity, you could (and perhaps should) move them all to a (new) separate table and establish a primary key/foreign key relationship between the two tables. For instance, if your table was like this: 

You just need to stop relying on the non-standard behaviour provided by GROUP BY when the SQL mode is not enabled. Normally, when grouping, you cannot retrieve a column that is not specified in the GROUP BY clause unless you wrap it in an aggregate function. That is because that column may have many different values (remember, we are talking about grouped rows) and there is no way to specify which value of the many should be used in the output. There is a recent extension to that behaviour according to which columns that are functionally dependent on the GROUP BY columns may be selected without specifying them in the GROUP BY, but MySQL has so far went the simple road of offering you just two modes: either you cannot retrieve non-GROUP BY columns without aggregation at all or you can retrieve any such columns, regardless of functional dependence. So, in this case, you are retrieving columns and that clearly can have various values per . Which value is ultimately selected is out of your control, and that is the problem. The solution is to define for yourself exactly which row from the other tables you want to get when you are grouping by and implement the logic the way it can be followed unambiguously. For instance, if you want to go by the behaviour you are observing in MySQL 5.6, you could define it as