Put the attached script somewhere on your hard drive. Open FileLocator Pro Go to the scripting tab Activate the 'Filename' script and select the path to the script Put the value 240 in the 'Custom' field Make sure the engine field is set to "JScript" Click Search The result box will list all files where the pathname > 240 chars in length. 

Is there any service which runs in the background, and does the job of prime95 (i.e. it tests server CPU/RAM for integrity now and again)? Regards, Shane. p.s. The reason I ask: we recently had some bad RAM on our server, which ended up slowly corrupting all of our business files. A file would be copied into memory, it would be corrupted in RAM, and copied back onto the RAID 1 hard drive in a corrupted state. 

This solution has been working well for me for a while. And as a bonus, you also get to use gmail to send company messages from your domain, without the unprofesional look of having a @gmail.com attached to the end of your email address. 

I'd recommend UltraCompare or SyncBackSE. UltraCompare allows you to compare directories full of files, then drill down to compare individual files. SyncBackSE if quite easy to use. It compares two directories, gives a list of files that have changed, and offers the option to synchronize the directories. Its pre-configured out of the box, but if you want to do anything unique it has a whole bunch of configurable options. I use SyncBackSE to backup my files to Amazon S3 (and Gladinet to mount Amazon S3 to a drive letter). 

If you want mail to go out in the background, use something that uses Sendmail in the background. Sendmail is part of most Linux distributions. Its a binary that sends mail. The nice thing about it is that control returns instantly to the webpage, and Sendmail will queue everything up to send in its own time. I wrote a shopping cart a while ago that sends an email once the shopping process is complete. I used phpmailer which in turn used Sendmail. Everything has been working perfectly since the day I switched to using Sendmail rather than a SMTP server. 

I do not believe this is possible as the name servers stored in the domain name do not have any given priority. The client will use the name server that it is given. 

If it's not set before it reaches this location block, then it will set it to a blank string. You can just as easily add a string between the quotes. I do not get any errors when doing a configuration test using this. Please let me know if you're seeing otherwise. 

You can also do various regular expressions so you can avoid from having to add a new one each time a new 'perma-link' is added. 

If you change your custom cookbooks, you need to update the cookbooks on each instance. You can do this under the "Deployments" area by clicking "Run Command", selecting "Update Custom Cookbooks" from the drop-down and then pressing the "Update Custom Cookbooks" button. You can usually leave all instances checked, unless you don't want to update one for some reason. 

Providing that you have loaded your private key on your client, then it sounds like this might be a permissions issue on the 'git' user home directory and .ssh directory. Please try changing your /home/git directory to a mask of 0711: 

Based on your location blocks, files ending with '.php' will only be passed through PHP. So if you have an actual file in the '/images' directory, it should serve it as a static file, or otherwise return a 404 error. If the file ends with '.php' and is in the '/images' directory, it will run through both location blocks in order. Should it do otherwise? 

Rather than having a location block for each redirect, you can always just add rewrite rules into an existing location block: 

We have a contractor working offsite. We would like to set everything up so that his screen is recorded to our Windows Server 2008 R2 machine in real time. We've been using TeamViewer, but we've run into limitations. What would you recommend? p.s. Constraints: 

If you want to search for files "less than" simply change the script. For each result, right click on the file and select "Explore Here". Rename the directory path to shorten it, then repeat the search until there are no results at all. -----start file named "max path length search.js"---- 

I am using Squid v2.7. I am getting lots of TCP_MISS messages in the log, and very few TCP_HIT. What does this mean? p.s. Here is a sample of the log: 

Update 2010-12-20 Tried MySQL v5.1, it didn't work either. Its amazing - if you type "mysqld /?", or "mysqld -help", it doesn't give you any help. And, if you try to restart the service manually, it doesn't display any error messages. Could it be any more unhelpful? Update 2010-12-21 Installed MySQL 6.0 alpha, and it worked. However, I'd rather not use an alpha release, given that the "stable" release is anything but :( Update 2010-12-21 Found $URL$ dealing with troubleshooting under Windows. Discovered that you can generate an error log if the service doesn't start - see here: $URL$ Update 2010-12-21 Aha! A clue. To actually see the error, add "--console": 

Here's how I fixed the problem: Step 1: By default, its impossible to find out the reason why the service is failing to start. So, tell it to start in non-service mode, and pipe errors to the console: 

Here is an example of shrinking a disk. This command line worked, in spite of the same command from the GUI failing: 

We did a lot of Moodle performance optimisations where I used to work and used PHP Xdebug profiling to help out with a lot of it. By enabling profiling, it will produce a *.cachegrind file which is then readable by many readers - I use QCacheGrind on Windows, or KCacheGrind on Linux. You should then be able to pinpoint the function which is taking its time. From memory, it is probably the amount of course data that it's trying to load when the user logs in. If there is too much data, and it's grabbing all of the data from the SQL server rather than only whats necessary, this could slow things down quite a bit. This can also be identified using the slow log as mentioned in a comment above, providing that its the SQL query that's causing the performance issue. I would also definitely advise upgrading your Moodle instance to something newer as I know there were a lot of performance fixes pushed across to the upstream repository since Moodle 2.2.3. We used to push these over to Moodle as an official Moodle partner. 

By default, when you create an account in WHM, it will configure local mail accounts and it will set the local DNS to use the local MX records. However, if you do not wish to rely on the local DNS for the MX records, then you can force WHM to always use a remote mail exchanger. In WHM, goto: DNS Functions -> Edit DNS Zone -> (select domain name) At the bottom, you will see something called Email Routing. Set this to Remote Mail Exchanger and then ensure the DNS server is restarted once saved. 

Now, do all of your access through the UNC file share, e.g. \computer\mydrive - RO\ I use this trick to provide an absolute guarantee that my file backup program won't inadvertently stomp on the data that its meant to be backing up. 

I've run into a brick wall trying to install MySQL v5.5 on my machine. My PC is Windows 7 x64, Enterprise edition. MySQL installs fine, but when I run the "MySQL Instance Configuration Wizard", it pauses forever on the step "Start Service" (I can let it run for 30 minutes with no response). If I go into services, I see that the "MySQL" service hasn't started, and if I try to start it, it says "Windows could not start MySQL Service on Local Computer. Error 1067: The process terminated unexpectedly." I've tried the following: 

You can also use the "Reliability and Performance Monitor" that is available under Windows Server 2008. As you can see below, it automatically keeps a record of the reliability of the server, and assigns it a "reliability score" out of 10. This score starts at 10, and drops if the server experiences any crashes or unexpected shutdowns. It even keeps a record of which programs were installed, and when, so you can diagnose if an installed program seemed to cause more faults. You can also set it up to continuously log the CPU usage of programs, to see which program is causing the 100% CPU utilization. 

I'm copying 600GB of data from external hard drive A to external hard drive B. Windows Server 2008 R2 has notified me that 100 files have filenames that are too long (i.e. >255 chars). Is there a utility that can allow me to search for these filenames, and manually shorten them? Shane. 

I recently setup a Git server where I work which required SSH and HTTP(S) access for public and private repositories. I first setup the entire thing manually which took some time, but then I came across Gitlab which seemed to solve all my problems - it's pretty much Github, but your own private hosted version. I'd strongly recommend using Gitlab over setting everything up manually, given that it has so many great features and is easily manageable. There's a great tutorial here: $URL$ However, if you do want to run Git through HTTP(S), you will need to setup Apache or Nginx to use the git-http-backend binary which does all the work. There are plenty of tutorials online depending on which configuration you decide to go with. 

The and variables are not required as they have default values, so you do not need to include either if you don't want to. Hopefully this solves your problem. 

Your research is right. You cannot redirect with a CNAME because DNS does not work this way. You would need some sort of web server to accept the request and rewrite the URL to HTTPS. Some domain registrars allow you to do a redirect on their web server if you use their name servers. DreamHost may provide this service but I have never used them so I cannot say. 

This is not possible in the current OpsWorks system. Even if you could, it would require the server to have the OpsWorks agent and Chef to be installed, which is handled already in the OpsWorks AMIs provided by Amazon. A bit late, but I thought I'd give it an answer. 

The Amazon free tier is only valid for 12 months since creation of your account and it has quite strict guidelines in terms of what you can do - if you've stuck by these and still being charged, it's best to contact Amazon billing and ask them why. It really depends what instances you're running, what AMI you've used to launch the instances, how much disk space you're using, how much I/O activity you have going on the servers, how much bandwidth you're using and if you're using other Amazon services. Without being able to see the activity statement, it's hard to analyse it. Usually if you give the Amazon billing department an e-mail, they will be able to help you out quite well. If you have anything more specific about the usage though, I'm happy to help you answer it.