If you used the IDE to perform the restore and not TSQL then you should get in the habit of verifying what server you are connected to before hitting ok. If you use TSQL then you can utilize registered servers in SSMS that will let you color code you query tabs. So you can use something like red for your production servers to indicate you need to be careful. 

If your current database is made up of one data file and one log file then that is how it will be restored. What purpose does the network admin in doing this if the current database is one single data file already? Does network admin understand how file structure works in a database? If you just wanted to split the tables and indexes between multiple files you would need to manually create each in their own file group, assigned to its own physical file. Then you could import the data into those tables. The other way is to do that same thing after you restore it. Outside of that if you involve partitioning tables between files it becomes more cumbersome and involved. 

Mind you I am not a PostgreSQL user by any means...but it looks like you are trying to run an tool through a PowerShell script that I don't think is going to work. The most common method of connecting to databases with PowerShell is using .NET classes to make a connection, then run the query, pull the output into a PowerShell object, and then put that data in whatever format supported by PowerShell. One thing you can try is put your call to the as one command. I am not sure if this works but I would think you should be able to do something like this: 

The gist of what I understand from the above... The files in this directory are log files for the database. However they may not the be the same type of log files you are thinking. These are actually tied to the LSN number of the associated transaction that modifies the image. You will basically get a file generated for before it was modified, and then one for after it was modified. The garbage collection process will keep up with which of those files can be cleaned up and are no longer needed. This is a feature built into FileStream and is handled internally by SQL Server. One of the methods that can cause this process to kick off is taking a or backup of the database. The article above shows taking a backup and issuing a . You may not see all the files removed immediately but you should start seeing a few go here-and-there. Something to note based on the large number of files being generated is to increase your log backup frequency to account for the activity. As well it is best practice to keep SQL Server files separate from the OS drive, prime example of why in your case. 

Your only option outside of setting up a separate process on another server is to configure the recovery actions for the specific service. This is done on the Windows side not SQL Server. Some folks will have it run a program/batch file that shoots an email or other such things. It is up to you. Overall, if I did not have monitoring in place to handle this I would say just build a small PowerShell script that just sends an email if the service is found in anything but running and run it with a Scheduled Task on the server. Adding the above recovery options is only if the service is stopped unexpectedly, it will not cover if some program or script is run that gracefully shuts it down without your knowledge. 

I use SSMS more than I do VS2010 because it is there when I install SQL Server. That is probably the most important thing why SSMS is used more than VS2010, IMHO. I did also find that vendors like Red-Gate are putting out tools that integrate with SSMS and not necessarily VS2010. This can be positive in that it allows you to improve SSMS where Microsoft has not. One example of this is Red-Gate's SQL Source Control, which is an add-on to SSMS that allows you to connect SSMS up to your company's Source Control system whether it be Visual Team Foundation or what have you. VS2010 has this built-in but compared to the price of Reg-Gate's tool I just saved a bunch of money without having to buy VS2010. I think overall it comes down to preference, you work with what you are comfortable in. If you are training a new person and you train them on VS2010 then that is going to become their preference because they know how to get around with it. If you started playing with SQL Server 2012 you may have noticed that SSMS is getting the makeup of VS2010 slowly but surely. So eventually you may not be able to tell the difference between them. 

EDIT: One additional requirement that might affect design: this package could will be called at minimum every 15 minutes. Example: 

The way you speak of only needing the primary file group, it sounds like to me that you only want the database structure and a very small amount of data. I would suggest if you just want an update of the database structure (objects, tables, etc) to simply script the database out with all the objects. This can be done quickly and easily with PowerShell. Then the data you actually do need, that cannot be regenerated, simply export that out (a PowerShell script could do this too). I am sure the export file can then be compressed down to a small size and then transfered over the connection to your site. Of course just like this one, @gbn suggestion could be scripted so it would be up to you to determine which option takes the least amount of time. 

There are none. You are not going to find anything that can show on paper proof that you need access. There may be scenarios that you can show where it can prove beneficial in turnaround time (or ROI) for solving an issue, especially in a mission critical server. However that is all based on opinion. I have supported SQL Server VOEs for a handful of years now and have not been provided the read access through VMWare tools. I have one particular environment where I have direct access through VSphere but that is only because that is the only direct access to the server that is available because of the network location I work from. I can say that being able to see that data from VMware is handy, but it is only for two or three servers. If I really want to get the configuration and performance information on a VMWare infrastructure I can just go to the database that stores it, as this is usually a SQL Server database. To the answer that mentioned Perfmon metrics are not accurate, that is not true at all. There are a few counters that may be off and not show the true activity, like some disk counters, but overall Perfmon is going to show you how the guest OS is operating. There are tail signs that if the underlying host is pulling resources away you will see this through performance counters. This is data I use as ammunition to the VMware admins to explain why I see such and such behavior. The particular counters pointed out in the question are actually only available if enabled through VMWare Tools. Those are special ones that can be very helpful, but I general don't see them. As well not having that access shows you can work with other members of the team in order to solve a problem. If you require access simply because you do not have a desire to work with them, then it is a personal problem that you have to work out yourself. 

Passing a variable to that SQL Agent step in that manner is not possible considering it does not know about the variable itself. A variable in T-SQL has to be defined/declared before it can be used...hence the error that it is not declared. Your further consideration is actually the proper way to do that with PowerShell. You would include looking up that value via PowerShell and then insert it into the T-SQL definition of the SQL Agent job. Now, an alternative that I would just note (because it is built around folks that are not day-to-day DBAs) is dbatools PowerShell module includes commands that are "Ola-aware". We have a command that you can use to deploy the complete Ola maintenance solution. You would use to deploy the jobs, and it includes a few parameters that allow you to customize the cleanup time, backup directory, etc. You can also utilize a few other commands to manipulate job schedules () according to your needs, or even based on a group of servers. SMO is more easily accessible in the module so grabbing thinks like the backup directory or the root instance directory can be done with a few lines: 

The only thing I am getting right now to figure out is an error on . Which I do have the March Preview of SSMS 2016 on my laptop, not sure if that has something to do with this or not. 

As labeled the SMO object outputs in "KB", you can modify this to be the measurement of your choice by just putting the abbreviated reference for it. So if I wanted "MB": . In the function , the brackets "[]" mean the parameter accepts an array of objects. So if you have your servers listed in a file you can call the function like so: 

I hope you tested your code! T-SQL syntax changed a good bit from SQL 2000 to SQL 2008 R2. I have supported many databases that broke due to some of these changes. Also note that Express has a limitation on how large a database can grow, 10GB with SQL 2008 R2 

You are receiving that error message because does not have the proper permissions. In order for that account to grant permission to another user you will have to grant the security permissions. You can review the documentation for here to find out what exact permission is needed. 

This is likely left over from SSMS 2016 in the sense that included the module, but MS was not placing it in version folders. Which modules from the PSGallery generally do now. If you check you will see which path they go to. I have SSMS 2016, SSMS 17 and the module from the PSGallery installed on my Windows 10 machine. 

This is a bit more than what I usually use but in case someone else comes across and wants to use it. The equates to in a DOS prompt and the flag simply just has it return or . This will default to 4 pings so setting just makes it do it twice instead. The variable is a method used to state that will accept an array of server names. So an example call of this function could look something like: 

You will need to adjust your script to call out the property returned by the AD command. So the following adjustment should work: 

Well if you are wanting pretty charts and all, there are not to many free ones that I know of that will automagically do all that you ask. In regards to PBM (Policy Based Management), you can run checks against SQL 2000, 2005, and 20008 instances. So I am not sure where it comes up short for you. With your primary requirements, most of that could be done with a simple PowerShell script that is run from one central server. I am sure a Google/Bing search on "sql monitor scripts" will come up with a load of things to look through. The desired items, however possible, might require a bit more time and effort to script those but is possible. 

I don't do much T-SQL work but from reading documentation... This is by design, as stated in the BEGIN TRANSACTION: 

I would expect this might be easier to do with Extended Events but have never tried to transfer this method over to Extended Events. I am not sure if SQL Server 2008's version of Extended Events opened up access to client level errors as 2012 and higher does. The above is just a quick and dirty method that still works. 

Publishing a report has not changed much in SQL Server since 2005. Developing a report changed when SQL Server 2012 came out just with the tool that you would use. I would suggest you start out by going through the Stairway To SQL Server Reporting Services by Jessica M. Moss. After reading through that series I would suggest getting a book on SSRS, anything by Stacia Misner on the subject is a good start.