Has anyone seen this behaviour or can say something about the strange "messages" on the console? I have never seen something like this and even do not know how I should describe this for a Google search. At the moment we have no very good idea what should be done next as it could be everything. Thanks in advance! 

We are running a KVM node which is crashing irregularly showing a very strange behaviour. The interesting thing is that we already had this problem with another node which crashed every 1-2 weeks. As we could not find a hardware issue, we began to migrate the VMs to a new node. About one week after we had migrated 50% of the VMs, the new node crashed while the "old" one is running fine since then (uptime 3 weeks, we have not seen such a great uptime for months). When a node crashes, we sometimes see these strange things on the Supermicro IPMI: 

A short update on this: After upgrading to the newest LTS kernel (4.4.39) the server is stable. Uptime 19 days now, so I think we got it. Although we do not really know the root cause, we think the CentOS 7 kernel (3.10) might be too old for some very modern hardware. As we can not deliver a helpful error message (like a kernel panic in the best case), we decided to not report this to the CentOS developers. 

What we never saw was a kernel panic or at least some messages in the logs before the crash, there is complete silence until suddenly the lights go out. As the problem "moved" from one server to another (a brand-new machine), there are only a few options left in my opinion: 

Rename all the *.config files in the Reporting Services directory to *.config.bak (so the installer thinks there are not present) Run the Repair option of the installer Configure the whole Reporting Service from scratch Delete all encrypted content (via RS Configuration Managers Encryption Key options) to avoid the rsReportServerNotActivated (due to no catalog access) error Re-Publish all the Reports 

I want to achieve the following: There is one DMZ-proxy over which we want to access multiple application-servers, represented by mod_jk workers. The web-context needs to be the same on all application servers, since they run the same application. We just want to access different servers via a additional "folder" within the URL. e.g.: some-fancy-url.com/hh1/some/application/ --> some-fancy-url.com/it1/some/application/ --> So depending on that keyword between URI and context we want to forward the request to different workers; the application server needs to receive the request without this keyword though. How can we achieve this? mod_rewrite I suppose? 

so far, so good. Now I want to have my external VIP (XXX.XXX.XXX.XXX) as a source IP on external sites. So when I run like curl ipinfo.io/ip from one of my hosts in the network, my VIP should be returned. At the moment, I get a physical IP of one gateway (ZZZ.ZZZ.ZZZ.ZZZ). Is this possible? How? Could I just set the default gw on the gateway to the VIP? keepalived config 

We have 2 Apache webserver behind a load balancer wich are connected to 2 (JBoss) application servers via mod ajp. To those webservers, mobile devices connect via a REST API. In our performance test we rather quickly ran into a lot of NonHttpResponse: errors which we identified to be coming from mod_reqtimeout: 

pinging the returned IP has 100% packet loss. I also did various traceroutes, here's the one to Google: 

This problem belongs to a KVM node on which the VMs get their storage via LVM. So each VM has it's own Logical Volume. Every night some VMs are backed up (snapshot - - nothing special). However, last night this somehow fucked up the LVM system. 2-3 minutes after the second backup started, the kernel began to log "hung tasks" - in short it reported three qemu-kvm processes as hanging and the dd process. At least one of the VMs (which is a managed server, so monitored by us) went down - to be more precisely: It was still running, but the services did not answer anymore. VNC showed hung tasks within the VM. After a hard reset (and a migration - see below) the VM was fine, but the process never terminated ( does nothing) and commands like don't work anymore - they just give out nothing. The also can't be restarted and every process which belongs to LVM can't be killed. They just hang in disk state forever, while the node generally runs fine. The VM which went down had to be migrated to another node as also didn't work anymore - "device or resource busy". But the other VMs also keep doing their job. We had this on another node a few weeks ago, where the "snapshotted" VM also went down, performed a kernel upgrade from 4.4 to 4.9 (as we had to reboot the machine anyway) and didn't see a problem like this again. But as the node which showed the problem today has got an uptime of two months, this doesn't say that this is really fixed. So - can anyone see more in this logs than we do? It would be greatly appreciated. Thanks for reading!