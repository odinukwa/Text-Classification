There are also results for monotone circuits and for a few nesting of majority/threshold gates. You can find references and more result in the second part of S. Arora and B. Barak, "Computational Complexity: A Modern Approach", $URL$ 

Amplifying lower bounds by means of self-reducibility by Eric Allender and Michal Kouck√Ω, Almost-natural proofs by Timothy Chow. 

Generally complexity theorist prefer to use as little formalism as possible. $\mathsf{IP}=\mathsf{PSpace}$ is on the list here but it doesn't seem that it has been verified with a proof assistant. I doubt that complexity theorists would be interested in writing formal proofs verifiable by proof assistants unless there is very good incentive to do it. Making proofs more formal might make it easier for proof assistants to verify them, but it makes them much less readable by humans. But if you want more detailed analysis of the proofs, some results and theorems have been studied in bounded reverse mathematics, you can find more on this in the papers by Steve Cook and some of his students, in particular Nguyen's thesis. Also Alexander Razborov has a few papers where he studies the required theories to prove complexity results like switching lemma and Smolensky's proof of Razborov-Smolensky. 

i.e is the uniformity of $P$ used in such an essential way that the barrier will not apply to it? (By essential I mean that the proof does not work for the non-uniform version.) So here is my questions: 

Another interesting topic is on performing computation on encrypted data. For example, you can have your data encrypted and yet use cloud services on them. There are mathematicians and computer scientists working on this, however the current constructions are not efficient enough for practical purposes yet (AFAIK). The keyword to search for is Homomorphic encryption. For example, have a look at Vinod Vaikuntanathan's expository article "Computing Blindfolded: New Developments in Homomorphic Encryption" for FOCS 2011. 

The following question was a research exercise (i.e. an open problem) in R. Graham, D.E. Knuth, and O. Patashnik, "Concrete Mathematics", 1988, chapter 1. It is easy to show that $$\sum_{1 \leq k } (\frac{1}{k} \times \frac{1}{k+1}) = 1.$$ The product $\frac{1}{k} \times \frac{1}{k+1}$ is equal to the area of a $\frac{1}{k}$by$\frac{1}{k+1}$ rectangle. The sum of the areas of these rectangles is equal to 1, which is the ares of a unit square. Can we use these rectangles to cover a unit square? 

If the class of structures is infinite there are going to be uncountably many properties. So there is no computable language that can capture all of them since any computable language is going to be countable. If the class of structures is finite, there are (essentially) finite number of properties corresponding to the subsets of the classes. Consider binary numbers where bit $i$ represents whether the $i$th structure has the property. 

As Carl said, Kolmogorov complexity is defined mainly for finite strings. It can be extended to other domains of objects but one needs to fix an enumeration of those objects. (An enumeration is a partial function from finite strings to those objects, e.g. Scott mentions two partial enumerations: the language decided by a TM represented by its encoding, and the language recognized by a TM represented by its encoding). If the domain of objects is uncountable then there is no enumeration that covers all of the objects in the domain so there will be objects left out of the enumeration. For the objects in the enumeration one can define the Kolmogorov complexity of an object as the minimum among the Kolmogorov complexity of its names. Those outside the enumeration will have no names (w.r.t. that enumeration) and one can say they have infinite Kolmogorov complexity. So if you have an infinite domain it will have objects which don't have finite Kolmogorov complexity. However, the choice of the enumeration is important here and easily change whether an object has a finite Kolmogorov complexity or not. Without fixing an enumeration the Kolmogorov complexity of an object doesn't have a meaning. 

The importance of Godel's theorems is not that $ZFC$ can't prove its own consistency but rather the weaker result that elementary methods (assuming that listing these methods is easy, i.e. recursively enumerable) cannot prove all elementary results, in other words, we need abstract objects even for doing elementary number theory. Hilbert wanted to show that although abstract objects are helpful for elementary mathematics in practice, they are not essential and can be avoided (at least in theory) if needed. But Godel's first incompleteness theorem already shows that this is not true. (Here elementary can arguably be identified with unbounded-quantifier-free formulas or $\Pi_1$ sentences.) 

$\mathsf{IE_1}$ doesn't have computable non-standard models. (George Wilmers, "Bounded existential induction", 1985) Any theory that contains it will not have computable non-standard models, e.g.: $\mathsf{I\Delta_0}$, $\mathsf{PRA}$, ... On the other hand, $\mathsf{IOpen}$ does have computable non-standard models (J. C. Shepherdson, "A non-standard model for a free variable fragment of number theory", 1964). Any theory contained in it will also have computable non-standard models, e.g.: $\mathsf{Q}$. The threshold(s) of having a computable non-standard model is somewhere between $\mathsf{IOpen}$ and $\mathsf{IE_1}$. There are various principles that one can add to $\mathsf{IOpen}$ and see if the resulting theory still has a computable non-standard model. It is known that $\mathsf{IOpen}$ plus cofinality of primes and Bezout axioms has computable non-standard models. Over $\mathsf{IOpen}$, Bezout implies normality and GCD axioms and is provable in $\mathsf{IE_1}$. 

To be meaningful the question needs to assume that $P$ and $P'$ is expressible in the language (otherwise proving it does not have meaning). In that case your question is equivalent to the following: 

I am looking for references about efficient type theories, efficiency in the sense of computational complexity, and type theory in the sense of Martin-Lof's type theories. Has there been any studies with the goal of creating efficient versions of these theories? Note: I am familiar with Stephen Cook and Alasdair Urquhart's theory $\mathsf{PV^\omega}$ and their "Functional Interpretations of Feasibly Constructive Arithmetic" paper from 1989 (doi:10.1145/73007.73017), but that is type theory in the sense of Kurt Godel's System $\mathsf{T}$. I am looking for feasible type theories in the sense of Martin-Lof's type theory. 

Yes, there is. See [Cob64]. The idea is to replace primitive recursion in the definition of primitive recursive functions with bounded recursion on notion. Another more delicate approach is taken in [BC92]. 

You may want to check "Dynamic Ordinal Analysis" by Arnold Beckmann which is an attempt to define a finer notion to classical ordinals that can be used ti distinguish between complexity classes.