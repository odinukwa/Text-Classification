As only you are aware of the word you may keep that word as a seperator. And at your application end split this word, if you get any other data after seperating 'clops' from the return type then treat it as a real return data. By this way you can ignore/suppress the warning that you get. 

If this works, try to take a mysqldump and keep it safe side. And recreate a fresh installation with correct settings and remember to remove the entry innodb_force_recovery. If this doesn't work, increase the value and see if it works. Since I'm not sure about the history of this server. You may need to verify your innodb_log_file_size set correctly as per recommended values. 

From Mysql 5.6 onwards you can explicitly mention the partition to be queried and it can go on further querying the data based index within the partition. I.E. 

User accounts are again stored in tables in terms of rows and columns and also its respected permissions under mysql database wherein it has primary key as host and username. So, I don't think so you have any limitations. But you have something called max_connections that has to be set to a value, say it has 100000 as max that you can set. Then keeping each user connection in mind then we can go upto 99999 users connected to it parallely. My question to you is. 

I used examples on value and node, because you provided the code with those two functions only, if you want to read more about it please visit this Hope this simple examples give you an idea of how to query xml types 

will show you the most CPU extensive queries which you might want to optimize further. Optimizing these queries you might find missing indexes, outdated statistics, Non sarg-able queries which are real issues that are behind high CPU usage. Its not the only blueprint how to fix CPU problems but i hope it gives you a good start! 

Monitoring CPU usage using task manager is not really a reliable source. There are many other(such as core OS activity,device drivers) non-sql processes running in the background that could be adding extra overhead without you even knowing. PerfMon is the tool you should reach for in these cases. Processor/%Privileged Time, Processor/ %User Time, Process (sqlservr.exe)/ %Processor Time Will give you an idea of what is actually happening with your SQL server, without explaining each of these counters, turn on description checkbox and read from there, but it will essentially show you the ratio of SQL Server vs Other processes usage. Even though its easy to spot, it is not so easy to diagnose. There could be other "hidden" issues that are indicating that the processor is the problem. Such as having lot of compilations/recompilations, which are issues related to non-parametrized queries or forced recompilations. You can find these metrics in Perfmon: SQLServer:SQL Statistics/SQL Compilations/sec, SQLServer:SQL Statistics/SQL Re-Compilations/sec. SQLServer:Plan Cache/Cache hit Ratio Indicates memory problem, but excessive page flushing in/out of memory also add extra CPU usage. DMVs can also help you diagnosing the problem. 

Have a plugin to monitor your load avg/CPU/IO. If any one of them seems to be breaching you may trigger a script to gather details for mysql and server processes and save it to a log_file with timestamp name. 

First lets understand that to take an incremental backup you need to keep full backup on the server where you are taking incremental. Though you take a full backup on SERVER(A) you may push to AWSSERVER(B). It doesn't matter, but inorder to take an incremental you need to keep the full backup copy as source backup. UPDATE Below is the approach for incremental, you will be able to get your answers whether the idea you have is a feasible one or not. 

Kill all processes for mysql in ps -ef | grep -i mysql If you see any any ibdata file generated under your default data directory you may remove them also iblogfile1,2 since its a fresh installation no need to safe backup them. chown -R mysql:root /var/lib/mysql chmod -R 775 /var/lib/mysql also the same permissions to /etc/my.cnf. Now try to start the instance by issuing /etc/init.d/mysql start and tail -f /var/log/mysqld.log. It should start without any issues if you get any error message . 

Dynamic quorum basically dynamically adjusts votes depending on available servers. Each time when one of nodes goes down, dynamic quorum will remove the vote from that node. In your scenario you have 2 nodes only and dynamic quorum will automatically remove the vote from your passive node, so the 1st node will have the majority of votes. In planned maintenance scenario when you are shutting down the first node quorum will transfer the vote from first to the second, and remove it from the first node. However in scenario where first node just crashes quorum does not have time to transfer the vote and your second node wont get to vote, which basically will just shut down your cluster. Therefore in scenario with 2 nodes only, it is recommended to have a witness. 

When you are all set and done, you can transfer those views from ViewSchema to dbo or some other more meaningful schema with command: 

Just like the master key, you need to back up this certificate as well. guide Now we are creating a user with certificate which will have all the rights: 

This ensures your backup data consistency and can avoid situations invalid backup data at the time of urgent restore. 

Doing by this way the SELECT statements are performed in a nonlocking fashion, but a possible earlier version of a row might be used. Thus, using this isolation level, such reads are not consistent.When you say not-consistent it means recently changing records i.e.. DML transactions that are currently in process will not be read. I assume which is in your case it is acceptable. This is also called a “dirty read.” Otherwise, this isolation level works like READ COMMITTED. If I were to be you, the below order is what I follow. 

Below url shows show innodb status \G and show open tables; at the time spike. And this reduced within 5 minutes. Sometimes rare scenarios like once in two months I see the processes takes more than 5 to 8 hours to drain normal. All time I notice the load processor utilization and how it gradually splits its task and keep monitoring the process and innodb status and IO status. I need not do anything to bring it down. It servers the applications promptly and after some time it drains down to normal. Can you find anything suspicious in the url if any locks or OS waits any suggestion to initially triage with or what could have caused such spikes ? $URL$ -> "show innodb status \G and show open tables at DB spikes." Also there are some concerns that I would like to share with you. 

After delete trigger is executed after the record has been removed from the table. Joining a table Emp will yield no result because record with that ID does not exist in that table. Also note that inserted table will always be empty in after delete trigger. 

Your second query runs within a second because it does not have to go through each record in (130 million record table) and compare whether it matches the record from a temp table. And there is not much you can do when you are using a temp table with a single record within. One solution would be to save it within a variable and use it in where condition without joining it, but you said temp table will contain more records. Note that ,it does not necessarily mean that more records will increase your execution time. With more rows in temp table, query optimizer will use Hash Join which could possibly give you even better results. However you could optimize your query like this: 

Looks like you are at the beginning of your investigation. Probably you should dive deeper in below areas to nail the issue. 

You may give it a try using percona backup alpha version for Windows - Download_Link . Below are the steps after installation in Windows Bash. 

Here is my opinion. As long as you are going to use innodb engine it doesn't matter which one are you in. Provided Maria gives you the option of multiple writes to master-master architecture using Galera-cache. But as your TPS goes beyond a particular point, you might face issues in getting writes paused due to flow-control. Hence you may use innodb engine let there be 2Nodes using MariaDB using GaleraCache Cluster or simple Master&Slave. But have your applications writing to one node only either case. For my.cnf you may use percona tool wizard to generate as per your requirement. 

Any best recommendation variables for my.cnf on percona server on a high 256GB RAM and 32 CPU with 4000 IOPS? How about below ones? 

Like i said there are other ways, including roles but these two could get you a job done. Just remember you cannot track a user that executed procedure by specifying 'execute as'(login is possible tho). Also creating a certificate/database keys can be a headache when you`re migrating DB, or simply restoring it somewhere else. 

2) This one will show you records that are placed in a log buffer, in what state transaction was, how many records are logged, size in bytes ,and a query that executed it. In a nutshell it displays all inserts/deletes/update from an active transaction that are not committed/rolled back yet 

Also if you are getting a lot of records from first procedure, consider using temp table instead, table variable is not the best choice. And lastly you can always diagnose procedure execution using Extended Events, and log it to a table/file if it exceeds certain threshold, along with execution plan,and waits so you can compare it with the regular executions. 

Reference Issue : Mariadb - Variable 'innodb_log_file_size' is a read only variable Reference Chat: $URL$ 

By this way DB performance will look good, quick recovery and data availability 24/7 and less time spent in-case of any maintenance activities. 

If you ask me there is no better way to fix this kind of issue with handlers or exceptional handling declarative statements. But you may try to give a constant value that will return even if your answer should be NULL. Let that constant value be a wierdest one that you can expect from the fucntion that could return. Add that constant value to your return function. Disclaimer: I haven't tested it, but you may give it a try. Let me know on your results. In this case I'm mentioning 'clops' as the constant. Try below changes to your function only under select clause and see. 

Have you tried mysqldump using --single-transaction ? In this way it doesn't lock the table for threads doing SQL,DML operations provided no DDL statements should be issued and you will get the backups for conistent states only. This will be efficient only for innodb tables. Below url might give some insights about the option. $URL$ 

Note that specifying database name you are limiting user only on that particular database. To build your own permissions on a user/role take a look at this pdf: SQL Server Permissions Map 

Executing this command, will give you no results (since we added that our data equals empty string) however, if you query the table ,table will be empty. Where as in sp_executesql you are explicitly declaring parameters and will be compared as it is So in the example: 

Depending on transaction level, those queries will block writes. Serializable & Repetable Read transaction isolation level will hold S locks (for the whole duration of the transaction), which are incompatible with X locks that are required for writes(inserts/updates) . And yes it makes no difference, if you check execution plan you will see that they are exactly the same. So in order to prevent locking and blocking implementing different kind of objects wont give you no results, but performance boost(if used stored proc). Instead you should change isolation level 

Note: I have used SPLIT_STR function to exactly cut for the said occurence position by said delimiter value. In unix it is like -> Returns "Dick". Try it on your application requirement and see if this suits, I guess I dragged it long. I wish you to get someone with a decent fix. Use my fiddle -> $URL$ for quick testing purposes. All the best!! 

What I'm I missing here? I need data greater that ISODate("2015-01-11T00:39:40.121Z") I know many forums have different answers for the same issue. But non resolved. Need a different point of view on this. 

You may try doing this by applying archival logs in Mysql which is called as binary logs. Have your instance my.cnf in the below manner under [mysqld]. 

But looking at your table definition you have secondary indexes almost on all. You might want to verify on performance of all your selects. Or tune them inorder to add the partition name or partition key as a mandate clause in where clause. So its better you take perf against selects and updates you use and then think whether is it feasible to go for partitioning or archiving the table. Also if you are anyway going for partition, you need to define the partition in such a way older partition can be removed. So I would suggest to go for