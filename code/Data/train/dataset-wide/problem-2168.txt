Microsoft documentation mentions they're removing it, it says "Next Version" but i assume this was written for a previous version. They advise to use maintenance plans in the future. Microsoft Documentation for sqlmaint Edit: Last update on that article was 03/14/2017. But given all the examples point to it being originally written with SQL Server 2008 in some and edited with later versions as examples (it's all over the place to be honest), it's probably safe to say the note at the top is wrong and it's actually removed already. 

This should hopefully work providing the user submitting has the correct permissions, you could hand this off to a stored proc maybe executing as higher level if needed. Get the most recent SQL statement from the current SPID. Might not always be 100% if lots have been executed at once but worth a go. 

You could pull more info from exec connections too if you wanted. EDIT: This above seems to only return the trigger. For completeness you can get it working like this example i mocked up, you could include query stats and plans to be logged too... 

By getting the last insert into that table from sql queries submitted. However this isn't as efficient or accurate as other answers, but as i said if you want the extra info to log like plans etc this could be handy. It helps to see other thought processes for fun. 

I'm drawing up a proposal for upcoming infrastructure changes. This will include a production server and reports/data warehouse server, each with Always On. To keep hardware and licencing costs down, is it possible to run in a configuration of Server-A running Prod-AG Primary and Rep-AG Secondary, and Server-B running Rep-AG Primary and Prod-AG Secondary? I presume each server would need 2x of the following WSFC instances,sql instances, AG's, listeners, DNS names/ports. I hope this makes sense, here's a diagram of what I think it will looks like. 

Try this using CTE, you're only calling the function a minimal amount of times this way. Could potentially be improved if you can filter the first queries somehow if appropriate. 

I finally got the right balance of performance and accuracy. The below will return the top 100 queries based on average total elapsed time. Its for SQLCMD mode, you'll have to change that if you don't want to use it in that setting. 

In the case of a fail-over on either node, the workload/business need isn't that great that running off the same server for a couple of hours would be a major issue. I've only found a couple of mentions of a similar setup that kind of worked but no definitive information from Microsoft or anyone who's successfully ran this setup. SQL Edition will be 2017, most likely standard, I don't think we'll be approved for Enterprise. OS will be Windows Server 2016 Core. 

I has this issue, this tutorial did the trick for me. $URL$ Should be noted an easy work around for me was just to change the account to NETWORK SERVICE in config manager, if you're not fussed about using a dedicated account for security reasons it will likely work for you also. 

how about going the OUTER APPLY route. You can modify the inner query to handle nulls differently, i've just disregarded them but if you want to handle them you could use ISNULL or CASE in the future. 

If this is a one off and this isn't production, you could put the database into simple recovery mode, do all the deleting you need to do. Shrink the log file at this point if you don't want it to be 500GB. Then put it back into its previous recovery mode. 

I believe this is to do with the plan cache. Just ran a little test on my end and I can replicate it. To remove the table from tempdb works, obviously target the plan handle for the stored procedure. You could do this but it would just come back again and freeing the cache all the time generally isn't the best idea. If you're concerned about it taking up space you can run this query I wrote for a monitoring tool that should work. On my tests it always shows 0KB after the SP has finished. This leads me to believe it just keeps the shell of the table there for some internal use. So it seems harmless to me. 

This has the advantage that all your data is in one row, but also has the disadvantage that it isn't immediately apparent from the row what each value means. The first method can always be joined to the field list to give a good definition of the data in each custom field. In addition, what happens if some company requires 100 custom fields? In the above example, you'd be making changes to your data structures and code whereas in the first example, you'd never have the issue -- customers could have infinite custom fields. I've seen it done both ways, and both ways work. Both ways have their downsides and upsides. The first is far more scalable, but harder to get in to columns (instead of rows). Everything's a trade-off. Hope that helps some! 

There are many ways to export data from Oracle and automate the functionality. Be sure to understand exactly what the data export is being used for, though. If it is for interop between systems, then export in a format your receiving system can understand. If it is for backup purposes, go for the exp/expdp (data pump) method because a database backup needs to store much more than simply data. (Better yet, just use RMAN. But I know many DBAs who also like to do full db exports on a regular basis as well.) You can use a number of tools to accomplish this, TOAD being the one that springs to mind. It has a powerful data export tool that supports scheduling. There is a free version available, but I am uncertain if it has the scheduling functionality. Worth a try, though. Alternatively, use the tools already at your disposal: SQL*PLUS, PL/SQL, and cron (or the Windows Scheduler if you run Windows). For a good example of how to write a PL/SQL routine that exports a table to a CSV file, see $URL$ . If you use a procedure, you can schedule it from Oracle's own job scheduler. You can do similar things purely with SQL*PLUS as well (without getting into writing a procedure), but it isn't quite as flexible, but for what you want, it might be just what you need. SQL*PLUS is also easily scriptable so that you can then call it whenever you want via cron/Windows Scheduler. SQL*PLUS works really well on its own to create good fixed-width reports, but it is possible to do HTML and CSV as well. XML will probably require a procedure of some sort, but I'm not an export here, so SQL*PLUS may not be perfect here (it will output to HTML, though, so that might be good enough). If exporting to Excel, remember that the current versions use XML as their file format, which makes things easy (in one way) and painful in other ways (like needing to know beforehand how many rows you're going to have in the output file). Regardless, with a little bit of work and the combination of two or three tools, you should be able to export your data in any format you wish on any schedule you desire. Hope that helps. 

Be really, really careful when mucking around in /etc/rc.d/; this is where your startup/shutdown programs live, so removing the wrong file will put you in to a world of hurt. (Unfortunately, I can't tell you which file is the one Oracle tends to use.) Second, Oracle's uninstall instructions always leave a lot to be desired, as does the actual process, which leaves just about everything hanging around on the file system even when it isn't necessary anymore. For removing Grid, IIRC (that is a big IF), I think you're in the clear -- pretty much exactly what I would have done. And, as to weblogic, same deal -- I'd have run the uninstall.sh and see what happens. I suppose you could nuke the entire directory at this point so that there'd be no chance of an old file coming back to bite you, so that'd be just about the only other thing I'd do. Good luck with the re-install... 

This only seems to happen with ONE table (all other tables are fine). So we tried recreating the table, but the same problem persists. Any idea of what might be causing this issue? 

This immediately made me think that a ODBC driver mismatch might be the issue, where SSIS is looking for the 32-bit drivers. So here is what I did: 

I am inserting values form a staging table in SQL Azure into a production table. Values in the staging table are varchar, while the target value in the production table is tinyint (it is an age field for humans, so I reckon that up to 255 will suffice ;) ) Problem is that due to a bug in the source system, we tend to get some values as -1 or -2 when they should be 0. This causes issues, since TINYINT only supports values form 0 to 255 (and also because this is factually wrong). I was wondering if there is some sort of case statement that I could use to convert values to 0 if they fall under 0, or perhaps if they fall outside the 0-255 range? 

Here is the answer. Even though the job was configured to run via a PROXY Account, the SQL Server Agent is still responsible for the job. I had a look and the SQL Server agent was configured to run under the Local System Account on that server. So what I did is to put the agent to run under the superuser admin account and it worked as expected. Now in this case the fact that the job no longer needs a proxy since the Server Agent itself is running under the ultimate account. However I appreciate that this is not the right way moving forward (even though this isn't my server and I hope I never get to touch it again!) I will be advising the customer to reconfigure SQL so every service runs under a dedicated domain account (i.e. created solely for this purpose), which is the way it should be! Now what I would love to understand is why the job would run as long as the proxy account used for scheduling the job was logged into the SQL server! 

The issue here is that I had to create a System DSN, which is actually odd because the SSIS job is scheduled to run with the account for which the User DSN exists, and it does work as long as I am logged into the server with that account. Anyway I am way too tired to try to understand why the User DSN didn't work. So I decided to go for a System DSN instead. It is not that I am against System DSN (on the contrary!), but it wasn't me that set this User DSN and I do not know the password for the credentials used in order to create a System DSN. So I had to be a little creative. I had to use Registry Editor and notepad to convert a User DSN into a System DSN. Here is how I did it: 

Is anyone aware of a SSAS (tabular) 2016 in which the number of records in a table are not fully shown in Excel? 

The solution was to change the processing in Visual Studio (VS) from Default to Full. The issue is described in detail here: $URL$ (see Cathy Dumas's reply under "Posted by Microsoft on 8/25/2011 at 2:02 PM") 

This is as far as I managed to get unfortunately. My skill is limited where it gets to do an INSERT with a condition with the NOT EXIST condition. How can I make sure that I only insert rows in the xxx.MyTable where rows with the same primary key (period AND genusId AND subjectId AND waitingStageId) does not exist already? 

If I connect to the SQL server's SSIS store (not the DB engine) via SQL Server Management Studio (SSMS) and I right click on the job in question that is stored under "Stored Packages\MSDB" and choose to execute it, the job will run without any issues. This happens whether I am using the local SSMS installed on the SQL Server in question, or if I am using a SSMS installation on a remote workstation. However if I schedule the job through the same SQL's server database engine, the job will fail -- both on a schedule and if I try to run the job manually. Now here is the puzzling bit: The job will not fail if I have on the background a remote desktop connection into the SQL server with that super user account (i.e. spadmin) while I run the job. By on the background I mean that I am not doing anything on this remote desktop connection except login in with the super user account. When the job fails, I get the following "Bad Gateway" error (see end of post) that suggests the problem is accessing SharePoint. However since I can run this job via the SSIS store with the same account for which the job has been scheduled, there is no doubt that this job is capable of running from the SQL Server. Server build: 10.50.1617 I am going mental here. Any ideas of what the problem might be? Here is the full error message for completeness sake: