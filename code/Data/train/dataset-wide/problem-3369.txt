Currently, what you're trying to do can't be done in a single operation. Move in S3cmd to the API is essentially a copy and delete all in one and it's a restriction of the copy operation. $URL$ 

I have the "OperationsGPO" applied to the "Biz-Users/Operations" OU This GPO is a User based GPO but it only applies to a single group of users within the Operations OU, and I ONLY want to apply the GPO to the user if they are logging into a computer in "Site2-Computers" OU Therefore I have added the required Operations users (not all of them) to a group "OpsUsers" and added the Site2 computers to a group called "Site2Comps" and I added those groups to the "Security Filtering" field of the GPO Since there are 2 groups defined in the Security filtering ("OpsUsers" and "Site2Comps") I am not clear on how they are applied -- do BOTH parameters need to match (AND logic) or does ANY of the objects need to match (OR logic)? Thanks! 

Keep in mind, I have no idea what your "containers" are actually doing, but it's assumed they would be able to route the incoming traffic out to the internet appropriately once they accept the incoming packet. Also keep in mind that once the outbound traffic leaves the container, the Docker engine will automatically do the masquerading for that outbound traffic. 

I've used flock a lot in the past to ensure that a process only spawns once (in case it gets hung up/etc) but I've never used it for two different processes. In this scenario two different scripts affect the same "temp" directory (the dir is clobbered at the start of each run). So I need to ensure the temp dir is not clobbered by one script while another one is still needing it. My goal: 

I have a tricky situation. I need a user to execute a specific command as another user -- However this command only works if the environment is fully loaded. Otherwise the command I'm running won't work, so I need to do a type command. Command works fine when ran from root: 

If ran as an active check: FAIL So, everything works perfectly from the command line, however, in Nagios console, it fails! 

Assuming each container is listening on a different port on the local server doing the routing, it seems like you could use the source IP as the deciding factor in which port (container) it gets routed to. For example, you have 3 containers each listening on separate ports 7771, 7772, 7772: 

So the only thing left over is the directory with the same 5x layers listed. This seems to be the reason why it's still listed on When I delete the folder (from ) then the repository is no longer shown in the This seems to be a method to purge it from the listing. However - what if an image has 2x tags, yet only 1 is deleted - is there a reason to delete anything from in that case? How would that be handled with multiple versions of an image? Obviously I can't just clobber the directory as the final method since, in the real world, there will be many tagged versions of an image. So this needs to be done intelligently. 

I have an odd scenario that I'm not sure how to get around. Normally this is doable with subnet ACL's - However they are not stateful. I need "reply" established packets to allow a return (like a typical firewall) I have a public and private subnet. Public needs to reach out to the internet so I have outbound allowed to 0.0.0.0/0 but I want to restrict outbound for specific subnets (10.100.1.0/24 and 10.150.2.0/24 for example) I could of course set this easily in ACL since it allows 'deny' but it won't allow reply packets since it's not stateful. Is the only option to control this with 'inbound' rules on the other internal subnets? That makes a lot more rules for our different networks when it would be much cleaner to just restrict it on the outbound. Any ideas are welcome, including re-architecting the whole thing (this is greenfield) The comment below asked to further clarify the environment needs, so here is the environment as it's being requested for me to build out: Think of it like a local and DMZ: 

Other than BOTO, many other AWS interaction utilities will often have a similar filter or query system based on instance tagging. Like Ansible, etc. 

Neither of the above work, it still asks for password. IT WORKS if I simply grant user full permissions: 

I currently have a WORKING SFTP login, using a private key for login and the user is chroot'ed into their home directory. Goal: Keep the user chroot but allow WRITE access to the relative chroot directory, without having to specific any path or cd anywhere. In other words, when the sftp user logs in, I don't want them to have to cd to another path in order to upload a file. 

I have a rather odd request. We have an Amazon VPC that is the primary network that my company controls. I have a client running a physical computer on a separate network in another part of the country. We do not control that physical network, so at this time I cannot install a site2site VPN over there for this project alone. That remote client computer is running OpenVPN Client and connecting successfully into our VPC. However, here is the problem, I need to send packets to the client since it's running a hardware dongle that (obviously) cannot run in AWS since it's hardware. It's essentially running a single service that needs to be talked to but I cannot initiate a connection into a client. I do realize that Amazon has direct connect and VPN services to connect a physical LAN to a VPC, however that isn't really possible in the scope of this project. I can easily create a new VPN server in the VPC if that's a requirement OpenVPN's client-to-client is the closes thing I can see, but that routes within the VPN server, so I don't think that will help because the machine initiating the request isn't a client of OpenVPN, it just sits on our VPC. One thing to note is that the connection to this machine is only lightweight traffic, no heavy bandwidth is needed. Is there some sort of tunnel that I can create? We would have to jump through some serious hoops in order to get the Gateway/Firewalls/Routers reconfigured at the remote site so that is an absolute last resort. EDIT: I wanted to add this for clarity: $URL$ The image on that page pretty much sums up what I need to do, however the PC on the right side isn't connected via OpenVPN, it's already on the same LAN/subnet as the OpenVPN server. 

In the server config file you utilize the item to push out routes to the clients. Ensure you have the routes defined in your server.conf. Here is an example: 

Docker service kicking you off SSH doesn't really make any sense. Do you have any iptables rules configured? Any other steps you're taking after provisioning the system? There has to be more to this story. Anyways -- You mentioned you tried on Ubuntu 16.04. Here is direct out of my notes and has worked perfectly for me several times on Ubuntu 16.04: 

Solution Background/Info: As Grant posted in the comments to the original question: Giving the logged in user or Group (Domain Users/etc) access to the share, and not explicitly defining the username/password in the command solved the problem. This also solved the problem with the GPO drive mount. Not defining the "Log in As" field and allowing it to auto-authenticate automatically with the logged in user worked. This DOES solve the problem, however this does not answer the root cause of why using a shared domain account as I was originally attempting did not work. Answer Summary / Step-by-step: