If you are worried about unauthorized data modifications, consider implementing an auditing strategy. As far as I know, there are no documented features that are of much help for post-change ad-hoc change detection if no auditing has been implemented. One, tedious way would be to restore a previous version of the database to a different a name. Script both the old and current databases into text files. Pick the relative complement of the scripts and you will get the changes. If there has been multiple changes on same a row, only the final result is seen. That being said, there are 3rd party tools available that can compare databases. There are tools that claim to be able to parse transaction logs too. ApexSql and Redgate seem to be popular picks. Whether any of these tools are any good, I cannot say. In addition, there is an undocumented fn_dblog function, which could be used to read transaction logs. 

Paul Randal (of the SqlSkills and fame) has a nice set of pre-corrupted databases and instructions how to work with those too. 

Develop the packages on development environment. Push those to the test and let production admins move packages into the production. 

Talk with your SAN administrator. That's the source that should be able to explain you with detailed information what kind of SAN you've got and what it is capable of. Have you got MPIO? What's the structure of your fabric? Sometimes it makes sense to use multiple LUNs for different files. Behind the scenes, there might be different types of storage available. A SAN with some SSD capacity might offer tiering. That is, the hottest data is elevated to SSD -backed part of the system whilst colder data is persisted on HDDs. Some models provide autotiering, in which the system decides what's hot and what's not. Some models require administrative action. HP EVAs provide VRAID levels 0, 1 and 5. Using those will spread a LUN on all the disks in an array. This increases reliability, but the cost with VRAID 5 is increased controller CPU usage. The SAN admin needs to balance increased storage costs against increased CPU usage. Aligning NTFS blocks with storage is dependent upon the SAN. If you are in addition using virtualization (Hyper-V, VMWare, whatever), that's additional complexity introducec. Your storage vendor is likely to offer white papers, best practices and consulting services about how to configure the storage for optimal performance. Network packet size is relevant - if you use iSCSI. For FibreChannel, IP settings are not relevant. 

Microsoft has a KB article about this behavior. You got a 32-bit OS with 32-bit application, so the process' address space is 4 GiB. AWE can be used to access more memory, but it is limited only for buffer pool usage. As per the documentation: 

Be aware that there are a few catches. For example, sequence values are reusable, are not unique by default and can contain gaps. Pay attention to the Limits part in the docs. 

This is because I think it is more efficient to get X, Y, Zth records in one query execution than in multiple ones. 

And say I want 3 indexes for this collection: one for the price, one for pages_in_book, and one for author. Also say I have 3 shards. Can I have each of these shards be responsible for updating one index each when a new document is inserted into the 'books' collection? I want to do this because I plan to make a write-heavy application. User actions would initiate the creation of multiple documents at once and each document would need to be added to multiple indices like above (but five or six indices in my real case). I could distribute the writes using a shard key (with luck here, the documents' writes and it's writes to the collection's indices would be distributed evenly to the different shards but I could get unlucky if no good shard keys) or distribute the writes so that each shard is assigned an equal amount of indexes to be responsible for (extremely more likely for there to be a near even distribution of writes this way???) 

Is it possible to delegate responsibility for specific indexes for a collection to specific shards in a mongoDB database? For instance, say I have a collection called 'books': 

I'd also like to ask about the idea of, for idea number one, creating a temporary table from the array and using JOIN with that temp table vs using the array. I'm avoiding using RAND() because it has been said it doesn't work for large tables. I wonder how large is large? Consider the idea that, at most, 600 posts will have to be excluded for any given user (so an array of 600 ids in the case of the first idea $idsOfPostsAlreadySeen... or 600 records in the seenPosts for a particular person). 

I have an api endpoint that returns X amount of random posts from a table called "posts". This endpoint is used by a mobile app. It retreives random posts by doing a SELECT COUNT(*) on the posts table and returning the amount of posts in the table. It then enters a for loop in which, at the start of each loop, a random number from 0 to the COUNT(*) is generated. A random post is then obtained using the handy OFFSET. This for loop goes until X amount of random posts obtained. pseudocode: 

I could store the ID's of already seen posts in an array IN the app and then send this array to the getRandPosts function. The getRandPosts function then uses a "NOT IN" clause. 

Now, for each call to a getRandPosts function I want them to always retrieve a unique post that wasn't retrieved before. For this current getRandPosts call AND FOR PASTS CALLS. I've thought of several ways of going about doing this: 

Synonyms have nothing to do with privileges. They are simply a way to simplify naming. The error you are getting appears to indicate that does not have privileges on the table. You'd need to grant that 

Presumably, you have a development and test instance of this database running on similar hardware with a similar data volume and the same database components installed, correct? And, presumably, you will be upgrading these lower environments (and testing that whatever applications use this database still function correctly), correct? Assuming that is the case, I'd time how long it took to upgrade the development database and use that as your estimate of the time required to upgrade the other instances. There are, obviously, a number of factors that determine how long the actual upgrade will take. My guess is that the downtime would likely only need to be an hour or two but you're much better off using the actual time required to upgrade dev. 

Can (and do) people successfully deliver projects that do this sort of thing? Unfortunately, yes, they do so reasonably often. Is this a good approach? No, it's not. You're basically taking your relatively expensive database and turning it into a relatively slow file system. If you really want to build a system that saves its state by serializing and de-serializing objects, you may as well use a file system rather than using a database. If you build systems that store data by serializing objects into the database, you won't make friends with your DBA. You'll end up storing redundant data. You'll end up with terribly inconsistent data-- any time shared data is updated, some objects will end up with the new values and some objects will end up with the old values. You'll make it impossible to do any sort of reporting on the data-- everything that anyone wants to do with the data is going to require someone to write additional code. That's a huge, huge issue in most enterprises because they want to do things like extracting data from one system to load into another system or to have a reporting system that can deliver reports from multiple front-end applications. Plus, as you point out, you're constantly going to have to deal with issues when you're evolving your data model. Are there advantages to this approach? I guess you can argue that it's pretty easy to implement the first version of the app. And it lets the developer completely ignore anything related to properly interacting with a database. I'm hard-pressed to imagine many cases where these advantages outweigh the numerous downsides to the approach. As for how you should deal with this particular professor, that's a separate issue (and one that's probably out of scope of this forum). If your professor is actively developing projects in the real world, he's probably not going to be terribly receptive to any argument from a student that his approach is fundamentally wrong (even if the approach really is fundamentally wrong). You may be better served doing your project the way the professor wants and learning the proper way to save data on your own (or in a different course).