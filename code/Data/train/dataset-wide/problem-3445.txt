Yea they didn't make this easy and it's still not perfect, but the v2 registry API now has the ability to delete images. 

Why did it say "Condition is no longer satisfied" and resolve the alert? My goal is to keep the alert open until someone resolves it or it's resolved through another condition/etc 

when developing Bash scripts, sometimes I'll run it in a Docker container, or on a VM to test it, but the ideal place for me to build out scripts is just on my local MacOS workstation. When it comes to Bash scripts, this has never been a problem so far. However today I noticed that the command behaves differently between Linux and MacOS. Example scenario, adding days to a date object: Linux: 

Route 53 is not required to host a website in AWS nor is it required to have name resolution to a service in AWS Wherever you currently manage the DNS records for your domain will be fine. Just point them to the IP or host address of the AWS site/service you're connecting to. 

I'm trying to find the history of container restarts. Of course the field on a will show the current uptime. However if I have a container with a restart policy such as and it's gone through several restarts - How can I check that restart/uptime history? If the docker engine doesn't natively track this - is there a known good method to handle this? 

So those directories don't match, you never actually mounted the htpassswd file If you write out to it will work since it's writing to your current directory, not to The following worked perfectly for me: 

EDIT: I've also tried setting the host definition to a standard ping check, and put in a service definition for the command. The host shows UP but the service is DOWN. Same exact error "Socket timeout after 10 seconds" -- The command line use is nearly instant with the UDP OK return. I am stumped. EDIT 2: I tweaked the debug settings, and have turned logging verbosity up to the highest value (2). It doesn't really tell me much, but it looks like Nagios is interpreting the command as expected... 

The config you have now is only going to make sure you only have a single simultaneous delivery to each destination. It's not going to actually throttle anything. I think adding this line to may accomplish what you want: 

Your script is being killed as a secondary effect. The log message indicates that cron itself is being killed. When a process with child processes is killed, it sends a SIGHUP to its children, causing them to exit as well. That's what's killing your python script. If the log line you list line is immediately followed by a line that says , then you know that something outside of init/upstart killed it and it was automatically respawned. If that message does not exist, then cron was killed by init/upstart, the same as if you ran "sudo restart cron" or "sudo stop cron". So your new research project is figuring out what is either killing or shutting down cron. 

Once the appropriate settings have been added to , run to activate the new interface. If, however, the new ip is on a different subnet, you need to either provision the ip on a physically distinct network interface or create a VLAN interface, depending on how your ISP is prepared to hand it off to you. That's a whole new topic. 

Then executing will execute your debugging command as root but still preserve the benefits of sudo (audit trail, not operating out of a root shell). Example: 

According to this guy it may just be faster to use , although I would guess it would be close to as efficient as compressing each file first before transferring. It should be faster than compressing the tar stream, as suggested by others. From the man page: 

The downside to the netcat approach is that it won't allow you to resume if your transfer dies 74GB in... 

Now accessing "hello.example.com" will load the site located in . Alternatively, as mentioned by Gabor, you can use mod_rewrite. The solution I'm going to outline assumes that you have no other content under example.com. 

You're on the right track with the cookie-based approach, but the initial check needs to be done using a process a lot cheaper than an Apache/PHP thread. I suggest an nginx proxy in front of your web host that forces redirecting and setting a cookie. Than only requests bearing the appropriate cookie are even allowed through to your PHP host. And as long as you're setting up a proxy, this relatively new piece of bot-detection software is pretty impressive: $URL$ I also highly suggest the DDoS presentation linked to from that page: $URL$ It covers anti-DDoS concepts overall and describes why they wrote Roboo. 

Update B - There's something in sharepoint I turned on netlogon debugging on one of the affected machines and found some interesting stuff. First, this is what I believe is a successful query being sent: 

It looks like you're being bitten by the forwarding delay. check out for details on how to tweak the various timing paramenters, but essentially what happens is that when brctl puts your bridge together it has to wait a little bit to learn the topology of your new network before it actually sends any packets. Long answer short, this might fix your problem: 

We have all of the logging turned on for MS DNS Server and see plenty of NXDOMAINs for requests of this form: Note that I am not talking about Those are working fine. Here is an error entry from the log: 

So I spent some time enabling and disabling these sharepoint services and watching the DNS queries go out. It appears that the User Profile Service is causing the queries for at least _ldap._tcp.deecee. I know the whole thing isn't sharepoint's fault; as I said earlier these queries are coming from all over the place. The ones for just _ldap._tcp.deecee, though, are coming only from our sharepoint hosts. So that adds another question. What is the user profile service doing that's causing the lookups to _ldap._tcp.deecee? It still leaves the question for the rest of our servers, though. 

Note that the client is requesting According to Microsoft's documentation, the proper request should be The requests come in from all of our AD joined machines. They include Windows 7, Server 2008, 2008 R2, 2012, and 2012 R2. Our DNS servers do have the appropriate SRV entries for and they do resolve correctly. So that's not the issue. A coworker opened a case with Microsoft and the tech finally claimed after a few days that this is normal. I don't buy it. Why is there no mention of this behavior at all in any documentation? So, Does anyone else see this behavior? Clients looking up SRV records for ? If so, are they getting NXDOMAIN results? Any ideas how to fix this? Thanks in advance. Update A - There's more In my domain I'm seeing these invalid queries in order of most common: 

The Ubuntu version is identical to the Debian version, so this is an example of a package that is safe to pull directly from the Debian repositories. $URL$ 

This will return a list of additional DNS servers that have been delegated to handle DNS for your TLD. Pick one and send the same query to it. For example: 

Well, the base package list (assuming this is a desktop system) is built by the combination of the and packages. These are metapackages that exist solely to depend on other packages. The problem, of course, is that their dependency list is not the complete package list. The packages they depend on may have other dependencies. So... I would suggest playing around in . Perhaps try flagging all installed packages for removal, then specifically selecting your active kernel, ubuntu-minimal, and ubuntu-desktop to be installed. With a little finagling, you should be able to reach a point where the only packages flagged to be removed are the ones not required by the two metapackages. I haven't tried this. I haven't even experimented with it. You may hose your system by following my suggestions. Have fun! 

OS X server uses Postfix for mail. It's reasonably safe to back up a host while Postfix is running. After restoring the queue may be a little messed up. These queue problems can be repaired by running after restoring. In fact, is required after restoring even if you don't shut down Postfix. Postfix relies on certain files in its queue having a name that's the same as their inode number. Obviously, this can no longer be guaranteed after restoring from backup. Regarding other services: Hard to say without knowing what other services you're running. Databases, for example, frequently require special care when backing up. 

If it's true server-grade hardware (not a re-purposed desktop) it will likely have built-in IPMI controls. For example, my Penguin Computing and Dell servers can all be rebooted or powered on/off remotely regardless of the state of the OS. 

That file can be read / parsed as if it was a normal file, using any tools you like. Including Python. Quick-n-dirty example: 

I think you're being too paranoid. User-driven password changes allow users to select passwords they find memorable, change passwords when they suspect their boyfriend / girlfriend / mother is spying on them, etc. There's no reason for a password-changing app to be a security hole. Separate the hell out of the UI and the back-end logic, perhaps writing a simple CLI tool to perform the database manipulation and calling it from the web app. This way your CLI tool can perform its own sanity checks on every request it receives. Some user/password databases have well-tested, reliable password changing mechanisms already, like OpenLDAP's Password Modify extended operation. Yeah, I recommend keeping webmail on a different host. I've managed several large email sites over the years and the only compromise I've experienced was when I failed to keep the webmail app up to date. Webmail apps are large, complex, and frequently not designed with an eye to security.