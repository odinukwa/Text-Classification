The main problem I've see is that clause performs non-constant comparison. For each of the 11574092 rows the same value calculated again and again that produce the overhead. Moreover, index can't help with such kind of comparison as far as is non-deterministic function that can return different results in two consequent invocation. Therefore engine is forced to check all and every row for desired condition, calculating each time from the scratch. It's easy to avoid this very common trap. Calculate the value once and store it in the user-defined variable: 

All that you need is to add an extra column where the total mileage should be written each time you get a new lat-lon point. That approach has a lot of advantages, most valuable of which are: 

You have to remove all the duplicates prior to create the or (that is also in fact) index. In your case that can be performed via joining the table with itself: 

The most important table of my database is about 300,000 records, and growing. More than 20 tables have a FK to it. Its primary key is a number, but for historical reasons is defined as nvarchar(6) which is obviously inefficient (and ugly). Records are sometimes deleted from the record, so the primary key is approaching 999,999 and I must change it. An identity field would be the obvious choice. An int key, or similar, with the increment produced either by a trigger or by the software, would be an alternative. This would be feasible because the records are always inserted one at a time. Would an int key provide better performances with complex queries? 

I have a SQL Server 2008R2, and I want to connect remotely to a SQL Server 2012SP1, for which I am given: 

The remote database is outside my LAN, but firewall is configured on both sides, so I can query it by opening SSMS directly from my database server. I perform the following: 

It's look like your reporting query lock for a while the tables needed by general queries. There is no general recipe for such cases and you have to refactor your reporting to avoid locking. Sometimes you can achieve this with splitting one big query into the series of smaller ones. Sometimes the good approach is to copy some tables for exclusive use by report generator. Sometimes it is something else. All depends on the your DB scheme. 

You have to use UUID() function that guarantee that returned string is unique not only within your table or database but all over the world. $URL$ 

But those duplicates where none of entries have CategoryID is set are still remains in the table. Let's eliminate them: 

Joining to the means "list all possible options and show the cars having them". When you join to the the resulting table have the different meaning: "list all the cars and show all the options they have". NULLs in my query means "that particular car has no available options" while for your query NULLs means "no cars having that particular option". If you want to eliminate any NULLs from output you have to add restriction to the clause. 

This shows me that the initial query actually performs two queries, one after the other. However, both of the updated columns have no index defined. I would like to make the initial query perform faster. Do you see any optimization possible? I am afraid that, since the trigger is on UPDATE, and it actually updates the column, it may trigger multiple times. Is it possible? If yes, how do I prevent it? 

I have a database on Microsoft SQL Server 2016, with a table of about one million rows, let's call it books. This query takes more than one minute, which is not acceptable: select * from books where publisher_id in (857413, 857317, 857316) There is a proper FK from publisher_id to the publishers table. I display the Estimated Execution Plan, and it tells me that 100% of the cost is in the "Clustered Index Scan (clustered)" on the primary key of the books table. And what worries me, it does not recommend an index at all. By looking at the query, it seems obvious that an index will help. And in fact, when I create the index the query returns results instantly. Did something go corrupt in my database, maybe statistics? Or do you believe I should nor, in general, trust what I read in the estimated execution plan? 

where '2' and '7' are IDs of corresponding tables and . Indexes for char/varchar/text fields are significantly slower than numeric ones. 

First of all you have to import all the timezones from into the . There is special script called that do all the magic. Overall explanation can be found here: $URL$ After the zones has been imported you can use function to convert raw datetime/timestamp in the UTC to the arbitrary TZ. Also your client can use for the session and all the datetime/timestamp columns will be converted to the desired timezone on the fly. 

When you want to calculate the distance between stations you have to take the absolute value of the difference between their distances. 

all the databases export all accounts with permissions deinstall install import databases and accounts 

Operator returns 1 if operand is 0, 0 if operand is non-zero, and if operand is . Parenthsis added because adding have higher precedence over negation. 

I have logged the slowest queries on my database, and one surprises me, showing many times in the list, and taking often many seconds to execute. 

What is the proper way to define a linked server from the Internet, and if possible assigning a nice name to it (rather than referring to it by IP)? Should I be using SQLNCLI as a provider? 

I read that the change in recovery model only has effect after a backup is taken. However, if after the performing the above I do 

There is a FK on documents.person_id pointing to persons.person_id Am I missing something? This is my @@version: Microsoft SQL Server 2016 (SP1-CU1) (KB3208177) - 13.0.4411.0 (X64) Jan 6 2017 14:24:37 Copyright (c) Microsoft Corporation Standard Edition (64-bit) on Windows Server 2012 R2 Standard 6.3 (Build 9600: ) (Hypervisor) 

The table has a trigger FOR DELETE, which I consider irrelevant here, and a trigger AFTER UPDATE, which reads: 

View is just an alias to the query that allow you to use its result in other queries in terms of tables not subqueries. But each time you refer to the view, the query from its definition is invoked. No temporary tables created when view is defined. Therefore views do not improve the overall performance at all and are intended to make DB structure more clear and logical. Prior to the MySQL 5.7 query results can be cached that really speed up some queries. But as far as caching conflicts with InnoDB internals, it is disabled now and can cause significant slowdown and overall performance losses when enabled. So views should be considered as threat to the performance. On the other hand, temporary tables, especially those with , can significantly improve the performance. But you have to ensure that temptable is updated each time the referred tables are updated. Triggers are the good tool to keep reference consistency of the derived tables. Sure even memory-based temptables should be properly indexed for maximum performance. 

book_id is int identity PK (clustered), last_read is a datetime. The query is written with the 15 in single quotes, thus requiring a conversion, but I cannot imagine this being a big deal, because the conversion would only be done once per query. There are 6 indexes on the table, but the column last_read is not involved or included in any of them. The PK is on book_id and is nothing special. The table is very central in database, contains 50 columns and 400,000 rows. The estimated execution plan tells me: 

I would like to do the same thing, in the most portable way possible, using only ASCII in my query, for instance: 

A server SERVER_A has some data. A server SERVER_B defines A as a linked server, and defines some views on the data. The queries are like this: 

The only reasonable way for you is to create stored procedure with that should scan the table row by row and calculate desired value. Otherwise you'll stuck with really HUGE on-disk temporary table without indexing that cause timeouts and disconnections just like you already have. 

Then replace with , , etc. The higher count means better selectivity and you have to arrange fields in the index from the best selectivity to worst. 

Here I suppose that is fetched from the another table and perform the mass update for all the rows at once. If the is bigger than then become negative therefore I've add it to the instead of substraction. 

Sometimes it is reasonable to replace NULL values by placeholders. That allows to keep logic simple on the client side. There are two approaches here. First is to use placeholders instead of NULLs directly in the tables. That make the data uniform but also means some loss of data because NULL have special meaning in the RDBMS. Second approach is to replace NULLs by placeholders on the fly when data is passed to the client. 

return the result in . Change the @HoursWorked data type in your scalar value function to instead of . 

The delete statement without the where clause delete all rows in the table without change of table structure. If the delete statement is within a transaction, then it can be rollback before the transaction is committed. If the delete transaction has been committed, the deleted transaction can't be rollback. Unless use of third party tool or restore the log backup to previous point if available. 

You could look into changing the database to contained database. Contained database user are authenticated by the database, not at instance level through login. It makes moving database to different instance simpler. If not, you could backup the login information using sp_help_revlogin scripts provided at this Microsoft support KB. And execute the output script on the new instance. 

I can see that the recovery model is reported as simple for TESTDB. Of course to be sure I could take a backup of TESTDB (and discard it immediately), but I would avoid it if possible. Do I need to take a backup to actually change the recovery model of my test database to simple? 

Up to now, no error, and I can see the linked server in SSMS, "Test Connection" is successful but I cannot query it. 

I have no idea what application is causing these queries, and therefore these annoying errors. By looking at the query, do you have any idea? What could I do to investigate? 

I considered using indexed views, but they cannot work with linked servers. Mirroring the data is possible, but I really want real time information. What can I ask to define on server B to be able to get data efficiently and in real time from a query on server A?