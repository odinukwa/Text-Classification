For an example of windowing functions and subqueries in T-SQL(if you assume g1 is your table and not some subquery I just invented): 

Simple answer, you grouped by your sum. Solution is simply to remove that from your group by statement, eg: $URL$ 

As a fairly newly minted DBA under the gun, I have run the gamut of free tools and done some experimentation in the paid space (DPA, SQL Sentry, and Foglight) and it really depends on what you want the tool for. In my experience the most important thing was not just communicating performance baselines (management vastly didn't care unless there was someone to yell at), but produce something in an easy to consume format that made the priorities clear and was able to track down performance issues in production. You can absolutely build up your skills by going the free route, and the tools for SQL Server are great. 

Yes, SQL Server can report how long it took to do any of those actions (though you may have to run it to get some additional details such as actual row counts returned) Statistics Time 

I dont think so except for the fact that there is a 5NF, which describes a design where your joins are only on the candidate keys. Many "4NF" designs meet this criteria, but not all, and it is definitely something you can change a 4NF "into" to be be more normalized. 

Solution : You need to run a table(s) repair on the database Identifying table corruption Checking tables There are three ways to check tables. All of these work with MyISAM tables, the default, non-transactional table type, and one with InnoDB, the most mature of the MySQL transactional table types. Fortunately, MySQL now allows you to check tables while the server is still running, so corruption in a minor table need not affect everything on the server. 

The most commonly used option, which repairs most corruption. If you have enough memory, increase the sort_buffer_size to make the recover go more quickly. Will not recover from the rare form of corruption where a unique key is not unique. 

The fastest repair, since the data file is not modified. A second -q will modify the data file if there are duplicate keys. Also uses much less disk space since the data file is not modified. 

I am running a Stored procedure to export data to an Excel sheet. Issue is that The column headers are being displayed at the bottom or they are missing at times. What am I missing here ? SQL Server Version 

Source where the information was found TechNotes,Lance Andersen's Blog Removal of the JDBC-ODBC Bridge from Java SE 8 JDK I suggest you try the following solutions 

T-SQL script which you can use to monitor the status of transactional replication and performance of publications and subscriptions. Things to be considered before executing the below script Requires permission on the following tables inside distribution and master databases 

You have to be very careful. Assuming your query is not restarting and you have a for that SPID. If you restart SQL Server it won't help, because transaction still would have to be rolled back. The problem with that is following: When you run a transaction in multi-CPU environment with not restricted degree of parallelism it most probably will generate parallel execution plan. Would say your transaction run for 10 minutes on 8 CPUs. When you Killed it, will be processed ONLY by ONE CPU. That means it might take up to 8 times longer to recover. 

Try both of them for both of your data sets to see the difference in query cost and amount of I/O using . For instance, when you force for the second data set I/O for table jumps 24 to 215 reads. So, be VERY CAREFUL using any kind of these hints. 

Might happen you still have some opened transactions, which hold you from shrinking individual files. See who hols them, close them. Reboot server if necessary. Shrink individual files while nobody accessing the database. switch it temporarily to Single User if necessary: 

I'm not a replication guru, but record may be marked as deleted only in case when an update is generating Page Split. For the case of replication you can test it by looking at DBCC PAGE and look through transaction log. 

Same as the CHECK TABLE ... QUICK option above. Note that you can specify multiple tables, and that mysqlcheck only works with MyISAM tables. Checking tables with myisamchk Finally, there is the myisamchk command-line utility. The server must be down, or the tables inactive (which is ensured if the --skip-external-locking option is not in use). The syntax is: , and you must be in, or specify, the path to the relevant .MYI files (each MyISAM database is stored in its own directory). These are the available check options: 

After we know which users we will drop, the below script can be used to drop the orphaned users taking in account the need to first remove the association to schemas and database roles. 

Following information was found on sybasewiki Hope this helps ! How to start the sybase server? After log in your Linux/Unix machine. 

Pls also consider I also noted that There is an issue with sp_replmonitorsubscriptionpendingcmds if you are still with SQL Server 2005 and 2008 Microsoft Connect If any of the above answer are unclear you can follow the source I have provided Thanks! 

Last process is to periodically delete rows from the replication status table so the data does not get stale 

our company is planning to administer the report server database and the requirement is to get the list of the tables used in stored-procedures in the productions server database.Is there any query/function to retrieve this information ? 

This table would only store information relevant to an exam, such as the title of the exam, the exam's attributes and categories. 

The easiest method is (probably) to uninstall and this time run the command line installer again with a different set of flags and it will install to a different location unless you are talking about components not listed below. From: $URL$ Proper Use of Setup Parameters Use the following guidelines to develop installation commands that have correct syntax: 

This means we could count all of Michael Jordan's games by sport(as he played multiple) with a query something like: 

You are grouping by the thing you are counting, not by the department name. Change your group by to: 

With these and some additional databases/tables and jobs and time you can build out a basic monitoring system (but it isn't pretty) these are tools for DBAs; unless you are good at BI stuff you will struggle to find time to produce useful business friendly stuff from it, though the Ozar sp_blitz app is pretty dang cool. After spending around a year doing the free thing and resolving plenty of issues (but not getting much buy in) I was able to make it clear, after a major issue, that perf monitoring software was a priority, and we were going to buy it come hell or high water. After demoing the previously mentioned clients, I chose DPA because management could easily consume the results, though I definitely have client licenses for SQL Sentry Plan Explorer Pro (1000% worth the money) and really liked using the server version, it just didnt grab them the same way. I also tried getting SQLNexus working at one point but I ended up working a lot than I was interested in, it may suit your needs. 

Repairing a table requires twice as much disk space as the original table (a copy of the data is made), so make sure you are not going to run out of disk space before you start. Repairing a table with REPAIR TABLE The syntax is, as would be expected, . This method only works with MyISAM tables. The following options are available. QUICK The quickest, as the data file is not modified. EXTENDED Will attempt to recover every possible data row file, which can result in garbage rows. Use as a last resort. USE_FRM To be used if the .MYI file is missing or has a corrupted header. Uses the .frm file definitions to rebuild the indexes. Repairing tables with mysqlcheck The mysqlcheck command-line utility can be used while the server is running, and, like all the methods of repair, only works with MyISAM tables. The syntax is: 

QUICK The quickest option, and does not scan the rows to check for incorrect links. Often used when you do not suspect an error. FAST Only checks tables if they have not been closed properly. Often used when you do not suspect an error, from a cron, or after a power failure that seems to have had no ill-effects. CHANGED Same as FAST, but also checks tables that have been changed since the last check. MEDIUM The default if no option is supplied. Scans rows to check that deleted links are correct, and verifies a calculated checksum for all keys with a calculated a key checksum for the rows. EXTENDED The slowest option, only used if the other checks report no errors but you still suspect corruption. Very slow, as it does a full key lookup for all keys for every row. Increasing the key-buffer-size variable in the MySQL config. file can help this go quicker. Note that CHECK TABLE only works with MyISAM and InnoDB tables. If CHECK finds corruption, it will mark the table as corrupt, and it will be unusable. See the Repairing tables section below for how to handle this Checking tables with mysqlcheck The second method is to run the mysqlcheck command-line utility. The syntax is: . The following options pertain to checking (mysqlcheck can also repair, as well as analyze and optimize. 

However, there are plenty of objects in the database, which were created with credentials of that account. I've tried to run one of these procedures and it was executed successfully. When I've tried to recreate that scenario in my test database it returned me an error: 

Your provides 2 sets of the data: - DB Size + Unallocated space - These numbers include BOTH: Data and log file; - Total statistics of RESERVED space for all objects within the database; I bet your 36 GB of free space are in the Log file. For real numbers use following query: 

You create clustered index on only one column. Technically, you can do it on multiple, but the main goal to have it as short as possible. If you created clustered index on you DO NOT NEED any other indexes on that column. I can guess that your performance improved because before there was no Clustered index and indexes were so bad that SQL decided to do full table scan instead. Suggestion: read a book about Indexes, their differences and how they work. 

At first I'd suggest to have group_id and visit_id in INT or at least BIGINT. If that table has only 3 columns it might worth to create not-unique clustered index by group_id and unique constraint on visit_id: 

Items with big values in and are red flags, such as Table scans and Key Lookups. There can be because of bad indexing, statistics, etc. Wild guess: when you use Temp Table Variable SQL comes up with better query plan. 

Edit:Adding a little flavor text as requested. Basically what is happening is you are choosing to aggregate one of the values (in this case the counts of the salary) so that it "rolls up", and the group by generally indicates which value you want to do the rolling up by. It makes some sense to say "I want to group by the number of employees" but you are actually trying to express "I want to return the number of employees grouped by department." 

SQL Server Setup Control /INSTALLSHAREDDIR Specifies a nondefault installation directory for 64-bit shared components. Default is %Program Files%\Microsoft SQL Server Cannot be set to %Program Files(x86)%\Microsoft SQL Server /INSTALLSHAREDWOWDIR Specifies a nondefault installation directory for 32-bit shared components. Supported only on a 64-bit system. /INSTANCEDIR Specifies a nondefault installation directory for instance-specific components. 

This table would store the ids for both the previous tables and any information that is specific to that exam's instance of that question. This would allow you to write queries such as "How many questions belong to one exam" 

To explore a bit more about the question and comment, I want to note that generally when normalizing your goal (among others) is to reduce duplication. A naive example of your table might be everything you have but adding things like the CSV list you mention, this ends with a very wide table that has multiple areas of duplication and which will require you to manage and clean your data (and is generally bad for performance.) A solution to this problem is to simply project the data into another table and provide the keys to join back to the original. Eg: 

Passes a new path for storing temporary files if you dont want to use the contents of the TMPDIR environment variable 

Checks tables (only needed if using mysqlcheck under another name, such as mysqlrepair. See the manual for more details) 

Can make the process faster by specifying which keys to use. Each binary bit stands for one key starting at 0 for the first key. 

The code below requires the Ad Hoc Distributed Queries server configuration option be enabled. Here I create the email to be sent assuming the previous Script 3 found an issue . 

Attempts to recover every possible row from the data file. This option should not be used except as a last resort, as it may produce garbage rows. 

This option stores when the table was checked, and the time of crash, in .MYI file. You can also use wildcard to check all the .MYI tables at the same time, for example: 

Note that myisamchk only works with MyISAM tables. For those of you still using the old ISAM table types, there is also isamchk, though there is really little reason not to upgrade to MyISAM. Repairing tables In most cases, only the index will be corrupted (the index is a separate, smaller, file with records that point to the main data file) - actual data corruption is extremely rare. Fixing most forms of corruption is relatively easy. As with checking, there are three ways to repair tables. These all only work with MyISAM tables - to repair corruption of the other table types, you will need to restore from backup: