Note that I replaced the hash signs with // to signify comments in the httpd.conf, because the hash sign causes the text to appear as a heading. In a reversal of the problem I have on the new server, I cannot tell which worker is being used on the old one, there are settings for both prefork and mpm_worker, but no actual specification of which is to be used that I can find. 

Hoping this is a good question and can be useful for anyone in a similar situation. I recently took over from a very senior Linux administrator in my company, and for the first time I've had to set up a new server to serve our content to our customers. My predecessor left limited documentation on this matter as other things were of higher priority at the time of his departure. I've referred extensively to what documentation I had, plus configurations from previous servers and a lot of online reading to set everything up. The "stack" of a server I am building is as follows: CentOS 7, Apache 2.4.6, MariaDB 5.X, Tomcat 7 serving JSP/Servlets. Previous installations for production used CentOS 5, older version of Apache and MySQL, and Tomcat 5.X. Apache is the front end, it uses AJP to connect to a Tomcat backend. Most traffic will be HTTPS. The server is, I think a Gen 9 HP server, it has 32GB RAM and SSDs in a RAID 1 array. After getting it to work, I am now concerned with maximising its ability to serve users simultaneously, I expect a burst of several hundred users would not be unusual. To that end, I've looked to prevent bottlenecks at any point in the stack: starting with Tomcat and MariaDB and taking measures specific to those, measures with which I am satisfied. Moving on to Apache however I find it more difficult to understand what I should be doing. There are 3 differnt types of "workers" and I have 3 major problems optimising these: 1. There are three options, prefork, mpm_worker and mpm_event. I don't really understand how to configure them though I do have a general understanding of what each is used for. 2. Unlike Apache in previous servers, the options for StartServers, ServerLimit, MaxRequestsPerChild etc are not listed in /etc/httpd/conf/httpd.conf. Could there be default settings that I have to manually override? Is there any reason other than "they were added manually last time" that settings would be found in an older httpd.conf but not a new one? I refer to this, in an older httpd.conf, I cannot see anything similar anywhere in the new configuration files, only a specification: 

So my understanding of one scenario that ZFS addresses is where a RAID5 drive fails, and then during a rebuild it encountered some corrupt blocks of data and thus cannot restore that data. From Googling around I don't see this failure scenario demonstrated; either articles on a disk failure, or articles on healing data corruption, but not both. 1) Is ZFS using 3 drive raidz1 susceptible to this problem? I.e. if one drive is lost, replaced, and data corruption is encountered when reading/rebuilding, then there is no redundancy to repair this data. My understanding is that the corrupted data will be lost, correct? (I do understand that periodic scrubbing will minimize the risk, but lets assume some tiny amount of corruption occurred on one disk since the last scrubbing, and a different disk also failed, and thus the corruption is detected during the rebuild) 2) Does raidz2 4 drive setup protect against this scenario? 3) Does a two drive mirrored setup with copies=2 would protect against this scenario? I.e. one drive fails, but the other drive contains 2 copies of all data, so if corruption is encountered during rebuild, there is a redundant copy on that disk to restore from? It's appealing to me because it uses half as many disks as the raidz2 setup, even though I'd need larger disks. I am not committed to ZFS, but it is what I've read the most about off and on for a couple years now. It would be really nice if there were something similar to par archive/reed-solomon that generates some amount of parity that protects up to 10% data corruption and only uses an amount of space proportional to how much x% corruption protection you want. Then I'd just use a mirror setup and each disk in the mirror would contain a copy of that parity, which would be relatively small when compared to option #3 above. Unfortunately I don't think reed-solomon fits this scenario very well. I've been reading an old NASA document on implementing reed-solomon(the only comprehensive explanation I could find that didn't require buying a journal articular) and as far as I my understanding goes, the set of parity data would need to be completely regenerated for each incremental change to the source data. I.e. there's not an easy way to do incremental changes to the reed-solomon parity data in response to small incremental changes to the source data. I'm wondering though if there's something similar in concept(proportionally small amount of parity data protecting X% corruption ANYWHERE in the source data) out there that someone is aware of, but I think that's probably a pipe dream. 

"Have you tried turning it off an on again?" :) No. Really. This seems to be a network messup where your client doesn't see the domain controllers. Maybe there's something wrong with DNS or IP addressing, or anything else preventing proper communication between Client and DCs. 

For the particular example (saved passwords in a browser) I would recommend using the master password provided by Firefox. It encrypts the password cache. In KDE (and Gnome) there are tools like Wallet, which also provide a secure storage for credentials (e.g. for browser, mail app, chat client, etc) using a separate password. I believe something like this is available for Windows, too. Another way would be to encrypt your home directory with TrueCrypt using a container file which contains your profile and documents. 

Are you absolutely sure you need to do that? It's always a security risk to have a user (or administrator) logged in locally to a server. If you need to start a program or something that is not available as service, have a look at Group Policy (Active Directory) or the Local Security Policy (if not in a domain). You can define scripts that will be run after boot (or before shutdown, after login/logout, etc) without user interaction. 

You could set up a free monitoring solution like Incinga or Nagios. There are tons of plugins for every check you can think of. Or you could just write your own script which reports a status ("good", "bad", whatever...). A good commercial thingy would be WhatsUp Gold. It includes performance measurement tools, too. If you want some real gold, take a look at Jazzey. It's a real high class, really expencive, end-to-end monitoring tool. It simulates real users working with your (web-) apps. Spent lots of ours with at work. 

You could send a series of commands to and wrap it in a shell script which calculates the filename of the too old files (see the script below). 

First, unsing the pre-packaged binaries is not a bad idea. Or do you like patching by hand and recompiling every time a security flaw is found? Let the package managers do that for you. Second, make a list of features you really need to implement and what could be needed in the future. Read through the documentation of the software of your choice to make sure it meets the criteria. postfix + Dovecot + virtual users + MySQL backend is a fairly common setup - again, read the documentation and/or grab one of the too many articles you found on the web already. I would start with the ISP style mail server-HOWTO or this Blog post. 

Scoped to a specific filesystem name :No header so that first line is a snapshot name : List snapshots (list can list other things like pools and volumes) : Display the snapshot name property. : Capital denotes descending sort, based on creation time. This puts most recent snapshot as the first line. : Says include children, which seems confusing but its because as far as this command is concerned, snapshots of TestOne are children. This will NOT list snapshots of volumes within TestOne such as . : Pipe to head and only return first line. 

We are doing nightly full backups and noon differential backups. We use Full recovery model with SQL Server 2005, but logs are never backed up and are truncated(TRUNCATE_ONLY) after the full backup. Restoring to a point in time is not a requirement, restoring to one of the nightly or noon backups is sufficient (Not my decision). So the question at hand is, since they are throwing away the logs every night, is there any reason to not use Simple Recovery model? Is there any benefit to using Full Recovery model if we are throwing out the logs every night? Thanks. 

If I have a mirrored pair of 250GB drives in a pool, and I later buy two more drives and add another mirrored pair to the same pool, can that second mirrored pair be 500gb? Such that my total usable space would be 750GB? Or do all the mirrored pairs in a pool need to be the same size? 

Where can the pfsense log files be located and viewed? I have searched the documentation and it doesn't indicate the log files location for the various components of pfsense. 

I am running pfsense 2.0.3 nanobsd 4g i386 on virtualbox. VM configured with 4gb ram, there's 8 gb total on host system, with two net interfaces configured as host only. This will go on an SSD mini atx box, but for now I am just running on VM for learning pfsense. I assigned interfaces, em0 to WAN, and em1 to LAN. From the windows host(hosting the VM) I brought up the browser and tried to connect to the LAN IP. I was intermettently getting timeouts and I would reboot the server or use the reboot web configurator option, and sometimes I could get the login screen but after logging in with default user/pass, I'd get a blank page. Absolutely no error messages or feedback of any kind. I typed password carefully, thinking maybe it was doing anonymous authentication, since according to their documentation provides a blank page by-design. After many tries and reboots I finally got the wizard screen. I completed the wizard and the final page indicated it was going to redirect after a few moments, after a few minutes it redirected but failed to retrieve the next page. From there the web configurator again was not responsive, timing out. I rebooted and still same thing. How do you troubleshoot something that gives you absolutely no feedback or error messages? Any ideas about what might be wrong would be welcome, but primarily: How do I troubleshoot failures in the web configurator? Is there logs specific to the web configurator, or do I need to poke around in the web server logs, pfsense logs, etc.? Is there any documentation on directory structure that would help me find these? I've found from distribution to distribution, that each has it's own idea of where user programs, logs, etc are stored. 

You could use one of the many well known tools out there to test passwords. One I saw is L0phtcrack. Maybe there is even a way to do this offline with a dump of you authentication database. In "the other world", we use "john the ripper" for stuff like this. 

Sounds like you want to use a versioning tool like /Subversion or rather than rsync. It perfectly fits your need plus the benefit of rolling back if an edit messes something up. Take a look at the Subersion Homepage and/or git HOWTO. 

It sounds like the script is called again before it finished - maybe the second instance has no access to called resources or something like that. Hard to tell without some code... What is that perl script actually doing? 

The problem here is, that your router does not NAT your internal client's address. Thus, the TCP handshake fails. Let's assume following IPs 

The error means that the user does not have sufficient rights on the target machine. "Something" went wrong with the administrator rights for that user... Is it a Domain setup or standalone? Could you paste the command line you are using (without password and/or public IP)? 

...and if the clocks are correctly set, just run . Don't be afraid. It's a testing distro - maybe they messed something up. ;) 

You could use to redirect port 80 to 8080. This is useful if your application is started by an unprivileged user instead of root. 

I prefer using varnish in front of tomcat instead of Apache, because Apache's tomcat connector is quite b0rken. Varnish is a very efficient reverse proxy and cache and speeds up delivery of tomcat's HTML output. Without the hassle of tomcat connector. 

This should not be a problem. I did this with several other vendors (Foundry, HP, Extreme, ...) and Cisco should not be a problem, too. Don't rely 100% on the syntax, tho. 

I'm not familiar with Windows servers (I'm some kind of networking guy) but have to setup an ISA server from the networking point of view. It will connect an internal LAN to a DMZ. There will be a cluster of two ISA servers using NLB. Obviously, the cluster will have 2 NLB "sessions", one for internal and one for DMZ zone. My question is concerning the heartbeat: What (and why) would be the best way to go out of the following possibilities: