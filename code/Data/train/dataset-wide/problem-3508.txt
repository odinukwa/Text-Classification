I don't think you need to do anything special as aliases aren't configured as separate networking devices. This is unlike Linux where you have , , that are treated as discrete interfaces. Consequently, the pf rules should expand to apply to all the IP address present on that interface: 

The first thing I would recommend is to get ahold of a copy of the manual or talk to the vendor to get an explanation of exactly what the error the devices generate means. I've wasted time looking for Layer-3/4 issues when the error actually meant something else. Not all vendors use terminology correctly or consistently. It sounds like the devices are not sending handling or doing keep-alive correctly. If there is no data traversing over your TCP connection it will be closed eventually. To prevent this one endpoint (or both) can send keep-alive packets to prevent the connection from being terminated. I know this can be done with TCP (Layer-4) and presumably it could also be done with SSL/TLS (Layer-7). Put a packet sniffer of choice between one of these devices and your infrastructure and record all of its traffic from the time it works until the time it doesn't. Then look through it and find where the device or the server it's connecting to is starting the termination sequence, then see what immediately precedes that. Also take a look at the point in time where the device throws the "TCP connection failure" error. Is it trying to use a connection that it thinks is established but that the server thinks is terminated? Something strange is happening here as well - if the connection isn't established, instead of throwing an error, your credit card device should attempt to create a new one (which apparently happens successfully the second time). And finally if you are using NAT, consider giving one of these devices a direct, non-NAT connection for testing purposes (again, take a packet capture). NAT can do very strange things to applications or protocols that depend on the End-to-End Princple and do not take the widespread use NAT or other stateful devices interfering with the connection into account. If you're using a proxy, ensure that it is not involved or that it is correctly configured to handle these devices. We have lots of devices or processes that are smart enough to use their host operating system's WPAD settings but not submit the active directory credentials of the user account that is running them with their HTTP/HTTPS requests and the proxy expects all connections to be authenticated and so the process will quietly fail client-side. 

Even if you are using a proxy you should disable WPAD and manually configure your client's proxy setting using GPOs. WPAD does not have any mechanism for authentication. A Man-in-the-Middle attack is simply a matter of attacker answering the DNS query for faster than the legitimate nameserver (see this article going over the WPAD Metasploit). You can disable WPAD by using the following GPO: 

Your goal should be to find another job and leave without anything breaking. If something does break (and it probably will) be prepared to walk out. This means you should be extremely careful regarding CYA procedures. You need to assume that no on at the company has your back - make sure you're not in a position where you need to rely on someone's support. While you're looking for a new job setup a environment to practice and learn with and do your best to familiarize yourself with the current infrastructure. And for what it's worth... my degree is in Philosophy. You can succeed in this field if you want to, but not at this company. Good luck. 

"XenServer Free" as in Citrix's XenServer? Last I recall, Citrix's XenServer was just dom0 implemented on CentOS. If that is still the case you should be able to change the MAC address of your dom0's interface by editing and adding/changing the setting. Executing a should bring up your interface with the new MAC address (providing the drivers for that interface support modifying the MAC address... they probably do). Refer to the CentOS/RHEL or Xen.org documentation for more information. 

Is there any reason why an USB-based install won't work? It will be much simpler and faster. If it all possible, I would try to go that route. PXEboot can be... er... troublesome. 

We have been handling all of our targeting logic for Packages (and now Applications) with Collections. Now that we have moved from SCCM 2007 to SCCM 2012 SP1 it was recommended that we move that logic to the Application-Program model and implement it using Global Conditions and Requirements. This has a number of positive benefits - Collections are used purely for hierarchical or logical grouping, we get a more seamless Application deployment when using Supercedence, and improved Detection logic. I'll use Adobe Flash Player Plugin as an example. We only want to deploy Adobe Flash Player Plugin to workstations that have Firefox installed. Using the SCCM 2007 Package-Program model we would create a Collection based on a WQL query that contained all workstations with Firefox installed: 

This largely depends on your Polling Schedule and your underlying network configuration. If your SCCM Site Server has good connectivity to a Domain Controller and you not using an insanely aggressive Polling Schedule (the default is a full discovery every seven days) you should be fine. You can adjust your Delta Discovery so that your SCCM Resource Records track Active Directory closely without generating a huge amount of network traffic. You also need to be aware that do a full Discovery is resource intensive for the SCCM server itself. Our organization is a little smaller, but not much, and we are fine. Our networking team is a separate organizational entity so I can't get you concrete metrics without a ticket but it is minimal enough that it has never been an issue. Start with a the default Polling Schedule and a conservative Delta Discovery schedule (maybe every 30 minutes?) and do some benchmarking with your network team. You can then increase your delta schedule to me more frequent. You can also set which attributes your Discovery is interested in to further fine tune the process (why poll attributes you don't care about?). Take to note to remember that while Delta Discovery will detect new items in Active Directory it will not detect deleted ones (with the exception of Active Directory Group Discovery). 

Or you can go the other way. If you're wondering what package required the package you can use this handy little perl script to return a list of reverse dependencies that are also installed. 

Either way, the installer did properly register the ProductCode so if that is all your running your Detection logic on there is no discernible difference between the two installation methods. However, if you do a compare of the Registry Keys for executable installer and the MSI installer you can see some differences: From the executable installer: 

I have a legacy NetWare 6.5 server that offers file shares to about 40 clients. It also acts as the router between our two internal subnets and our parent agency's network. We are planning on replacing the routing functionality with a Juniper product. At the same time we are refactoring our physical network infrastructure - currently the NetWare server has an IP presence on both subnets. I would like to disable two of its three interfaces so it only has one connection to the network. Unfortunately, I cannot figure out how the Novell Client (4.91.5.20080922 on Windows XP SP3) resolves the NDS tree netware_server.department.mycorp to an actual IP address. Testing has shown that if those interfaces are no longer there, clients will fail to "resolve" the NDS object to the correct (still existant) IP address and chaos will ensue. I have tried setting the "Server Cache Timeout" to 0 in an effort to force a "name resolution" of netware_server so I can look at the actual TCP/IP conversation in Wireshark. I have also tried adding an entry to the C:\WINDOWS\system32\drivers\etc\hosts file with the NetWare server's NDS Object Name and then limiting the Novell's Client's Name Space Providers (Properties - Protocol Preferences) to just "Host File" trying to force a new "lookup" using the /etc/hosts file instead of whatever arcane method is currently used. Both of these attempts came from this TID10057730. Both have failed. How does the Novell Client resolve an NDS Object Name like netware_server.department.mycorp to an actual IP address? How can I force the clearing of any client-side "NDS name" cache? How can I force that "NDS name" resolution to always resolve to an IP address that I manually specify? EDIT: First off, we're running pure IP. If you happen to still be running IPX the Novell Client behaves quite differently. 

I'm not really sure how to interpret this error but I'm going to go with "the directory the file share is exposing to Expression is already being used by IIS as the location for a web site" which if that is a correct interpretation makes this feature pretty useless (wouldn't every website you'd want to publish to also be served by IIS?). I have tried looking through the Event Log on the server and compared the IIS configuration between the two server groups and cannot find anything immediately obvious. I would rather avoid the other connection methods (WebDAV, FTP, SFTP and Frontpage Extensions if possible). How can I setup our web servers so our development teams can work on a website in Development and then publish it to Testing or Production without resorting to manually copying files or running a script? As someone who is not terribly familiar with Windows and IIS am I "just doing it wrong"? 

And interestingly enough the dhcp-relay agent seems very busy on Switch B, but why? As far as I can tell there is no reason why dhcp requests need a relay with this topology. And furthermore I can't tell why the upstream switch is dropping legitimate dhcp requests for untrusted relay information when the relay agent in question (on Switch B) isn't modifying the option 82 attributes anyway. Adding the on Switch A allows the dhcp traffic from Switch B to be approved by Switch A, by virtue of just turning off that feature. What are the repercussions of not validating option 82 modified dhcp traffic? If I disable option 82 on all my "upstream" switches - will they pass dhcp traffic from any downstream switch regardless of that traffic's legitimacy? This behavior is client operating system agnostic. I see it with both Windows and Linux clients. Our DHCP servers are either Windows Server 2003 or Windows Server 2008 R2 machines. I see this behavior regardless of the DHCP servers' operating system. Can anyone shed some light on what's happening here and give me some recommendations on how I should proceed with configuring the option 82 setting? I feel like i just haven't completely grokked dhcp-relaying and option 82 attributes. 

I dislike answering my own question but I did some more digging in the OptiQroute's documentation and discovered the magic settings. The OptiQroute does something called 'HyperNAT'. It is not completely clear whether or not this is Network Address Translation. As far as I can tell, it is presumably just forwarding and re-direction but the actual implementation is pretty opaque. Regardless, the OptiQroute supports multiple means of "redirection" when more than one interface is involved. These settings are under Configuration -> NAT -> Scheduling-method for HyperNAT. The default is 'auto-learning' which completely ignores any of the Priority and Weighted settings. Auto-learning "chooses a WAN port with the highest unused downstream bandwidth and a feedback threshold under 66% (default)." There's a separate algorithms using the Priority and Weight (Weighted Round Robin) settings. I decided to use the Weighted Round Robin as it seemed closest to my goal of evenly spreading the traffic across the two WAN ports but at the same time "preferring" the WAN2 connection. With the Weights set at WAN1/10 and WAN2/100 almost all of our traffic leaves through the WAN2 side. Using Sflow to look at the protocols, the WAN1 connection essentially sits around 1Mbps doing all of our HTTPS and DNS traffic with occasional bursts up to 2.5Mbps when email is sent or received. Also mixed in there is our VPN access. I'm still playing around with less aggressive weight settings, but even with pretty limited HTTP traffic (which composes about 80% of the outgoing traffic) going over the WAN1 link it looks like we can still saturate the connection when email kicks off. Moving to a less aggressive weighting of 10/50 pushes a pretty constant 1Mbps of traffic (mostly HTTP) over the WAN1 link. There's no specific documentation on the nature of the Weights settings. I can't tell whether or not the values are absolute or relational. That is to say, is the important value the difference between the WAN interface's Weights (e.g., 1:10) or is each Weight evaluated independently as an absolute value. The documentation is not helpful here, but my experimentation leads me to believe the Weights are evaluated proportionally to each other as ratios. I eventually settled on using WAN1/10 and WAN2/60 for Weight settings. The other magic setting is in Configuration -> Interface -> WAN -> 'Smart Outgoing'. If a WAN connection has reached its maximum specified bandwidth, this will push the traffic to another WAN interface. Without this enabled (and with aggressive weighting settings), it'll just completely saturate the 4Mbps available on the WAN2 side. Instantly. Tubes unclogged. 

I take issue with the idea that users are inherently incompetent. I believe the word you are actually looking for is ignorant. They don't know any better and frankly why should they? It's your job to do the technical knowing. Expecting end-users to be technically competent is like expecting drivers to have a PHDs or Master's worth of mechanical engineering knowledge about their car. I find this works pretty well for me. 

The real issue is administrative: People expect SSH to be at 22, MSSQL to be at 1433 and so on. Moving these around is one more layer of complexity and required documentation. It's very annoying to sit down at a network and have to use nmap just to figure out where things have been moved. The additions to security are ephemeral at best and the downsides are not insignificant. Don't do it. Fix the real problem. 

Huh. That's probably not good. The WsusPool Application Pool should probably be running... If I manually start the WsusPool I can then connect with the WSUS WebServices by browsing to ... and then after about 15 minutes the App Pool stops. Also its running on the wrong ports (8530/8531)! About a month ago with the assistance of PFE we configured this SUP to be available to Internet-based clients. Part of that reconfiguration meant the WSUS web services need to be relocated to 80/443 so they are available through our perimeter firewall. I don't have documentation on the exact commands we used but I am reasonably sure it was WSUSUtil.exe usecustomwebsite false which should move WSUS from its "WSUS Administration" IIS back to the Default Web Site which is bound under *:80 and *:443. Again. This is not the case: 

Lee B's answer is right on, but here's some relevant RFCs in case you're interested. 0.0.0.0: From RFC1122, Section 3.1.2.3: 

You should end up with a list of and which you can then manipulate with more PowerShell. I like doing something like this: 

The Soekris and ALIX products have all of these features except they require an AC adapter (albeit a small one). 

Yes. There is an easier way to solve your immediate problem but it doesn't solve the root cause and professionals solve the root cause. Look at your apt-cache policy results. You're pulling postgresql and postgresql-common from the non-debian apt.postgresql.org repositories but it looks like you have the ones added for Ubuntu (precise-pgdg) instead of Debian Squeeze (squeeze-pgdg) which could have an entirely different set of dependencies. In the case of logrotate it is 3.7.8-6ubuntu5 instead of the 3.7.8-6 from oldstable/squeeze. Logrotate is being pulled from the Debian Stable repositories which means from Wheezy which is presumably out of sync with the rest of your oldstable/squeeze system. Your postgresql packages from Ubuntu Precise are looking for 3.7.8-6ubuntu5 but the closest you have is 3.8.1-4 from stable/wheezy. You need to 1) stop using Ubuntu repositories when you're using Debian and 2) figure out what release (Squeeze or Wheezy) you're actually running and change the to that. I suspect that will fix your problem. That being said, your is not at all configured correctly and there is a distinct possibility your server is pretty messed up. You shouldn't have Ubuntu repositories if you're using Debian. Ubuntu is based on Debian but they are two separate beasts and there is an unfortunate amount of advice on the Internet recommending that if you need a certain version of a package or a package that exists in the Ubuntu PPAs but not in the Debian repos to just add the Ubuntu PPAs. This is wrong. Please don't do it or listen to people that recommend it. You're also tracking a release-type (stable) and not a Release (Squeeze or Wheezy). This means that as soon as Wheezy was released as Stable all of those packages became available. You may already be running Wheezy or some horrible amalgamation of Squeeze and Wheezy. This is also bad. I really recommend you do a clean install and migrate your application there.