We can see the cost would be 1242, and the internal view VM_NWVW_1 appeared as a result of a Cost-based Query Transformation. Because of the GROUP BY, this can be only Complex View Merging and Cost-based Query Transformation, where the cost is not ignored, that is why Oracle did not choose this plan. So knowing that DISTINCT can be eliminated more easily, even without Cost-Based Query Transformations, and that we do not need aggregate results in this query, it would be better to use the double DISTINCT version: 

Forget and . AMM has always been a feature that worked on paper, but failed in practice. Oracle finally admitted it semi-officially as of the release of 12.2: 

What? Forget the above. You are missing some basic concept if you write a trigger like this. Why would you want to select a value that you already have? This does the job: 

This happens when you log in to the root container of a CDB, because tablespaces are local to PDBs. Configure EM Express at PDB level, for example: 

Yes, there is a difference, and the issue is that the estimated selectivity (thus cardinality) will be different in the 2 cases. For a simple example, lets say the year column has years in it from 2000 (low_value) to 2014 (high_value), so there are 15 distinct values (num_distinct), they are evenly distributed, 1000 rows for each, 15000 rows (num_rows) in total in your table. In the first case, where year < 2010 (limit = 2010), the estimated selectivity will be: 

Notice that the RUID is different, it is the UID of grid user. For example, this can lead to a situation, where file generation from the database with is successful when executed in a scheduled job, but fails when executed from a remote session. With the above setup, server processes forked by the listener inherit the privileges of grid user. Scheduled jobs spawned by the database itself inherit the privileges of oracle user. If they do not have the same privileges, they will behave differently. 

So? Just copy a dumpfile to a location accessible to database2. is not a directory, it is a directory object defined in the database, that serves as an "alias" to the real path in the filesystem. Overview of Oracle Data Pump File Allocation 

, by default, contains automatic backups of the controlfile and the spfile (if the database has one), if the feature is enabled. 

This is a false conclusion. When you restart the database, the buffer cache will be empty again. Post your test that confirms the opposite. 

If you have configured Data Guard properly, this will not cause any problem. And by 'properly', I mean is enabled. Data Pump can use direct path load (in which case there is minimal redo generated), but there are several conditions that will prevent it: Situations in Which Direct Path Load Is Not Used Anyway, to check : 

I can easily create an index on "large" (over 200K character) JSON documents, so first a JSON with a lot (50000) of short (2 characters) records: 

The database/RMAN layer has nothing to do with. Just configure TSM and TDPO properly, and this will be transparent for the database, always using the same file. 

So it wanted to recover the image copies created on to an earlier state: = . That does not work like that. You can not rewind image copies to earlier states. When you try to do something like this, RMAN will look for an earlier image copy, which it can recover (forward, not backward), but there is no earlier image copy with the specified TAG, hence: . So this happened all day until today. Today, this script ran again: 

When I have no information about earlier memory usage, I just go with a 1:4 ratio for PGA:SGA target. 

Depends on the data distribution. I can easily construct examples for both cases (index vs table scan). Just think about the case when the columns are unique or nearly unique (index), or when all the rows are the same (table scan). Technically your index can be used for the above query. Given the general nature of a column named , I would say the index will be chosen, but it is not guaraanteed. The predicate will be processed at the table level, not the index level, even if you make sure that the NULL rows are indexed as well: 

And that is what we have VPD (Virtual Private Database) for, a feature that developers like to reinvent. Using Oracle Virtual Private Database to Control Data Access Set up environment: 

The whole error message may reveal more, and I would check the specific table in the dictionary views (, ), and look for such "anomalies". 

The above should work without the attribute. The point of proxy privilege is to enable logging in without disclosing the password of the schema. 

As the error message already states, this is not an Oracle issue. The backup piece simply can not be found on the TSM server where Oracle searches - based on the tdpo configuration. Maybe it was created with a different tdpo configuration under a different node name or in another filespace. Maybe it is simply not there on the TSM server. You can query the contents of TSM for this tdpo config with: 

There you should see LGWR. However, specifying ARCH or LGWR became deprecated in 11g: $URL$ Deprecated does not mean it will not work, so it is still possible to specify in and revert to the way it worked in 10.2 (and what you described). But that is not how you configure anymore, because of the changes, even uses and standby redo logs. See below: $URL$ 

And by create, I really mean create, from scratch. A DBCA custom database, or running and dictionary scripts manually. If this happened, these will be your database level NLS properties: 

Could be anything. Typically environmental/configuration issue. Time to do an /, depending on the platform, which we don't know: 

I do not see your exact database version anywhere, but it seems to be the bug described in the below MOS note: RMAN-06025 - RMAN RESTORE DATABASE PREVIEW at standby site is asking for old log (Doc ID 1599013.1) 

The unique names of the databases in a Data Guard configuration are listed in the log_archive_config parameter in the following form: 

Not an Oracle/SQL*Plus problem. The default size of commandline window screen buffer is 300 (lines), and the maximum is 9999 (lines), so it is unable to display 157998 rows. Better spool the output to a file: 

1158 is the default port for Enterprise Manager Database Console, not the database listener, and you can not log in there using SQL*Net or JDBC. The default port for the database listener is 1521, try connecting using that port. You can check the listener port by: 

You got the error, because you have a container database, and you tried to create a common user in the root container without the common user prefix. Introduction to the Multitenant Architecture 

UPDATE In order to start the instance without interfering with an existing running instance (e.g ORCL), before starting anything: 

You can configure this behaviour through the Administration Assistant, where you can choose to start (or not to start) the database instance with the Windows service. Alternatively, you can do this with oradim, e.g: 

Standard index build locks the table for the whole index build process, and DML statements on the table will be blocked. You may call that a complication. Online index build is a feature of Enterprise Edition. This method locks the table at the beginning, and at the end of the index build process. In between, DML operations on the table can run as usual. Creating an Index Online 

Start the old application, and log in. Find your session in the database (let's say: sid=12, serial#=3456), and enable SQL tracing for that session, for example: 

It is better to create a role, grant that role to the other user, and maintain the grants to the role when you create new objects. 

can be locked without any difficulties. is different though. You can't lock it, even if you can, you can't. 

After that you can use your above commands for identifying or deleting these files. Deleting old backups in a safe way is achieved by running , with the required retention set in configuration or specified in the command (e.g. ). 

With only the pfile and datafiles, you will not be able to open the database "by the book". You can recreate the lost controlfiles with a statement (specifying the remaining datafiles), but the next step would be a recovery, using the redo logs, which you do not have. If you want to try to salvage some data from the database, you can open it without recovering. After you have created the controlfile and your database is in state: 

Filtering options in Data Pump are still something that should be improved. Anyway, you could try the below parameters: