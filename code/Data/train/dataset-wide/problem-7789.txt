If $X$ is a set and $I$ is an ideal on $X$. Let $\mathbb{P}$ be the forcing poset consisting of $I^+$ subsets of $X$ with the subset partial ordering. Let $G$ be $\mathbb{P}$-generic filter over $M$, where $M$ is the ground model. As in Jech Lemma 22.13, it can be shown that $G$ is a $M$-ultrafilter on $X$. I presume this means that in $M[G]$, it is proved that $G$ is $M$-ultrafilter. Then the generic ultrapower is defined in $M[G]$ as follows : it consist of the equivalence classes of function $f : X \rightarrow M$ with $f \in M$ under the usual ultrapower equivalence relation with $\in^{Ult_G(M)}$ defined in the usual way. I would like to think of the generic ultrapower as a class model in $M[G]$, but it is not immediately clear that it is definable. The first issue is how exactly is Lemma 22.13 phrased in $M[G]$. How does $M[G]$ talk about $M$? Then in the construction of the ultrapower, the equivalence class can of course be made into sets by considering element of least rank in each class. However, $M[G]$ still needs to distinguish those $f : X \rightarrow M$ which are in the ground models. Again, the main issue here seems to be whether $M$ is an inner model of $M[G]$. I am vaguely aware of some result about $M$ being in some sense definable in $M[G]$? I do not know the result precisely to be able to tell if this result could be used to think of the generic ultrapower as a proper class. Nevertheless, it seems that Solovay used the generic ultrapower well before this result was known. How did he handle the definability issue of the generic ultrapower? 

To me, the only "unintuitive" applications of uncountable choice is when it turns up in physics. The only case I know where this happens is in the maximal-extension theorem of Choquet-Bruhat (QM does not use uncountable choice). This uses local extension properties of solutions to General Relativity to prove, using Zorn's lemma, that there exists a maximal extension. The use of axiom of choice is, I think, essential. I couldn't see how to sidestep it when I read the paper a long time ago (somebody please correct me if I am wrong). What is the axiom of choice doing in physics? I believe that it is entirely due to the issue of double-sided maximally extended black holes. A maximal extension of General Relativity can contain "wormhole" like solutions (for example, a charged black holes with two patches connected by an interior region), and there can be countably many such bridges in any asymptotically flat patch. But each of these branches can connect you to another different asymptotically flat region, which might have its own countably infinite collection of bridges to other flat regions. The resulting spacetime is like a tree with countably many branches at each node, where each node represents an asymptotically flat spacetime, and each edge is a double-sided maximally extended black hole bridging the two nodes. Such a tree can have infinite depth, and you must extend the solution to the whole tree. It seems intuitive that to patch the solutions together you need to extend the local solutions over continuum many nodes, and since GR is hyperbolic, you will get to make some arbitrary choices at each extension step. The dependence on choice then simply shows how unreasonable the maximally extended model of General Relativity is for physics. 

Let $\mathbb{S}$ denote Sacks forcing. Let $\mathbb{S}_{\omega_2}$ denote the $\omega_2$-length countable support iteration of Sacks forcing. Let $G_{\omega_2} \subseteq \mathbb{S}_{\omega_2}$ be generic over $L$ for $\mathbb{S}_{\omega_2}$. By the usual results around proper forcing, you can show that $L[G_{\omega_2}] \models 2^{\aleph_0} = \aleph_2$. You can show that no Cohen reals over $L$ are added by showing that every real of $L[G_{\omega_2}]$ is contained in a ground model coded closed meager set. Note that Cohen forcing is forcing equivalent to $\mathbb{P}_{I_\text{meager}}$, the forcing of nonmeager Borel sets. Hence a Cohen real is not contained in any ground model meager set. For simplicity, let first consider one Sacks forcing $\mathbb{S}$. Let $\tau \in V^{\mathbb{S}}$ and $\tau$ be a name for a real not in the ground model. Then do a fusion argument to produce a condition $p$ so that at every split node $s$ of $p$, $p_{s0}$ and $p_{s1}$ determines a certain finite amount of $\tau$ and what $p_{s0}$ determines about $\tau$ and what $p_{s1}$ determines about $\tau$ differs in at least two places. If you let $T$ be the set of all finite strings $t$ so that $t$ is an initial segment of what $p_s$ can determined about $\tau$ for some $s \in {}^{<\omega}2$, then you get a tree so that at each split node the two sides differ two times before splitting again. By the definability of forcing, $T \in L$. You can show that body $[T]$ is meager closed set and $p \Vdash \tau \in [\check T]$. If you understand how to do this for 1-Sacks forcing, now do an iterated Sacks forcing version of this. This tends to be quite heavy in notation. See Geschke and Qickert $\textit{On Sacks Forcing and the Sacks property}$ for more information. Essentially the argument above is a modified version of their proof of the 2-localization property. Also see their paper on how to do the iterated Sacks version of the 2-localization property. 

The reason this is obvious is because the deSitter space maximizes the horizon area, which is a measure of the entropy. 

Here are a different class of obvious theorems, these are only obvious in the sense of physical intuition. They took a long time to prove: 

The most destructive aspect of uncountable choice is that it conflicts with random choice. With uncountable choice, any object which is constructed using randomness, like a random walk, a random field, or even a randomly picked real number, cannot exist, because there are sets which it cannot consistently be assigned membership to. In order to define what it means to have a random walk, or a random graph, or a random infinite Ising model configuration, or whatever, you need to define what it means to have an infinite sequence of random coin flips. The result can be encoded as a real number, the binary digits of which are the results of the coin flip, and if this real number really exists, as an actual mathematical object, then this object either belongs to any given set S, or it doesn't. It is so intuitive to think of random objects this way, that they are often illustrated with pictures, showing us what they look like (see $URL$ for a picture of a "realization" of a random walk). These pictures do not signify anything when the axiom of choice is present. The reason is that once you have actual random objects, for which you can assign membership to any set S, then you can define the probability of landing in S by choosing random objects again and again, and asking what fraction of the time you land in S. This always converges, because given any long finite sequence of 1's and 0's which represent independent random events, any permutation of the 1's and 0's has the same likelihood. This means that it is probability 0 that the seqeunce will oscillate in any way, and with certainty it will converge to a unique answer. This answer is the measure of the set S, and every set is measurable in this universe. This makes analysis much easier, because everything is integrable, measurable, etc. This is so intuitive, that if you look at any probability paper, they will illustrate with random objects without hesistation, implicitly denying choice. (I realize that this answer overlaps with a previous one, but it corrects a serious central mistake in the former.) 

My ideas were: Since $|\mathbb{P}| < \kappa$, $\kappa$ would still be measurable in any forcing extension of $V$ by a generic of $\mathbb{P}$. So if there exists $G \subseteq \mathbb{P}$ which is $V$-generic and $\pi'' g \subseteq G$, then if one could lift $\pi$ to a map $M[g] \rightarrow V[G]$ which takes some measure $\tilde U' \in M$ to some measure $\tilde U \in V[G]$, then this should show $M[g]$ is iterable. However, I am not sure such a $G$ can exists. Since $g \in V$, I do not think it can be used make $G$ in the ways that usual liftings are done. Thanks for any information about this problem. 

Let $V$ satisfy there exists a measurable cardinal. Let $\kappa$ be a measurable cardinal and $U$ be the normal measure on $\kappa$ witnessing this. Let $\mathbb{P}$ be a forcing of size less than $\kappa$. Let $M$ be a countable transitive structure such that there exists an elementary embedding $\pi : M \rightarrow V$ (or a large $V_\Theta$). Suppose $U' \in M$ is such that $\pi(U') = U$. Suppose $\mathbb{Q} \in M$ is such that $\pi(\mathbb{Q}) = \mathbb{P}$. It is known that $M$ can be iterated using $U'$ throughout the ordinals of $V$. In particular, $M$ can be iterated $\omega_1^V$ many times. Since $M$ is countable, there is a $g \subseteq \mathbb{Q}$ which is $\mathbb{Q}$-generic over $M$ and $g \in V$. My question is: Is $M[g]$ also $\omega_1^V$-iterable? 

The question is muddled regarding quantum mechanics: The projection operator T takes measures to measures, but it does not define a Markov chain, or if you like, the resulting Markov chain is not an interesting one, because it is deterministic most of the time. The reason is that T is idempotent, a second application of T does nothing. Even if omega varies continuously, doing T on a continuously varying omega is deterministic in the limit of continuous measurement (see below). The specific questions are not the right questions, but here is an answer: 

Here is a physically obviously true statement, which can be seen from physical intuition, but which is not proven (as far as I know): 

Here is a problem that requires nontrivial integration techniques, because the closed form answer is a sum of an algebraic and trigonometric function. If you have ever pulled down blinds by the edge, the parallel slats slant down and make an envelope for a certain curve. In the limit of infinitely dense slats, what is this curve? In case you can't picture this, the curve is defined by the condition that the length of the tangent line from the curve to the y-axis is constant. This is one of the few cases where a reasonably general integral comes out naturally. As for the IVT/MVT, they are not particularly profound. It is in my opinion better to prove things by bisection (which easily proves both, gives intuitive proofs for all their consequences, and essentially is sequential compactness). Bisection was used in 19th century texts, but fell out of favor when the completeness of the reals became standardly axiomatized as the least upper bound principle. 

I was reading this question, specifically Brian's answer. In particular I am having a bit of trouble digesting the following sentence: 

Unfortunately, in my mind there are several ways in which to interpret this statement. Let's assume that finite generating set refers as a module (not as an $R$-algebra). One interpretation that I tried and gives me trouble is the following: For simplicity, take $S$ to affine, $\newcommand{\Spec}{\text{Spec}}$ $S=\Spec(R)$, and $Z=\Spec(B)$ even with $B$ free as a module over $R$, say of rank $N$ and a basis $b_1,\ldots,b_N$; then I don't think the condition for full set of sections (say in the form (1) of definition (1.8.2) on page 33 of Katz-Mazur) for $b_1,\ldots,b_N$ implies it for every $b\in B$. When trying to prove it, roughly speaking, I ran into the problem that "additivity fails", i.e. if (1) holds for $b$ and $c$, then it doesn't hold for $b+c$ (if this were usual linear algebra over a field, I believe this translates to the fact that $b$ and $c$ viewed as linear maps on $B$ via multiplication, have different eigenvectors). However, I didn't completely disprove it. It may be possible that the algebra structure on $B$ (or even the Hopf algebra structure) somehow guarantees that all is good? (doesn't seem so to me though, even if the matrices of $b$ and $c$ commute since $bc=cb$). Or maybe it was meant that even though we work Zariski-locally, say on $\Spec R$ we still require the definition of full set of sections for any $\Spec R'\to\Spec R$. (i.e. I am allowed to pass to $R$-algebras). In this direction, alternatively (and maybe that's what Brian meant, but it doesn't seem exactly 'Zariski locally' anymore) one can pass to the "universal element" $f=\sum T_ib_i\in B\otimes_RR[T_1,\ldots,T_n]$ as on page 38 of K-M and then indeed, it suffices to check condition (1) (or (2)) for this universal element, yielding a big identity but in finitely many variables. Could anybody help clarify this please? As a bonus, I would appreciate some intuition as to why the condition for full set of sections is so "linear" (in my not-very-developed understanding it seems to barely touch upon the algebra structure of $B$ and almost not at all on the Hopf algebra structure) - it seems a bit strange that it encompasses so much "geometric" information (at least the way I think of level structures). Thanks! 

Suppose $\mathbb{P}$ is a definable proper forcing (for instance Sacks forcing). Let $\alpha$ be some ordinal. Let $\mathbb{P}_\alpha$ be the countable support iteration of $\mathbb{P}$ of length $\alpha$. It is well known that $\mathbb{P}_\alpha$ is also proper. Hence for any countable elementary structure $M \prec H_\Theta$ for sufficiently large $\Theta$ such that $\mathbb{P}_\alpha \in M$ and any $p \in \mathbb{P}_\alpha \cap M$, there exists some $q \leq_{\mathbb{P}_\alpha} p$ which is a $(M, \mathbb{P}_\alpha)$-generic condition. Let $X = \alpha \cap M$. The question is whether a $(M, \mathbb{P}_\alpha)$ generic condition $q \leq_{\mathbb{P}_\alpha} p$ can be found such that $\text{supp}(q) \subseteq X$. Thanks for any information that can be provided. 

Of course, a formula $\varphi$ with all these properties must be a counterexample to the Vaught's conjecture. So the real essence of this question is whether (regardless of the status of Vaught's conjecture), is it possible that the Scott rank of models of $\varphi$ take on all admissible values but no model of $\varphi$ has Scott rank in between two consecutive admissible ordinal. The natural idea to try is to use Barwise compactness to produce a model of $\varphi$ of intermediate Scott rank. However, whenever one works in a countable admissible fragment $\mathscr{L}_A$, one can not really express non-isomorphism to the model whose Scott rank is the ordinal height of the admissible set $A$. So any model $M$ produced using a Barwise argument using $L_A$, could just be isomorphic to the possibly unique model with Scott rank the ordinal height of $A$. The question is whether by some reason there must be a model of $\varphi$ whose Scott rank is between two consecutive admissible ordinals. Thanks for any information on this question. 

Since this was resurrected, here is the statement that at this time seems to me to have the greatest gap between obviousness of truth and obviousness of proof: 

Constructing quantum field theories is a well-known problem. In Euclidean space, you want to define a certain measure on the space of distributions on R^n. The trickiness is that the detailed properties of the distributions that you get is sensitive to the dimension of the theory and the precise form of the action. In classical mathematics, measures are hard to define, because one has to worry about somebody well-ordering your space of distributions, or finding a Hamel basis for it, or some other AC idiocy. I want to sidestep these issues, because they are stupid, they are annoying, and they are irrelevant. Physicists know how to define these measures algorithmically in many cases, so that there is a computer program which will generate a random distribution with the right probability to be a pick from the measure (were it well defined for mathematicians). I find it galling that there is a construction which can be carried out on a computer, which will asymptotically converge to a uniquely defined random object, which then defines a random-picking notion of measure which is good enough to compute any correlation function or any other property of the measure, but which is not sufficient by itself to define a measure within the field of mathematics, only because of infantile Axiom of Choice absurdities. So is the following physics construction mathematically rigorous? Question: Given a randomized algorithm P which with certainty generates a distribution $\rho$, does P define a measure on any space of distributions which includes all possible outputs with certain probability? This is a no-brainer in the Solovay universe, where every subset S of the unit interval [0,1] has a well defined Lebesgue measure. Given a randomized computation in Solovay-land which will produce an element of some arbitrary set U with certainty, there is the associated map from the infinite sequence of random bits, which can be thought of as a random element of [0,1], into U, and one can then define the measure of any subset S of U to be the Lebesgue measure of the inverse image of S under this map. Any randomized algorithm which converges to a unique element of U defines a measure on U. Question: Is it trivial to de-Solovay this construction? Is there is a standard way of converting an arbitrary convergent random computation into a measure, that doesn't involve a detour into logic or forcing? The same procedure should work for any random algorithm, or for any map, random or not. EDIT: (in response to Andreas Blass) The question is how to translate the theorems one can prove when every subset of U gets an induced measure into the same theorems in standard set theory. You get stuck precisely in showing that the set of measurable subsets of U is sufficiently rich (even though we know from Solovay's construction that they might as well be assumed to be everything!) The most boring standard example is the free scalar fields in a periodic box with all side length L. To generate a random field configuration, you pick every Fourier mode $\phi(k_1,...k_n)$ as a Gaussian with inverse variance $k^2/L^d$, then take the Fourier transform to define a distribution on the box. This defines a distribution, since the convolution with any smooth test function gives a sum in Fourier space which is convergent with certain probability. So in Solovay land, we are free to conclude that it defines a measure on the space of all distributions dual to smooth test functions. But the random free field is constructed in recent papers of Sheffield and coworkers by a much more laborious route, using the exact same idea, but with a serious detour into functional analysis to show that the measure exists (see for instance theorem 2.3 in $URL$ This kind of thing drives me up the wall, because in a Solovay universe, there is nothing to do--- the maps defined are automatically measurable. I want to know if there is a meta-theorem which guarantees that Sheffield stuff had to come out right without any work, just by knowing that the Solovay world is consistent. In other words, is the construction: pick a random Gaussian free field by choosing each Fourier component as a random gaussian of appropriate width and fourier transforming considered a rigorous construction of measure without any further rigamarole? EDIT IN RESPONSE TO COMMENTS: I realize that I did not specify what is required from a measure to define a quantum field theory, but this is well known in mathematical physics, and also explicitly spelled out in Sheffield's paper. I realize now that it was never clearly stated in the question I asked (and I apologize to Andreas Blass and others who made thoughtful comments below). For a measure to define a quantum field theory (or a statistical field theory), you have to be able to compute reasonably arbitrary correlation functions over the space of random distributions. These correlation functions are averages of certain real valued functions on a randomly chosen distribution--- not necessarily polynomials, but for the usual examples, they always are. By "reasonably arbitrary" I actually mean "any real valued function except for some specially constructed axiom of choice nonsense counterexample". I don't know what these distribtions look like a-priory, so honestly, I don't know how to say anything at all about them. You only know what distributions you get out after you define the measure, generate some samples, and seeing what properties they have. But in Solovay-land (a universe where every subset S of [0,1] is forced to have Lebesgue measure equal to the probability that a randomly chosen real number happens to be an element of S) you don't have to know anything. The moment you have a randomized algorithm that converges to an element of some set of distributions U, you can immediately define a measure, and the expectation value of any real valued function on U is equal to the integral of this function over U against that measure. This works for any function and any distribution space, without any topology or Borel Sets, without knowing anything at all, because there are no measurability issues--- all the subsets of [0,1] are measurable. Then once you have the measure, you can prove that the distributions are continuous functions, or have this or that singularity structure, or whatever, just by studying different correlation functions. For Sheffield, the goal was to show that the level sets of the distributions are well defined and given by a particular SLE in 2d, but whatever. I am not hung up on 2d, or SLE. If one were to suggest that this is the proper way to do field theory, and by "one" I mean "me", then one would get laughed out of town. So one must make sure that there isn't some simple way to de-Solovay such a construction for a general picking algorithm. This is my question. EDIT (in response to a comment by Qiaochu Yuan): In my view, operator algebras are not a good substitute for measure theory for defining general Euclidean quantum fields. For Euclidean fields, statistical fields really, you are interested any question one can ask about typical picks from a statistical distribution, for example "What is the SLE structure of the level sets in 2d"(Sheffield's problem), "What is the structure of the discontinuity set"? "Which nonlinear functions of a given smeared-by-a-test-function-field are certainly bounded?" etc, etc. The answer to all these questions (probably even just the solution to all the moment problems) contains all the interesting information in the measure, so if you have some non-measure substitute, you should be able to reconstruct the measure from it, and vice-versa. Why hide the measure? The only reason would be to prevent someone from bring up set-theoretic AC constructions. For the quantities which can be computed by a stochastic computation, it is traditional to ignore all issues of measurability. This is completely justified in a Solovay universe where there are no issues of measurability. I think that any reluctance to use the language of measure theory is due solely to the old paradoxes. 

First, several bullet points in your question are nontrivial results. However, there is another way to see $\mathsf{AD}_\mathbb{R}$ does not holds in $L(\mathbb{R})$. Using a two step game, you can show $\mathsf{AD}_\mathbb{R}$ can prove uniformization for all sets of reals. Consider the relation $R(x,y)$ if and only if $y$ is not ordinal definable from $x$. Assuming that $L(\mathbb{R}) \models \mathsf{AD}_\mathbb{R}$, $R$ would have a uniformization. Every set of reals in $L(\mathbb{R})$ is ordinal definable from some real. Use this real to get a contradiction by diagonalization. Also note that if you put the discrete topology on $\mathbb{R}$, a two point game is a closed game. Hence $\mathsf{ZF + AD}$ can not prove the Gale-Stewart theorem (closed determinacy) for games on $\mathbb{R}$. Of course, this is because there is no well-ordering of $\mathbb{R}$.