When $n=1$ then your matrices $\sigma$ and $\tau$ must be zero (since they are skew-symmetric), and hence your two generators are equal to one. But $-id\in SO_{2n}(\mathbb F_p)$, so the group is not actually trivial. But even if $n>1$ there is nothing that keeps you from choosing $\sigma=\tau=0$. So maybe you want to at least consider all matrices of the given form. Edit: So, following the comments, I now assume that you let $\sigma$ and $\tau$ range over all skew-symmetric matrices (instead of just picking two; however it still suffices to let them range over a basis). Still, for $n=2$, the group generated by the matrices from the question is isomorphic to $SL_2(\mathbb F_p)$. Now one just has to compare orders to see that this isn't $SO_{4}(\mathbb F_p)$. Or just use GAP: 

As others have mentioned, your claim isn't necessarily true. It seems to be true though that we may write $S=1\cdot R \oplus P$ for some projective $R$-module $P$. This shows that $1_S$ can be choosen as a basis element for instance if $R=K[x_1,\ldots,x_n]$ or if $R$ is a PID (basically whenever there is a reason for a projective module to be free). Let $X_1,\ldots,X_n$ be an $R$-basis of $S$, and write $$ S \cong R[x_1,\ldots,x_s]/(x_ix_j -\sum_{k}r_{ijk} x_k,\ a_1x_1+\ldots+a_nx_n-1) $$ such that sending $X_i$ to $x_i$ is an isomorphism. I claim the ideal $(a_1,\ldots, a_n)_R$ in equal to $(1)_R$. If that is so, then there are $t_1,\ldots,t_n\in R$ such that $\sum t_i a_i = 1$, and therefore $$\varphi: \ S\longrightarrow R: X_i \mapsto t_i$$ is an $R$-module homomorphism mapping $1_S$ to $1_R$. Sending $1_R$ to $1_S$ defines a splitting of this epimorphism, and therefore $S \cong 1_S\cdot R \oplus {\rm ker}(\varphi)$. So $S$ is a direct sum of $R$ and the projective $R$-module ${\rm ker}(\varphi)$. All that is left to show is that $(a_1,\ldots,a_n)_R = (1)_R$ actually holds. Assume this is not true, and suppose $a_1,\ldots,a_n$ are all contained in some maximal ideal $\mathfrak m$ of $R$. Then, by looking at the relation $a_1 x_1+\ldots+a_nx_n-1=0$ mod $\mathfrak m$, we see that $S/\mathfrak m S =0$, which is a contradiction, since $S$ is assumed to be free of rank $n$ over $R$. 

Okay, suppose $K$ is a field and you have three elements $a,b,c\in K[t]$ with $gcd(a,b,c)=1$. I will show that the row $(a,b,c)$ can always be extended to a matrix in $SL(3,K[t])$. Suppose that at least one of $b$ and $c$ is nonzero (otherwise $a\in K$ and extending the row is trivial). Let $g:=gcd(b,c)$ and choose $x,y\in K[x]$ such that $xb+yc=g$. Since $gcd(a,g)$ must be one, we find $r,s\in K[x]$ such that $ra+sg=1$. Now the following $3\times 3$-matrix has determinant $1$ and extends the row $(a,b,c)$: $$ \left[\begin{array}{ccc}a&b&c\newline -s& \frac{rb}{g} & \frac{rc}{g} \newline 0 & -y & x\\ \end{array}\right] $$ It is an easy computation that this matrix has determinant one (unless I mistyped something). Note that the occurring fractions lie in $K[x]$, since $g$ divides both $b$ and $c$ by definition. 

Edit 2: The matrices don't necessarily generate the group for $n=3$ either (checked for $p=5$ and $p=7$; for bigger $n$ the computation take longer than a few seconds so I didn't go there): 

I think the author accidentally described the dual of the Hopf algebra you're thinking of. Finite group rings are usually endowed with multiplication $(g,h)\mapsto gh$ and comultiplication $g \mapsto g\otimes g$ (see here). The coordinate ring $k[G]$ is obtained by dualizing. Then $g \mapsto g\otimes g$ becomes $e_g^2 = e_g$, where $e_g$ is the function on $G$ that maps $g$ to $1$ and all other group elements to $0$. Comultiplication will look exactly the way you described it (i.e. $e_g \mapsto \sum_h e_{gh^{-1}}\otimes e_h$). 

No, the kernel of the map $\varphi: K_0(\Lambda)\longrightarrow K_0(A)$ is not necessarily trivial for all orders $\Lambda$ in $A$. In fact, it is only trivial for all orders in $A$ if $A$ is a direct sum of fields and skew fields. Assume the Wedderburn decomposition of $A$ has the matrix algebra $M_n(D)$ as a simple component (with $n > 1$ and $D$ a skew field). Let $\Delta$ be the maximal order in $D$ and let $\Pi$ be its maximal ideal. Then define an order $$ \Lambda_1 := \left[ \begin{array}{ccccc} \Delta & \Pi & \Pi& \cdots & \Pi \newline \Delta & \Delta & \Pi & \cdots & \Pi \newline \vdots & \vdots & \ddots & & \vdots \newline \Delta &\Delta &\Delta &\cdots & \Delta \end{array}\right] \subset M_n(D) $$ Note that $\Lambda_1$ has $n$ non-isomorphic projective lattices (corresponding to the rows or, if you prefer left modules, the columns ). All of these become isomorphic to $D^{n}$ upon tensoring with the ground field. Thus the map $K_0(\Lambda_1)\cong \mathbb Z^n\longrightarrow K_0(A)\cong \mathbb Z$ has non-trivial kernel. Now if $A$ decomposes as $A \cong M_n(D)\oplus B$, then just choose an any order $\Lambda_2 \subset B$ and you get a counterexample $\Lambda=\Lambda_1\oplus \Lambda_2 \subset A$. Note that the choice of $\Lambda_1$ above is arbitrary. Any order in $M_n(D)$ with at least two non-isomorphic projective modules would do the trick. 

Edit: The answer is yes for all finite groups $G$ (even, as far as this makes sense, independently of the ground field). A reference is Theorem 1 on page 45 of Alperin: "Local representation theory". While the claim of this theorem only refers to the tensor product $V\otimes \ldots \otimes V$ for some faithful $kG$-module $V$, the proof actually constructs a free submodule in $$ k[V] \cong \bigoplus_{i=0}^{\infty} S^k V $$ which gives you exactly what you need. 

Edit (concerning the rank, which is $n!$): Let $G$ be a finite group acting on $R=k[x_1,\ldots,x_n]$ and let $H\leq G$ be a subgroup. Then $$ {\rm frac}(R^G)\otimes_{R^G} R^H \cong (R^G-\{0\})^{-1}R^H \cong {\rm frac}(R^H) $$ where ${\rm frac}$ denotes the fraction field (this is easily seen from the fact that by a construction similar to the Reynols operator, the denominator of a fraction over $R^H$ can always be made $R^G$-invariant). Let $Q={\rm frac}(R)$, then one can show in a similar fashion $Q^G = {\rm frac}(R^G)$. We conclude that $$ {\rm rank}_{R^G} R^H = {\rm dim}_{{\rm frac}(R^G)} {\rm frac} (R^H) = [Q^H:Q^G] = [G:H] $$ the last equality being due to Galois theory. 

For a finite abelian group $G$ the answer is yes. Assume $V=\langle v_1\rangle\oplus \ldots \oplus \langle v_n \rangle$ is the decomposition of a faithful $\mathbb C G$-module $V$ into irreducibles. Denote the character associated to $v_i$ by $\chi_i$. Since $V$ is faithful the $\chi_i$ must generate the character group $\textrm{Hom}(G,\mathbb C^\times)$. So we can write any irreducible character $\psi$ of $G$ as $$ \psi = \chi_1^{k_1}\cdots \chi_n^{k_n} $$ and then the vector $v_1^{k_1}\cdots v_n^{k_n} \in S^{k_1+\ldots+k_n} V$ spans a one-dimensional submodule of this symmetric power with character $\psi$. 

Take $\textrm{Mod-}A$, where $A$ is a finite dimensional local $k$-algebra ($k$ a field). Let $\mathcal C$ be the category whose objects are the same as those of $\textrm{Mod-}A$, but whose homomorphisms are replaced by all $k$-vector space homomorphsims, i. e. $\textrm{Hom}_{\mathcal C}(X,Y) := \textrm{Hom}_k(X,Y)$. By construction, $\textrm{Mod-}A$ is a wide subcategory of $A$, and clearly it isn't full unless $A$ is equal to $k$. Moreover, $\mathcal C$ is equivalent to the category of vector spaces over $k$ (since every $k$-vector space can be given the structure of a $k$-module by letting $A$ act via $A/\textrm{Rad}(A)\cong k$). Hence $\mathcal C$ is abelian. Even kernels and cokernels in $\textrm{Mod-}A$ coincide with those in $\mathcal C$. 

The minimal polynomial $\mu_A$ of each matrix $A\in M_n(\mathbb R)$ with $P(A)=0$ divides $P$. Clearly, due to Jordan normal form, there are only finitely many conjugacy classes of such matrices. If you decompose $P$ into irreducible factors: $$ P = P_1^{k_1}\cdots P_r^{k_r} $$ then representatives for each equivalence classes of solutions are given by diagonal joins Jordan blocks with minimal polynomial dividing one of the factors $P_i^{k_i}$. Assume for simplicity that the $P_i$ all have degree $1$, then the number of equivalence classes is given by $$ \sum_{\lambda \textrm{ partiton of $n$ with $r$ parts}} \prod_{i=1}^r \textrm{(Number partitions of $\lambda_i$ with parts $\leq k_i$)} $$ If $P$ doesn't split into linear factors you can modify the formula accordingly. Or, you could just work over $\mathbb C$, since the exact number of eq. classes of solutions over $\mathbb C$ will still be an upper bound for the number of solutions over $\mathbb R$. 

The reason is simple: if $W\oplus W$ (for some irreducible module $W$) is a direct summand of $V$, then the non-commutative matrix ring $\textrm{End}(W\oplus W) \cong M_2(\mathbb C)$ embeds into $\textrm{End(V)}$). You are dealing with permutation modules, and for those there is a very explicit description of the endomorphism ring in terms of double cosets: $$ \textrm{End}_{\mathbb CG}(\mathbb C [G/H]) \cong \langle \sum_{h\in HgH} h \mid \textrm{ $g$ runs over double coset representatives}\rangle_{\mathbb C} $$ You just need to check whether this ring is commutative. One criterion is that $H\backslash G /H $ has cardinality $\leq 3$ (because any semisimple algebra of dimension $\leq 3$ must be commutative). A reference is Curtis-Reiner: "Methods of representation theory vol. 1". In this book these endomorphism rings are called "Hecke algebras", defined on page 281 (the endomorphism ring above corresponds to the Hecke algebra $\mathcal H(G,H,1_H)$). However, computations in GAP show that $\mathbb C [GL_3(3)/GL_2(3)]$ is not multiplicity-free, so I see little hope for you question about permutation modules for $GL_n(q)$. 

No (at least if you really mean abstract group embeddings). Choose $m=n=2$. Note that $$UT(2,\mathbb R) \cong (\mathbb R,+)\quad \textrm{ and }\quad T^*(2,\mathbb R)\cong UT(2,\mathbb R)\rtimes (\mathbb R_+,\cdot)^2 $$ Note secondly that the image of the conjugation action of $T^*(2,\mathbb R)$ on $UT(2,\mathbb R)$ is equal to $(\mathbb R_+,\cdot)\leq Aut((\mathbb R,+))$. In particular, $UT(2,\mathbb R)$ splits up into only three orbits under this action. I claim there is an injective homomorphism from $UT(2,\mathbb R)$ into itself such that its image $G$ splits up into many more orbits under the action of the normalizer of $G$ in $T^*(2,\mathbb R)$. Such an embedding clearly cannot be extended to all of $T^*(2,\mathbb R)$. As a $\mathbb Q$-vector space (and hence also as an abelian group), $\mathbb R\cong \bigoplus_I \mathbb Q$ for some index set $I$ of the same cardinality as $\mathbb R$. Let $S$ be a transcendence basis of $\mathbb R$ over $\mathbb Q$ (so the cardinality of $S$ will be the same as that of $I$). Let $G$ be the $\mathbb Q$-span of $S$. So there is an isomorphism from $(\mathbb R, +)$ to $(G,+)\subsetneq (\mathbb R,+)$. Now if $s_1\neq s_2\in S$, then the unique element in $\mathbb R-\{0\}$ sending $s_1$ to $s_2$ is $r=s_2/s_1$. But if $r\cdot s_2 = s_2^2/s_1$ could be written as a $\mathbb Q$-linear combination $\sum_i c_i s_i$ of elements in $S$, then we would get an algebraic relation $$ s_1(\sum_i c_i s_i)-s_2^2 $$ between the elements in the transcendence basis $S$. Therefore $r\cdot s_2 \notin G$, i.e. $r\cdot G \nsubseteq G$, thus proving that all elements of $S$ lie in different orbits under the action of the normalizer of $G$ in $T^*(2,\mathbb R)$. 

I assume (based on your example) that you're primarily interested in the case where $R$ is the ring of integers in an algebraic extension of $\mathbb Q$. Then your property of being "prime-like" is equivalent to the property of generating a primary ideal. So assume $R$ is a Dedekind domain (in particular ideals factor uniquely into products of prime ideals), and let $p\in R$ be an arbitrary element. Then 

I can show that the claim is true if $A$ is defined over $\mathbb F_{p^\infty}$, the algebraic closure of the field with $p$ elements. This proof will unfortunately not generalize to arbitrary algebraically closed fields (not even such of characteristic $p$), nor will it bound the exponent $n$ such that $\Omega^n \cong \rm id$ in any useful way. It does however show that it suffices to know that each simple module is periodic in order to conclude that $\Omega^n\cong \rm id$ for some $n$ (again, of course, only for algebras over $\mathbb F_{p^\infty}$). Fix $n\in\mathbb N$ such that $\Omega^{n}(S) \cong S$ for any simple $A$-module $S$. All but the last paragraph is the same as Theorem 2.5 in K. Erdmann and A. Skowronski. Periodic algebras. Trends in Representation Theory and Related Topics. European Math. Soc., Zurich, 2008. (you can view the relevant parts of this on google books). First choose a projective cover $P$ of $_A A_A$ (i.e., $P$ is a projective $A$-$A$-bimodule). Then choose an $A$-$A$-bimodule $X$ as the kernel of the epimorphism from $P$ to $A$, i.e. we get a s.e.q. of $A$-$A$-bimdoules $$ 0 \longrightarrow X \longrightarrow P \longrightarrow A \longrightarrow 0 $$ As a sequence of left or right $A$-modules this sequence is split, hence $X$ is projective as a left and as a right $A$-module. This implies that tensoring the above sequence with any (left or right) $A$-module will again yield an exact sequence of left $A$-modules, with projective middle term (since $P\otimes_A M$ is projective for any $A$-module $M$). This shows that $-\otimes_A X$ is isomorphic to $\Omega(-)$ on the stable module category. Now $-\otimes_AX$ is a stable auto-equivalence of Morita-type, and so is $-\otimes_A X^{\otimes n}$. The latter sends simple modules to themselves, and therefore lifts to a Morita auto-equivalence, w.l.o.g. induced by the $A$-$A$-bimodule $Y$ (the fact that stable equivalences of Morita type which send simple modules to simple modules lift to Morita equivalences is usually attributed to Linckelmann). That is, $-\otimes_A Y \cong \Omega^n(-)$ on the stable module category. In particular $S\otimes_A Y \cong S$ for all simple $A$-modules $S$. But any Morita-autoequivalence which sends simple modules to themselves is induced by an automorphism $\alpha$ of $A$, i.e. $Y$ is isomorphic to the twisted $A$-$A$-bimodule $_{id} A_{\alpha}$. Now comes the ugly part which doesn't work over arbitrary fields: $\alpha: A \longrightarrow A$ must have finite order, since every non-zero element in $\mathbb F_{p^\infty}$ is a root of unity and therefore every invertible matrix over $\mathbb F_{p^\infty}$ has finite order ( ${\rm Aut}(A) \leq {\rm GL}(A)$, and ${\rm GL}(A)$ is a torsion group). Hence there is some $m\in \mathbb N$ such that $Y^{\otimes m} \cong {_AA_A}$. But then $$\Omega^{m\cdot n}(-)\cong -\otimes_A Y^{\otimes m} \cong -\otimes_A {_{id} A _{\alpha^m}} \cong \rm id$$ 

Okay, so basically the answer can be found in here: $URL$ Here's how the argument works (a simplified version of what is done in the paper with finite dimensions and $p=q$): (Note: we define "$\Re$" of a vector by taking the real part componentswise) Lemma 3.4 says (applied to the finite dimensional situation) $$ \int_0^{2\pi} \| \Re(e^{i\varphi} x) \|_p^p d\varphi = \int_0^{2\pi} |\cos(\varphi)|^p d\varphi $$ for any $x\in \mathbb C^n$ with $\|x\|_p=1$. This is fairly elementary to verify. Therefore, whenever $x,y\in \mathbb C^n$ both have norm $1$, we will find a $\varphi\in[0,2\pi]$ such that $$\|\Re(e^{i\varphi}x)\|_p \leq \|\Re(e^{i\varphi}y)\|_p$$ since the integral $$ \int_0^{2\pi}\|\Re(e^{i\varphi}y)\|_p^p - \|\Re(e^{i\varphi}x)\|_p^p d\varphi = 0 $$ is zero und thus the integrand has to be non-negative somewhere. Then Lemma 3.2 of the paper yields the result. What the authors do here is to take a vector $0\neq x\in \mathbb C^n$ such that $\|Ax\|_p/\|x\|_p$ is maximal (assume also that $A\neq 0$; then $Ax\neq 0$ follows automatically and we can divide by its norm below). Then they take a $\varphi$ such that $$ \left\|\Re(e^{i\varphi} \frac{x}{\|x\|_p})\right\|_p \leq \left\| \Re(e^{i\varphi} \frac{Ax}{\|Ax\|_p})\right\|_p $$ which is possible by the above. If $\Re(e^{i\varphi}x)\neq 0$, this can then be rewritten as $$ \frac{\|Ax\|_p}{\|x\|_p} \leq \frac{\|A \Re(e^{i\varphi}x)\|_p}{\|\Re(e^{i\varphi}x)\|_p} $$ which shows that the maximum is also attained at the real vector $\Re(e^{i\varphi}x)\in \mathbb R^n$. If $\Re(e^{i\varphi}x)=0$, then $i\cdot e^{i\varphi}x$ is a real vector at which the maximum is attained. 

i.e. "prime-like" in Dedekind domains is the same as "primary". An element $x$ in the fraction field of $R$ lies in $R$ iff the valuations $\nu_Q(x)$ are non-negative for all prime ideals $Q$ of $R$. So let $a,b\in R$ and assume $p^2\mid ab$. Then $\nu_Q(a/p)=\nu_Q(a)\geq 0$ and $\nu_Q(b/p)=\nu_Q(b)\geq 0$ for all primes $Q\neq P$. So to see that either $a/p$ or $b/p$ lies in $R$, one just has to check that one of them has positive $P$-valuation. But 

so either $\nu_P(a/p)\geq 0$ or $\nu_P(b/p)\geq 0$. On the other hand, if the ideal $pR$ isn't primary then $p$ is not prime-like (the construction in Julian's comment can be generalized). Of course I'm not sure what exactly you're looking for, but at least this clears up what is going on in your last example: while $2$ isn't "prime-like" in $\mathbb Z[\sqrt{5}]$, it is prime like in its integral closure $\mathbb Z[\frac{1+\sqrt{5}}{2}]$ (which is however unspectacular because it remains a prime in that ring).