You can find more on database storage in Oracle documentation: $URL$ $URL$ $URL$ Some basic information about and : $URL$ $URL$ $URL$ 

Yes, OS (Windows Server) and database (SQL Server) are separate entities. So the only question is compatibility between those two products. I'm guessing it was tested and should be ok, but at this moment SQL Server 2012 doesn't have Windows Server 2016 listed (and won't have, at least not till September): $URL$ Best thing You can do is download latest Technical Preview and test it with SQL Server 2012 in Your environment: $URL$ 

Another popular idea used in cases like this would be moving relation between and out of their own tables. Create third table, that will store just the relations between both existing tables and will allow for n:n relationship. Columns could look like this: with both of them being foreign keys to and . All You need would be adding unique constraint on both columns. Be aware that this (normalised) approach creates some overhead, as it will require additional in queries, so it might not be solution for Your data volume. 

CPU limits are described in various Microsoft documents, for example in Compute capacity limits by edition of SQL Server. On 3rd party resources, TF 8017 is documented on Steinar Anderson's Trace Flag list which, by the way, links to this question. 

It's in syntax, so You might want to check some tutorial or course, like one from CodeAcademy. Another great source helpful with learning about joins is Jeff Atwood's visual explanation of sql joins. Just like @a_horse_with_no_name and @Vladimir_Oselsky commented, You should switch from old ANSI-89 SQL syntax to current ANSI-92, which allows for easier to write (and read) queries with more flexible syntax. 

Don't think because it looks boring for a human, that it is bad for a computer (especially when running Oracle). But don't add further parameters just to force me show a solution using dynamic sql, instead avoid insane designs requiring such solutions. 

but at which levels session, database, server can this setting be influenced? Edit: I found this in msdn it seems to be possible to set this on server level. In the Server Properties dialog (Connections Page) I found the option changed and scripted it: implicit transactions on: 

After executing your code, you can check if it worked by selecting from the database. The next step will be to use some begin end; block not returning any result sets. Generally Ado can't process whole scripts. Using sql-server it can process batches, that are parts of a script separated by go statements. As far as I know for Oracle it can only process a single sql-statements or a begin end block. Edit: Here is a litte PowerShell you can use to try what is possible: 

I guess it is just a historic fact. It is just not yet done that way. From SQL Server I know that they are releasing such restrictions between 2005 and 2008 and we often haven step into problems when we run such scripts on 2005. 

The solution is very simple. The string of the query is just too long. I'm trying to optimize a stored procedure using dynamic sql with a parameter to decide if the sql is executed or output. I just copied the output into a fresh sql developer pane and tried to use the tuning advisor. The problem is that the generated output has a lot of trailing blanks, I just have to remove them. 

It will affect overall server performance (so second database will be affected). Shrinking database affects I/O mostly, as it moves pages around data files. But whole operation does put load on CPUs too and it's fully logged, so with database in mode it will put a strain on Your transaction log file too - it will grow up a lot. Shrinking file has more downsides You should be aware of. It might make Your datafiles bigger (because it needs space to move pages around). It will cause fragmentation, and rebuilding indexes needs space in data files too. Only time shrink is relatively safe is using option. Try avoiding data file shrink, as there are better options. For example moving tables and indexes into new filegroup and dropping old one - this one also requires provisioning enough space for operation, but is more elegant than shrink and doesn't cause fragmentation. Of course there are situations when shrink is ok (or is the only possible option), but You have to be aware of it's downsides and remember to defragment database afterwards. When space is an issue, defragmenting with or will prevent database from growing up again after shrink Paul Randal wrote few important articles about shrinking: $URL$ $URL$ 

It appears You had some MySQL/MariaDB/Percona instance previously installed, and some of data and config files were left in Your OS. The way that worked for me on that case ( with installed), with forgotten Maria's root password, was: 

Is there something like a directive which I can use in a script to force SSMS to enable/disable SQLCMD mode? 

In about 95 % of the queries I only want to select the non canceled rows. Further I want to be able implement unique constraints to some columns for the not canceled columns. What are the pros and cons of using NULL in some column as indicator for not canceled ? 

Here I will show some results I got while using Vincent Malgrat's approach. First I learned, that when using more than 1 such top 1 sub query based on different tables or order, I have to use the RANK() function and not the ROW_NUMBER() function. Second when using rank() there is the problem of ties. SQL Server's top 1 arbitrarily selects one of the rows with rank = 1 while using rank() can return more than 1 row. I think it is bad design to use SQL Server top 1 in these cases. To make the design correct some unique constraints (e.g. unique indexes ) have to be found, which prevent this ambiguity. Here is a SQL Server example if you want to try it by yourself. Comparing the execution plans of the last two select statements below shows that Vincent Malgrat's approach is better than the top 1 solution. 

I'm given the task to export the content of a huge table from a production database and to import the data into a database at a remote location. The Table has about 45,000,000 rows. using about 4 GB space in the database. Its 10 Columns are of type int, datetime and varchar(n) <= 255, but some of the varchar fields contain newlines and the usual field separators used by bulkcopy. I guess export to Excel is no option due to the number of rows. Export with SSIS to Flatfile is possible (took about 30 minutes), but reimport is not automatic, since some fields where spitted. My current idea is 

Oracle doesn't work like (or engines like or ). There is no concept of storing tables as stand alone files in filesystem. Your data is stored physically in (big) , which contain many logical objects, like or . are also grouped in another logical objects, called . While this doesn't cover whole concept of Oracle's storage system, it should show You, in short, how it is designed. It's bit more complicated with database on , which is Oracle's volume manager and file system (and a variation/implementation of ), or, in older versions, mechanism called . If You have administrative rights in database You can check this query. It will show You data files and their location on filesystem: 

Mostly because performance issues are hidden somewhere else and defragmenting indexes works as quick band-aid (that doesn't last long, tho). It could be issue with statistics not up to date, or other issues. Just like Brent Ozar and Kendra Little wrote in their articles. 

I assume You are asking about database, right? Neither of those two operations is really responsible for improving performance (and shrink doesn't improve anything about database performance/speed). 

Team responsible for SMSS got separated and works with its' own development cycle. That means SMSS gets updated/released more often, and is independent from SQL Server releases. Licencing You've mentioned is probably another reason behind that (and a reasonable one, if You think about it), but what's the most important is that SMSS has got some much needed attention at last. 

I installed the 32-bit ODAC (11.2.0.2.1) from here. The Oracle Universal Installer version contains Oracle SQL*Plus 11.2.0.2.0. But I'm missing the sqlplus/admin/ folder in the installation, where I usually put the glogin.sql file. Edit: This install is on a new Windows 7 - 64 bit Notebook, which is my first 64-bit Windows system. The initial idea is not to install any Oracle server there, but I guess I'll change my mind about this during the first 2 month. I find sqlplus.exe in the folder D:\app\berndk\product\11.2.0\client_1. BTW This install has no sqlplus subfolder and the ORACLE_HOME environment varaiable is not set. As I do not need ODBC that install generally works quite well. 

Inspired by Erics answer, I found the following solution which only depends on the table names and doesn't use any specific column name : 

The annoying thing is, that the button to change the connection ("Verbindung Ã¤ndern" for me) is inactive, and I have to open a new tab, open the connection and copy the contents of the inactive window to it. Is there a better way to handle this situation? 

Trying to remove the duplicate rows delete myTable where pkcol = 1; Yields: ORA-01502: index 'MYTABLE.PK_MT' or partition of such index is in usable state. I'm using Oracle.DataAccess.Client.OracleBulkCopy to fill the table. As far as I understand documentation from Oracle PRIMARY KEY constraints had to be checked. Obviously they are not checked, as I found by doing the same bulkcopy two times in succession which ended in duplicates in all row. Now I'm only using it after deleting all rows and I'm using a table with a similar primary key as source. As result I expect no problems. But embedded deep inside my MS Build scripts, I end up with just 2 duplicates out of 2210 rows. I guess that ignoring the primary key in the first place is a clear bug. No Bulkcopy should be allowed to ignore primary key constraints. Edit: Meanwhile I found, that the 2 conflicting rows where normally inserted by some script before bulkcopy was called. The problem reduces to my known problem, that bulkcopy doesn't check primary keys here. 

To add more to what Raadee and Paul White (also confirm what eckes's comment already stated), TF 8017 is enabled by default in all SQL Server Express Edition versions since 2005. It's probably a way of throttling number of CPUs (sockets and/or cores) unsupported by SQL Server edition. Tested on: 

No, shrink won't break Your log shipping configuration. But You must be aware that both shrink (and rebuild/reorganize You will have to do afterwards) will make Your transaction log files grow a lot. All of those operations cause a lot of I/O load that is logged to transaction logs. This, in practice, might mean that while Your log shipping won't break, it will make restore last a lot longer, depending on Your backup/copy/restore jobs frequency of course. That might lead to secondary falling behind a bit till shrink's (and defrag's) end. Remember that shrink has serious issues, read Paul Randal's post about it, if You haven't already: $URL$ $URL$ $URL$ 

Just like @KookieMonster noticed, You have Auto Shrink turned on. And one of disadvantages of shrink commands is fragmenting Your indexes once again: $URL$ 

Try using Ola Halengren's SQL Server Maintenance Solution. It has a lot of options, most important thing - it skips indexes that doesn't need maintenance (depending how You configure it). There are few thing about index maintenance to remember. From my experience 3-4 days for full rebuilds on bigger databases isn't that much, depending on Your storage. In some cases isn't needed, is often enough for fragmentation between 5% and 30% (as stated in Microsoft recommendations in Books Online, and any kind of index maintenance is recommended only for indexes bigger than 1000 pages, while difference in performance might be noticable on indexes bigger than 50000 pages (as stated here). In most cases index defragmentation isn't source of performance problems either, it just masks other issues hidden beneath. Defragmenting them too often won't improve Your performance and will stretch Your maintenance windows. Another thing is, that maintenance creates noticeable load on IO and CPU, rebuild/reorganize operations are written in transaction log (for databases in mode). So Your log files will get bigger, same with log backups. More read about index maintenance on Brent Ozar's blog posts: $URL$ $URL$ $URL$