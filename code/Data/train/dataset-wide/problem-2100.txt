The 'UPDATED' column from this query can be used to do a quick find and replace on whatever database or table name might be found in the view definition. You will want to make sure you do the same for every possible version of the text you are searching. For instance a database called 'db' might be written as [db] or db. Once you have what you want from the query just copy and paste to a query window and execute the whole thing. Please make sure to backup the database prior to running it. Also carefully proof read the output text so you can verify that the replace worked correctly and did not accidentally change more than you intended. BTW, the answer WEI_DBA gave is perfectly acceptable also. It's just a different approach to the same thing. In his example, the GUI will aide in getting all of the object definitions to the query window. Then you would just use Find/Replace(ctrl+h) to replace the database name. To each his own in this case. I don't know that there's an advantage in either approach except maybe that my approach will allow you to limit the query only to certain objects (views in this case) with additional filters added if needed in the where clause. Generate scripts will allow you to filter to only views, stored procs, etc, and then individually select objects but additional filters will be limited to whatever the UI can do. 

As long as it is run as a single transaction, the value will be re-used on each line, not recalculated. I'm not 100% sure about Postgres or SQL Lite but I'm fairly confident (98%) they will work the same. In the past when I was more confused about this question, I would pre-fetch the date value to a variable and then reuse the variable in the where clause. Now I realize that was pointless unless I have more than one place in a longer set of statements where I would like to use the value. As a test, move the calculated date value to the select clause and execute against a large table that takes more than a few seconds to return. You will notice that the first line will have the same date/time value as the last record. 

First off, the sort operation on a simple query like this will probably always be the most costly. That's because it's an operation that has to run after it has all of the data returned. That being said, there are no clustered indexes on the tables. That means they are heaps and they are storing the data totally unsorted on the disk. Without adding a clustered index to those tables where your sort is applied, it's unlikely that any changes to your query will improve performance. The only other thing that might help(might being a very big might), is move the 'WHERE type = 4' part into the corresponding join statement that it belongs to. So instead of WHERE type = 4 it would be added to the JOIN ON for ArchiveConfig (I think that's where type comes from at least) as AND type = 4. The other ON clause for that join seems redundant. Keep in mind, I doubt very much that this change will actually improve performance but it will make your code a bit cleaner. As the others have said, I don't know if it's a good idea to use the (NOLOCK) hint. I doubt it will improve performance in this case and might result in bad data being returned. 

Once you have the anchor statement, you can UNION below it for all iterations down the chain. In fact, I'm not going to bother rewriting everything Pinal Dave said since it's so right on point in this case. 

This is especially useful where you need to use that variable in multiple places in your script. This example uses setting a variable to a literal value. A literal can also be used in the WHERE clause directly. 

You're question is very broad. But in general the answer is yes. A single instance has to share resources across all databases unless configured specifically not to. By default, SQL uses all available CPU's for all operations. You can configure MAXDOP (max degree of parallelism) on the database so a single query will only have access to that many CPU's. IO is another tricky issue. Generally it's best to seperate your workloads on physical hardware whenever possible. Logs, data and tempdb all on seperate physical storage. In addition, if you are trying to create a seperation of resources for the databases, you can use filegroups to assign the objects or the entire database to a separate storage device. There are many, many other answers but again. The question of 'how can I avoid that?'(in terms of cross database resource allocation) is way too broad for a specific answer. If you are looking at one aspect, say IO, or CPU usage, it's an easier, more specific answer. 

If you want to have a computer be assigned to more than one user, you're going to need a junction table between the two to store the relationships. This was not stated in the requirements but it seems like a logical approach given the project scope. 1. Account 2. Computers 3. Account_Computer_Assignments. The third one is your junction table that stores the relationship between the other two. It's primary key would consist of IDAccounts and IDComputers using your naming convention. A foreign key relating back to their parent tables would also be advisable. This structure will also allow for multiple users to link to the same computer. I don't know if that is the intended behavior but it will allow for that with this design. If that is not intended, and a single computer can only be monitored by one and only one user, then a single field on the Computer table for the AccountID is all that is needed. No junction table. 

Also, you may want to wrap the concatenated values with ISNULL() if any of those values might actually return NULL. A NULL value concatenated with anything else will always return NULL. A trigger will actually be written very similarly except would need to be updated each time a value in the table is updated. Unless there's some performance or data structure related need, I would avoid the trigger approach. Either way for a "correct" answer we would need to know your objectives. How do you intend to use the generated value. BTW, this answer assumes you are using MS SQL. 

I'm attempting to do a server migration with SSRS 2016 to Power BI (March 2018 release). I have been successful at getting my Data Sources, Data Sets, Folders, Reports, settings and subscriptions to come across. Each of the KPI's that we set up have failed to transfer. Other than this it's essentially the last thing that needs to transfer. I'm using the process outlined here; RS.exe migration process Has anyone else run into the issue using RS.exe that none of the KPI's transfer? Is there a workaround or some step that I have to take to get that to work? I've been looking on Microsofts pages for a bug report but haven't been able to locate anything. I realize that I can script copy the records out of the Catalog table but I was hoping for something a bit more out of the box. Also, to clarify, we are using Power BI report server through our Enterprise license with SA. I have verified that I can create a KPI manually on the new server. 

The CTE is the way to go on this one. You should post your attempted code so we can critique. Otherwise all I can do is get you started. Here's an example of what you are doing, almost exactly since this is one of the most common purposes for a cte.Pinal Dave Recursive CTE First step in the hierarchical cte is to create a query to select the top/terminal level of your data AKA the anchor. The example had, IMO, better source data because all you had to do to determine the top level was look for a record with a NULL manager. To me that makes more sense. With your data it will be something like; 

Ok. A literal, not a lateral value... Gotcha. A literal value in really any programming language is basically just a value that is defined within the code and does not change programatically. In SQL I might use that by simply defining a varchar variable with a set value. For example, at the start of a command, 

I'm seeing a few misses. First, the role table. If one user can belong to more than one role, you're going to need another table between role and user. 1 table to store the roll(like you have already) and another to store a junction table which will store the relation between a role and a user. That table would probably only have two columns, role_id and user_id. The user table would not need to identify the role unless it was a primary role or default role. That could also be accomplished in the junction table. Similar situation with Company - Member Type. Since you only have a single column in the company table, you will only be able to store one member type per company. If the requirement is to have companies be able to be assigned to more than one member type, you will need a junction table to store company|member_type relationships. One last thing I noticed, is there is currently no relation between Employee and Company other than the address. That will definitely get confusing. Since you want contacts to be associated with multiple companies, you will need another junction table to store the contact_id | company_id relationship. There might be a clever solution using your employee table. Since that essentially is the only direct connection between employee and company, maybe just change that to the junction table, company_employee. Then add a boolean/bit column (IsEmployee) to indicate whether the contact is actually an employee of the company. 

This is largely opinion based as I'm sure a lot of design questions are. As you stated, it can be made to work in both scenarios. From a growth perspective, I definitely agree with your choice to supply the values in a vertical table with identifiers for each value type. It's a more normalized approach and won't require much dev work if/when more fields are added later. That's really the main benefit. As business needs change(they always do), how much work is it going to be to change or add. A normalized approach is almost always better for changing business requirements. The other option is simple to be sure. Writing reports against data like that would be very simple. That's really the only advantage that comes to mind. Unfortunately it's not a good one. This is most definitely an OLTP application we are talking about. So we should be focused on user input, not reporting. As long as the data makes sense, reporting shouldn't be that hard anyways. One other point. We have an application onsite that was designed by a business user with no actual developer input. It was designed as a single table with no primary key and about 100 columns. Everything is stored on that one table. Normalization WHO CARES!!! Now that the business is tired of the shortcomings of the old system and wants something better, there is almost nothing we can do. The old data can not be directly translated to the new system because the requirements are sooooo different. All of that data has become almost entirely useless. Anyways, the point of my story is, don't take these decisions lightly. They tend to come back and bite you. To add a bit of ammo to your point, try asking the business the following questions, Will we ever want to record who filled in values or when they were updated? Will we ever want to record multiple users answers for each field? If the answer of either of these is yes, even if it's a remote possibility, the normalized approach is better.