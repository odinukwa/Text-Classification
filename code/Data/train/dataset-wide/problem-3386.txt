Hmm, from the founder of the company: "But they gave me something much more valuable, which was the computer time, and they also gave me the computer terminals. So that’s what put me in business. I got a little 1,000 square foot office in New York, and we divvyed up the $9,500 a month, $1,000 went for the rent. I went to a small space specialist. He specialized in under 5,000 feet. He got me this place, and they whitewashed it totally white. I mean lemon’s not going to do a whole lot for you. Today I can still look back on my starting point, that’s true. I was President so I took so much a month, and Peter took, and Marty took, and we had a sector, and we had two programmers part-time who ran the program for American Information Services. We labeled this program FOCUS. I guess it stood for online computer users, but we needed was an acronym that was easy to remember. So we gave it a computer type name, and FOCUS was our term for this non-procedural language. We got started about March 1, 1975. American Can gave us our first check and we set up shop." Link: $URL$ 

Select the icon to the left of menu item "File", then select menu option "Customize View..." Via this alternative method, you may configure screen "Customize View" to display menus Action and View. 

BACKGROUND Each FTP server implementation decides if/how to invoke access controls, per File Transfer Protocol (FTP) technical specification RFC 959: 

Windows Command-line scriptable, so we can automate it...sorry, FileZilla (?) FTPS, as it seems to perform better than SFTP The ability to send KeepAlive commands to the FTPS control port during a transfer No passwords sent on the command line...sorry, curl 

Our IT environment provides 10 shared, Microsoft Windows 7 laptops for an office staff of several hundred people. After checking-out and logging into a laptop with an Active Directory domain account, office staff frequently run Microsoft Outlook 2010. However, the first time office staff do this, Microsoft Outlook 2010 prompts the user to create and configure their local account. This takes just several clicks, as Microsoft Outlook 2010 auto-detects the office staff member's Microsoft Exchange Server 2007 (SP3) account. The problem is: all office staff have to do this on each new laptop they use. Until they do so, some functionality does not work (for example, Microsoft Word 2010 Save & Send fails with error "There was a problem creating the message"). How might our IT department "pre-configure" the shared laptops so office staff can simply log-in and use Microsoft Outlook 2010 functionality without the need to configure a local account? 

These set up the actual service description and the "host_name" assigns it to a server. Look at the documentation here when creating your service. The stuff in red is necessary, the others are used to refine how your service check behaves. I hope this helped in some way. If not, please reply and let me know. You should also know that the default layout for Nagios configs is simple but unproductive. I wrote some documentation on how I lay mine out, and it's saved me a lot of time trying to find exactly the definition I was looking for. Depending on your installation, it may be overkill. $URL$ Good luck! 

This is directly dependent on your business. I have users who routinely get files in the 40MB range, and sometimes well above that. I essentially set an unlimited size for that reason. Take a look at your legitimate attachments, take the average size and double it, then look at the largest legitimate attachment you've received. If it's bigger than double the average, then make it 50% bigger than the largest so far. 

Just so we're clear, Nagios itself does state monitoring. If you're using plugins that support providing extended information, then you can pull that data from the results of the plugins. There's a piece of software for Cacti that uses Nagios plugins to create data that I'm considering using. It's NPC, or Nagios Plugins for Cacti ( $URL$ ). I have not used it, so can't speak to the performance or anything, but it may be what you're looking for. If you want to read the raw data yourself, check your configuration for the status log file (typically /var/nagios/status.log or /usr/local/nagios/var/status.log). 

Your best bet is probably to use a load balancer (or two, in a cluster configuration) to act as the "concentrator". It's not too different from the solution you mention, but has the benefit of permitting high availability of the resources behind the exterior access. 

Also, the only "sync"-like thing that does what you're looking for are things like revision control software. I know Subversion can do things like that, and I suspect git and the other newer ones can as well. 

I got it! Unbeknownst to me, ScreenOS has the ability to pipe the output from any command to a tftp server! The usage is: 

Do you care at which layer in the stack the encryption happens? Short of massively modifying the source code to decrypt all DB queries (and that's ignoring the key management aspect), the best bet might be to encrypt the partition that the database lives on. What is the reasoning behind it? There may be a better way. 

Sampling a random CD-ROM drive in the old junk box shows that it's rated at 29W. Even if it ran all the time, it wouldn't amount to much. I'm guessing that the power draw while idle would be negligible. EDIT: Worst case scenario example for my $0.08/kWh, my CD drive running at its fully-rated power draw of 29W would amount to $20.32/year or about $1.69/month. 

You might try bouncing off a proxy on a machine with a static IP, such as squid. Better yet, if you have SSH access to any machine with a static IP, you can use the SOCKS option for SSH: 

Of course, there's a little more to it, but you get the idea. Since you already have Apache and nginx installed, you may want to browse this link, which describes a very simliar situation to yours, but uses nginx as the front end for static content and then passes on requests to Apache. If you want to go really simple, you could simply use a reverse caching proxy (such as Varnish or nginx) in front of Apache. What it will do is cache requests to quickly serve them out to clients, while at the same time relieving the web server itself from serving identical requests. This, by its very nature, will give the same effect as what you're asking for. Since static pages and images rarely change, they will be almost always cached by the front end, whereas dynamic pages will be detected as such and always passed on to the back end. 

This may sound like sort of a bad hack, but on several past occasions when copying huge directory structures, I used WinRAR to archive (with compression) to a file and then extract it to the destination drive. Not the most graceful solution, I know, but it worked in a pinch. These days, I'd probably try to use rsync or tar from the cygwin utilities. 

I've got a handful of Plesk machines of various versions and operating systems, each hosting mail as well as web sites. Anyone who has dealt with Plesk knows that it can be pain to keep locked down. Between qmail's tendency to support backscatter spam and insecure contact forms published by users in PHP and ASP, there is a lot of potential for spamming and getting listed in RBLs. In addition to locking down these machines as tightly as possible, I want to funnel all outbound mail to one or more outbound-only mail relays that have the ability to scan the mail for spam before sending it on its way. I'd prefer Postfix, but I'm open to just about any open source solution. There are many, many tutorials for filtering inbound spam, but very few that even address the outbound spam problem, and most of those have little useful info. Even a method to have Postfix sequester all outbound mail into a special queue that I could scan with home-rolled scripts and then re-inject into the outbound queue would be an option. Any and all ideas and suggestions welcome. 

Since password policy represents a form of access control, and RFC 959 specifies no password reset mechanism, it seems reasonable to conclude each FTP server implementation decides if/how to support FTP client-initiated password resets. In practice, this represents what we see: 

Number 4, above, is critical: we have set KeepAlive in some other clients (e.g., CoreFTP LE) but we seem to have some routing equipment in the server environment which drops our connection when transferring a 7GB+ file. We have also set passive mode and "resume transfer" functionality seems currently broken with this secure file transport server...so we need to download the file in one go. What FTPS clients might meet our needs? 

IBM Z/OS FTP SERVER PASSWORD RESET MECHANISM IBM z/OS FTP server extends File Transfer Protocol (FTP) command "PASS", to grant FTP users the ability to initiate the password change, via the FTP client. Specifically, enter "oldpass/newpass/newpass", in FTP client field "password" (substitute the actual old and new passwords, for "oldpass" and "newpass", respectively, preserving the forward-slash delimiters). Upon successful login, this triggers the FTP server to subsequently change the FTP user password, on the remote server. Subsequent logins require the FTP user to provide only the new password. Additional constraints exist; please refer to the IBM FTP command "PASS" documentation for full details. AUTOMATING IBM Z/OS PASSWORD RESETS Automation depends on both the FTP server password reset mechanism and an FTP client which lends itself to automation. We currently use Ipswitch WS_FTP Professional Client v12: 

We wish to solve the business problem of how to schedule automated password updates, on a remote FTP server (note: z/OS) over which we have no administrative control, before the password expires. For example, once each month, update user JDoe's password, both locally and on remote FTP server ftp.abc.com. Our business process necessitates transferring files, using protocol FTPS, to/from a remote FTP server (note: z/OS). A separate organization administers this remote FTP server, providing our team with a user account but no server-level administrative control. FTP server policy automatically expires user account passwords, after a period of time. When this happens, FTP server staff require the person tied to the named account to call their FTP server help desk, to verify their identity. Upon successful verification, FTP server help desk staff reset the password, requiring the user to choose a new password upon the next login. The FTP server allows users to reset their password via the command-line, by setting password to string "oldpw/newpw/newpw"; subsequently, users login with only "newpw". FTP server administrators will not set an FTP user password to never expire. Years ago, a now-retired team member created an in-house app to perform this task, using FTPS functionality provided by ComponentPro "FTP/SSL Client Component for .NET". Per resource constraints, we'd prefer a solution we don't have to maintain in-house. Note: if possible; if in-house represents the way to go, so be it. How can we do this better? 

Question 1) Can I host it in one of the cloud services? Yeah, absolutely. It doesn't really matter what platform you're talking about, either. Win/Lin/etc are all able to be hosted. The Cloud Computing group in Google Groups has a good list of cloud platforms, if you're looking for options. Question 2) Will the cloud service be reliable over a long time? That's a harder question to answer. The end result is that we won't know until it fails, which leads to the logical conclusion that you should select a provider with a strong background of service and dedication to clients. One of my biggest concerns is making sure there's an exit strategy from whichever cloud you select. If you're rolling in the bucks (which nearly no one is), consider using two cloud providers much in the same way that you might have, at one point, considered using two data centers. If one fails, you've got a standby solution in short order. 

I'm attempting to set up Cacti to monitor a router's interfaces, and I'm having trouble getting the graph templates to show the information that I'd like. Our interface configuration looks like this: 

Hopefully some diagnostic message will appear and let you know why it wasn't accepted for delivery. If it WAS accepted for delivery, pretend to be an IMAP server Telnet to port 443 on the machine, and do this: 

Assuming you're not using an unlocked iPhone, neither of those devices you listed have AT&T as a supported carrier. I'm sure the iphone could roam onto those other networks, but what that would do to your bill, I have no idea. 

I suppose it depends on your environment. I occasionally get an optical disk that can't be read, but rare is the time that I've actually lost data because of it. If there were a solution that was nearly universal, and unobtrusive, and didn't double the amount of space needed, then I think it would be worth it, but it hasn't affected me much, so I don't see the need, even though others might. When I'm transferring things, I typically do so over the network, and when I don't md5sum tells me whether I got a good copy or not, even before I disconnect the storage. 

Do you know what changes you've made to it? If so, the FAI ($URL$ looks to be full featured enough to "clone" a config, as long as you know what the config is. And honestly, you should make it a point to document a config to the point that you can use a tool like FAI anyway.