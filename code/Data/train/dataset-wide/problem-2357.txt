I would study this white paper and the AsPartitionProcessing sample for ideas. Ignore that it targets Azure Analysis Services as it should work against your SSAS instance if you use that servername instead of asazure:// as the servername. It uses the Tabular Object Model (TOM). 

You need to process D1 and D5 in the same transaction. Then both will complete processing before measure groups are processed. Make sure you mark Transaction=true (the default unless you explicitly set it to false) and Parallel. 

The only time you need to use Schema Generation is if you did top down development and have never generated the schema before. Now that you have previously generated the SQL schema, you can just script out all the objects in SSMS and provide that script along with the SSAS project to other developers. Then the new developer would deploy the SQL scripts and edit the data source in the SSAS project to point to their database. This is the most straightforward approach I believe. 

In SQL Server Management Studio you can connect Object Explorer to SSAS, expand to the table, right click, choose Partitions, then click to process one or more partitions and do a Process Clear: 

In the next step, all the functional dependencies (projected over the subschemas) should be tested to see if they violate the BCNF, first for , then for . But obvioulsy in relation , (which has key , for the way in which it has been build), the dependency does not violate the BCNF (that says that each dependency should have a superkey as determinant), so it is not used anymore to decompose the relation. So, from this example you can see that the dependency used for the decomposition is not used in the later steps of the algorithm: its determinant is the key of the first of the two new relations produced, and for this reason that particular dependency will not used anymore, since it does violate the BCNF of the schema produced (while other dependencies in the schema could still be used to decompose, like in the example linked). 

A foreign key in a table that refers to a table is a column such that all its values must be present also in the primary key column of . So, the question is meaningless if you intend that a foreign key is two different column (this of course is impossible), while the answer to this question is always “yes” if you intend that the values of the foreign keys are also values in a primary key in another table. 

The Customer dimension is joined to the measure group at a non-key granularity. If the Customer Name attribute isn't related to the attribute you join on then data repeats in reports. You need to do one of two things: 

And for SQL 2016 Tabular models using the latest 1200 compatibility level, use the following script. Just change the name to the desired database name and the DOMAIN\GroupName to your administrators group. 

Recommended. Change the fact table to have the surrogate key of the dimension. Then change the Dimension Usage tab to join to the key of the dimension. Change the dimension to change which column is used as the key of the dimension. Make sure the fact table has this column. Fix the Dimension Usage tab to join to the dimension key. 

My recommendation is not to attempt to do the sequence logic in MDX. You will be disappointed in performance. I would recommend the following approach: First create a view (or a physical table if you prefer) which returns one row per part/customer/person per day showing the currently effective value. The view would look something like this: 

Let’s try it for the dependency BC → A. Initially: BC+G = BC, using R1(AB), BC ∩ AB = B, B+F = BD, so intersecting it with AB gives B, which is already in BC+G; using R2(BC), BC ∩ BC = BC, BC+F = ABCDEG, so intersecting it with BC gives BC which is already in BC+G; using R3(ABDE), BC ∩ ABDE = B, B+F = BD, so intersecting it with ABDE we obtain BD and we can add D to BC+G, which now becomes BCD; BC+G is changed, so we must restart checking for the new value BCD. The first two relations do not give any contribute, since D is not present in them. The third one must be considered again: using R3(ABDE), BCD ∩ ABDE = BD, BD+F = BD, we can not add anything; using R4(EG), BCD ∩ EG = ∅, nothing to add again. So we have terminated, beacause no new attribute can be added to the closure, and since BC+G = BCD does not contain A, we can conclude that at least this dependency is not preserved, and so that the decomposition does not preserve the dependencies. And since you asked how to decompose a relation without losing dependencies, you can look at the synthesis algorithm to decompose a relation in Third Normal Form (3NF). You can find it on any good book on databases, and it guarantees to preserve both the dependencies as well as the data (lossless decomposition), while reducing redundancies and anomalies. In this case a decomposition in 3NF is the following: R1 < (ADE) , { AD → E } > R2 < (BD) , { B → D } > R3 < (ABC) , { BC → A, AC → B, AB → C } > R4 < (EG) , { E → G } > 

If your drillthrough command includes a dimension attribute from a many-to-many dimension then it behaves as described: showing a row zero to many times. The easiest solution is to create a drillthrough action and mark that action as the default drillthrough action and specify the columns you want to include in the drillthrough results. If you don't include the many-to-many dimensions then it will stop skipping or duplicating rows. 

But this is still not what you want because a particular query won't union results from DirectQuery partitions and sample In-Memory partitions. I will try to update this answer if a future release of SSAS adds the feature you are wanting. 

For some reason, SSMS doesn't provide this New Database menu option for SSAS Tabular. But you can easily create a new empty or shell database with the following XMLA script. Just click the XMLA button in the toolbar and then paste in the following. Edit the ID and Name property to the desired database name. And edit the DOMAIN\GroupName to set the group that has admin permission. This will allow members of that group to deploy over this database without being SSAS admins for the whole instance. The following script is for SSAS 2012 SP1 or 2014 Tabular models: 

Initialize X+G with X, at each step, for each relation Ri, add to X+G the attributes obtained by calculating the closure of the attributes of X+G ∩ Ri with respect to F, and projecting the result again on Ri. In symbol: X+G := X+G ∪ ((X+G ∩ Ri)+F ∩ Ri) repeat 2 while X+G changes. 

The relation with the above functional dependencies has two different candidate keys: B and G. You can verify this if you compute the closure of both to see if it contains all the attributes: 

is: both of them are correct, since both of them satisfies the definition of the 3NF. You have simply discovered that the synthesis algorithm for decomposing a relation in 3NF can produce different solutions. A different question is: which is “better”, and of course the solution with a single relation is “better”, since you do not need to join tables when making queries. Of course, if one could check at the beginning if the relation is already in 3NF, than he can avoid to apply the algorithm. But this in general cannot be done, since the check requires the exponential computation of all the keys, to find the prime attributes of the relation. 

In SSAS 2014 Tabular partitions inside a table process serially. In SSAS 2016 Tabular partitions inside a table can process in parallel. So in your version partitioning won't speed up processing if you process the whole table. However partitioning is great for use in incremental processing. If you partition by year and only 2017 rows changed then you can just process the 2017 partition. Also to archive off data older than 10 years old you can drop the oldest partition. I don't know where the 2 partition rule you mentioned came from but don't believe it makes sense. If you have a reference for that assertion with some context then feel free to share. 

You are correct that currently (up through SSAS 2017 Tabular) is is not possible to have mixed mode partitions. The whole model can operate in DirectQuery or In-Memory. The DirectQueryMode connection strong property can specify whether SSAS should use DirectQuery or In-Memory for answering the current query. But that's not honestly too much better than just deploying the model to two different databases, one DirectQuery and one In-Memory. There is a similar connection string DataView=Sample which has SSAS answer the query using a special cached partition marked as the sample partition. 

In your relation schema, there are three candidate keys: , and . Since, for instance, violates the BCNF, we can decompose the original relation in: 

It is efficient in terms of memory and does not require the use of a complex type like a PostgreSQL array (actually it is a bit array), and more, you do not have to pay attention to the difference between false and null (and also you could set the entire field to a null value, if you need to). 

Your decomposition is not correct, since in R2 you still have dependencies that violates the BCNF, for instance ( is not a key of that relation). The problem is that your algorithm is not correct. When you find a dependency that violates the BCNF, you should decompose a relation in two relations, the first with X+, not XA, and the second one with T – X+ + X. Then you should repeat the algorithm, if you find in one of the two decomposed relation some other dependency that violates the BCNF. So, in your example, a correct decomposition is: 

If you close Visual Studio then open it from a command prompt with the following command it will prompt you for your password needed to connect to the other server. 

I would highly recommend you digest this article. Also are you processing all cubes as ProcessFull or only some cubes or some measure groups or some partitions. If you are processing all cubes and measure groups and partitions then don't bother with all this. Just do a ProcessFull on the database object. ProcessUpdate on dimensions is only for incremental processing which assumes you aren't processing all partitions. 

What happened is that developers added new data sources instead of adding tables from an existing connection. In the future go to the Model menu... Existing Connections... Open. Then choose another table or query to import. This process is documented here. I don't believe there is a UI way of switching an existing table to use a different data source. The best approach is probably to just delete and recreate that table with the process above. You should be able to delete the extra unused data sources from the Existing Connections dialog. 

Let's find the 3NF with the synthesis algorithm. The first step is to collect together all the dependencies of the canonical cover with the same left part, that is: 

that produce three relations, R1(A, B), R2(A, B, C), R3(B, C), and, following the algorithm, you obtain as result only R2, since the other two have attributes contained in them. So you have two different outputs from the algorithm, depending on the minimal base used (which in turn depends on the order in which you consider the dependencies when calculating the minimal cover). So, the answer to your question: 

Normal forms are used to eliminate or at least reduce redundancy in data, as well as to eliminate the so called insertion/deletion anomalies. The BCNF eliminates redundancies and anomalies, but decomposing a relation in BCNF sometimes has the unpleasant effect of causing the loss of one or more functional dependencies during the process. For this reason the 3NF is used instead of the BCNF in practice, since a decomposition of a relation in this form always mantains data and dependencies, and reduces anomalies and redundancy (even if to a lesser extent that the BCNF). Another reason is that the decomposition in 3NF can be obtained with a polynomial-time algorithm, while the decomposition in BCNF requires an exponential algorithm. 

/netonly tells it to just use those alternate credentials when talking over the network. Change the OTHERDOMAIN\OtherUser to match the username needed to connect to the remote system. And then the last part is the path to Visual Studio 2015. Once that's done you should be able to deploy without supplying alternate credentials again. By the way, I would highly recommend you install the free BIDS Helper in order to leverage the Deploy SSIS Packages feature which makes it easier to deploy single packages. You still need runas.exe though. 

Next, load that new view as a measure group in the cube. Instead of having a Sum measure, use a measure on the Value column with AggregateFunction=FirstChild. This will cause the February 2018 total to reflect the rows that were effective on February 1, 2018. FirstChild is a semi-additive measure which causes it to return the first member in the selected date range. I would recommend you read this blog post to make sure you mark the Date dimension appropriately so it will work. Counterintuitively, even though you are exploding the amount of data, this approach will perform better in a cube because all the hard work is being done during cube processing time and during query time it just displays the right date of data. Please do the math on how many rows you will end up with if you follow my recommendation. Hopefully it is a reasonable number of rows (say, 100,000,000 or less, or a billion or more if you have good hardware). If it's an unreasonable amount of rows (like 100,000,000,000) then there are other options like a many-to-many date range dimension that are much more complex to implement. I wouldn't recommend that approach unless my recommendation isn't appropriate.