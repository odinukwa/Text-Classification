Partition by Thing and find the minimum (alphabetically first) event in each partition. If it is rather than this Thing has no end event. 

I'm not sure what you mean by "ignoring nulls" here. It returns the number of rows irrespective of any s 

The basic problem here is one of statistics. For both queries the estimated row count shows that it believes the final will return 1,048,580 rows (the same number of rows estimated to exist in ) rather than the 0 that actually ensue. Both your conditions do match and would preserve all rows. They end up getting eliminated because the single row in does not match the predicate. If you run 

The slightly greater overhead of the first run of the stored procedure can in no way account for the big overall difference however as it still only takes a few ms to clear the procedure cache and run both procedures once so I don't believe either statistics or recompiles can be the cause. Create Required Database Objects 

No. If you try the following from two different connections then the second one will be blocked by the first (visible in ) but neither will result in any transaction log activity and running will report "No active open transactions" (assuming no other activity). 

You cannot make schema changes when the database is read only but you could put all your user tables on a new file group and mark that as read only. You can expect a modest performance benefit from absence of locking. On versions of SQL Server prior to 2012 statistics can't be auto created or updated on read only databases. Before making it read only you might as well remove all logical fragmentation and make page density as high as possible. Any non default settings will not be of benefit in a read only environment. Additionally create/update any statistics anticipated to be of use for queries if on version < 2012. 

and returns on input If you can live with instead of then you could just use . If you must have then you could use the following pattern 

You will see that in the screenshot below SSMS manages to display all errors fine. Are you missing some context in your question? 

The logical built in place to put this kind of metadata about columns might be in the description box when the column is selected in the table designer. 

You can use instead of . With a severity of 0 the two are equivalent but there is some primitive string formatting functionality available as below. 

When you run the package from msdb it executes on the machine that you are running it from. (Unlike packages stored in the 2012+ SSISDB catalog where executing them spawns an execution process on the Server itself). You must have installed SQL Server Integration Services onto your own machine from the SQL Server installation media, whereas your colleague hasn't. If Integration Services isn't installed you are restricted to running packages in SSDT. Executing packages that run locally with dtexecui will fail. 

The formula no longer works after is added to the in list (Estimated rows rather than the that adding another to the total would generate). seems to use yet another method of calculating cardinality estimates. estimates only 1829.6 rows. I've no idea how that is derived from the histogram shown. 

The literal then gets parameterised and it is no longer guaranteed that the filtered index will match 

They satisfy different queries. The first one will be useful for queries that seek on and return (without the this would require a lookup back to the base table) The second one will be useful for queries that seek on Both of them can be used for queries like but the second one will do this more efficiently as the first index would require a seek on then the residual predicate be evaluated against all matching index rows. 

It isn't absolutely guaranteed to be more efficient however. There are occasionally edge cases where adding a causes a worse overall plan than that for the query which simply brings back the whole result set. 

You can do this with find and replace but be warned that if you have string literals with embedded line breaks these will be altered too. The following finds all instances of consecutive line breaks and replaces them with a single one. 

So instead of it reads then discards all the rows. Another disadvantage of relying on it is that the cardinality estimates may not be as accurate as with the traditional range query. This can be seen in an amended version of your SQL Fiddle. All 100 rows in the table now match the predicate (with datetimes 1 minute apart all on the same day). The second (range) query correctly estimates that 100 will match and uses a clustered index scan. The query incorrectly estimates that only one row will match and produces a plan with key lookups. The statistics aren't ignored completely. If all rows in the table have the same and it matches the predicate (e.g. or ) then the plan shows a clustered index scan with an estimated 31.6228 rows. 

Or alternatively if this is only a development database, you have backups and you aren't going to come complaining to me if it all goes wrong you could do some experimenting with the undocumented and highly warned against command. 

This is running on Microsoft SQL Server 2012 (SP1) - 11.0.3513.0 (X64) so writable columnstore indexes are not available. The table contains data for two distinct Market keys. The build switches out the partition for a specific MarketKey to a staging table, disables the columnstore index, performs necessary writes, rebuilds the columnstore, then switches it back in. The execution plan for the update statistics shows that it pulls out all rows from the table, sorts them, gets the estimated number of rows badly wrong and spills to with spill level 2. 

Tables in SQL Server can be either organised with a clustered index or have no CI in which case they are a heap. You need to look at . Despite the name this also does analysis of heaps too (though logical fragmentation does not apply to these, pages cannot be out of logical order as there is no "correct" ordering in a heap). For tables with a CI the clustered index has an of . The leaf level of the CI is the table. For a heap this is given an of .