I found this website which might meet your needs but I honestly can't figure out exactly how you'd use it. If that doesn't work, it's easy enough to write your own program which reads a corpus, counts the bigrams, and outputs the data you're looking for. Even if you don't have any programming background, this is actually quite easy. I have given this task as an assignment to classes where the majority of students had little-to-no programming experience and that didn't have much trouble. Try a quick Google search for tutorials. The problem is choosing a corpus. Generally corpora tend to be specialized by the type of language included (such as only news articles, only tweets, only novels, etc.). You will need to take a look at what's available and decide which corpus (or combination of corpora) are right for you. This website offers multiple corpora for free download as well as TREC offers multiple corpora across various types of language (some free, some requiring approval, and some requiring purchase). 

It's related to the concept of time. You know how the present refers to the exact moment we live in? Everything after it (i.e. everything that hasn't happened yet) is called the future and everything before it (i.e. everything that has already happened) is called the past. Also, you need to realize that the future will eventually become the past. For example, image it's currently Wednesday. Today, Wednesday, is the present. Yesterday, Tuesday, is the past and tomorrow, Thursday, is the future. Now imagine it's Thursday. Is Thursday still the future? No, it's now the present. And when it becomes Friday then Thursday will be the past. So on Wednesday Thursday was "the future" but on Friday, Thursday is "the past". Speech time (which I'd probably write as "time of speech") refers to the exact moment a person speaks/writes anything. Tense tells use when an event is happening in relation to when the person is speaking. So on Wednesday, I would use future tense to talk about events on Thursday (e.g. "I will go to the store on Thursday"). On Friday I need to use past tense to talk about Thursday (e.g. "I went to the store on Thursday"). Here is where is gets tricky and here is where I think your problem is: If it is Wednesday and I am talking about Thursday I must use the future tense but what if I wrote my message, "I will go to the store on Thursday", in an email and sent it to you but you didn't read it until Friday? To you Thursday is in the past but I still used future tense. The reason is that at the time I wrote the message (the speech time) Thursday (when the event was to take place) was still in the future. However, time moved forward and by the time you read it, Thursday was in the past. However, this does not affect the grammatical of my message at all. In the context of the Harry Potter quote, the story takes place in the past. That is why the narrator (the person telling the story to you, the reader) uses past tense. But the events the characters talk about are happening at the same time as they are speaking about them (i.e. the event time is the same as the speech time) so even though to you, the reader, the events are all in the past, the characters must use present or future tense because to them the events either are happening at that moment or have yet to happen. In summary, tense is decided by the temporal relationship between when a speech is made (the speech time) and when the event it talks about takes place (the event time). The three simplest of these are past, present, and future. Obviously tenses are more complicated than just past, present, future. Once you feel you understand the basic principle, check out this Wikipedia article for more information on Tenses in English. 

See how the [ ] brackets trace out the rough structure of the tree? For increased readability the brackets normally also have a subscript explaining what type of node they represent. You can also see that ">" is not the mathematical "greater than", but rather something akin to an arrow or "=". The use of ">" in this manner is not actually standardized and will vary from author to author. This was a very simple example. Unfortunately, the modern understanding of syntax is not this simple and there is a lot of detail I left out. Modern syntax attempts to exclusively use binary branching to simplify the structure of each node. This is hard to do with some sentences, such as sentences with double object (aka "ditransative") verbs like "give" in "John gave Mary the book". Such a sentence is impossible to explain using the structure I used above. How can we separate "Mary" from "the book" but keep them both inside the VP while keeping a binary structure? (Incidentally, Larson writes a very good paper on this subject although some of the vocabulary he uses is out-of-date). Basically, to account for these structure most non-leaf nodes in modern syntax trees are in the following form: 

There are a multitude of online pinyin to Hanzi converters as well as several pinyin-based IMEs that do what you want but not in a way that you can include them in your own program. Over on the Chinese Language Stack Exchange they suggest using the Google Translate API. This would allow you to integrate it with you own program but won't give you a list of all possible Hanzi characters, like you request. I can't seem to find an out-of-the-box solution that does everything you need but depending on your programming ability, you may be able to build your own rudimentary pinyin-to-Hanzi converter using available resources. Looking into several Hanzi-to-pinyin converters, I've noticed that most simply perform a dictionary look-up and that they store the actual Hanzi-to-pinyin conversions as plain text files. You could easily read those files yourself, strip out the tonal information, and build a reverse mapping where each valid pinyin syllable maps to a list of Hanzi characters. 

These two examples have different pronunciations. I am not a phoneticist so I cannot comment on exactly how they are different, but the effect is noticeable. It may have something to do with how humans comprehend word boundaries or it may be due to Korean's semi-tonal nature. Again, due to my lack of expertise in phonetics I do not feel comfortable attempting to answer your second question. 

The simple answer is that there are two "a/an"s in English. One is an article and one is a preposition. When you consider that languages are full of homonyms it is natural that there can be multiple items in a lexicon that share the same form (pronunciation/spelling) while having different meanings. Consider the word "sanction" which can mean the exact opposite of itself depending on whether it is a noun or a verb yet we have no problem understanding it. This is a good comparison because like "a/an" the type of word it is interpreted as depends on how it is used in the sentence but "a/an" has the added advantage that, unlike "sanction", it does not carry a semantic value (or at least none beyond the logical implications of using an indefinite article). When trying to decide if a particular instance of "a/an" is an article or a preposition you can use a substitution test like the ones put forward by hippietrail: substitute "a/an" in a sentence with other articles or prepositions and see if the sentence is still syntactically valid (it might be semantically weird but that's OK) 

A list of extractive summary corpora can be found over on ResearchGate. Additionally, a quick search revealed multiple papers on supervised extractive summarization (for example, this one), using the DUC or TAC datasets. Although I've not personally worked with these datasets, if they're used in supervised learning approaches they should contain the information you need. 

A lot of the ambiguity is mitigated by Korean phonological rules. For example, "aki" could be either 아기 (a.ki) or 악이(ak.i) but since Korean phonology avoids codas wherever possible they would both be pronounced as "a.ki". Similarly, "akhi" could be 아키 (a.khi), 앜이 (akh.i), or 악히 (ak.hi) but they would all be pronounced as "a.khi". 아키 and 앜이 for the same reason as above and 악히 because Korean phonology tends to prefer aspirations over plosive-glottal chains. Additionally, complex consonents can only appear in the coda position. Thus, "ilka" could be interpretted as 읽아 (ilk.a) or 일가 (il.ka) but not "i.lka" since complex onsets are banned. If a complex coda is followed by a null onset, as with 읽아 (ilk.a), the complex consonent splits and the last part of the coda becomes an onset as above, resulting in a pronunciation of "il.ka". So basically what I'm trying to say is that although the lack of syllable markings is ambiguous in terms of spelling, but it is no more ambiguous than Korean pronunciation. In fact, spoken Korean is even more ambiguous than Yale Romanized text as Korean only allows k, p, t, n, m, and ng sounds in SR (surface representation, i.e. after the coda-to-onset shifts mentioned above) coda positions. Aspiration and glottalization features are lost, s and c become t, and h is silent (remembering that "h" can only be a coda in SR if the following onset is not null and is not a plosive). Thus 밭 (path), 밧 (pas), 밪 (pac), and 받 (pat) are all pronounced identically, despite different romanized forms. So really, Yale Romanization is more ambigious than Korean spelling but less ambiguous than Korean pronunciation. Where you do get true ambiguity is with glottalized consonents like ㄸ (tt), ㄲ (kk), ㅃ (pp), ㅆ (ss), and ㅉ (cc). Without marking syllables 악기 and 아끼 would share the same romanization ("akki") despite having different pronunciations ("ak.ki" and "a.kki", respectively). I should note for people who aren't familiar with Hangul (Korean writing system) that although these glottalized consonents may look like complex consonents (ㄲ looks like two ㄱs, ㅃ looks like two ㅂs, etc.), they are not and, unlike complex consonents, they never split. Diphthong vowels show a similar ambiguity. 에어 (ey.e) and 어여 (e.ye) would both be written as "eye" despite, as with glottalized consonents, having different pronunciations. From my experience, in literature people tend not to make syllables at all. This is because in practice these ambiguities don't come up too often and the inclusion of an English gloss solves any ambiguities that do occur. You need to remember the context of Yale Romanization; it is used in academic papers because it is the only romanization system with a 1:1 character conversion. Typically these papers must be written for an audience that does not speak Korean at all. In fact, Yale Romanization would be a very bad system to use to teach Hangul if only because Korean consonent/vowel pronunciations are not 100% compatible with English phonology. In the end, it comes down to your style, the style guide of the journal or conference you're writing for, and what exactly you're trying to convey. If you're doing a paper on syllabification or Korean morphological rules then marking syllables would be important. In a syntax or semantics paper, not so much. On a personaly note, I too find Yale Romanization frustrating. I once read 6 pages of a paper about NPI licensing in Korean BEFORE clauses before I realized "ceney" was "전에" (cen.ey), which probably says more about my low Korean proficency than Yale Romanization. Also, how is ㅈ, pronounced as a voiceless "j" or unaspirated "ch", romanized as "c"? Yes, I know it's so ㅊ can be romanized as "ch" (which it does actually sound like) much like the aspirated "k", ㅋ, is romanized as "kh". I don't care! 

The big name in computer-based dictionaries is WordNet which groups English lexical items by concept (called synsets). I can be downloaded and used offline. Obviously, this tool is quite powerful and is used extensively a lot in various Computational Linguistic and Natural Language Processing applications. There are even projects to create WordNet equivalents for other languages. Not surprisingly, some people have tried to map synsets across different languages. While I'm not an expert, a quick Google search revealed multiple projects for linking WordNet to various other ontological hierarchies. Perhaps these would fit your needs. 

While none of these sentences are ungrammatical, (4) seems marginally unacceptable to me. Given a choice between (5) and (6), I find (6) more natural. Along with this perceived confidence, (1), (2), and (3) also differ in perceived forcefulness. I can think of numerous times where I have used "think" to soften the impact of my opinion when talking to a superior. In summary, there seem to be multiple stylistic and societal factors which influence the choice of the progressive "think" construct over stative "think" construct (over no "think" at all). Combined with the historical precedent, I don't believe progressive "think" is a new phenomenon nor that it is overtaking stative "think" in any meaningful way. One final note, as I stated above, my answer relies heavily on my own personal intuition. I'd be very interested to hear what other native speakers think about my interpretation. It's entirely possible that I am part of the tend of this "dying stative 'think'" but I see no evidence suggesting that such a trend even exists. 

Although they do sound a bit odd and stilted, I personally have no trouble accepting and . I think your problem is that you aren't making a distinction between unacceptable and weird or awkward. Humans judge utterances based on several criteria; of course syntax and semantics but also stylistic factors and even ease of comprehension. In my [limited] experience, these are considered as extra-linguistic factors and not including in an utterances acceptability unless explicitly noted. Note how and use embedded clauses ( and , respectively) while doesn't. Also note how in the versions the embedded clauses come at the end of the sentence while in they come in the middle. I unfortunately cannot find an academic source addressing this exact issue, embedding clauses in the middle of a sentence causes difficulty because the reader needs to figure out where the embedded clause ends. When the embedded clause comes at the end of the sentence it is trivial to see where it ends. I suspect that you, like me, don't consider and as wrong so much as harder to parse compared to and . I was able to find multiple sources for how embedding can affect understandability but they describe different types of embedding from the one in your question. 

Currently taking a Sociolinguistics course so I am, obviously, an expert on this subject (please note the sarcasm). @Gaston Ümlaut did a very good job describing why there's less variation in written form but I wanted to bring up a few more points. First, the reason there's less [observable] variation in written forms compared to spoken forms is simply that there is greater societal pressure to conform to the "standard dialect" in written form, as opposed to spoken. Think of writing as akin to making a presentation for a crowd of people. In formal situations like that, most people will make a concious effort to better conform to the standard dialect to be more easily understood. It also helps that since written form has an editting process, people can take more time to think and craft their words and go back and fix "mistakes" (i.e. deviations from standard dialect). Second, you need to think about the context. Language Style as Audience Design by Allan Bell (from 1984) shows that people adjust their speech patterns based on the audience. I see no reason why this won't apply to writting as well. In emails, IM, and facebooking between friends slang, varient spellings, and sentence structures which reflect the speaker's (writer's) native accent are extremely common. Then you have the issue of emoticons, slang, and the fact that spelling errors and typos are more easily accepted than, for example, in a newspaper. I do have anecdotal evidence to support this view cross-linguistically. I currently live in Seoul, South Korea and I have observed that my friend's IMs and text messages frequently use slang and short forms they would not use in more formal situations. It should be noted though that Korean spell adheres quite closesly to pronunciation (not 100%, but maybe 90% identical. Mostly because they occassionally revise their dictionaries to conform to modern pronunciation) so these spellings are rather consistent across writers (but still vary from standard form). 

I'm currently completing a Masters of Linguistics, specializing in Computational Linguistics, after a Software Engineering undergrad so I think my experience might be relevant to you. I think you may be a bit disappointed by the actual math involved in linguistics. Setting aside Computational Linguistics, the only real math I see is statistical analysis used in academic papers (and not something you'll generally need for your course work). Of course, semantics requires some basic set theory and algebraic logic but as a computer programmer by training and [formerly] by trade, it was all fairly basic. My [limited] understanding is that the cutting-edge work in syntax, semantics, and phonology come from examination and theorizing of frameworks involved in language usage. Computational Linguistics is where you're going to see more advanced math; being a cross-disciplinary field with Computer Science. Machine learning and AI use various mathematical models and statistics. Information Retrieval heavily uses linear algebra and statistics (with some algorithms even using graph theory). Speech recognition and speech production all involve a certain amount of signal processing which you may find interesting. The problem with Computational Linguistics though is that it is typically part of the Computer Science department and this might limit the type of projects you'll get to do and the types of classes you'll get to take. However, I'm more concerned that you don't seem to have any set goal other than "I did a degree in math and this linguistics stuff looks interesting. I wonder if I can combine the two?" I wasn't much better off when I decided to pursue linguistics but it sounds like you need to do a bit of leg work before you deciding that linguistics is right for you. Linguistics is a very broad and diverse field of study. Your first step should be finding what part you're interested in. Try to audit some introductory linguistics courses and/or skim an introductory textbook. Think about what made you interested in linguistics in the first place. Once you decide what area you want to pursue then you can start deciding what courses would help prepare you. In general, the math used in linguistics is very applied (as opposed to the more theoretical stuff you're probably used to) so you might be better off taking some engineering or computer science courses. A course on signal processing would be very helpful if you're interested in speech recognition but useless if you're interested in semantics. Similarly, if your interests lie in question-answer systems then some courses in AI would be helpful but would get you nowhere if you're interested in phonetics. Without an area-of-interest it is very difficult to answer your question.