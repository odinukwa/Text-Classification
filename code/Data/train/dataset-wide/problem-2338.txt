Response to comment: My reading of your question is that contains structured text in which links are embedded. Your application shreds from time to time to find and use these links. This is expensive and as a performance optimisation you would like to store the links separately in . This is where the update anomalies can creep in. If is updated so that has a different set of links, but is not also updated then there is inconsistent data in the database. It works the other way, too - a row can be deleted from without changing and, again, the data is inconsistent. This is why the application has to be diligently coded and tested. There is another rare-but-possible case to consider, too. Changing the data will require two statements - and . RDBM systems use pre-emptive multitasking. This means that at any time the RDBMS may choose to halt one workload and run another instead. So it could, for example, halt the updating stream after the first update statement and before the second. If there is another workload which is trying to read or at that point it may run, and get inconsistent values, if the reading transaction's isolation level allows it to do so. As I said, rare but possible. If these problems are acceptably unlikely in your application and the implications of it happening are not severe enough to worry about then denormalisation can serve performance advantages. If the risk of data inconsistency is too great then you'll have to find other solutions. 

So, primary first, then secondary. 2) Are you asking how node 1 is brought back on line? That's what your operations people and DBAs are paid their huge salaries to do. Alerts would be a good idea. Maybe even a very good idea. If node 1 dies AlwaysOn will re-direct connects to node 2, transparently to the application. In-flight work may receive an error. New connections will be routed to node 2. This is the HA and DR in action. When node 1 comes back you can continue running on node 2 as primary and have node 1 as secondary, or organise a managed failback at a time of your choice. If node 2 dies while node 1 is still down, then you have a problem. 3) Active-passive has the secondary as "warm": it is ready to become the primary when required but cannot do any application work until then. Active-active has a "hot" secondary: it can process read-only transaction from the application, and be used for backups and such like. IIRC one license is needed for active-passive and two for active-active. 

Create a new table with exactly the same schema as . Partition both these tables by, say, one-month date ranges. As data becomes stale in swap that partition out and swap it into . Your data retention policy can be efficiently enforced this way, too. A view which unions the two tables can make querying simpler. 

The points you raise are valid. An alternative design would be to store only the changed values in the history store. A record there would then be (primary key field(s), name of changed field, old value, new value, written by, written when). Storing only the parts that changed leads directly to two new problems. One, you have a mandatory overhead of separating the changed parts for every single write, as opposed to deferring the diff costs until needed. Typically we want OLTP writes to be fast and audit reporting is not performance critical. So it would seem reasonable to me to defer that work to when the report is required. Of course the cost of writing the whole record will be more than of writing the shorter diff-only record. You can benchmark your infrastructure to see if that is significant. Two, by storing only deltas you loose the data context in which a change occured. By this I mean by only storing deltas we can read a single audit record to see that, say, field 53 changed from "A" to "B". What values did all the other fields have when that change occurred? To find out you have to read backwards through history until you find the most recent value for the other fields in which you're interested, potentially all the way to the "live" record. Again, I don't know if this is a valid use case for your scenario, or if change frequency and audit read frequency would make it a problem, but it is something worth considering. To show "last updated by .. on .. " with either scheme is a single record lookup of the last history record written because both ways store this information on every history record. To find what changed with the full record stored will require a second read and a diff, which is not needed with a delta store. To reconstruct a historical record will be light work with full records and processing-heavy with deltas. I don't know your requirements or constraints well enough to fully evaluate one design over the other. My preference would be for the one I documented due to ease of implementation and (likely) performance. I will note, however, that there are no free lunches. The work has to be done somewhere, sometime. 

Two general principles I would commend to you. One - name your relationships so you understand why one entity type connects to another. You may find there are more than one relationships between any two, or that some you have imagined has do not represent anything concrete in the user's world. Two - imagine some of the uses cases for this system. Think about the queries you would need to write to fulfill each. Walk through your data model to ensure it supports your queries and constraints. The data model posted encodes many, many more rules than those you have listed. It would be impossible for a third party to validate the model in its entirety without all the rules. This is a large task which would typically take some days in a room with user representation, in my experience. 

Certainly not one table for each document. Perhaps one for each document type. In general you are correct - reading or writing to larger tables typically takes longer than the same operation on smaller tables. There are techniques to help with this - indexing for example. Modern DBMSs can easily handle tables of several million rows, however, so unless you are pushing these limits it is better for your design to match your requirements than to work around the limitations of the technology. If you commonly several different documents of different types simultaneously then a single table would be beneficial. If each document type is processed independently then separate tables would be better. 

The way I've achieved this is to add two steps to each backup job. The first writes to a results table; it doesn't matter where this lives so long as all jobs can reach it. The second step runs , targetting a common, new job. This new job has three steps. One, check the results table. If any backup job has not yet reported then stop. If all backup jobs have reported success then continue. Step 2 - reset the results table. Step 3 - send the email. Two and three can be reversed depending on your tolerence for failure and your recovery practices. The results table can be managed by / or /. You can get inventive with run numbers and history logging, too, I've that's needed. You have to think how you want to recover from failed jobs? Purge the results and start all again? Re-start failed jobs only? Have a precursor step which launches your backups, which then must be idempotent etc. etc. 

Although Normalization and partitioning both produce a rearrangement of the columns between tables they have very different purposes. Normalization is first considered during logical datamodel design. It is a set of rules which ensure that each entity type has a well-defined primary key and each non-key attribute depends solely and fully upon that primary key. Partitioning comes in during physical database design, when we start to map logical attributes to physical columns and determine the operational characteristics required from the system. Sometimes it is an optimisation added after testing under load because performance was found to be inadequate. It can also play a role in implementing a data retention policy. In partitioning we recognise that a table is made from rows and columns. When we partition we separate some of those rows (or columns) from the others and hold them in a physically different location. Horizontal partitioning is when some rows are stored in one table, and some in another. There could be many sub-tables. A typical example is when currently-active transactional data is separated from old "archive" data. This keeps "hot" data compact, with associated performance improvements. We many be able to make the archive tables read-only, compressed and on cheaper disk, too. As the next step each partition may be moved onto separate hardware. This is commonly know as "sharding." Advantages include being able to use many cheaper boxes rather than one very large, very expensive server, and being able to position a user's data geographically close to her. The cost is increased application complexity. Some DBMS incorporate this ability natively. Vertical partitioning is when some columns are moved to a different table or tables. Similar to horizontal partitioning the motivation is to keep the "hot" table small so access is faster. Say you run an e-marketing company. 99% for the time you need a person's name and email address and nothing else. These will go in one table and all the other stuff which is useful but seldom-used - birthday, golf handicap, PA's phone number etc. - go in a different table. It can also help when the partitions have different update regimes or are owned by different sections of the business. The two tables can have the same primary key column, and corresponding rows could have the same key value. While it is possible to have multiple vertical partitions for a table, and to shard vertically, I've never come across it. Vertical and horizontal partitioning can be mixed. One may choose to keep all closed orders in a single table and open ones in a separate table i.e. two horizontal partitions. For the open orders, order data may be in one vertical partition and fulfilment data in a separate partition. The techniques I've talked about are ways to change the design to improve performance. Scaling is when you change the hardware. One can scale up by buying a bigger box with more RAM, CPU or faster disk, or scale out by moving some of the work onto a different box. Scale up is sometimes called scaling vertically whereas scale out can be called horizontal scaling. While horizontal scaling and sharding have an obvious relationship they are not synonymous. It would be possible to use replication technologies to copy an entire database to another location for use by the users there, thus achieving scale-out, without having to partition any tables. 

Migrate piece-wise. Rename your table to tblsapdispatch_old. Create a new table called tblsapdispatch_new. This new table has the partitioning you want. Create a view called tblsapdispatch which unions the two together. This way the application is agnostic to the change. Move data from _old to _new in batches. The batch size will be found by testing. Move data one partition at a time. Rebuild that partition's index once full. You may be able to make the historical partitions read-only so you'll never have to rebuild those indexes again. When all's done drop the view and rename _new. You should only require an outage at the beginning and end, when the table names change. 

Since first starting to type up an anser I have argued myself into, and out of, a set position. I think this question steps over the border from the science of normalisation into the craft of data modelling. The answer will be determined by how you (or, more importantly, your users) understand the semantics of the situation. Is "extra cheese" a new "thing" which is sold, just like "burger" is a thing which is sold, or do they see "extra cheese" as a simple modification of the component list and "burger" is still the product? I'll take a different business area to show the difference more clearly. If two people each bought a Ford Focus but with different engine, radio, wheels, paint etc., would you think they had bought esentially the same "thing"? Chances are you would. Even though different specifications go into the factory, the item that emerges is thereafter treated as a single unit for sales, transport, handling, support and all other purposes. However, if one customer orders, say, a tow bar that would be a post-factory option and would be handled through different sales, distribution, stock keeping, and maintenance channels, so is likely to be seen by the business as a separate product. So to the burger. If a burger with extra cheese is still a burger then model it as variations to the composition. If "extra cheese" and "no cheese" are seen by the users as products in their own right model them thus, with their own composition and price, which may be negative. In term of line items, treating the variations as separate products is the easiest. Once set up in and they will flow naturally through to the invoice and stock totals just like the standard products will. You may need to add a product dependency table to ensure the variations make sense ("no cheese" only applies to cheese burgers; can't order "cola with extra cheese"). For the scenario where there is a single product with variaitons I would model it thus: 

Will have the obvious cost of the write to and read from #T1. There may be a benefit, however, if you can index #T1 in a way which helps the second query, perhaps if complex manipulation was performed on Table1's values. Stored procedures can suffer from parameter sniffing. This can sometimes be rectified by recompiling the procedure at each execution. Recompiles can be expensive. By splitting a complex query into several simpler ones you can put the recompile hint on just the statement(s) that benefits from it. An optimizer will not search indefinitely for the best execution plan. It will eventually time out and run with the best available at that point in time. The work the optimiser must do is exponentially larger with the complexity of the query. Having simpler queries may allow the optimizer to find the best plan for each individual query, providing a lower-cost execution overall. 

Unfortunately I don't have the environment to test the following, and there are undoubtedly several points at which it may fail, but I'll throw it out there for what it's worth. A CLR stored procedure has access to the current connection through the construct (taken from here). The SqlConnection type exposes a ConnectionString property. Since ApplicationIntent is in the initial connection string I'm supposing it will be available in this property and can be parsed out. There's a lot of hand-offs in that chain, of course, so plenty of opportunities for it all to go pear-shaped. This would run from a Logon Trigger and the required values persisted as needed. 

Quick-and-dirty, not tested on a live instance. For each row of Event data we need one row from Price - the one which happened most recently, but before the Event's timestamp. TSQL supports the notation. By embedding the Price lookup as a sub-query in a SELECT list it will be executed once per row in Event. Predicating Price on Event's values will ensure the most recent is returned. Something like this: