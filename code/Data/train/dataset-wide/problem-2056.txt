Throw hardware at the problem (SSDs, bigger server, more RAM etc.), or Talk to whomever manages the relationship with your vendor. Push the responsibility for system performance back on to the vendor, and/or Investigate the business case for moving to a competitor's product. 

SQL Server will treat SQL scripts as being case sensitive if the database has a case sensitive collation. I know one outfit that uses something like Polish binary in a test environment specifically to test for this. As to why your Arabic collation is treating latin characters as case sensitive, I'm not sure. EDIT: What is the default server collation - is the default collation case sensitive or binary? 

If you have access to a set of unix shell utilities (see gnu win32 if you need to get ones that run on Windows) you could export the column and process it with a shell pipeline along the lines of: 

As with your other questions, this one could be more specific. However, I will assume that you want to document mappings arising from analysis work. An ETL tool will actually load the data, and the mappings tend to take the form of wiring loom diagrams that show the mappings graphically. If you have relatively simple transformations the internal self-documenting capabilities of the ETL tool may suffice for this. However, this tends to break down on more complex transformations or in situations where not all of the logic is done within the tool. The more complex the transformation the more likely it is to happen. In many cases a decisions is made to do all the transformation work in stored procedure code and the ETL tooling is largely used for extraction, logging and load control tasks. If you are mapping for documentation or specification purposes, people often use spreadsheets if no other tool is available. However, this really carries an assumption of an isomorphic 1:1 mapping from source to target and does a poor job of capturing and documenting complex logic. Some CASE tools have meta models for mapping available. I've done this with Sparx Enterprise Architect, although we had to build a custom metamodel for this. Some tools have this feature out of the box. Finally (and this is my preferred approach) a functional specification for the ETL can be written up as a technical document. This can describe mappings and detailed specifications for the logic. In the case of a system with complex logic in the ETL a spec document is by far the best way to do this. 

* I'm not an ORACLE Dev, but the function should look something like the one above... Once you create the function, you will create a functional index that will use the function to look for the rows you need: 

In your first query, both valid values + null values are returned from the database, but because you use the "limit 50", and because NULLs are displayed last, you don't see the rows containing NULL Price. The correct for for the first query should be: 

Replication was made for small changes, so you're on the right track. To keep the systems in sync, at the database level, you will need to setup a mysql master-master replication scenario. I've used the tutorial from this linke ($URL$ to setup such configuration. Depending on how much data is changed, you can choose between [statement] or [row-based] replication. More info about the differences between the 2 types: $URL$ 

In the first use case, you (or the application) do a select to retrieve some metadata about a file; In the second one, you enter an ID (select query) and the application does the archiving (insert statement into table_b); What exactly do you need to automate here ? 

2NF states that a table contains no fields that are logically a function of a part of the primary key. 3NF states that a table contains no fields that are logically a function of any field of the table but the 'whole' key. 3NF can be viewed as a specialisation of 2NF. 

I'm not quite sure what you're asking, but I think you want to do the following (correct me if I'm wrong): 

Often the data can be profiled by just poking around it with a database query tool such as SSMS and writing SQL queries directly against the source, or a copy of the data loaded into a staging or scratch area. Desktop tools such as spreadsheets (pivot tables can be useful) or database systems such as MS-Access may also be helpful. Purpose built data profiling tools such as Pandora X88 are also available, but these tend to be relatively expensive. Often they are quite a hard sell, even on larger enterprise projects where they represent a tiny fraction of the overall budget. Often your best approach is to simply copy all your source data into a staging database and poke around it. This work can also form the basis of your staqging processes later on. 

Because your hierarchy models is Group A -> Group B -> Group C, it's enough to store in [user_groups_data] table just the user_id, the last group id (in the groups hierarchy tree), and the info So, data in the tables can look like: 

Your free space in the current innodb file (ibdata[1] ?) is decreasing. It doesn't necessarily mean that your website is slow. Maybe the number of users (concurrent users) has increased in the last few days ? (also causing lots of inserts int he DB?) There is no error in the print screen. it's just your monitoring system, reporting a value that have reached the critical threshold. 

Why not recreating the primary key as (nid, vid), and creating a new index on the column "order" (just for fast retrieval / ORDER BY clause). --> The worst that can happen is to have 2 ingredients with the same "order" value, but the order-change-logic should be already correctly defined in the application. 

This can and will work (theoretically :) ...), but you will need a recursive function (in application or in the database) that can return the parent group id (ex. B-1), or the top group id (ex. A-1) for any group id value given as an input parameter (C-1). But ... like I said: The more info you give, the better responses you get. Hope that helps. 

From your question I think you want to do the latter. Typically this is done by having a separate set of cash transactions, and a bridging table that has the allocation of cash payments to invoices. If the values are equal or the cash payment comes with a single invoice reference you can do the allocation automatically. If there's a M:M relationship between invoices and payments you will need to do a manual matching process (doing this automatically is actually a variant of the knapsack problem). A basic cash matching system Imagine that you have an invoice table, a cash payments table and an allocation table. When you issue an invoice then you set up an invoice record in the invoices table and a 'receivable' or 'payable' record in the allocations table. 

(note not tested, just off the top of my head, but you can fiddle with it). Then you can re-load the stored procedures. Note that if you're frigging with the code base you should really test what you're doing rather than doing a blind search/replace on production code. What could possibly go wrong? 

Scenario 2: M:M relationship between dimensions: Harder to think of a use case, but one could envisage something out of healthcare with ICD codes again. On a cost analysis system, the inpatient visit may become a dimension, and will have M:M relationships between the visit (or consultant-episode in NHS-speak) and the codings. In this case you can set up the M:M relationships, and possibly codify a human-readable rendering of them on the base dimension. The relationships can be done through straight M:M link tables or through a bridging 'combinations' table as before. This data structure can be queried correctly through Business Objects or better quality ROLAP tools. Off the top of my head, I can't see SSAS being able to consume this without taking the relationship right down to the fact table, so you would need to present a view of the M:M relationship between the coding and the fact table rows to use SSAS with this data. 

Inserting 10 rows each 2 minutes will result in (24*60/2)*10 rows per day (7200) -> that is not a large value to worry about. Also, it's great that you think on the "future - 10 years", but don't lose time with premature optimization. If your only concern it's about this table where you're inserting data every 2 mins, there's no point in creating additional databases (one per year), so let's stick with this table (Table_A). Now, because this table will slowly increase in time, and your want your queries to run fast, you have plenty of options: 

you can partition the table by certain criteria. Because your using the table for a "weather station", and your data is time series values, your best option would be to partition by [station_id] then by [created] Create a Table_A_Archive, where you can move data that would be to old to keep in Table_A. How long do you intend to keep data in Table_A ? would it make sense to delete old rows that become obsolete for you application 

... And so on. Best Bet: Partitioning the existing table by [station_id] and [created], you will have "A" partitions for each station, "B" partitions for each month, and a total of AxB posible number of partitions. Once you partition Table_A, do the same thing for Table_A_Archive, and on the end of each year, move the data from Table_A to Table_A_Archive. ** IMPORTANT:** After you make the partitioning schema, keep in mind that all queries should have in the WHERE clause the conditions necesarly so that the query will hit as little partitions as posible. Ex. 

200 million rows is certainly in the range where you could benefit from table partitioning. Depending on your application, you could bet some of the benefits listed below: 

Oracle comes with less B.I. tooling out of the box, although Oracle do sell OLAP servers, reporting tools and ETL tooling. 

Query tuning 101 There is no magic silver bullet to query tuning, although I can give you some hints and tips. The first thing to do is to understand what's actually going on behind the scenes. Get a good internals book like the third Guru's Guide book. Poorly performing queries tend to come in two basic flavours: Transactional queries that take too long, and grinding batch jobs (or reports) that take too long. One good sign of a query with something wrong with it is a single item in the query plan taking 99% of the time. Transactional queries On most occasions a poorly performing transactional query is one of a few things: 

Generally, it is only sensible to use embedded code in the DBMS for applications where it is strictly necessary. An alternative: OODBMS platforms Some OODBMS platforms such as Gemstone/S, are designed to support an closely coupled O-O language and the system is intended to run like this. However, in the case of Gemstone/S the system is tightly coupled to a limited set of languages, prinicpally Smalltalk. However, the Gemstone/S VM is good enough that somebody even ported ruby to run on it with a project called maglev.. This might get you closer to what you want, but it requires you to move to a specific platform. 

Your questions is a little foggy You don't mention how the archiving process works You're basically asking about a feature on the User Interface side ... not on the DB side. 

Also, I believe that the application that's using your database already knows what those values mean. So what is the real use case for your constants ? If you really want to go with your approach, without changing the existing column type, you can create a function that will return the STRING value for each integer code: 

This is a sample matrix. The query should find all the green cells, with value = F (as is TO BE FOUND :) ) ... 

From docs: ($URL$ The Oracle Flashback Database feature, which provides an convenient alternative to point-in-time recovery, generates flashback logs, which are also considered transient files and must be stored in the flash recovery area. However, unlike other transient files, flashback logs cannot be backed up to other media. They are automatically deleted as space is needed for other files in the flash recovery area. When Files are Eligible for Deletion from the Flash Recovery Area There are relatively simple rules governing when files become eligible for deleteion from the flash recovery area: 

Worth noting is that oracle dump files are a stream format with snippets of SQL embedded. You can also use sed to do substitutions on it, such as schema names. 

Don't get too worked up about data volumes. You can mitigate a lot of performance issues by partitioning the table on a time key. Just load the data into the table and make sure all queries against it use the partition key so the optimiser can ignore partitions that it doesn't need. You will need to make an incremental loader, though. If you want to analyse trends over time then make up summary tables and refresh them on a periodic basis. This will be less complex than consolidating historic data, and gives you the option of going back and auditing the detail if necessary. Additionally, if your source systems are purged on a regular basis (quite common with POS systems due to the data volumes) then you might not have the option of reloading historic data. Warehousing the detailed transactional data also gives you a repository and frees the line-of-business systems from having to maintain historic data. If you really need to, you can keep data for a finite length of time by clearing out older partitions and archiving them off somewhere else. The partitioned architecture makes this fairly easy to do. An SQL database is a good fit for this because its query facilities are much better than those available from nosql databases and (more importantly) because it will play nicely with third party reporting tools. If you want to do any significant analytical work you might be better off with PostgreSQL than MySQL. 

2 questions: - Where is the IO problem ? On the master or on the replicas ? - If the IO contention is on a replica, can you create a TEST replica, replicating from the same master, but having innoDb engine ? The thing with MyISAM storage is that an UPDATE / INSERT query will lock the whole table. And the replication thread on each replica is also a serial process, running just one query at a time (event) from the master binlog. My suggestion would be change the storage engine for all tables, from MyISAM to InnoDb, then re-check the replication parameters. 

Because both of them are virtual servers (you didn't mention the technology used (ESX/KVM...), you can try this: 

FRA will be purged automatically when free space is needed Put the archived logs in FRA as well Use RMAN to make the backups and use RMAN commands (REPORT OBSOLETE / DELETE OBSOLETE) to manage the purging of backup pieces + archived logs from FRA 

Just for fun, and the sake of your question, if are using MySQL, this would be a stupid hack to check if the hourly file has been received on the server: 

The reason for which you first query didn't use the index was because the result of isnull(Price) returned an un-indexed value.