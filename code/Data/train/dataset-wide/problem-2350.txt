I partially solved it: PHONE is actually a VARCHAR2 column. However, this doesn't at all explain why comparing in one direction versus another works or doesn't work, when I make that mistake. So I'm still very curious. 

Very weird question: Is there any way to artificially inflate the amount of time a query will take? An infinite loop would be great. 

Sorry if the way I phrased that is confusing. I'm writing a program that acts kind of like a filter - it retrieves an answer to a query, counts the rows, then I want to do a slightly more narrow query. I want to do those queries until I get a count of 0, which means the query just before it is the narrowest I can get. I then use a similar version of that query, except instead of counting I actually retrieve the records. The best implementation I know of is to just write all of the different queries, put them into different statements and feed those statements into different result statements, and retrieve the counts from there. Then, I use the last one that counted > 0 and use that main query without a count, and use those results. This seems like a horrible implementation. It seems like I'm just going to be bombing the database over and over. I'd prefer to hit it once, and then narrow the resultset on the client. Any ideas? 

Does this sound right to you guys? I expected it to drop most of my indexes but then to create a ton of new indexes. Also, if it takes 4 hours to analyze 9k queries, is it even feasible for me to get this to consider a normal day's worth of usage? Compared to most large databases, ours is fairly light on consumption (~50 users total). I think I'm either misunderstanding something or am simply doing something wrong. 

The script works fine as-is but the moment you uncomment out that second , all hell breaks loose. I understand this is invalid syntax because I'm mixing boolean logic with bitwise operators. But what is the right way to get this logic in there? Without going crazy with statements, is there some way to do this? 

That table originally had only 2 indexes: the Clustered PK index (that this shows it doing the Key Lookup on) and another FK index on a column not referenced here. Given that this heavy query always needs these add'l columns (DateForValue [datetime], CurveValue [float], BTUFactor [float], and FuelShrink [float]), I thought a covering index was the obvious solution here to remove the (slow) Key Lookup being performed here. So I added the following covering index: 

Redgate SQL Compare appears to do this decently well. I'm still nailing down the settings to ignore certain thing (like ) but at a glance, this seems like the tool I want. 

You'll need to put a trigger on the siteArea table so that whenever it's updated or inserted, it'll call a stored procedure with an argument representing the new site area and that procedure will calculate the needed values and insert them in a table. Your question is too ambiguous for me to describe further. 

Will this trigger propagate? Like, if it updates a ROOT_ID for another record, will that trigger its own trigger? Further, if it does trigger that, will it use the new ROOT_ID? I want the ROOT_ID to propagate down the tree I've built. Edit: How this works is that each record has a unique ID, a parent ID, and a root ID. I basically have a tree, each member of that tree has a root_ID pointing at the unique ID of the root and a parent ID pointing at the one above it. The root's root and parent IDs are its own unique ID. in the case that a user manually changes a record to point at a new root and parent, I want all the children of that node to have the new root ID. Is there a better way to do this? 

I've got some resource leaking and I'm trying to find what's causing them. I think I have some statements or resultsets that aren't being closed properly. Is there any way to see all of the cursors that my JDBC connection currently has open? I am using Eclipse. 

As expected, this index gets fragmented pretty quickly so we have to be on the ball with keeping it rebuilt. However, this is where we have a problem. My maintenance script that goes through and rebuilds all indexes over x% fragmented with always hangs forever on these indexes. I've tried to let one run for over 3 hours without success when the database was mostly to completely idle. Just for kicks, I even put the DB into single user mode and nothing changed. I also tried with , again with no change. However, I am able to simply drop/create the index and it completes within ~15 seconds! Just to make sure there's nothing funny with my maintenance script, I've manually tried rebuilding these indexes with no success. The script I use is: and it seems like it will never succeed. Given that I can drop and recreate it in only 15 seconds, 3 hours is WAY more than plenty to see this succeed on an idle database. For now, I've altered my maintenance script to filter out indexes with "GlobalID" in the name and then have a follow-up script that drops/creates these indexes. This gets us by until we start having naming variations on these types of columns (i.e. we need a uniqueidentifier for some other purpose). Any idea why rebuilding such indexes would never finish within a reasonable amount of time? I see this happen across ~12 tables, all with essentially the same column/index on them. 

I want to determine if a resultset exists very quickly. At the moment, I'm doing a count - this is taking roughly 55ms, which is unfeasable.The table has ~100k records - I don't care if it has 2, 5, 100k rows that fit a query; I care if it has 0 or 1. Maybe 2 in certain situations. Is there a way to do this? Would limiting a count using ROWCOUNT (so it only counts about the first 2 rows it finds) speed up the count at all? 

Turning the original select into a table is not an option. Ideally for readability, I'd like to not copy/paste the select; I'd rather alias it somehow but I feel like my syntax for that isn't quite right. Here's my actual code. It is a horrible mess (actually if you have any suggestions for making it less of a mess that'd be great). It gets on the top layer - the layer below that works fine (producing the first table above, essentially) 

I have a query which generates a result set (key, a, b). I want to use an update statement where, for each entry in a table with Tkey = key, set ColA = a, ColB = b. How would I do this? Right now I'm trying: 

I want a PL/SQL procedure to give me progress updates as it runs. However, DBMS_OUTPUT seems only to give me the output when the whole procedure is done. Is there a way to make it give me updates during runtime? Thanks! 

We're working on migrating our database (~400 tables) from SQL 2008 R2 to SQL Azure. We're currently in the proof-of-concept stage, figuring out the process we'll be following. We have a process that we think is solid but I'd like to perform some validation diffs on both schema and data to confirm that we have a successful migration. I've never done this before with SQL Azure and am looking for a good way to do this. How can I perform this verification effort on both the schema and data? Ultimately, this is a one-time migration (we'll do it a few times but the real migration will only be done once). 

However, even after adding this index, it seems the query is still doing the Key Lookup. Am I missing something obvious here or is this the right idea and I just have a problem elsewhere? Note that all statistics and indexes have been refreshed and this isn't THAT highly dynamic of a table but it is approaching ~1M records. A simplified version of this query, focusing on this table of interest, is as follows. Nothing I removed references the PrimaryTableOfInterest. 

It only tuned ~9k out of ~530k queries It recommended I drop a ton of indexes (in fact, most of them) It recommended I create 0 indexes 

We have a number of tables (~1M records) that have a column on them defined as: that gets auto-populated with . We use this ID for synchronizing data across multiple systems, databases, file imports/exports, etc. For this column, we have the following index created: 

Here's the weird thing, the following query DOES work, the only difference being whether rating needs to be larger than or smaller than 1. Suffice it to say that I'm utterly perplexed. Is this a bug? I'm on 11.2.0.4.0. Could it be something to do with JARO_WINKLER? 

I've been using sysdate to time procedures, but a lot of my procedures are very small procedures that get called thousands of times. Is there a way to get milliseconds elapsed instead of seconds? I know it has something to do with timestamps, but I can't find any exact way to do it. 

I'm using UTL_MATCH's Jaro Winkler similarity function and it seems to be performing well. However, I would like to adjust the prefix scale according to the situation. Is this possible? Is it possible to see what the default prefix scale is? I could not find any documentation on this, but it seems that in order to be a J-W distance, it must use a prefix scale. 

So, I have an insert statement in a Java program I'm writing. Under some conditions, I want it to insert some values as null. However, before I can execute the statement, I have to set all of the tokens to values. How can I make it set something to null instead? Edit: Example: 

I have this sub-procedure in a fairly large program, and it is taking hundreds of times longer than comparable sub-procedures. Is there any way I could improve efficiency here? None of this procedure's sub-procedures are absurdly slow, so I am sure that it has to do with the structure of this one. This program looks through a table containing hierarchies of doctors. If it finds a matching root doctor, it adds to that root. Otherwise, it tries to find a matching not-root doctor. I think this might be the source of the inefficiency; the fact that it's opening two very similar cursors and looping through both quite often. Additional info: UNIQUE_GOOD_MATCH is, on average, taking about four seconds. UNIQUE_PHYSICIAN has roughly 200k records, and is indexed by first and last names. It doesn't seem to me that it should be taking this long, especially when other procedures are completing at a fraction of this time. 

We have this complicated query that I'm trying to make "better" until we move it to pull from a data warehouse. I need a solution that's "good enough" for now and I think I'm about 2-3 indexes away from making that happen. I'm stuck on this part, however. I'm specifically targeting this part of my Execution Plan: 

How can I mix boolean logic with bitwise and/or operators in SQL Server, specifically SQL Azure Database and/or SQL Server 2016? To demonstrate what I'm trying to do, consider the following script: 

I'm working with each of the 3 portions of the UNION ALL independent of one another and the other two parts are nice and speedy and executing this third of the unions either by itself or in the union performs similarly (i.e. ~30 seconds). So the UNION isn't a factor but I included it just for thoroughness sake. 

My background: I'm a dev/architect, not a DBA. Sorry! So we have a ~400 table 75GB database. I ran Profiler for ~24 hours ("Tuning" template minus ) and have ~7GB of usage recorded in a trace file. I then ran the Database Engine Tuning Advisor on this trace file (no partitioning, keep clustered indexes, PDS Recommend: Indexes) against our production database (after hours). I gave it ~4 hours to analyze. And here are the summary results I got: 

The BlitzIndex tool that @JMarx suggested is working great! However, I'm also finding this additional script to make some good suggestions as well. Not necessarily using all or even most of its suggestions, but cherry-picking from the top is proving very useful!