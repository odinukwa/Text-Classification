Conway's Game of Life in 2-dimensions, as my exemplar instance in the class of (what used to be my overly general answer of...) Automata: deterministic finite state machines and nondeterministic and probabilistic automata and the theory behind them leading to things like acceptors of regular languages and the concepts of simulation, computational equivalence and computability as in Turing machines and "Turing equivalent", and the concept of "power of computing", computational complexity and complexity classes, bisimulation (and the equivalent computing power of single-tape vs. multi-tape and other classes of Turing machines, and the equivalent computing power of systems which can simulate other systems). 

The answer depends upon, of course, if the players make their moves intelligently or stupidly. In either case, build an alpha-beta tree of move sequences and search either depth-first or breadth-first, stopping and backtracking as winning configurations are found. However, there are some rigorous points you have not defined about this question, including whether perhaps it came to you as a homework problem. Hoping that this is not the case, here's a take on it: Yes, if $n=2$, then the first player to move obviously wins by picking any one of the four open spots, and the game is over. If $n=3$ (which you don't define for odd $n$, but lets say for odd $n$, $(n+1)/2$ spaces wins, then the first player to move wins automatically also, because no matter which space they take in the $3 \times 3$ grid, the next player can only block one space, and in the 2nd round, first player easily wins. For $n=4$, the first player wins, with the same strategy as shown for $n=3$. $A$ picks any square in the $4 \times 4$ grid, $B$ picks any other square but can only block $A$ in one of the two dimensions in which this grid lies, therefore on the first step of the second round, $A$ wins by picking a square adjacent to his first square. Are you sure that you've thought this out for the smaller grid games? 

Consider a sequence of i.i.d. random variables $(X_n)_{n\geq 0}$, and set $S_N = \sum_{n=1}^N X_n$. For which $p> 0$ do we have that \begin{equation} \lim \inf \frac{|S_N|}{N^{1/p}} > 0 \text{ almost surely}? \end{equation} In [1], important results are shown for the quantity $M_N := \max_{n\leq N} |S_n|$. In particular, $\lim \inf \frac{|M_N|}{N^{1/p}}$ is almost surely $\infty$ for $p> \delta$ and almost surely $0$ for $p< \delta$, where $\delta$ is an index that is explicitly defined from the law of the $X_n$. The index $\delta$ is typically $\alpha$ for symmetric-$\alpha$-stable random variables. However, the case I am interested in is presented as "much more difficult" by William Pruitt. I would be curious by any partial results (stable laws, infinitely divisible laws). [1] Pruitt, The Growth of Random Walks and Levy Processes, 1981, Annals of Probability $URL$ 

If the $X_i$ are i.i.d. Gaussian with variance $1$, then you have $$ c_p := \mathbb{E} |X_k|^p = \frac{2^{p/2} \Gamma(\frac{p+1}{2})}{\sqrt{\pi}}.$$ The variable $S_n$ is also Gaussian with variance $n$, therefore you have $$\mathbb{E} |S_n|^p = c_p n^{p/2}.$$ Hence, $\frac{\sum_{k=1}^n \mathbb{E} |X_k|^p}{\mathbb{E} |S_n|^p} = n^{1-p/2} \rightarrow \infty$ for $1<p<2$. At least, it means that you cannot hope for a constant $C$ as you expected. 

Joseph, I believe that there is not a simple linear smooth mapping. My short explanation for it is that the morph from $S_0$ to $S_1$ does not really consist of a single continuous deformation, but instead of the concatenation of a sequence of multiple piece-wise linear deformations consisting of more than three separate steps: $S_0 \to S_a \to S_b \to S_c... \to S_1$ Your $S_0$ is the frame of an octahedron. 

Now, drawing the graph structure of this binary matrix leads to a three component graph composed of these three graphs (which all happen to be $K_4$, the complete graph on 4 vertices) with vertex sets composed of 

For an $n \times n$ grid, the probability of finding a path of length $n\sqrt{2}$ is $1/2^n = 2^{-n}$. For a grid of size $(n,0)$ or $(0,n)$, the expected path length is $n$ with probability $p=1$. Let's call the expected path length $L(x,y)$ $$L(n,0)=L(0,n)=1 \cdot n = n$$ For a grid of size $(n,1)$ or $(1,n)$, the expected path length is $n+1$ if all possible diagonals face the incorrect way, and $n+\sqrt{2}$ if there exists at least one-diagonal facing the correct way to create a short-cut: $$L(n,1)=L(1,n)=\binom{n}{1} \cdot \frac{1}{2^n} \cdot n + (1 - \binom{n}{1} \cdot \frac{1}{2^n}) \cdot (n+\sqrt{2} )$$ $$L(n,1)=L(1,n)= n + (1 - \binom{n}{1} \cdot \frac{1}{2^n}) \cdot \sqrt{2}$$ $$L(n,1)=L(1,n)= n + (\frac{2^n -1}{2^n}) \cdot \sqrt{2}$$ For a grid size $(n,2)$ or $(2,n)$, the expected shortest path length is $n+2$ if in all of the locations, there are no correct facing diagonals; $n+1+\sqrt{2}$ if the short-cut diagonal only occurs in the last square (top-most); or length $n+2\sqrt{2}$, if there is a short-cut diagonal in one of the first column's $n-1$ lower squares, and a short-cut diagonal in one of the second column's upper squares after the lower square. This could probably be written as a recursive formula to see what the limit yields, as $L(0,0)=0$, $L(1,0)=L(0,1)=1$, $L(1,1)=\sqrt(2)\cdot\frac{1}{2}+2\cdot\frac{1}{2}=1+\frac{\sqrt{2}}{2}$, 

The theory of generalized stochastic processes was introduced independently in the 50's by Ito* and Gel'fand in a short paper. The latter then developed his theory more extensively in the fourth tome of his work on Generalized functions**. I am looking for the initial short paper of Gel'fand, in Russian, with the following reference: I. M. Gelfand,Generalized random processes, Doklady Akademii Nauk SSSR100 (1955), no. 5, 853–856, in Russian. I couln't find it on the web. According to WorldCat, it is in the MIT, too far for me. Does anybody have another solution to obtain this historical paper? *K. Itô, Stationary random distributions, Kyoto Journal of Mathematics 28 (1954), no. 3, 209–223. **I. M. Gelfand and N. Ya. Vilenkin, Generalized functions. Vol. 4. Applications of harmonic analysis, Academic press, New York, USA, 1964. 

I am using the notations, definitions, and results of the Section X of [1] on generalized Orlicz spaces. We say that $\varphi : \mathbb{R} \rightarrow \mathbb{R}^+$ is a $\varphi$-function if it is symmetric, nondecreasing, continuous, and satisfies $\varphi(0) = 0$. Then, for a measure space $(\Omega, \Sigma, \mu)$ and $\varphi$ a $\varphi$-function, $L^{\varphi}(\mu)$ is the set of measurable functions $f : \Omega \rightarrow \mathbb{R}$ such that \begin{equation} \rho_\varphi (\alpha f) := \int_{\Omega} \varphi( \alpha f ) \mathrm{d}\mu < \infty \end{equation} for some $\alpha > 0$. We call $L^\varphi(\mu)$ a generalized Orlicz space. We set \begin{align} \lVert f \rVert_\varphi = \inf \{ k > 0, \int_{\Omega} \varphi \left( \frac{\alpha f }{k}\right) \mathrm{d}\mu \leq k\}. \end{align} Then, $(L^{\varphi}(\mu),\lVert \cdot \rVert_\varphi)$ is a complete linear metric space (after identifying $f_1$ and $f_2$ when $\lVert f_1 - f_2 \rVert_\varphi = 0$), cf. [1], Section X, Theorem 2. Question: Consider a continuous and linear operator $\mathrm{L}$ between two generalized Orlicz spaces $L^\varphi(\mu)$ and $L^\psi(\mu)$. When is it true that there exists a constant $C>0$ such that \begin{equation} \rho_\psi(\mathrm{L} f) \leq C \rho_\varphi(f) \end{equation} for every $f \in L^\varphi(\mu)$? Some motivations: When the function $\varphi$ is convex, $\rho_\varphi$ defines a norm on $L^\varphi(\mu)$ that is therefore a Banach space. We talk in that case of Orlicz spaces. Here, we do not assume that $\varphi$ is convex, and not even that $\varphi$ goes to infinity at infinity. This situation occurs in the study of infinitely divisible random variables taking values in spaces of infinite dimension, see for instance [2]. Still, $\mathrm{L}$ being continuous and linear, it is bounded for the metrics $\lVert \cdot \rVert_{\varphi}$ and $\lVert \cdot \rVert_{\psi}$ but it is not so clear to me what can we say for the quantities $\rho_{\varphi}$ and $\rho_{\psi}$ in the general case. [1] M.M. Rao and Z.D. Ren (1991), Theory of Orlicz spaces [2] B.S. Rajput and J. Rosinski (1989), Spectral representation of infinitely divisible processes. Probability theory and related fields, 82(3), 451-487 

Yes. In the same way that flipping a fair coin (with equal probabilities of getting heads of tails) eight times in a row is likely to come up all heads 1/256 times, or all tails 1/256 times. The psychological perception of a sequence with a run of 8 heads or 8 tails is that it is so unlikely as to never occur at all; whereas we mathematicians see the likelihood of a run of 8 in 8 flips as occuring with 2/256 or just under 1% of the time. The opposite error is true, and also occurs with some frequency in biomedical experiments and medical experiments. The standard for accepting a result in a clinical or medical trial is for $p<0.05$: that there is less than a 5% probability that the results occured by chance. Thus, one in twenty times, it is possible that a random occurence or set of occurences will be perceived or accepted as being statistically valid when it is not. But it also depends on how much data (how many draws) are in the sample being gauged for randomness. The smaller the sample size, the more likely you are to discard a valid but unreasonable appearing "true" random sequence. So my answer is really a qualified "maybe". Shouldn't the validity or "true randomness" of the method be the gauge, along with a check to see that the algorithm is properly implemented? The problem, of course, with software is that bugs can creep into the implementation at any point: 

Since decades, mathematicians are studying function spaces, discovering new structures more and more adapted for a general theory of functional analysis. In that works, sequence spaces are generally seen as a very simple example of "function" spaces. They become interesting in approximation theory (for instance to obtain some interesting characterization of Besov spaces using wavelet theory), being isomorphic to some important function spaces. However, I was unable to find a good reference discussing important questions of functional analysis for the special case of sequence spaces, and discussing the particularity of this "simple" case, actually the simpler in infinite dimension. I precise a bit my request with the kind of topics I am interested in. 

I couldn't find this reference in the traditional databases, either on the website of the journal. Is it possible to find it on the internet? 

Consider the Sobolev spaces with $p=2$, defined for $s \in \mathbb{R}$ as \begin{equation} W^{s} = \left\{ u \in \mathcal{S}', \ (1 + \lvert \cdot \rvert^2)^{{s}/{2}} \widehat{u} \in L_2 \right\}. \end{equation} It is a Hilbert space for the norm $\lVert u \rVert_{W^s} = \left(\int_{\mathbb{R}} (1 + \lvert \xi \rvert^2)^s \lvert \widehat{u} (\xi) \rvert^2 \mathrm{d} \xi \right)^{1/2}$. The weighted Sobolev space $W^{s,r}$ is now defined for $s,r \in \mathbb{R}$ as \begin{equation} W^{s,r} = \left\{ u \in \mathcal{S}', \ (1 + \lvert \cdot \rvert^2)^{{r}/{2}} u \in W^s \right\}. \end{equation} Again, it is a Hilbert space for the norm $\lVert u \rVert_{W^{s,r}} = \lVert (1 + \lvert \cdot \rvert^2)^{{r}/{2}} u \rVert_{W^s}$. We have the obvious embeddings, for $s_1 \leq s_2$ and $r_1 \leq r_2$, \begin{align} W^{s_2,r} \subseteq W^{s_1,r}, \\ W^{s,r_2} \subseteq W^{s,r_1}. \end{align} Now, is the following result true? Conjecture: Fix $s_1, s_2, r_1, r_2 \in \mathbb{R}$. Then, \begin{equation} W^{s_1,r_1} \cap W^{s_2,r_2} = W^{\max(s_1,s_2),\max(r_1,r_2)}. \end{equation} Of course, due to the embeddings above, $W^{\max(s_1,s_2),\max(r_1,r_2)}$ is included in $W^{s_1,r_1}$ and $W^{s_2,r_2}$ and therefore in their intersection. Is the other inclusion also valid? 

A long comment, too long and painstakingly difficult to keep re-editing in the comment boxes: If you don't require a draw to be declared, there are multiple scenarios in which king vs. king or (king+queen) vs. (king+queen) can play on infinitely; in that case, the game tree of chess is unbounded. There must be a strict rule for when to prune a branch in the game tree. @Didier-Piau, the upper-bound concept as posited by the poster of this question appears to have 3 mistakes in it. It may be the concept of {white pawn, white other, black pawn, black other, empty}$^{64}$, which has a set size of $5^{64}$. 

And the next few years with ascending runs of 1,2,3, and 4 prime factors will start after the years 3216, 4056, and 4176 with 3217, 4057, and 4177 as prime years. Unfortunately, these computational results are not giving me the germ of any shortcut or understanding. There are also some descending sequences in terms of the number of prime factors, and their placement also does not help. If you want an ascending run of 1,2,3,4, and 5 prime factors, we have to wait almost half-a-million years to get to the exciting years of $n=491850$ and $n=521880$ for $k=5$ 

Let $S(\mathbb{N})$ be the space of rapidly decreasing sequences and $S'(\mathbb{N})$ its topological dual, the space of sequences bounded by a polynomial. For $m\in \mathbb{Z}$, we also define $\ell_2^m (\mathbb{N})$ as Hilbert spaces of sequences such that $(u_n (n+1)^m)_{n\in \mathbb{N}} \in \ell_2 (\mathbb{N})$. It is known than $S(\mathbb{N})$ is the projective limit of the spaces $\ell_2^m (\mathbb{N})$. As such, $S(\mathbb{N})$ is a Frechet space with a nuclear topology. Its dual $S'(\mathbb{N})$ is hence the inductive limit of the spaces $(\ell_2^m)' (\mathbb{N}) = \ell_2^{-m} (\mathbb{N})$. So: Fact 1: $S'(\mathbb{N})$ has a natural complete nuclear topology defined as a countable inductive limit of Hilbert spaces. It is also known that any complete nuclear space is isomorphic with the projective limit of a suitable family of Hilbert spaces. See for instance Corollary 3, Section 7.2 of Topological Vector Spaces. Fact 2: $S'(\mathbb{N})$ has an abstract complete nuclear topology as a projective limit of Hilbert spaces. Question: Is it possible to describe the topology of $S'(\mathbb{N})$ as a countable projective limit of Hilbert spaces $H_m$, meaning that $S'(\mathbb{N}) =\bigcap_{m\in \mathbb{N}} H_m$, such that the $H_m$ are described as sequence spaces (bigger than $S'(\mathbb{N})$ of course)? 

Numerous papers are referring to the following one R. M. Blumenthal and R. K. Getoor, Sample functions of stochastic processes with stationary independent increments, J. Math. Mech. 10 (1961), 493–516 in the literature, where they authors define what is now referred as the Blumenthal-Getoor index. However, I couldn't find this reference in the traditional databases, either on the website of this journal (does it still exist?). Does anyone have some information to find this paper? Thank you. 

Also, make sure to speak with more than one professor, and do not take any single person's advice as being the final word. Mathematicians are human beings too, and subject to the foibles and inclinations and disinclinations that all human beings have. If you run into disgruntled and critical individuals, do not let that dissuade you from going on into mathematics or decrease your desires. If you run into overly optimistic individuals who praise you too much and are too eager to take you on to do "scut work" computer programming, thank them for their time and let them know you'll come back to speak with them after you've spoken with other professors and weighed your options. Don't turn anyone down immediately. Always be polite in speaking with professors and teachers. Ask them how they chose their topics for their degrees, and you'll learn a lot. 

The era of searching on the internet has increased the ease of looking up answers and solutions to textbook questions, and so perhaps it has increased the percentage of students who go to the trouble of looking for a shortcut rather than working out the problem on their own. But I do not think that it is a new problem. There have always been resources to turn to which had solutions to problems: