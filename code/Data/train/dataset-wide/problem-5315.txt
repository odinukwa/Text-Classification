I think this is a rather difficult question to address directly/compactly, but here's my take: Firstly, "science" is a term that requires clarification. The existing practice of science involves both predictions of observable phenomena and also explanations (models, theories) of such phenomena. Now, every metaphysical theory essentially has to accept the "predictive" aspect of science. For example, if you have a metaphysical theory that asserts purely divine mechanisms of disease and cure, and you refuse to use the scientific fact that antibiotics cure bacterial infections, pretty soon you'll run out of adherents. So all viable metaphysical theories have to accept/adjust to the "strong" facts (i.e. predictable patterns) that science discovers about the manifest world. However, when we talk about the "explanatory" aspect of science, which pretty much always involves the use of certain entities or abstractions that are not directly observable, these explanations themselves are essentially (or at least heavily) metaphysics -- certainly in a positivist/empiricist sense. For example, science used to refer to something called "phlogiston" to explain the phenomena of burning; we know that it was a purely metaphysical entity, because it turned out not to exist! And nowadays we can talk about "gravity", which used to be considered a "force", but now is perhaps thought to be some characteristic of "curvature of spacetime", and in the future may get discarded in favor of another explanatory entity. Incidentally, there is nothing wrong with this -- science advances by testing various "metaphysical" models and keeping those that do not contradict observations. But it would be misleading, I think, to think of "science" and "metaphysics" as some completely unrelated intellectual pursuits, because at least the practice of science certainly involves metaphysics to a significant extent. To summarize so far: 1. When it comes to prediction of observed phenomena in the manifest world, any metaphysical theory has to yield to science (and indeed whatever can be predicted accurately immediately becomes part of science). 2. When it comes to explanations of either the manifest or some kind of other "ultimate" reality, one can't easily contrast metaphysical vs. "scientific" explanation, because any such explanation can be viewed as fundamentally metaphysical -- as soon as it involves something that's not directly observable. So, at best, we can perhaps ask whether a particular metaphysical theory A does (or should) accept another metaphysical theory B that is prevalent in the scientific community. And I can't think of any generic answer here. It may, but it doesn't have to, and often doesn't. Which is not particularly surprising, for even within the scientific community, at various times different groups of scientists also prefer different (explanatory) theories. IMO, it comes down to the criteria by which to judge the goodness of an explanatory (metaphysical) theory. Scientific theories tend to be more economical (in the use of unobservabe entities), designed to be falsifiable in some way, reuse existing terms from accepted theories, etc. "General" metaphysical theories tend to focus on a certain kind of simplicity, or aesthetic beauty, or ethical considerations, or emotional needs, etc. Whatever one thinks are the most important criteria (of explanation) will guide one's choice of theory. And then one will tend to view other theories, including scientific ones, also in terms of how well they satisfy one's preferred criteria. For example, if your preferred criteria is to minimize the number of unobservable entities in a theory, you might choose/build a metaphysical theory that mimics existing scientific theories, re-uses some of the same terms (e.g. "quantum entanglement"), etc. Or, you might minimize the unobservables by choosing a theory where "everything" is explainable in terms of a single unobservable Being. In both cases, you'd likely prefer to fly on a plane built using "pure" scientific facts (i.e. wings generate lift, etc); but your explanations might range from a (scientific) "this is simply how the universe works" to a (theistic) "this is how Being designed this universe to work". 

By choosing the orientation the humans think the cube may have, we can say that we generated a random sequence of 0s and 1s. But is it truly so ? If we do this test to thousands or more, we may find that a selection is way more preferred compared to the other. That means some other pre-determined factor (like spatial orientation capabilities, geometry knowledge, the distance between the eyes and object) influences the choice. So in order to actually have the random result we must consider all these factors when making the test. So basically, if we are able to account for everything involved, we may find a trick to generate something truly random. Otherwise...slim chances. 

The overall issue where these terms play a central role is that of the nature of the observable world -- what are things made of and how do they come into being. One of the principal questions that Aristotle addressed was whether the process of "generation" of new objects (accompanied by destruction/"corruption" of old objects) is something different from the process of "alteration", in which old objects undergo change to become new objects. Aristotle identified one group of Greek philosophers (those who thought that there was a single fundamental material "principle" of all things, e.g. Thales::water or Anaximenes::air) as compatible with the belief that "alteration" is the only possible way for entities to come into being, since (following the single-principle logic) all objects must be the different "alterations" of the same primary substance. He identified another group (those who thought that there were multiple fundamental material "principles/elements", e.g. Empedocles::4 elements or Democritus::atoms) as compatible with the belief that "generation" is the primary mechanism of coming into being (and is different from "alteration"), since (following the multiple-principle logic) objects must be compositions of the multiple material principles/elements (that do not themselves change). Aristotle based his own account of how objects ("substances") come-to-be on his conception of substances as "having" matter and form (which are two of his Four Causes). 

The above is a 4D object represented on 2D. Quite the loss of visual information here. A 3D holographic projection could offer way more clues as to how a hypercube may look like. In that case. you would a 4D object represented on 3D. Much more visual info to get from that. But what humans lack in this case is the initial image of how the 4D hypercube looks in 4D. Due to that, there is no end point for the extrapolation to work, so in most cases humans will not be able to extrapolate at all a 4D object. 

A substance can come into being as the result of matter combining with form -- this is "substantial change" or "generation of a substance" (e.g. the matter "bronze" combining with form to become the new substance "bronze statue"). A substance can also come into being as the result of another substance changing its form -- this is "accidental change" or "alteration of a substance" (e.g. the substance "unmusical man" changes to become the substance "musical man"). 

My question is, what exactly is Kenny saying about Plato's and Aristotle's epistemological views? I mean, it seems that he's talking about contigent vs. necessary truth, and that Plato & Aristotle thought of all truth as necessary -- but I don't understand what that really means in terms of their theories. Is it being claimed that if Aristotle saw a blue unicorn, he'd assert that all unicorns are necessarily blue? I.e. that he would not be able to conceive of (the truth of) the color as being contingent on the particular unicorn? Or is it being claimed that he'd consider the blue-ness of a specific unicorn to be something less than truth, precisely because this blue-ness is not necessary of other unicorns? Or something else? Kenny closes by making a strong claim about the "impossible ideal of Aristotelean science". I think here he ascribes to Aristotle the theory that science should be a deductive (via syllogisms) process, producing only "necessary" truths. But is calling it an "impossible ideal" really justified, especially in the sense of Aristotle committing some kind of clear fallacy..? After all, didn't the logical positivists take up exactly this line of thinking about truth, and haven't they come up with some interesting/useful/influential arguments about what science is, even if other approaches have become more popular..? 

University research is not a guarantee for accuracy of theory. Any theory can get to be widely accepted and then something else comes in that invalidates the initial theory. It's how science evolves. Given this and how today's science and specially physics tends to work, I'd rather trust an independent research of a small group of scientists than that of an university's research. The logic is most of the time university projects are bound to comply by the very restrictive already established theories (there are exceptions, but they do not form a majority) while independent research can practically dig in any direction unrestricted. So far this approach worked very well for my research teams. 

For the sake of limiting the scope of discussion, my question is about the first fallacy. Here's how Kenny describes it: 

I am reading "Ancient Philosophy" by Anthony Kenny (Vol 1 of his "A New History of Western Philosophy", OUP, ISBN 0–19–875273–3). I was intrigued by the following statement (p.176): 

I think it's a big assumption, but not without some merit. Firstly, we don't know if there are truly random events at all. To say that an event is random means that we don't have sufficient information (and method of calculation) to determine the exact outcome. But we cannot be absolutely certain that such information doesn't exist (e.g. some yet-to-be-discovered hidden variables) or may not eventually become available to us. So, one can at least reasonably entertain the possibility that there are no truly random events at all -- and in this case, there wouldn't be any random processes in the brain (and decision-making, etc), either. Secondly, to quote George Musser (in reference to Butterfield, Dennett and List), "human cognition involves different structures than atomic physics and is governed by different laws, so determinism at micro level need not imply determinism at the agential level." As an analogy, if you consider a process like gas expansion, the behavior of individual particles may be random, but certain "important" aspects of the "overall" behavior of the system are quite deterministic. So, in this case as well, it may be possible to view the brain as a deterministic mechanism (perhaps to some extremely high degree of accuracy) on the level of decision-making, even if you allow random events on a small scale. Now, as to your last question, if we were to grant that the decision-making mechanism is "truly" random, then it's very difficult to reconcile it with (at-least) a common-sense meaning of free-will. In this (common) sense, free-will implies "control" over the choice being made. However, if the decision-making mechanism is random, then to talk of "control" is (nearly) as meaningless as in the case when your decisions are determined by material/physical factors. So, at the very least you'd have to redefine the meaning/scope of "free will". Of course, people are certainly trying -- and it's worth reading about (here's one overview: $URL$ 

Here's a example: We do an experiment and get valid results. We say X causes A to happen. We do another experiment and get valid results. We say Y causes A to happen. In both cases, we have math and observations backing up our claims. But X and Y are self-exclusive (a xor if you will). So what can we do in this case ? We can do experiments of another nature and see what was actually correct. But if that's not possible, we should select the cause that actually can explain more. I encountered this in physics many times, where theories were validated both by observation and math and in the math part we had a constant "c" and a variable "v" because in this manner the observations could be explained. But re-thinking everything, one could see that in the math formulas, the same valid result is obtained if "c" varies and "v" is declared a constant. Both cannot be variable or constant in the same time because it would invalidate the math part supporting the theory. So we got 2 options leading to the same result, confirming the same theory but we do not know which is a variable and which is a constant and we have no observable/experimental way to determine this. What did in such a case was choosing the option that can explain more. In the current example, let's say if we had a constant "c" and a variable "v" we can perfectly explain how a car engine works, but cannot determine anything related to the car wheels. If "c" varies and "v" is constant and we explain how the engine works just as in the 1st theory but we can also explain how the wheels work, we select the second option as the valid one. 

It's possible for a flawed ("non-special") agent to arrive at objective ("special") facts/truths. For example, Newton held all kinds of occult beliefs (of which at least some are probably false!), but this did not prevent him from discovering objective facts about the observable universe, e.g. Newton's laws of motion. So, IMO, there's no contradiction between the claim that our reasoning ability may be in some sense flawed (e.g. limited), and the claim that our reasoning ability can (still) help us discover some important facts. 

It's not clear whether Aristotle's own account is entirely coherent, but that seems to be the basic idea.