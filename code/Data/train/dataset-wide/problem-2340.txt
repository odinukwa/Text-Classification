Transactions do of course run concurrently and simultaneously. The question is whether or not simultaneous commits can occur, which is what @gbn is addressing in his answer. @gbn is correct that two simultaneous commits cannot occur if you consider the transaction to be committed at the point the WAL entry is hardened to disk. From a crash recovery perspective, the transaction is definitely committed at this point. In the event of a failure occurring, this transaction would be rolled forward (redo), not rolled back (undo). As two writes to disk cannot occur at the same time, we could argue that two simultaneous commits cannot occur. However, you could also argue that a transaction is committed at the point the RDBMS is notified by the IO subsystem that the WAL write has completed successfully AND has released any locks it was holding for the duration of the transaction. Therefore, in a multi-cpu environment simultaneous commits can and will occur. The mechanics of how this can occur for SQL Server is explained in Performance Tuning Waits and Queues, section titled 'Execution Model (simplified)'. Again for the SQL Server folk, some background reading on the WRITELOG wait type is useful to understand the concepts. 

If you're having trouble getting the client to connect via an alias, I'd suggest detailing the error message in a separate question. 

will suffice, there is no need for to achieve what you've described so far. If the transaction you mention later updates or inserts to the table then , if it's just to read for the it's unnecessary and other processes will at least be able to read from the table. Whether you use the isolation level or hint depends on what proportion of the table you will be scanning to find the aggregate. If you need to touch any significant percentage of the rows is logical. A small percentage and a desire to maintain some degree of concurrency then try . would also eliminate the deadlock risk. Both options would achieve your aim of preventing an inconsistent result from the aggregate. 

Couple of additions to Rolando's suggestion. If you're clearing out the old table, as well as populating new you could use the OUTPUT clause. Be mindful of the potential for log growth, consider a loop/batch approach if this may be a problem. 

Only recently waved goodbye to the last SQL2K server I looked after, so have a few scripts in the toolbox still. You could also root around in the SSC scripts archive. Here's an old index maintenance script by Kimberly Tripp: 

The short answer... This has the hallmarks of solving the wrong problem with the wrong solution. It sounds like you're looking for a workaround to a fundamental design flaw, introducing an entirely new set of problems on the way. An attempt at a longer answer... It's almost impossible to offer any practical advice without a better understanding of the database, the application and interaction between the two. That said, there are one or two warning signs. Logic in a table function It sounds like you have a hierarchy of entities, which could be represented in a relational model and navigated in a set oriented fashion. If complicated logic is required in the database to navigate the hierarchy, the model is probably wrong. Do you have a relational database or an object store/bit bucket? Your question refers to entities, rather than tables and records. Did you build an object model and dump it to tables, rather than map your objects to a relational model? How much data are you returning to the user? A 2 second stored procedure call could be sub-optimal, accessing a sub-optimal model or returning too much data. Requiring 10 calls to this procedure suggests it could well be all 3. Navigating a 40k record graph is not big data territory, unless you're application is loading 40k records from the database to determine what's navigable. You already have a better model Your workaround may actually be the solution, if applied differently. If the output from your table function is a model which works, perhaps it should be a permanent part of the model. Instead of building this temporary table every time a user logs in, why not keep the data in that form? 

Indexing strategy tends to evolve as usage patterns emerge. That said, there are also strategies and design guidelines that can be applied up front. 

SQL Server has Automatic Page Repair During a Database Mirroring Session in 2008+. Is this the sort of thing you're looking for? 

This could be an explanation for the issue @DavidHayes experienced. More importantly it might be a useful signpost for future visitors, hence the edit. Original answer: My first thoughts were as per Aaron's answer but after scratching my head a while and digging through SSC, I found a couple of very similar threads: Reclaiming deleted but unused LOB space How to free table space OP of the second thread posted the fix (as given to him by MS Support) as: 

You are confusing various concepts here, the major error being primary key != clustered key. Because of this misunderstanding, the majority of the guidance is incorrect. To be brutally honest, you are probably not well placed to be writing these guidelines. 

You may be experiencing an issue with ascending keys, which is leading to a poor execution plan choice. 

Yes. The buffer and plan cache associated with the database before restore are not related to the database post-restore, so they will be cleared. To all intents and purposes the restored database is a completely separate, unrelated entity. There is nothing in cache that can be reused. 

Without context, this is a poor question. Bound to a single machine your requirement is a function of IO performance, not platform. An Access mdb file on a FusionIO card could outperform Trinity on a 5400rpm drive in a narrow band of tests. You'll have to be more specific if you want answers of any value. Edit: following comment. Context would be a description of what you're building. As I indicated, whichever k-v system you choose you will be IO bound when constrained to a single machine. On EC2 block storage the choice of k-v becomes even more irrelevant. If you're building on EC2 look at the native products they already provide e.g. SimpleDB or Elasticache. 

If you are can't build every version of the database from source and upgrade to to any future version, your source control is broken. If you don't think so, ask Eric Sink. 

All of the platforms you have mentioned can run close to zero data loss configurations. All of them could be deployed in a configuration that will fail. Platform choice is one part of the puzzle. It will be your implementation of the platform that determines whether or not your requirements are met. 

From your comments, it sounds like this is early days for your software and company. So, Iâ€™d be looking for ways to maximise flexibility and minimise capital expenditure. 

It is possible for two separate servers to simultaneously access a shared read-only database. The feature is known as Scalable Shared Databases. From your description, it sounds more like you need a failover cluster, not a shared read-only copy. 

A transaction is started for each statement that occurs outside of an explicit transaction block. Whether a commit is automatically issued following the statement is dependent on the RDBMS configuration. MySQL has the autocommit option, SQL Server has IMPLICIT_TRANSACTIONS, PostgreSQL is always auto commit. PostgreSQL: 

Previously, we've had issues related to the relative path that's used to reference the master.dacpac (as described here). If I remember correctly, we pulled a copy of the master.dacpac file into the local project and referenced it using the last option shown in the dialog above. This is not ideal however as you'll need to remember to update your copy of the reference periodically. Will update if I can find the details of what was causing a problem with the referencing method. 

99% of the time, the asp.net session state database does not warrant FULL recovery and transaction log backups. In fact, it usually doesn't warrant any backups at all. I'd favour a script to recreate it over taking backups. NB: Be wary of folk sneaking persistent objects in to your state database. Lock 'em out. If your SQL Server isn't clustered, you have the option of targeting for state storage as an alternative to a persistent database, so it's treated as truly disposable. Assuming your usage is typical and recovery of the data isn't required either: 

Sounds like your looking for query notifications. Exactly as you describe, this feature allows you to subscribe to notifications which are generated when the results of a query change. Typically more efficient than repeatedly polling the database for changes. Under the hood, notifications rely on the mechanisms used to maintain indexed views. Because of this, notification queries are constrained by the same restrictions as apply to indexed views. It's a lengthy set of requirements that you should review in detail to determine if it would make this a non-starter for your scenario. 

For SQL Server, you could argue that a commit operation is nothing more than writing LOP_COMMIT_XACT to the log file and releasing locks, which is of course going to be faster than the ROLLBACK of every action your transaction performed since BEGIN TRAN. If you are considering every action of a transaction, not just the commit, I'd still argue your statement is not true. Excluding external factors, speed of log disk compared to data disk speed for example, it's likely the rollback of any work done by a transaction will be faster than doing the work in the first place. A rollback is reading a sequential file of changes and applying them to in-memory data pages. The original "work" had to generate an execution plan, acquire pages, join rows etc. Edit: The it depends bit... @JackDouglas pointed to this article which describes one of the situations where rollback can take significantly longer than the original operation. The example being a 14 hour transaction, inevitably using parallelism, that takes 48+ hours to rollback because rollback is mostly single threaded. You would most likely also be churning the buffer pool repeatedly, so no longer are you reversing changes to in-memory pages. So, a revised version of my earlier answer. How much slower is rollback? All other things considered, for a typical OLTP transaction it isn't. Outside the bounds of typical, it can take longer to "undo" than "do" but (is this a potential tongue twister?) why will depend on how the "do" was done. Edit2: Following on from discussion in the comments, here is a very contrived example to demonstrate that the work being done is the major factor in determining the relative expense of commit vs rollback as operations. Create two tables and pack them inefficiently (wasted space per page): 

I wouldn't delete it, just in case you run into problems. If space isn't an issue create a copy of the original files with the instance offline, rename the original (so they aren't found on restart) then try: 

, as with all TSQL commands, is executed synchronously. Each statement in a SQL batch will complete in turn before executing the next, whether its executed standalone or via as in your example. Backup using SMO is different and unrelated to the TSQL command. With SMO you have the option of executing a synchronous backup with Backup.SqlBackup or an asynchronous backup with Backup.SqlBackupAsync. 

I'm going to skip past your questions and try to offer broader guidelines/advice instead. The definitive/canonical guide to dynamic SQL, the situations where it is applicable and where it can be avoided, is Erland Sommarskog's Dynamic Search Conditions in T-SQL. Read it, re-read, run through Erland's examples, make sure you understand the reasoning behind the recommendations. You're dealing with a fairly common scenario and the approach you've taken is not unusual. A couple of points worth highlighting: 

Assuming you have a maintenance window that allows for a short period of downtime I would suggest using BCP to dump the table to a file. If space is an issue, compress the target folder in advance of the export. 

sp_executesql supports parameterisation, whereas EXEC only accepts a string. Only performance differences that may arise are due to the parameterisation i.e. a parameterised sp_executesql call is more likely to have a reusable cached plan. An EXEC call is likely to lead to lots of single use space wasters in the plan cache. 

A 2008 and 2008R2 user database is schema compatible, hence one version for both in SSDT. In other words, there aren't any schema objects that you could add to a 2008R2 SSDT model that couldn't be created in 2008. This isn't the same as either database version or database compatibility level. Database version: 

I'm 99% sure that setting a file to offline is a one-way operation. The only route to bring them back online is to restore the data from a full backup and then apply log backups. There may be an obscure hack route to get this working but I'd be inclined to rebuild the database. Create a new database, transfer the schema you want to keep, copy the data across. 

Remember that a clustered index is the table. The only overhead that exists for a clustered index over and above that for a heap (which is generally undesirable) is for the non-leaf index pages and the cluster key's inclusion in all non-clustered indexes for that table. This is why narrow cluster key's are preferred. Kimberley Tripp's articles on clustered key choices are an excellent reference for this. 

While they are not great examples of best practice, the SQL Server sample databases would be good place to start. They include an OLTP, data warehouse and analysis services databases for a fictional organisation. Studying the differences between them should help you make sense of how OLTP (transaction) and OLAP (analytical/BI) databases differ and why. $URL$ 

If you aren't using any SQL2008+ only features (e.g. compression) you will be able to script out the objects you require and execute in SQL2005. If you're not sure if any SQL2008+ features are in use, trial and error (script and execute, see what happens) will soon tell you where the problems are. This is very different to restoring a backup from 2008 to 2005, which is not possible.