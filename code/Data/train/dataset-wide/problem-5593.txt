But one needn't necessarily go to such lengths: just say that existence is a property of your model objects (regarding how they correspond with reality) and don't make such a claim about actual objects, and you're pretty much free of problems. 

The issue with the chair is that the semantics of the question imply that an object possesses a quality that it does not have. Let's consider a disjunction: 

The core of the scientific method is to have an observer, something to observe, a mechanism for generating and evaluating predictions about future observations, and a way to either select similar future observations or generate new similar events to observe. You then observe, model, test, observe, model, test, etc., as a way of improving your predictor's performance. The constraints are therefore extraordinarily weak, though not exactly the ones you describe. Conditions need to be temporally stable enough so that you can run your observe-model-test loop many times before the rules completely change (slow drift in rules is okay, if you know to expect it); outcomes need to be sufficiently reproducible so that there is something to predict (but broad distributions are okay). Both randomness and extreme complexity can frustrate reproducibility; the more sophisticated of an observer/modeler you are, the more complexity you'll be able to tackle. Vastly stronger than the constraints on the rest of the world are the constraints on the observer and modeler (possibly the same entity, though there is no reason it needs to be; the observer can use a modeler-oracle). Between the pair of them they need to be able to translate events into a representation of those events, detect which differences are purely stochastic and which are regular, and devise some sort of compact representation of such things that can be used to make future predictions. This is an immense amount of computational work, and it seems unlikely that in a badly chaotic time-varying universe that such entities could exist. So the answer is probably very close to: if you exist and have adequate capacities to attempt to follow the scientific method, you can probably use it to find out at least some things. 

ChristopherE has already given what I would consider the correct answer, but let me give a particular example of science answering a why-question: why do birds have wings? Because, given the fluid dynamics of the atmosphere, wings allow birds to fly; and with our present environment, flying gives a significant fitness advantage (albeit with sizable costs also). You can get very far with answering why-questions in this perfectly scientific manner. There are also why questions that are fundamentally metaphysical in nature ("why is there randomness"), and questions that can be taken in either a metaphysical or non-metaphysical sense ("why am I me and not Adam"). 

This is how likely it is that an expert will say a thesis is true if it is in fact true. Assuming we limit ourselves to considering experts who have an opinion on a thesis, this could be pretty high--let's say it's 99%. 

The key phrase is exactly like is in all physical respects. Physicalism states that if X and Y are physically identical, X and Y are identical. It doesn't matter for the argument that it happens to be zombies or has something to do with consciousness; it just states: imagine that X and Y are physically identical but X and Y are not identical. Of course we have just imagined non-physicalism! The interest, if there is any, in this form of argumentation is that there is no clear causal mechanism by which physical configuration is known to generate consciousness. We also don't have a clear causal mechanism by which people laugh at the antics of Bart Simpson, but for some reason Bart-Simpson-grumps have not made it into philosophical discourse. 

Is it critical, logical, and imaginative? Sure. This is nowhere more evident than when comparing the ability of a baby to recognize and respond to faces and the ability of a computer programmed to attempt to do the same things. The machine learning side of the problem shows how ferociously difficult it is, and the baby shows that we humans do it innately and with considerable talent. Synthesizing "red" isn't such a great example because it's so simple (and so low-level: you literally need your eyes to produce different proteins to be able to effectively distinguish red and green, so it's more automatic than many other syntheses). Is it conscious? Sometimes. One can show this even with colors; although much of western Europe divides things into "yellow", "green", "blue", "purple", in Russia there is a pale blueish color (transliterated "goluboy", голубо́й) that is considered part of the normal spectrum between green and (a darker) blue. It is not something exotic like "azure" or "teal". This difference in synthesis of visual inputs is entirely a product of culture, so it stands to reason that similar distinctions are within reach of intention. (This does not mean that everything is within reach of intent, only that some things are.) 

A philosopher should really, really study cognitive science / neuroscience. There are all sorts of insights we have now about how perception works, how we learn, and so on. For example, after studying neuroscience, one realizes that introspection is an absolutely horrible guide to how perception actually works. The system is designed to present a seamless view of the world without alerting us to its inbuilt flaws and nature. This is very helpful when you're trying to dodge a lion long enough for the rest of your tribe to arrive and save you, but not so helpful when you're trying to understand the nature of perception sans neuroscience. 

To answer the question directly: yes, it should be completely disregarded except as an intuitive historical artifact. We have a much better understanding of what things are composed of now, and those details are within even a child's capacity to understand (e.g. phases of matter, "metal", "plastic", "glass", etc.). However, the elements are fun in stories. 

This is more properly a question for Biology.SE, but in brief, the genome of a species is substantially shaped by its environment and the environment of its ancestors. So there is certainly information about the environment there (in the mathematical sense). Indeed, genetic algorithms (an optimization technique used in computer science) work precisely because evolution, or a generalization thereof, stores information about the environment. However, knowledge is usually considered to be some sort of justified true belief. Although the information in DNA is in some sense justified on average in that the creature containing that DNA existed, this isn't what is meant in the philosophical context. So information: yes. Knowledge: no. (At least using those terms as understood by philosophers.) 

It's perfectly appropriate to use reasoning like if the train is so fast (80 mph) then it should be here by now if in fact it is fast enough so that it should be here by now. You do the appropriate calculation or comparison and find out. Or, for a probabilistic version: 

Philosophy is a nontrivial endeavor, and honestly, saying you're a "self-taught philosopher" would strike me, and probably most others, as about as reassuring as if you were a "self-taught aerospace engineer". Yes, it's possible. But it's much easier to make the claim than to have actually done it. Your motivations for introducing yourself as such--that you want to prove something--would only reinforce the urge to ask: if you are so interested in philosophy, why did you not bother to get a degree? So I suspect that while nobody will arrest you, it won't produce the results you are seeking. If you want to alter the present-day term "philosopher" to mean "anyone devoted to the love of wisdom", well, good luck with that. If you want people to take you seriously on matters of philosophy, show, don't tell. If academic philosophers find that you have something interesting to say on some nontrivial topic, that's a much better indication that your self-teaching actually resulted in you being a philosopher rather than someone who merely likes to call themselves one. 

You're overthinking this. He wants to hit the bottle. He does some stuff. He hits the bottle. Yes, it's intentional. That's what intention means. We don't care that the "some stuff" was aiming at a different bottle, or singing the Star Spangled Banner, or using a carefully-calibrated laser sight, or praying to the Flying Spaghetti Monster, or anything else. That's an implementation detail; the important bit is that he knows something that works to achieve his aims, and does what he needs to to achieve his aims. 

he posits a dichotomy between aesthetic art as an object to be experienced by a subject and some deeper "presencing" connection, yet he doesn't have particularly compelling evidence that this distinction even exists, and it's less than clear that "great art" is at an end or in trouble anyway even in the aesthetic tradition. Perhaps the medium changes, as people do get tired of, say, oil on canvas, and instead build spectacular buildings or moving memorials or documentaries or movies that cause you reflect deeply. 

It poses a challenge to overly-specific notions of ontology, but there aren't many around that are that specific. Relatively general ontological frameworks like process philosophy (e.g. as promoted by Whitehead) have a sufficiently general notion of what is "actual" that a phenomenon that has both wave-like and particle-like properties (depending on context) is not a serious challenge. But even substance metaphysics has to deal with stuff like water and sand and sound and light, and so it's usually phrased in a general enough way that it doesn't have at its core the idea of involately particulate matter. Some specific arguments may fall flat, but overall it is so general that ideas e.g. of substance being a combination of form and matter survive with only minor tweaks when one considers that the matter has characteristics best described by quantum mechanics. In particular, with wave vs particle duality, you just say that the substance is what it is, and you will notice different aspects of its behavior in different conditions. And, thus, all you have to do to rescue most arguments is to insert an "in the classical regime" and everything goes through as before, albeit with a little less satisfaction about having gotten to the true nature of things. So, in general: no, not very much. In any particular case: you need to check the details. 

The argument is (presumably) flawed because there are counterexamples. (1) If it is a psychological claim (i.e. specific to the details of thinking as done by humans), I do most of my thinking without explicit symbology or an internal narrative. This includes evaluation of my own abilities and "thinking". You can't easily examine my thoughts, but I bet most readers have had the very clear sense, unverbalized but equivalent to "this will be easy" or "this will be too hard for me", when it comes to some cognitive task. That is a type of metathinking. (2) If it is a computational claim, practically any recurrent network can do the job. You can build an associative network between sets of symbols xi and yj (which is "thinking") and have one of the y's trigger an estimate of the time of computation of the associative network which then feeds back into one of the x's on the next pass ("thinking about thinking"). There is some wiggle room to make the claim true if one defines "meta-" narrowly enough so that only expression of linguistic symbols counts as "meta" (or as "thinking"). But the claim is then basically a tautology. 

John McLaughlin used to host a talk show on PBS called the McLaughlin group, with two conservative and two liberal panelists; McLaughlin was conservative. When one of the liberal panelists (especially Mort Kondracke) made an insightful point, McLaughlin would say that the panelist had "stumbled uncontrollably into the truth". There is a certain amount of historical wisdom that is only accidentally right--the proponents did stumble into the truth, despite their justification being wholly wrong. Whatever label you give to it ("knowledge" or something else), the key is that if the rightness of their justification had really mattered at the time, people would have been doing things that were wrong, which demonstrates that it wasn't a good model of reality. If it didn't matter at the time, pragmatically, they had functional knowledge. When it comes to the relationship between the Sun and Earth, however, the ancient Greeks did have some compelling evidence that the Sun was more likely to be at the center of the system. And if some of the details of Aristarchus' calculations were wrong, the logic did not require them to be correct to that degree of accuracy, just like many proofs will go through just fine if a function is merely continuous instead of smooth. The rationale may have made stronger assumptions than necessary, or two conceptual structures may be largely isomorphic such that even if the wrong one was used, you'll get the right answer in many cases. And that, if you care to employ it (at least conceptually) is a reasonable method for assessing degree of functionality of "knowledge" (or whatever you call it). So you have to be careful when thinking about historical knowledge; people may have had good enough reasons to believe what they believed for you to want to credit those beliefs with some positive tag like "knowledge". But in many ways it's simply the wrong question. Reality does not seem to be structured such that we can cleanly separate things into "knowledge" and "not". This is, to my mind, much of the point of Popper's ideas of falsification and corroboration: you don't, in the end, have "knowledge", only beliefs with varying degrees of corroboration and problems. I don't think Kuhn meaningfully expanded upon that basic point even if he viewed the process as rather different (at once more mundane, between revolutions, and more radical, during them). 

Let's try an example to see what these things mean. Let's let be our thesis, and is "an expert says is true". is our estimate of how likely our thesis is presuming that an expert says it's true. Let's evaluate each of the terms on the right. 

Most contemporary thought seems to be that the two are compatible. Personally, I have not managed to come up with a wholly satisfying account of this without constraining either science or theism to something unrecognizable by the overwhelming majority of its followers. In any case, it is certainly not a cut and dried issue. One recent popular work--not written by a philosopher--that takes a rather pessimistic but fairly carefully reasoned view of the prospects for compatibility is Victor Stenger's "God and the Folly of Faith". In contrast, both Alvin Plantinga and Daniel Dennett--despite disagreeing over particular theistic beliefs--seem happy to accept that the two can be compatible. (Each have recent works at least vaguely on the topic: Plantinga's brand new "Where the Conflict Really Lies: Science, Religion, and Naturalism" and Dennett's "Breaking the Spell: Religion as a Natural Phenomenon" from about five years ago.) Anyway, I don't think there is any reason that science and theism are necessarily opposed; if theism makes any meaningful claims about the world at all, then the scientific method can be used to evaluate those claims. If a deity created the world according to any sort of regularity at all, the scientific method could be used to probe those regularities. The issues are really over what the scientific evidence is: is any room left for theism?; and over whether theistic thinking corrupts our ability to do scientific thinking and thus robs us of our ability to understand. In essence, the inevitable-conflict argument from the scientific side goes something like: if you look at how much we really know now, the gaps left for God to fit in are too small to contain anything like a satisfying deity (or one that provides useful explanatory power). Effectively, theism is a disproved hypothesis. From the religious side, it seems to go something like: God is primary, while science is a human construct; if the two collide, and they can and have, it is human foolishness and limitations that are and will continue to be the source of error. There's a concept of non-overlapping magesteria that you should be aware of. As far as I can tell, it is intellectually bankrupt, in that theism does make meaningful statements about the physical world (and science can make meaningful statements about feelings of spirituality and such), but it has been a fairly popular view that seeks to harmonize theism and science. 

The intuitive version (that seems justified to me) is a statement of pragmatism, not truth: if and explain things equally well, and is simpler, why would I bother with the extra headache of ? I think there's a truthier version that is entangled with Kolmogorov complexity and dynamic semantics in deep ways. I've never seen anything approaching a proof of this, but the intuition is that although it is easy to write a true statement, writing a true statement that conveys a lot of information (in the Kolmogorov sense) is very difficult. Almost everything you try to say will either have very little content ("My name is not Joe") or will be wrong ("My name is Fred") or will not take advantage of context and thus be impossibly bulky ("My name is Matt, where by 'name' I mean that verbal utterance and corresponding written string of symbols by which I am commonly referred, in contrast to the legal name on my birth certificate..."). Occam's Razor is then a statement of three things: first, how special it is to find a compact description of anything; second, that we have to a large extent organized language to match causally-separable or independent processes; and third, that we observe that very often there is a single proximal causal process rather than an indecipherable muddle that gives rise to recognizable patterns. With these three together, you have reasonable hope that if you find one of these rare compact but effective descriptions, you're really onto something. Even so it's just a rule of thumb, but I think it's a deeply and subtly true rule of thumb.