A great example that I have in mind is the concentration phenomena in high dimensions. Consider the simplest multivariate normal distribution $X\sim N_d(0_d,I_d)$, we can compute its $L^2$ norm $\sum_iX^2_i=:\|X\|^2\sim\chi^2(d)$. and $X_i^2\sim \chi^2(1)$ independently. With central limit theorem applied on each component, we have that $$\frac{1}{d}\sum^d_{i=1}X^2_i=\frac{1}{d}\|X\|^2\overset{P}{\rightarrow}N_1(1,\frac{2}{d})$$ as $d\rightarrow\infty$. Use the delta method we can see that $\|X\|\overset{P}{\rightarrow}\sqrt{d}N_1(1,\frac{1}{d})=N_1(\sqrt{d},1)$ and therefore we can actually assert that as dimension $d=dim\mathcal{X}\rightarrow \infty$ the random vectors are concentrated around a sphere. Even more surprising is that if we have another independent $Y\sim N_d(0_d,I_d)$, then we can compute the distribution of $\frac{X\cdot Y}{\|X\|\|Y\|}$ as $d\rightarrow \infty$ is $N_1(0,d)$(multidimensional CLT and delta method) and the distribution of $\|X-Y\|$ as $d\rightarrow \infty$ is $N_1(0,2d)$. These two results claimed that as $d\rightarrow \infty$ two random vectors are most likely to be orthogonal and evenly distributed on the sphere, which is not expected when $d=1,2$. I really hope to know if there are more such examples with a motivation from consideration of the difference between the geometry of high and low dimensional spaces. 

Update in response to update3. Why is the maximal uniform spacing so small in magnitude compared to the maximal oscillation in the empirical distribution? This is not something surprising. you can always have very large spacings but small Kolomogorov-Smirnov norm, which measures on the space of $\mathcal{M}(\mathbb{R})$ while the spacing is measuring $\mathbb{R}$. If you look into Luc's argument, you will see his comments on it. For the simplest example, given a set of data $\{0,0.1,0.9\cdots,0.9\text{(n repeated 0.9)},1\}$ and $\{0,0.1\cdots,0.1\text{(n repeated 0.1)},0.9,1\}$, their empirical measures has completely different cdfs but they both have the same maximal spacing of $0.8$. as the number $n$ of repeated observation increase, the KS norm between these two cdfs can be arbitrarily close to 1. Reference [1]Korolyuk, Vladimir S., and Yu V. Borovskich. Theory of U-statistics. Vol. 273. Springer Science & Business Media, 2013. [2] Gupta, S. Das, et al. "Inequalities on the probability content of convex regions for elliptically contoured distributions." Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability (Univ. California, Berkeley, Calif., 1970/1971). Vol. 2.1972. 

[Arcones]Arcones, Miguel A. "A Bernstein-type inequality for U-statistics and U-processes." Statistics & probability letters 22.3 (1995): 239-247. [Lester Mackey et.al]Mackey, Lester, et al. "Matrix concentration inequalities via the method of exchangeable pairs." The Annals of Probability 42.3 (2014): 906-945. $URL$ 

Answer: this does NOT extends to other cases than f-divergence. For the case $u=0$, there are many counter examples since $D_u$ is not separating any densities. So I think it suffices to consider $u\in (0,1]$. Let $U^n=Unif(0,\frac{1}{n})$ the uniform measure on unit interval. And $\mathscr{P}_0$ corresponds to those with odd $n$, $\mathscr{P}_1$ contains exactly those with even $n$. Let $Q_0=U^n,Q_1=U^m$, thus $D_u(Q_0,Q_1)< \infty$ if $n<m$; and $=\infty$ otherwise due to the fact that $$\frac{q_{1}}{q_{0}}=\frac{m\boldsymbol{1}_{[0,\frac{1}{m}]}(\omega)}{n\boldsymbol{1}_{[0,\frac{1}{n}]}(\omega)}=\begin{cases} \frac{m}{n}>1 & \omega\in[0,\frac{1}{m}]\\ 0 & \omega\in[\frac{1}{m},\frac{1}{n}]\\ \infty & \omega\in[\frac{1}{n},1] \end{cases},n<m$$.(It suffices to consider the integration over the last segment w.r.t. $Q_0$'s density $q_0$.) Therefore $D_u(Q_0,Q_1)\geq D_u(P_0,P_1)\quad \forall (P_0,P_1)\in \mathscr P _0\times \mathscr P _1,\forall u\in[0,1]$ as long as $Q_0=U^n,Q_1=U^m$ satisfying $n\geq m$. Now for any pair $Q_0=U^n,Q_1=U^m\text{ such that },n\geq m$, $$\frac{q_{1}}{q_{0}}=\frac{m\boldsymbol{1}_{[0,\frac{1}{m}]}(\omega)}{n\boldsymbol{1}_{[0,\frac{1}{n}]}(\omega)}=\begin{cases} \frac{m}{n}\leq1 & \omega\in[0,\frac{1}{n}]\\ \infty & \omega\in[\frac{1}{n},1] \end{cases},n\geq m$$ Let $t=1$ now and$ \left\{ \omega\in[0,1]\mid\frac{q_{1}}{q_{0}}>t\right\} =[\frac{1}{n},1]$ $$Q_{1}\left[\frac{q_{1}}{q_{0}}>t\right]=m\cdot\mu\left[[\frac{1}{n},1]\cap[0,\frac{1}{m}]\right]=m\cdot\mu\left[\frac{1}{n},\frac{1}{m}\right]=m[{\frac{1}{m}-\frac{1}{n}}]>0$$ Choose $P_{1}=U^{k}$ for any $k\geq n$ $$P_{1}\left[\frac{q_{1}}{q_{0}}>t\right]=k\cdot\mu\left[[\frac{1}{n},1]\cap[0,\frac{1}{k}]\right]=0$$ 

Individual sampler. We used another random walk/probability generator like truncated version of normal random walk to replace the sampler. The simulation result showed that if we directly use such a sampler then acceptance rate(decision to keep the sample) is usually oddly low. Modify the model. We can modify the model, for example, using the transformation $\eta=log\tau^{2}$(half line to the whole space) to sample for variance parameter in a exponential model and the model likelihood becomes $$\begin{array}{c} \underbrace{(e^{\eta})^{-2-1}\cdot exp\left(\frac{2}{e^{\eta}}\right)\left|\frac{\partial e^{\eta}}{\partial\eta}\right|}\\ \tau^{2}=e^{\eta}\:prior \end{array}$$with Jacobian of the transformation, now the parameter $\eta$ is supported on the whole $\mathbb{R}$ and the metropolis step can be conducted using the usual normal random walk because $\eta\in\mathbb{R}$. Drop the random walk when it goes out of the support of prior. That is to say, when the normal random walk goes out of the support of the prior we simply rejected the proposal value and stay with the old value of the parameter. This is the "trivial" solution you mentioned above. Extended the model. This is a remark from Gelman, Andrew, et al. Bayesian data analysis., we can sometimes extend the problem to allow a larger parametric space if the support if actually too small to put, say, a normal random walk on it. 

(1) supported on half planes of $\mathbb{R}^n$, you may want to look at folded Gaussian distributions. (2) supported on a compact surface like $\mathbb{S}^n$, you may want to look at projected Gaussian distributions. If $\vec{a}$ is fixed you will get a degenerated version of projected Gaussian. 

See pp.87~107 of Prenter, Paddy M. Splines and variational methods. Courier Corporation, 2008. especially p.100 where "uniqueness theorem" is proved and spline is defined as minimizer to $\displaystyle{\int}_{K}[f^{(m)}(u)]^2 du$ among $C^m$ functions over a set $K=[a,b]\subset\mathbb{R}$. The basic technique there is to realize $\int_K (Lf)^2=\int_K (L(f-g))^2 +\int_K (Lg)^2$ where $L=D^m$ is the differentiation operator by integration by parts. 

On pp.13~15 of Fox, L. Parker. Chebyshev polynomials in numerical analysis. No. 519.4 F6. 1968., especially (64)(65), we can see the arguement. As an approach to the minimax solution to the function $\Pi(x)=(x-x_{0})\cdots(x-x_{n})$ with equal weights $w(x)=1$, we can write $$\Pi(x)=\frac{2^{n+1}(n+1)!^{2}}{(2n+2)!}P_{n+1}(x)$$ where $P_{k}(x)$ is a Legendre polynomial of degree $k$. and by the orthogonal transformation provided there we could also write it in terms of weight $w(x)=\frac{1}{\sqrt{(1-x^{2})}}$ and $$\Pi(x)=2^{-n}T_{n+1}(x)$$ where $T_{k}(x)$ is a Chebyshev polynomial of degree $k$. And these two equalities gave identity involving both Legendre and Chebyshev polynomials. 

Since the results you mentioned look not quite precise what they refer to, I regarded it as scheme (2.2.9) in [Nesterov]. The optimization problem has following settings. (i) $f$ admits a minimizer $x^{*}$ on $\mathbb{R}^{n}$ such that $\|x^{*}\|\leq R$. (ii) $f$ is convex on $\mathbb{R}^{n}$. (iii) $f$ is $L$-smooth ($\nabla f$ exists) on the $\ell_{2}$-ball of radius $R$, that is for any $x\in\mathbb{R}^{n}$ such that $\|x\|\leq R$ and any $g\in\partial f(x)$, one has $\|g\|\leq L$. Any algorithm in a gradient scheme follows an update $x_{t+1}=x_{t}-\eta\partial f(x_{t})$, for some step size $\eta>0$, its optimal rate is proven to be $O(\frac{1}{t^{2}})$. The plain gradient algorithm has a rate of $O(\frac{1}{t})$. Nesterov's accelerated gradient algorithm has a rate of $O(\frac{1}{t^{2}})$, i.e. it has an optimal convergence rate within the gradient scheme. To be more precise, if you consider “the worst function in the world” constructed on [Nesterov] p.59, then the following family of $n$ functions $$f_{k}(\boldsymbol{x})=\frac{L}{4}\left\{ \frac{1}{2}\left[x_{1}^{2}+\sum_{i=1}^{k-1}\left(x_{i}-x_{i+1}\right)^{2}+x_{k}^{2}\right]-x_{1}\right\} ,\forall\boldsymbol{x}=\left(x_{1},x_{2},\cdots x_{n}\right)\in\mathbb{R}^{n},0\leq k\leq n$$ and the optimal quadractic bound is actually reached for this family as explained on pp.60-61. The last comment I want to make on this method from a more mathematical perspective is that a 2013 paper of Su-Boyd-Candes [Su et.al] greatly expands the influence of Nesterov's method in stat community. [Nesterov] Nesterov, Yurii. Introductory lectures on convex optimization: A basic course. Vol. 87. Springer Science & Business Media, 2013. (This book is by no means "basic" in American sense...) [Su et.al]Su, Weijie, Stephen Boyd, and Emmanuel Candes. "A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights." Advances in Neural Information Processing Systems. 2014. 

Steve. Consider the following $$\mathbb{E} [ X_j \cdot f(X_1, X_2, \ldots, X_d)-\partial _jf( X_1, \ldots, X_j^*, X_{j+1}, \ldots, X_n) ] = 0$$ We have to assume that $f$ has a pointwise nonvanishing Jacobian in following derivations, which is not the same as nonvanishing Fisher information in the usual regularity conditions. If $X_j \cdot f(X_1, X_2, \ldots, X_d)-\partial _jf( X_1, \ldots, X_j^*, X_{j+1}, \ldots, X_n)$ is a complete family $(X_j,X_j^{*})\mid X_1\cdots X_{j-1},X_{j+1},\cdots X_n$, then the expectation equation reduces to $$X_j \cdot f(X_1, X_2, \ldots, X_d)-\partial _jf( X_1, \ldots, X_j^*, X_{j+1}, \ldots, X_n)=0$$ which is a differential equation in form of $x\cdot y(x)-y'(z(x))=0$ where $y(\bullet )$ is known and $z(x)$ is to be solved. I am not sure such a variational equation has a solution... On the contrary if the above $(X_j,X_j^{*})\mid X_1\cdots X_{j-1},X_{j+1},\cdots X_n$ is not complete for some $j$, then there will be more than one solutions since expectation(integral) equation usually have Fredholm structure. Could you please state your motivation to the problem in OP too? 

In response to the critical comments below I revised my answer. Hope this is more helpful! (1) Two kinds of metrics are defined on generally different spaces. It is not fair to compare these two metric since the Fisher-Rao is defined for probability densities defined on space $(X,\mu)$, the elements of concern are in space $M(X,\mu)$; while the Wasserstein is defined directly on probability measures on $X$ with/without densities, the elements of concern are in space $M(X)$. Even if we step back and say we concern those probability measures with absolutely continuous densities and the manifold they defined, it is readily observed that 

I think a more appropriate model is some sort of Bayesian model using Dirichlet process prior with the number of conversations as a variable; in that way you can use stick-breaking process/beta process with flexible choice of base measures. And this nonparametric Bayesian model is capable of catching the change of break intervals of "switch of topics" better than estimating $\lambda$'s jointly by allocating new conversations to new urns with new base measure. Also, this model will catch the rare events relatively well. Say if you got a long break after you sent a message to a lady; Nested Poisson process will generally rule out the possibility that the conversation is still going on, while the Dirichlet mixture model will still retain a relatively small possibility that the conversation is going on...one of "heavy-tail" phenomena [Hong et.al] that indicates the Poisson model is inappropriate. 

Let $X=(X_{1},\cdots X_{n})\in\mathbb{R}^{n}$ be a random vector with independent components $X_{i}$ such that (i)$EX_{i}=0$ (ii) $\left\Vert X_{i}\right\Vert _{\psi_{2}}= sup_{p\geq1}p^{-\frac{1}{2}}\left[E\left|X\right|^{p}\right]^{\frac{1}{p}}\leq K$ i.e. its components have uniform sub-gaussian norm. Then for arbitrary n\times n constant matrix A and $\forall t\geq 0$ we can assert that $Pr\left\{ \left|X^{t}AX-EX^{t}AX\right|>t\right\} \leq2exp\left(-c\cdot min\left(\frac{t^{2}}{K^{4}\left\Vert A\right\Vert _{HS}^{2}},\frac{t}{K^{2}\left\Vert A\right\Vert }\right)\right)$ for some constant $c>0$. where $\left\Vert A\right\Vert =max_{x\neq0}\frac{\left\Vert Ax\right\Vert _{L^{2}}}{\left\Vert x\right\Vert _{L^{2}}}$ and $\left\Vert A\right\Vert _{HS}=\sqrt{\sum_{i,j}\left|a_{ij}\right|^{2}}$. It is readily verifed that a Bernoulli random vector satisfies (i)(ii) with $K=2$. Therefore we can assert that $$Pr\left\{ \left|X^{t}YX-EX^{t}YX\right|>t\right\} \leq2exp\left(-c\cdot min\left(\frac{t^{2}}{K^{4}\left\Vert Y\right\Vert _{HS}^{2}},\frac{t}{K^{2}\left\Vert Y\right\Vert }\right)\right)$$ where $Y=yy^{t}-zz^{t}$ is symmetric as stated in the OP. 

Disclaimer: Not an expert in analysis/PDE, happen to know tangential results while studying Whitney-type embeddings. 

Rather than measuring the rate of convergence of a sequence, it is more widely accepted that we measure the rate of convergence of a stochastic process that converges to a stationary measure.For a general stochastic process that generates the data $x_1,x_2,\dots x_n$ there is not a uniform rate available since we can always construct a (normal) ergodic process that breaks down the rate. The common measure of convergence rate mentioned in ergodic theory is the so-called rate function $r_k:\mathbb{R}^+\times\mathbb{Z}^+\mapsto\mathbb{R}^+$ of frequencies for a process with joint meausre $\mu$ if $$\mu_n(\{ x_1^n: |\mu_k-p_k(\bullet\mid x_1^n)| \})\leq r_k(\epsilon,n)\text{ for fixed k,}\epsilon>0 \text{ as }n\rightarrow\infty$$ where $\mu_k$ is the finite dimensional distribution (of a sample of size $k$) and $p_k$ is the empirical measure defined by a sample of size $k$. Note that we also require $r_k(\epsilon,n)\rightarrow 0$ as $n\rightarrow \infty$. If the log-rate function $$\frac{1}{n}logr_k(\epsilon,n)>0$$ is bounded from zero then we usually want to choose an optimal value for $\frac{1}{n}logr_k(\epsilon,n)$ which is studies by large deviation theory. But if you want to measure the convergence rate between two filtrations then a more appropriate (yet not equivalent) notion is the Rosenblatt's mixing coefficient. This notion measures the similarity between two $\sigma$-algebras directly, when two algebras are independent their mixing coefficient is zero. This is more like an entrophy costraint on convergence rate as following. The measure of rate in terms of entropy we be specified similarly as $$\mu_n(\{ x_1^n: 2^{-n(h+\epsilon)}\le\mu_n(x_1^n)\le2^{-n(h-\epsilon)}\})\geq 1-r_k(\epsilon,n)\text{ for fixed }\epsilon>0 \text{ as }n\rightarrow\infty$$ where $h$ is the entropy of the joint measure $\mu$. Since a martingale adapted to a filtration sequence can as well be regarded as a stochastic process, its convergence rate in terms of filtration can also be measured by the rate function defined above. [Shiedls]Shields, Paul C. "The ergodic theory of discrete sample paths." Graduate Studies in Mathematics, American Mathematics Society (1996). 

(2)What are the connections between these two metrics? They both somehow characterize the dependence between two distributions using geodesic distance w.r.t. metrics. (2.1)Dependence characterization using metric geodesic distance 

Peter Michor's book is exactly what you need, and some of his recent work involves more discussion about reparameterization. 

Reference [1]Amari, Shun-ichi. "Divergence function, information monotonicity and information geometry." Workshop on Information Theoretic Methods in Science and Engineering (WITMSE). 2009. [2]Marti, Gautier, et al. "Optimal transport vs. Fisher-Rao distance between copulas for clustering multivariate time series." Statistical Signal Processing Workshop (SSP), 2016 IEEE. IEEE, 2016. [3]Barbaresco, Frédéric. "Geometric radar processing based on Fréchet distance: information geometry versus optimal transport theory." Radar Symposium (IRS), 2011 Proceedings International. IEEE, 2011. [4]Newman, Morris. "Periodicity modulo m and divisibility properties of the partition function." Transactions of the American Mathematical Society 97.2 (1960): 225-236. [5]Bulinski, A. V. and Vronski, M. A. (1996). Statistical variant of the central limit theorem for associated random elds, Fundam. Prikl. Mat., 2, 4, pp. 999{1018 (in Russian). [6]Cencov, Nikolai Nikolaevich. Statistical decision rules and optimal inference. No. 53. American Mathematical Soc., 2000. [7]Peter, Adrian, and Anand Rangarajan. "Shape analysis using the Fisher-Rao Riemannian metric: Unifying shape representation and deformation." Biomedical Imaging: Nano to Macro, 2006. 3rd IEEE International Symposium on. IEEE, 2006. [8]Rüschendorf, L., & Rachev, S. T. (1990). A characterization of random variables with minimum L2-distance. Journal of Multivariate Analysis, 32(1), 48-54. 

Any reference will be appreciated. Reference [wiki1]$URL$ [Zoubin] Ghahramani, Zoubin. "An introduction to hidden Markov models and Bayesian networks." International journal of pattern recognition and artificial intelligence 15.01 (2001): 9-42. $URL$ [Geiger et.al]Geiger, Dan, David Heckerman, and Christopher Meek. "Asymptotic model selection for directed networks with hidden variables." Learning in graphical models. Springer Netherlands, 1998. 461-477. [Rusakov&Geiger]Rusakov, Dmitry, and Dan Geiger. "Asymptotic model selection for naive Bayesian networks." Journal of Machine Learning Research 6.Jan (2005): 1-35. [Bickel]Bickel, Peter J., Ya’acov Ritov, and Tobias Ryden. "Asymptotic normality of the maximum-likelihood estimator for general hidden Markov models." The Annals of Statistics 26.4 (1998): 1614-1635. [de Gunst&Shcherbakova]de Gunst, M. CM, and O. Shcherbakova. "Asymptotic behavior of Bayes estimators for hidden Markov models with application to ion channels." Mathematical Methods of Statistics 17.4 (2008): 342-356. 

The most recent and astonishing one is Zeev Dvir's proof on Kakeya conjecture over finite field, it is surprisingly elementary and beautiful. It probably tells us sometimes it is just a change of thoughts we need to prove hard theorems. $URL$ Another one is the elementary proof of the GCI, which is so simple that it has been a while for people to believe. Later people wrote another paper to explain it. $URL$ 

If the answer is yes, in which part of the matrix theory(or L.A.) can I find such results? If the answer is no, whether there's a similar result or some counterexample? 

You need a global convexity to enjoy the optimal convergence rate, otherwise even local convexity will almost surely(not in probabilistic sense) lead to the worst rate you pointed out. MCMC(Markov Chain Monte Carlo) does not overcome the curse of dimensionality. Quite the contrary, Bayesians are working very hard in two directions to solve the problems that caused by the high dimension of the parameter space. (1) The ABC(Approximate Bayesian Computation) scheme. This originated from Laplace's approximation of an exponential integral and its main idea is to discard those samples from each MH step that is "dissimilar from existing samples". But this method suffers from failure of detecting outliers and over-concentration of the posterior. On the other hand even if this method works well during sampling, we do not have any consistency guarantee since when nuisance parameters $\theta_2$ consist of the majority of the parameter space $\boldsymbol{\theta}=(\theta_1,\theta_2)$, Neyman-Scott example will nullify the information about the parameter $\theta_1$ of concern brought in by the samples. That is also a reason why regularization methods are so popular in high dimension inference problems, they simply produce a weighted norm that cannot be washed out by samples. (2) The scalable methods(For example). These are sometimes referred to as divide-and-conquer problems. The Bayesian model, or more precisely the parameter space is decomposed into subspaces, and the high dimension issue is divided into many low dimension issues and the $O(n^d)$ problem becomes $O(n^{d/k}(n/k))$ problems. But the problem is also obvious, that is decomposing and combining parameter spaces will artificially delete and add correlations between parameter spaces, which makes the prior information about $\boldsymbol \theta$ lose. 

If you are just concerned with logistic kernels, and you are willing to put a mild assumption on the $\beta$-Hölder function $f$ to be estimated, then [Rousseau] assumes mild condition $\boldsymbol{A}_0$ and proved the $k^{-\beta}$ decay in Theorem 3.1, which is also cited by [Kruijer&Rousseau]. The good thing is that with this additional assumption the resulting approximation is explicitly given there. The downside is that a more general result in [DeVore&Lorentz] 2.6~2.7 holds. [DeVore&Lorentz] discussed this thoroughly around Chap 2. As for tensor kernel, a popular easy-catch is [Gu]'s books on splines, which basically eliminated all theoretic complications but still point to useful results in statistics. BTW, sometimes you need to swallow down a bit math, statistics is no more than a variant of mathematics:-) Reference [Kruijer&Rousseau]Kruijer, Willem, Judith Rousseau, and Aad Van Der Vaart. "Adaptive Bayesian density estimation with location-scale mixtures." Electronic Journal of Statistics 4 (2010): 1225-1257. [Rousseau]Rousseau, Judith. "Rates of convergence for the posterior distributions of mixtures of betas and adaptive nonparametric estimation of the density." The Annals of Statistics 38.1 (2010): 146-180. $URL$ [Gu]Gu, Chong. Smoothing spline ANOVA models. Vol. 297. Springer Science & Business Media, 2013. 

Such results may also refine some statistical inference like the one here [Gao&Massam]. So I want to know the state-of-art in the direction of finding an asymptotic estimation/bound on the number of graphs with a bound on degrees of vertices. This post is different from Enumeration of graphs with a given and bounded degree sequence and the paper given there since we are dealing with unlabeled graphs in this post. Reference [Strauss&Ikeda]Strauss, David, and Michael Ikeda. "Pseudolikelihood estimation for social networks." Journal of the American Statistical Association 85.409 (1990): 204-212. $URL$ [Mark J. Gotay et.al]Mathematical Aspects of Classical Field Theory,AMS 2010 $URL$ [Gao&Massam]Gao, Xin, and Hélène Massam. "Estimation of Symmetry-Constrained Gaussian Graphical Models: Application to Clustered Dense Networks." Journal of Computational and Graphical Statistics 24.4 (2015): 909-929.