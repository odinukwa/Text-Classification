For small tables above approache using CTE does not matter for performance. For big table, use WINDOWING functions. A very good article is written by Aaron Bertrand : Best approaches for running totals – updated for SQL Server 2012 

I would not rely on the scripts generated by SSMS as they are not 100% accurate. SSMS is not the right tool to generate changes for the database, though it has functionality to do it. Same functionality can be achieved using native or open source or third party tools . Native tools : 

So next time you insert into TableName, the identity value inserted will be 1. When you delete rows from the table, it will not reset the Identity value, but it will keep increasing it. Just like what happened in your case. Now when you truncate the table, it will reset the Identity value to its original Seed value of the table. Refer to : SQL SERVER – DELETE, TRUNCATE and RESEED Identity for a detailed example and some good explanation of Difference between Truncate and Delete 

Yes, backup compression is available in sql server 2008 R2 standard edition and up. Enable backup compression : 

Distribution database should not reside on the servers that are part of AlwaysON availability group that the publishing database is (or will become) a member of. 

Working with SybaseASE since last 7 years, I can tell that the newer versions of sybaseASE relies on MDA tables (just like SQLServer DMVs). The tools out there for SybaseASE monitoring are pretty expensive. I my company, I have build reporting on the top of MDA tables. The prettiness of the graphs you have linked comes at cost. As a cheap alternative (It is a modest 6.99 U$K per core ) would be to use ASEProfiler or DBACockpit (sybase 16 and up). As for the configuration settings, each environment is different, so the configuration settings will be different. Start with the common ones and monitor them .. esp the reuse count should never be more than 0. 

The above will not allow connections to secondary. So your secondary will be TRUE standby and will be used when you do a failover. 

If logshipping paths are same on the primary and secondary servers then adding an will be seamless. If the paths are different on primary and secondary servers then 

I would suggest using sp_WhoIsActive - provided you have access to DMV data. Alternatively, you can use diagnostic queries from Glen Berry or sp_AskBrent. A good start would be A Performance Troubleshooting Methodology for SQL Server 

Also, look at Generate a SQL Server Instance Inventory. It has all tools available for scanning sql servers on network. 

Method 2 - BCP OUT / BCP IN Script out the database SCHEMA_ONLY and recreate an empty database on the same server with a different Name. Use BCP OUT and BULK INSERT to insert data. Below script will help you with that : 

System databases cannot be mirrored or logshipping. The best way is to script and sync up desired objects e.g. logins, jobs, etc. The best tool that I have used so far for migrating databases, jobs, agent schedules, operators, logins, etc is powershell based dbatools.io 

Note: You need to transaction log backups available or the transaction should not have been cleared from the active portion of the log Excellent reading at : Using fn_dblog, fn_dump_dblog, and restoring with STOPBEFOREMARK to an LSN and Joining sys.dm_tran_database_transactions to fn_dblog Results 

SQL Server 2008 Microsoft Certified Master (MCM) Readiness videos especially Backup Internals. A Look at Backup Internals and How to Track Backup and Restore Throughput (Part 1) - By: Jonathan Kehayias A Look at Backup Internals and How to Track Backup and Restore Throughput (Part 2)- By: Jonathan Kehayias 

Yes, a read-only database wont be having any DML operations. So best practice is to change the recovery model to . Make sure that you take regular FULL backups (and most important is testing your restore) - just incase if the server goes down. 

Recently, we upgrade from SQL server 2012 to SQL server 2014 and got hit by the new Cardinality estimator short coming - queries were timing out, cpu pegging close to 100%. After much troubleshooting, updating stats, rebuilding indexes, doing query plan analysis, we figured out that changing compatibility level to sql 2012 works well. Paul White explains - Cardinality Estimation for Multiple Predicates 

You will get error as there schema.object_name has to be unique in the database. Instead when you create an SP, then suffix it with usp_SP_NAME. Naming conventions should be followed and enforced (through triggers or Policy based management), so that you don't encounter such issues. e.g. (below is just a convention that I use, you can use a different convention as well) usp_SP_NAME ==> user stored procedure (usp) and then Stored Procedure Name. or ==> user stored procedure that does select (s) from Buyers table. ==> user stored procedure that does updates (u) to Buyers table. 

The method is minimally logged. If you use regular statement, it would result in overwriting the entire string using . This would become inefficient when dealing with large updates. To support update for large value data types, the syntax supports method. This will result in less Transaction log due to its nature of minimal logging - including insert or appending new data. Note : The using method will fail if the target is . Below is a quick and dirty repro showing the transaction log generated using and using method. 

Now the user account that you use should have permissions on the database server instance. You can check the permissions using below T-SQL : 

Total Server Memory: Amount of memory currently allocated to Buffer Pool and not the total amount of memory to SQL Server Target Server memory: Ideal Size of the buffer pool corresponding to max memory for the instance. Note: If Total Server Memory > Target Server memory, then it suggests memory pressure. Below script will help you find LOW or HIGH Memory notifications from - system health session : 

For the copying backups from one site to another, it depends on your network bandwidth and the size of the backups. Highly recommend to use robocopy as it is n/w resilient - has retry option along with logging and many more. As you are using web edition, you wont be able to use backup compression unless you use custom compression techniques or redgate's sql backup. 

SybaseASE does not have a native function. Also, I am not aware of an existing software (there might be, but I did not have a need to use them for getting BLOB data out of my sybaseASE). 

Below will explain you why you can skip the log backup. It relies on LSN (Log Sequence Number). It is beyond the scope of this answer to go into detail about the LSN, but the link will give you a good idea of what it is. will tell you the and as below : 

How do we "export" a complete Microsoft SQL server database from one server to a "remote" server (including all the "stored procedures" and "triggers"). Right click database --> tasks --> Generate scripts The export facility provided in the management studio seem to only export the tables and nothing else. Incorrect. You can infact script the entire database including data as well (though I would not recommend scripting data -- as the script wont be able to run using SSMS or sqlcmd, etc). 

Now even if you want to use SQL Server authentication, then log-in using your windows authentication and then create a login with sql authentication and map that to the database that you want to access with ONLY required privileges. 

Have a look a the fragment_count - that's one of the fields in the sys.dm_db_index_physical_stats view. You should really be rebuilding indexes with a certain page threshold and as per best practices, it is best to rebuild an index having more than 1000 pages. some reference can be found $URL$ 

You might have selected "restore with NORECOVERY" as below. You have to select "RESTORE WITH RECOVERY" to bring the database out of restoring state. 

QP metrics are always captured in the default group 1 in each respective database and you can access the metrics info using view. So I would suggest you to 

Agree with Mike that its a good question and +1 for that. Mike has answered your question. Out of curiosity, I tried to confirmed that using an undocumented (but widely used) function . This will have a and and you can use that from the T-Log backup to determine at what point-in-time you want to recover. The and are both from the server that the backup was taken. I have servers in NY, HK and LD and tried on them and it confirms it. Below is a screenshot that explains it : 

So the bottom line is that, if we disable the Clustered Index, then Data in the table still exists, but will not be accessible for anything other than Drop or REBUILD operations. All related Non-clustered Indexes and views will be unavailable as well as Foreign Keys referencing the table will be disabled and there by leading the FAILURE for all the queries that are referencing the table. Note: There is no option to ENABLE the Index. You have to REBUILD it. 

Define two NOT NULL column constraints on the table distributors, one of which is explicitly given a name: 

I was being tasked to do similar task recently. IT is not easy in especially SybaseASE. Option 1 : You can write custom tool to BCP out data in files and then grep those files to search for particular text that you want to look for. You can use PERL regex to do as well or use native Unix/Linux commands to parse those files and find the required text that you want to look for. Option 2 : Another option (which is not that great - quick and dirty way using TSQL ) that worked for me . Note: This will take long time (slow and unoptimized) depending on your database size, hardware, workload running on the server as well as database layout. TEST, TEST and TEST it before hand !! 

Based on the error message, your best bet would be to try doing a or bcp out as much data as possible and accept the loss for the data that you cannot bcp out. If you are lucky, you will get some data back, else you have to restore your database to a good backup that you have and accept the data loss. You might argue that data loss is not allowed, but if data loss is not acceptable then you should have a good backup and tested recovery strategy. This incident is your time to revisit your backup strategy and implement a routine of your backups to get restored and tested. Also, you can put alerts in place so that SQL Server can alert you if there are CRITICAL Errors. 

You should use SQL Database Audit. Below is an example that you are looking for (based on your comments) (Modify as per your criteria and needs) 

You can even zip the backup or compress using third party tools as well. Caution: Unless the database backup does not contain any sensitive data and it does not violate your client or company's agreements, you should be fine. Always de-sensitize the data before you start playing with it. 

I tried to repro your scenario on - Dev Edition server by creating a test memory optimized table and checking the dll loaded using 

You are out of luck (since you do not have any backups). Also, it is not possible to use the undocumented command in Azure. It is only supported in SQL Server. Also Transaction logs are no longer managed by the DBA in SQL Azure; this is automatically managed by SQL Azure's infrastructure. check out : Supported Transact-SQL Statements (Azure SQL Database) 

I would highly suggest to be a 1-1 match. If old servers are physical, go with physical with same or more resources (CPU, RAM and Disk (match mountpoints, folder layout, etc)). Physical vs Virtual is a whole different debate with many facets and it would be unfit in the current scope of the question. Note: If you still have questions, I would highly suggest hire a professional as you might run into issues if you do not do a dry-run end-to-end migration test. 

So database mirroring is another option as well and you can create snapshots on the mirrored database and have it as reporting database. Also, if your business does not want real time data, then a custom solution can be designed that can Extract, Transform and Load the data to another server using SSIS (or any tool of your choice) and that can be used as reporting. 

When dealing with SQL 2000 replication, the replication bits does not get cleaned up easily. You can follow this method for cleaning up the left over bits. As Denny mentioned, they are safe to remove as the replication is already removed. 

SQL Server (and Sybase as well :-) ) uses LRU (Least Recently Used) algorithm to keep track of pages that are aged in the buffer pool. It increments a counter every time a page is referenced and decrements the counter every time the lazy writer process sweeps the page. Any worker thread will check the memory status of Buffer Pool to make sure that there are healthy number of Free pages to honor incoming new requests. Note that : SQL Server will always keep a minimum number of free pages on the free list, so that it can serve incoming requests without any significant delay. The amount of free space is calculated based on Buffer Pool and amount of incoming requests. IF there are no free buffers or very little left then : 

Have a startup stored procedure using sp_procoption that runs and checks and alerts you that SQL Agent is not running. 

If it resumes by itself then everything is good. You dont have to worry. If it does not then there is a problem and you will start troubleshooting. 

Below is the result .. we are able to restore the log backup 1,2,3 and 5. We skipped Tlog4 as the LastLSN of Tlog3 was the firstLSN of Tlog5. 

As a side note, highly recommend you to look into Ola's : SQL Server Maintenance Solution - backup and index maintenance 

The second step is to run the SSIS package only if the above condition is FALSE. If the above step is true meaning if it is a holiday as per your HOLIDAY Table, then it should silently fail. 

Depending on the edition of sql server, if you are using Enterprise edition then you can do index rebuild as online operation. Also, the script that you have referenced just blindly does rebuild of indexes which is a horrible idea (Blindly rebuild all indexes and update stats ??) Instead use a much proven and intelligent script. I would not put the database in single user mode as alluded by Scott as if somehow e.g. sql agent connects to the database, then its difficult to kill the connection unless you connect using DAC or if is ON then background thread will grab the connection and you will be out of luck ! Instead put database in restricted mode - so that only users with dbo rights on database allowed (e.g. db_owner, dbcreator, sysadmin). This assumes that your end users are not allowed to connect as db_owner or sysadmin. Remember that Index rebuild is an offline operation (unless you are using Enterprise edition with REBUILD = ONLINE option). Command to put database in restricted user mode & back to multi user : 

is your friend since it exposes the Windows version of the SID. Refer to Aaron's solution : Map between SQL Server SIDs and Windows SIDs 

There is another way that you will find really useful as described excellently by Chad Miller on SSC - Database Space Capacity Planning. He also focuses on which is very useful. 

A readonly database is "READ_ONLY". No writes are allowed - the application will get an error stating that the database is readonly. You can run a server side trace (since you are stuck with sql server 2005) and look for and filter your database by ID. 

I would suggest that they are equally important on physical or VMs. suggest you to create all critical alerts as highlighted by Glenn Berry. 

If there’s no other merge replication set up on this database you may also truncate the MSmerge* tables on the publisher/subscriber databases. 

IF you want to execute query on multiple servers, then Powershell or SQLCMD is your best option. Using linked servers, you have to use OPENROWSET or OPENQUERY. IF you just have couple of servers then creating a linked server to the main will work fine as below : Note : SQL2005 is a linked server.