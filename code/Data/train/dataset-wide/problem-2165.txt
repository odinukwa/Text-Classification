You need to create my.ini Go to You should see a bin folder and the data folder you have in your question Create my.ini in I noticed you have 

CAVEAT There could be other things I could suggest. However, if the amount of data is relatively small, indexing is probably all you need. Give it a Try !!! 

In MySQL Replication, you have two threads: IO Thread and SQL Thread. They are, for all intents and purposes, DB Connections as they are counted (included) under the global status variable . DB Connections and Threads are really one and the same. What then gets counted? 

I used Google Chart a few times and got away from it. It does work, but you have a lot of coding to store the absolute values and calculate deltas for specific variables. Then, you script the URL from the values and render the URL in a browser. GIVE IT A TRY, HERO !!! 

Then, go look into the error log and see if those same warnings reappear. If they do not, you are good to go. You should also look into tuning the InnoDB Fulltext Options. UPDATE 2017-08-22 23:17 EDT METHOD #3 Another thing you can try is to drop the Fulltext Index and create it again 

You will have to setup replication of other Slaves to the newly promoted Master and make sure the data on the Slave match up with the newly promoted Master UPDATE 2012-08-13 17:47 EDT According to the MySQL Documentation on option, you should define it. Here is why: 

Removing foreign keys does not damage data because you are doing DDL to the indexes. Once you do that, data integrity (even for existing data) going down the road needs its integrity tested. EXAMPLE 

The default for group_concat_max_len is 1024 (1K). I set it to 100M in case the list is long. This query will work faster if you have a comound index on If you do not have such an index, then create one: 

As long as the IO Thread is dead (run ), replication from the remote master is no longer possible, especially since you set the address to a nonexistent server. My guess is that you are using row-based replication, If you are, the SQL thread is probably staring at an incomplete transaction and waiting for the IO thread to complete a block of changes that never comes over from the master. When that is the case, you can kill the SQL thread by running 

Here are some changes I see that can be made CHANGE #1 Slide WHERE clause for pt2 into a subquery called pt2 

and see if any new users were added by a hacker. If you see any remote users that should not be there, you can remove them with: 

CAVEAT #1 Running is the same as running followed by . Therefore, there is no need to run a separate . CAVEAT #2 Please note I use instead of . This ensures that the is not recorded in the binary logs because it can replicate to Slaves. If you want the to be replicated all Slaves, is fine. CAVEAT #3 Note that I ordered all the tables by the biggest table first. If you want to optimize all tables starting with the smallest table, change this line 

Deadlock Detection and Resolution Issues has been around as long as RDBMSs have. Even though Oracle owns InnoDB, do not expect Oracle to fix InnoDB. Most applications are to blame for deadlocks, not so much the RDBMS. Regardless of Oracle, MySQL or any other RDBMS, a Deadlock Error can rear its ugly head. Oracle acquired InnoDB October 7, 2005 when the partnership between InnoBase Oy and MySQL expired and MySQL was lax in renewing. IMHO, Oracle tried to stem the tide of MySQL by doing so. Sure, it has made commitments to improve InnoDB and MySQL. It has no choice now due to PR and possible antitrust issues. In light of this, we should not expect InnoDB to have improvements that would overshadow or become comparable to Oracle. Getting away from business politics now, look at InnoDB on its own. It has the variable innodb_lock_wait_time. That option is there to provide for increasing or decreasing the length of time to permit a lock to succeed, and nothing more. If deadlocks occur internally, I would normally look to the clustered index as the victim in InnoDB because non-clustered indexes drag clustered index pointers as well. Updating a row in InnoDB where indexed columns are updated, I expect a deadlock to occur due to the clustered index. Performing SELECT ... FOR UPDATE or SELECT ... LOCK IN SHARE MODE could exacerbate the problem more. How Oracle handles it internally (if they have handled it) would be one of among the long list of things that give it the edge over MySQL. I do not expect Oracle to provide that same edge to an Open Source/Freeware product unless it is Oracle XE. 

Since CURRENT_USER() shows the definer, try using USER(). It might display the MySQL user and the host where that user was coming in from. Please note that this may not work if USER() is an internal thread running the Stored Procedure. 

Other than these things, all other features of mysql's --safe-updates is totally your responsibility. 

You could try to query the table information_schema.columns and find every table in your MySQL DB Instance that have character fields. Here is the table's layout: 

Your table uses INT UNSIGNED (4 bytes), DATETIME (8 bytes), 2 SMALLINTs (4 bytes) per row. That's a total of 16 bytes per row. This means you can fit 1000 rows into a single InnoDB page, but there is a very small piece of fragmentation. Using the default innodb_page_size (16K or 16384), there will be 384 bytes of fragmentation with a page. It is most likely used for mapping the data within the one page. Doing mass INSERTs, DELETEs, and UPDATEs will generate many pages with empty space for whole rows that are marked as unused. This is in addition to the 384 bytes region of the data page. INDEX LAYOUT You will generate many index pages 

If nothing breaks, you can then do this Set these values on M2's /etc/my.cnf - auto_increment_increment = 10 - auto_increment_offset = 2 Restart mysql on M2 Everything should be clean now. From here, INSERTs should be properly handled. Give it a Try !!1 

SHUTDOWN MySQL 5.7 now features the SHUTDOWN command, which requiresthe SHUTDOWN privilege. SHUTDOWN METHOD #1: Within mysql client 

If everything is identical, should 0. Otherwise, there are duplicates present. STEP 04) If There Are Duplicates, Find Them 

No, it does not export indexes. Indexes are rebuilt upon loading the mysqldump back into mysql. The options you found "--disable-keys" cause the the mysqldump to write something like this before the table's load via INSERTs: 

These counts match your output for the Without introducing another subquery, this is my suggested query. SUMMARY The output would have to be used as follows 

Give it a Try !!! UPDATE 2013-02-22 21:29 EDT @Michael-sqlbot had just pointed out to me that is a MySQL 5.1-specific plugin option to force an all-or-nothing situation: If InnoDB plugin fails to start, mysqld just dies rather than use the built-in InnoDB. Your solution would simply be to remove that line 

Running is required because replication is updating , , and relay logs. Once stopped, running does everything. Why ? MySQL 5.5 You needed to do this: 

OK, I hope you are sitting down. This is the lazy way Oracle implemented sql_mode in MySQL 5.6: There is an additional file. If you run 

Here is where the meeting of the minds, that is to say, the minds of Developers (DVs) and DBAs, must inevitably happen. Working with Business Logic (BL) and storing such in a database can have an impact that can either glorify or horrify its implementation. For some RDBMS products, there exists superior libraries/tools/APIs for Business Logic and Object Infrastructures one could quickly learn and employ in their applications. For other RDBMS, no libraries/tools/APIs exist. In the past, client server apps made the bridge into BL via Stored Procedures (SP). For products such as Oracle and SQL Server, this was done early. As open source databases such as PostgreSQL and MySQL came into being, those using them were at risk of breaking new ground with stored procedures in BL. PostgreSQL matured very quickly in this, since not only stored procedures were implemented but also the ability to craft customer languages also came along. MySQL basically stopped evolving in the world of stored procedures and came in a stripped-down form of a language with many restrictions. Thus, when it comes to BL, you are completely at the mercy of MySQL and its Stored Procedure language. There only really remains one question: Regardless of the RDBMS, should BL resides in whole or in part in the database ? Think of the Developer. When things go awry in an application, the debug process will have the Developer hop in and out of a database to follow data chanages that may or may not be correct intermittently. It is like coding a C++ application and calling Assembler code in the middle. You have to switch from source code, classes and structs to interrupts, registers and offsets AND BACK !!! This taking debugging to that same level. Developers may be able to craft a high speed method of executing BL in conjunction with language configurations (compiler flags for C++, different settings for PHP/Python, etc) via business objects sitting in memory rather than in a database. Some have tried to bridge this ideology for faster runnng code into the database by writing libraries where debugging Stored Procedures and Triggers is well integrated in the Database and seemlessly useable. Thus, the Developer is challenged to develop, debug, and maintain source code and BL in two langauges. Now think of the DBA. The DBA wants to keep the Database lean and mean as much as possible in the realm of stored procedures. The DBA may see BL as something external to the Database. Yet, when SQL calls for the data needed for BL, the SQL needs be lean and mean. Now, for the meeting of the minds !!! Developer codes SP and uses iteractive methods. DBA looks at the SP. DBA determines that a single SQL statement can replace iteractive methods written by the Developer. Developer sees that the SQL statement suggested by the DBA requires calling other BL-related code or SQL that does not follow normal execution plans of the SQL statement. In light of this, the configuration, performance tuning, and SP coding becomes a function of the depth and data-intensiveness of BL for data retrieval. The more depth and data-intensiveness the BL, the more Developers and DBA must be on the same page for the amount of data and processing power given to the Database. CONCLUSION The manner of data retrieval should be always involving both Developer and DBA camps. Concessions must always be made as to what coding methods and data retrieval paradigms can work together, for both speed and efficiency. If the preparation of data for source code to handle is done only one time before the code gets the data, the DBA should dictate the use of the lean and mean SQL. If the BL is something the DBA is not in tune with, the reins are then in the hands of the Developer. This is why the DBA should see himself/herself and part of the project team and not an island unto himself/herself, while the Developer must let DBA do the fine tuning of the SQL if it does warrant it. 

If you have this corrupt state due to a system crash or a false positive on a successful write to disk, you will have to consult Tokutek directly as Tokutek is a little out of my wheelhouse. 

For a million rows, that would 77,000,000 bytes (73.43 MB) As for measuring the table, for a given table mydb.mytable, you can run this query