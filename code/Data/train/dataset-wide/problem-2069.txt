In a read-heavy, low-write environment on a RAID5, I would just leave that to one's budget, tolerance, and blood pressure. In a write-heavy, low-read or write-heavy, read-heavy environment, RAID5 is simply out of the question. This is especially true for InnoDB. Think of an InnoDB's table interaction. InnoDB If you do not use innodb_file_per_table, OMG all the activity would be centered around just one file, ibdata1. What is contained in ibdata1? 

You had setup MySQL Replication, you could achieve the following: SLAVE You could do the following on the Slave in Session #1 

@Richard and @DTest already have good advice in their answers, but I would like add a little as well. RECOMMENDATION #1 : Check SUPER Privilege for all DB Users Make sure all users do not have SUPER privilege. Once you hit max_connections, mysqld will only allow one more connection to itself, and that user must have SUPER privilege. If all users have SUPER privilege, no DBA or sysadmin can ever login to MySQL and do admin tasks. Please do an inventory of all users and make sure that only necessary users have SUPER privilege. RECOMMENDATION #2 : Check MySQL Connections from the OS Point-of-View If you cannot connect to mysqld, try running this in Linux on the DB Server 

InnoDB likes to write everything it does to the Double Write Buffer located somewhere in the middle of the file ibdata1 as a precaution should there be a crash. Missing data and pages are recovered from the Double Write Buffer. Since you are doing a big migration to a slow device, you should disable it for the duration of the migration. In this instance, just do the following: 

Since moving to SAS improved things 10%, try moving other parts of the ACID compliance to SAS as well. Perhaps moving , , may help as well. 

As for the runtime settings and actual defaults, it's a little different STEP 01) Install mysql on a DevServer with no /etc/my.cnf STEP 02) STEP 03) Run STEP 04) Run the Diff 

OBSERVATION #1 Since your buffer pool is 6G (6144M), the innodb_log_file_size should be 1536M (25% of 6G) OBSERVATION #2 You have sync_binlog set to 1. This provides the safest ACID compliant setup. It can also slow things down dramatically. You say . That's the case because each completed DML (INSERT, UPDATE, DELETE) and DDL (ALTER TABLE) statement gets written to the binary logs. The default for sync_binlog is 0. That let's the OS be responsible for flush binary log changes to disk. OBSERVATION #3 You have innodb_io_capacity set at 10000. That's really 10000 IOPs you are expecting of mysql. Try lowering it. There are some things to do in this respect 

Look for the query in the slow log. From there, you should be able to tell whether USER() or CURRENT_USER() was used to set . 

SUGGESTION I don't know if you will encounter the bug with my suggestion, but here it goes ... If you are using an option file, make the header use instead of . As a utility, I know can be used as a group header. Please try out as a group header and see if it helps. ADDITIONAL OBSERVATION Since you are connecting to DB Server running MySQL 5.5, its mysqld will never understand and variables because they belong to MySQL 5.6.3. I guess this means you were connecting from a MySQL 5.6 machine using the mysqlbinlog for MySQL 5.6. You will not need to specify any SSL connection options if you are remote connecting for binlgogs 

One sure way to speed up an ALTER TABLE is to remove unnecessary indexes Here are the initial steps to load a new version of the table 

I have an incredibly cheesy but effective way to create a backup file with a datetime as a name. SAMPLE DATA I have a table called as a sample 

In addition to MaxVernon's answer (hitting the nail on the head, +1 !!!), you could also consider using tombstone tables to monitor which rows can be soft deleted. See me post from Tombstone Table vs Deleted Flag in database syncronization & soft-delete scenarios. Please consider which way you can live with 

This might be a little slower since you cannot index the subquery like the first query shows PROPER INDEXING What may also help both queries go faster is to index status and unsub_date 

As for the disappearing of logs, any time FLUSH LOGS is manually or internally executed, it will delete binary logs older than expire_logs_days*86400 seconds. I remember years ago that expire_logs_days used to trigger every midnight (I am going way back to 5.0.x). Now, expire_logs_days has seconds granularity. 

This will erase all relay logs in the Slave, starting with a fresh one. It should start retrieving binlog events from the Master. If it fails again, then on the Master may be corrupt. In that event, you would have to set up replication from scratch by doing this: 

Keep in mind that while InnoDB support for an auto_increment column in a exists, it does not have the same auto_increment behavior as MyISAM. MyISAM can bind with other columns to allow a number to exist multiple times, InnoDB does not do that. Once an auto_increment value is used, it is unique for the whole table. The excerpt you found 

From here, you could load each file into a BLOB and store that BLOB in a table. You would have to make sure the DB Server has SAN storage mounted. You would then specify the file with its fullpath. 

Above this output are histograms of these 20 top worst-performing queries Example of the first entry's histogram 

Please refer to the post on how to setup and empty stopword list. UPDATE 2018-05-04 22:44 EDT If you run this in SQL Fiddle 

The reason you get a 0KB file for output is becuse of a syntax error in the mysqldump You have this in the question 

If the values do not come back the same, then something is out-of-sync. If you want to examine a bunch of table in bulk, you can down Percona's MAATKIT. You will need two specific tools (Percona also has the Percona Toolkit that they themselves forked from MAATKIT which is now being promoted more) 

To make sure your table is clean and the stored procedure is working, assuming ID 200 is the default, run these steps: 

My suspicion would be that you are either using or you have character set issues. It might center around the following bugs 

Look closely at the Buffer Pool in the upper left corner. There is an Insert Buffer inside it. It is responsible for migrating changes to nonunique indexes from the Buffer Pool to the Insert Buffer inside the system tablespace (better known to us commoners as ibdata1). When InnoDB tables have lots of secondary indexes, heavy INSERTs, UPDATEs, and DELETEs to those tables will trigger growth and congestion in the Buffer Pool's Insert Buffer. The setting innodb_change_buffer_max_size governs what percentage of the Buffer Pool is to be use for the Insert Buffer. The default is 25. That means if you allocate 24G for innodb_buffer_pool_size, don't be shocked if the Insert Buffer uses 6G of it. When a Buffer Pool is 90% full of data, it's only good for doing heavy SELECTs (when doing reports and analytics). In your particular case, since you ran that query and determined that 90% of the Buffer Pool had data, that leaves only 10% for the Insert Buffer and other InnoDB incidentals. It is entirely possible that a loading of a mysqldump could push the limits of a Buffer Pool that has an insufficient amount of room for its Insert Buffer. Personally, I have never heard anyone complain about InnoDB crashing MySQL because of this. For the sake of erring on the side of caution, perhaps you may want to try setting innodb_change_buffer_max_size to 50 when doing heavy INSERTs with 

UPDATE 2013-03-19 11:05 EDT OK, I have some bad news. According to MySQL (still hate saying Oracle, Yuck), that version was never released. Here is a bug report from Mar 15, 2013 : $URL$ From the desperate tone in the message, something tells me you submitted this to MySQL. If you read a MySQL book from one of the original MySQL developers, he/she would have access to that unreleased version of MySQL. Your best shot would be to contact the book's author to get a copy of MySQL 5.1.8. 

How could a SLAVE produce different results? If you have a several hundred INSERTs commands occur on a Master, they get serialized in the Binary Logs. Even if a DB Connection 1 completes an INSERT before DB Connection 2, it is possible for DB Connection 2 to write its event in the Master's Binary Logs before DB Connection 1. This is particularly true in a write-intensive, heavy OLAP environment. This would make a Slave's copy of physically out-of-sync with its Master. Running queries can hide this. OK, you are probably thinking it's just a SELECT. SELECTs are not in Binary Logs. INSERTs, DELETEs, and UPDATEs are. Did you know you could do a query like this ? 

This looks like a job for Dynamic SQL against the information_schema Suppose the table you are doing the against is and the query looks like this: 

Personally, I have searched high and low for the Perl equivalent of these C/C++ API calls that would open an embedded MySQL Server. Apparently, none exists for the Perl API. Here is an excerpt from dev.mysql.com that verifies this: 

Unfortunately, you are not going to get those queries from . Why ? There are two options evidently being used 

Since you have already converted to MyISAM and indexed, I would recommend making snapshots of that table based on a timestamp range. What I mean by snapshot is a temp table that contains just the events you wish to mark. Suppose you want to log entries from the last 10 minutes. This Dynamic SQL should do it for you : 

the option is not supported in Windows. You can set [innodb_flush_method] to to personally make InnoDB handle its own disk caching. Even with Linux, be careful. VMs and bare metal machines with new ext4 kernals actually fake O_DIRECT. See mysqlperformanceblog post on this Definitely, you should NOT USE WINDOWS !!! 

User can only access the if and only if is authenticating from . If you want to access other another user's stored procedures, thenyou need to run this: 

You may want to try one of two things IDEA #1 : Create a column to store difference in seconds Create the column and populate as follows 

YOU HAVE NO DISK SPACE !!! Get more diskspace freed up or have increase diskspace. According to MySQL 5.0 Certification Study Guide 

Partitions seem to be numerically represented as a 4-byte unsigned integer. As mentioned, takes up 3 bytes, but, from what I have observed, partitions are bounded by 4-byte numbers. Given this, expressing a as a parameter really does not buy you anything except cleaner syntax for defining the range. Expressing to_days of a DATE yourself or having mysql do it for you boils down to personal choice. 

If the SSH drops without putting the mysql session into the background (along with nohup), then the DB connection is at the SSH session's mercy. So, yes, mysql would care about the lifecycle of the SSH session. You should launch it with at the end as follows: 

This will show you how sparse and how dense each composite index combination is. Sparse combinations (low rowCount) should use an index scan. Dense combinations (high rowCount) should use a full table if the Query Optimizer believes it must read too much of the composite index. You can try this out by running 

Please note the physical independence of a table from the ibdata1, and how log files are related. Since the box is a Slave, you have two options : OPTION #1 (Warm Backup) 

All of this could have been accomplished with the INFORMATION_SCHEMA database The table names are in the INFORMATION_SCHEMA