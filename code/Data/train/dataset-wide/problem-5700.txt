Seems like a reasonable assessment - the justified component means that you have to believe something for a reason, not just because you believe it from birth. I'm not sure of any major philosophical theories that put much stock in the value of innate knowledge, however. 

All I can conclude here is that Utilitarianism does a "just ok" job of solving this problem. Basically, the conclusion is that from this perspective, we can't just concern ourselves with how our host country will fare - we must also consider how our actions affect those in the source country as well. In some cases, it may be that despite A being very prevalent and very bad, we're actually helping the world by importing them to a society that will hold them responsible for their evil actions. It's possible that another ethical framework provides a more satisfactory answer (I'd expect deontologists to have an interesting response). I'm not sure if this is the example you were looking for, but it does provide an ethics-based reasoning as to what the best thing to do is. Part of my problem with this question is in handling all the variables - maybe my treatment of some of the variables leads me to this conclusion. Maybe we ought to consider what percent of the migrants would make of the host country upon moving as well? 

Telling truth is the product of an intention. If the mind of the truth teller is uncluttered by an intent to deceive, he/she is not subject to disapproval, in normal circumstances. But the intent to tell the truth bears a specious connection to what is actually true. We are begging questions of epistemology - what it is possible to "know." We shall assume, however tentatively, that there is such a thing as "the truth." A person's intent to tell it, however sincere, is only a guarantee of the teller's intention, nothing more. Experience with the human mind teaches that mistakes are such a commonplace, their absence in our thinking is the exception. 

I believe you are up against semantics. The term "void" is commonly used to describe any expanse that appears to contain no objects. However, the true void described by Parmenides is different - that which is not. I truly wish that there were two terms for these two, distinct ideas. I distinguish them this way: The first void can be measured or described in terms of space/time dimension or some other feature, whether concrete or abstract. The true void cannot be measured or described. It is not, therefore there is nothing to measure, nothing to describe. It is not a place you can go. It is not the edge of the universe. It is not even a concept you can form in your mind. Ontologically speaking, and using the terminology of my native Nebraska, it just plain ain't. I would welcome the wisdom and knowledge of trained philosophers who might come up with two different terms for these two different ideas. In popular usage, however, I believe they are forever confounded. 

So for human beings to be able to observe galaxies, humans need to 1) exist (to do the observing) and 2) galaxies need to exist (to be observed). If humans can't exist without galaxies, then anything that disrupts galaxy formation will thwart us humans from doing the observing right at that first step. Thus, once we have proved the necessity of galaxies for human existence we can conclude that the cosmological constant must be in the range of values that allow galaxies to form, thanks in part to the weak anthropic principle. 

Trivially True Languages work because we construct shared meanings to associate with specific patterns of symbols to form words (syllables for spoken languages, gestures for sign languages, etc.). As such: 

Yes to both parts: Once you move beyond a catchphrase-based understanding of biology (and the emergent human sociology) the conflict dries up like so much straw left in the sun. That codified human rights are a human invention should be obvious, and their being a human invention - built by humans, for humans - in no way diminishes their worth. 

The answer to your question is yes, if you are polling a general population. When it comes to a community of interest, such as Stack Exchange seeks to build, the answer may be less clear. I believe the activities of the moderators and the rules that are enforced seek to avoid this fallacy. We are approaching - though probably not achieving - something like peer review. However, even the most rigorous peer review can fall prey to the fallacy. Whenever I encounter an up voting criteria for crediting an idea, I think of what I call the Copernicus Factor. Had the survival of Copernican ideas depended on the approval of even his most learned peers, well, it is not that the truth about the structure of the solar system would have never come out, but it certainly would have been frustrated. It is good to be reminded of your question regularly when communicating in this stimulating virtual environment. 

Atheism is a null hypothesis. Treating atheism scientifically is to say "I don't believe that god(s) have any effect on [thing being studied]". Calling atheism into question is as simple as providing statistically significant data demonstrating an effect where the null hypothesis would predict none. For example, an experiment to test the efficacy of some arbitrary treatment on some arbitrary disease would be roughly formulated as: QUESTION: Does [treatment X] have any effect on [disease/condition Y]? Null hypothesis: Use of [treatment X] is statistically indistinguishable from a similarly-administered placebo. Now, using 'prayer' or any other supernaturally-themed technique as a possible treatment, it's easy to see that the null hypothesis is one formulation of atheism. To scientifically demonstrate that god(s) exist requires starting from an assumption of atheism and then demonstrating that the null hypothesis fails to explain some set of phenomena. The long history of failure to do so is actually evidence against certain god-concepts, especially interventionist ones. Indeed, if god(s) exist, we might expect to see some unambiguous statistical trends such as: 

This is a tough question, but consider the following scenario: In my lab, I mix 2 moles of pure Sodium with 2 moles of H20, which produces the following reaction: 2Na(s) + 2H2O â†’ 2NaOH(aq) + H2(g) + some heat The fact that I just mixed my Sodium and water means that in a couple seconds I will have a couple spare moles of aqueous NaOH and Hydrogen gas, with some heat to boot. We understand basic chemistry to such a deep level that it's nearly impossible to argue that this will not take place. However, there are some unspoken assumptions here: we're assuming that a meteorite is not going to come flying through the air and into a vat of HCl, causing a new reaction to take place with the Water. We're assuming that no one will throw a rock through the window, causing them same. We're assuming a massive earthquake won't take place and cause the sodium to fall out of my beaker and into a miles-deep crevasse. I could go on. Generally, when we make scientific predictions, especially predictions that fit a well-tested theory, we're saying "Given what we know about our universe (which is entirely physical and deterministic), the results of past experiments, and barring any unforeseen circumstances, X will happen if certain prior conditions are met". However, in daily parlance, we're often saying things like "The sun will rise tomorrow," or "The Boston Red Sox will win" which forgo some of the formality of strictly scientific statements of theories. Specifically, the only kinds of "facts" we can assume about the future are tautologies: it's always going to be true that water = H20, it's never going to be true that triangles have 4 sides, it's never going to be true that T = F, etc. But these aren't predictions in the sense that you're asking for, they're just trivially true by definition. TL;DR - there are facts about the future, but they are sort of trivial. There are good predictions about the future, but they rely on assumptions of normality and induction, to some degree. It appears what your professor is saying is that there are no non-trivial facts about the future which we can know with absolute certainty. 

Social animals (including humans) learned long ago that cooperation is a winning long-term strategy. Can you imagine how shitty life would be if we didn't have our modern division of labour? If nobody had time to program a computer because we were all so busy foraging food for today and desperately hoping we'll be able to do the same tomorrow? Evolution (descent with modification) is an 'is', not an 'ought'. It happens to populations, not individuals. Selfish behaviours may help some individuals short-term, yet can be disastrous for the population as a long-term strategy. Our history shows the evolution of the concept and codification of human rights from earlier social constructs, such as the earliest expressions of the golden rule right back in antiquity. There is nothing unnatural about an evolving social contract among social animals such as humans. 

Ah. Short answer: No, and a solid argument for the opposite. ASSUMPTIONS: First, by secular argument I assume that you mean one that does not include magic - i.e., that relies only on what we know of the physical universe, how it works, and some reasonable extrapolation from there into areas where the results aren't in yet. Second, be aware that you are taking as a given that something called a 'soul' exists. While the nature and properties of 'soul' are not well defined, I will assume for the sake of argument that by 'soul' we can mean 'some core aspect of self'. Third, we must assume that the 'soul' has some effect on/control of our minds/bodies while we are alive in order to meaningfully represent 'us'. This should follow fairly obviously from the second assumption, but it is a critical consideration to keep in mind. Fourth, in order for a soul to carry that core aspect of self into the future postmortem, we must assume the soul pre-death is largely the same as the soul post-death. 

It might help to think of it like this: A: I am a qualified chef B: I have great knife skills I pick this example because it's absolutely impossible to be a decent chef, much less a qualified one, if you can't properly use a knife. You'll chop ingredients too slowly, cut yourself (or others) and not know when your knife is dull. I don't like your example because cooking well & being a qualified chef are less distinct from each other than knife skills and how good a chef one is. Lets say , as you did. In the absence of , it still may be true that . It's not hard to imagine an assassin or samurai who is an awful chef yet still has amazing knife skills (i.e. ). We also know that (there's a technical name for this that escapes me). In my case, where I exhibit both (i.e. I'm both awful with knives and a poor chef) it is still true that - it's just that in this case the operator is picking out instead of either one being a possible option. Since is still true, the material conditional itself is also true because of this equality. Hopefully this clears things up for you. 

Yes, in exactly the same way that law exists - that is, as a social construct that we human beings have come together to (mostly) agree upon. Declarations of human rights are living documents, in that they are open to interpretation when conflicts arise and are subject to change as we refine the language used to express the ideals behind them. Codes of human rights (or lack thereof) help shape the world, or at least in the places and aspects where humans spend the most time. If you mean 'exists' purely in a physical sense, then no - you'll never be able to pick up a Human Right and chew on it. Human rights are (often codified) ideas, and like other ideas exist in a more abstract manner. At its most basic, the social contract (explicit or implied) that we human beings have with one another implies there is some set of rules we should all follow. There are many different possible social rulesets, many at odds with each other. The concept of human rights forms the basis of one possible ruleset that takes fair treatment of all human beings as a core concept. In other words, human rights exist because we humans created them - in much the same way that art, music and philosophy exist because humans created those, too. 

We do not respond to ethical dilemmas with binary switches and levers, but with complex and nuanced human behavior. Our ethical obligations in a scenario may or may not be relative to a lot of things, real or imagined, but it is undeniable that it will be relative to the capacity of the agent in the moment to make the ethical judgment. That agent may seek to act according to objective, universal norms, but we can guarantee that another agent will see those norms differently. Ethical relativism can be slammed as a means of avoiding ethical decision making altogether, and there is some credit to that criticism. However, ethical absolutism can fail to take into account the varied make up and judgment of the agents who must carry out ethically sound behavior. Or the position of an ethical absolutist may be that all agents should surrender their independent agency to some supposedly objective judge. To my mind, such an idea is itself unethical, and would obviate the whole scheme of objective enforcement. So I believe there is a position between the two extremes: If we posit a moral and social obligation for all persons to strive to act according to ethical norms, we must acknowledge the agency of those persons, which is to say the necessarily flawed humanity of those persons. We should understand they may disappoint and even anger us when they appear to fail in this obligation. They may disappoint and anger us even as they strive with undeniable integrity to act according to objective ethical norms that we may profess to share.