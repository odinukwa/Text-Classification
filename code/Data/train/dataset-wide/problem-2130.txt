If all other things are equal, it is likely (as per @gbn's answer) that a different execution plan is being generated on each server. As an academic exercise, it would be interesting to see both plans so grab them from the plan cache on each server and add them to your question if possible. We can then identify the differences in the plans that are causing such a big variation in performance. For a quick fix, take a look at the USE PLAN hint. This makes it possible to attach the good plan from the fast server, to the stored procedure on the slow server. Edit: Following update re: cursor One other variation on your query to try that I don't see mentioned in other answers: 

The parameter changes import batch size only. I'm not aware of anyway to change the export row count reporting. 

A logical backup device is just an abstraction from the physical device. This simplifies switching between physical devices, as you can change the logical device definition rather than have to edit your backup scripts or maintenance packages. 

What you're trying to do would leave the database in a (transactionally) inconsistent state, hence it isn't possible. The Partial Database Availability whitepaper is a useful reference guide and includes an example of how to check whether a particular table or file is online. If your data access were through stored procedures, you could relatively easily incorporate that check. One alternative (but somewhat hacky) approach that might be worth a look in your scenario would be to hide the table and replace it with a view. 

If only it were that simple. You're going to be on shaky ground if you offer this sort of advice to customers, especially if you're sending them a bill for the proposed upgrades. You can certainly offer some insight but not guarantees: 

As others have indicated in the comments, it's difficult to form an answer to this question without an understanding of the application. It depends, it really really does. The nature of the question (and answer) also changes on the basis of the physical environments i.e. multiple databases on a single server or spread across several? Is a customer “typical” or do some consume a disproportionate percentage of server resources? In 3 years, will there be 50 customers or 50000? That said, let’s have a crack at it. 

What's your definition of "doesn't need it"? Once the buffer pool has been filled, it isn't going to drop pages out until they become disfavoured by other pages being brought in. If you want the memory back immediately, as is the case for @jrara, you've got to stop/start. 

Most likely a recompilation was triggered and unfortunately a non-typical set of inputs were used for the compilation. 

It has to wait on the scheduler A runnable queue, there is no switching of tasks to alternative schedulers. The time accumulated waiting for the scheduler to be available is referred to as a signal wait. 

For 2008R2, I'd take the extra cores every time. If the server might be upgraded to Denali/2012 on release you may need to factor in the switch from per socket to per core licensing. @MrDenny summarised this well in SQL Server 2012 Licensing Changes. 

Open file in SSMS, no sign of the filter. Rinse, repeat, same again. Assuming I was doing something daft I looked up the value to use (harder to find than it should be!) and added to my trace definition manually, which worked. 

Prove there is a problem. Identify and document crazy queries and data access patterns. Approach the vendor with your evidence of a problem. If you can show that a re-write of a query shows improvements are possible, it gets harder for the vendor to ignore your concerns. If your getting no response from your contact at the vendor, escalate. If you can't make enough noise to be heard, maybe your CTO can. If you've reached step 4 without any success, you're likely to be sacrificing follicles or noticing the telling first signs of grey. This is where a voodoo doll of the third party development lead can ease the stresses of the day. A dartboard featuring the company logo is an acceptable alternative. 

Create matching usernames and passwords. It really is that simple. For your example, create an account for on with the same password as on . Just in case this is an issue you're having with connecting SSMS with a non-domain account, that requires jumping through an additional hoop. Some applications don't support stored usernames and passwords via the windows credential store. SSMS is one of them. I have a bunch of runas shortcuts to deal with connecting SSMS to a variety of servers in different domains: 

I haven't found an option to turn off the design portion of the split view. Nor have I found a way to have the order of the windows remembered after closing a window, which is annoying! However, if you want to ditch the SSDT editor completely you can change the default for .sql files in Tools/Options/Text Editor/File Extension, as shown below: 

If you are running your backups as per the posted example code, they will not overlap. An statement will complete before the next statement is executed, which you can demonstrate easily: 

The first two will allow you to track allocations at a query & session level. The third tracks allocations across version store, user and internal objects. The following example query will give you allocations per session: 

I'm inclined to agree with @Catcall, database recovery should be top of the list. The implications of both backup and recovery options are typically the most poorly understood outside of a DBA team and the most likely to result in disaster. 

Your example should indeed work but it's not the recommended approach. By doing a drop/create on the CL followed by a drop/create in the NC, you're rebuilding the NC indexes 3 times. 

With an index defined on K1, an ORDERED BACKWARD scan of the index is chosen, which probably translates to 4 pages read from the non-leaf index pages plus 1 for the leaf level. 

It's dirty, it's cheating, DO NOT EVER DO IT IN LIVE, but it works. Quick test of a newly created database with a 32MB log file shows it as occupying 330kb on disk when compressed, decompress the folder and on disk size is back to 32MB. 

Backup the log Shrink the log file with DBCC SHRINKFILE with TRUNCATEONLY to get it as small as possible Run ALTER DATABASE MODIFY FILE to re-size the log to an appropriate size 

Caveat: Also assuming that the typical usage pattern would be to find alerts for a specified user, not results for a group of users. 

If there is no other activity that you need to persist (assumed from the reference to testing) then a database snapshot can be useful here. There's obviously a tipping point around the volume of change whereby a regular full backup could be faster if you're churning a large percentage of the database. But, as the creation of the snapshot is instantaneous (the mechanism uses a sparse file) and the regular backup route requires you to create the backup first, snapshot is usually the winner for dev/test activity. Note that reverting from a database snapshot shrinks the transaction log to 0.5MB due to a rather nasty bug, so you'll need to re-size your transaction log appropriately. 

The values reported are cumulative since the last restart. If you want to measure over a fixed period, use the command below to reset wait stats. 

A volume license agreement is essentially a discount pricing arrangement, it does not involve a different installation, key distribution policy or CPU utilisation limitation: 

Log != transaction log That said, if you were hell bent on torturing yourself or a development team with an arduous time consuming task that would yield minimal value, this would be an ideal project. The cheaper option (if viewing transaction log records is really what you need) would be a 3rd party tool like ApexSQL Log. Depending on what you're trying to achieve with this, the undocumented fn_dblog command might be of interest to you. Also see Tracking Transaction Log Activity in Denali for a neater way of handling this with extended events in the 2012. If you expand your question with a better description of why you want to look at log records, we can suggest alternative (less hacky) approaches. 

These are only added to the row when it is modified, subsequent to your enabling snapshot isolation. There is no blocking or additional load generated to add the 14 bytes to each row at the point you switch snapshot on. The only blocking action you may encounter at the point of enabling is due to the need to wait for all current transactions to commit, which is worth keeping in mind. Ideally make the change during a quiet period or preferably a moments downtime where you shut all activity out. If downtime isn't an option, avoid any period where long running transactions might occur (ETL for example). If you don't get a response within a few seconds, you can query to identify what's getting in the way. 

If you have an edge feature (something non-core/rarely used) that falls into the categories above then you can make a case for disabling or removing it until you move to 64bit. Often, 80% of features at 99.99% availability is more important than 100% of features at 90% availability. 

You can determine which port an instance is using from the error log but not whether its static or dynamic. 

Or could be an issue with MSDTC configuration, which is a laborious and lengthy checklist to run through, documented fully in Recommended MSDTC settings for using Distributed Transactions in SQL Server. 

This is a Service Broker process listening to a queue for messages. 300 seconds will be the time out specified in @rec_timeout. See RECEIVE reference in books online. 

Looks like a natural vs surrogate key decision, opinion on which ranges from considered and practical to academic, bordering on dogma. Depending on the RDBMS, there are considerations for the physical model which can have significant performance implications e.g. clustered key choice in SQL Server. Personally, if I have a narrow, single attribute candidate key, I’m tempted to make use of it. Wide and/or composite keys, by default I’m adding a surrogate to the model. In your case, I’d vote for the identity column on Sheet_Size as primary clustered key and a unique constraint on type/length/width/thickness. 

If you're inclined to dig into the code, you may be able to re-purpose TPCCBench which is SQL Server specific. 

First up, well done for trying to get to grips with how WAL works. It's often misunderstood but when people grasp the concept it's a genuine lightbulb moment for understanding databases. We need to re-word your description of the steps that occur to commit a transaction: 

That would depend on the queries against the table. If query patterns are such that they target a subset of the 500 or 100 rows that your filtered index covers, perfect. On the flip side, it's unlikely that the optimiser is going to choose a filtered index that includes 85% unless it is a covering index for a particular query. I haven't tested this but I would expect filtered index utilisation to exhibit the same tipping point behaviour as non-filtered indexes. 

Your backup is from a database that is using Enterprise Edition features. You are trying to restore it to an Express Edition server. That will not work. The message in your error log is telling you exactly that: 

If you post the execution plans there may be obvious differences that we can highlight. Other than plans it could be: 

Cancel your other plans and do this now. Stop trying to optimise the evaluation of 3 billion rows and instead evaluate rollup + 300k rows. From your outline of the situation, anything else is a waste of time and someone's money. A parody definition of insanity is doing the same thing over and over and expecting different results. Reading 3 billion rows that aren't changing fits that definition. Stop. 

The issue of separating logs from data where SSDs are used is a matter of RPO (recovery point objective) for the system, rather than performance. If the RPO is defined in minutes go with one shared array and take log backups every [RPO] minutes. If the RPO is defined in seconds go with separate arrays. To be honest, if the RPO was tight I'd keep SSDs for the data array and use a mirrored pair of expensive (enterprise) reliable spinners for the log. 

Your question is a little sparse on the details necessary to provide a conclusive answer. To start with, can you add the table DDL (including clustered index definition) to your question along with row counts and database size. Also, a description of the server spec (memory, cpu, drive configuration) and whether or not tempdb is on a separate array. Next, the following will give us an indication of how your IO subsystem is performing. 

According to @SqlKiwi... the change from creation_time to cached_time was just because procedures and triggers were added in 2008 and the opportunity was taken to choose a more descriptive name. The created/cached time reflects the last compilation, not the creation time of the original plan. 

My initial playing around with various queries suggested no pattern at all but on paying closer attention it appears to be predictable for serial plans. I ended up at KB314648 which @AustinZellner mentions: 

Yes, the service name is always for a named instance and for a default instance. I don't believe either can be altered or overridden, nor can I think of any reason why you'd want to. You can enumerate the installed instances on a server via the registry, using Powershell for example: 

There is a comparable issue logged on connect for the version you're having a problem with - An INSERT statement using XML.nodes() is very very very slow in SQL2008 SP1. 

There is no best practice. Your choice of concurrency control must be based on the application's requirements. Some applications/transactions need to execute as if they had exclusive ownership of the database, avoiding anomalies and inaccuracy at all costs. Other applications/transactions can tolerate some degree of interference from each other. 

If you're always looking at data for a single , perhaps they need to flip to . This is optimisation through the eye of a needle. Query optimisation usually needs to assess the query and execution plan as a whole. 

Capture SQL:StmtCompleted, with a column filter on TextData %TableName%. Capture Lock:Acquired event, with a filter on ObjectID with the id of the table. 

Not my field of expertise but as I understand it the difference in the majority of so-called in-memory OLAP databases (not a term I'm fond of, it's used as marketing pitch more than as a fair comparison of technologies) is column store indexes. Column-Stores vs Row-Stores (How Different Are They Really) is a good intro to the technology if you're familiar with traditional OLTP and OLAP database structures. 

I'm going to have to disagree with Aaron (and the accepted answer). Aaron's approach is the way I like to deal with "DBA stuff" e.g. maintenance scripts. I would never want to do this with a library function that would be called by a user database. Why? You'll be heading for the database equivalent of DLL Hell. 

I would favour @StanleyJohns suggestion if you have SQL2008. Extended events are worth familiarising yourself with as a diagnostic tool and Jonathon's An XEvent a Day series is a great place to start. An alternative for deadlocks is to enable trace flags 1204 and 1222, which dump deadlock information to the SQL error log. Enable both, so you get the information in two differing formats, which can make it easier to understand complex deadlock chains. 

The aggregate will only be calculated once, so you're ok as you are. That said, I tend to use a subquery (as per @Aaron's comment) but purely for readability. 

Create a compressed folder Create a symbolic link to the compressed folder Restore your database with the ldf file pointing at Shrink the log file to an appropriate size Detach the database, move the log file to an uncompressed folder, attach 

Your Excel ninja users will likely prefer Power Pivot, your more typical tabular report folk will feel right at home with Report Builder. There's a useful summary of the differences between Report Builder and the BIDS Report Designer at Designing Reports in Report Designer and Report Builder 3.0 (SSRS).