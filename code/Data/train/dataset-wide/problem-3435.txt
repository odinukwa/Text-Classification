The advice in the comments is completely correct. You really do need to restore from backup, but if the hacker's code looks EXACTLY like your example above you can try this (make a backup copy of your webroot first.) 

As noted in the PHP documentation, a common cause for this in my experience is output buffering by mod_gzip on Apache. Many distributions have this enabled by default now. Mark 

Technically, yes. Though a local DHCP server is generally a better choice. DHCP is a pretty "easy" thing, just setting up a old desktop with Linux on it at each site works well, and will be less failure prone that centralizing everything. If you want to do it, the best way is to connect the sites together via VPN. Either back to a master site, or a node on EC2. Then run a DHCP server on the master site and configure the routers or switches at the other sites to do DHCP forwarding. If the VPN links or the master site goes down, none of the other sites will be able to pull new addresses. Think carefully about the implications before doing so. 

This is generally because cron does not enable the PATH the same way as the shell does. Try typing "echo $PATH" at a prompt. Take the resulting PATH string and add it to the top of crontab file as PATH= That will probably fix it. Mark 

Will depend on how much time/page loads the average user has. Social networking tends to be "sticky" according to Facebook, so people hang around for a while. More page loads means more load on the system. The code behind the site will have another huge effect, better code will put a lighter load on the system. These days if you don't have a good idea how much/how fast your site will grow you might want to consider one of the cloud hosting environments like EC2 or Rackspace Cloud/Slicehost. You can buy two server instances to get started, and add more servers quickly as load changes. Experience with your app is the best way to get a solid idea on how much capacity you will really need. Excess capacity sitting around is expensive, so avoid it if you can. Having said that, 100,000 users isn't a huge load if they only load the page a few times a day. You should be able to get started on that with as little as a single server and probably no more than 2-3 total. 

You could make a script that calls the mysql command line and selects the tables out: Something like (this probably won't run): 

syslog-ng via TCP works pretty well, but as you stated it is limited to more or less single line logs. You might take a look at Splunk, it is expensive but it has log forward agents on each server that can handle almost any format. The GUI is wonderful, and has some very powerful search and reporting tools. 

Is it possible the TZ environment variable is set for your user? TZ will override /etc/localtime with a different timezone. Run "echo $TZ" 

Also make sure CouchDB is listening on the VM's public IP address. By default CouchDB is bound to localhost only. 

It is very likely that either your local firewall is blocking port 25, or your Internet provider is doing so. 

You could setup modsecurity and add a rule to log the POST data whenever it sees the misspelled version. If it is someone editing it, or a SQL injection it should get logged. 

50,000 files should not be enough to cause a significant speed issue on Linux. You mention caching the listing, so I'm thinking you are doing some kinda of processing on the files instead of plain serving. I would look for issues on how you process the files. 

Clients needs hints in the DHCP request to do the Dynamic DNS work, they do not use the DNS settings generally. What is probably happening is the remote sites are using local DHCP server (perhaps running on the routers) that do not provide those hints. Either configure the router for proxy DHCP back to the Windows DC, or setup the local DHCP server to tell the clients to do dynamic DNS. Some instructions fpor the most common Linux DHCP server are here: $URL$ Also you could setup a DC for a subdomain at the remote site, but I'm guessing you don't want to do that. 

From my experience it is temporary, and only effects the command window you issued it in. You have to modify the System settings via Control panel to make it happen for all new windows. 

If it is not going to do SSL decryption almost any piece of hardware built in the last 5 years will be able to saturate a GB NIC. Stick with something new enough you trust its reliability since this is a pretty critical piece of hardware. 

Generally there are two triggers for load balancing. When the site grows beyond the capabilities of one server or when you want eh redundancy and reliability that may come from having two servers in case one may fail. Also, sometimes it can be cheaper to buy/rent two or more low end servers and load balance, instead of one large server. 30k hits a day is less than one request per second, so unless your traffic has a significant peak, or you need the redundancy, it is probably pretty early to be considering it. Mark 

I've never seen the aspmx4 and aspmx5 entries before (but google does have then in DNS so they are probably good.) It might be that you have aspmx2-5 all with the same MX priority of 30. Google's generic instructions are: 

Does anyone have advice or a pointer to articles on how to centralize logs in JBoss? JBoss will log to syslog, which makes it easy, but doing so breaks multi line debug messages (and Jboss loves dropping exception stack traces in the logs). I can rsync the logs, but that isn't realtime. Log4j has appenders for TCP and multicast sockets, so it seems like something probably exists for streaming logs, but I haven't found a receiver for the data. Thanks 

Guessing here (been a while since I saw Mandriva) but if this was Redhat I'd check that you do not have a HWADDR= line in the /etc/sysconfig/network-scripts/ifcfg-eth0. If that line exists it will not apply the eth0 configuration to a network adapter with a different MAC address. 

I don't think packages in Fedora install firewall rules to allow themselves. Generally in the firewall setup part of the install you have to add access for http and other ports. Look at the /etc/sysconfig/iptables script file. In there you will see a rule for tcp port 22 (ssh). You can copy that rule and change the port to 80. Then run "iptables-restore < /etc/sysconfig/iptables" Or just use the firewall configuration tool to add access for the additional ports you need. 

SSH tunnels are a pretty good "quick" solution for getting data thru firewalls but they are designed for interactive stuff like X-forwarding. They don't work well for long term, or bulk transfers. You should probably look at setting up a permanent VPN, if that is within the possible. If not, I would look a reducing the amount of work done over the tunnel, and making it more batch oriented. (grab a little data every hour, and drop the connection between runs) You might also experiment with ClientAlive and ServerAlive settings. This will cause the system to ping over the encrypted channel periodically. This will often keep firewalls from disconnected the idle TCP connection. 

I could be wrong, but I want to say the execute command doesn't run a shell. echo is a command provided by the shell. Try using the "run" command instead of "execute". Also note that both run and execute are deprecated in the newest versions. 

The feed from the generator transfer switch (ATS) to the UPS had a unnecessary fused disconnect on the ATS side, and another in the computer room before the UPS. The fuse in the ATS side disconnect blew on a Friday at 6:30 in the evening. The UPS ran the batteries down until the room dumped. Staring at the problem, someone came up with the brilliant idea of stealing the fuses from the utility input side of the ATS and moving them to the blown output side. The generator was started and the computer room brought back up on generator power. The generator ran until the correct fuses could be located. Subsequent maintenances removed the disconnect, and solved the slight overload that caused the fuse to blow in the first place. 

216.239.32.21 is owned by google, so it looks like they are emitting the redirect to www.vekslers.org. When they say they do not handle "naked" domains I think they mean you cannot serve your site from a naked domain. Instead you have to let them redirect the naked address to the non-naked one as they are doing above. 

Spamassassin can whitelist your domain, but it generally does not (spammers love to forge it). It also has a mechanism for trusting your mail server that might be the issue. Can you provide more details on how you are testing? Are you using SA with a mail server, or just piping your mail into the commandline version of it? 

First idea: if the beta testers will be coming from know IP addresses, just filter based on IP. This could be done at the firewall, or the webserver, or the application. If users IPs will be changing, maybe a second webpage were they can go, enter a username and password, and their IP gets whitelisted inside the beta app (or on the firewall/webserver). Next idea is hiding the beta site behind a firewall, and giving the Beta testers a VPN that gives access. Pretty easy to setup, but now you have the hassle of trying to get credentials and client software out to all of the Beta testers. If they don't work for you directly they may not like installing special software. Fourth idea: Setup squid or a webserver as a proxy. Have that proxy server require authentication before passing the request on to the "real" beta webserver. 

Load average is based on the processes waiting in the run queue. That means if you have processes that use fractional time slices often you can see a high load average without a high CPU utilization. The best example of this is mail. The amount of CPU time require to send a message is very limited, but when thousands of pieces of mail are moving around the system (especially if the mail daemon forks processes to handle each one) the run queue gets very long. It is common to see well functioning, responsive mail servers with load averages of 25, 50 to over 100. For a web server I would use page response time as the primary metric, do not worry about load average. Under modern schedulers load average less than twice the number of cores will usually have no negative effects. You may want to experiment with number of cores per VM versus total number of VMs. Some applications will benefit from many cores on a few machines, others are better at a small number of cores and many instances.