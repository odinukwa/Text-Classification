We don't know. As you said in the question, the National Grid did not answer when asked about it and "being offline" is sufficiently undefined to not answer the question. It's actually four twitter posts in this thread which ask this question and none of them is answered. I searched the press coverage of the events, but none of what I could find answers this question. BBC, Guardian, NY Times, and Fortune. Looking at public available information about the West Burton Power station (the one that needed to be taken offline) does unfortunately not answer this question. Wikipedia, EDF energy, Hadek.com. In absence of specific information of what happened in the West Burton power station on 21st of April I guess that most likely the coal consumption was reduced but maybe not to zero. Coal power stations are known to need some time to stop and again to start again. It all comes down to the technology and economy of coal power plants. Please see the article about cycling of coal power stations cited by Mark in the first comment under the question. The Technical Assessment of the Operation of Coal & Gas Fired Plants by the UK government lists some technical limitations for starting and stopping coal power plants as well as some typical cold/hot startup times in the order of hours. From reading this brochure The Future Role of Fossil Power Generation by Siemens I gather that: 

(The mechanism and dangers in the claim are discussed in one of the a popular science book about statistics and statistical fallacies by Beck-Bornholdt and Dubben (afaik available only in German), either "Der Hund der Eier legt" or "Der Schein der Weisen" or "Mit an Wahrscheinlichkeit grenzender Sicherheit" - I don't have them here, so I cannot look up the exact quote nor the papers they refer to. In any case, these books were published 2001 - 2005, so the general problem is well known since a long time. They also propose a study design for ongoing comparison of medical treatments that avoids the ethical issue of subjecting one patient group to inefficient treatment.) 

This is a (too long) comment about the Swedish study, in addition to particularly @igelkott's link about balancing the risks of death by head trauma and death by cancer due to the unnecessary CT. 

I'm wondering how much can be skeptically concluded from the The Swedish study. They list as participants 

On the other hand, the fact that any breed of dog can be trained for low aggression, does not answer the question of breed specific dangerousness, neither. One would need to study how much training is necessary on average to get to low/high aggression levels for different breeds and then account for the damage they are likely to achieve. So, I'd expect that different breeds have different levels of aggression and "dangerousness". In order to study this, I'd assume that such properties are related to target properties for breeding, and choose a comparison where a large difference can reasonably be expected. E.g. low agression potential for rescue dogs (e.g. Newfoundland; though note that today's breeding targets often developed far away from the working aspect, so that e.g. today's St. Bernhard dogs are usually not considered fit as working dogs any longer, so are German shepherds). Higher bred-in danger for working breeds that were/are used to guard herds also without human supervision against predators as well as theft (aggression potential against unknown humans, size & strength to be effectively dangerous enough to guard the hers). That would probe the "hereditary" part in Kathy's answer. And the last two sentences in ratchet's answer (whose literature tells us to expect higher variance also for certain breeds). So the meta analysis of answers already given points strongly to a "yes". (Jolanta's answer deals with a different aspect of dangerousness which I'd not consider "inherent". Those findings can be in accordance with higher inherent danger, see below). However, I doubt that incidence statistics (of any kind) allow to "measure" danger levels inherent to the breed. The reason is the already mentioned possibility of a positive correlation with respect to agression. It is quite possible (and IMHO probable) that certain dog owners who prefer an dangerous dog will both tend to have breeds that have the reputation of being more dangerous and also train them to exhibit more aggression rather than train them for low aggression (the other part of being dangerous, the level of damage the dog can achieve does depend on the breed. One bite of a St. Bernhard's can cause far more damage than a bite of a chihuahua - though ultimately a newborn may die from a chihuahua's nip into the nose). The answer to the "Is breed a valid method of determining relative safety of a given dog?" sub-question may be yes (personally I think this very likely; but I cannot give "hard" scientific evidence; the studies cited above did not find significance at 5% level [discussion whether p-values are appropriate are off-topic here], but they also found a factor 6 between the proportions of dogs handled by there owner with inadequately aggressive behaviour between Dobermans or Am. staffordshire terriers and golden retrievers (or bull terriers)). The breed of dog may be (personal expectation: is) a good predictor for the aggression level of the dog owner. Meaning that there is a positive feedback loop between alleged or existing aggressiveness and dangerousness and training target characteristic. Which makes the answer to the subquestion a stronger "yes". Again, I have no hard evidence. But detailed studies for dog ban policies could give hints in favour or against this possibility: If this were the case, one would expect 

Most drone strikes have been performed so far the US or UK military. So I guess the claim might be about some of these. Also it is unclear (even from context) which list is meant specifically. There is no complete official list of drone strikes from the US or UK military. Most information comes from NGO organizations like dronewars or reprieve which in turn rely partly on non-exhaustive, summarizing official reports of government agencies, partly on local media reports after drone strikes took place. For example, the UK ministry of defense seems to use the terms insurgent, civilian, child in their reports of drone strikes. They do not specify the age or role of the killed people, instead just count them as fatalities (civilian or combatant). According to the Bureau of Investigative Journalism US officials or spokespersons are quoted speaking about air strikes against "individuals threatening the force" which might be seen as a synonym for combatant. The possibility of hitting individuals not threatening the force seems to be not discussed very often from an official side. A report in the NY Times discussed a disparity of the number of civilian casualties from drone strikes and another article there mentioned that "all military-age males in a strike zone [are counted] as combatants, according to several administration officials". However, the article fails to name the administration officials although it could explain the usual low number of reported civilian casualties. However, this could also be explained by really precise drone weapons, difficulties in determining the status of drone strike victims or deliberate misinformation. The report on Counting Deaths from Drone Strikes created by the Columbia Law School concludes that "the uncertainty about civilian deaths is largely due to the U.S. government’s resistance to openly providing information about strikes.". All in all, I conclude that most official reports available do not specifically mention the status of a drone strike victim and while there is an indication that the US government might count all "military-aged" male victims as combatants (which would make the claim true) it is not sure because there is not enough available information about the details of the process of officially determining civilian and military casualties as well as the total count. It may be plausible, but we don't know for sure. 

You are asking three different things, and I think that is the cause of the confusion here. "As good as new" is something quite different from "safe", and still different from whether replacing the tire it is a good idea. 

Personal statement at the end: this type of problems is not restricted to pharmaceutical industry developing inefficient drugs - it starts much earlier: there's a whole body of literature showing that scientific research practices have lead to large numbers of false-positive findings in publications in (at least) the biomedical field. For a start, see e.g. the Nature news feature Buchen: Cancer: Missing the Mark, Nature 471, 428-432 (2011), doi:10.1038/471428a or Ioannidis' seminal paper: Why Most Published Research Findings Are False, PLoS Med 2(8): e124, 2005 or Forstmeier et al,: Detecting and avoiding likely false‐positive findings – a practical guide, Biological Reviews, 92, 4, 1941-1968, 2017 Of particular interest for this question is e.g.: Begley, C. G. & Ellis, L. M. Drug development: Raise standards for preclinical cancer research. Nature, 2012, 483, 531-533 This is Amgen, i.e. pharmaceutical industry, complaining about (lack of) reliability of scientific publications. 

1000 seed weight is 26.74 g according to Kew Gardens seed database. 0.06 - 0.2 mg CN⁻/g apple seed gives 1,6 - 5.3 mg CN⁻/1000 apple seeds My old paper pharmacology and toxicology textbook (Estler: Lehrbuch der allgemeinen und systematischen Pharmakologie und Toxikologie, Schattauer 1990.) gives 1 mg CN⁻/kg body weight as lethal dose. The CDC info linked by the claim agrees with that. Thus, for the standard human of 70 kg, about 70 mg of CN⁻ are lethal. I.e. eating (and chewing) 350 - 1170 g of apple seeds, which translates to roughly 13000 - 44000 seeds. Now, I didn't find citable numbers of seeds per fruit for apples (the claim assumes 10 seeds/apple - which I judge is a plausible number), but for the claim that 20 apples contain a fatal amount of CN⁻ to be true, with the numbers above, each apple would need to contain 660 to 2200 seeds. I'd judge that this is about 2 orders of magnitude off. 

Just focusing on this part. The other answers already seem to indicate that such an area might be large enough to produce electricity in the order of what is needed. Now this doesn't mean, it's economically beneficial to do that. You can only expect people to actually realize this project if the costs and other aspects are manageable. I will only focus on costs. You would need to buy that much land; prepare the land (flatten it locally and remove obstacles); produce 10,000 square miles of solar panels yourself or buy them; install them; wire them; pay for maintenance like ongoing replacement of worn out panels. Let's only take the costs of buying 10,000 square miles of solar panels. Of course for such a huge project you could expect quite some effect of scale and much lower costs than are paid now. This will add quite a big uncertainty. Taking $0.75 per watt (mentioned as discount price for solar companies in How much do solar panels cost in the U.S. in 2017?) and a total power of 425 GW (taken from this answer), I arrive at roughly 300 billion US dollar. This is of course only a very, very rough estimate but still hints at why nobody built this square so far. It might be just too expensive compared to producing electricity by other means. On the positive side, the figure is not totally crazy (compared for example with the national debt in the order of 14 trillions). 

The intermediate report is available in German: $URL$ and the final report at $URL$ Here's what I gather about laundry machines: First of all, most of the data measures how long the first owner of such an appliance keeps it and why they replace it. However, they cite two Canadian studies on laundry machines with the conclusion that secondary use is a significant part of the life time of laundry machines and also show some more data in full agreement with that. There's also indication that socioeconomic status correlates with replacing machines that are in working order by new/better ones on the one hand and with buying machines that break down early on the other hand. Anyways what the German study shows is that replacements of large household appliances during the first 5 years have increased - in all 3 categories of reasons: because of defects, because the machine is not reliable and because they wanted a better one. This latter category has increased for all 3 timespans the summary (up to 5, up to 11, 11 and more years) and is responsible for about 30% of the replacements. Between 2004 and 2012/13 the average lifespan of laundry machines that were replaced by their first owners because they were broken (75% of all laundry machines) went down from 12.5 to 11.6 years, while the average age of laundry machines that were replaced because the first owners although they were still working well basically stayed at 13.2ish years (earlier numbers are not so accurate because of smaller sample size). Again, there is no indication, how many of these machines are resold/reused (possibly after repair) and for how long. There is a sharp increase in broken machines within the first 5 years (introduction says that early failure is usually connected with issues with quality control), from 6 % over 9% to 15%). There is a slight correlation of machines that fail early with young owners and low income, and those failures also correlate with larger household size (heavier use). Finally, they cite results from Stiftung Warentest who regularly test laundry machines with a program of 1840 runs (which is thought to be equivalent to 10 years use). Test results are shown for 3 different price categories: 350 - 550, up to 700 and > 700 EUR, and they find: