This means that the packet sent to NRPE would have a source address of 172.20.0.2 (which is the Docker container IP, within the docker bridge network). If so, how would it make it through the firewall?! This doesn't quite make sense, and I'm a bit stumped Of course, by setting in the NRPE config gets around the issue, but that's not persistent and doesn't truly solve the issue here. Does Nagios send what it 'thinks' is the "source" IP in the NRPE packet, and that's what NRPE judges the "source" address from? If so, how can that be altered? What am I missing here? My goal is to put the Docker host as the allowed_host as I know that's static and won't change. 

Normally I would assume since it's a remote host, it doesn't have the variables.. however what's odd is that and translate and work fine on the remote host, why doesn't ?? (it comes out empty/null on remote host) 

Now creating the routes is easy, what I don't understand is the DNS settings I need to create in order to allow the machines to properly access the KMS activation server. Any help is appreciated, thanks. 

What a pain! A simple bash script and now I need a container or VM just to do some simple date manipulation. This is making me feel like I'm shooting myself in the foot developing this way, and the whole reason I got a Mac was because it's much closer to a 'nix environment than any Windows version so I could do simple local development on it. My question(s): 

So, the above authenticates users against AD and works as designed. It searches for the user group "VPN Users" within (the default '/Users' OU) If the user has the proper group set then it allows a user in! I want to add a user to another separate OU, for example: Team1/Desktop-users/Standard users could be my new OU with users in it. I want to allow users in that OU as well as the original one. Would this be as easy as adding another object in the config? 

I have an OpenVPN server that authenticates against a single AD domain. Here is my auth-ldap.conf that is used with OpenVPN 

It looks like you're missing the wildcard in the object declaration. It should be instead of when declaring bucket objects 

Firstly, I want to make it clear I don't care about data loss, and I understand the risks involved. What I'm looking for is guidance and if what I am hoping for is even possible. My Scenario: I have 3 1TB SAS drives in the server. Wanting to combine to a main 3TB volume. OS is Ubuntu Server 14.04 I want to avoid using the RAID controller (RAID 0) since I know that if a single disk fails, then the entire array is compromised. I can comfortable use LVM but I'm not sure if it can do what I'm trying to do. My goal is if ONE disk fails, then I lose the data on that bad disk but the other disks continue to operate in the array and the files on the good disks are still available. I know this isn't technically "striping" because no data would span across disks (all blocks in a file on one physical disk) -- Is this possible? One more time to reiterate - lost DATA is OK, but a lost VOLUME is not. If it's possible, great, if not, that's fine too as I am just looking for guidance. 

I needed to inject my intermediary certs. I used $URL$ to easily perform this. It can also be done manually by concatenating the certs together to form the chain, but the above tool does it quick and easy. After the chain was installed as the new cert, it's showing as fully trusted. 

I want to open up TCP to the docker daemon so that Jenkins can build containers against it. I'm getting lots of info about how to do this. Hoping to get the best method. Goals of dockerd: 

Any time I authenticate a system with LDAP/AD I like to have at least one "local" admin that can get in even if Directory Services go down/have an issue. Essentially a "backdoor" just in case the directory service has an issue. Is this possible in Jenkins? I can only see the option to enable 1 authentication method at a time. 

EDIT - I wanted to add some more info on how the registry looks before and after the above deletion takes place. Before the Delete Operation Above: 

This means that the entire image is deleted. Deletion of tags is in an open PR for a future version of the Registry ($URL$ What this means in regards to this question is that the proper method is to delete the image from the repo list is exactly as you assumed. Remove it from disk. (Such as where is the image name that you deleted via API.) It will then be removed from the repo list in and you're finished with your deletion process. There is no need to restart anything like another answer mentioned. When the ability to delete specific tags from the registry is added, then this procedure will change. For now it's all or nothing. Reference: $URL$ $URL$ $URL$ 

Gather image digest: with added to the header on the call The call returns header key with a value such as Using that value from step 2, run the delete call: Registry API returns Run garbage collection manually: Garbage collector deletes the associated blobs from disk (log omitted here, but it successfully deletes the blobs) 

Create a COMPUTER group policy under the OU of the desired computer(s) Edit the new GPO and browse to: Computer Configuration/Administrative Templates/System/Group Policy Enable "Configure user Group Policy Loopback Processing Mode" and set it to "Merge" Add any USER policies you want to enable to this same COMPUTER policy (loopback makes this legal) Ensure the new GPO has security filters to validate the users you want, and the computer(s) you're applying to. 

I have a docker container running web server thatâ€™s listening on port 80 (jenkins/jenkins:lts) I have 2x interfaces on the host, one is intended to be dedicated to the container only (once this is working it will be locked down only to allow HTTP/HTTPS Here is the NIC Config on the instance (This is AWS so they are Elastic Network Interfaces): 

I feel like I've pulled down from other git servers using HTTPS without having to do anything special so I'm not sure what's up in this case. From my workstation, I use SSH so I never run into this, but I just tested HTTPS from my Mac workstation and I get the same message. Does anyone know why git is not trusting my trusted cert? 

If you provided your server config it would really help out. Summary: Ensure your server.conf is configured to assign addresses properly, and it's configured to push your desired routes. 

I see this similar question was asked before, however it was either not answered or it pertained 100% to Windows workarounds. This is specifically Linux and I am trying to restore a Clonezilla image to a 4TB hard drive. I have used this image many times before on other disks, however due to MBR restrictions on this larger drive, it needs to be GPT so I cannot restore the disk image. The exact error that Clonezilla states is "Error: Destination disk size is 4.00TB, which is larger than the MBR partition table entry maximum 2TiB. You have to use GUID partition table format (GPT)" I understand "what" the problem is but I do not know the steps to resolve. I'm going to try cloning the master machine using Macrium reflect and restoring, hopefully it can migrate the partition structure automatically. NOTE: This drive is blank, therefore I have no concern over data loss. Experimentation is fine as I have nothing to lose on the drive. EDIT/UPDATE: So it seems Clonezilla actually allows the process to continue, however it will write everything as-is to MBR and the disk will be seen as 2TB. With this in place, the question then changes to converting the MBR to GPT on the OS/boot volume (yes I am booting into the system that I am working with, data loss is not a problem since I can just restore from image if something breaks, which is has many times so far in my trials) I have opened the disk with gdisk and ran the conversion to GPT. This was succesful, however this kills the GRUB boot partition and indeed the system does not boot after conversion. I am following this: $URL$ What I need help with now: So right now I seem to have restored my Clonezilla image to the new disk in MBR format and have done and in-place conversion to GPT using gdisk. I am now looking for guidance on creating the GRUB partition and re-installing GRUB so that the system can boot after conversion. Once I have this all laid out I can format it to a full step by step answer with all the components together to help others in the future. Thanks!! 

The actual image data is stored in the blobs directory on disk but they are shared between different manifests so it's not safe to just purge that directory out unless you've considered all images that may share the blobs. 

Nagios Server is running in a docker container. It's reaching out for host checks through NRPE running on various hosts within the network. The docker host is 10.10.100.100 Iptables ONLY allows inbound on tcp 5666 from 10.10.100.100 for inbound NRPE checks 

API returns Run garbage collection manually if you don't want to wait for its next scheduled run: Example if running registry as a container: Garbage collector deletes the associated blobs and manifests from disk for you. 

This solved the problem for both the logon script and the GPO drive mount. I ended up ditching the script and just using the GPO. Unfortunately this doesn't identify the root cause of why it failed with the explicit account in the first place. If someone had an environment were they "had to" set it up that way, I foresee them running into much frustration. 

my main question is this: Is the security filtering for a GPO calculated using AND logic or with OR logic to apply the filters? Here is the context of my situation for a background on why I am asking: OU Structure: 

Following the official documentation ($URL$ I have been able to successfully delete an image. As expected, after deleting, the image can no longer be pulled nor its manifest called via API. I feel like I've got the hard part done, however the problem is that the repo is still listed under after the deletion is finished. I'm trying to fully purge the registry. Here is my registry compose file: 

If you post your server.conf it will greatly assist others in helping. Firstly, the errors you are getting are specifically about the ROUTES being pushed, not necessarily the IP assignment. The logs show that it's trying to get an IP, so I assume that's configured on the server, but it doesn't look like routes are. I'll go over both items... IP Assignment: To define what range of IP addresses you get from the OpenVPN server, you set it at the "server" item in your server.conf and, in my working config, defined the topology as "subnet": Example: 

1 AD domain. 3 total Domain controllers, all on 2008 R2 The primary DC (with FSMO roles) is being moved to a new office and it will be offline for a maximum of 3 days. This DC also serves DNS but the other DC also has that role. I am planning to migrate the FSMO roles to secondary server, and then just simply power this one down and boot it back up at the new site. Currently not planning on taking any other steps to prep for this server move. Once the new office is ready and the DC is booted back up, I plan to allow synchronization from the other controllers, and then re-apply the FSMO roles back to it. My question is -- Is my method ok? Is there any other steps I need to take to plan for this? 

Graylog v2.3.2 My goal is to have a condition raise an alert, and the alert remain open until it's marked resolved or a defined resolved condition applies. I have an alert setup and here is the condition configuration: 

Would like to forcefully kick a specific user connection. OS is Ubuntu 16, OVPN Server is OpenVPN 2.3.10 I can see them connected in which lists out current client status. And their persistent connection pool is saved to I can kick all users by simply cycling the openvpn daemon however I want to kick ONE single user. I've tried and as well as searched google but not seeing anything. OS is Ubuntu 16, Server is OpenVPN 2.3.10 

ANSWER TO THE QUESTION: Logical 'OR' -- the GPO will apply if any of the Security Filters match. SOLUTION TO SITUATION: Configure user Group Policy Loopback Processing Mode 

Sometimes I would like to view log entries as a "standard" looking text based log file. Right now sometimes I'll end up going to the server where the logs were generated just so I can display a block of text to follow a timeline of logs a little quicker than going through graylog. This totally defeats the purpose of having central logging, but it's still saving me time. For example, sometimes a script needs to have debug enabled while it parses through a file with 1,000 lines of data in it. Since debug is enabled it literally spits every line it processes into the log files as it parses lines. I would like to view this as a sequential log entry file rather than each and every entry having its own object in the graylog ui. I do realize that it has a "Show Surrounding Messages" function, but that still shows each message in an individual object in the gui. It would be really nice to see this in a standard log view. So that it literally looks like a normal text based log. Use Case: In troubleshooting an application behavior over a 5 minute period which consists of 700 log messages, I don't want to have to expand each message individually. It would be much more human readable if I can view these logs as a "standard" log file rather than through the GUI. Is this possible, and if so, how can I do it? Thanks