Short answer: No. InnoDB clusters via the primary key, and in the absence of a primary key, it picks the first unique index. In the absence of a unique index, it creates a hidden 6 byte key for clustering. When you have the hidden 6 byte key, any secondary indexes refer to this key, rather than exact pointers to row locations (as in MyISAM), so you end up with a secondary key traversal, and then a primary key traversal to find your records. 

The short answer is no: I can't think of a difference from MySQL defaults that would cause this. The longer answer is that it is possible to reduce some locking with READ-COMMITTED as the isolation level + Row-based Replication. To which RDS does not support switching to Row-based Replication :( Whether or not this reduction in locking will help you depends on your schema. See my blog post here under "Write scalability of certain statements". 

You've got to put it in context - InnoDB only verifies the checksums when it reads a page from the block storage device, and updates it before flushing it back to the storage device. While in memory, the checksum is not maintained. For many years, an IO to a disk has taken something on the order of 5-10ms (1ms = 1/1000th of a second). Computing a checksum probably takes somewhere around 50us (1us = 1/1000000th of a second). I don't have the actual data what it is in InnoDB's case, but if you Google "Numbers everyone should know", you'll hopefully agree I'm correct within an order of magnitude. Enter an era now where we have things like Fusion-io flash devices, which on paper have a ~20-30us access time, and you can see that reevaluating the checksum makes sense. My general advice: Don't ruin your backwards compatibility with MySQL releases unless you really need it. Most people do not need it yet. 

According to the MySQL manual, the recommended size is "up to 80% of system memory". Or more specifically, most people will recommend 50-80% of memory. I think you are misquoting the 10% figure for the approximate overhead that will be required on top of whatever you allocate. 

Small starting clarification: the article you linked to on InnoDB text/blob storage is a little out of date with MySQL 5.5, and the barracuda row format. This article is more up to date. On to your question: In the schema you have, each row will be less than ~8K, so you can guarantee across both antelope and barracuda row formats that all data will be stored in-line - not requiring external/overflow pages. If you were to require overflow pages, they are never de-duplicated (which is what I would probably describe your 'pooling' mechanism as). Even worse than they are never de-duplicated, they are never shared... If you could have a record too big to fit inline (~8K limit), each text/blob that needs to be moved out will take a minimum of a 16K page to itself. 

Virtual columns are created on base tables, and will co-exist with columns which will be material. This can be useful as it has more of a mixing/matching behavior than views. Virtual columns permit indexes (which are materialized). Views do not. 

Since the granularity of caching is done at a page level (both in InnoDB and de-facto in MyISAM due to filesystem block), having large 42-column rows means that you will fit fewer rows per page on average and there is no split where the hot sub-set of columns are kept in memory while the inactive ones can not. This results in a sort-of cache dilution, where you may require more memory than if you were to normalize the schema and split into a few different tables. (Note: InnoDB does overflow large text/varchar/blob columns to separate pages. I agree with Rick's comment that InnoDB is the way to go.) 

The entries in the slow_log help you but they can be misleading, e.g. when a connection explicitly or implicitly locks tables. I have seen it many times that the statements showing up in the slow_log are the victims, not the culprits. You already got a very valuable suggestion with switching to InnoDB, MyISAM tables are notorious for locking the whole table for changes, mostly blocking the whole table on any update with default settings. There is however one database engine that has even worse locking than MyISAM and most people will not ever assume it: the memory table engine (HEAP) If a memory table is very active, and you are running low on memory, I have seen ungodly high waiting/lock waiting times on these. If your are very daring you can mount a directory in your MySQL database directory and create a temporary InnoDB database/table, otherwise a regular InnoDB with a somewhat more relaxed write policy will do the trick as well. Regular memory tables do not allow for the use of indexes which sometimes leads to very bad optimization of complex queries involving them. To narrow down what is really causing the queries to wait for a lock, or better to find out what is locking them, you can use or the nice tool. The processes with the highest time values are you most likely culprits. It is also noteworthy that you might find processes that are running way longer than but will never show up in your slowlog, because they consist of multiple statements, all of which may execute fast enough one by one... If you see a number of processes piling up in mytop, you should investigate the list carefully, since this is an easy way of finding the culprit! If locking is introduced trough transactions (implicit or explicit) you can try to optimize the performance by changing your transaction isolation level with . One has to be careful with this since thses changes sometimes tend to break (poorly written) applications, or better said applications that rely on a certain behaviour. So if your locked tables on SELECT statements are locked by design, because someone misused the database as a glorified process lock (I have seen that before) it might break that intended behaviour at the same time as fixing your performance issue... good luck! 

So here some advice I find useful when looking for performance issues like these. Use the -- MYSQL PERFORMANCE TUNING PRIMER -- - By: Matthew Montgomery - to check if your system in healthy, if you can improve parameter, do it first, since it will allow you to better test other options. 

or even might not be long enough to store all possible phone numbers. Depending which country and if a PBX is involved 18 digits are not going to be enough. I have seen quite a number of longer ones already. Even so E.164 recommends to not assign phone numbers with more than 15 digits to phone providers world wide, not all of them adhere to these standards. I am not sure where exactly you are heading with your database, but if you want to store phone numbers I would strongly suggest to use a character based format. In most applications that really use phone numbers you might not only want to store them but also use them. For using a phone number you might need some transformations to add, change or replace prefixes, or maybe filter for certain prefixes, since international phone numbers do not have all the same length it is much easier and faster on the database to use a character based format like or . If speed of character based filter operations (number pattern, prefixes, areacodes, etc) is important for you the typ will usually outperform but you will have to clean up trailing spaces. Internationally there are two common ways to realize direct in dial lines. I would call them "direct dial" and "post connection dial". Any direct dial number can be put into the "regular" number field since they can be dialed in one succession with the carrier assigned number. The other type usually requires a pause before continuing to dial the extension (like in most smaller US legacy PBX systems) these "post dial digits" should go into a separate field, which for better design should be nullable. I would not use the E.164 extension format that you mentioned, it is so rarely used that in myself working in the industry for well over 10 Years I have never seen it in production. 

Is switching to recovery mode a bad idea on a production database, even if there are no other writes to this database at the time this script runs? What damage could be done to existing backup chains? If I remain in recovery mode, how can I prevent the transaction log growing out of control? At the end of each loop, is it worth truncating the transaction log, and/or backing it up? 

I've got a copy of our 130GB production database restored on our development SQL Server 2008 R2 Standard Edition 64-bit. I'm developing some test scripts, which need to de-duplicate a lot of data that has been created in error, so I've got a copy of the database in order to develop and test the de-duplication scripts. The full restore just took 45 minutes and I want to be able to test my script, then quickly restore the database back to the initial state, then fix any issues with my script and re-test the improved database script. Since I'm not on Enterprise Edition snapshot backups is not available to me. What backup and restore strategy could I use to prevent this 45 minute bottleneck? If there is one available, please can you suggest a script for the backup and restore? 

I'm in the process of trying to develop a script that has to delete accidentally duplicated data (due to a bug in some code), from a client's production database. It's quite a lot of data that I need to remove: 600,000 records in the primary table, plus similar volumes in a number of linked tables. The script needs to run as quickly as possible, so I've done things like drop indexes and FKs before the script runs. It basically copies out the duplicated data into temporary (backup) tables, then deletes it from the original table(s). It runs through chunks of 10k records, using the view wrapped around the CTE. It then restores all the indexes and FKs at the end once no records are left in the view. I've also switched to SIMPLE recovery mode before the script runs, but my db administrator asked me not to do this (if possible) on a production environment. If I leave this in FULL recovery mode the transaction log appears to grow wildly out of control. The question is, how can I avoid the transaction log growing way out of control whilst this script runs? I plan on kicking all users off this database whilst the script runs, so we have no risk of losing new data during the run, should I need to restore. I need to have a backup of the database before I run the script and I need to have a backup point after the script has run. The changes to the data in between (as the script runs), seems to be irrelevant to me. The transaction log of that stuff could almost be thrown away (unless there is a reason why I might need it, but I can't think why?). I'm aware I can't use anymore when I'm in full recovery mode. Should I shrink the transaction file after every run of the loop? My script is below. I would welcome any suggestions in how I can improve the script (it is somewhat anonymized from the original), and most importantly, how I prevent myself from impacting the integrity of the data and current backups. This is live customer data, so it is really important I don't screw this up. I'm a programmer and not much of a DBA, so I can write the odd SQL script, but this is pushing the boundaries of my comfort zone!