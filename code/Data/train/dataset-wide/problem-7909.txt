Take $X$ to be the “infinite-dimensional dunce’s cap”, with a unique non-degenerate simplex $x_n$ in each dimension, and with every face of $x_n$ equal to $x_{n-1}$. Explicitly, $X_n = \coprod_{m \leq n} \mathrm{Surj}([n],[m])$. So it’s clear that this has finitely many simplices in each dimension, but infinitely many non-degenerate ones in total. 

This is heuristics only, not a rigorous argument, but I would strongly expect that if $D$ is sufficiently co-complete, then very little restriction on $J$ is needed. The “moral adjoint functor theorem” says that if a functor preserves all limits, then it is probably a right adjoint. The inclusion functor $[C,D]_J \to [C,D]$ creates (hence reflects) all or most limits that exist in $[C,D]$, since they’re usually pointwise — always pointwise, if $D$ has a 1 and sufficient coproducts, since then we can run a version of the Yoneda lemma — and limits commute with limits. So assuming some co-completeness for $D$ and smallness for either $J$ or $C$ (I guess something like “$J$ or $C$ is bounded by some $\kappa$, such that $D$ has all colimits of size $< \kappa$” should suffice), then one of the genuine adjoint functor theorems should imply that the inclusion of $[C,D]_J$ has a left adjoint. 

Is anything known asymptotically about the binary "primes mod 3" sequence besides Dirichlet's result that 1 and 2 occur half of the time? For example, can you prove that it does not eventually cycle forever with a simple "12" repetition? (I would guess that this sequence is asymptotically random with no correlations, but it really wouldn't be that surprising if, for example, there were some tendency to switch back and forth in consecutive terms. If anyone has an argument, or data, indicating that this sequence is not so random, I'd like to hear about it.) 

Given coprime positive integers M,N, and a corresponding integer z outside of the range (for all integers x,a,b,c) of $Mx^2-N(a^2+b^2+c^2)$, is there any such z which is "deceptive", meaning that it is modularly inside the range of the smaller components, i.e., inside the range of both $Mx^2$ mod N and $-N(a^2+b^2+c^2)$ mod M? Feel free to replace $a^2+b^2+c^2$ with "All integers except for $4^a(8b+7)$" per Lagrange. 

I'd agree that the result is true, and “well-known” to category-theorists! Unfortunately I don't know a specific reference, but my best guess would be something like Kelly’s “Elements of Enriched CT”, which proves lots of useful things about (co)ends and weighted (co)limits, which specialise to lots of useful things about limits. It's also hard not to wonder about the legendary treatise of Chevalley on “all possible properties of limits” that got lost in the mail... If you can't find a reference, though, the proof can certainly be made pretty short — I used 6 well-spaced or 2 cramped lines, and it can probably be compressed further... [this was meant to be just a comment, but it got a bit too long] $\newcommand{\C}{\mathbf{C}} \newcommand{\D}{\mathbf{D}} \DeclareMathOperator{\colim}{colim}$ Edit: For clarification, the precise statement I had in mind is that $$\colim_{I}\ (\colim_{\C_i}\ F_i)\ \cong\ \colim_{\left( \colim_{I} \C_i \right)} [F_i]_{i \in I}$$ where $I$ is a small cat, $\C_i$ is an $I$-indexed diagram of small cats, $F_i : \C_i \to \D$ is a co-cone of functors, $[F\_i]\_{i \in I}$ denotes the induced cotuple functor $\colim_{I} \C\_i \to \D$, and all the relevant colimits exist in $ \D$. 

I believe the current lowest-memory algorithm for computing the $n^{th}$ binary digit of $\pi$ requires $O(log(n))$ bytes and $O(n^2 log(n))$ days (I pick Bellard over Bailey–Borwein–Plouffe for speed). Can any of the common irrational constants be currently computed with the same low memory, but in faster time? Please consider constants which combine integers and $\pi$, natural exponential functions, and/or root functions (e.g., $\pi$, $\pi^2$, $e$, $e^{-\pi}$, $\sqrt2$, $\sqrt{2\pi}$). For any such constant, I am also curious about the randomness in its binary tail. These digit extraction constraints cause correlations in the constant's tail bits which probably go to zero for high $n$ (but, for $\pi$, the Bellard/BBP constraints are too subtle for me to conclude anything). If anyone has a low-memory method to distinguish a tail of some common irrational number from random bits (with the same speed in the limit of high $n$), please comment. 

Is there a positive 128-bit integer whose square has all middle bits equal to 1? (The "middle bits" are naturally the 65th bit through the 192nd bit, defining the 1st bit as the least significant bit of the full integer.) 

The simple difference: 0 is always indecomposable by Lambek and Scott's definition (since any map into 0 is epi), but never by the Elephant's (since the uniqueness condition won't hold; or by considering when the coproduct decomposition is empty). So, let's temporarily change one of the definitions to fix this. I'd suggest we add “…and the map $0 \to X$ is not epi.” to Lambek and Scott's definition. (As you noted, their binary condition generalises to a $k$-ary one; this is just the case $k=0$.) In eg Top, however, we can see that the Elephant def still doesn't imply the LS def. $[0,1]$ satisfies the former (it's not decomposable by an iso), but not the latter (it is decomposable by an epi). Even more, it’s decomposable by a regular epi (more on this distinction below). Conversely, the LS definition doesn't imply the Elephant one either; it fails in eg $\mathbf{Set}^\mathrm{op}$, since in $\mathbf{Set}$, $0$ is co-decomposable by iso ($0 \cong A \times 0$) but not co-decomposable by monos (for any map $(f,g) \colon 0 \to A \times B$, not just one but both of $f$ and $g$ are mono). When do they imply each other? If we upgrade the LS definition to involve regular epis, then in a regular lextensive category, it implies the Elephant definition, if I'm not mistaken. For this, suppose $X$ is “indecomposable by reg epis”, and suppose $X \cong A + B$ — WLOG $X = A + B$. The coproduct inclusions are then jointly reg epi, so one of them is reg epi. But it's also mono (in a lextensive category, every coproduct inclusion is a pullback of $1 \to 1 + 1$, so is mono); so it's iso. There's a little more fiddly stuff to check involving messing around with $0$, but it's all the same sort of thing. Edit from Mike Shulman's comments: if moreover we're in a pretopos, all epis are regular, so there the original LS definition will imply the Elephant definition. On the other hand, the Elephant definition doesn't imply the LS even in a topos: the terminal object of $\mathbf{Sh}([0,1])$ is a counterexample, essentially for the same reasons that $[0,1]$ was a counterexample in $\mathbf{Top}$. However, the two definitions are equivalent for projective objects… and I guess that's how this situation has arisen, since a common use of indecomposable objects in topos theory is the theorem that the indecomposable projectives in a presheaf category are exactly the retracts of representables. (This is useful because it lets us recover the idempotent-completion of $\mathbf{C}$, which is very close to $\mathbf{C}$ itself, from $[\mathbf{C}^\mathrm{op},\mathbf{Set}]$.) 

I assume that this optimization problem is not studied much in math since there was no response to my original post (please see my original version for a few more details though its solution used more steps). So, is there a solution with fewer steps than 13? Better yet, is there any systematic method/theory for finding a minimal 128-bit solution? Notice that I could have asked this problem for any number, like 99 (instead of pi), so I was hoping that there might be some number theory devoted to it. To be clear, in this problem, for each step (e.g., defining x08), you are free to choose two previously-defined variables (e.g., chosen from x01 to x07), their two bitshifts (integers from -infinity to +infinity), and their two inclusion signs (positive or negative). The code above uses # signs to show these six variable columns. 

What is the maximum number of spheres that can be placed in 3D such that all inter-touch? One can of course place four unit spheres tetrahedrally and then add a smaller sphere in the middle, so this number must be at least 5. [By the way, I was trying to extend the "five points in 2D cannot be inter-connected without a crossing" limitation to 3D with a simple statement, but this was sadly the best I could do. If anyone knows a better simple extension, please comment.] 

(Prompted by reflection on this old answer, and its suggestion of the “harmlessness” of the axiom of regularity.) In ZFC, one may justify the axiom of foundation (AF, aka the axiom of regularity) as being “convenient, and harmless”, as follows. If $V = (V,\epsilon)$ is a model of ZFC–AF, then its well-founded part $V_{wf}$ models ZFC. Moreover, ZFC–AF suffices for the standard proof of the well-ordering principle, so $V$ believes that every set is isomorphic to some von Neumann ordinal, which will lie in $V_{wf}$. Hence the inclusion $V_{wf} \hookrightarrow V$ underlies an equivalence of their (internal, large) categories of sets; so any “purely structural” statement should hold in $V$ if and only if it holds in $V_{wf}$. “Purely structural” can be made precise in various reasonable ways (e.g. any statement expressible in the language of categories/elementary toposes/higher-order logic/type theory, interpreted into set theory), and includes essentially all of ordinary mathematics, excluding only a few explicitly material-set-theoretic statements such as AF itself. Summing up: given any model of ZFC–AF, one can replace it by its well-founded part, which is now a model of ZFC, believing the same purely structural statements as the original model. So relative to ZFC–AF, AF has no purely structural consequences — i.e. it’s not quite conservative, but pretty close to it. However, this argument relied essentially on the axiom of choice! Relative to ZF–AF, is AF still harmless, or does it have some structural consequences? Precisely: is there some “purely structural” statement $\varphi$ (in one of the senses above, or some similar sense), such that $ZF \vdash \varphi$, but $ZF-AF \not \vdash \varphi$? I’m also interested in the same question with ZF weakened to IZF, CZF, and similar theories. Over these of course AF should be replaced by $\in$-induction (a classically equivalent and constructively better statement), as described in this older question (which considers closely related issues to the present question, but doesn’t as far as I can see directly imply an answer here). 

I have many polynomial equations in many variables which I want to jointly minimize (in a mean square sense, but you could pick a different reasonable measure which favors anything where all quantities go to zero). For example, I am looking to make the 6 equations below as "small" as possible (a-j are unknown real numbers). This example probably actually has a solution where all equations are zero, but I also have cases which have no zero solution, so I'd rather not do the "repeatedly eliminate variables and solve for the quadratic root" approach (also, this approach takes too long; is there even any machine which could find a full zero for these equations within 10 minutes?). I'm thinking there might be some software tool that considers the "terrain" smartly and is locally minimizing on many global fronts...or maybe that is impractical. So, is there a free math tool (like Sage) which can minimize things for me (and be certain that no other point is better within some tolerance)? I'm open to theoretical advice, but feel like the options will all look like brute force. Should I give up if I need to minimize a similar set of equations within 10 minutes on one machine? 

I don’t know about your first question; but for the second one, the answer is no — these structures can’t be axiomatised by algebraic identities. If they could be, then any product of such structures, with the natural induced operations, would again be one. But this is not the case: if $S$, $T$ are any such structures with $0 \neq 1$ in each of them, then the resulting operation $\nu_{S \times T}$ on their product will satisfy $\nu_{S \times T}(0_S,1_T) = (0_S,1_T)$, which is equal to neither $0_{S \times T}$ or $1_{S \times T}$. So $\nu_{S \times T}$ does not satisfy the desired defining property. The big picture here is Birkhoff’s HSP theorem: a class of algebraic structures, over a fixed language, can be axiomatised by algebraic identities if and only if it is closed under arbitrary products and subobjects (in categorical language: under all limits), and under direct images along homorphisms. 

Yes. …at least, for Leinster’s reformulation of Batanin’s definition of globular operadic weak ω-category (and hence also for the finite-dimensional versions of this). Showing this is essentially a matter of repeatedly applying one lemma: if $\mathbf{T}$ is an essentially algebraic theory with a computable presentation, then the free $\mathbf{T}$-structure on a computably presented object is again computably presented. By “computably presented”, I mean essentially that the sets of operations and axioms are all computably enumerable. In the Leinster/Batanin definition, one starts with strict $\omega$-categories (certainly a computably presentable theory, by the standard explicit axiomatisaion); by their observation above, their monad $T$ is computably presentable; from this, one can show that the theory of $T$-operads is computably presentable; similarly, then, the theory of $T$-operads-with-contraction; so the free $T$-operad-with-contraction $L$ is computably presentable. But now the operations of the theory of weak $\omega$-categories are the elements of $L$; and the axioms are given by elements of “powers” of $L$, in the monoidal structure $\otimes$ built by $T$ and pullbacks; so these sets are all computably enumerable, so we’re done. 

It sometimes happens that 1D problems are easier to solve by somehow adding a dimension. For example, we convert linear differential equations for a real unknown to a complex unknown (to use complex exponentials), or we compute a power series' radius of convergence by thinking in the complex plane (or use complex analytic properties in path integrals), or we evaluate $\int^\infty_{-\infty} e^{-x^2}\ dx$ by squaring it... So, are any 2D problems easier to solve in even higher dimensions? I can't think of any. 

Is there any integer N such that 2^N=3 mod N? I understand that N must be an odd non-prime. I checked up to a million with no success (but, FYI, 2^N=5 and 2^N=7 have solutions). 

How can I most quickly find a big prime, p, for which 4p+1 is also prime? For example, p=37 works. I wonder if these special primes have been researched and some characteristics are known. Are there infinitely many of these primes?