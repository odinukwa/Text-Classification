Let $A_1,\dots,A_L$ be $N\times N$ hermitian matrices. Define the simplex \begin{align} \mathcal{S}=\left\{[x_1,\dots,x_L]\mid x_i\geq 0,~\sum_{i=1}^{L}x_i=1 \right\} \end{align} and consider the function over the simplex \begin{align} f(x_1,\dots,x_L)=\lambda_{min}\left(\sum_{i=1}^{L}x_iA_i\right) \end{align} where $\lambda_{min}(.)$ denotes the minimum eigenvalue. What are the properties of this function? 

Let $A_i$, $i=1,\dots,L$ be given $N\times N$ positive definite real matrices. I have this sum of exponentials \begin{align} f(\mathbf{x})=\sum_{i=1}^{L}\operatorname{exp}(-{\mathbf{x}^T\mathbf{A}_i\mathbf{x}}),~~~~~~\mathbf{x}\in \mathbb{R}^N,\mathbf{x}^T\mathbf{x}=1 \end{align} Has this function been studied before. Can someone point me to relevant references?. Or anyone can make some comment on it as if it is convex or concave? 

Suppose we have a primal problem $$ p^{*}=\min_x f(x), \\\text{s.t.}~~ h_i(x) \leq 0, $$ where $f(.)$ and $h_i(.)$ are possibly non-convex. Then its Lagrangian is $$\mathcal{L}(x,z_i)= f(x) + \sum_i z_i h_i(x)$$ and the dual problem is $$d^{*}=\max_{z_i\geq 0} \min_x \mathcal{L}(x,z_i)$$ Now it is well-known that $p^{*}\geq d^{*}$ and that the dual problem is convex. Is this the best possible convex lower bound in general? Are there instances of well-known problems where we know that there exists a better lower bound which also comes from a convex optimization problem. 

In my problem, I came across numerical ranges of rank-one positive semidefinite matrices. Through Toeplitz-Hausdorff theorem and some other extensions, I know if there are at most three matrices, then the numerical range is convex for any given set of hermitian matrices. Is there any results on rank-one positive semi-definite matrices? 

The following is the proof for it which I can't verify. Proof: Consider the following step-wise procedure whose inputs are $X$ and $A$. $~~$0.$~~$ Inputs are $X$ (given $X\geq 0$, $rank(X)=r$) and $A$ (symmetric). 

Let $\mathbf{A}_1$ and $\mathbf{A}_2$ be two $N\times N$ hermitian matrices. Their Joint Numerical Range is defined as the 2-D set \begin{align} \mathbb{S}_2=\{[\textbf{u}^H\mathbf{A}_1\mathbf{u},\textbf{u}^H\mathbf{A}_2\mathbf{u}]\in\mathbb{R}^2, \mathbf{u}\in \mathbb{C}^{N\times 1},\mathbf{u}^H\mathbf{u}=1\} \end{align} By a famous theorem of Toeplitz-Hausdorff, $\mathbb{S}_2$ is a closed compact convex set. Does the converse hold? I mean, can every closed compact convex set in 2-D be the joint numerical range of any two hermitian matrices?. 

I have an engineering back ground. Due to work, I came across this problem \begin{align} &\max_{\lambda,y_i\in \mathbb{R}}~\lambda \\\ s.t.~&\left(\mathbf{A}_0+\sum_{i=1}^{K}y_i\mathbf{A}_i\right)-\lambda\mathbf{I}\geq 0 \end{align} where $\mathbf{A}_i$ are all hermitian matrices. We are seeking $\lambda$ and $y_i$. I know that this is called a Linear Matrix Inequality problem and can be solved by a general convex package (for eg, CVX). To me, it seems like we are looking for a matrix formed from the linear combination of given hermitian matrices whose smallest eigenvalue is as maximum as possible among all such combinations. I was wondering if they are iterative algorithms to solve this problem which are simple to implement. Please point me to relevant references. 

Let $A_1,A_2,\dots,A_M$ be given $N\times N$ hermitian matrices. The numerical range is defined as the set \begin{align} \mathbb{S}=\{(u^HA_1u,\dots,u^HA_Mu)\in \mathbb{R}^M\mid u^Hu=1\} \end{align} By toeplitz hausdorff theorem, $\mathbb{S}$ is convex for 

Let $\mathcal{J}$ be a closed, bounded, compact, convex set in $\mathbb{R}^L$. (Notations: vector $\mathbf{x}$ is denoted in bold letters and its $i^{th}$ co-ordinate is denoted as $x_i$. $\mid\mathcal{S}\mid$ denotes the cardinality of set $\mathcal{S}$.) Consider the following problem \begin{align} \max_{\substack{\mathcal{S}\subset\{1,\dots,L\}\\ \mathbf{x}\in\mathcal{J}}}\,\mid\mathcal{S}\mid \\ x_i\,\leq\, -1,~i\in\mathcal{S } \end{align} Is there a way to rewrite this problem in terms of variables rather than the cardinality? For instance, one way I am familiar is \begin{align} \max_{\substack{v_1,\dots,v_L\\\mathbf{x}\in\mathcal{J} }}&\sum_{i=1}^{L}v_i\\ &v_i(x_i+1)\leq 0,i\in\{1,\dots,L\}\\ &v_i\in\{0,1\},\,\forall i \end{align} Actually I am trying to derive the the dual of this problem. Any suggestion in that regard would be really helpful. 

Let $\mathbb{S}$ be a closed and bounded convex body in 2-D with some non-empty intersection with positive quadrant and let it also contain origin. Let $c>0$ be the right-most point on the x-axis such that $(c,y)\in \mathbb{S}$ for some $y$. Define the function \begin{align} f(x)=\max_{(x,y)~\in~\mathbb{S}}y ~~,x\in[0,c] \end{align} Clearly,for a given $x'$, $f(x')$ is the northernmost point in the vertical strip $x=x'$, is $f(x)$ a concave function? (or does it have some nice properties.). If you look at $f(x)$, it is the pointwise supremum of an affine function. And also, all the examples I can imagine is concave. 

Matrices I discuss are all $N\times N$ hermitian matrices. Define two positive (semi)definite matrices $H_1$ and $H_2$. Define the following matrices \begin{align} P_1&=H_1+(I+H_2)^{-1} \\\ P_2&=(I+H_1)^{-1}+H_2 \end{align} I was just curious if there are any connections between them. It can be from any perspective, eigenvalues, rank, eigenbasis, simultaneous diagonalization or any such concept. Even special cases are welcome, for instance, say they are rank-one matrices, Does it make any difference? Context: This arises as a (very) special case of problem formulation in multiuser wireless communication. $H_1$ is a kind of signal Covariance matrix and $(I+H_2)$ is a kind of interfering signal's covariance matrix. if you take every parameter in the system to be one, you get $P_1$ at user1 receiver and you get $P_2$ at user2 receiver. Then this will become the constraint matrices in a homogenous convex quadratic programming problem with two non-convex homogenous quadratic constraints. 

This arose from a discussion with a friend (people involved are two engineers) who argued that every result in mathematics should be transformable into another branch. For example, he argued that Pythagoras theorem can be proved using tools of probability. Another example is that, he believed there should be a way to transform every result in algebra to calculus and that this should be known to the core mathematicians. Us being engineers may not be able to appreciate the breadth and depth of it. How much truth is there in this? 

Decompose $X=RR^T$. Generate the eigen decomposition $R^TAR=U\Lambda U^T$. Let $h$ be any $N\times 1$ vector such that $\lvert h_i\rvert=1$ (each entry of $h$). Generate the vector $x_1$ and matrix $X_1$ as \begin{align} x_1&=\frac{1}{\sqrt{r}}RUh \\ X_1&=X-x_1x_1^T \end{align} Outputs are $X_1$ and $x_1$. 

Consider the following result which I recently came across in a research paper in my area (Signal Processing) 

My question, 1) how do I explain this behavior 2) Any comments on $t$ where the maxima occurs? Some Thoughts: I have a feeling that this is connected to the Toeplitz-Hausdorff theorem. From it, it follows that the subset of 2-D plane \begin{align} \mathbb{S}=\{[x_1,x_2]\in \mathbb{R}^2\mid ~x_1=x^HA_1x,~x_2=x^HA_2x,~x^{H}x=1\} \end{align} is a closed compact convex subset of $\mathbb{R}^2$. Thus, it follows that minimum eigenvalue of $M(t)$ is \begin{align} \lambda(t)=(1-t)x_1+tx_2, ~~[x_1,x_2]\in \mathbb{S} \end{align} Now from here (if this is correct?), How do I conclude the observations, I made earlier? 

Let $C_1$,$C_2$,...$C_N$ be $M \times M$ indefinite hermitian matrices. What can we say about the following quadratic constriants \begin{align} w^{H}C_1w>0 \\\ w^{H}C_2w>0 \\\ ...~~~~~~~~~~ \\\ ...~~~~~~~~~~ \\\ w^{H}C_Nw>0 \end{align} where $w$ is a non-zero $M\times 1$ complex vector. Can we come up with conditions for existence of such a $w$? I tried using the CVX package to determine the same (after using semi-definite relaxation). It always gives the zero vector as the answer. 

Let $A$ be a given symmetric positive definite $N\times N$ matrix. I need to find a symmetric positive semi-definite matrix $S$ which is the solution to the following optimization problem \begin{align} \max_{S}~&\det(A+S) \\s.t.~&\sum_{i}^{N}\sigma_i(S)\,=\,c \\&S\geq0 \end{align}where $\sigma_i(S)$ are the singular values of $S$ and $c$ is a given positive constant. Note that I already know that this can be converted into a standard convex problem by replacing the constraint on sum of singular values by trace and also considering log-determinant. However, I am interested in a different approach. My hunch is that following is the solution. Let $A=U_a\Sigma_aU_a^H$ be its Eigen-decomposition (EVD). Since both $A$ and $S$ are symmetric positive semi-definite, their EVD and SVD are same. Let $S= U_s\Sigma_sU_s^H$ be its EVD which we are seeking. Then first part of the solution is $$U_s = U_a$$ It only remains find $\Sigma_s$ which contains the singular values of $S$. Substituting this in the original optimization problem, it breaks down into the simple form of finding the variables $\sigma_i(S)$ from the optimization problem \begin{align} \max_{\sigma_1(S),\dots,\sigma_N(S)}~&\prod_{i=1}^{N}(\sigma_i(A)+\sigma_i(S)) \\s.t.&\sum_{i=1}^N\sigma_i(S)=c \end{align} This is also equivalent to maximizing the $\log$ of the objective since the objective is a increasing function of each variable. Let us substitute $$x_i=\frac{\sigma_i(S)}{\sigma_i(A)}$$ Also, we can use the fact that determinant is the product of singular values for a symmetric positive definite matrix. Combining all this, we have the optimization problem \begin{align}\max_{x_i}\sum_{i=1}^{N}~&\log(1+x_i)+\log\det(A)\\s.t.~&\sum_{i=1}^{N}\sigma_i(A)x_i\,=\,c\end{align} The constant term in the objective can be neglected. This is an instance of a very famous problem in wireless communications where one tries to maximize the capacity of set of $N$ wireless channels subject to power constraints in each. Its solution is very well known and is typically referred to as the water filling solution. So I know the process after reaching this point. Note that all this was facilitated by the assumption that $U_s = U_a$. Is that true? How do I prove this?. If not, what are other approaches I can try? 

Consider functions $f_i(x)$ that map $\mathbb{R}^n$ to $\mathbb{R}$ for $i\in{1,\dots,k}$. Assume these functions are monotonically increasing in their arguments and continuous everywhere Also, one can assume that they are bounded. Consider the set of inequalities $f_i(x)\leq 0$. What are some properties of the feasible set of this set of inequalities defined as $$\mathcal{F}=\bigcap_{1\leq i\leq k}\{x~|~f_i(x)\leq 0,~\forall i\}$$ For instance, consider the case of only one function ($k=1$). Let $x=(x_1,\dots,x_n)$ be a feasible point such that $f_1(x)$. Then define the set $$\mathcal{S}_x=\{y~|~y_j\leq x_j\}$$ Essentially $\mathcal{S}_x$ is set of all vectors that are component-wise lesser than $x$. This is a convex set. Moreover, due to monotonicity of $f_1(.)$, all points in $S_x$ are feasible as well, i.e. $\mathcal{S}_x\subset\mathcal{F}$. In fact, we can define the set $$\mathcal{A}=\{x~|~f_1(x)=0\}$$ It is easy to see that the set $$\mathcal{B}=\bigcup_{x\in \mathcal{A}}\mathcal{S}_x$$ is also a feasible set ($\mathcal{B}\subset\mathcal{F}$). Due to monotonicity of $f_1(.)$, we can prove that $\mathcal{B}$ is the only feasible set ($\mathcal{F}\subset\mathcal{B}$) . However, I am not able to generalize this kind of arguments beyond one function ($k>1$). Appreciate any help in this direction