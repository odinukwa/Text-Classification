Also, some taxes are calculated on the base amount and others are compounded (applied to other taxes) as well. These are some of the kinds of rules that apply in the jurisdiction where I live. You might have others as well. I would suggest not limiting yourself to two applicable taxes per item. Instead, I would create an intersection table (many-to-many) that indicates which taxes are applied to each item. This could be driven by a similar intersection table between PRODUCT and TAX to indicate which taxes are usually applicable to each product, if that is an important distinction in your case. While adding another table may not seem like a simplification, it will actually make your code simpler and easier to maintain because you are normalizing your applicable tax data. The design you have illustrated isn't even in 1NF, which is almost always a recipe for trouble. 

NOTE to people with itchy downvote fingers: I know that OP has asked about MongoDB and the answer that follows is RDBMS. However, if you check out the comments you'll see that I did ask why MongoDB and OP's answer is a presumption of necessity due to performance. Since nobody has come forth with a Mongo-centric answer in four days, I am going to offer my suggestion, which is to let RDBMS do what it's good at. 

I have read that this should be done with caution because it can take a long time in a large table. See also the documentation for AUTO_INCREMENT. 

The third option is to keep a transaction log of points transactions. Use the schema from your first option to record the "going rate" for points. Whenever you have a transaction that either gives or takes points away from a user, write a record (or multiple records) into a points transaction log table. The transaction log table will have a FK to and a date/time to indicate when the transaction took place. It could also have FK's to the items which generated the points, such as FK's to your and tables, that's up to you. Most importantly, the transaction log table will have a column for the number of points awarded(+) or spent(-). To get a user's point balance, you run a sum of all transactions for the user up until the current time. The important thing is to keep a record of how many points were given or taken for that user, at that time regardless of what may happen to other users or at other times. If performance becomes an issue, you can keep the user's current point total on the table, but this should be denormalized data based on the transaction log details. If you do this, then you need to have controls in place to make sure you're managing this redundancy appropriately. 

If the concern is manual maintenance of a large number of rows, you could solve this (potentially) by creating tables for each of your dimensions: weeks, time slots, and venues. The complete set of possible slots for booking would be the cross product of these three dimensions. Actual bookings would be another table with foreign keys pointing to all three of these dimension tables. With this type of design, instead of maintaining 3 x 25 x 26 records you will maintain 3 + 25 + 26 records. Note that if you segregate day of week and time of day into two tables you can reduce the number of records to be maintained even further (3 + 5 + 5 + 26). The problem with this approach is when (and if) you have an exception. This design assumes that there are no blackouts in your schedule. For example, what if you close a room for a couple of weeks to be renovated? One way to handle this issue is to create booking records that cover the blackouts. If you have enough exceptions, then managing them may be almost as bad as just using the brute force method. The question I would seriously consider is whether or not generating the initial list of slots available for booking is really that big a deal. You could easily automate the process to generate your big pile of available slots. This is really just a single query. 

The answer is a little bit like the old joke: "Doctor, doctor! It hurts when I do this." If you want a foreign key to an entity then that entity needs to have a unique key defined to be referenced. The fact that your table has a unique candidate key in should not prevent you from defining another candidate key that combines ExpectationId and BranchId. This seems like a violation of third normal form, and technically it is. However, that is only true because you happen to define ExpectationId as unique. You could have defined ExpectationId as a sequence within BranchId, such that ExpectationId is not unique on its own. As it is, you can think of it as the table defining the combination of two unique values as being the fact of interest. In any case, if you want to define a foreign key to that is the combination of and then there is no way around it but to define the primary key that way. 

Store the running time as an integer in total seconds. For display purposes you can convert the number of seconds into a string in hours:minutes:seconds format or whatever display format you choose. 

When drawing ER diagrams, I have used the following graphical convention: Label the relationship lines with the foreign key column name(s), like so: 

You need to normalize your schema. Your table is trying to do too many things. Consider this schema: 

Your first design is the way to go. Keep everything about a process on a single record. Your challenge isn't one of database design, it's one of restartability/disaster recovery. Since your processes are long-running, you have a higher risk of the data getting out of sync with the processes that are being tracked in the data. For example, you lose power causing a process to crash. The solution is that you need to change your processes so that when they start they confirm what the data says. Does the database say that process X is still unfinished? Is process X still running? If not, your data is out of sync and you need to mark the record for process X as failed. You might want to consider adding a couple of columns to your first table design, for example: 

You may want to recognize directional relationships (e.g. "is the mother of", "is the son of") or you may need to acknowledge that relationships are not bilateral insofar as the users don't grant each other exactly the same rights. In that case you need to keep two records in for each relationship. Otherwise, I would just capture each user linkage once. Union queries take about twice as long as single queries, but when these queries are built on fast operations like index seeks this isn't so bad. In any case you'd be trading this off against twice as much storage, which mitigates the impact slightly. I'd be much more concerned about the potential headaches of managing the redundant data (if it is indeed redundant). You could pick a convention like lowest first to make certain kinds of operations more predictable - such as updates and deletes. 

Both designs are technically 3NF, depending on your data population. What a lot of people who have only a cursory understanding of formal normalization theory don't understand is that redundancy in foreign keys isn't really redundancy by definition. Transitive dependencies and partial dependencies apply to the way non-key attributes relate to key attributes. If your composite keys happen to contain (or even consist of) foreign keys then whether your foreign key has the look of being denormalized isn't relevant. (i.e. Design 1 isn't redundant in a way which is significant to the formal definition of normal forms.) The practical issue comes down to what the values of your keys are. If you have natural keys and the key consists of =123 and =1 - i.e. your IDPart values are natural number sequences which are only unique within the context of a given IDObject value, then you need to use design 1 (weak entities). However, what a lot of people do for good (and less good) reasons is apply surrogate primary keys to every or nearly every table. I'm not going to get into whether or why to use surrogate keys. That is a holy war for another question. However, if you happen to have table-wide unique (e.g. IDENTITY) values for IDPart, then your primary key for is more complex than it needs to be using design 1. In that case, there is no need for in the PK of because the table already has a unique key just using . In such a case, design 2 is sufficient and simpler, therefore better, as a rule of thumb. In practice, I would say weak entities are much more common in logical models than in physical models, if for no other reason than that most physical models tend to use surrogate primary keys. 

Assuming you have some kind of transaction table containing events that occur at various times throughout the day, then the best thing is to start by recording these events with the full date and time to whatever precision you like in one table. To group these events into time periods for reporting, you can create a reporting period table. This table should have a start time (of day) and an end time (of day). Then you can join your event records to your reporting period records using something like... 

There are two ways to look at this, the informal way and the formal, theoretical way. The Informal Way: If something is of so much interest to your system that you want to record attributes about it, then it probably belongs in its own table. If the only thing you care about Social Security Number is what digits it contains (i.e. what the number is) then this number is best modelled as an attribute of something else (e.g. PERSON). If, on the other hand, your system cares about other facts that pertain to SSID, like maybe when it was issued, which office issued it and whatever else you might know and care about an SSID, then you probably want to split everything you know about the SSID into its own table and then relate that table to PERSON. The Formal Way: If the rules of normalization demand that you split your multiple facts (attributes) about an SSID into a separate relation (table) - of which the social security number is the primary key, then SSID is a box and not a circle. Specifically, if you have attributes which depend solely on the SSID then you would want to remove these from a PERSON table because they would have only a transitive dependency on the ID of PERSON and therefore 3NF demands that these attributes be removed to a separate table. 

It isn't necessary to have a surrogate primary key on your clubs_chains table. The combination of the two foreign keys in clubs_chains is adequate for the primary key. You can use a foreign key constraint to ensure that your clubs_chains_paymethod table references an existing record in clubs_chains using the compound primary key. This might be helpful since it would allow more direct joins between clubs_chains_paymethod and your clubs and chains tables, without sacrificing any referential integrity. On the other hand, some people just love using a surrogate primary key, even on intersection tables. If that is your data modelling style, then it's OK too. 

What you're asking for is a way to make your relational data model fit an arbitrary code constraint, namely: all "attached" items use a single ORM class. Rather than tell you how you might do that, I'm going to try to explain why this is not a good idea and what you should do instead. Code reuse is a great principle to apply to code. Data isn't code, and relational databases shouldn't be designed primarily to facilitate code reuse. You should design your database for data integrity not for code reuse. Designing for data integrity means that you should treat distinct types of things in distinct ways, even if they happen to "look similar" at present. This is what relational databases are designed and built to do. They aren't designed to optimize for code reuse. Furthermore, ORM code is generally not reusable, because it's meant to reflect the structure of and to operate on specific tables. Even if you happened to have multiple tables that all looked alike structurally and even if you defined interfaces which allowed these tables to work through a common set of code, you still have the need for the data to be persisted in different tables. There's also the matter of ORM code generators, which are just not built to look for code reuse opportunities like what you're trying to achieve. You could conceivably develop your own ORM (I built one myself in the 1990's). If you did this you could build it in such a way as to make it meta-data driven at runtime. This would give you the feel of using a document database in code while storing your data in a relational database. However, what you're really doing is mashing a square peg into a round hole. If your application calls for a document database you'd be better off using one instead of a relational database. The best you could (and probably should) try to achieve is to make a reusable code component of the middle tier/model code (and the front-end/view, if possible) which branches internally at the back-end/data tier to use per table ORM classes. This lets you achieve the benefits of using relational databases, the benefits of using ORMs for persistence code, and the benefits of components in your business layer and user interface code. 

Your data model, which is the structure of your entities, attributes and relationships. The graphical representation of your data model. 

From what I understand of your spreadsheet and your description, this is the normalized logical ERD for your data: 

Your proposed design is alright. Being derivable is not quite the same thing as being redundant. Semantically, registering for an event is different from committing to attend that event at a particular location. This is true because everyone who is going must commit to the event, but they don't have to commit to a location. The only issue that your proposed design should deal with is the potential for logical inconsistency between the chosen location (if any) and the event. This could be handled procedurally by your application, or you could use a solution such as the one suggested by ypercube to impose this constraint declaratively. 

One other important aspect of audit tables that hasn't been highlighted thus far, is that in addition to keeping track of who did what to which record (often including before and after snapshots) audit tables are write-once. Records in an audit table may not be updated or deleted (see note), only inserted. This is sometimes imposed using triggers or maybe just application logic, but it's important in practice because it gives you "proof" that nothing's been tampered with in a way which is difficult to detect. Note: Cleaning out old records from an audit table requires special processes which often have to be approved by management or auditors. 

Note that the has been normalised up to the level, but has been left at the item level, since you might want to allow a single order to contain a mixture of rental periods. This design allows a customer to hire a piece of equipment more than once on a given date but it prevents hiring the same equipment more than once on the same order - which is what you would expect.