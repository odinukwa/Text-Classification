My suggestion: Name things based on what they do or represent, and not how they do those things. In other words, don't name a table "tblCustomers", or else you may end up with a backward compatibility view named "tblCustomers", or similar stupidity. Likewise, don't do that with column names, so that you don't end up getting decimal values from "fltBalance". The one exception is that I will occasionally prefix a view name with "vw" to make it clear that it's a view, rather than a table, and thus there may be performance implications of fetching data from it. 

Using parentheses allows you to logically evaluate the inner (required) join before the optional outer join, preventing undesired filtering of records. 

If the server is using TCP/IP, then the simple way is to just telnet to the SQL Server port and see if it connects. By default, that's port 1433, so this should work: 

Usually in my lab setups, I use separate accounts, but place them into a domain group, and grant any permissions to that group. This can be nice if you want to allow for more fine-grained auditing, or verify that you can properly build such an environment, but from a functional standpoint, using one account will typically get the job done. 

Now, as for whether declaring a PRIMARY KEY instead of a plain CLUSTERED INDEX (which 2014 lets you do) will result in different query plans? That I can't say authoritatively. This contrived test was still a clustered index seek: 

You may have some LOB data (text/image/varchar(max)/nvarchar(max)) sitting in the filegroup still. I got caught up by this briefly not too long ago. Rebuilding a table/index on a different filegroup/partition does not move any of the LOB data. 

This may be a simple matter of Internet Explorer not automatically detecting server2.newdomain.net as an "intranet" site. By default, IE will only do automatic Windows authentication in the intranet zone. To see if this is the case, you can add "server2.newdomain.net" to the list of trusted intranet sites. Open IE, and do this: 

I suspect the answer to this is "you're boned", but I'd like to see if there are any good options that I haven't explored. We're building a system that will integrate with a 3rd-party POS system and display tickets on boards at various stations in a kitchen. We don't have direct access to the POS system's database; rather, the database is replicated to another server that we can read data from. In order to send new tickets to the clients, or change/remove modified tickets, we need to watch this database for changes. If we were running it all on a single database server, I'd add a few change logging triggers to the tables in question (which total a couple GB), then poll that log table every 5-10 seconds to see which tickets need to be rebroadcast. Unfortunately, we're dealing with this replicated system. I know that I can create triggers on the slave, but they'll only run for operations that are logged in statement format rather than row format. The master server is configured for mixed binlog format, and looking through the list of operations that are unsafe to log in statement format, I'd be surprised if none of them are being used on the POS tables. We absolutely can't have tickets get missed due to the trigger not being run on the slave. These POS tables also don't appear to have any update timestamp columns on them, so I can't simply query rows that have been updated since the last polling time (not to mention the possible race conditions we'd have to address there). Further, these tables are much too big to snapshot and diff them (or a relevant subset of their columns) at every polling. Oh, and I'm guessing it would be way too much work to connect to the master directly pretending to be a slave and reading the binlog content directly. So, anybody have any clever tricks I haven't thought of before we tell the client we can't do it with what we have? (They would probably be comfortable letting us access the master server, but it's the application vendor that's cagey about it. The client owns the server and data, and we could certainly force our way in, but I don't want to void any contracts and leave them really screwed.) 

The simplest approach I can think of is to have one fact table for order items, and have two price columns in that fact table: original price, and discounted price. Original price would be the price as listed on the line item. Discounted price would be calculated by totaling the price of all line items on the order, calculating the percent difference between that and the total stated on the order header, then scaling down the prices of the line items using that percentage. 

The / statements are mostly superfluous in this specific case. As for running all queries with isolation level, I'd strongly advise against it, unless you have a specific need, and aren't worried about the implications of dirty reads: 

I'm having to do a little bit of psychic debugging here, but generally speaking, you should have the subform bound to a query that joins your junction table to the table of related records. The form inside the subform shouldn't know about the table in the left side of the relationship (tbl_apptltrs). So the chain is something like this: 

The most important guideline: Don't, unless it's for OS or SQL Server patching reboots. If you've configured server memory usage properly, and you don't have a CLR component, SQL Server bug, or other software leaking resources, then "preventive reboots" aren't necessary. I also wouldn't recommend keeping the server up for a year without patching. If you need this kind of uptime, you're better off looking at SQL Server's clustering/high-availability options and performing rolling updates rather than running way behind on patches. With a good HA setup, you can essentially create the illusion that the server reboot only takes a couple of seconds. 

It's not as sophisticated as what full-text indexing does, and it only accounts for matching on a single column, but it gives decent enough results. 

Thus when it looks for the latest sale date in, say, 2012, it will ignore Feb. 29, and compare to the previous year's sales from Jan. 1 to Feb. 28 instead. I'm still open to suggestions if there's a better way to do this (particularly that awful Filter expression I used). 

You will probably get better performance by using a . This almost always works better in cases where I need to do a conditional join or filter. 

My hunch is that it has something to do with your loop opening and closing the connection 75,000 times. Maybe try something like this and see what happens: 

We just migrated our web site database from a server running MySQL 5.5.11 to one running MariaDB 5.5.41. Everything is working mostly fine, except there's at least one query from our storefront that's generating pretty terrible execution plans on the new server, causing scans of a table that's around 6 GB. It's a query with two InnoDB tables and a relatively simple join between them, on two columns that are indexed (PK in catalog_category_flat_store_1, but not a covering index on core_url_rewrite): 

We've got an SQL Server instance that's used for email archiving (courtesy of a 3rd party archiving package). Every so often, the software is rolled over to a new empty database. We've done this quarterly in the past, but we're looking to do it monthly now. The amount of data being archived is about 15 - 20 GB per month, and the bulk of the data resides in only a handful of tables (usually 2 - 4). Once we roll over to a new database, the old one becomes used on a strictly read-only basis. What I'd like to do is optimize it into a nice, tight data file, with all the tables/indexes contiguous and having a very high fill factor, and not much empty space at the end of the data file. Also, we're using Standard Edition on this server, with all the limitations that implies (otherwise I'd be using data compression already). A few possibilities I can think of: 

You'll notice one of the tables this view references is , which is probably a good place to start. You'll find a column named in this table, which appears to store the hashed passwords. When you're done tinkering, just go to your command window that's running SQL Server in single-user mode and hit Ctrl-C to shut it down. Remember, don't do this to a production system, ever, unless Microsoft support has given you specific instructions otherwise. Other than that, have fun, and make sure you've got backups for when you wreck the instance! 

I vaguely recall you have to do something like this, since the SQL Server log file is written cyclically. 

If that doesn't work, try stopping SQL Server, starting it in single user mode ("sqlservr.exe -m" from an administrator command prompt), connecting with sqlcmd.exe (which will default to the local default instance with Windows authentication), and running the script that way. 

I just tested with a scratch database, and using the TRUNCATEONLY option caused both the data file and log file to be shrunk. I believe what they're trying to clarify is that the TRUNCATEONLY option changes the behavior when shrinking the data file (i.e. don't rearrange any data pages, just chop off the unused space at the end), but the log file will still be shrunk normally. If my understanding of the option is correct, then this is probably a better way to describe it: 

One possible solution: Create a local Windows user, create a Windows login in SQL Server for that user, give it the appropriate rights (sysadmin?), then launch Management Studio (or whichever tool you need) via right click, Run As. Specify the credentials for the local user. You can also launch a program with the "runas /netonly" command to run the program as the currently logged-in user, but authenticate to network services with different credentials. I have no idea how this behaves if the "network" service is on the same machine. 

This is pretty easy to do. Open two query windows in Management Studio. Run something like this in the first one (choose a test table that nobody is using, because this will block them, and make sure it has at least one row in it): 

My personal preference would be to use triggers to handle at least some part of the synchronization. I don't particularly care for scheduled polling synchronization, as you have to deal with potential conflicts, stale data, performance impact of the repeating job, etc. If they want to do it as aggressively as 1 to 5 minutes, I'm guessing it's to mitigate conflicts and staleness, and the immediacy of a trigger would address that. If it's all happening within the same server, you're probably fine putting the sync code within the triggers, or having the triggers call a stored procedure that synchronizes each affected record. If the synchronization spans servers, or you want to make sure that having one database offline won't prevent the other database from being usable, look into using Service Broker to handle asynchronous updates. I've done this to synchronize sales entry figures with CRM data between two different servers, while allowing for either server to be taken offline without affecting the other application. Once it comes back up, Service Broker delivers the messages and updates the data on the remote server. And there's really nothing inherently bad about triggers, but like most aspects of T-SQL, it's very possible to write code that performs horribly. I wrote an article about the common pitfalls of triggers, if that helps any. Writing Well-Behaved Triggers