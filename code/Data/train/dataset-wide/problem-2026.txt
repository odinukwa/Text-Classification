The usual problem with selective copying of many tables (but not all tables, and not all rows) is that people run into foreign key violations because they forget something somewhere. If django isn't declaring foreign keys (and I don't know anything about django, but I am old enough to remember when ORM mean "run away quickly"), some functionality might just wind up broken, and you will have to troubleshoot those problems because it might be missing data or it might be code. In short, I suspect that this is more trouble than it's worth. 30KR doesn't sound like a lot of data, unless you have thousands of tables. I'd evaluate the size of the tables and see if I could just restore the database to a seperate machine running the dev edition. You could also restore a copy of the database back to the produciton server (assuming that you have enough space) but then you'd have to be extra careful to use the test database for testing and that whatever test load you have doesn't impede production. 

You say that everything is fine, then after a couple of weeks, performance drops. (Usually, people claim that performance drops quickly, or at specific times, or at seemingly random intervals. That could mean bad I/O performance or lock storms or cpu-intensive queries running at wierd times, or a heavyweight scheduled job or lack of indexing or bad stats causing cpu-intensive queries or disk reads. Or other stuff.) Weeks is unusual. My hypothesis is that another application on your server is leaking memory. I have seen this with virus software (every DBA's favorite server software villain) and 3rd party monitoring software. I would double check the memory usage of SQL Server, over time, and I'd grab all of the memory usage of all of the other applications on the box as well. If you have hard limits set on SQL Server's memory usage and have it set to not allow paging, it might be other apps that are getting paged out and eating up I/O capacity. It's not hard to look for. If you are not already keeping metrics on the server, I would just start up Perfmon and have it grab a sample every 30 or 60 minutes. After a few days, you may see another applications memory usage creep upwards. Are there error messages in the SQL Server log stating that "significant portions of sql server have been paged out"? That would also be a big clue. 

While there is no limit on the number of simultaneous connections by a single login, other than the usual @@MAX_CONNECTIONS value, using one login for multiple users or developers is generally frowned on because doing so makes it difficult or impossible to limit permissions on a per-person basis. (You may still be able to identify/contain people based on something like a workstation name, but this would require much hackery on the DBA's part, while simply using seperete logins addresses the issue.) Such "well-known" logins also have a way of getting hard-coded into things, along with the well-known password, and then developers and users become very resistant to ever changing that password. With so many ways to get onto a corporate network these days (a rogue LogMeIn running somewhere, for example) being able to turn database access off via an AD group is beneficial. Using a single login for a web application, which can have many simultaneous users, is more common. It might not be feasible to give out SQL Server logins or AD logins to individuals for large sites (for example, facebook). In that case, there is a benefit in that IIS can use connection pooling. I haven't worked in an environment where it was standard operating procedure to use a shared login like that for many years, although there were some smaller/older projects where a shared login was used (and sometimes abused). 

I have used DNS C names extensively for similar purposes in the past. For the uninitiated, a C name works like an alias for a server and is configured via the DNS system for your network. One catch is that you need to shut the old server off when putting the C name into production, so all applications need to be migrated at the same time. BUT: This doesn't work for named instances. I would suggest that you consider using a named instance in order to make the migration easier. 

Visual Studio can validate the existence of objects named in your stored procedures, if you are using VS to manage your database projects. There may be other third-party tools that can do something similar, but I've not been introduced to any. 

Have you considered using SQL Servers built-in "Service Broker" Feature? It is designed to help developers implement asynchronous, queued systems. 

Each job step has a setting that allows it's output to be sent to a text file. This can be cumbersome because it's a text file located in the file system and not in a table and might be hard to get to. There is also an option to send the output of a job step to a table. You can query the table, but it's still clunky. (It is hard to write a query to pick the "X rows affected" string out of a blob of text, then pull the number out so that you can aggregate it, etc.) I would recommend that you build a simple table in your user database (not in msdb or master) to keep that information. IMO, this is the best way to keep close track of the number of rows affected by each command, particularly if you want to be able to write simple queries to do historical reporting. This way, you can decide to log at the job step level, or if the step calls a stored procedure with multiple commands, you can log a audit record after each command. As always, @@ROWCOUNT is your friend. 

As someone else pointed out, you can't upgrade directly from SQL 2000 to SQL 2012. you will need to upgrade to an intermediate release, it seems like you probably have sql2005 around somewhere, and then you can upgrade from there. This upgrade process is very simple, essentially you would backup your sql2000 database, restore it to sql2005, backup that sql2005 database and restore it to sql2012. The upgrade processing happens during that restore operaton. After you are done with that restore to sql2012, you should be sure to perform a full reindex of the database (per MS best practices) and I like to do a full DBCC CHECKDB. The amount of time required will mainly depend on the size of your database and the speed of your disk systems. If I have a small outage window available, I always like to try this once on a test machine before I try it in production so I have a good idea of how long it will take. If you can't do that, you could look at how long it takes to back up the database (restoration generally takes a bit longer) and the time it takes to do the current checkdb and reindex processing and use those as estimates. If you are moving to a new server, it is probably faster, so your estimates using the old server will probably be on the high side. BUT: That is the easy part. The more complicated part is testing your application so that you have some level of confidence that it will work properly on the new version. SQL Server isn't 100% forwards compatible. For example, if the old application is using system tables for anything, those system tables may have changed quite a bit (particularly from sql2000 to sql2005). Another example might be that the app makes assumptions about there being very loose security on the server (xp_cmdshell being a popular culprit) and that might no longer be true. Another issue might be required conversions from DTS to SSIS. Another issue might be lack of source code for some critical extended stored procedure that your vendor wrote, or something that prevents you from porting it to the SQL CLR. Another might be a lack of 64 bit drivers to third party databases. It's hard to quantify how much testing time is needed, even if you are pretty familiar with the app, and it's very, very hard to know how much time would be required to fix the code if you do find a problem. Since you don't have a good relationship with the vendor, that makes things harder. Since the app wasn't upgraded from sql2000 to sql2005 back in 2007, I can't help but suspect that there was some sort of roadblock discovered then and that it probably hasn't been fixed in the interim. In short, getting the data onto sql2012 is easy. Making sure that your app works isn't. If it were me and I had no budget, I'd be looking at running a P2V on the existing old server and sticking the image into a VMWare (or HyperV) infrastructure that already exists. That way, you can junk the old server and won't have too much work to do. 

A common problem when developers involves the schema that SQL Server assumes when creating or altering an object. This is especially likely in the following circumstances: 

The developer has recently been removed from the db_owner role The developer is working on a server that they normally don't. They are likely to see the problem on a production server (where their permissions are tightly controlled) rather than a development server (where they might be a member of db_owner or even sysadmin). 

Obviously, a lot of this devolves to simple personal choice. Here are my own, personal, rationalizations. I've been using Powershell with SQL SQL since PSH v 1.0, and before SQL Server started officially integrating it. (When I started with PSH, I was administering SQL Server 2000 and 2005 servers.) So, I learned with SMO (or it's slightly older incarnation, the name of which escapes me at the moment) and .Net and I'm used to them. I'd generally lean towards SMO, since it makes some things a lot easier, like scripting out objects. My own code uses SMO some times and .Net some times. I think it's handier to use .Net to get simple result sets, for instance. I think that Invoke-SQLCMD makes more sense if you have lots of existing TSQL scripts. If you are creating strings and executing them through -Query, that's going to be messy. If you have a good grasp of how Powershell works with .Net and SMO, using Invoke-SQLCMD occasionally, when you have a script file to run, is easy. I've always found the PSDrive thing clunky and felt that they implemented it because they got caught up in the "everything can look like a file system" idea. I know that the *nix guys love \proc and such, but I feel that this implmentation feels sort of forced. I think that PSDrive is OK, maybe even good if you hate the UI, for exploring things but I've never written a script that uses it. I have never seen anyone use the WMI provider. So, that would be my last choice. So, I'd lead with SMO and fall back to .Net when it's handier to. 

Since no one else has answered, I'll share some opinions and do some hand-waving. As long as you aren't locking common resources, or are locking resources in the same order, you shouldn't have problems with deadlocks. I'd look at separate tables before separate databases. Each additional database will definitely cost you more but additional tables won't necessarily cost you more. You might need to use more than 1 database becuase of the sheer volume of data you will store or because of the rate at which you need to store your burst traffic. If you can manage it, I think that a table-level granularlity will be more flexible and possibly a good deal cheaper than starting with a database-level granularity. The problem with putting each device's data into it's own tables is that it makes the reporting hard since all of the table names will be different. I presume that you have some way of detecting when you get a "failure resend" of data. You don't want to put the same value in a table twice and I i'm sure that the devices can fail (local power failure?) in ways that have nothing to do with whether or not earlier values where properly stored. WAG: Assuming each "value" is 4 bytes, I calculated about 11.5 MB of collected data per device, per day. (This ignores all kinds of stuff, like device identifiers and timestamps, but I think it is OK as a rough estimate.) So, with "thousands" of sites, we are looking at tens of GB, per day. You don't mention any kind of lifetime on that data. The largest Azure database currently maxes out at 150 GB. You could fill those up pretty quickly. Getting anything to happen in a web browser in a short period of time is iffy. When you are reading from (possibly multiple) databases with GBs of data, continuously inserting lots of new data into the tables you are reading from and interacting with web servers across the open internet, "real time" is wishful thinking. IMO. "Fast enough" is the usual goal. If you can't keep all of the data you need in a single report in one SQL Azure database, it's a problem. There are no linked servers or distributed views (at this point). There is no simple way to aggregate accross many Azure databases. You'd have to pull all of the data to a central location and report from there. I'd guess that the aggregated data would be too large to store in a single SQL Azure database, so you'd have to go to on-premise or maybe EC2. A data mart or warehouse with a star-schema structure would be the classic answer there, but that takes significant processing time and that means no "real time". Also, that's potentially a lot more data transfer from Azure to wherever it goes, and that will cost you. I wouldn't commit to this strategy without a pilot program first. The first thing to do would be to build a single instance (can it handle 400 sensor values a second? (Is that a series of rows, a big denormalized row, an XML document or something else? The format of the incoming data will affect how fast the data can be stored. Can you do bulk inserts, or does it have to be row-by-row?) How about 4,000 sensor values a second? It's possible that an single SQL Azure instance might not be able to store that much that quickly.) and see how it handles insertions at your expected rates and see how the reporting might work. And I'd talk to Microsoft too. Just dealing with the billing for hundreds or thousands of seperate databases might be quirky. I don't know if this is applicable to you, but have you looked at Microsoft's "Stream Insight" product? It seems to be aimed at situations like yours. Caveat: I've never used it. The marketting blurb: Effectively analyze large amounts of event data streaming in from multiple sources. Derive insights from critical information in near real time by using Microsoft StreamInsight. Monitor, analyze, and act on data in motion and make informed decisions almost instantaneously While doing some quickly googling, I noticed a blog posting which states that StreamInsight available on SQL Azure as a CTP last year. it might be ready for prime time by now. Good luck, it sounds like an interesting project. 

I've seen this dozens of times. It was always because the statistics for the table became progressively out of date. If this is your case, updating the statistics or reindexing the table should bring performance back up. It's also possible that getting your batch to recompile, by whatever method, would help. 

This problem is often described by developers as "SQL Server is using the old version of my proc". What really happens is that the developer is modifying a copy of the procedure and executing a different copy of the procedure. The quick way out of this is to ensure that you are providing a schema for the stored procedure with your drop, create, alter and execute statements. In SQL Server, the vast majority of the time, that schema is "dbo". In other words, prefer this: