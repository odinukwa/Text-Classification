This leads to an analysis in which it is sensible to treat /ɕ/ as its own phoneme. Not counting the less nativized loanword pronunciations of words (like [siː] for the English letter 'C'), we never get [s] before [i]. Related forms like [osu] 'press-PRESENT' and [oɕita] 'press-PAST' give us further evidence that at least some instances of [ɕ] before [i] are likely surface realizations of /s/. So, one analysis that captures all of the facts is that there are two phonemes--/s/, which has the allophones [s] and [ɕ], and /ɕ/, which always surfaces as [ɕ]. EDIT: In response to comments below, here is an alternative analysis that can also capture the facts: We observe that palatalized versions of consonants may appear before [a], [o], and [u]: 

This phenomenon is indeed quite common, and it makes sense if you think about it in articulatory terms. Let's take the sequence /ns/. For most speakers, [n] and [s] are quite similar, if not the same, in terms of their place of articulation. But they differ in terms of manner and voicing. The nasal [n] is produced with a full oral closure, an open nasal passage, and voicing, while the fricative [s] is produced with only a partial oral closure, a closed nasal passage, and no voicing. In order to transition from an [n] to an [s], three things must happen: (1) The front of the tongue must re-position itself a bit in order to release some air out of the mouth, (2) The velum must raise to block air from escaping through the nose, and (3) The glottis needs to open to allow the passage of air without the oscillation of the vocal folds. If all three of these independent articulatory actions happen simultaneously, the [n] transitions into an [s] without any intermediate output. However, if (2) and (3) happen slightly before (1), there is a short period of time when voicing has ceased and air is not escaping through the nose or the mouth, with the oral closure being that of the [n], i.e. alveolar. What is the resulting speech segment? The voiceless alveolar stop, [t]! Once (1) kicks in the air gets released and the [t] "gives way" to the fricative [s]. Incidentally, whether we decide to call the resulting [ts] sequence a stop-fricative sequence or an affricate is in some sense academic, since the [t] is really just the result of the [n] and the [s] overlapping articulatorily. (Wow, try to say that last sentence three times fast without any epenthetic [t]s!) 

I would leave it as an exercise to compute this for other metrics, but I am 100% certain you will get similar results for any other metric. What you could do is investigate metrics that consider similarity and not relatedness, i.e. would rank the similarity of love, romance higher than love, hate. Jurafsky and Martin don't seem to give any references to papers about this, however. Hope this helps. 

Basically each of these algorithms center around calculating the probability of the lowest common subsumer between two word senses $c_1$ and $c_2$, which is the lowest node in the hierarchy that is the parent of both $c_1$ and $c_2$. The probability comes from the distribution sampled from, i.e. the corpora used. Resnik is the simplest implementation of this, while Lin expands it by considering similarity as both the information content shared between two senses, and the difference. Jiang-Conrath is actually a distance function, best summarized from the chapter: 

How do these work? Path similarity computes shortest number of edges from one word sense to another word sense, assuming a hierarchical structure like WordNet (essentially a graph). In general, word senses which have a longer path distance are less similar than those with a very short path distance, e.g. man, dog versus man, tree (expectation is that man is more similar to dog than it is to tree). The path similarity can be defined as: sim$_{\text{path}}(c_1, c_2) = \text{pathlen}(c_1, c_2)$ where $c_1$, $c_2$ are word senses, and $\text{pathlen}(c_1, c_2)$ is the shortest number of edges between those two word senses in a given thesaurus like WordNet. Leacock-Chodorow Similarity, or LCH, is practically the same thing, except it uses the negative logarithm of the result of path similarity. sim$_{\text{path}}(c_1, c_2) = -log \text{pathlen}(c_1, c_2)$ The negative logarithm is in the domain of information theory. The Wu-Palmer metric (WUP) is very similar to LCH, except it weights the edges based on distance in the hierarchy. For example, jumping from inanimate to animate is a larger distance than jumping from say Felid to Canid. In some sense we can think of it as sort of edit distance, assigning type changing operations a higher cost the higher they are in the hierarchy. These metrics belong to the thesaurus- and corpus-based ones (also called the Information Content metrics): 

Many opponents of a rule-based approach to language-learning support an exemplar-based approach instead. In these models, the language learner does not form an internal grammar by learning abstract rules that manipulate segments; rather, she stores vast "clouds" of tokens that are committed to long-term memory on a word-by-word basis. Every time we encounter a new token, we compare it with every similar token we've heard before and adjust our mental representation of that token accordingly. It is important to point out that Port et al do not deny the existence of segment-sized units as a cultural construct, or even as part of individual impressions of speech: 

Also, in non-rhotic (i.e. r-dropping) dialects, the addition of a vowel-initial suffix to an r-final root can trigger the surfacing of a linking-r: 

A lot of systems make little to no use of phonologically informed prosodic models. Some just can't, others are designed to and ostensibly do, and others have the potential to but don't. One widely used type of synthesis system involves the storage of hours of digitized human speech. In this type of system, which uses a method called unit selection, pieces of the digitized waveforms are concatenated to produce new utterances (like a ransom note made out of letters and words cut out of magazines, only the units are fragments of speech). Because it uses bits of actual digitized human speech, the voice quality sounds quite natural. However, the overall quality of the output for any given utterance is limited by what units are available in the database, and sometimes the system must use a unit that is optimal in one dimension (its formant values, for example) but suboptimal in another dimension (its duration, for example). The result is output that can sometimes have very natural-sounding voice quality but prosody that is all over the place and not at all natural-sounding. The algorithms for unit selection are statistically trained, and so in theory they may take into account certain prosodic factors, like if the utterance is a yes-no question they may try to select units for the end of the utterance that were originally from the end of a yes-no question. But more often than not, questions produced by these systems won't even sound like questions. You might wonder why the systems don't just manipulate the units to have the appropriate durations and F0 (fundamental frequency), as predicted by prosodic models. In fact, this technique was explored in the past, but it has largely fallen out of favor because it often distorted the speech so much that the voice quality stopped sounding natural. If you've worked with PSOLA manipulation in Praat, then you are familiar with this kind of distortion. Another type of synthesis system, which uses rule-based formant synthesis, involves modeling speech "from scratch"--that is, constructing a set of rules to predict the values of a bunch of different parameters (formant values, durations, F0, etc.). The rules yield a final set of parameter values for an utterance and those parameter values are sent to a synthesizer that converts them into a waveform. Since this kind of system doesn't rely on any pre-stored human speech, developers have a lot more control in terms of how the prosody of the output can be manipulated. A prosodic model can be translated directly into rules that form part of the entire rule set. The downside to not using pre-stored speech is that it is difficult for the voice quality of the output to reach the level of naturalness reached by the highest quality unit selection systems. A third type of system is one that uses large amounts of natural speech data to train Hidden Markov Models (HMMs). In this kind of statistical parametric synthesis system, the training results in sets of decision trees that ask questions about the input context at each node (the decision trees are statistically derived analogs to the linguistically informed rules in the rule-based formant systems). These decision trees are stored, along with the HMMs themselves. For a given utterance, the synthesizer takes the HMMs selected by the decision trees and converts them into an output waveform. The HMMs are trained on F0 and duration information, among other things, so in theory if the training databases contain the appropriate types of prosodic information and an appropriately wide range of prosodic contexts, the resulting decision trees should capture the correct prosodic generalizations. In practice, however, it is quite difficult for such databases to be created. The output of HMM-based systems tends to be judged as less "glitchy" than that of unit selection systems, since it uses parameters instead of pre-stored speech units, but the voice quality is often criticized as sounding too "smooth". The intonation is not as erratic as with unit selection systems, but conversely it is often judged as sounding too flat or "bored". In theory, components of these different types of systems could be combined to exploit their respective advantages. For example, the knowledge that goes into creating the prosodic rules in a rule-based formant system could be used to inform the labeling of the training data as well as to properly delineate the set of questions available for the decision trees in a statistical parametric system. To my knowledge, however, no such system has been developed. 

If you are going for a computational approach to predicates, I think you may wish to consider consulting VerbNet. It deals a lot with knowing just what arguments a (verb) predicate can take and also classifies verbs in many different semantically categories. I do not think there is a Russian form of VerbNet yet, but I think it is a relevant resource to your problem. You may also wish to read about Beth Levin's work on verb classes. This could prove helpful in creating your own ontology of Russian (verb) predicates, but I suspect that such a resource may already exist or at least someone has worked on it. I know there has been work for an Arabic VerbNet. 

The most accessible resource that explains the difference between each of these word similarity metrics would be Dan Jurafsky and James H. Martin's ubiquitous Speech and Language Processing 2nd Edition. Specifically, pages 652-667 in chapter 20 (Computational Lexical Semantics) briefly and comprehensively cover each metric/algorithm in a way that anyone with just a basic understanding of math, language, and graphs can understand. I will do my best to summarize each metric, using Jurafsky and Martin as my primary citation (attribute all my summaries to that book), annotated with my own understanding / insight where relevant / useful. Broadly we can group the metrics based on what parameters they operate on. Roughly there are two groups: (1) metrics which use only a thesaurus (e.g. WordNet) and (2) which use a thesaurus and probabilistic information from distributions in corpora. These metrics belong to the thesaurus-based ones: 

Using this (and the basic understanding of each similarity metric), the word senses of love and hate, while antonyms, are very related, since they essentially belong to the same semantic type (one's feeling for something else). Thus, it would be expected that the metrics give a higher similarity to them, than say love and romance; romance is very similar to love, but its type is not as close as say hate or dislike. This hypothesis is quickly confirmed by testing the WUP metric: 

You may find the Interactive Atlas of Romance Intonation (created by Pilar Prieto and others) to be helpful. It includes a whole bunch of useful resources and references, including interactive maps that allow you to click on a city and hear audio recordings of different utterances elicited from speakers in that city. Vocatives are one of the intonational contour categories that were included. Browsing through the Italian recordings, I encountered many instances of the minor third (and some major thirds and some major seconds). The survey made a distinction between the "vocative" and the "insistent vocative" (you can read in the methodology section about how the two were elicited), and interestingly there were some cases where one involved a minor third at the end and the other didn't and some cases where both involved a minor third but the "insistent vocative" just placed the minor third interval higher in the speaker's vocal range. 

It is standard to talk about the prosodic hierarchy, which is a theoretical construct that divides utterances into smaller, phonologically relevant constituents called phrases, which are in turn divided into smaller constituents called prosodic words, and so on. There is not an absolute consensus as to what the exact levels of the prosodic hierarchy are or even how many there are, and (as with many theoretical constructs in linguistics) slightly different models lend themselves to different languages. For example, the mora is a prosodic unit that is more motivated in some languages than in others. Here is a (non-exhaustive) list of levels in the hierarchy: 

It's really difficult to answer a question like this; you're asking "what am I NOT hearing?" and there are potentially an infinite number of things you aren't hearing! Here is a spectrogram of the sound samples you provided: