If you haven't already looked at Thomas Bolander's Stanford Encyclopedia article on Self-Reference, I can certainly recommend it and its excellent bibliography. As a general collection of essays on the problems of Liar phenomena and their strengthenings in language, I would also endorse J. C. Beall's Revenge of the Liar: New essays on the paradox, and I myself am currently reading Raymond Smullyan's Diagonalization and self-reference, which is a technical but awesome book looking at the mathematical issues around fixed points, recursion and incompleteness. 

A good couple of questions! I can't claim to represent Rosenberg's position, but I can attempt to give a line that seems charitable to some of his aims. So "aboutness" in a propositional content sense is to say something like "thought has a structure that exhibits patterns of meaning in very close synchronisation with those of natural language". We might say of an agent that they think that the sky is blue, or that they believe that the world is round. Whenever we attribute propositional content to an agent's attitudes like this, we seem to impose a kind of model of how they see the world as filtered through an object-predicate logical structuring. This seems to be a good first-attempt way to account for a lot of our intuitive sense of why people do what they do, often because it seems like a good way to account for our own internal sense of what it is to put the world into understanding and logical structure. To see the human being as a rational creature in this sense is to make a very particular claim about why we make the choices and decisions that we do - we appeal to an internal logically structured model and say that thereby does action arise. But in the shift from folk psychology to cognitive science, we are increasingly seeing evidence to the effect that actually things are much more nuanced than that; far from the grammatical structure of treating all things as simple logical tokens, we exhibit multiple Biases and Heuristics in our cognitive behavioural patterns. The things in the world we encounter have a profound importance to the cognitive resources we use to process them prior to conscious interpretation, so there doesn't seem to be one unequivocal notion of what it is for something to have psychological significance. Now that's not to say that human brains, cognitive structures, behaviour patterns and what have you somehow "discard meanings", that meanings are there but are filtered out in the process of human experience. It's important to recognise, rather, that what this says about the limitations of a singular "one size fits all" model of neural information processing is that the general notion of "meaning" is in doubt as a tool for investigating human action. It's not implicit in the simple state of affairs of being confronted by Norma Jean that one should think that what is happening is you're comparing your visual data with a list of attributes or a generalized notion of pattern matching that corresponds to a psychological picture that we connect to the cultural figure of Marilyn Monroe. One person may draw this connection purely prereflectively as a mental shortcut, another may not at all, depending on how they have learned to make the relevant perceptions associate the one with the other. So the notion of meaning becomes more fine-grained, more sensitive to the learned experiences of the various agents being theorised about. For Rosenberg, then, this is very much the sort of thing we might expect to happen as we delve deeper into a physicalist foundation for broader scientific epistemology. We might suggest that "meaningfulness" as an essential component of intentional action carries a more nuanced and possibly contextually sensitive logical structure than previously thought, but as our theory of what exactly human animals and brains actually act like develops and improves under careful, physically grounded investigation, the role that "meaning" serves in our most effective theories of peoples' behaviour (or indeed of what behaviour I might want to express to best bring about some desirable (?) outcome) gradually diminishes to nothing in favour of a nuanced account of the physical cognitive machinery you do in your thinking processes. So Yes, of course you still process information in a common sense way. What Rosenberg might have to say is that your account of what you think "processing information" amounts to might be very different from what an effective physicalist theory of the actual mechanical implementation of these abstract information systems is. What happens in your brain is that your sensory data are passed through various biochemical systems shaped through adaptation and previous use to draw connections between what is observed and responses to stimulation, such as to bring about certain physiological effects in, say, the speech patterns you subsequently form. Meaning just doesn't have a role to play at this level of causal explanation. Now the objection to Rosenberg that this line raises is "well, okay, fine and good, but is our best cognitive model of how people behave actually completely physicalistic anyway?" I think a lot of the functionalist work in cognitive sciences makes things a bit more abstract than a purely chemical account of brain action would facilitate; it's not so much the idea that there is some state as much as trying to find abstract functional descriptions of how these different brain states are tied to emergent behaviours on the human animal scale, which are what really interest us. Trying to tie the one to the other doesn't necessarily have to be a reductionist project to be valuable. Also, AI, for instance, would seem to be importantly linked to the elaboration of cognitive processing strategies and mechanisms, and it would seem that it and psychology might be usefully informative to one another in terms of comparing and inspiring new modelling techniques for the processing, retaining and using of information. But then again, perhaps there a mechanistic explanation might ultimately prevail in terms of what such an explanation might empower us to do. It seems like an open possibility that Rosenberg's perspective might work out, but ultimately I'm not convinced it's the philosopher's position to insist on a physicalist explaining away of representational content in the methodology of cognitive science. There're some valuable general thoughts in the SEP article on Intentionality about how much of our general ideas about information seem to be involved in distinctly mental content over and above the physical; it's worth a read, if you get the chance! 

In effect, any arbitrary sentence is a theorem of the system you've proposed. Something doesn't seem right there. To fix it, you probably need to amend the rule about conditional introduction to be in some sense Relevantly restricted. That is, rather than 

The reasoning does seem to have some obvious issues, but rather than trying to correct them, I think it's worth thinking of what it is at root trying to say in terms of the philosophical school of Logical Positivism. Very much out of fashion in analytical philosophy as a methodology for conceptual analysis, the basic idea of Logical Positivism/Empiricism is that the meaning of a statement should be strictly given in terms of logical conditions under which the statement would be verified or falsified. If there are some statements under which there are no strictly logical conditions under which they would be verified or falsified, those statements are treated as strictly nonsensical. And since we cannot tell nonsensical statements apart (because they have the exact same verification/falsification conditions, i.e. none at all), nonsense is nonsense is nonsense. So if we take it that the future really is factively undecided, such that there is no way for us to even in principle validate what will happen in the future from our current standpoint, then a logical empiricist might denounce all future case statements as devoid of meaning. And so by a simple meaning-theoretic substitution principle, they can plug any old nonsense statement they like in there. If we say that this kind of "verification conditionality" is all there is to meaning, and that truth conditions must be strictly explained in terms of finite proof or empirical procedures, then many statements that we would naturally think to have some kind of simple understanding simply fall out as non-cognate. It's widely recognised, however, that this kind of basic form of logical empiricism is not a good fit for the practice of classical mathematics, and hence for the wider part of science as is in fact practised. Moreover, it is all too easy for questions about verification and proof to be dominated by subjective considerations that we might reasonably pose have no business being imposed upon objective scientific analysis. Logical Empiricists have a tendency to dismiss an awful lot of things as nonsensical simply on the grounds that they do not accept certain premises. It seems a mistake to think that the actual falsity of a premise makes it logically impossible. Following work by Kripke in the 70's, metaphysicists have widely reintroduced the idea of using simple set-theoretic models to ground talk about alternative possibilities than the brutely first-order factive notion of what can be strictly proven from presently observed and reified facts. This is particularly so if we want to be able to talk in terms of an objectively probabilistic methodology in science; we would like to be able to model counterfactual situations in order to establish a kind of state space framework for probability-based scientific models. Nonetheless, certain projects might be interested in trying to squeeze out the methodology as far as it goes, and I think we see this a lot in certain "New Atheist" philosophy inspired by evolutionary biology. Strictly grounding mathematics in neuropsychology, for instance, seems to be one way to go about it, and it's premature to think they can't get something interesting out of a "finitistic" conception of theoretical resources by going down that road.