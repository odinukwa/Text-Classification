Now if this snippet runs concurrently by 2 transactions, T1 and T2 what will happen? I know that in MySQL by default the isolation level is . So let's say that amount initially is 500. So when the 2 transactions finish will the final amount be 300 or 100? I mean when they start both read from table which I guess is a shared lock. When one updates the table it gets an exclusive lock right? So what happens to the other transaction? Will it proceed with the amount it "view" when it started i.e. 500? 

These block each other but not always! I can not understand this. These refer to different rows why do they block each other? 

I have noticed that even on huge tables with millions of records when I do a the index is created immediately (I get the console back in nsecs). Why is this? Doesn't it create the B-Tree with the table data at that point? I can only assume not due to the immediate return of the . But then how is the index created? On each access? 

I can not understand or visualize how this is formed in the HD. I think that 4 * 5 = 20 physical files are created. Is this correct? And what data are stored together? Is there a "clustering" of 2008 dates for all customers with the same hash? I am confused on that. Also I am confused how the data are fetched. Is the customer_id hash used first to determine the range partition or the opposite. What are the exact steps to fetch the data? UPDATE Another example. I did the following: 

When having a table in SQLite that has a data type, is there any implication if the length of the contained string differ significantly? 

Why does the first sum of the fractions is summed up to 1 while when summing the decimal equivalent we get 0.9999? 

But this seems useless to me. I mean if I set serialized to a client connection then I would want all currently running transactions to be sequential, right? Otherwise what's the point? 

OR do I need any transactions or locks for the DELETE vs INSERT/SELECT? It seems not but I was thinking I may be misunderstanding something Note: My question is not about transactions and their benefits.My question is if 2 statements from different processes on the same table SELECT/DELETE need for any reason to be synchronized 

I read that I can have different isolation levels per connection and in the server. The default isolation level is REPEATABLE-READ. So if I have a transaction that issues a to the client connection, how does it interact with other transactions from other client connections? Do they all end up being serialized? From set transaction seems not: 

When using triggers, if an update is done to a table then a trigger is executed. This is very convenient. But what I would need is to execute an external script. Is it possible to configure MySQL somehow so on a trigger/change of a value in a table an external process/scrip is executed? 

What I am doing is something like: This way I pick up each request one at a time. Then I do (the id=123 assume it is from the SELECT request). This works but I am not sure if this is another approach. What would be better? E.g. doing a LIMIT 100? My concern is that the order by is not a good idea as the data grow. Does it make an impact if I always process e.g. per 100? I still have to order by right? 

What happens to a transaction when the client connection is lost. Example the client is a web application code that starts a transaction to a remote server instance. The client sends the sql code to the server and waits for the transaction to complete. What happens if the network connection fails? Does it depend on the point we are in a transaction? Is it irrelevant? Is it aborted? 

I have a rather basic question on transactions. Assume a table with a column amount. Now we have the following code: 

If you have a big table that you want to partition and you have 2 candidates as the partitioning key (in the sense that 50% of the queries use one column and 50% of the queries use the other column and almost no query uses both in the same SQL query unless all the code is rewritten to do that) how can one determine which key is the best candidate to be used? Does it matter for example if the "entity" that one key represents is "fixed" and the other is constantly increasing? E.g. as an example of what I mean "growth" (taken just as an analogy of what I am asking) if you have decide between using the doctor id or the patient id as the partitioning key where we can have only so much doctors but infinite number of patients 

Databases use the file system to store the data. As far as I know it is not possible to delete a record from a random access file. So does that mean that when we do a the size of the table i.e. the file that stores the table never decreases? So databases essentially keep growing and never reduce in size? 

I know that transactions are meant to group several operations as one. But if for example in the same table one transaction does 

You could significantly speed it up further if you have a fixed list of values that range in the dozens (or low hundreds), by turning the type of the column into an enum. You'll have to drop indexes that use this column prior: 

You have two options: either you loop results from PHP, or you store all the custom fields and values in one single JSON datatype field (stored as a json object, requires MySQL 5.7) in the users table. 

For your system using the database, this is the standard for non-object-owners (i.e. they can do whatever is being granted to them, but not alter/drop the object). Finally your DBA is simply a standard superuser. So yes, what you require being able to do is perfectly possible. 

The is for compressing the transfer, which is something you probably want. For further compression, you may use instead: 

Your strategy for getting information from can be useful for a one-off, but for ongoing queries to it, especially over millions of records and expecting quick results, it is far from optimal. Considering the sheer size of your tables, you'll probably benefit from datawarehousing. If your tables are constantly updated, you'll need a trigger to keep your datawarehouse updated. The following is not exactly datawarehousing, but it follows the principle of creating the most lightweight extract of the data you'll need for future queries: 

If you can allow for several skills per employee, using a many-to-many table table (e.g. ) would be the proper way to normalise the data, however there are alternatives that may suit you better for simplicity: 

You'll speed-up the query significantly if you create an index (ideally a unique index) on and , and another on just : 

Adding some junk to the phrase breaks it, unless you pass it twice via quote_literal, which makes any string safe to use: 

All of the options above have a significant performance impact. A possibly better performing option could involve a table for this purpose: 

A possibly simple way is replacing all MS-Access tables with links to views in your SQL Server with the exact same structure as the old Access tables. If the views are simple enough (e.g. a select statement from a single table with a primary key and unmodified columns -other than renaming them-) they'll be directly updatable, otherwise you can use updatable views. 

The following schema is what I came up as a fully denormalised set of tables. There are pros (e.g. flexibility) and cons (e.g. user interface complexity) to this approach, but you may find it useful, or it may shed some insights: 

Ideally, your system could comprise: (i.e. your many-to-many table) The column in your table should be calculated from rather than stored. 

Your design seems almost correct. I can pinpoint the following problems though: You're using the table for two purposes: 

Edit Once you find the offending duplicates, should you consider that the first occurrence of each case is good enough (e.g. trivial differences in description or coordinate fields), you can use DISTINCT ON: 

You can also use . Ensure that your local user can ssh passwordlessly to your remote host () and that its public key () is included in the remote hosts authorized_keys files (). Once you (as user) can ssh to the remote host, update the following in your : 

This query doesn't really require , because it includes a primary key. If you have any duplicated rows, you're then performing a join incorrectly, and you should either use a subselect within the SELECT clause, or adopt a different your strategy. The undesired duplicates are likely coming from either or . The more tables you join (especially if you use any OUTER JOIN), then the more complex the possibly query plans become, and the more likely your RDBMS will be to choose one that doesn't work very well. If you keep your queries as compact and minimalist as possible, the queries will be more likely to run more efficiently. 

Something like that could be achieved through dynamic sql, or via using stored procedures instead of views (for selecting) or triggers (for controlling inserts/updates).alter But you can also create the memberview to return values according to the currently logged on user, which is probably more practical: 

Your software is most likely not using the default database (that is a database that is normally reserved only to store the list of databases, users, and other globals). From psql, run to list the available databases. Your database will be something other than postgres, template0, or template1. Try connecting to one that is not one of the above () and then run again to list tables. 

Your relationships are correct, however, they would allow for a customer to have records of products they've never bought. Is this something you want? Otherwise, you'd be better off establishing the relation between Review and OrderDetails (the intermediate table for Order-Product which in your ERD doesn't have a name), rather than to Product directly. That being said, your ERD would probably be clearer if it'd be , where OrderDetail includes quantity, price used, discount, special notes, etc. Also, Rate should perhaps not be a separate entity, but rather be Review's attributes. 

Perhaps this will help. If you'll rely on the account_id from full_path often, then you'll benefit from a function and a functional index for it: 

JSON documents are ideal for this purpose. MySQL can store JSON documents via its JSON datatype, and it has a variety of function for extracting information in such documents. However, MySQL doesn't currently have an effective native way of indexing JSON documents for fast search. Googling for "mysql json index" will render several strategies for doing so, some using triggers and external tables, others using indexes on computed columns. PostgreSQL, however, has very good support for JSON datatypes including indexing, opening up documents as recordsets (using LEFT JOIN LATERAL in combination with is very powerful), and GIN indexes that support JSON documents (for both tags and contents). Unless you're heavily invested in MySQL already, PostgreSQL would be a great option for your application. Alternatively, you may want to consider PostgreSQL instead of MongoDB if you'd use it for this purpose alone. Additionally, PostgreSQL can easily access MySQL tables with via Foreign Data Wrappers, it has good support for regex text manipulation, and you can do all sorts of magic on JSON documents through standard perl libraries, using pl/perl stored procedures.