At some point in these debates, somebody often chips in with a comment about XML columns ("I know, I'll just use XML"). To paraphrase JWZ, now you have two problems. Around 1998 they installed another circle in hell just for people who defile their systems with XML blob fields. Although it doesn't look slick and high-tech, just having a set of user attribute columns on the table is by far the best solution. It is the most efficient, and there are many ways to make this approach more user friendly at the database level. EDIT: Here is a snippet of T-SQL showing how to create a view that flattens out an EAV structure. Note that you wound have to write a generator for the view based on the application's attribute metadata and re-create it every time this is changed. 

I presume you have substantial data volumes if you have separate staging and data warehouse servers. From your posting It sounds like your staging server is doing a bit more than just staging the data. You describe merging data into the fact table on the warehouse server, so I presume that the staging server actually has ETL processing on it as well. I would not generally have recommended a transform-push approach like this; running ETL processing from the warehouse and pulling from a staging area is generally better. The merge happens on the warehouse. If you need to scale out you can replicate the warehouse, and the same operations will happen on the source and targets. However, I guess you're stuck with the transform-push architecture, so you've got a couple of options: 

If I wanted to see the total of Var1 for all Key2='A' then the reporting tool would allow me to slice on Key2 and add up all the rows. If I wanted to see the total of Var1 for Key1='Foo' then I could do something similar. In this instance the total of Var1 for Key2 = 'A' would be 30. If this is the sort of thing you want to do the for data then a dimensional model could help, and would allow you to put up a statistical reporting facility with an off-the-shelf reporting tool such as Report Builder. A dimension table for Key1 might look like 

You won't be able to automate this process. Any release of the base system will need to include an impact analysis and change to the analytic system, and this will be a manual process. You will have to update the ETL process to source data from the new structure and possibly modify the data mart as well. This will add latency to the release process, and I've seen plenty of occasions where this was neglected and the analytic system broke. The business or owners of the transactional system will have to do one of three things: 

I am a completely certification free zone, although some might describe me as certifiable. Hasn't hurt my career in the slightest, but I have worked for a couple of MS gold partner companies where there was a requirement to have a minimum level of certified staff in order to keep the gold partner status. 

The export process will be quite expensive, though. On SQL Server, you could create a temporary table with the last names using a query along the lines of: 

It sounds like the vendor's snapshotting functionality is putting some pretty serious limitations on your flexibility for database layouts. If you have performance issues (and it would be a very long way from the first time someone had database performance issues on a SAN) then you have very little flexibility to spread the database across multiple physical volumes. Does the vendor insist you also put the log files on the same volume as well? If so, then your log files will be on a busy shared volume with no write buffering from the SAN's cache due to Microsoft's policy I/O reliability certification programme for SQL Server. MS requires SAN vendors to honour FILE_FLAG_WRITETHROUGH, which SQL Server opens its files with. If your SAN vendor is going to force ALL files for a database to live on the SAME volume you could have performance issues. I'd look into alternatives to the snapshot manager for DR before signing up to that. What did you do on your old system? EDIT Per the chat - if you can use mount points to organise your database files then one set of LUNs per database is probably better than forcing all the databases into the same format. 

. . . So help me Codd In your example we can assume 1NF to begin with as the relational structure doesn't imply any repeating groups within the row (i.e. no D1, D2, D3 etc.). 

I was a bit underwhelmed with Oracle OLAP when I evaluated it circa 2005, mainly as it had poor support from front-end tools at the time (Discoverer 'Drake' had no drill-through support, and there was practically no support from third party tools). In the end that project went with MS Analysis services. @Ali's post suggests that it does have support from OBIEE now, so if you have licenses for that you can probably put a reasonably frendly front-end on the cube. While somewhat exuberant, the points he makes are fundamentally sound. In answer to your question: 

Operational vs. analytic reports Ad-hoc reporting against an operational system is a bad idea, so the answer depends on what your real-time requirements are. 

Locking Locking can also cause performance issues. If your system is performing badly under load, look at the profiler and perfmon counters related to locks and check whether there is any significant contention. has a 'BlkBy' column in the result set that will show if a query is blocked and what is blocking it. Also, profiles with 'deadlock graph' events (if you have queries deadlocking) and lock related events can be useful to troubleshoot lock issues. 

If you have lost the historical data (i.e. it gets overwritten rather than stored as a transaction history) then you can't reconstruct the historical state - full stop. However, there are a few approaches to dealing with this. 

In order to implement what you want you need something functionally equivalent to or on unix. Windows does ship with a utility called that has similar functionality to grep, so you may be able to use that to extract relevant lines from your output file. As far as I am aware there is no equivalent to shipped with base windows builds. This sort of thing is trivial in a unix shell scripting environment but can cause issues on a raw Windows build. If you can't implement what you want using , three basic approaches come to mind: 

I don't think MS actually guarantee anything about the underlying SSRS database, so you're getting into unsupported territory by actually frigging with that. Generally this sort of thing is best avoided on production servers. You can query data sources and other report server metadata programatically through a web service API exposed by SSRS. The API would allow you to tree-walk the folders hunting for shared data sources in places they aren't supposed to be. Hunting and exception reporting is probably the best way to deal with this, rather than attempting to prevent it at the source. You can also query the report metadata to hunt for reports with references to sources outside the designated areas. This allows you to educate the authors. MS provide a utility called that lets you write scripts to do this in VB.NET. Later versions might also support C# but I haven't done this with anything more recent than SSRS 2005. Essentially it tops and tails your script with a bit of boilerplate, compiles it and then executes it. Some documentation about the utility and SSRS web service API can be found here, here and here. 

Disclaimer: I have never built a Teradata system, so I can't claim this from first-hand experience, but I will explain the reasoning. I think that Teradata will be able to produce this view efficiently. From what you say, it appears to do little more than join some very small dimension tables against a fact table. The join operations will be relatively efficient. Unless I misunderstand your requirements these columns are allowing your application to select various rollups of data from a multi-grain fact table. Even though Teradata is a shared-nothing system, I can't see any requirement for the view to push large semi-joins across nodes or anything like that. Beyond that, all I can suggest is that you suck it and see. If you don't have anywhere to experiment you could download the express version of Teradata off their web site and see if you can prototype this structure to see what the query plan actually is. 

Embedded Sprocs on a DBMS I think for the purposes of your question the answer is probably somewhere between 'no' and 'it depends'. Most relational DBMS platforms support stored procedures in a SQL derived dialect or a procedural language with embedded SQL capabilities. PL/SQL is an example of the latter class of languages. Writing queries in arbitrary languages is not really an option as they still have to interact with the query optimiser, so they can only express semantics that are compatible with its architecture. Even 'procedural' SQL dialects like T-SQL actually issue multiple queries to the optimiser. SQL is always going to be limited in this way because of the tight coupling to the architecture of the query optimiser. Many DBMS platforms support alternative languages for stored procedures, such as a JVM in Oracle, CLR integration in SQL Server and various embedded langauges in PostgreSQL. These allow you to write procedural code, support fast local connections to the DBMS, and (in some cases) integrate with APIs in the database manager. These APIs let you write custom aggregate functions and other extensions. However, there good reasons not to use this for Non-SQL application code unless it is really necessary, especially in the case of Oracle. Three major reasons are: 

You can import your data into a staging table then update the product reference data from that. The product table can hold your 'available' flag. Unfortunately you will have to import the XML data into SQL Server to load it, but SSIS has an XML reader, so you can use that to do the imports. 

Now, put up a cube over the database with Customer and Product dimensions and a measure group for the sales leads. A MDX query like the following would show you which customers wanted to buy Widgets but not Dohickeys: 

You can have more than one hierarchy on a dimension. Your time dimension could have a year-quarter-month-day hierarchy and a year-week-day hierarchy. The hierarchies can share the same 'day' attribute - set up attribute relationships for both of the hierarchies, and 'week' and 'month' can both be attributes of 'day' 

Managing environments I think you definitely don't want to be forced into a single database version. You've got enough developers that you will inevitably have multiple development work streams, and requirements to apply patches to the current production environment independent of development workstreams. You can use Liquibase or a manual process to produce patch scripts to upgrade versions. I suggest starting out with a manual process and using the schema comparison tool for QA on the patches. Clean, automated, transparent synchronisation of a nontrivially complex database is a bit utopian. Your central data model can be kept in whatever system takes your fancy. I've used everything from tedious enterprise repository tools to create table scripts. Create table scripts play nicely with ordinary source control tools such as subversion and not all repository tools do a good job of versioning. Whatever you use as your master data model repository you need a fairly clean mechanism for deploying an environment from that model. It should be structured so that rollouts to an environment are easy. You also need a mechanism to patch from one released version to the next. What I've done I've done the following in the past when I was managing development environments. It's not particularly high tech, but it's amenable to version control and automated builds, so it makes it easy to roll out an environment to a specific version, and maitaining a large number of environments is quite practical. Maintain a central repository: This could be a set of database creation scripts held in a version control systems, or a repository model in a data modelling tool. Take your pick. This model should have a build mechanism that allows an environment to be rolled out from the scripts without a lot of manual intervention. If you have a lot of reference data you will need a load mechanism for it. Depending on how you want to do it, you could keep this in a database or in a set of files. The advantage of files is that they can also be versioned and labelled from the same version control system as your code base. A bunch of CSV files and bulk loader scripts in a source control repository can do this easily enough. One option for deploying development environments is to take backups of the production database patched to the appropriate version and make them available for devs to restore into a development environment. Make it easy to roll out: Like any CI build process, the database should be deployable from a single script. Set it up so that database connections can be paramaterised, or the script is location independent and can just be run through the connection. Patch scripts: You will need roll forward and probably roll back scripts from each released version. Build test environments from the repository model: This ensures that development on environments that are out of sync with the repository gets caught in testing. Test the deployment process: Automated patching scripts, however they are created should be testable. Schema comparison tools are quite good for this, even if you dont't use them to generate the patch scripts. 

You can generalise this to a M:M relationship where you get multiple payments against a single invoice or a payment covering multiple invoices. This structure also makes it quite easy to build credit control reports. The report just needs to find invoices older than (say) 180 days that still have outstanding balances. Here's an example of the schema plus a couple of scenarios and an aged debt query. Unfortunately I don't have a running mysql instance to hand, so this one is for SQL Server. 

1 Note that the optimiser may choose to use join operators within the plan behind the scenes, although the actual operation specified is a correlated subquery and not a join against a nested subquery. 

What I like about this process This is a bit heavyweight, and was designed for deploying into fairly bureaucratic and opaque production environments. However, it has the following strengths: 

The way to do this is what Kimball called a Type-2 or Type-6 slowly changing dimension.. Essentially, a type-2 SCD has a synthetic dimension key, and a unique key consisting of the natural key of the underlying entity (in this case the flyer) and an 'effective from' date. The synthetic key is joined against the fact table, so you can attach it with a simple equi-join (i.e. you don't have to filter by date range in the query). All the attributes (e.g. club in this case) are attributes of the flyer. If one of these attributes changes, a new row is created on the dimension recording the new state, effective from the date of the change. The type-6 is like an ordinary type 2, but has a self-join to the current version of the row. Whenever a new row is created for a given natural key all rows for that natural key are updated with the self-join to the current row. You may or may not need this functionality. You can query an as-at status by joining the fact tables against the row that was recorded on them - i.e. the state that was current. If you have a type-6 the current status can be queried through the self-join, which can also be materialised on the fact table if desired. This data will also play nicely with ad-hoc reporting tools and cubes, although implementing complex cube hiererchies on a slowly changing dimension is a bit fiddly (you need to keep placeholders for the natural keys of the hierarchy levels and combinations over time). A good point to start would be a google search on "type 2 slowly changing dimension"