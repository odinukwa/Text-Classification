I see two reasons to answer "No." At least initially. (As my answer progresses they seem to grow in number.) Short answer: I think the notion of anything like such predictions is more akin to incoherent than impossible or even merely impracticable, though this is hardly obvious from the outset. Long answer. A fascinating question, that merits quite a long journey. A more rigorous treatment would probably merit a whole book. Preliminaries I speak a lot of a simulator though the OP mentioned none. While prediction might be a purely mental exercise, at the scale queried it plainly requires significant extra-somatic data processing or complex analogue modelling. This I call a simulator. Analysis First, the premise that no occurrence is random is false in every sense known to us. Chaos theory might come to mind in support of the notion that no macroscopic event has finite knowable inputs, but I don't understand that to be an ontological claim of chaos theory, which theory appears to reside in the domain of deterministic analysis, a technique which sidesteps the heart of the problem of crediting the prospect of predicting the future. The outcome of quantum states are thoroughly described by probabilities rather than deterministically. This is not an epistemological claim (though some accounts of the uncertainty principle read that way, though, in any event, the uncertainty principle is not the sum of quantum theory). Rather, it is an ontological one. In other words, the world has randomness, it does not merely appear random. Second, our biosphere, called Earth, is not a closed system. Nor is our solar system, nor any other scale short of the entire universe. And that is a lot of objects and states to keep track of. And I suspect predicting your future is no less a task than predicting the whole future (of everything). Or at least the two merge asymptotically as the length your reach into the future grows. (I give short shrift to the potential that short-term predictions avoid this for most of my analysis -- but my comments on the impossibilities of collecting an initial-state data set should be construed as dispositive of the hope this is a fruitful exception to otherwise fatal problems.) Now, if you think about it carefully, it seems you would need to keep track of the spin, charge, mass, direction, and speed of every particle in the universe to predict the future of everything. What would the record keeping look like? What would the record keeping media look like? Actually, that's startlingly easy to imagine. The most compact record of the state of a quark would be an identical quark. Or, in our scenario the subject quark itself. Building from that observation, we quickly realize the most compact account of the present state of the universe is the present universe itself. Any independent description would have to be as large or larger. Of course, the notion an independent description (necessarily outside the universe) is essentially useless. My point being that the universe can be accurately thought of as an analog computer that is capable of predicting its own future states in real time. In some sense, this is the only thing the universe actually "does" (other than merely being, which is really just the same thing). The notion that anything less would suffice as a thorough predictive mechanism violates what we know about information storage. You simply cannot encode however many bits of information are needed to describe the totality of the present state of a quark into something less than a quark. (Unless the ultimate indivisible particle is smaller than a quark, then the notational limit I describe migrates down to that smaller particle.) Then these same observations all flow uphill to the larger structures of the universe which are wholly characterized by the totality of their substructures. But is this really true. Maybe a description of the universe is compressible. But for that to be true, it seems there would have to be redundant structures in the universe, because that is ultimately what all compression algorithms leverage. That would seem to mean something like noting there are two identical molecules in the universe and I can use one as a proxy for the other. But that makes no sense. Even assuming they were identical to the most elemental level (I make no claim to the reasonableness or absurdity of such a notion), there remains the fact that it seems meaningless to assert they are fated to identical futures such that one can remain the proxy of the other. Does not data compression, in fact, work because it always has a static subject? Don't let moving pictures confuse the issue. The movie's data stream is static taken en toto. The fact that our minds view it in pieces sequentially is not the same as the data content being dynamic. It's the same movie each time you watch it. Yet nothing in reality is actually static (even digital data streams decay). Now I admitted the future of a person in the short-term (and I have no idea how short "short-term" is) could turn on less than the state of the whole universe. But the notational problem still remains immense. And the computational model is perhaps even more so. The maximum scale of complex physical systems that can be modeled (or simulated) in real time with infinitesimal precision presently is stunning small. To be honest, I am not entirely sure there is any scale we can model this way in real time. There are no examples I can think of where we have complete momentary data describing a starting point absolutely outside of those experiments in particle physics utilizing chambered high energy collisions. And frankly, the most detailed accounts I have seen of those suggest they are not based on complete knowledge notwithstanding the minuscule scope of those highly isolated "worlds." Analyses of these experiments are peppered with references to statistical methods, a seeming indication of less than absolute knowledge. Another substantial wrinkle is that the most energetic particles can pass through as much as a light-year thick layer of lead. Unless these cast no influence upon "history" the notion that one can isolate analysis to some "manageable" subset of the near universe seems quite delusional. Still another problem with predictions is that inasmuch as a simulator is essential, that requires knowledge of how the modeled system works. Plainly predicting the future of a person requires, among countless things, predicting the future of their mind. But our knowledge of the mind's workings is not even 3% what would be necessary to model it. Further, we need a thorough account of an initial state to feed our prediction engine. Is not the crux of all interpretations of Heisenberg's uncertainty principle that one cannot collect a fine-scaled complete account of anything? On top of all the above, how does one power such a grand simulator? Does the existence of the simulator not become a factor that needs simulation? Can a simulator be conceived of which simulates both the world (or even some subset of it) outside of itself and itself? I don't mean can such a simulator be conceived, I mean could such a conception be coherent or is it inherently nonsensical? Ultimately, the notion of such predictive engines sounds no less absurd and nonsensical than the notion of the mind of an omniscient god. And is not the penultimate play of those clinging to such notions that the actuality of god transcends reason? Tentative Conclusion The more I reason this through the more convinced I am that I am converging on the conclusion that the only simulation that can predict the outcome of any non-trivial subset of existing reality is reality (the universe) itself. PS: The is also the question of data integrity. As our capacity to store ever larger data sets grow we are coming to see the ever expanding complexity of maintaining a static account of a static data set (a subset of the problem of a reliable account of a dynamic data set). And this is in a world limited to nothing achieving even yottabit storage. I don't know off the top of my head what size a petabyte account of subatomic particles amounts to, but I suspect it's not even person-scaled. This problem of data "decay" is external to the simulation and a defect that makes the scale of the simulation problem larger. The only scale at which it is manageable is at the scale of universe-as-a-predictor-of-history, because that simulation seems to move data decay inside the simulation where it manifests as quantum uncertainty, a causal element in history rather than mere noise. PPS: Fantasies of predicting the future always seem pinned to hopes the world can be saved from quantum mechanics and returned to the comfortable deterministic model of nineteenth-century science. This is symptomatic of the admission that quantum mechanics has historic influence. But if QM has historic influence, does this not doom a simulator to have the requirement of matching the scale of what it models? Otherwise, the simulator will experience materially greater randomness than the subject. Of course raising this objection suggests I am ignoring the more basic problem that a simulator even at matching-scale is doomed to experience different randomness than the subject, thus corrupting it conclusions. Indeed, if the simulator worked, it would seem an epistemological apocalypse (1), as the production of a correct prediction could only seem to follow if either the randomness experienced by the simulator and the subject were the same (thus violating the definition of randomness and refuting a core tenet of QM) or the significance of QM in the causality of macroscopic phenomenon was trivial (violating a tenet of determinism -- which would be even more catastrophic to our understanding of the world than the collapse of quantum theory). PPS: The subject of data integrity oddly raises perhaps the best tangible example of the interface of subatomic particles and macroscopic effects for our purposes. Some accounts of the subject identify energetic stray subatomic particles as among the causes of corruption of isolated bits within a data stream. If these claims are accurate, they strongly validate the notion that a predictive engine is a self-defeating objective more akin to perpetual motion machines than anything worthy of serious attempts. This conclusion builds on the notion that a predictive engine cannot differ in scale from the phenonmenon it models as noted above, a consequence of which is that attempts at error correction are intrinsically self-defeating in this endeavor as they grow the scale of the simulator. (1) I am startled to learn from Google that no less than 109 people beat me to the punch coining that word pairing. 

The point of course is that the experience of non-existence is a logical impossibility. If you experience something - anything - then you only do so as a consequence of the fact that you exist - in some form or other. As Descartes said "Cogito ergo sum". Even if the universe itself ceased to exist if you can still think/experience then that is proof enough that at least your consciousness exists. Any other than an extremely broadminded atheist would of course pooh pooh this idea since most atheism is bound to the dogma that the only reality ultimately is the physical universe we can perceive through our five senses, and therefore any notion that some sort of amorphous detached consciousness could obtain any sort of being in the absence of a physical universe to support it is just fanciful nonsense. Personally I find this blind faith in this dogma as questionable as any blind faith in any religious idea. For what after all is metaphyics? Just that - meta + physics. The idea that the physical universe after all is not the last word in reality, but instead embraced by and dependent upon another, greater and presumably non physical more fundamental reality. People will say "Show us this reality, prove it" and expect the same kind of proofs used to illuminate the familiar physical reality. But if this meta reality exists and if it has a nature completely different to the physical then obviously the physical proofs are not going to show anything. But to answer your question you are going to have to ask yourself if it could be possible that existence can have some sort of being "outside" and "beyond" the known framework of the physical universe. That is to say precisely "outside" of the time frame you are speaking of. Because who can say what time is? The atheist can no more objectively prove the existence of time than the metaphysicist can prove the existence of some spiritual dimension or whatever, yet without our subjective sense of time we would not be able to perceive the physical universe, and anybody who came along and postulated that it in fact exists, we would denounce as irrational. So it's all down to what you subjectively can sense. What/who we were before we were born or conceived, what we might become after death, may well be whatever we always have and will be in the spaces between time and the physical world. And perhaps the essence of our being, our pure sense, draws its real life energy not from the world of physical matter but from the roots it has in this "other world". And then again perhaps not. This is the agnostic viewpoint. We do not and cannot know, but what we do know is that existence triumphs over non existence, that non-existence is itself non-existent. And we know this not because we objectively can prove it but because we are beings of sense and sensibility, and therefore have no choice but to know it. 

It's always subjective, I'm afraid. When you speak of things that are "objectively true" you're talking about 2+2=4 and A==A, and not about anything which it is possible to have an opinion about. Many people would argue that objectivity is impossible for subjective creatures like humans (though phenomenologists would argue the exact opposite: that everyone's subjectivity is objectively valid. How this differs from "everyone's viewpoint is subjective" is an exercise I'll leave to the reader). 

It's not quite correct to think that deriving a contradiction implies all propositions. What deriving a contradiction does is prove a modal realm to have arbitrary truth values. It says that, in order for your argument to be valid, you have to live in an alternate, crazy, universe, where anything is possible. If an argument leads you to ~A==A, it doesn't break the world. It breaks your argument. The solution is to fix your argument. This is the whole point of Paraconsistent Logic: to force people to abandon flawed arguments. There are a great many questions in philosophy that are considered important, but which rely on apparent contradictions: "Do I exist?", "Can god create a stone heavier than he can lift", "This sentence is not provable". It's an interesting feature of human linguistic ability that we can create seemingly meaningful sentences like those. HOWEVER, to then spend hundreds (thousands) of years arguing about them is viewed as counterproductive by many philosophers. We could afford to sit by the fire and sip sherry and indulge ourselves back when philosophy included all science. Now? Not so much. 

This is, at its root, a question of set theory. The statement "X does not exist" can be easily translated to, "X is not a member of the set of things with the property of existence." Existence is the same: "X is a member of the set of things with the property existence." Very simple, right? So where does the problem come from? The problem comes from the fact that we haven't enumerated the set of things that have the property of existence. If we had, it would be trivial to prove non-existence. Most people feel that the set of things that exist can never be enumerated, as the universe is big enough to make this effectively impossible. Therefore it is effectively impossible to prove non-existence. 

Loved this discussion at uni because it has no conclusion. I think that's the way Sartre wanted it. His idea is that there is no core to our being human; specifically there is no soul. Our being is defined by our doing. At any and every moment in our lives the very next thing we do is up to the private choice we make at that moment. Nothing is pre-ordained even though we might sometimes wish it to be so. Our character is but the sum of the choices we have made up to this point in time and nothing real can prevent us choosing differently next time and thus out of character, with the result that character can change eventually. At least this is what I understand by phenomenological ontology, because since ontology is the study of the human soul and Sartre doesn't believe in the human soul, then the phrase must Refer to an explaining away of the soul rather than just an explanation of the soul. So both yes and no phenomenologically we have the experience (or perhaps would like to convince ourselves of the experience) that certain features in our character are bound by the genetic prescription we are born with, or lie in the configurations of stars or some other deterministic invention, all the while we are 'condemned' to not only to appear to ourselves as utterly free, but are in fact utterly free. To be is to do - Rene Descartes To do is to be - Jean Paul Sartre Do-be-do-be-do-be doo - Frank Sinatra 

. . an important point precisely because it is obviously not true. Non existence excludes the possibility of experience. We can only experience existence; not non-existence. Therefore there is a case for turning the orthodox rationalist view of reality (that is typically the atheist view of reality) where the inanimate 3D or 4D material universe is perceived as the baseline for all existence, on its head. So - since non existence cannot be experienced doesn't it make more sense to regard conscious experience as the baseline of all existence? And the 3 - 4 or 5 or 6 dimensional material universe as a contingent phenomenon subsumed under consciousness of existence? If the physical existence is not an ultimate reality then death need be seen neither as an ending nor a temporary phase of non-existence. In this case death is but the shedding of one frame of conscious reference and (presumably) the adoption of a new one. Merely some form of transition.