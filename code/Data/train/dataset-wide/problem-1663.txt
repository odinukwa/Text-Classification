Could you give a source or example: The Tycho-II catalogue paper by Hog et al. (2000) suggests there are about 19,000 stars in Hipparcos or Tycho-I that are not in Tycho-II. This can be because they are too bright (but that is not very many stars) or for other reasons that are not clearly specified (binarity is an issue I think). Looking at this website, it suggests 4% of stars did not have proper motions and could not be assigned epoch 2000 positions in Tycho II but I don't think these are missing. 

Hence my assertion that with current methods and telescopes there is not much chance of success. But of course technology advances and in the next 10-20 years there may be better opportunities. The first step in a directed search would be to find planets like Earth. The first major opportunity will be with the TESS spacecraft, launching in 2017, capable of detecting earth-sized planets around the brightest 500,000 stars. However, it's 2-year mission would limit the ability to detect an Earth-analogue. The best bet for finding other Earths will come later (2024 perhaps) with the launch of Plato, a six-year mission that again, studies the brightest stars. However, there is then a big leap forward required to perform studies of the atmospheres of these planets. Direct imaging and spectroscopy would probably require space-borne nulling interferometers; indirect observations of phase-effects and transmission spectroscopy through an exoplanet atmosphere does not require great angular resolution, just massive precision and collecting area. Spectroscopy of something the size of Earth around a normal star will probably require a bigger successor to the James Webb Space Telescope (JWST - launch 2018), or even more collecting area than will be provided by the E-ELT in the next decade. For example Snellen (2013) argues it would take 80-400 transits-worth of exposure time (i.e. 80-400 years!) to detect the biomarker signal of an Earth-analogue with the E-ELT! It has been suggested that new radio telescope projects and technology like the Square Kilometre Array may be capable of serendipitously detecting radio "chatter" out to distances of 50 pc ($\sim 150$ light years) - see Loeb & Zaldarriaga (2007). This array, due to begin full operation some time after 2025 could also monitor a multitude of directions at once for beamed signals. A good overview of what might be possible in the near future is given by Tarter et al. (2009). 

Trappist-1 was first catalogued by the 2MASS survey about 17 years ago and has the catalogue number 2MASS J23062928-0502285. It was identified as an ultra- low-mass star with a spectral type of M7.5 by Gizis et al. (2000) and Cruz et al. (2003), using a combination of 2MASS and proper motion. The reason it was monitored by the Trappist telescope is that is was found to be reasonably close ($12.2\pm 0.4$ pc) by Costa et al. (2006) (who assigned it a spectral type M8) and is therefore quite bright for a star of its type at $V=18.8$. 

Impacting solar system objects would have relative closing speeds from around 11 to 72 km/s. We could take the optimal case that the asteroid approaches whilst fully lit by the Sun (which I think precludes the minimum and maximum speed in the range quoted above) and then scale from another similar body - say the asteroid Vesta. This has a diameter of around $a=520$ km, gets as close as $d=1.14$ au from the Earth and has a maximum brightness of about $m=5.2$ apparent magnitude (and hence just visible to the naked eye) and an observed flux $f = f_0 10^{-0.4m}$, where $f_0$ is a zeropoint for the magnitude scale. Thus the flux $f_n$ received by a near-Earth asteroid of diameter $a_n$, at a distance $d_n$ from Earth and with the same reflectivity would be $$ f_n = f\left(\frac{a_n}{a}\right)^2 \left(\frac{1+d}{1+d_n}\right)^2 \left(\frac{d}{d_n}\right)^2$$ The magnitude of the dinosaur killer would then be $$m_n = m -2.5\log (f/f_n)$$ To be an at all conspicuous naked eye object, $f_n \geq f$. If we assume $a_n=10$km, then $$ d_n^2(1+d_n)^2 \leq 0.0022$$ An approximate solution is obtained by assuming $d_n \ll 1$ and thus we find $ d_n \leq 0.047$ au or 7 million km. Moving at say 30 km/s, then it gets closer by 2.6 million km per day, thus hitting the Earth about 3 days after becoming a naked eye object. Obviously this would be longer for a slower approach speed or for a larger or more reflective asteroid. But shorter for a smaller, faster asteroid or if the asteroid approached from a direction not fully illuminated by the Sun. It thus seems to me that there is a plausible range of parameters and trajectories where a dinosaur-killing asteroid could be observed and then observed to grow brighter over a few nights, but probably not much longer than that. 

Perhaps the way to answer this is ask - could we detect the planets in our solar system if we were looking at the Sun, using current technology, from distances of many light years? The short answer is that we could detect Jupiter using the Doppler radial velocity technique, if we observed for more than 10 years (at least one orbit is required). If we were lucky, and the orientation is right, we might then also be able to detect a transit of Venus or the Earth, using a satellite observatory like Kepler. Kepler could detect Earthlike planets by the transit technique, but the solar system is not "flat" enough that you would observe multiple transiting planets. So the answer is that we would currently have seen Jupiter and maybe one other planet. Therefore we cannot at the moment conclude that 8 planets is an unusually high number; it may be quite typical. Although we do know that solar systems can be much more densely populated with planets (in their "terrestrial planet zones") than our own. 

The solution to the Friedmann equation in a flat universe is $$H^2 = \frac{8\pi G}{3}\rho + \frac{\Lambda}{3},$$ where $\rho$ is the matter density (including dark matter) and $\Lambda$ is the cosmological constant. As the universe expands, $\rho$ of course decreases, but $\Lambda$ remains constant. Thus the Hubble "constant" actually decreases from its current value $H_0$ and asymptotically tends towards $ H = \sqrt{\Lambda/3}$ as time tends towards infinity. As $\Lambda = 3H_0^{2} \Omega_\Lambda$, and measurements suggest that $\Omega_{\Lambda} \simeq 2/3$, then $\Lambda \simeq 2H_0^2$, and the Hubble parameter will therefore decrease to approximately $\sqrt{2/3}$ of its present value if the cosmological constant stays constant. Of course if $\Lambda = \Lambda(t)$, (ie not the basic $\Lambda$-CDM model) then the behaviour will be different. EDIT: Another useful form of the solution (for the case of a constant vacuum energy density) is $$H^2 = H_0^2 \left( \frac{\Omega_r}{a^4} + \frac{\Omega_M}{a^3} + \frac{\Omega_k}{a^2} + \Omega_{\Lambda}\right),$$ where $H_0$ is the Hubble parameter now, $a(t)$ is the scale factor of the universe, $\Omega_r$ is the current (i.e. $a=1$) ratio of the radiation density to the critical density and $\Omega_M$, $\Omega_k$ and $\Omega_{\Lambda}$ are the equivalent densities for the matter (baryonic and dark), curvature and (constant) vacuum energy densities. As $a$ increases you can see that all three of the leading terms get smaller and the Hubble parameter decreases at all times. When $a$ is very large, $H$ approaches $\sqrt{\Omega_{\Lambda}} H_0$ as before. 

Although you might think that Y-dwarfs (brown dwarfs with temperatures $<500$ K) might become cool enough to support liquid water. In fact that does not happen. As discussed by Burrows et al. (2003) and Morley et al. (2014), in Y-dwarf atmospheres, when the temperature becomes low enough for water to condense out of the gas phase ($<375$K), it goes straight to water ice particles. Thus, although water vapour will condense in clouds in a cold brown dwarf, it will be in the form of ice particles, and there is not expected to be liquid water in a brown dwarf atmosphere. 

(a) We can't detect ALL of anything, but we can certainly makes estimates of their integral properties. For example, we estimate that there are about 100 billion stars in the galaxy, but we can observe only a tiny fraction of these. We have also observed a tiny fraction of the stellar black holes in our galaxy - those few examples that have revealed themselves through their binary companions. (b) Yes we do. It is just over 4 million solar masses and thus a negligible fraction of the Galaxy's total mass. (c) and (d) It is not just stellar evolution that is involved, it is the distribution of stellar birth masses (the initial mass function) that needs to be known, possibly as a function of time. The stellar evolution part has a major uncertainty in that although it is reasonably certain that all stars above about 10 solar masses end their lives in supernovae, it is not certain what fraction will leave black hole remnants. This is probably a strong function of mass - higher masses favour a black hole and progenitor metallicity; low metallicity stars lose less mass during their lives and are more likely to directly collapse to a black hole - see Heger et al. (2003). You can do a back of the envelope calculation. Suppose the IMF has the Saltpeter form $N(M)\propto M^{-2.35}$ between 0.1 and 100 solar masses. Further assume that: because a 1 solar mass star has a lifetime of 10 billion years, the age of the Galaxy is 10 billion years and the stellar lifetime is a very strong inverse function of mass $(\propto M^{-2.5})$, that the 100 billion stars that we deduce exist in our Galaxy (by counting up the local stars and measuring stellar densities elsewhere and extrapolating) are predominantly all the 0.1 to 1 solar mass stars that have ever been born. This allows us to then estimate how many stars have been born in any mass range - even if they have long-since died. So making the above assumptions, I will make the further assumption that all stars from 20-100 solar masses die and leave a black hole remnant.Then if my maths is correct then there are about $10^{11}/1400 \sim 7\times 10^{7}$ black holes in our Galaxy. That is there is 1 black hole for every 1400 stars of 0.1-1 solar mass. The remaining parts of the puzzle are what is the mass of a typical black hole and what is the mass of the Galaxy? A good number for the former is about 10 solar masses, since the black hole binaries appear to cluster at this value or a little lower (Ozel et al. 2010). The latter is still a topic of investigation. It appears to be an order of magnitude bigger than all the stars, gas and dust due to some form of dark matter. Let's use the number $7\times 10^{11}$ solar masses (Eadie & Harris 2016). So my final number is that black holes form $7\times 10^{7}\times 10/7\times 10^{11} = 0.1$ percent of the Milky Way mass. You could push this number up by a factor of a few by assuming that the IMF contained more higher mass stars in the past, or that the threshold mass for black hole production was lower in the past for low metallicity stars. Both seem theoretically possible and are active topics of investigation. 

The key to understanding this is the concept of forbidden lines and forbidden transitions. A forbidden transition is one that cannot occur radiatively via an electric dipole interaction. Instead, it must proceed either through magnetic dipole or electric quadrupole emission, with much lower probability, or the transition may be accomplished through collisional (de)excitation. A forbidden line is the radiation at a wavelength corresponding to a forbidden transition. In laboratory plasmas these are usually not seen - hence the term forbidden lines - because the transitions are normally accomplished through collisional de-excitation on a timescale much shorter than the radiative lifetime through magnetic dipole/electric quadrupole emission. However, in astrophysical plasmas the densities can be many orders of magnitude lower than even the best laboratory vacuums. In this case, forbidden lines are seen and indeed are often an important means of radiative cooling. The property of forbidden lines which makes them an excellent density-sensitive tool is that they can be "quenched" if the density becomes large enough to make collisional de-excitation more probable than radiative de-excitation. The strength of the forbidden line is thus sensitive to the electron density (which is what dominates the collisions) between densities where "quenching" starts to become effective and higher densities where the line essentially becomes unobservable. Taking the specific example of OIII (doubly ionised oxygen) optical line emission. There are three forbidden transitions of interest between the $^1D_2 \rightarrow\ ^3P_2$ (501 nm), $^1D_2 \rightarrow\ ^3P_1$ (496 nm) and $^1S_0 \rightarrow\ ^1D_2$ (436 nm) states. These transitions produce optical forbidden lines which are quenched at different characteristic densities. Measuring a line strength ratio is important because the ratio will be independent of the abundance of OIII. The ratio will depend on electron density, providing the electron density is $>10^{5}\ cm^{-3}$, and temperature (at lower densities, the ratio is just temperature dependent). To get the density in this case requires more information - usually supplied by a similar line ratio for something like NII, which has a similar set of forbidden lines, but with differing density and temperature dependence. A more straightforward route is to use the line ratio for forbidden lines of OII or SII where there are a closely spaced pair of energy levels, both undergoing forbidden transitions to the same lower level. In this case the ratios are density sensitive, but not temperature sensitive, again over a range of densities where one or other of the transitions is in the process of being quenched (e.g. for OII 372.6/372.8nm, $10^2 <n_e < 10^{6}\ cm^{-3}$.) 

Yes. In terms of pulsar timing measurements, this is a massive effect! A +/- 30 km/s doppler shift changes the pulsar frequency by +/- 1 part in 10000. This sounds small, but the accumulated phase shift over many periods is readily apparent. In addition, the light travel time across the solar system has to be taken into account, as well as the Earth's rotation and some other smaller effects - such as Shapiro delay. If the question refers to the specific annually varying difference in clock rates caused by the different gravitational potential experienced by an Earthbound telescope on an elliptical (as opposed to circular) orbit -the answer is still yes. This is item 4 in the list of applied corrections given on p.52 of "Pulsar Astronomy" by Lyne et al. The maximum effect is a rate change of $3\times 10^{-9}$ leading to a maximum lead or lag of 1.7 ms.