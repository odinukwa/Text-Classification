Following Gaius post: You can create an .SQL script which does what you need with use db in front of the script -> create a SQL Agent job of Operating system type which calls the script: sqlcmd -E -S SERVERNAME -i"c:\YOURSCRIPT.sql" -o"C:\YOURSCRIPT_LOG.log" Add new step and use msdb.dbo.sp_send_dbmail procedure to send email. This feature can be customized to display inside the mail a specific query from SQL tables to confirm the execution of the script... for example dbcc showcontig of your rebuild indexes. 

If you still want to develop smth custom where you can log what info you want take these steps as start: Create tables with servers, databases linked to server id, backup info linked both to server id and db id. Create scripts that will be launched on destination servers using xp_cmdshell. The scripts will insert collected info to destination tempdb table and insert through linked server to your DBA server. This is very shortly. you can contact me on private if you need more details. I am using this method as it is the best way to track many stuff. 

Check to Show all columns and select each one of the columns under Performance: Performance statistics event only. The rest of events can be left with default setting. Next, Select Column Filters and filter by DatabaseName and/or LoginName/ApplicationName/HostName etc.., if you know them. The purpose is to limit the number of rows dispalyed in Profiler and concentrate only on your needs. Next, press Run and let it run for a while (2-3 min as long as you need). Analyse the results dispalyed looking primarily at: Performance statistics event. If Performance Statistics will occur often it means that the plan of a query was cached for the first time, compiled, re-compiled or evicted from PlanCache. From my knowledge if a query does not have its query plan in Plan Cache - you will see 2 rows of PerformanceStatistics event and followed by SQL:BatchStarting, then SQL:BatchCompleted. It means that the Query Plan was first compiled, cached and then the query started and completed. Look at following columns under Performance Statistics event: 

That way, SQL Server is able to parse the statement and will notice that it's a command that contains sensitive information and will not show it in any trace output. it will show up as: 

OUTPUT: File activation failure. The physical file name "e:\SQLServer\Log\RepSource_log.ldf" may be incorrect. The log cannot be rebuilt because there were open transactions/users when the database was shutdown, no checkpoint occurred to the database, or the database was read-only. This error could occur if the transaction log file was manually deleted or lost due to a hardware or environment failure. Warning: The log for database 'RepSource' has been rebuilt. Transactional consistency has been lost. The RESTORE chain was broken, and the server no longer has context on the previous log files, so you will need to know what they were. You should run DBCC CHECKDB to validate physical consistency. The database has been put in dbo-only mode. When you are ready to make the database available for use, you will need to reset database options and delete any extra log files. Now, if you look at the output you can see why transactional replication would no longer work: 

You can see that the "size" property is changed to reflect the new size. However, the "MinSize" property still contains the "Initial" size. It's the minimal size to which the shrink command will go. However, having said all this. I still don't understand why you want to complicate things by first altering the initial size and then shrink to that initial size. Instead of just shrinking directly to a targetsize. Anyway, to answer your question. The "initial" size is not exposed as a property to the user/dba. 

Keep in mind, that you should evaluate this over time. Your workload, (data mod) might change over time. Also keep in mind what your current problems are. Page plits cause extra load on the log file (potentially a lot). Extra writes to the data files (higher checkpoint peaks). And read ahead reads become less effective because of fragmentation. However, low fill factors, eat up buffer space, increase read load. One thing to be very careful about is that the pagesplits/sec counters Aren't really reliable. They keep track of all page splits. (sql 2012 is better) What I mean by all is that when you insert a record at the very end of an index, effectively you need a new page added to the index to continue. The adding of the new data page is counted as a pagesplit as well. But in reality nothing is split. So in order inserts on a cluster index will still have page splits when you look at the pagesplit/sec counter, but they aren't bad. So only look at this counter in relation with other facts. For example: Unexplainable high trans log load together with high out of order inserts together with high page splits/sec. One last word of warning. Especially on large tables. Always check what the current page density is of your index and calculate what the size impact would be when you rebuild the index with a different fill factor. (Rebuilding an index with a 100% page density with a new 50% fill factor will effectively double the size of the index) 

I have figured this on my own and I wrote on my blog. For those interested in the solution visit this posts: RangeS-S, RangeS-U, RangeX-X 

Also, important stuff to pay attention to understand the different behavior: are the DB2 and DB3 identical in minor versions of MySQL? You only provided the major version which is 5.6. But from 5.6.6 there have been some important changes mainly on this issue. The replication to DB3 is done using the binlogs created by DB2 and applying the sql_mode specified on DB3. So these should be checked and seen what exactly is trying DB3 to replicate from the binlogs created by DB2? Can you paste the query here? Out of curiosity , if you test an insert from DB1 to DB2 and it goes ok, mysql doesn't throw any warnings? 

Looks like the disk subsystem on Server B is performing worse than on Server A but, I used to see disk issues not entirely related to disk specs. You could collect some other performance counters such as physical disk --> avg disk sec write (> 25 ms very slow), memory --> page file usage (> 70% bad), cpu --> processor queue length (> 12 very bad), memory --> pages/sec(> 600 slow, > 2500 very slow disk subsystem), sql server: buffer manager --> page life expectancy (< 300 memory issue). You can also limit the growth of 6 tempdb data files to the size they have now. See what happens. You should check the waitstats as well on both servers. If you see a lot of PAGELATCH_XX then you will know where to dig more. Jonathan Kehayias has a good article on this theme. And Paul Randal has also a lot of analysis which could help. 

the script I use will show you all objects (SP, Tables, Functions) for a specified Database name and all the users that have rights on them, but you can narrow the search and extract exactly what you need. 

as long as you use tiny int (0 to 255) using a char(15) or 15 tinyint is the same (size wise). So then from a performance perspective, go for the 15 tinyints since you save on the extraction and string handling. UPDATE if the marks are double digits, you'll need CHAR(30) and that is twice the size of 15 times a tinyint. 

By copying the complete DATA directory, you most likely also copied the system databases, specifically your master database from your old server and overwritten the system databases of your second SQL Server. When copying a USER database in general, the easiest method is to make a backup of that user db on the source server and restore that backup on the destination server. there is no need to ever copy the master, model, msdb or tempdb files from one server to another just to migrate a user database. To fix your server again: 

How big is the initial database size? What will be the daily, monthly growth rate? What will be the daily growth rate of the transaction log? is it possible to have both locations in the same domain? Is it possible to build a secure connection between the two locations? What is the bandwidth between the to locations? what is the latency? 

There are other causes for page splits but these are the biggest ones. All of the above will cause fragmentation. Now that's bad, but in a strange way good. Because that is about the only thing that can properly help you decide on the proper fill factor. I'll explain why. What you do is: See if any of the above apply. For the indexes that do, do the following. 

General setup I'm using the following script to setup a publisher, distributor, subscriber on the same server, create a source and destination database and a table with 10k rows and a publication and subscription for this table. I'll use this setup for the next tests: