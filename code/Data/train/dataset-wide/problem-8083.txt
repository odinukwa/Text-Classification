Such a matrix (integer version) can be constructed from a magic square, say with entries $1,\dots,n^2$, by adding suitable multiples of all $n\times n$ permutation matrices: e.g. take $N=n^2$ and add all $N^iP_i$, where $P_1, \dots,P_{n!}$ are in any order. This is such a wasteful construction that the question raised in my comment above might deserve some attention: what are good upper bounds for the common row and column sum of such a matrix? 

It is in fact straightforward to see here that both "inner pentagrams" must belong to any 2-factor. In view of this kind of construction, it doesn't seem of high interest to ask which partitions can be realized by 2-connected graphs. But it is much more interesting to look at 3-connected graphs. It is not at all obvious for instance that there are 3-connected graphs which have Hamilton cycles as their only 2-factors. The following construction gives one for $n=26$: 

Some examples : $n=3$: best remainder is $2x+1$, e.g. $M=\begin{pmatrix} 1&x&x\\ x&1&x\\ x&x&1\\ \end{pmatrix}$ with signature $(222,222) $ $n=4$: best remainder is $3x+2$, e.g. $M=\begin{pmatrix} 1&x&x&x\\ x&x&1&1\\ x&1&x&1\\ x&1&1&x\\ \end{pmatrix}$ with signature $(3222,3222) $ $n=5$: best is $5x+4$ for $M=\begin{pmatrix} x&x&1&1&x\\ x&x&1&x&1\\ 1&1&x&x&x\\ 1&x&x&1&1\\ x&1&x&1&1\\ \end{pmatrix}$ with signature $(33322,33322)$ $n=6$: best is $9x+9$ with $M=\begin{pmatrix} \color{blue}x&\color{blue}1&x&x&1&1\\ \color{blue}1&\color{blue}x&x&x&1&1\\ 1&1&\color{blue}x&\color{blue}1&x&x\\ 1&1&\color{blue}1&\color{blue}x&x&x\\ x&x&1&1&\color{blue}x&\color{blue}1\\ x&x&1&1&\color{blue}1&\color{blue}x\\ \end{pmatrix}$ and signature $(3_6,3_6)$. Note the circulant block structure of the $2\times2$ blocks, which will be referred to below. $n=7$: very probably best is $32x+24$ with, e.g., $M=\begin{pmatrix} x&1&1&\color{blue}x&1&x&x\\ 1&x&1&\color{blue}x&x&1&x\\ 1&1&x&\color{blue}x&x&x&1\\ \color{blue}x&\color{blue}x&\color{blue}x&\color{blue}x&\color{blue}1&\color{blue}1&\color{blue}1\\ 1&x&x&\color{blue}1&1&x&x\\ x&1&x&\color{blue}1&x&1&x\\ x&x&1&\color{blue}1&x&x&1\\ \end{pmatrix}$ or $M=\begin{pmatrix} x&x&x&1&x&1&1\\ 1&x&x&x&1&x&1\\ 1&1&x&x&x&1&x\\ x&1&1&x&x&x&1\\ 1&x&1&1&x&x&x\\ x&1&x&1&1&x&x\\ x&x&1&x&1&1&x \end{pmatrix}$, which both have signature $(4_7,4_7) $. Note the symmetry of the first one and the $3\times3$ blocks in the 4 corners. The second one is not symmetric, but circulant. BTW the question whether both matrices are essentially the same (i.e. obtainable from one another by suitable permutations of rows and columns) has motivated this question. $n=8$: best so far is $56x+40$ with $M=\begin{pmatrix} 1&x&x&x&1&x&\color{blue}1&\color{blue}x\\ x&1&x&x&x&1&\color{blue}1&\color{blue}x\\ x&x&1&x&x&x&\color{blue}1&\color{blue}1\\ x&x&x&1&x&x&\color{blue}1&\color{blue}1\\ 1&x&x&x&x&1&\color{blue}x&\color{blue}1\\ x&1&x&x&1&x&\color{blue}x&\color{blue}1\\ \color{blue}1&\color{blue}1&\color{blue}1&\color{blue}1&\color{blue}x&\color{blue}x&\color{blue}x&\color{blue}x\\ \color{blue}x&\color{blue}x&\color{blue}1&\color{blue}1&\color{blue}1&\color{blue}1&\color{blue}x&\color{blue}x\\ \end{pmatrix}$ and signature $(5_64_2,5_64_2) $. Look again at the $2\times2$ blocks. The sequence $2,3,5,9,32,56...$ of the $a_n$'s is the same as A003432, which is the largest determinant of a {0,1}-matrix of order n. This is clear from the following argument: If $M\in\mathcal M_n$ with $\det(M)=f(x)=(x-1)^{n-1}(ax+b)$ and $M^\sim$ is defined by swapping the $1$'s with the $x$'s, we have $f^\sim:=\det(M^\sim)=x^nf(\frac1x)=(x-1)^{n-1}(bx+a)$, so by letting $x\to\infty$ in $M$ we get for the leading coefficient $f_n=f^\sim(0)=a$, while on the other hand putting $x=0$ in $M^\sim$ yields a {0,1}-matrix with determinant $a$. Further, all these steps can be reversed. Now I am also wondering what is the link with the "Hadamard maximal determinant problem", which asks when a matrix of a given order with entries -1 and +1 has the largest possible determinant. The relationship between $\pm1$-matrices and 0-1-matrices is vaguely explained on the dedicated site as "a consequence of a mapping between binary and sign matrices" (which is supposedly bijective). But e.g. for $n=6$ the extremal $\pm1$-matrix \begin{pmatrix} -&+&+&+&+&+\\ +&-&+&+&+&+\\ +&+&-&+&+&+\\ -&-&-&-&+&+\\ -&-&-&+&-&+\\ -&-&-&+&+&-\\ \end{pmatrix} (essentially unique, up to permutations and negations of rows and columns) has obviously symmetries corresponding to $3\times3$ blocks, while the symmetries of the (also essentially unique) extremal 0-x-matrix above correspond to $2\times2$ blocks. The intriguing thing is further that Hadamard matrices only seem to encapsulate the $a_n$'s of the 1-x-matrices, but not the $b_n$'s. Well, it all may depend on the mapping. 

First, this model is typically denoted $D(n,p)$. I'm not aware of a formal name, but personally I like to call it the Bernoulli random digraph. For $p=c/n$, note that the number of vertices with out-degree zero is going to be on the order of $n$. So in this range, the number of strong components will also be on the order of $n$. The phase transition for this model where a large strongly connected component emerges is due to Richard Karp and Tomasz Łuczak (Łuczak considers the analogous model $D(n,m)$ where $m$ directed edges are uniformly at random chosen to be present). Karp shows that for $p=c/n$ and $c<1$, and any $\omega \to \infty$ however slowly, w.h.p. each strong component of $D(n,p)$ has at most $\omega$ vertices. For $c>1$, there is a unique strong component with roughly $\Theta^2 n$ vertices where $\Theta$ is the unique positive root of $1-\Theta = e^{-c \Theta}$; in this case, w.h.p. all other strong components contain at most $\omega$ vertices. It is not a coincidence that the number of vertices in the giant component of $G(n,p=c/n)$ is roughly $\Theta n$ vertices. Boris Pittel and I recently proved that for $D(n, p=c/n), c>1$, the number of vertices in the largest strong component is asymptotically normal. In fact, we show that the pair of the numbers of vertices and arcs in the strong giant is jointly asymptotically 2-dim normal. 

When searching for the originator of the method of removing radicals from equations I found the following remark by D. Mooney: 

Is there already an established name for properties or subgraphs of finite, symmetric graphs, that are invariant under a change of vertex weights? Some of those invariants are 

If the number and position of the knots are fixed, then the problem is a linear least squares problem for determining the coefficients of linear B-Splines (cf e.g $URL$ if the knot-positions also have to be determined, then the problem becomes non-linear, but is also studied in approximation theory. This article seems to fit your needs: "Fixed- and Free-knot Univariate Least Squares Data Approximation by Polynomial Splines" by Maurice Cox, Peter Harris and Paul Kenward ($URL$ A heuristic for finding a good knot-placement would be to start with an approximation for evenly spaced knots and then move them to the places, where the error is maximal. The placing of the knots to the places where the error is maximal can then repeated as long as subsequent approximations with the new knot-placements yield better error measures. 

One can break up the probability that a specific vertex is in a component of size $k$ by $$ P(v \text{ is in a component of size }k) = {n-1 \choose k-1}P(k,p)q^{k(n-k)}, $$ where $P(k,p)$ is the probability that $G(k,p)$ is connected. However, now you would need to find $P(k,p)$, which is just as complicated. Often in random graphs, one only cares to bound this probability from above, such as an union bound over all spanning trees, $$ P(k,p) \leq k^{k-2} p^{k-1}, $$ or Stepanov's Inequality, $$ P(k,p) \leq (1-q^{k-1})^{k-1}. $$ 

Using the Chen-Stein method, one can bound the total variation distance between a sum of possibly dependent Bernoulli random variables $W=\sum_{i=1}^n X_i$ and a Poisson distribution using only the first and second moments of the $X_i$. For example, see Two Moments Suffice for Poisson Approximations, by Arratia et al.. Is there an analogous bound for the total variation distance in the multivariate case? For example, if \begin{equation*} W_1 : = \sum_{i=1}^n X_i, \ \ \ W_2 : = \sum_{j=1}^n Y_j. \end{equation*} are sums of (not necessarily independent) Bernoulli random variables, then can we bound $d_{TV}(\mathcal{L}(W_1, W_2), \mathcal{L}( Poi(E[W_1]), Poi(E[W_2]))$, using the first and second moments of $X_i$ and $Y_j$ (these poisson random variables are independent)$? In the literature, I've only found bounds for the case where these Bernoulli random variables are presumed to be independent. EDIT: Whoops. I should have read through the paper that I cited. See Theorem 2 in Arratia et al. 

A candidate curve with few directions with finite number of intersections might constructed from the Blancmange curve as a fractal curve, that can serve as a building block for constructing "evil" Jordan curves with Hausdorff dimension 1 (for the properties of the more general Takagi functions refer to e.g. this article). The simplest idea would be to take the union of the curve itself and its reflection at the x-axis (refering to the images on the linked webseite). A bit more complicated is the idea of constructing an analoge of a "sine wave" by point-reflecting the curve at one of its endpoints and use a phase shifted version of the result as an analogue of a "cosine wave". A parametric function $x(t)=cosinewave(t),\ y(t)=sinewave(t)$ may be even better, but I haven't done any investigations on whether the set of inner point of the region bounded by that "Blancmonge circle" is connected. Caveat: But, despite having Hausdorff dimension 1, the resulting curves are not rectifiable and thus not an example of the sought kind of curves.