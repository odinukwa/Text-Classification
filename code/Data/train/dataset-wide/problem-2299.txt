That means that I probably got a bit unlucky and I had just a bit more data than would fit in 79 extents totaling 64 MB so I had to allocate an 80th extent that was 8 MB in size, for a total of 72 MB. We can use the dbms_space package to get more details about how much space is being used. When we do that, we see that we're actually only using 66.22 MB of the 72 MB that have been allocated. So our actual estimation error is really only ~10% 

I would strongly suggest that you always alias any tables in your query and that you use those aliases for every column in the list. That makes it much easier to see where the different data elements are coming from and makes it easier when you add additional joins in the future. 

The two major benefits tend to be efficiency and flexibility. Streams is a much more efficient architecture. Rather than synchronously capturing changes in materialized view logs which adds overhead to the transaction, Streams mines the redo logs that are generated by the change in order to generate the change vector to send to the target system. That also allows the change to be replicated to the other system(s) much more quickly. Streams is a much more flexible architecture. You can write your own custom apply processes which makes it relatively easy to inject additional processing or logging, to ignore certain changes, etc. You can send a Streams logical change record (LCR) to a JMS queue or to a non-Oracle system (though, of course, you'd need to write code to apply the LCR on the non-Oracle system). 

There doesn't appear to be a problem. The listener is a process that generally runs on the database server, not on the client machine. Doing a client-only install will not install a listener. If you want to have a listener on your machine, you'd need to do a database install (though you can do a software-only install rather than actually creating a database). If you're just trying to connect to a remote database, however, you'd use the listener that is (presumably) running on the database server. It's theoretically possible to install a listener on one machine for a database running on another machine but that isn't particularly common. It would be extremely, extremely unusual to install a listener on the client machine that listens for connections to a database on a remote server. 

in a (from a user standpoint) idle database, you'll note that the is incrementing every few seconds. That is Oracle doing various sorts of housekeeping. That housekeeping will generate a small amount of redo each time. Over the course of a full day, it's pretty common that this would add up to enough for a couple of archived logs even if your system is otherwise pretty idle. 

You can't use because there is no Oracle Text index on the database source. You could, I suppose, write a query that copied the data from to a custom table, create an Oracle Text index on that table, and search that table using the function. It would generally make more sense, though, to just query with a query 

No database can possibly work around deadlock errors in general-- in Oracle, a deadlock indicates a bug in the application, not in the database. The Oracle database will detect the deadlock condition (i.e. session A has a lock that session B is waiting on and session B has a lock that session A is waiting on) and terminate one of the blocked statements to resolve the problem. Your application ought to avoid creating a deadlock in the first place-- one option is to always generate locks in the same order in every session. 

Oracle doesn't have an (or a or an ) table. That's something that SQL Server provides in a trigger. The Oracle substring function is also not . My guess is that you just want 

which works out to 59.19 MB. Now, let's test our estimate We'll insert 1 million rows where goes from 1 to 1,000,000 and is a string with a random length between 1 and 100. 

This takes advantage of the fact that Oracle does not index entries where all the columns in the index are in order to reduce the size of the index. In order for your queries to use the index, however, you'd need to use the same expression that you used to create the index. Something like 

A fast refresh would copy incremental changes over the network but requires that a materialized view log be created on the master site on the source table. That adds some overhead to the inserts happening on the master table but would generally make the refresh more efficient. A complete refresh would copy every row over the network every time the materialized view is refreshed. That is likely to be less efficient from a refresh perspective but there will be no overhead to inserts on the source table and the master site does not need to create a materialized view log. Oracle provides a host of data replication technologies-- materialized views are the oldest and probably the least efficient but are relatively trivial to set up. Streams is a newer technology that has much lower overhead but is quite a bit more complex to set up. Golden Gate is the preferred replication technology today but that has extra licensing costs. 

Metalink 207303.1 is the Client/ Server Interoperability Matrix which will be the definitive documentation. Since the express edition of 10g is based on Oracle 10.2, a database link to an 8.1.7 database should be supported. Database links to earlier versions of 8i (8.1.5 or 8.1.6) are not supported from a 10.2 database. For general sanity, I'd strongly suggest that the 8i side be running at least the 8.1.7.4 patchset but that isn't strictly required. 

Your list of privileges contains a number that don't exist. There is no privilege. Nor is there a or or privilege. You can grant a user access on individual tables that are owned by other users. 

If you're designing an OLTP database, you should really design it in third normal form initially. If, once you've done that, you find that you can make measurable performance improvements by denormalizing something and there is no other way to get those improvements, it makes sense to denormalize selectively. It does not make sense, however, to denormalize to first normal form proactively. I don't really understand why you expect that adding a mapping table would be problematic. Yes, you'll need to join to it. But that's what a relational database was built to do-- it joins tables very very efficiently. Plus, you won't have to constantly parse the delimited string to figure out the keys. And you will be able to create a foreign key that ensures that the keys in the mapping table are valid and that you don't have keys in your delimited string that don't map to a valid row in . I don't really see the downside. If you have a separate mapping table and you want to return a single row of data to the client with the data and the keys (which seems awfully suspicious-- the keys are only useful if you are going to subsequently use them to query in which case you almost always want to just let the database join to in the initial query) you can always pivot the data in the mapping table to return a delimited string of keys. I certainly wouldn't recommend that but it can be done.