Know your app. If your database has 14 GB of free space in the data files and your app is generating 2 GB of activity a day then there will be an auto-growth event in 7 days. If your database is set to "Autogrow by 4 GB" then there will be an auto-growth event every 2 days after that. Data Collection is an easy-to-set-up monitoring system that allows you too look at the history f database growth for a number of servers. 

Consider the [Date] or [Created] column for a clustered, non-unique, index key (especially if they are increasing columns) and keep the [TypeID] column as a non-clustered, unique, key. Clustered index keys don't have to be unique - SQL Server will add a uniquifier to non-unique values. As long as most of the values are distinct, this shouldn't noticably impact performance. 

A non-RDBMS suggestions: if you are doing these sorts of queries often then I would suggest creating a cube of the source data and aggressively pre-aggregating the cube. This does, of course, require an installation of Analysis Services and a DBI with Analysis Services experience but if you are doing OLAP queries often then creating the data warehouse may be cost-effective in the long run. If you have Excel 2010 or 2013 then PowerPivot for Excel is a cheaper alternative to a full Analysis Services installation. 

Look to see if the data is stored somewhere else. Perhaps the system sends email confirmations. If so, retrieve the emails (from sent items or from auditing on the email system) and hire some temps to retype the information. Perhaps the system prints out reports. If so, obtain the printouts and get the temps typing. Perhaps the system exports data and sends it somewhere else. If so, get the exported data back from where it went and get those temps typing. 

Note: I came here from another answer where I misstook SQLIO for SQLIOSim. Hopefully I'll get it right this time. :-) SQLIOSim is "NOT" an I/O Performance Tuning Tool because it uses random patterns, so is not repeatable (a primary requirement for benchmarking). It is a stress-testing tool. Additional links: 

After switching to a different master you must reseed the identity value, otherwise you will get duplicates in the id column. Use the statement. 

Log Shipping essentially involves three jobs. The first job backs up the log on the primary server and stores those backups in a local folder. The second job copies those files across the network to the secondary server. The third jobs restores those backups using the WITH STANDBY option. Log Shipping is set up on an entire database. The database on the secondary server is accessible but is read-only. Requires: A shared folder on the primary server. Firewall configuration to allow the secondary server to access the file share. Security set up so the proxy on the second server has permissions to access the file share. This may be an issue for you in a hosted situation. Caveat: Log shipping is not real-time. The common interval for the first job is 15 minutes. An unplanned failover will lose data. Caveat: Log Shipping uses transaction log backups, so its design must be done in conjunction with your backup design. Database Mirroring involves the primary server sending individual transactions to the secondary server, in either a synchronous fashion or an asynchronous fashion (asynchronous is enterprise edition only). Like Log Shipping, Mirroring It is set up on an entire database. Unlike Log Shipping, the secondary database is inaccessible. Requires: An endpoint to be created on each sever for the mirroring traffic. Firewall allowing traffic from and to that one port. In a synchronous mirroring setup, an unplanned failover will not lose data. Both Mirroring and Log Shipping are creating a copy of an entire database so there is no object requirements. Heaps, Clustered Tables, tables with and without keys - all get copied. Note: Given the requirement for file sharing, I disagree with Szymon's comment about log shipping being easier to set up and maintain and requiring less resource. Installing the File Server role on a Windows Server is increasing resource requirements as well as increasing the surface area of attack. Additionally, in a log shipping unplanned failover, bringing the secondary online is a pain. Lots of steps, most of which involve running stored procedures in a query window. 

Nothing should ever access the Staging Database, because that is where partially-cleansed data is stored and no systems should ever access partially cleansed data (except, of course, those processes that finish the cleansing and load the data into the Data Warehouse Database). Now, once the data has been fully cleansed then allowing Operational Source databases to access it can be considered, based on requirements like timeliness of data, speed of access, write-back, and so on. Personally, I think the Kimball one-way flow ( Operational Source → Data Warehouse Star Schema → Data Warehouse Cube → Presentation) is a good idea so it would take a very compelling argument for me to implement Data Warehouse → Operational Source. 

I know you asked about reading log files but I there are better strategies for dealing with slowly changing dimensions. If you have the Enterprise Edition of SQL Server then you can use Change Data Capture. It needs to be set up on the database and tables beforehand but will then track inserts, updates and deletes. You can use the lighter-weight Change Tracking. Microsoft have published a comparison of Change Data Capture and Change Tracking. You can use a column to implement high-watermark tracking. The nightly process records the current highest rowversion then queries the table for all rows with the rowversion greater than last night's highest value. This is for inserts and updates. If you wish to track deletes then you need to write delete triggers that change a "deleted" flag. You can use triggers on the tables that copy the inserted, updated and deleted data into tracking tables. The nightly process reads then empties these tracking tables. As an aside, this is similar to how merge replication and updating-subscriber transactional replication work. All of these methods require modification to the source database objects. I'm guessing from your question that this might not be a possibility. If that's the case, see VonPryz' answer. 

"…is there any benefit to changing the GUID generation to sequential using newsequentialid()?" No. Sequential GUIDs are only appropriate when there is a clustered index on the GUID column and you want to avoid page splits caused by inserts. Edit to address the comment below: All nonclustered indexes suffer from page splits when data is inserted. For example, when you enter a record for Homer Simpson, it gets entered into the "S" leaf pages for the LastName index, possibly causing a page split. You don't however, require that customers join in strict alphabetical order. Additionally, the latch system used for index leaf and non-leaf pages means that page splits require less processing time and resource than page splits on data pages. Further to this, what would it require for the OP to change to a sequential id? They would have to replace the Default constraint on the Key column to NEWSEQUENTIALID(). This will not affect any existing rows in the table (which is good because there are foreign keys using those keys) - just the new rows. From that point on, inserted rows will have increasing keys but those increasing keys are not necessarily going to be greater than the existing data in the table (NEWSEQUENTIALID() only guarantees that the GUID is greater than any other GUID generated by NEWSEQUENTIALID() on that computer since it restarted). This means whose inserts are still going to cause page splits in the nonclustered index! 

The job owner determines the execution context of the job's steps. For a T-SQL job step: If the job owner is a sysadmin then the step will execute as either the SQL Server Agent service account or a specified database user (set on the Advanced page of the Job Step Properties dialog). If the job owner is not a sysadmin then the step will execute as the job owner. For other job steps: If the job owner is a sysadmin then the step will execute as either the SQL Server Agent service account or a specified Proxy. If the job owner is not a sysadmin then the step will execute as a specified Proxy (which requires the job owner to have permission to use the proxy). A SQL Agent Proxy is created based on a SQL Server Credential which stores the secret associated with some external security principal. For example, sysadmin Kim creates a Credential for ADVENTUREWORKS\Bob and then creates a Proxy based on that Credential. She makes the Proxy active for the CmdExec subsystem. She gives (nonsysadmin) Qin the rights to use that Proxy. Qin can now create a CmdExec job step that executes as ADVERNTUREWORKS\Bob. See Implement SQL Server Agent Security in Books Online. 

I have an alternative suggestion, applicable if you are running an edition of SQL Server rather than Express. Use a Maintenance Plan to do the backups on a schedule. SQL Agent will log the scheduled job' s execution results. 

If you are using an edition of SQL Server other than Express then you can use a Maintenance Plan to generate a new backup file every time a backup is made, as well as to remove old backup files if wished. Also see Maintenance Plan Wizard. 

First - stop using the phrase "Null value", it will just lead you astray. Instead, use the phrase "null marker" - a marker in a column indicating that the actual value in this column is either missing or inapplicable (but note that the marker does not say which of those options is actually the case¹). Now, imagine the following (where the database does not have complete knowledge of the modeled situation). 

Short answer: no. Sysadmins are gods. Long answer: Database administrators can use one or more of Server Audit (some editions of SQL Server 2008 and later), Change Data Capture (some editions of SQL Server 2008 and later, very limited information captures), Profiler (almost all editions and versions of SQL Server, including Analysis Services as well), SQL Trace (SQL Server 2008 and later, I think), DML Triggers (only for some operations) and DDL triggers (for SQL Server 2005 and later, and only for some operations). Also Extended Events, as mentioned by Stray Cat below. System administrators can use network sniffing to see everything you do on the network or remote control software to see everything you do on your workstation. All of these can be completely invisible to you. Modern operating systems almost all run on the assumption that you can't hide from sysadmins. If that were not the case then they would be unable to do their jobs. 

If it is sqlservr.exe that is listening then the next step is to use SQL Configuration Manager to see what instances of SQL Server are installed. One of them is using tcp1433. 

In addition to the points in other answers, here are some key differences between the two. Note: The error messages are from SQL Server 2012. Errors Violation of a unique constraint returns error 2627. 

You are in luck - I have exactly such a table here that I use to set up a SQL Server Analysis Services demo. This is for SQL Server 2008 R2. Table definition 

I can only comment on the SQL Server concept of databases, so this may or may not apply to other RDBMSs. In SQL Server, a database is (1) the boundary for HADR (high availability and disaster recovery) and (2) the boundary for security. Performance? SQL Server shares one buffer cache and one log cache for all sessions for all databases so there is little performance implication of one database vs many databases. If you have 15 megabytes of changes, that means 15 megabytes of traffic to be written to disk, regardless of how many databases it belongs to. Of course, more files does add a very small overhead in management traffic. Connections? One client application makes one connection to the SQL Server database engine service and can then query multiple databases. The client does not need multiple connections for multiple databases. However, since a database is the security boundary then if you want a session to query multiple databases then the login will require permissions set separately in those databases. In summary, usually one database is created for one application. For example, the Payroll database for the Payroll application.