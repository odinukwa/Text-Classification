It used to be that only swap partitions in /etc/fstab were used automatically, however, systemd may be changing that slightly. You might need to do: 

For security and performance, I would be inclined to push the incoming http handling into a presentation tier (1 or more servers). Static content can be served in the presentation tier. Also your database / business logic is less exposed to attacks. 

There are not a lot of size limitations on initrd (I've put 50 MB of extra stuff in there on occasion). I believe the initrd store is freed after the pivot is complete. So.... a couple things come to mind. You could just wait until the real /lib is available before inserting your special purpose module (you probably thought of this). Another option would be to load your special purpose module optionally based on a kernel cmdline parameter. In this way you could create a grub-menu that provides booting with or without the special module. 

If the incoming requests are http protocol, then perhaps Apache virtual hosts in conjunction with mod_proxy ( $URL$ ) may do what you want. 

"Server not found" from a browser often indicates a failure to resolve the host-name into an IP address. I notice that ubnt.aqtsolutions.com does not resolve, but the other two names do. It may be that your nginx configuration is fine and you just have a missing DNS entry. 

In Domain cluster, your properties really should be maintained in domain.xml To have the properties NOT on the JVM command-line, and updated immediately, you need to make sure that you are using boot-time=false on each system-property. 

If this doesn't work and the previous steps all did, check the firewall again, and check the firewall on the remote systems. 

Dumps wouldn't show no activity, they'd show lack of updates which is NOT the same thing. I do not know if mysql continually touches the databases that it has open or not. If it doesn't, mtime should tell you. If it does, you'll probably have to look at the logs. 

The way that I do this is to initiate a TCP connection to a known open port. If it's a web server, it will respond on port 80 (or 443). A positive response indicates that the machine is alive. A negative response indicates one of many possible problems, such as connectivity through the network, incorrect firewall configuration, service outage, or the machine being down. If you're setting up monitoring and just want to know when to pay attention to the machine, just rely on a known-open port. At least if it stops responding, you'll know something is up. 

I'm going to just add that before taking the advice of my esteemed fellow serverfaulties and erasing the line from your known_hosts file, you should talk to customer service at the other end and make sure that they did do something that would have caused this key change. 

The ".*" pattern got me into a lot of trouble way back in my youth, when I tried to remove all of the hidden files from someone's home directory... 

I got it! Unbeknownst to me, ScreenOS has the ability to pipe the output from any command to a tftp server! The usage is: 

I get the 503 then, too. What is going on? Is there some global setting I need to change? Note that this is a new Windows 7 workstation. I've checked and Classic ASP is definitely installed. The web site is associated with an application pool that has Classic ASP enabled, and 32-bit applications are also enabled. Event Log: 

I am being asked to resurrect a classic ASP application for temporary use. Wonderful. Well, the pay's the same, so I am now trying to get the thing to run on my localhost so I can test it and so on. I have copied all the files to c:\inetpub\wwwroot\ClassicASPapp. Using IIS 7.5 I set this folder as an Application I get handed a 503. Same result if I make it a Virtual Directory. 

Pick the website, right-click on it and choose Properties. Click the HTTP Headers tab and at the bottom of the panel is Mime Map. Click on the File Types button and use the resulting dialog to add file extensions and MIME types. Commenter is possibly correct, and ServerFault is possibly a better place for questions like this. But we coders have to worry about this kind of thing often enough, so I don't think it's too far off-topic. 

When users want to log into one of our servers, the default domain is set to the machine name instead of the organization's domain, which is to say that the domain dropdown shows the machine name in the text part, while the organizational domain does appear in the dropdown (populated by the network, I assume). I have heard that one may set a default domain to appear instead of the machine name, and I need this to be that way. I expect it's a registry entry, but I have given up finding one which applies. Can someone please tell me how to do this? 

To change the nice level of an already-running process, use "renice". So if I wanted my firefox session to go faster (dumb idea), I would do this: 

I would recommend a Mac server and using the Apple Directory Service to manage the users: $URL$ It's an LDAP backend and similar to Active Directory in some ways. Since you're a Mac house, it is probably your best bet, although you /can/ authenticate Macs against Active Directory (and other LDAP based models) if you really want to. 

People are really creative in ways that you wouldn't believe, in order to get clusters running and reliable. When it comes to clustering (or at least HA-clustering), there are shared-storage clusters and shared-nothing clusters. Shared storage clusters typically use cluster-aware filesystems on a centralized array, like a SAN. They use OCFS, GFS, or something similar. Services running on these are sometimes Active/Active, where both machines are fully capable of providing a full range of services to clients, and typically use weighted or round-robin style load balancing, but could also be setup as Active/Passive, where the "preferred" machine acts as a server until it fails, in which case another cluster member takes over. Shared-nothing clusters have been typically Active/Passive, since there needed to be a state change to activate the passive member. This is changing with the advent of things like DRBD, which uses block-level filesystem replication over the network. Between one of these two methods, pretty much every service that I can think of can be replicated across an array of servers, especially if you don't care much where you put your state files. Even NFS can be replicated without clients freezing if everything including the lock files are referenced from the centralized storage. Enterprise computing in general has been very dedicated to the mindset that single-machine uptime doesn't matter as much as service availability. To that end, services have been engineered such that one machine failure doesn't mean disruption to the users. 

Since I happen to have a CAL for a Windows 2003 server instance and plan to put together a home network using Win2k+3, I am wondering what books or other resources there might be out there giving advice, detailed or general, on how to do this. Also thinking as an alternative that I might be better off with Windows Home Server? Keep in mind that I am a developer, not a network admin. My primary goal here is to have a functioning small network so that I can, for instance, print documents from my laptop while using WiFi downstairs, have access to documents on a fileshare that I can work on while either upstairs or downstairs, and so on. I expect to have the server and two to three workstations on the network. 

Note that there is a file in wwwroot called "iisstart.htm". If I double-click on the file from Windows Explorer, the browser displays the rendered HTML correctly. If I try to navigate to the file using: 

I have a server that is running all versions of .NET Framework, from 2.0 up to 4.0. An application I need to deploy won't run correctly on later versions, and was originally compiled for 1.1. Is it safe to install version 1.1 on the machine without causing any problems with the later versions? Edited to add: To the accepted answer I must comment on how it went after actually installing .NET 1.1. It went fine, except there was one problem that occurred. Sql Server 2005 Developer Edition was on the machine, and after 1.1 was installed, Sql Server Management Studio was no longer working correctly, and had to be removed and re-installed. Only a small problem, but noteworthy. 

A dual homed machine capable of acting as your gateway router would be perfect for this. Here's a howto that might help: $URL$ Using that would permit you to watch the streams, packet counts, etc etc so you could examine every packet transmitting across your network. 

If MySQL isn't getting enough ram, it usually just dies. That also isn't affected whenever you rsync files. So you're saying that you have large files (how big?), and you copy them, the load goes up. Have you run iostat ( $URL$ ) to see how your machine's I/O performance is doing? To someone who isn't really all that experienced in performance tuning, it sounds like your CPU is waiting on your I/O to complete, which is backing everything else up. What kind of server is this? Virtual? Dedicated? If it's dedicated, what is the drive configuration? How many spindles? 

I might be terribly off-kilter here (so let me know if I'm making an incorrect assumption) but disk caching in Linux is known as swap, and uses a swapfile or swap partition. 

Nagios is great for small to medium networks. OpenNMS is supposed to be the gold standard free monitoring for large infrastructure (thousands of hosts) 

Ok, questions: 1 - Do you have vmware tools installed? 2 - Has it always been like this, or did a recent upgrade do it? 3 - How were the graphics during install? 

This doesn't really answer the question, but have you considered writing a script to do it, then redirecting the output of a script? $URL$ 

(change sdXX) to your real formatted swap partition, which, begs the question of why you have a swap partition if you don't want it used.... If you are not using systemd, then, removing the swap entries from /etc/fstab should be sufficient (as far as I know). Maybe the real solution is to get rid of the swap partitions so they won't be used accidentally. To remove the swap partitions, I would use fdisk to change the partition type from swap to something else, then reformat the partition or use: in order to zero it out and prevent it's use. See also: $URL$ 

Has anyone created or used a deployable module pushed out to slaves (from the Domain Host-Controller) without a separate script or step to update the {JBOSS_HOME}/modules/ directory? I would like to be able to push out updates to a module centrally, or, if that is not possible, is there a way to detect if a slave has a different version of the module installed (than the Master)? (I am running Wildfly 8.2, but JBOSS solution would likely apply also) 

On my system (debian lenny), I need to need binary-to-ascii in order to match mac-addresses. In this (working) example from my dhcpd.conf, server247 is in class "local", however, I give it a fixed address that it not in the pool. I would recommend that the fixed addresses be in a separate range from the dynamically assigned addresses (they can still be in the same subnet). 

What is the network like between the mac and server? Is it possible you are having an MTU (transmission unit problem), that is, you are sending a packet that is to big? This can happen with some DSL providers. 

Would it be possible to configure the networking of the ESXi machine to create static routes matching those IP addresses or blocks, and route them to localhost? That would effectively eliminate the chance of traffic passing back to those hosts. 

Usually those IP KVM machines load a java (or in the case of one particularly annoying Belkin, ActiveX) program that displays the remote screen and allows you to interact that way. I've never seen one with VNC, and I'm fairly sure that RDP/X wouldn't work due to the limitations of only reading the video port. 

Well, VPS's are limited primarily by I/O, CPU (OS independent, as you know) and RAM. I would go with the smallest footprint that has functionality that you're comfortable with. I've never used Arch, but from what I've heard, it's got a relatively small footprint. You could try Debian or Slack or whatever, really. Why are you paying for a VPN, though, when you could just use VirtualBox to test with? 

DNS. Make sure that your reverse DNS is working properly. You can verify this by typing "ping -n www.google.com". Immediate responses. 

For me, it has been centralized authentication. I got to the point that I was administering 40 or so Slackware machines, and each one had local authentication PLUS local Samba authentication. I also had a VPN solution where each account needed to be setup, plus an internal jabber server and an internal email server. Everything had its own account. MAC (Moves, Adds, Changes) were insane. So I switched from Slackware to CentOS, created an Active Directory infrastructure, and used Likewise Open to authenticate all of my Linux machines against AD. It probably saved me 20 hours a month without joking even a little. Now, I've got everything authenticated through AD that I can, and it works tremendously. I can't recommend centralized authentication enough if you're still doing things the bad old way.