To the best of my knowledge, booting from an SD card is not supported for Hyper-V. This makes the rest of the discussion mostly irrelevant as you simply shouldn't be doing this in production. 

You're going to need some kind of console access in order to do this. If it's not your web server then you'll need to request it from the administrator but I wouldn't hold your breathe. 

I can't see any reason not to use Robocopy on the destination machines, with /MIR*. It's very good at handling large amounts of files and will only copy changed files. Be sure to read all of the options and reduce the maximum retries and wait time. It won't 've continuous though, you would need some kind of task. *Apart from it being a hacky solution overall. Can you not use Git? 

You have no persistent routes configured on your system, which means any you created were lost on a reboot. The only ones showing are dynamically added by Windows. Try adding your routes again but add "-p" to the end of the command. 

Use a DNS server. Make it authorative for myapp.dev, set it to forward all other requests and then make it the primary DNS server for your network. 

If these home machines are supplied by them, then your demarcation point is the VPN software. I don't think you should be worried about how they log in. 

Without a doubt, your host should not be running AD DS. Definitely create a virtual Domain Controller. 

Add an exclusion in your DHCP server so that it never distributes that address and continue assigning .150 manually Add a reservation to your DHCP server so that .150 is always assigned to whatever MAC address eth0 is. 

Do you have another computer on the same network? If so, can that computer access your webserver? That's your first step, then worry about being visible to the outside world. 

I've never had to any administrative installs when using the Adobe Customization Tool. Simply make your changes, export the MST Transform file and install using the original MSI and your new Transform. 

Within each of the latter 3, each 'level' can have multiple GPO's and their order is decided by the system administrator. This is called the "link order" and the lowest number is processed last, which means that policy has the final say. OU policies are applied starting at the "root", and then downwards, if that makes sense. Here is some good reading on the subject: $URL$ With regards as to what to actually do with the individual GPO, well that kind of depends on the policy itself, but generally, they have the following three options: 

After some trial and error, I developed some 'blank' templates using the default Libraries which I'l post below. You can use the following known folders guide to customise new default locations: $URL$ And I have found the following to be useful references regarding the library architecture: $URL$ $URL$ Essentially, though, the way I found best to experiment was to simply make changes using the GUI and examine what is changed. Documents 

In your case, the correct answer is that the software needs to add its configuration items to these sub-locations. It's unlikely you can do that, so unfortunately you're stuck. Your ADM is still perfectly usable, but you need to be very careful about tattooing. 

Folder redirection doesn't work by using Junctions or anything fancy, it simply sets some registry values to tell Windows to literally load a different location instead of the defaults. The old locations may still exist explicitly but they will have no bearing on the new location, anyway. Because you can't use UNC paths in a command prompt you won't be able to do exactly what you want. A work around may be to use a mapped drive, but I don't know if this is supported now. I'd advise avoiding it where possible, though. 

php5-fpm settings (i changed this values due to php5-fpm error log messages higher and higher.. (freeze-problem was there before as well)* 

Bad part is: this mini-freeze here only happens from time to time as well. note: meanwhile i cannot even upload files via scp. I currently feel like running form bad to worse and back by googling for my server-problem due to immense lack of knowledge regarding server configurations. It still makes me wonder, why those problems even appear, since 250 users a time is not such a high amount, right? So my questions: 

so the values are the same, but the quoted time is completely wrong. using time command as prefix also tells me that ~ 15 minutes were used. I searched in dmesg, /var/log/[messages|syslog] - nothing found. /var/log/errors however tells me that: 

multiple times. now that message does tell me that php5-fpm task was blocked or did block ? - but not if that is the cause or just one of the results of that "freeze". Anyone? to cut the long story short, i dont know where even to start analyzing. So if you can give me any advice by looking at following specs and configs, or ask me to provide more information, i`d be glad. 

Website consists of 2 subdomains, forum. and www. where forum is a phpBB3.x PHP-Board, and www a Ruby on Rails 2.3.11 application (portal). Mini-Note: sometimes i notice that the forum is pretty slow, in contrast to the always-fast (except for this "freeze") portal. Both share the same Database, but the portal is using it read-only. The Webserver is nginx, using phusion passenger module to communicate with the ruby-application. Also, for the forum it communicates with php5-fpm via socket: relevant nginx configuration parts ( with comments/questions starting by ; ) 

Thanks for your time and help. Best Regards, Daniel P.S.: i renamed the configfiles to domain.tld since i dont want to have any % more load to the server until its fixed. might be a exaggeratedly thought.. P.P.S: if i asked a complete duplicate question, sorry. my search results seemed to be quite specific in their own way. Edit: just got some iotop 99.99% values while system seems to be frozen. can this fact be considered? Edit2: now i just noticed that this even occours with a load of 3-5.. iotop results are from 0-99% raid/mysql.. mhmm 

i am running a small community ( 6000+ Members ) on a non-virtual 64-bit ubuntu 11.04 system. I am not a Linux-pro, not even advanced, i just tried to setup a webserver, which does nothing special actually. Delivering some dynamic PHP and RoR websites is its task. So it might be that my configuration files do look horrible bad. Also, i might use the wrong vocabulary, so in doubt, please ask. Having a current all-time record of 520 registered users (board-accounts, no system-users) online at same time, average server-load is about 2.0 - 5.0. Meantime (~250 users) average server load value is at about 0.4 - 0.8, sometimes, on some expensive searches a bit higher. everything fine. From time to time however, the load increases up to 120 (120.0, not 12.0 ;) ). In this time, its hard to even connect via SSH, but when i reach the server, and use top/htop/iotop to see whats happening, i cannot identify any process causing high CPU load. iotop tells me about a current reading/writing speed of about approx. 70kb/s, which is quite equal to power-off i think. Memory-Usage is max. at ~ 12GB of 16GB, so swap remains empty. now the odd (at least for me:) waiting some minutes ( since i always get a bit into a panic when this happens, it feels like 5 minutes, but i suppose its more like 20-30 minutes) and the server is back to normal. everything continues as normal. another odd fact: when i run hdparm -tT /dev/sda, i get answer like: 

The vmx file simply holds the virtual machine configuration. Simply create a new VM using the vSphere Client and associate your current vmdk files with it. 

You generally don't. These systems use something like carrier grade NAT to share a single public IP meaning you would need ISP assistance to configure this. You're unlikely to get that. 

Yes, by default a standard user can get to a bunch of stuff. Most, if not all, of this can be fixed with policies, but I can absolutely understand why people end up using a deep freeze type solution and, indeed, non persistant images is a selling point of VDI solutions. Depending on whether UAC is enabled users may be able to access and modify files on the local drives, install applications (though generally to their own profile), run viruses or malware etc etc. To clarify, by default, the standard user is nowhere near secure enough for use in an environment where users need to be heavily controlled such as schools, public stations and so on. Correct me if I'm wrong, but I didn't think Deep Freeze was about securing a particular session, but to prevent changes from sticking? 

DHCP works using broadcasts which will travel between collision domains (Layer 2, switches) but not broadcast domains (Subnets, routers). You can, however configure ip forwarders / ip helpers on the routers to do this. Even better, the DHCP sever will know which subnet the request originated from, meaning a single server can server for multiple subnets. 

It really all depends on your deployment - fundamentally Citrix sits on top of Server 200x Terminal Services and that's that. However, there's lots more such as web interface servers, secure gateways, access gateways etc that you may wish to consider. Thing is, all of these things depend very much on your environment, where you want people to access from and what you want them to do once they're there. Plus it gets more complicated by the virtue that XenApp 5 goes on 2008, and XenApp 6 goes on R2. I'm a big Citrix advocate, but I agree with @KJ-SRS that it's something you should chat over with a consultancy. They have a huge amount of products to answer pretty much any business need, but It's a lot of money to spend on going gung ho. Citrix Presentation Server was re branded as XenApp, by the way - so they're sort of the same product. I don't think you're going to be able to get the indepth insight you need over the internet. I certainly couldn't give it to you without taking a look at your environment and what you do. 

You're not trying to do anything too special, and it's definitely possible. The way in which this is generally achieved is as follows: You configure a subdomain which is designed for receiving visitors redirected from an external domain. For example, . The page on the end of this will look at the headers and query the database to decide where to redirect to. If a user attempt to access directly, the page should return a 404, another error or redirect to your home page. The user configures their domain (or subdomain, whatever) with a new record of And finally, to prevent account hijacking, your service will normally generate some kind of verification code which the user adds as a record. This is exactly how Google do it for Google Apps. 

No, in short. A lot of the marketing towards it is geared around the fact that it will be a decentralised, outsourced, managed system. Locally hosting would pretty much defeat the point. 

Oh gosh - don't even think about it, please. If it's still running, with no errors then you have no choice but to wait. Doing anything else stands a high chance of killing the task and leaving you with a inconsistent file system.