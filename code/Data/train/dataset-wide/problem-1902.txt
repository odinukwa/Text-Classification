Operationally, cloud cover is determined subjectively by visual contact with the sky from the human observer (meteorologist on duty). No instruments or objective measures are used, so the two measures cannot be related in an obvious way. The unit used to describe cloud cover is called Okta, and ranges from 0 (sky completely clear) to 8 (full cover). Each value has a graphical representation which can be included on synoptic weather maps for each station. Another important reason why cloud cover cannot be unambiguously related to illuminance is explained in detail in Casey's answer - different types of clouds and cloud depths may lead to different illuminance values for same values of cloud cover. 

(Source: NCEI) The detailed structure is somewhat different, which has to due with local climate and circulation, but the overall trend related to humidity and proximity to ocean shows up in both. We can also see the mountains, which I would guess to be a snow albedo feedback effect (ever notice how nights get colder when the ground is snow-covered?). At NCEI you will also find similar maps for the rest of the USA. 

Does that mean we can say nothing? Well, it's not quite so pessimistic. There are many attempts to answer this question. The figure below shows estimates for the regional distribution of the shortwave and longwave cloud radiative effect, again from AR5 WG1: 

Thus, is geographically located $\Delta x /2$ East of and $\Delta x /2$ south of . 2) For computational efficiency and accuracy, the data from the child (nested) domain is copied to the parent domain. This is opposite from the data going from parent to child, which is interpolated. For more specifics, see the WRF Registry documentation by Michalakes and Schaffer. As a consequence of the above two, it is more computationally efficient and accurate to use a nesting ratio of 3, rather than 2. In the example below, for 2:1 nesting ratio and 1-d case for simplicity, values are simply copied from child to parent array because these points are guaranteed to overlap between two grids. Notice that the points do not overlap and child values must be interpolated to the parent. This induces computational overhead and numerical damping of the $2\Delta x$ wavelength in mass: 

In atmospheric remote sensing of methane, a dominant source of error is the uncertainty in spectroscopic parameters. For example, in the retrieval algorithms described by Batchelor et al. (2009), the CH₄ error due to line intensities is estimated at 16.5% (dominating the total error of 16.8%), versus 10.5% for O₃, 3.8% for N₂O, and only 1.6% for HCl. The article documenting the 2012 HITRAN spectroscopic database update (Rothman et al., 2013) states: 

(I had the opposite problem, needing skin temperature, and being tempted to use 2 metre temperature instead, until I realised where the data were). If you encounter a model that really only has the full temperature profile, then I would contact the authors to inquire. Maybe it is a model that really only does well in the free atmosphere, and is not suitable for the extraction of (near-) surface temperatures. The boundary layer can be a hard thing to model! 

The intensity of a rainstorm does not actually cause the duration of the precipitation to be shorter. There is a strong correlation, but not in the sense you may be implying here (let me get back to that in moment). First you have to dispel the model that a cloud is a big container of water, and if it "rains harder", the cloud will run out of water quicker and the rain event will be shorter in duration. Rain doesn't really work like that. The atmosphere everywhere (even outside the cloud) contains lots and lots and lots of water. And under a stable combination of temperature, pressure, and humidity, all that moisture is perfectly happy to exist in a completely gaseous state. But as differing air masses (different temperatures, pressure, and humidity) start to collide, the local atmospheric conditions can fall outside the range where water can exist in a completely gaseous state. That is known as the dew point. A cloud is where two differing atmospheric conditions have met, and the resulting condensed water droplets simply do not have sufficient weight to overcome the updraft speed of the air around it. If the air masses mix slowly, the rate of condensation will occur slowly and over a longer period of time. The resulting clouds may simply "evaporate" as the local conditions stabilize, or the condensing water may slowly precipitate over the ground (gentle rain), or the precipitation may never reach the ground at all (virga). These gentler atmospheric changes tend to play out over longer periods of time because the air masses are moving more slowly so it takes longer for the differences to mix and settle down into a new steady state. Here's the part where I talk about intensity If two air masses have widely varying conditions (think warm moist air over the ocean meeting dryer hotter conditions over the land) and they meet more abruptly (higher wind speeds), this causes a more-violent disturbance; the moisture from the air will condense more quickly, the rain falls faster and harder, and the whole thing ends (relatively speaking) more quickly. But the duration tends to be shorter because the reaction between the differing masses is occurring at a faster rate — so the whole event will generally play out more quickly. But it is perfectly conceivable that "harder rain" can last much longer if the storm system continues to pull in more of the moist atmosphere around it. In extreme circumstances, that's pretty much what a hurricane is; the storm itself gets so big that it starts to create it's own system that pulls in more and more atmosphere around it. The rain doesn't simply stop when all the rain has fallen out of the original clouds. It actually (continually) makes new clouds/precipitation as it continues to draw in the atmosphere surrounding it. But speaking in broad generalities, the faster the disturbance occurs, the faster the moisture will condense, the harder the precipitation will fall, the more quickly the conditions will stabilize, and the duration of the rain will be less. 

The answer to this question depends on what process one is interested in simulating well. As you know, short/mid-range weather prediction models and climate models have very different applications and goals. Because the short range weather prediction model is typically of much higher resolution than climate models (~1-10 km versus ~50-200 km), it is almost always more skilled at simulating clouds forming at a specific location and time. Cloud and rain prediction skill tends to be greater in synoptic scale fronts and near topographic features, and smaller for sporadic, small-scale, tropical thunderstorms. In very high-resolution NWP models (2-3 km or smaller), convection and clouds may be simulated reasonably well without using any cloud parameterization scheme. Nevertheless, clouds and rainfall are still hard to simulate or forecast very accurately. On the other hand, due to very low grid resolution, climate models can really aim only for correctly simulating the occurrence frequency and the amount of clouds and rain in a larger geographical region over a longer period of time. In climate, clouds play a significant role in radiative feedbacks. Climate models still rely on cloud parameterization schemes. 

Clive D. Rodgers, Inverse methods for atmospheric sounding, Theory and Practice. ©2000 World Scientific Publishing Co., London, UK. 

You can use simple logarithms to calculate the answer. The number of half-lives that have elapsed can be calculated with $$ - \frac{\log{f}}{\log{2}} $$ where $f$ is the fraction that remains. So plugging in the numbers gives $$ - \frac{\log(0.75)}{\log(2)} = 0.415 = 41.5\% $$ 

Provide a monthly average in every grid cell, and describe how many measurements were used for each cell. There is no fixed rule for the minimum number of days for reporting a monthly average. A reasonable threshold will depend on the geophysical quantity of interest y, in particular on how much y varies on short and long timescales. If y varies from month to month, but does not vary a lot from day to day (example: sea surface temperature), you can probably get away with reporting a monthly average even when only one or two days have cloud-free measurements. On the other hand, if you're measuring precipitation, your signal will be relatively noisy even if you choose a threshold of 20 days. And any threshold is going to introduce a bias, because a lack of observation is typically due to clouds. Although you will get a bias in any case. In other words, if you only have one day per month you see a gap and in the clouds to observe whatever you want to know, it is a pity to throw it away. Therefore, personally I choose a minimum of 1 observation. Critically important though: when releasing the product, add a field that tells the user how many days worth of measurements make up the monthly average in any given cell! Like that, the user can decide which ones to use and which ones not to use. More broadly speaking: producers should document everything they do. For your specific examples: search for the ATBD and/or scientific papers describing the product. Hopefully, there is a field in the level-3 product that describes the number of measurements per cell. If the answer is not in there, write to the producers and tell them to redo the product... You will need this information to accurately calculate a yearly average! 

Relatively important, depending on the basic principles of the modeling you are interested in, and to what extent you want to get yourself involved in it. In many areas of computational geophysics, e.g. atmospheric, oceanic, hydrological modeling etc., there are modelers, modelers, and modelers. First group of modelers get model output from somebody else, do some analysis, make some plots, and write about what they found. Then, there are modelers who design their own experiments, play with tunable parameters, maybe even change a few lines in the code and run the model. Finally, there are modelers - these spend most of their time on building and improving models, both in terms of numerical methods and software design. They could go for few years without publishing a single paper, but boy did they work hard. If you are interested in computational science aspects of modeling, i.e. the inner workings of the model and how to improve it, understanding linear algebra as well as numerical methods will prove to be very valuable. While it is likely that you will never end up having to implement that matrix inversion or sparse matrix multiply yourself, and instead use one of many well documented production libraries, it is important to understand how they do it and why they do it in a particular way. From my own experience, I can say that most of the time it won't matter, but sooner or later there comes a situation where having knowledge about how low-level operations work will save you from pulling your hair out for weeks. And this does not go only for linear algebra - I would extend this advice and urge you to strive at also understanding differential equations, discretization, computer architecture, floating-point arithmetic. 

The turbopause separates (by definition) the homosphere from the heterosphere. What factors cause the turbopause to be where it is? Is it affected by mesopheric composition, solar irradiance, global atmospheric change, and other factors that may change significantly? Or is it mostly determined by density in the absolute sense, and therefore relatively constant? 

No, the atmosphere is not becoming thicker. If anything at all, the atmosphere is getting thinner, but only on very long time scales. Planet Earth very slowly loses parts of it atmosphere due to atmospheric escape, either to space or to the solid Earth. Historical atmospheric pressure is hard to determine, but billions of years ago, it might have been much thicker than it is now, in particular with a lot more greenhouse gases. One clue is the faint young sun paradox; we know there was liquid water at a time the solar output was 20% less, which would require so much greenhouse effect that the total atmospheric pressure must have been much higher. But the jury is still out on that one. The atmosphere is not becoming significantly denser, either. The concentration of gases that we are adding to the atmosphere is typically measured in parts per million (ppm) or parts per billion (ppb). The effect of adding to this is small compared to the natural variability in atmospheric surface pressure / density due to weather and other effects. The atmosphere does not act like a lens. 

In both cases, if you are interested in more precise rainfall estimates at a particular point in space and time, look at level 1 data. It is raw data that provides a time step for every single point along an orbital trajectory of the satellite. Use with caution. In addition, ground validation data is available on the same server, and some or most of these data is assimilated (merged) into level 3 TRMM and GPM products. Once you download the data, you can use their data viewer, THOR, or use your favorite programming language or analysis software to display it yourself.