One example is the evidence for bǎ becoming generalized as a focused object-marker in Chinese. Post suggests that the points noted above: 

@hippietrail, re: your query about which language the documentation should actually begin in, it's generally thought that your work is most valid if all of the data are collected through the actual language you are documenting. This means that you can either essentially start out 'mute', and learn from scratch, or try to learn some of the language before you start the project. This is all well and good, but for languages that are under-documented (or undocumented), you would be lucky to find sufficient materials available to help you learn the language on your own, before going into the field. So, mostly, people will first learn the lingua franca of the area - if you plan to work on a small native language of Africa, the lingua franca might be Arabic, French, English, or a dominant native language like Kiswahili. And yes, in Australia, the lingua franca might be Kriol. Then, ideally, you'd gradually learn the language you are actually documenting, and use this to verify data you already have, and continue collecting more. In reality, this situation won't always work - it's far more time consuming for the linguist if it's necessary to get an extra language or two under your belt before getting down to the nuts and bolts of documentation, and funding might not always allow for those extra few months as a learner. Not all language documentation entails years spent working on the one language - sometimes an opportunity arises to just work on a language for a short time, or on a specific feature, and it's important to make the most of these opportunities because some documentation is still better than no documentation. In those situations, you might be more likely to end up working with educated speakers who are as fluent in English (or any given language) as you, and that's ok too, as long as you're aware of the possible pitfalls of the 'translation method'. 

I've searched the site a bit for this topic, and I recognize that there's quite a bit of variability about the classification of phones like [w] and [j]. My primary understanding comes from Catford's A Practical Introduction to Phonetics, which, if my memory is correct, presents these in their guise as semivowels. In this context, he seems to define a semivowel as a momentary, rather quick move into and out of a vowel. He demonstrates by having the reader "discover" [w] as a momentary [u] by having the reader start by making a [aaaauuuuaaaa] sound and gradually reduce the length of the [uuu] portion until [aaaauuaaaa] becomes [aaaaauaaaa] becomes [aaaawaaaa]. Similarly, [j] can be produced by starting with [aaaaiiiiaaaa] and then shortening the [ii] portion until the sound is [aaaajaaaa]. This all made a great deal of sense to me and has helped me find phones that are less familiar to me, such as [ɥ]. What isn't clear to me, however, is what exactly is happening when I make sounds where the semivowel is preceded or followed by the same vowel sound from which it's derived. For example, [ji] and [wu] are clearly different sounds to my ear than [i] and [u] alone (or [ʔi] and [ʔu]...?). If I try a similar Catford-esque experiment by making a [uuuuwuuuu] sound, it seems like something is happening—perhaps more intense labialization—but I'm not sure. When I try to do the same thing with [iiiiijiiiii], I have trouble hearing as much difference. My question is, if I strictly go by the first paragraph's understanding of a [w] sound as simply an abbreviated [u], then it seems that a sound like [wu] should be indistinguishable from [u] (and same goes for [ji] vs. [i]). Is there something wrong with my understanding of the [w] and [j] semivowels, or does one make some sort of "extra" distinction when a semivowel is surrounded by its corresponding vowel? 

One thing to keep in mind is that IPA is language-dependent, i.e. there is not a one-to-one correspondence between IPA symbols and speech sounds. This point is sometimes glossed over in introductory linguistics courses, where students sometimes are given the impression that the same symbol represents the same sound in all different languages. But this is far from true. Take the symbol [s] for example. What we call [s] in various languages is articulated in a variety of ways, and it is acoustically different in those languages, as well. When my Icelandic friends produce [s] I hear it as somewhere between [s] and [ʃ]; Icelanders, meanwhile, have trouble hearing the [s]/[ʃ] distinction in English because they have no such contrast phonemically (/s/ vs. /ʃ/) in their native language. Indeed, in a spectrogram, the [s] produced by my Icelandic friends shows fricative noise in a frequency range somewhere between that of my [s] and my [ʃ]. Another example comes from Hebrew, which allows many more complex onset clusters than English does. Rina Kreitman played some Hebrew clusters for non-Hebrew-speaking listeners, and those listeners sometimes reported hearing consonant-schwa-consonant sequences, presumably because their native languages didn't allow for such clusters; Japanese speakers also report "hearing" vowels between consonants in English clusters. All of this is to make the point that, yes, you are likely being influenced by your native language when you are hearing this so-called voiced velar fricative as a vowel. English doesn't have such a fricative; it doesn't even have the approximant version of it (transcribed as [ɰ]), nor are approximants allowed in that position in onset clusters (*[wga], e.g.), so it makes sense that you would map this sound to the closest thing that your native language's phonotactics would allow in that syllabic position--a mid-central-ish vowel. That being said, it is indeed quite possible that, articulatorily, the sound most often being transcribed as a velar fricative is being lenited to an approximant, in the sense that there is not enough of a constriction in the vocal tract to produce audible turbulent noise. This is quite common cross-linguistically; for example, the pronunciation of the Icelandic word saga is usually transcribed as [saɣa], but in reality it is often produced as [saɰa]; that is, the back of the tongue and velum approach each other for the velar consonant but they never get close enough to result in audible fricative noise. Acoustics aren't going to help you distinguish between a vowel and an approximant (unless you have minimal pairs in the language under investigation), because the distinction between the two is largely phonological. You might be able to see evidence for deciding between the fricative [ɣ] and the approximant [ɰ], though. The former should manifest itself with some aperiodic "noise" in the spectrum--some of the otherwise regular vertical lines corresponding to the regular glottal pulses in vowels and approximants will be "smudged over" with textured noise. Note, though, that even in the absence of visible aperiodic noise, researchers might still label this sound as a velar fricative in broader transcriptions (see the second paragraph in the Wikipedia entry for the velar approximant)! 

The idea that literacy may slow the process of language change has been around for a while, but as far as I know there is no comprehensive study of this, partly because there are lots of issues with finding groups which are otherwise comparable except for literacy rates. There is a key paper on the topic called Literacy as a Factor in Language Change (Zengel 1962) which hypothesizes about this but doesn't actually provide any real evidence for it - Zengel is basically surprised by the higher-than-expected rate of retention of vocabulary items in a small sample of English and Latin over 1000 years, and suggests that this could be because literacy was primary and integral in these languages and therefore added extra stability. This study excluded natural phonetic change and structural change in the words because, of course, this was rampant, so Zengel just looked at word stems. (A glance at just about any English sentence is good evidence for the fact that sound change stampedes on ahead, regardless of spelling systems). Some arguments against the literacy-slows-change idea are contained in Chapter 3 of this book on Literacy and Lexical Diffusion. It's an accessible discussion, basically arguing that literacy actually introduces rapid and widespread language change, and that literacy itself is diverse and variable rather than uniform and stable. Furthermore, even in so-called literate cultures, widespread literacy is generally still a relatively recent phenomenon. An additional point to think about is all of the types of changes that can be observed in English usage through the use of new digital mediums of communication - these occur very rapidly, directly mediated by literacy. 

Yes, Icelandic is quite conservative compared to many other Indo-European languages - it could be compared to Latin in terms of the complex morphology it retains. There are a number of explanations for this. The geographical isolation of Iceland (being an island) means that speakers of Icelandic had less direct contact with speakers of other languages, and therefore there were fewer competing influences from other languages. Prolonged contact with languages that have very different features is generally considered to have a role in the stripping-down of inflectional systems - English is a good example of this. (Though there was, apparently, a Basque-Icelandic pidgin spoken in the 17th century.) Similarly, the population of Iceland is mostly made up of people who are Icelandic, and the number of Icelandic-speaking people living outside of Iceland is relatively small. Iceland is also a relatively small geographical area, with the majority of the population concentrated around the Reykjavik area (200,000 out of 320,000). The dialectal differences are minimal, because the population is not dispersed in the same way as in other countries, and homogeneity in a language can be self-reinforcing of its characteristics. Other factors may also contribute to the conservative nature of the language - there is a recent dissertation on language change and stability in Icelandic which suggests that "strong linguistic nationalism and a stability-oriented language policy, are instrumental in creating the sociolinguistic conditions in Iceland which support language stability". This is interesting, because while 'language academies' such as the Académie française and similar policy-creating bodies generally hold little sway against the tide of language change, the attitudes of speakers themselves can be very influential, and this may be of particular relevance in Iceland. Lastly, while Icelandic is still remarkably similar to Old Icelandic (which is essentially Old Norse), the ability of speakers to easily read the old sagas is usually exaggerated (so i've been told by Icelandic people). It's a bit like English speakers reading Shakespeare - you usually have to read a modern version with lots of footnotes and explanations and modernized spelling (because the pronunciation has changed quite a bit over the centuries). Even so, this is pretty impressive - Shakespeare is only four hundred years worth of change for English speakers, while the Icelandic sagas are about twice as old. 

It is standard to talk about the prosodic hierarchy, which is a theoretical construct that divides utterances into smaller, phonologically relevant constituents called phrases, which are in turn divided into smaller constituents called prosodic words, and so on. There is not an absolute consensus as to what the exact levels of the prosodic hierarchy are or even how many there are, and (as with many theoretical constructs in linguistics) slightly different models lend themselves to different languages. For example, the mora is a prosodic unit that is more motivated in some languages than in others. Here is a (non-exhaustive) list of levels in the hierarchy: 

As has been noted in the comments, [s] and [ɕ] (sometimes broadly transcribed as [ʃ]) are not in complementary distribution in Japanese. However, they can be analyzed as allophones of the same phoneme. The following rule can be used to explain some of the facts: /s/ --> [ɕ] before [i], [s] elsewhere 

Meanwhile, exemplarists cite experimentally confirmed frequency effects and "neighborhood density" (i.e. degree of proximity to similar sounds) effects on production as evidence for exemplars and claim that purely abstractionist approaches cannot explain such effects. Recently, some researchers have supported a middle-ground approach in which insights from both camps are integrated into a model that allows for both the storage of phonetic detail and the manipulation and categorization of abstract units in the internal grammar. 

Yes! What you are describing is often referred to as the calling contour or the vocative chant, and it is very common, especially among European languages. Bob Ladd talks about it in his book, Intonational Phonology (first edition 1996, second edition 2008). The tune is characterized by a sequence of one or more syllables on a relatively high level pitch followed by one or more syllables on a somewhat lower medium level pitch. Ladd notes that the interval between the two notes is often, but not necessarily, three semitones, i.e. a minor third (p.117 in the first edition, p.136 in the second edition). Some other languages that have been noted to make use of this tune are (not an exhaustive list): English (North America and UK), French, and Hungarian. French is a language that is analyzed as having final stress; nevertheless, the tune goes down at the end, not up. Stress does play a role in some languages in determining when the note changes. In German and English, the higher pitch starts on the stressed syllable of the name (examples adapted from Ladd 2008): 

I was performing some Catford-style "experiments" with nasal consonants, and found that slight opening of the mouth or rounding/unrounding of the lips has no particular sonic effect on, for example, [n], [ɲ] or [ŋ]. That matches intuitively with my understanding of nasal consonants, since it doesn't seem possible for any changes in front of the point of articulation to have an effect on a sound with no air past the articulation point. However, whenever I try to produce [ɴ], rounding/unrounding and mouth movement have a relatively strong effect—one that sounds a lot like a shifting formant. I hear no fricative sounds, so I'm virtually certain that I'm not accidentally producing some sort of nasalized [ʁ]. I suppose that leaves the possibility that, depending on rounding, I'm accidentally producing a nasalized back vowel such as [o] or [ɤ] (apologies, I haven't figured out how to add the nasalization diacritic). I looked at the section in Catford on uvular consonants, and although it does mention a certain "messiness" for the stops [q] and [ɢ] (which seems potentially significant to my problem), it describes the uvular nasal as unproblematic and easily found by producing [ŋ] and simply lowering and bringing back the tongue. My main question is: is this just the nature of [ɴ], or should it be possible to produce it in a manner such that changes in the lips have no effect? Secondarily, if indeed an ideal [ɴ] is possible, do you have any advice on how to produce it properly? 

As an English speaker, I've had very little experience with retroflex consonants, but have recently come upon their use in Polish, and am having difficulty hearing them as particularly distinct from non-retroflex consonants at similar articulation points. In particular, I've learned that the glyph <ż> indicates the [ʐ] phoneme, but I have enormous difficulty hearing any distinction from [ʒ]. Whenever I have this impression of any sounds, my first assumption is that the distinction simply lies somewhere I'm not used to listening for, but any suggestions on what aspects of the sound I should focus on would be helpful. On the other hand, I do notice a fairly significant change to any surrounding phonemes, especially when they are vowels. For example, if I just make the sound sequences [aʐa] followed by [aʒa] I don't notice much difference in the consonantal part of the sound, but the [a] sounds in the former are very, very noticeably retroflexed. My initial assumption is that this effect is probably more a result of my relative clumsiness and slowness in moving my tongue into retroflex position; on the other hand, it also seems possible that the primary phonemic distinction might simply involve surrounding phonemes to a greater degree than in other types of consonants. Is this off base? 

It's true that this is a simpler analysis than trying to instead account for [b] becoming [p] in a wide range of environments. But, the actual reason why [b] is a variant of [p] is because [p] can occur anywhere, while [b] can only occur in a specific phonetic environment. That is, the voiceless bilabial plosive only becomes voiced when it occurs inter-vocalically, because the voicing of the surrounding vowels carries across to the otherwise voiceless plosive. So, when you start to find phones which seem to be variants of the same sound, you need to look at the environments they occur in to work out which phones have plausibly been affected by the surrounding phones, and which phones seem to be robust in a range of environments. That said, there are languages which permit a lot of 'free variation' (allophones that don't seem to arise through phonetic conditioning), in which case linguists might choose 'the phoneme' based on frequency, ease of explanation, or familiarity with one type of sound over another. But, in many analyses noting 'free variation', there do turn out to be phonetic factors governing the variation. 

He then goes on to suggest that there might be a stronger correlation between more compounding and 'deeper' grammaticalization; the obsolescence of basic source lexemes for the grammaticalizing forms (the source lexemes typically being replaced by compounds) might actually encourage the partially grammaticalized versions to shed their lexical-like attributes, and allow behavioural restrictions to become more firmly established. There are other hints of similar observations around, but these sorts of things can be very difficult to tease out in the early (or mid-term) stages, and require a lot of data, so I think it will be some time before we know for sure whether some of these languages are becoming more agglutinating. Seeing as there is really no such thing as a purely isolating language, we also can't know for sure whether some of these languages are on their way to becoming more or less isolating - they might not be ready to move on to agglutinating if they're not done with the isolating stage!