I'm not aware of anything that's accepted that is exactly what you want, but I will note that Bayesian reasoning is a framework that is great at modeling exactly what you want. See the Wikipedia entry, for example. In particular, you can include your inference steps as part of the model. So a valid proof of assuming would have ; if you have any invalid step you can swap in the probability of the actual step taken (or an estimate thereof). Let's suggest two alternate hypotheses, and that cover all cases, and we'll let be our evidence (either a logical proof or the presence of an argument from authority or whatever). Then, according to Bayes' theorem 

Some of us are not born with such a futile longing, and there is no particular reason to suppose that such desires prove that what one desires exists or is a good idea, any more than desire to eat sweet things proves that we should eat candy all the time, or intense desires by the mentally unsound prove that they have a different God or live in a different universe than do we. I am not exceedingly familiar with Christian philosophy and apologetics, but if such was taken as proof (rather than as rather weak confirming evidence), it would be an invalid argument. When I have encountered the argument, it has always been in that context: not a proof, but a gentle suggestion that supports that world-view. 

*The "the artists who created this get almost none of the money" argument often modulates this claim--that is, they're happy to buy things that support the artists, but not necessarily to download music from iTunes where an overwhelming majority of the money goes to people other than the artists. I've also heard the following argument in various guises, mostly from affluent people who rely on the current system for their affluence: 

It's really much simpler than this. Natural philosophers, and after them natural scientists, noticed that some easily-expressed mathematical relationships held between various measurable quantities. , for instance--Newton's Third law. Among other things, physicists noticed that two apparently distinct phenomena, electricity and magnetism, were actually tightly interrelated--unified by Maxwell's Equations to such an extent that it made more sense to think of it as "electromagnetism" than as two separate things. But there are more fields than electric and magnetic fields. There are strong and weak nuclear forces, also, mediated by their own fields. Maybe they all could be neatly wrapped together in one framework, at least under some limiting condition (e.g. absurdly high energy)? When scientists go after a Grand Unified Theory, they just mean that they would like to find a nice relationship, if one exists. When they go after the Theory of Everything, they want gravity to be in the mix also. That's really it. Don't mistake the flowery names for some sort of philsophical overreaching that is doomed to failure. It's just plain old ordinary physics, trying to find relationships between fundamental ways in which the universe behaves. The abstractions may not work out very well or at all. (There are no highly compelling TOEs, and the GUTs are complex to the point where one wonders whether they are worth the bother especially since they don't seem to produce interesting predictions.) But we can understand the endeavor in analogy to the one with EM perfectly well, without any reference to "dominant societies of the cosmic epoch" or things that "atomize the extensive continuum". This really doesn't provide any insight into what physics will work and what will not. The Standard Model has held up amazingly well, and Maxwell's Equations are beautiful, genius, and astoundingly good descriptions of how the world works. Before they were found, you could have leveled the Whitehead's criticism against them. But they were found, and they work. I'm not sure how you'd know what else will except by trying. 

Consequentialism not only admits but advocates cruel treatment in exactly those cases where there is an overall benefit. Depending on whether it is plain-vanilla utilitarianism or something more sophisticated, exactly what situations call for cruel treatment of some individuals will vary. If your objective is to minimize the suffering of those who suffer the most, you will only, for example, advocate moderate torture of a criminal who knows where a bunch of innocent people are being held and are being tortured severely. There are lots and lots of considerations that I won't go into here regarding to what extent different measures of what is good are justified, and what the framework should be given that it has to apply to real humans (with limited ability to make carefully-reasoned snap judgments and be entirely impartial), but theoretically, the answer is a resounding yes in principle. The reason why this sort of thing is not done more seems to be a combination of both suffering and causing suffering pretty widely being accepted as the opposite of good (so you try to avoid it), and a lack of sufficiently dire situations where one would legitimately have to consider doing something truly horrific in order to avoid something even more horrific. 

Bertrand Russel's paradox overturned the 19th century ideas of how to ground mathematics and set theory, arguably setting the stage for Godel's incompleteness theorem, and giving a very deep new appreciation for unknowability (both directly and thanks to Godel). I count these as both mathematical and philosophical ideas, given how important they are for epistemology. The idea of relativism, though difficult for me to trace, is now dominant in social and political sciences, and I think has had a huge impact on modern life, partly positive (tolerance) and partly negative (opinion trumps research). I am not sure whether it gained favor as fallout from Russel's paradox and Wittgenstein, but it seems related in spirit in some ways. 

The purpose of learning is, minimally, to survive. The world is so complex that we as individuals cannot be specified de novo to operate in it. Rather, even during development there are massive amounts of learning that shape who and what we are. (For example, refinement of visual cortex.) The fundamental point of learning--to be able to deal better with the complexities present in the world--never really changes, even if it no longer becomes a matter of life and death once you've learned enough. (Even so, if you don't ever remember (i.e. learn) where you parked your car, you could have some trouble.) Social animals tend to like social interactions. In humans (but not only), this includes teaching each other. It's also usually more efficient to learn something from someone else than to figure it out yourself from scratch, so given that we have finite amounts of time to learn useful things, it's a good strategy for all involved (at least in the pass-it-forward sense). 

It's highly adaptive to be able to imagine things that aren't actually there. This is an incredibly basic problem in perception, in fact. Suppose, for instance, that you see part of a lion hiding behind some grass. You can probably work out that 

The premise is flawed: there is no "current state" due to relativity. You have your current local state, but what has affected that state is potentially anything in the light-cone into the past (basically, what could have affected you spreads out at the speed of light backwards into the past--so if you care about what affected you within the last nanosecond, you only have to consider things about a foot away). There are, thanks to quantum mechanics, various instances of "spooky action at a distance", some of which are apparently (but not actually, in an information-transmitting way, anyway) faster than light, and because of the equivalence between space and time, it's not exactly clear to me how to answer the question (or if it even has an answer--the premises may be so flawed that it doesn't really have an answer, as with "how duplicitous is the moon?"). Let me give an example of why reality doesn't play nice with questions like these. Certain processes can result in the emission of a pair of photons which are entangled, meaning that some of their properties are randomly chosen but correlated with each other. (For example, they may have been generated with a random polarization, but the same polarization.) Now you have two ways to interpret their behavior: either at t = now they are in a state of being entangled, or they are both impacted by what happened at t = when they were produced. My general advice with philosophical questions like these is: it doesn't work enough like that to even ask the question, and the answer isn't as important as you think because of how it does work. 

Philip Kl√∂cking has a decent technical answer, but it's much simpler. Humans can't violate the second law of thermodynamics. We're part of the system too. If humans can generate "value" nonetheless, so can machines. Depending on details, maybe they can generate a lot more per input of energy or capital or whatever. One formulation of the 2nd law is that entropy never decreases in a closed system. The Earth isn't a closed system: tons of sunlight comes in and tons of heat goes out. So it's not even applicable! (And if it were, we don't care much about the entropy of uranium isotopes, so we can still construct value.) There are lots of interesting things to say about the relationship to human and machine labor, but one can safely ignore the 2nd law of thermodynamics until one starts talking about "infinite power" and stuff like that. 

It is a logic error to assume that people who are "smarter" always detect logic errors that those "less smart" will overlook. Therefore, you know the same way as always: you catch yourself, or someone else catches them and tells you. (You might have to wait a little longer or have a few more people check if you've already done a pretty good job of reasoning correctly and thus caught the easier-to-detect errors.) 

From a physical perspective, your particular neural impulses are so wildly improbable that when you're dead, there won't be another "you" in any meaningful sense, unless the universe happens to be infinite (in which case every possible thing will happen an infinite number of times). From a philosophical (or psychological!) perspective, you get into tricky questions about why you would call this other thing with similar neural pathways "you". For example, if there were two clones with very similar neural pathways, you would not call them one person; one of them would be one of them, and the other would be the other. One wouldn't say, "Oh, that's me also." So I'm afraid that we are left with feeling spooked. Such a feeling is not a particularly good indicator of the veracity of something, you know. One might feel equally spooked to realize that you're blind when you blink and you don't even notice. Heck, you're blind when you move your eyes, and you neither normally notice you're moving them or that you're blind while doing so. And then there's sleeping. Being a human is weird. 

To be conscious of something, one may need some sort of processing distinct from non-conscious processing. So there could be some specialized centers for conscious processing. (Might not be localized to one spot in the brain, though.) There are also "afferent copies" of neural activity in many regions--signals not really used for normal processing, but broadcast nonetheless for various reasons (e.g. afferent copies of motor commands seem to function as a predictive signal for where limbs will be). Some afferent copies of neural activity would then presumably go to centers for conscious processing. Afferent copies of the computations underlying conscious processing could be detected by centers for conscious processing, giving a conscious sense that, "Yes, I am conscious right now". This is probably not how it works, but there is no known reason why it couldn't be, so it satisfies "what could possibly be a correlate". 

Science can tell you an awful lot about contingent oughts ("in order to achieve X you ought to do Y"), and it's not completely clear that non-contingent oughts even exist. (We like to think they do since it saves arguing about whether X is worth achieving, which ends up with the realization that we don't have a satisfyingly solid grounding for knowledge, etc..) Unfortunately, Sam Harris doesn't really delve into the issue adequately; rather than making a spirited defense of the value of contingent oughts like e.g. Daniel Dennett does, he just expresses his feeling that of course science can tell you what you ought to do. (And not in a very impressive way, either--he starts with really clear cases where everyone agrees what to do and notes that adding a scientific perspective doesn't change anything, and then as far as I can tell dismisses the rest as details.) Instead, it would have been nice if he had brought to bear the full force of contingency, including extinction if you screw up too badly. Harris' personal views seem to be very typically American--highly individualistic and happiness-based, in particular--which may explain in part why he didn't go in that direction. Happiness is great and all, and it's nice not to worry about people telling you what to do. But when one starts from "we're intelligent social primates in an indifferent and largely deadly universe", it's hard to get to a point where you don't start thinking it could potentially be a good idea to curtail individual freedoms to maintain environmental sanity, and that we should probably demand a much higher degree of attention to nurturing our offspring to enable them to make informed decisions about this complicated technological society we've built. (Someone needs to write a book titled "Your Feelings are Trying to Make You Evolutionarily Fit in a Social Context".) Anyway, I view the is/ought divide as probably much ado about nothing; I think when we end up fully exploring the force of contingent oughts, we may have enough. (In a way similar to coherentist views of knowledge--you can't perfectly ground things, but when everything you care about ties together, that's good enough.) 

The question invokes Husserl, for whom intentionality means something very different from the colloquial sense of the word; if we are using his definition, the following answer applies. Husserl's "intentionality" is, roughly speaking, the capacity to represent. Saying that consciousness is intentional, therefore, only says that consciousness cannot be about nothing, nor is it somehow the literal thing itself. Now, of course matter can be used to represent things: magnetic patterns on a hard disk, genes that represent receptor proteins that represent external cAMP concentration that represents likelihood of starvation, etc.. But Darwinian evolution is seen as not intentional in the colloquial sense. Evolutionists don't spend much time thinking about whether evolution has the capacity to represent things itself. Certainly the evolved things represent an awful lot, both molecularly and, in animals with a nervous system, via neurons. And the theory of evolution represents what actually goes on, so of course it is Husserl-intentional in that (not very interesting) sense also. So, the theory of evolution and the output of evolutionary processes are Husserl-intentional, and that's not a problem at all. The problem, if any, is with easily-confused terminology. 

Chalmers has a really nice overview of the critiques of the hard problem of consciousness here. Which one seems "particularly interesting" depends, I think, on how sympathetic you are to Chalmers' conception of the hard problem; it doesn't seem that he finds any of them terribly interesting (though the Dennett/Churchland approach least of all). Personally, I don't deny that the hard problem is hard, but I offer the following counter to postulates of impossibility: how do we know that qualia are not just a particular emergent phenomenon of computational systems of a certain structure? If it were an emergent phenomenon, presumably there would be preconditions, and then every way we use to detect qualia in us would match up with the presence of those preconditions; and if we created artificial systems that met those preconditions and could communicate in a way that would let us assess whether it acted as though it had qualia, we would be forced to conclude that this was very strong evidence that this was an emergent phenomenon. Of course there is always the problem of other minds, and you can always insert Descartes' demon to confuse things, but aside from these approaches of sheer stubbornness (which, so wielded, can obliterate any knowledge), there seems no barrier to the project. So, okay, it's maybe fiendishly hard to do in practice, but in principle it is just like the understanding of other sorts of emergent phenomena, if it does in fact have a physical basis. So it's not metaphysically hard, just experimentally hard (This is somewhere between the views of the Type A and Type B materialists as defined in Chalmers' summary.) 

There is no particular reason why it must be either way. The utility function just corresponds to what your choices would be; it doesn't constrain any particular reason why you might make those choices. There is reason to suspect that various choices are made sub-optimally. For example, happiness research suggests that as long as you're making ends meet, experiences not stuff is what makes you happy. Of course, happiness may not be synonymous with utility, but people with money don't seem to do a very good job using it the best way they can to convert it into happiness. So if you are wondering about any particular person's direction of causality with respect to their utility function, you probably need to examine that person (possibly even that decision) for evidence one way or the other.