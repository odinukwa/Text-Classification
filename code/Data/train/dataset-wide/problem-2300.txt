That's exactly how it was injected. They declare some variables to move data around, then get the name of each of the non-system databases. From there I'm not sure what it's doing but I'm guessing it goes through all the tables looking for max columns and if it finds one, it finds some rows and writes links to them. 

Our senior programmer has been having me write stored procedures in the following format to protect against injection attacks. He says that the best practice is to take the parameter in, then declare a new variable in the body and assign that var the parameter, that this step is extra protection against injection attacks as it forces any injection attempts to be considered data and not taken literally. Is that true? I would think this would slow down the query and use extra memory and not add any extra protection, but I could be wrong. Example below. 

We use stored procedures and so far we haven't detected any injections that have been successful, but we see attempts all of the time. I started logging some of the data hoping to gain insight on what people were attempting and possibly use it to block any future attempts by new methods. Here is the code of something someone tried to run on our site today, they tried it with syntax 8 different ways. I can understand some of it, but not all of it. So I was hoping someone else who knows more could tell me how exactly this works and what it was trying to do (I believe it was trying to insert links to a page using hidden divs in MAX columns in all of our databases). 

There are lot of factors that influence how long it would take for failover cluster to come online on other node. The factors are (but not limited to) 

This would not give you absolute value but is correct. There are some more allocations which are not included in this but that would be just few MB and would hardly matter So memory utilized by SQL Server is (169000+13762560)KB which is approx 13G. Which matches with max server limit you have set. So don't worry SQL Server is using memory allocated to it. Plus whatever may be result please upgrade to 64 bit version of both Windows server and SQL Server 

The default compatibility level of database created on SQL Server 2014 Azure database will be 130, what do you want to change in that ?. Yes you can connect but there would be more than just changing connection string. Please read Next Step Section Of Connecting To Azure Database. It has various links which details about how to use different application to connect to Azure SQL Database 

I'm using SQL Server 2008 R2. Someone else set everything up. I encountered an error today: Autogrow of file 'ASPState_log' in database 'ASPState' was cancelled by user or timed out after 1748 milliseconds. Use ALTER DATABASE to set a smaller FILEGROWTH value for this file or to explicitly set a new file size. This was a very large transaction log (much bigger than the table) trying to grow and as a result it shutdown our website. The recovery model is set to Full but the logs are not being backed up and truncated. But somehow the database is being backed up nightly as there is a timestamp under Last DB Backup. However, under Management there are no maintenance plans...so I'm not sure how the DB is getting backed up nightly unless there is an outside task or server doing it (which I couldn't find). Should I do a full backup including logs and then truncate them? And where can I find how I am currently being backed up? Thanks, my first question. 

You are correct since you figured out your database grows approx 1G its really bad to keep autogrowth settings to 1MB. When SQL Server performs an auto-grow event, the transaction that triggered the auto-grow event will have to wait until the auto-grow event completes before the transaction can finish. These auto-growth events cause your performance to degrade a little when an auto-grow event is taking place. For this reason it is best if you can size your database appropriately so auto-growth events rarely occur. Make sure you have instant file initialization for Data files The best thing is you can do is presize your database so that it does not have to encounter any autogrowth events like if you know it would grow upto 30G in month please allocate that space to datafile before hand. You can use below query to check auto growth events. This query is taken from This Link I would like you to spend time and read this article. 

When you use the server name (DNS name) then a change of IP address will be used by your clients as soon as all DNS servers have this new address. Since you change your subnet you must be able to 'reach' this subnet from your clients. The only 'problem' can be that you loose the open connections. 

The length of the key would be big. The data would be stored in the Person table too. Streets can sometimes be renamed or renumbered. 

The command gives the CPU usage per core. Since you have 12 I would not create another Linode for that. Check the line with . There you will find the which is a better indication weather or not your server is using it's full CPU capacity. 

Do not think that there is a script for this. It needs to be decided/defined before the data is entered. Otherwise it might block your application from functioning. Normally a table needs 1 field to be unique. Only if you create a table to link 2 different tables (for an N-to-M) relation then you need both the M and the N key to make it unique. There are exceptions but it is up to the designer to decide what field(s) make up the unique key. The data can grow and so can the uniqueness. 

You just installed this is SQL Server management studio. This is a tool used to connect to SQL server database engine. Since you have not installed SQL Server database engine you cannot connect. 

pagefile.sys % Usage shows total system committed not what currently is utilized. This value can increase due to load when system finds out it has to back process with more page file. Have a look at below link for detailed explanation $URL$ I would like to know what problem you are facing 

The undo phase is where transactions that are not committed are rolled back so that they don't end up being in database when it comes online. Yes this information about transaction which is not committed is present in transaction log. To understand more about this please read Logging and recovery in SQL Server Tempdb is always in simple recovery model but "Minimal Logging" is followed in tempdb. This means ONLY information to rollback transaction is there no information to rollforward the transaction is present. Crash recovery does not happens in Tempdb. Further Reading. 

means undefined and is a special value. You can only compare it with or . If you want to use it with then you need to use something like or where has the value instead of . Consider to give it a default value instead of in your table to prevent this. 

So change the DBID and unique name (Check the documentation on these subjects) and check the configuration files of sqlnet (normally in the directory) for references to server A. 

In this trigger you make the ID NULL. When the trigger is finished the insert is done and Postgres will provide an ID. I assume that you have defined the ID as . 

In my opinion option 3 is the best. Option 1 has the problem that you always have to use a like on the tags field to find the articles. Option 2 will cause a rewrite of the tag each time that an article is added or deleted. When there are a lot of articles for the tag then the record becomes long. Both option 1 and 2 will end up with 'big' rows which means less rows in a physical block which means more physical reads and 'slower' result. Option 3 has a simple setup. It is the way to present an M-to-N relation. Put an index on the name of the tag to help to speed up the finding of the tags (will not help much if you have a very limited number of tags). Adding an article or adding a tag to an article will be fast too. 

So this means when queries run on this workload they can request almost memory as memory grant for the execution there by forcing others to starve for memory. And this is may definitely cause OOM error. Further more what Microsoft says is 

Simple answer: You are following correct path. You can create your own maintenance plan from MP wizard its easy and works fine. You don't need to use complex Ola Script. 

Please note that intermediate sort operation is different from row versioning what Remus pointed out which is only used with ONLINE index rebuild. If you use option the intermediate sort result will be stored in tempdb database. If you read Disk Space Requirement for Index DDL operation it says for a clustered index is created, rebuilt, or dropped online, a temporary nonclustered index is created to map old bookmarks to new bookmarks. If the SORT_IN_TEMPDB option is set to ON, this temporary index is created in tempdb. If SORT_IN_TEMPDB is set to OFF, the same filegroup or partition scheme as the target index is used. The confusion originated because of below statement from Alter Index BOL Page.