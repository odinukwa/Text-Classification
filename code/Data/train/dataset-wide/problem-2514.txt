Also, I suppose that a shouldn't alter the object being drawn. Therefore, you better all of your methods. You could also be more consistent with the way you use : you inlined 's method but not while it basically does the same thing. 

Now, if a destructor is and in a base class, then the derived class automagically have a destructor, you don't have to write anything. Therefore, the simplest thing to do would be to simply remove the destructors from , and if they don't do anything more than 's destructor; you program will then be correct but simpler. 

To access the functions contained in the global struct, I use the following macros (some also provide extra features): 

In order to do that, you will have to tell that you are using the stadard library user-defined literals for since they are not enabled by default. Add this line in your code: 

With all optimizations turned on, this code is a little bit faster than the previous version, but it's still far from being as efficient as the version which converts the Gray code back to a regular integer, performs an integer addition and converts the result back to a Gray code. Hey, we can't optimize such an algorithm without twisting it in every way possible! This is why, we will also try to... Find a better end condition Currently, the algorithm loops over every bit in before ending. It may be possible to end the loop sooner. Let's have a look at a big old truth table that we will use for the rest of the answer. For the sake of brevity, I used the names from the original pseudocode to name the columns. I hope that the parallel with the code I wrote isn't too hard to make (\$ A = lhs_i \$, \$ B = rhs_i \$, \$ E_{old} = lhs_p \$, \$ F_{old} = rhs_p \$, \$ S = A \oplus B \oplus res_i \$, \$ E_{new} = lhs_p' \$, \$ F_{new} = rhs_p' \$) \begin{array} {|cccc|cc|cc|} \hline A & B & E_{old} & F_{old} & A \oplus B & S & E_{new} & F_{new}\\ \hline 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1\\ 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0\\ 0 & 0 & 1 & 1 & 0 & 1 & 0 & 0\\ 0 & 1 & 0 & 0 & 1 & 1 & 0 & 1\\ 0 & 1 & 0 & 1 & 1 & 1 & 0 & 0\\ 0 & 1 & 1 & 0 & 1 & 1 & 1 & 1\\ 0 & 1 & 1 & 1 & 1 & 0 & 0 & 1\\ \hline 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0\\ 1 & 0 & 0 & 1 & 1 & 1 & 1 & 1\\ 1 & 0 & 1 & 0 & 1 & 1 & 0 & 0\\ 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0\\ 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1\\ 1 & 1 & 0 & 1 & 0 & 0 & 1 & 0\\ 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1\\ 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1\\ \hline \end{array} Looking at the very first line of that table, we can see that when \$ A = B = E_{old} = F_{old} = 0 \$, then as a result, we have \$ S = E_{new} = F_{new} = 0 \$, which means that since our result was filled with \$ A \oplus B \$ from the start, then we can stop the loop when the aforementioned condition is met (everything equals \$0\$) since the following bits S will always be \$0\$. An easy way to do so is to "consume" and in the code with right shifts so that they eventually reach zero and to change the loop condition with bitwise ORs (here, only the loop for the sake of simplicity): 

There is not much to be said actually, the concept-based polymorphism seems to be well implemented. You can slightly improve your method by having it the . It will be a little bit less verbose: 

Ok, I tried to compile your code with GCC, and it seems that you actually have errors that MSVC is not able to catch: 

My tests show that it always perform at worst as many comparisons as the original version, and often less when looking for the same element in the same sequence (except when looking for the second element of the collection for some reason). I know that the algorithm could be more efficient when equivalent values appear in the collection, but I didn't find any elegant way to solve this, even though three-way comparators could have been a nice solution. Is there any way I can improve this algorithm, be it from a style, correctness or efficiency point of view? 

Pre-increment vs. post-increment Depending on the type, may be faster than . It does not change anything for but if your container contains large type, remember that the in is generally defined as: 

One thing that I find odd is that you duplicate a great deal of standard library code to implement your sorting algorithm: 

This will be a somewhat strange answer but the thing is that I can't find anything wrong with your code: it is not that easy to understand what it is doing, but it is only because the algorithms are not the most straightforward in the world; if we compare your code and the algorithms pseudo-code, it's easy to see that they are performing the same computations. Everything seems well-written, the language is used as it should be used... In the end, I only have a few almost-irrelevant remarks left: 

Exponentiation by squaring Your algorithm for exponentiation may not be the most efficient in the world. While it is "free" at runtime since everything is computed at compile-time, C++ is already known to have long compilation times so using a more efficient algorithm at compile time may help reducing the compilation time. Therefore, instead of the naive "multiply \$n\$ times" algorithm, I would use the exponentiation by squaring algorithm instead. I have an old C++11 implementation, so I simply pasted it below, but you could probably use the new capabilities of in C++14 to implement a better version of it. It also handles negative exponents: 

With such a type, you don't even have to comment what the constants mean anymore and you make sure that your enumeration won't implicitly convert to an integer. It's all benefit :) Miscellaneous pedantic tidbits Syntax, idioms... we leave the real of semantics and focus on writing idiomatic C++ code. While it won't change a thing for users, it may please people reading your implementation: 

Define in terms of As pointed by @Yuushi, Boost has a class which will apply a function to the return value of . Therefore, could be define as an alias template: 

A very simple but useful algorithm: rotate the values of a given number of variables in-place to the left so that the first variable gets the value of the second, the second gets the value of the third, etc... and the last variable gets the value of the first one. 

More than an in-depth review of your algorithms, this answer was more about good pratices when writing algorithms. Basically, here is what you should keep in mind: 

Now, you don't have any mean to know whether is a simple or a "smart" one though. The name was confusing, and since there is already such a trait in the standard, your class should reflect its behaviour. Now, we want another mean to differenciate simple enums from smart ones. I propose to add another constant in , along the lines of . This constant would tell whether is specialized for a given type, and the name is less ambiguous than the previous one. 

I know that enough memory has been reserved for 8 elements, but I would expect the size to be 0. Moreover, if I write this: 

Anybody who knows how Python dictionaries work will know what this lign is doing, the code is explicit by itself. Instead of always repeating what your is doing, you should just explain once what it does when you declare it; the code will tell the rest. You should only explain once at the beginning how your algorithm work and let your code tell the rest. Honestly, you could remove almost all of your comments. Python is designed to be readable and it is. Second point, avoid to name a variable : it is already the name of a built-in type. It's bad pratice and can be confusing for people who will try to read your code. Also, you open your file once, but never close it. You should close it once you have filled your . You could use the construct to have your file closed automatically at the end of the block, therefore, you won't even have to remember to close it. Also, , like is also the name of a built-in type in Python, find another name :) 

The PEP8 explicitely says that comments that contradicts the code (that's often the case when you modified the code but not the comments) is worse than no comments 

I think that the goal of avoiding overflows is reached: the accumulation is guarded against overflow, and the division of every element to add by the size ensures that the result itself can't overflow either. Regarding the precision, adding integers in an as long as possible ensures that no precision is lost before the division. In the best case, a single division will occur. Moreover, the algorithm should divide bit integers most of the time, which may help the precision too. I picked to have the best precision guaranteed by the standard too. Now, there are a few known shortcomings, to name a few: 

There are at least a couple of things that you could improve in your code without changing the way it works: 

computes directly . Morevoer, the CPython implementation should be based on the underlying C function ; therefore, this function is safer than the naive implementation since it does its best to avoid overflows and underflows at an intermediate stage of the computation. 

I don't know about optimization, but seems to scream for a . It seems more appropriate than a list of lists, especially since the indices are known and the "titles" never mutate: 

By the way can also be used to compute \$ lhs_{new} \$ so that the code looks more symmetrical (even though it does not improve the performance): 

Also, as a side note, most modern compilers just don't care about , at least they don't take it as a hint for inlining since they already do a great job when it comes to inlining stuff. Nowadays, is just a way to solve ODR problems, which don't happen with templates. All of your functions are function templates, so you can safely remove . 

Anyway, whenever possible, try to use reference smantics and to hide pointer semantics inside the classes so that the users of the classes don't have to bother with the pointers. 

Sometimes, more is less Currently, we use the fact that at some point and will eventually become \$0\$ and that only one modification can happen right after this point. However, we currently don't use the fact that at some point or is \$0\$. We can actually write two loops instead of one: one loop that runs until the smallest of and is \$0\$ and another that runs until the other one reaches \$0\$. The second loop can be simplified since we know that one (the smallest value from the ) is \$0\$ during the iteration: 

I did not test the code above, but theorically it should. That allows you to create instances of classes derived from directly without having to bother with the pointers. Here is how you could use it: 

I was writing some geometry-related code again and had a closer look at my function supposed to compute the Euclidean distance between two points (N-dimensional points by the way, hence the template parameter). Here is a simplified version: 

Loop invariants code motion This is the name of an optimization performed by most compilers: when they detect code that actually does not depend on the state of the loop, they move it out of the loop. It's not as obvious as the usual invariants, but let's have a look at this line: 

Since you are using matrices, I don't think that you need the runtime polymorphic behaviour offered by but that you created functions so that you could call methods of the derived class from methods of the base class. If this is the case, you could use static polymorphism instead, thanks to the Curiously Recurring Template Pattern idiom (CRTP). In other words, make take a template parameter which corresponds to the type of the derived class: