It was a database scoped trigger which may or may not explain why I couldn't see it in sys.objects. This didn't work: 

Insert profanity of choice. The problem was caused by a database scoped trigger. Disabling / rewriting the trigger resolved the issue. 

Create 1 or more databases (current process is to restore the database from a .BAK file made from a template that is adjusted periodically) Create SQL Server logins for the server (two SQL logins a MyAdmin and MyUser account) Create SQL Server users for each database (one for each SQL login) 

If I do this I can still successfully create the database by restoring from .BAK as MyAdmin, but when I try to use the database (so I can create users in [New Database]) I get an error. 

Something tells me that I am taking the wrong approach. Should I be creating the database with a script instead of restoring from a .BAK? Can I use an account with less privileges than SA to execute all of these things, and if so what privileges would this account require? Should I remove these things from the installer package and have a separate script that can be run under the SA or another account that has the required privileges? What I am looking for is a process that will create what I need with minimal privileges. If possible, I would like to minimise the changes required to the process I am currently using. 

There are a couple of issues here. Firstly, some errors terminate the current statement and some (an inconsistent and rare few) terminate the whole batch. set XACT_ABORT on makes statement terminating errors become batch aborting errors (which is good because it forces some consistency). The problem here is that each of these go statements mark the beginning and ending of a batch. When a batch-aborting error occurs, I believe that SQL is reverting to either the start of the batch or the transaction beginning. Also, because the table create is in a subsequent batch, it is executed just fine. As mentioned by the other answer, preventing errors is better than detecting them. The drop table should be proceeded by an if statement that checks to see if the table exists before dropping it. 

I see the error log listed in the process has the wrong name right off the bat. I've tried manually restarting mysql from /etc/init.d, via 'service mysql restart', via restarting the entire server. Nothing seems to get MySQL to read the conf. Maybe something is breaking when mysqld_safe spins up a mysql instance? No problems reported in the error log when the server starts. Any ideas? 

We have the most basic of MySQL replication setups, a single Master doing statement-based replication to a single Slave, a 90/10 mix of innodb/myisam tables. Both run Percona MySQL 5.5 at the moment but we are upgrading to 5.6 in a couple weeks and have scheduled downtime. Will this kind of setup see much benefit by switching to GTID replication? Do I need to switch to row-based replication as well? Trying to decide if it is worth the extra downtime for converting to and testing GTID right now. 

Our MySQL master is allocated 24GB of RAM (innodb_buffer_pool_size = 24GB), but was curious if there was some standard amount to allocate to a replication slave? Right now the slave is just used to take dumps without locking tables on master, and in theory be used as an emergency master if something happens (hasn't come up in the last 18mo). If master fails I could always quickly allocate the slave 24GB, but how much does a slave really need to just sit there replicating? 

Via Homebrew I just upgraded from MySQL 5.5.29 to 5.6.10 and all my queries and existing users are working fine, but whenever I try to modify permissions or create users now MySQL disconnects me. Are there any my.cnf settings I need to change for 5.6, or system table updates I need to do etc? 

You can use pt-query-digest which is part of the Percona Toolkit to identify queries that are running long. It can analyze queries from General Query log, Binary Log and the Slow Query Log. Usage: to get the slowest queries from logs, where slow.log is the log file name. The log file is usually in the default directory. The option can be used to save historical data and is helpful for analyzing Query performance over time. If the slow-query-log option is disabled, you will need to enable it to log the query information. Refer to the Slow Query Log official documentation on how to enable it and for all the available configuration parameters. 

Refer to Partitioned Tables and Indexes on MSDN. is the column name that's commonly known as is the index_id. Lets do some testing now: I have used AdventureWorks 2012 from CodePlex. 

It looks like includes the following additional features (list may not included all) that are not available in the : 

In SQLServer, there is no option to backup/restore just the tables. You could create an SSIS package to import data from specific/all tables and schedule it using the SQL Agent or run manually. As both the databases Production and testing are on the same server, the data load may be faster compared to pulling data from a remote server. To minimize the load on the production database, restore the backup to a new database and then import the data to the Test database. If you drop and recreate the tables for the refresh make sure the indexes are created after the data import. Or if you choose to bulk import on to an existing table, drop the indexes and recreate them later and change the recovery model to simple or bulk logged. If you have few very large tables, you can probably consider moving to separate file groups and do a partial backup and restore. You can also try third part tools like Dell Litespeed, Idera virtual database or Apex SQL restore etc. for object level restore.