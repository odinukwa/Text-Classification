I've used "int" as a datatype in my example, but datetime2 will work as well. You can place more than one partition on the same file group if you want. Here, you'll have to do some planning with regards to how the load is distributed across different partitions, so you don't put all the I/O load on a single filegroup. A filegroup can have one or more .mdf files. A database can have one or more .ldf file. 

means that you'll only see rows where there are matching records in A and B. If you want all the rows in A and matching records in B, you could change to . Conversely, if you want all the records from B and only the matching ones from A, use . Finally, if you need everything from both tables, matching or not, you can use . 

Here's a stab at an algorithm. It's not perfect, and depending on how much time you want to spend refining it, there are probably some further small gains to be made. Let's assume you have a table of tasks to be performed by four queues. You know the amount of work associated with performing each task, and you want all four queues to get an almost equal amount of work to do, so all queues will complete at about the same time. First off, I'd partition the tasks using a modulous, ordered by their size, from small to large. 

I'm building an ETL process in SSIS that extracts data from a 4D data source using the manufacturer's own ODBC provider (). When I try to extract any type of "string" column, it comes out as a LOB column, meaning that 

Admittedly, not a pretty construct, but I wrote it for readability. The following query might perform better: 

Please, don't ever kill the sqlserver.exe task in Task Manager - what you're doing is equivalent to pulling the power plug on the server in mid-stride. "In recovery" means that SQL Server is trying to go through and repair ("recover") any work that needs to be rolled back in the database as a result of this uncontrolled interruption, which could be a considerable amount since your query took so long to complete. Often, rolling back a query can take longer than it actually took to execute. I would advise you just to wait it out and never do it again. ;) Next time a query runs away, there are a few options: If the query is in your Management Studio, just click the "Cancel Executing Query" button (Alt+Break) If the query is running in the background (for instance, in a service), or it's not your query, you can find it using sp_who2: 

This expression is sargable which is what you want for optimum performance. Like @Mikael indicates, you would do well to design one of your indexes so that is the first column, and that all the other columns used in the query are included in the index. If you have lots of columns in the query, using the table's clustered index is probably best. Read up on covering indexes. 

So you can see that there are 3 rows with a=X and 2 rows with a=Y. The clause comes into play when you want an ordered window function, like a row number or a running total. 

A dynamic management view, like the name implies, isn't a table with indexes and statistics, but rather a view which could potentially use a large number of system tables. To generally speed up complex DMV queries, you could try dumping the contents of some of the larger DMVs into a temp table or table variable and then use those. You can control the indexing on the temp tables, and it also reduces the number of join operations that the server has to perform. For the same reason, if your solutions allows for it, consider using in order to minimize locking on those system tables, which could otherwise affect your entire database or server. 

Your clause contains an explicit datatype conversion, which is terrible for performance, and can also cause this type of problem. This query will probably show you what's wrong: 

This will return the difference in number of seconds between the previous (the function) and the current . Bear in mind the clause, which defines how you partition your records. 

Assuming the column is an identity column and you want to retrieve the identity value of the record you just inserted, you can use the OUTPUT clause, like this: 

I would guess that is not really part of a relation, but rather an ordering key used to denote what book is the user's first, second, third and so on. Or, like you say, it could be that a user can have multiple copies of a book, but that still wouldn't make part of the relation, only of the primary key of . Or, a combination of the above two (ordering key and user can have multiple books). 

If you have a clustered (or covering) index starting with (Key1, Key2, Key3, ...) on the table that the CTE is querying, this should be a well-performing query. Can you add a "Key4", preferably an identity column, in the source table to your non-unique clustered index, so you perhaps can make the index unique? That way, you could also set "Key4" as your in the . This might improve performance, but it's just a guess. The fact that the Clustered Index Merge operator is over 90% of your plan doesn't by itself indicate a performance problem, it just says that there's not that much else to do in the query plan apart from the join. In fact, it may indicate that you have a well-optimized plan - when you have the proper indexes, a can be very efficient. 

As other answers have noted, overloading is not supported for SQL Server stored procedures. One workaround could be having your stored procedure accept an xml variable as parameter, so your application could send a "set" of arguments in an xml blob: 

What's probably going on is that you're running the query with different input values in your parameter. Let's look at this simplified example: 

Using like this returns one record for every unique occurrence of (cart, barcode) and the totals of counted_in and counted_out for each. 

The "best way" is a matter of opinion, but I've previously created separate logging tables on which I've built custom reports. The logging tables can be populated by the application or using triggers. 

If both and are indexed on the columns , a highly optimal strategy would be to join those two could be to merge those two datastreams (in SQL Server and other platforms known as a Merge Join). This is particularly true if you add , in which case you would want to maintain the ordering from the underlying table. As a general answer: imagine if your query was two decks of cards in your hands. How would you join them? Would you lay them out in order and merge them (a merge join), perhaps go through one card at a time from one deck and look for matching cards in the other deck (a nested loop join), or maybe subdivide all the cards into 10-15 smaller heaps and join those (more like a hash join)? How would that strategy change if one or both of your decks was ordered or not? 

Your join has to describe a condition where rows in are matched to rows in . In your example, you're just filtering rows using variables; the join is effectively a (a cartesian product), which is why you get on every row. The following query should solve your problem, with the assumption that rows in cannot span multiple weeks in . 

For more complex queries (where you need to get first row in a partition), you could probably make something useful with . 

The example is simplified, but it still illustrates what's going on. Your query Try the following, then go through the results, looking for invalid dates like 31st of november, 0th of january, 29th of february on non-leap-years, negative values (I'm assuming is numeric), etc. 

Database backups give you point-in-time restore capability (provided you have recovery model). Even if your IT people take backups every few minutes, which is extremely unlikely, you'll still have a gap. Server backups do not replace database backups, they complement them by "archiving" the database backup files long-term (i.e. more than just today). In the end you and your management have to decide your RPO (the recovery point objective - how much you need to be able to recover in a crash). With only daily server backups and no database backups, you stand to lose a full day's work in the worst case. Edit: @Sting has a valid point in that shadow copies (the mechanism most likely used to make server backups) are not very likely to take an exactly simultaneous copy of all your database files (including log files), which may lead to inconsistencies when you restore the backup. For instance, if the shadow copy reads the transaction log a few milliseconds before it reads the database file, the database file could contain an uncommitted transaction, but as the transaction was committed a millisecond later, the log will not have any record of it. 

The following solution uses a common table expression that scans the table once. In this scan, the "next" points level is found using the window function, so you have (from the row) and (the next for the current ). After that, you can simply join the common table expression, , on and the / range, like so: 

Clustered indexes include every column in the table, which automatically makes a clustered index a covering indexes. From here, I've updated your query to do three things: 

Don't try to over-design your solution. Relational theory, like many other academic subjects can often be taken to extremes, to the point where they are difficult to grasp for a third party that will (eventually) inherit your work. I think the most common data model you'll find, with regards to time reporting, looks something like this: 

Note: will be deprecated and replaced by where n is the number of decimals of the seconds. See also: CAST and CONVERT (Transact-SQL) 

Functions need to be prefixed with the schema, to separate them from built-in functions. The correct syntax should read 

Oh, and if you're on SQL Server 2014/2016 Enterprise Edition, your staging table could be in-memory. 

The best argument against the existing design is that there's really no good primary key on the table (except perhaps an identity column, which I don't consider a good primary key). That makes it hard to join to other tables. An advantage with your design is that a query can easily identify parcels where you (for instance) have mineral rights but not surface rights using a simple WHERE clause. With the existing design, this is still possible, but it's a lot messier (and not really feasible using graphical tools or entry-level SQL). This is particularly important if the table in question is a dimension table in a datawarehouse. 

With that set, all the rows in the table, but the rows that you're deleting are right into the very same table - but with specific values for the column (from the backup column). 

.. tells SQL Server to always choose the index , which can absolutely kill your query if the index doesn't properly cover all the data you want to retrieve, including the columns , , as well as the huge column list in the SELECT clause and the columns in ORDER BY. Right off the bat, I would try creating indexes like these to try to resolve the problem. 

This is entirely possible, although the only way I can think of to solve this is quite inefficient and really doesn't scale very well. 

Your question is tagged , so I'll provide an answer that applies to SQL Server (though I suspect that it goes for most modern platforms). For SQL Server, the answer is no; DDL statements do not automatically or an open transaction. It normally places a schema lock on the object(s) you're touching with the DDL statement, which is held until the end of the transaction. In fact, transactions are very useful whenever you want to combine DDL and DML statements, in order to maintain atomic business rules in your database. Don't know about other database platforms, though. 

Remember that there's a disk space issue involved as well, as you stated that your table contains a large number of rows. The clustered index will not change the amount of disk space your table consumes either way, but adding a non-clustered index will allocate extra space. If you all the columns from your table, like I did in the example, the non-clustered index will roughly take up as much space as the rest of the table does. 

The SQL Server Agent job definitions are stored in the database, in the and tables. You could do a plaintext search on those tables with the following query: 

The condition evaluates at compile-time, rather than at runtime, which means that your condition does not generate a seek. And just to clean up the code, I refactored out your to make the code a bit more readable. I would try changing the clause to: 

This is a "kitchen sink" query, for which SQL Server MVP Aaron Bertrand has a good video on how to optimize using dynamic SQL. A few points to get you started on the performance of your query: 

In my mind, the query plan for the above should be the same as if you had an . The query plan should now have a Segment, Sequence Project and finally a Filter operator, the rest should look just like your good plan. 

I hope I've understood your question correctly. Also, note that this is ad-hoc coded, without testing. Let me know how it works out. 

I added to eliminate key lookups. You can try the query with or without this hint. If I had a lot more time to work on this and it was a super-critical query, I would probably (a) remodel the tables and/or (b) consider storing the data in #FORMULAS as XML data and add XML indexes, which would eliminate the LIKE match. Footnote: Text matching like you do here ( and are pretty much the same here) will include formulas with the parameters "parameter10" and "parameter11" even when you're just searching for "parameter1".