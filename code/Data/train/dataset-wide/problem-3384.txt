Yes, you will need to restore each snapshot to a new EBS volume, and then attach the root and data instances to a new EC2 instance. The instance will boot with the root volume from the snapshot, and should be able to reassemble the disks into your working volume. 

This seems to indicate that nginx is crashing immediately, if it had been started earlier. Have you checked the contents of to see what the process is doing? EDIT: Also, if you tell us your OS and version of nginx, we can give more detailed answers. 

The answer from Mick's comment: Problem: Zabbix proxy poller processes more than 75% busy Solution: In zabbix_server.conf 

You will want to "Pin" the specific package version: $URL$ $URL$ This post specifically describes how you would accomplish this: $URL$ 

No, if you don't touch those other disks during the installation, and there's no RAID or mirroring going on, modifying the C: drive won't harm your other disks. You should backup before reinstalling your OS, just because something can always go wrong. Or at the very least unplug those drives from your computer before installing the OS so that you can't accidentally wipe them. 

When you restore with Clonezilla, the partition letters it's showing for the source system are just there so you can remember which partition is which. It won't actually restore sda5 to sda5, for example. However if you are going to restore partitions from a Clonezilla image, you need to first create corresponding partitions on your destiation disk. For example, on your source computer you cloned /dev/sda5 and /dev/sda7 to an image. On your destination you have 2 existing partitions, say /dev/sda2 and /dev/sda3. You will need to create two more partitions, which may end up being /dev/sda8 and /dev/sda9. Then you will launch clonezilla and select "device to image", and then "restore from parts". You will then select which partitions in the image you want to restore (i.e. /dev/sda5 and /dev/sda7), and then it will ask you where you want to restore them, so you'll select the new partitions you just created (e.g. /dev/sda8 and /dev/sda9). And that should be it! 

Your interface is a Point-to-Point Protocol tunnel. PPP connections are designed to only go from one point to another, so there is no need for IP addressing. Essentially, if your machine sends a packet out over the PPP interface, there's only one place it can go! So no need to differentiate the possible recipients with different IP addresses. You didn't specify which VPN software you're using, but configuring which routes are pushed to a client is a common option. If you're using OpenVPN, you can find information on defining which routes to push here. EDIT: It seems like you may be using a PPTP VPN (which, by the way, is no longer sercure and should be avoided if possible). In that case, you can configure what routing entries are added by doing something like this: Route traffic via ppp0 To route traffic via PPP0 interface, edit 

I would open Disk Management. You can search for it in the start menu, or find it under the Control Panel in Administrative Tools. Alternatively, press and then type and hit enter. From there, you should be able to find your boot volume, as it's typically labeled "Boot". If you right click on the partition image on the lower half of the window, you should be able to "Extend Volume". From there, you can add the additional free space of your bigger RAID volume. This provides some more detail on the process: $URL$ EDIT: This would work but for the fact that the second partition would need to "moved" to the right, i.e. the end of the new logical RAID volume, before the boot/C: drive can be expanded. 

I'm trying to monitor a log file for the word "error", and if the word appears, fire a trigger for each new occurence of "error." I'm starting simply, with a file () that is world-readable. It contains the following lines: 

node.js is not an operating system. You need something, be it Linux (Fedora, Ubuntu, etc) or Windows or BSD, but you probably just want to start with Ubuntu. See this post for a good list of resources on getting started: $URL$ 

I have always configured my /etc/hosts file with both the local hostname and FQDN. Per the /etc/hosts manpage: 

I have a main backup server running rsnapshot, with ~2TB of backups stored on it. After the nightly backups, I copy the contents of the rsnapshot directory to an offsite server, using . However, this seems to copy the entire contents of the backup directory every night, as the offsite copy takes ~9 hours to complete. I assume my rsync command doesn't have the right switches, but there may be something else I'm missing. Any ideas? 

The order in which DNS queries are resolved is based on the contents of . Specifically, the order of the options on the line for For example, this line will cause your machine to query a server before checking local files: 

You can definitely have a secondary NIC, USB or otherwise, that is connected to the device. You can then configure that NIC to have an IP in the same range as your PLC. 

There is also netcat, which is another very simple networking tool that can make arbitrary TCP and UDP connections. 

$URL$ Also, I hope that you're not using this server for anything else, because giving an application full root privileges without restricting it via sudo at the bare minimum is very dangerous. Especially an application that is connecting over the network, as you say. 

If you used the deduplication built in to a filesystem like ZFS (which is what FreeNAS can use), then any file-level backups (e.g. rsync) will not see the deduplication and copy the files as if they were stored normally. 

If you're trying to disable the wireless entirely, I'd suggest disabling the adapter in the BIOS, and configuring a BIOS setup password. This will prevent the wireless adapter from appearing in the OS at all. Alternatively, you could physically remove the wireless adapters from the laptops. 

Have you tried running iptables-save on the server after you've made the changes? This should write the current iptables rules to a file that will be loaded on subsequent boots. 

I'm planning on migrating 2 HyperV VMs that are currently running on Windows Server 2012R2 to a new server running 2008R2. What's the best way to migrate the VMs? Will replication work between the two OSes? How about the export feature in HyperV? I know that it's not possible to move directly from 2008R2 to 2012R2 without an intermediate conversion, but I can't find any info on doing the reverse. 

I was having the same issue, and disabling SELinux resolved it. I'm running CentOS 6.5, and using DRBL/Clonezilla. Steps: 

A good hardware controller shouldn't have a bottleneck between the controller and disks. You will certainly be bottlenecked by your SATA II onboard controller if you connect a SATA III SSD to it. You can't upgrade the onboard controller, assuming by onboard you mean that it's built in to the motherboard. If that's the case, then you'll need to buy a PCI express RAID card with SATA III ports. As for which particular model or brand to buy, that's up to you. Check the reviews, and make sure your particular drive doesn't have compatibility issues. 

The item in Zabbix is , type "Zabbix Agent (Active)", with information type "log". I'm using the following trigger tied to this item: I've tried leaving the "error" part out of the item, and changing the trigger value to equal 0. No matter what I do, I get a trigger status of UNKNOWN in the Events tab. I'm not sure what I'm missing here, does someone see the issue? I checked the agent log on the server, and it doesn't mention any issues. 

My guess is that running will create a default gateway automatically, and that this default gateway is conflicting when you run the command. Swapping the order of the commands should add the route first, and then the command will see an existing gateway for that network and not bother to configure one. You can test this by comparing the routing table after running only the command. If this command adds two lines to the routing table, then this is the source of the error. 

This seems to work pretty well, though I do seem to get some duplicate alerts, instead of unique alerts for each line. Please feel free to point out any places this could be improved, as this is my first time making a script like this. 

I realize this is a bit convoluted, please let me know if I should clarify or explain what I've done. 

This is highly dependent on your OS, what are you using? If you're using Linux, ethtool will do this: 

If SuperMicro says their implementation supports it, and Intel says it's available on some SKUs (that is, some specific versions of the chipset), then I don't see any reason to believe why that server wouldn't support PCIe 3.0 

I've been trying to replicate the OpenLDAP server we currently have, so that we can use it in the event the main server crashes. I was able to get the database transferred, as well as the TLS certs, and ldapsearch works fine and return the correct information. However, I can't seem to authenticate using the database on the new server. I am unable to su to an LDAP user on the new server when it is using itself for authentication. I currently have an entry in that redirects traffic to the old server to localhost, e.g. . If I comment out this line, the new server can authenticate against the old server perfectly well. However, when the line is uncommented, I get whenever I try to su to an LDAP user. I get the same issue if I change ldap.conf to point to the new server's FQDN. For some reason, it seems that the data is all in place on the new server, but it's not being made available for authentication. Also, shows the correct output when the new server is authenticating against the old, but only shows a few LDAP users when authenticating against itself. The current configuration is: 

The output should be pretty explicit in whether or not SSL/TLS is enabled. Specifically, it'll returned the message , followed by a lot of SSL handshake info. Also, you can use $URL$ if your site is available on the internet. 

Also, they state "Mixing components operating at different internal clock frequencies is not supported and will not be validated by Intel. Combining processors from different power segments is also not supported." $URL$ (page 165) $URL$ (page 28) $URL$ (page 25) 

Given that you said it's not feasible to keep spare Enterprise drives around, I'd say that going for the consumer drives (that is, the Samsung EVO or PRO SSDs) is a better call, since you lose all remaining redundancy in RAID 5 when you lose a disk. And having to wait for a new drive to show up means that you're running the risk of total data loss if another disk has an issue before the array can be rebuild. So keeping the spare drives on hand is a good call. Also, what's the reason for using RAID5? It's performance is pretty awful on writes, and though the SSD will make up for some of that, I'd go for RAID1 or 10 unless you absolutely need to maximize the usable space on 3 drives. 

I think that the /isolinux/ ones are for booting a live CD using ISOLINUX, whereas the pxeboot ones are for booting CentOS from a PXE server. PXE booting is when you obtain the kernel and initrd from another server using a TFTP client built into the network interface. Typically you'll see something like "Network Boot" in the boot list in your BIOS. 

Have you investigated using a container service, such as Docker or LXC? That would allow you to use the version of PHP and OpenSSL (and many other programs as well) that you desire. It also wouldn't interfere with the system's PHP and SSL installations, it's entirely self-contained. Docker has prebuilt PHP environments it seems: $URL$ LXC how-to: $URL$ If you're brand new to containers, I'd say that Docker is a good place to start, as there is a boat load of community support for it. 

OpenVPN is running on pfSense 2.0-release. Until recently, these messages had not occurred. In the OpenVPN server logs, the follwing set of messages repeats every minute 

I agree with a lot of what ignvis says, but I think the key factor is whether or not this server is internet-facing. If it is, you should heavily favor upgrading most of the software to the latest bug/security fix release. You can usually update Apache to the latest 2.2.x release without issue. I would not want to upgrade PHP beyond the first two release digits, e.g. 5.4.x, but it seems that 5.4 is no longer supported, which is a big worry. I'd do some testing if possible to see if your site's PHP files work on a similar server with a newer PHP version. phpMyAdmin is a buggy insecure nightmare, so I'd update that to the most recent version possible. You should also likely upgrade OpenSSL, as there are too many vulnerabilities in the last couple of years to list. Don't upgrade MySQL beyond the first two release digits, without extensive testing. And really, you should test every upgrade, regardless of how important the security implications are, just to know what issues you might encounter. Can you setup a development or sandbox version of this server to test upgrades on? Even a VM on your laptop should suffice. 

I've searched high and low trying to find a method that allows me to store GPG keys for existing users in an OpenLDAP server. The only relevant how-to I've found is this. However, I'm unable to get this method to work with the existing OpenLDAP database. I've successfully imported the schema, but I can't figure out how to actually add information to the fields specified in the schema. If I can provide any additional information, please let me know. 

I can't seem to find an example of how each of the log levels in slapd work. I want slapd to log the users who are logging in, and the server that they are trying to log into, as well as any authentication errors. I've tried using this LDIF to modify the logging level to every level (except for the debugging levels): dn: cn=config changetype: modify replace: olcLogLevel olcLogLevel: Can someone point me to a more helpful resource than the OpenLDAP manual? Or give me explanation on what typs of information the logging levels produce? Thanks for your help. 

Clonezilla lacks automatic support for this feature: "Software RAID/fake RAID/firmware RAID is not supported by default. It can be done manually only." See "Limitations" on the Clonezilla web page. 

Yes, your hosts file will always override DNS resolution, as it is the first thing checked by your local DNS resolver. However this should not be an issue in any network configuration I can think of, unless you need your client to ask the DNS server to resolve the client's own hostname for some reason.