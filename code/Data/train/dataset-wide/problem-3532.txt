Some packages are held by more than one repository. The plugin choose packages from the highest-priority repository, excluding duplicate entries from other repos. 

Well, it's not working on all platforms. It's not working on Solaris, until you enable direct I/O. Direct I/O forces synchronous writes and disables readahead, so you are changing timing of I/O, possibly masking the root cause. You can try debugging your app, which may be difficult and time consuming, or you can just slap a band-aid and enable direct I/O on AIX too. 

Root servers should never be asked a recursive query (i.e.) for a server.example.tld, only about the servers handling the top level domains (.tld in the example). However I think, that what you mean you want an answer from an authoritative server. In order to do that do: 1) Ask for NS i.e. name server field, instead of A -- IPv4 address, for domain nasa.gov 2) Change the server field to one of servers obtained in point 1). 3) Ask the question about address of www.nasa.gov. The same procedure works for any other domain, of course. The procedure works for a CLI dig and with $URL$ interface. 

I'd recommend 2 HOWTOs: This one: $URL$ will let you configure your NAT (look for the section "I just want masquerading! Help!"). This one will give you some basics on networking: $URL$ The HOWTOs are rather old (made for 2.4 kernels), but they are still relevant. 

Remote to the target system using a secure connection and issue commands there. Not always feasible, because you'd need to have needed tools installed on the target systems. Employ encryption on a lower level of a stack. This could take a form of VPN between the systems involved. Probably a lot of hassle to configure it for all systems you'd want to connect to. 

I'd brute force it. First I'd try if it works with iptables stopped. If it does, then it's something in the iptables. Then I'd add rules one by one and watch which one causes connection failure. Then I'd play with that rule until it did what I wanted without totally disrupting the traffic. If it doesn't work with iptables stopped, then it would start to be really strange. 

I do not think it would be a bug in the ext3 code, because it's been around for a while, but do you use any exotic mount options? Do you have 4K block on the storage? Anything exotic? Did the server ever work OK? If so, can you name the change, that caused it to start failing? If you are going to troubleshoot it yourself, then your best bet would be to make a minimal set of options that makes the system fail. More practical approach could be to re-organise your storage so that you use only one vendor's storage at any given server. This could save you a ping-pong between vendors. Your best bet, however, would be to contact your OS vendor and make them drive the case, I think. 

If you do not plan any HA features, then iSCSI NAS isn't going to buy you a lot. About the only scenario would be failure of the server, when you could re-install the server and just re-connect the datastores on the NAS. OTOH it gives you one new failure mode, where NAS box fails and you have to restore the data somewhere. I assume the disks will be connected to some HW RAID card? Otherwise a single HDD failure will take out your datastore, which is bad. Locally attached disks chould give you better performance than NAS, especially in response times. I have another question: if you do not plan to grow beyond single VMware server, then maybe you could take a look at the free VMware hypervisor? Or just buy support for ESXi? If your VMs do not exceed limitations of the free license (RAM and CPUs of the host), then maybe you don't really need vCenter? 

If it's RHEL 5 or 6, then don't use any of the two LSI controllers in virtual machine definition. Use the paravirtualised storage driver. See this VMware KB article for the supported configurations and migration procedure. 

If the time to restore data isn't prohibitive, then re-installing from scratch will be faster and less error prone. I'm not sure what is your planned final setup, but if you intend to use all 3 disks, you could do 3-disk RAID 1 for /boot partition, and have the rest of the disks set up in RAID 5, if it's acceptable performance-wise for you. 

This particular failure (loading of unwanted module) could be prevented by blacklisting it in modprobe.conf (man modprobe.conf). 1) Installation of britty package? It's a difficult question for one who didn't experience similar problem. There are 2 possible scenarios: a) You have a race condition in the order of modules loading and you were lucky so far. b) Somebody changed something in-between reboots. Without tripwire or similar tool that watches (important) file changes it's difficult to impossible to tell what was changed and by whom. I'd bet on scenario b). From my experience Linux is a nice repeatable system. It either works every time or it doesn't work every time, until you fix it. 2) I like Red Hat manuals. Go to $URL$ and read to your heart's content. There will be differences between RHEL and Ubuntu, but it's a good starting point (disclaimer: I use RHEL a lot. I don't use Ubuntu). $URL$ could be a nice read. Unclean shutdown does not guarantee any sensible state of data on the filesystem. I'd recommend an UPS to protect the server. 

Yes, converting to ext4 should give you better performance, for reasons like delayed allocation. ext4 is also better at preventing file fragmentation, which also helps performance. Moreover, there's a serious talk about handling of ext3 filesystems by ext4 module, so very probably you are going to use the same code Pretty Soon Now(TM) even if you stay at ext3. Going to ext2 is a bad idea. You'd move to an old code, without all the improvements your hard-working file system developers put into ext3 and ext4 over a decade or so (ext3 was introduced in fall of 2001). How do you convert? There are 2 approaches: 

If you decide to go via route 3, you again have several options :). Just note, that DNS servers are a crucial piece of infrastructure -- if they fail then people will be unable to reach anything within your domain. Also note, that to have your own DNS server, you have to own a domain. This means, that if you want to have a server in domainone.com you have to own the whole domain. If your host is alpha.domainone.com, but someone else owns beta.domainone.com, then you have to have the same server. There are several ways to get your DNS servers point where you need: 

not only Additionally should have permission set to allow read the directory, i.e. list its content. 

Your dhcpd is complaining, that it did not find configuration for any network, so it has noting to do. Therefore, instead of wasting memory and CPU cycles of your box, it quit. You need to create configuration file for the daemon. I'm not familiar with Ubuntu, but usually dhcpd package includes an example file. Run and then to find this example file (could be named ). If you do not find it, there's or examples in the net, e.g. here: $URL$ 

If you set up your networking as static, then in my personal opinion it should work. If you use DHCP to assign addresses, then you don't have a sensible at the time %post scripts are executed (unless Ubuntu, which I don't know, has Anaconda wildly different to what RHEL, which I know, has), so should fail. If the installation fails even with fully configured networking, then a quick-and-dirty solution would be to copy the required files to the installation server, then wget them over to the machine being deployed and install them from a local filesystem. 

If professionals question if 99.999 percent availability [is] ever a practical or financially viable possibility, then 99.9999% availability is even less possible or practical. Let alone 100%. You will not meet 100% availability goal for an extended period of time. You may get away with it for a week or a year, but then something will happen and you will be held responsible. The outfall can range from damaged reputation (you promised, you didn't deliver) to bankruptcy from contractual fines. 

You only packets going out of . Your default route specifies both and . This means, that outgoing packets can get out of any of the interfaces. If they go out of , NAT works, if they go out of , it doesn't. You have 2 default routes with NATting. This creates a problem of source IP address for outgoing packets -- they may end up having any of the 2 addresses, depending on the route they go. Therefore, for the destination host, they appear as connection from 2 different sources. The easiest thing would be to specify SNAT with just one address. This will cause all outgoing packets to have a well-defined source IP address, and come back just via 1 interface. The down side is, if you loose that interface, you'll loose outside connectivity as well. 

Storage -- is the filesystem OK after a remount? Can you do a full fsck on it to see if the problem with journal is the only thing that went wrong, or maybe you have a lot of silent corruption, and only when the bug hits the journal it becomes visible. This particular LUN. Can you (as in: is it feasible) to format it, restore data and see if it happens again? Can you create another LUN on the same array and see if you can reproduce the error? A LUN on a different array on the same storage? Multipathing -- can you reproduce errors if you access the storage directly, over just one path (this requires changes to SAN zoning or lun masking at the storage). Drivers collision between powerpath and the native multipathing. Can you reproduce an error on the same LUN when you don't have powerpath installed? 

What did you change before the reboot? Why the machine rebooted? From what's on the screen there are 2 SATA controllers in the box, each with 2 drives plugged in. From your description, there should be 2 and 4 disks. Two disks have disappeared somewhere. Can you determine, why there are only 4 disks visible, instead of 6? Does re-plugging help? What does your raid controller say about the arrays? Still, it's strange. If the boot disk disappeared, then where the kernel boot from? If the non-boot disks disappeared, why didn't the boot progress to the stage where a mount of some filesystem fails? If one disk from each group is gone, there shouldn't be any arror at the OS level at all... Something's very strange here. Try re-booting again, but remove option from command line, this should provide more information. 

Identifying the repository What method do you use to access the repo? If it's , then look for , the path to the repository will be inside. If it's HTTP/HTTPS, then look into your web server configuration file. If you access it using protocol, then I'm afraid you'll have to figure its location by yourself ;). Exporting the data Stop the server used to give access to the repository and make sure no one is using it locally (via ). You don't want to miss some last-second update to the code. Then you can run or , whichever you prefer, to have a ready-to-move set of data files. Additionally, save the files responsible for authentication to the repository. You may want to re-create corresponding access hierarchy in the new location, or simply know in 2 years, what users had access to which parts of code. If, for reasons incomprehensible, you want to drop the history of the project(s) within the repo, then simply check out the latest version(s) of project(s) and forget you've ever been using a version management system. Removing the repository and Subversion After you migrated the data off the server being decommissioned (at least 2 independent data copies recommended), and, preferably, have it imported into their new home (to verify that you can actually use whatever you exported) you can remove binaries used by it. It means the subversion packages plus whatever was used to access the repository. If you use , then you can simply remove it, using standard procedures for your distribution. For HTTP/HTTPS based access you need to remove modules responsible for Subversion interface, but may as well mean removing the whole server (if its sole role was to let developers access the code). If the web server stays, then remember to comment out/remove section of the configuration, that was responsible for interfacing the server with Subversion. The last step is a simple on the Subversion repository directory. 

I assume you expect biggest gain from dedup to come from the OS part of VM images. In that case if your VMs aren't clones of each other, then I'd say that 128 KiB is too big block for optimal dedup. If your bottleneck is the network and more efficient deduplication would be helpful, I'd go way down. If you are deduping disk images, then optimum size would be the minimum VM OS-level unit of allocation. In Linux that's a 4KB block (by default) in ext3 and ext4 filesystems. With bigger block sizes pay attention to partitioning, you may have identical systems shifted by half the dedup-block-size because of different virtual disk layouts. It's hard to give better answer with a rather vague question.