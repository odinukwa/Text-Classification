We see that there is one interaction scree process, 2298, which has /dev/ttyp3 open. Process 2299 also has this tty open, but process 1979 does not access any tty. So you can infer from this output which child processes are talking to which interaction processes. 

My own experience is that very little comment spam is intelligent, in the sense of getting around filters, in the way that email spam is. 

That stops a client accessing more than one mailbox; That might stop badly written clients who don't close old connections from making new connections, if they won't clear up their mess. I'm guessing this means that maybe you don't want what you think you want. 

You can tell iptables to only allow each IP address to connect once to port 110. There are a couple of disadvantages to that: 

If you're familiar adminning desktop systems with the same OS as your VPS, then you'll be spared the biggest shock that comes with VPS. VPS is to shared hosting what (remote) adminning a system is to having a user account on that system. Things you have to worry about with VPS that you didn't have to with shared hosting: 

Now, the filesystem reports a usage of approx 110GB, but the zramfs device reports 165GB. At the same time, the zramfs memory is exhausted, and the filesystem becomes read-only. The zram figures confirm that we are getting a 2.2 : 1 compression ratio between orig_data_size and compr_data_size; however, why does the filesystem show much more free space than the zram device? Even if this is space already allocated for re-use by the filesystem, shouldn't it be reused rather than allocating new space? The data consists of a large number of small files which are added and removed at irregular intervals. 

I think that, for some reason, your Sympa installation thinks the hostname is now whereas previously it was . As a result, emails coming in to are passing through the alias file only to be rejected as having an invalid domain. In your , explicitly set the host domain: 

Check your nagios.log and see if it is showing that NSCA has submitted commands, but for the wrong hostname/servicename Make sure NSCA has write permission to the Nagios command pipe file Make sure nagios.cfg specifies that external commands are processed Make sure Nagios has a reasonable command processing interval in nagios.cfg Check that NSCA is configured to accept commands from the remote sender - if you use Xinetd for NSCA then this will be in the file, and if you run NSCA as a daemon you should look in your file. Check your local host firewall to make sure it is not dropping the inbound NSCA connections. 

You can tell which screen processes are linked to s by looking at the output of : if a screen client process is connected to the screen interaction processes, then they will share tty devices. So for instance, with: 

I agree with Tom that this is only an attempt to subvert your comment system. I get hundreds of these, and similar, each day, and I don't bother trying to filter them on the server, because: 

The only thing that happens to them is that they are emailed to me; the server does nothing else with them. I can approve comments offline, using Steve Kemp's chronicle. I prefer to have a lean set of filters on my server, to deal only with things that are dangerous. Nearly all are trivial to spot using mail filtering software, and that seems the right place to deal with comment spam. I prefer reviewing spam using mail rather than server logs. 

You are trying to conserve power: eagerly swapping out will generally increase the amount of disk spinning, since some propertion of swapping out will not be needed; The swap might improve the zippiness of a process whose performance you might not care about, at the expense of a process whose latency is an issue. 

When you change any custom ruby code, such as a custom function, you must restart the Puppetmaster. If you are running puppet under passenger, this means restarting Apache. Otherwise, you'll get the old version. In addition, there is a rather nasty bug that kicks in if you are hosting multiple environments in your puppetmaster with an identically named function in the other environment. In this case, you have no way to be sure which environment's function is used when you call the function name! It uses the same function namespace across all environments... (this is confirmed to happen in Puppet 2.7.22, not sure about 3.x) 

If you have the $, then you can get the Shield software for Elasticsearch. This will allow you to use PKI certificates for authentication and authorisation to the ES cluster. Combined with iptables to restrict access, this should be everything you need. If you're trying to do it for free, then use Apache as a reverse proxy in front of ES to place SSL over the top with some sort of authentication - either Basic user/password, or Certificate-based if you want to be really secure. Again, iptables can block direct access to ES from outside and you can use the Apache access rules or iptables to block unauthorised IPs. 

Look at twill, which gives you a command-line interface. It doesn't support Javascript, but it does support cookies and forms. The Mozilla project has a more complex offering, XULrunner, which is supposed to support the whole XUL runtime, but I don't know how well this works in practice. My gut feeling is that the semantics of javascript are hard to model satisfactorily with a browser-in-the-middle. 

Data swapped out of RAM will exist both in RAM and on the swap disk until the RAM is claimed for another purpose, so are rather like mmap'ed files. As I understand it, if a process then decides to use this data before it is reclaimed, it therefore does not have to be swapped in again. Swapping is usually good, but there are two main reasons why you might not want it: 

Namecheap - $URL$ Xname - $URL$ (more than 25 domains is considered "abusive" without donation; see their conditions) ClouDNS - $URL$ (max 6 domains free) 

seems like a common place to put clojure's binaries and libraries — why I don't know, it seems a natural for — so creating a subdirectory under this for these bash scripts seems fine. The general point is that it makes more sense to organise scripts by function, not have all bash scripts in the same place. 

If you place another gateway device between your Ironport and the Internet, then your only option is to disable SenderBase and any other IP-based authorisation in your Incoming Policy definitions in the HAT. You cannot tell the Ironport to obtain the previous-hop IP address from any other method than the incoing TCP connection itself (for obvious reasons - otherwise it would be far too easy to forge). One option would be to reverse the order of the devices -- IE, put the Ironport between the Internet and the Proofpoint box, and set the Ironport to have a fixed SMTP route to send all incoming email via the ProofPoint. Otherwise, you lose out on the Ironport's Senderbase rules, which are (IMHO) one of the primary benefits of the Ironport. 

You should migrate your mail using the utility from Dovecot. This will preserve the UIDs and even POP3 UIDLs if necessary. Run using the option, to 'reverse backup' from the remote IMAP server to the local Dovecot server. You need to have a special configuration file created, something like this: 

This scans the first $1+$2 lines twice, so is much worse than Dennis' answer. But you don't need to remember all those sed letters to use it.... 

If you're using linux, 'iptables' allows you great freedom in choosing a policy for throttling new connections from IP addresses or IP address ranges. Try: 

The good news is that running a web server shouldn't look much different, in my experience, after the move, except that you can change some things faster because you don't have to ask anyone. If you enjoy looking after boxes, you will find a VPS much more fun than a shared host. Try running an experimental VPS for a few weeks before you commit anything to it. 

Use rsync if you can. Rsync allows you to generate diff files that show what has changed, what exists on the target but not in the source, etc. It will make this kind of task much easier. There are several repackagings of rsync for Windows. 

Steve Kemp (again) has an xml-rpc-based comment filter: it's how Debian filters comments, and the code is free software, meaning you can run your own comment filtering server if you like; There's Akismet, which is from the WordPress universe; There's Mollom, which has an impressive list of users. It's closed source; it might say "not sure" about comments, intended to suggest offering a captcha to check the user. 

RRDTool (as of version 1.4) does not allow you to have different scales on the Y-axis above and below. What it does allow you to do is to create a secondary Y-axis with a scale shift argument. Items are still plotted according to the primary Y axis, though, so you need to perform any necessary calculations yourself. The necessary parameter is --right-axis. $URL$ 

I am trying to avoid having to implement a UDP load balancing proxy, or write a new custom plugin to replace ec_dns_lookup. 

The command only retrieves data from the raw RRA; if you do not have an RRA of the requested granularity, then the nearest available will be used, but no additional calculations will be performed. Add a new RRA to your RRD file when you create your RRD file, with a 1cdp==5pdp rule. EG 

It is not Dovecot recreating the Trash folder; it is your mail client (Outlook in this case). Some mail clients, when first setting up the definition, will probe the mail server to identify folders with the special use flags such as \Junk, and will then use these flagged folders for the special purpose. Others, such as Outlook, will just go ahead and do things their own way, and will create a folder with the name that they want to use regardless. What you can do, is to use the plugin to make both names valid. See here for an example, which makes both "Sent" and "Sent Items" equivalent: $URL$ 

If you approve all messages that appear on your website, then don't use a captcha. There are comment filtering services out there that can analyse comments in a manner similar to mail spam filters (all links to the client API page, organised from simplest API to most complex): 

For myself, I'm happy with offline by-hand filtering, but I suggested Kemp's service to someone who had an underwhelming experience with Mollom, and I'd like to pass on more reports from anyone who has tried these or other services. 

DNS: you probably have to set up and maintain a nameserver; Mail: likewise a mailserver; Securing ports and syslogging: clients on shared host systems have to worry about only a small fraction of these issues. Ssh daemon and user authentication. 

Your emails are going to get marked as spam regardless of how you send them, if the recipients mark them as spam: GMail, &c, will learn from this, and mark other emails from you as spam. Make sure that people want to receive the emails you send. What perfomance you need will depend pretty heavily on what the social networking software will need, as well as on the number of users and what they do. As a rule, say, PHP sites are less resource intensive than Ruby sites. 

This assumes your DS to be names of course, and is not as efficient as doing a when you already have the required RRA. 

The cause of this turns out to be that when files are deleted from the ext4 filesystem living in the zram0 device, the memory is not freed back to the system. This means that, although the space is available for the filesystem to use (the output of ) it is nevertheless still allocated memory ( the stats in ). As a result, the memory usage heads up to 100% of allocation, though the filesystem still finds itself half-full due to deletions. This does mean that you can still fill the filesystem and new files will not use as much new memory space; however it does affect the compression ratio negatively. The solution is to mount the filesystem with the and options. The releases freed filespace back to the memory device and as a result the usage on the two matches again. 

You can read the disk queue on a Windows server by using the or agents. These agents are intended for use with Nagios; however they can also be queried by MRTG. You can use the plugin for MRTG that is distributed with the Routers2 frontend. This queries using the NSClient protocol and can access any of the Windows PerfMon counters, which includes the value you require. The plugin has a compilable C version and a native Perl version; the C version is better if you can compile it. To define a MRTG Target using this, have a definition of the form: 

Not explicitly, as far as I can tell! At least, nothing under /etc or /var/tmp mentions these IP addresses. But says something I can't make sense of: 

If you do want a host physically resident in the EU, use a European VPS. Go for something based in Amsterdam: that's where the AMS-IX International Peer Connection point, which means it is pretty central for most of the European internet, has as good connectivity to the US as you'll get, and has a huge number of VPS providers. Cf. The ICANN IP hubs map. 

If the alternative is irregularly applying updates, you don't actively follow security updates, and you are running a vanilla stable Lenny, then auto-updating probably increase the security of your machine, since you will update known security holes faster. 

If you're worried about lots of idle pop3-login processes cluttering up your process space, I recommend having cron periodically issue a HUP to the master dovecot process. It doesn't sound like the right thing, but it's clean enough.