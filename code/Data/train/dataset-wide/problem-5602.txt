I guess I am answering the rest of it anyway... (Inauthentic of me...) 1) No. I don't think you properly understand Sartre's position. People don't always choose freedom. Inauthenticity is also an option. So one can choose against freedom, but only hypocritically. From a certain point of view, we generally do so. Evasive rationalization is very natural in a complex society with a lot of determining forces like government, the opinions of others, the possibility of violent disagreement... We often justify our actions and pretend we have not chosen, because we don't like the result and would rather disown it. Freedom is not always good. I can be pointless: we can need to decide when it won't affect any relevant outcome. It can be horrifying: we can be asked to make decisions no one should make knowing that we cannot make them well. It always presents the risk of choosing wrong, which can be as bad as possible. But it is literally inescapable, if one is truthful. Truthfulness is not always good, it can have all the same problems. But habitual lack of truthfulness is a widespread disease. Lying is not necessarily bad, but it makes things more complex and degrades the sense of meaning in one's actions. We can choose to lie to ourselves about our freedom. That can even be good. It just has a price. Too many people are not aware of the price because they are so used to choosing to be inauthentic that they do not know what a sense of meaning is, anymore; and that is a widespread form of mild insanity. 2) That is not really a fair way to put it. It puts a syllogism in his mouth that has an undistributed middle. Point a implies c: freedom is absolute, you will choose freedom because absolute freedom does not permit any other option. What it comes down to is that whether and how to choose is one thing in life, perhaps the central thing, over which one ultimately has no choice. You will do it, and you will choose freedom. You can do so consciously or subconsciously. You might as well be honest with yourself. Yes that contradicts everything I said in 1), but this is to some degree a theory about lying and not lying, aimed at a herd of people steeped in lies. It is not hard to see that the contradictions themselves are superficial, but inescapable -- that there are two senses of 'choice' at play, but that they can't really be separated. 3) As pointed out in the comments Kant purposely takes autonomy as the only value, and builds from there. So clearly Kant is an option for an Existentialist ethics. That is, as long as one realizes that it is an option, and that Kant's own choice to claim he is basically coerced into it by logic is dishonest and evasive. So the whole remainder of Kant's philosophy is not an option, and you need a different way to look at your motivation to frame things in this way. Cynicism is another system based entirely on freedom as a goal. It adds contrarianism as a subgoal, with the added insight that we can all create more consciousness of our options by subverting patterns of assumptions that are too popular. Within a Nietzschean worldview power and freedom form a direct feedback loop. So within an ethic of power, freedom is a value equal to the primary value, giving yet another option for ways of choosing ethical priorities. Existentialism can be used to sew all of this together and prune it for efficiency. 

Science does not work directly from induction, it requires a theory in addition to observations. That provides a mechanism that forces statistics to apply. As you repeat an experiment, the odds it will ultimately fail to repeat in the future do go down. They never get to zero. You could still be wrong. But that becomes less and less likely as you successfully reuse the theory over time. Even if you don't do the p-value computations, you are safely covered by the Rule of Succession, as long as you have a falsifiable theory, and you challenge it. You have some odds p < 1 of being wrong, and if you successfully repeatedly use your result n times the odds it will eventually fail are <= p^n, which converges to zero when n is infinite. Since 'being falsifiable' implies you would recognize when you failed, you are legitimately sampling a distribution, and this is not induction, it is math. Each time you 'fail to falsify' you 'reject the null hypothesis' and in doing so you are building a statistical basis for believing your model and its underlying assumptions are less and less likely to be wrong to a significant degree. If you do keep actually compute the p-values, as people in very slippery sciences like psychology and sociology force themselves to do, you can go back and figure out just how unlikely ultimate failure is becoming due to seeing the outcome in various instantiations. Statistical convergence is not really induction in the pure philosophical sense, it is deduction with probabilistic truth values, that never really reaches 100% reliability. So he is 'right' that science is never logically watertight. And we already knew that, given that it constantly changes and evolves. But he is wrong to say the procedure is not deductive. 

The focus in classical logic and mathematics is essentially conservative. We want to preserve the truth of the premises, and avoid inconsistencies and ambiguity. We do not necessarily intend to trace cause and effect properly. (The implicit lack of temporality should be an adequate indication of this. How can one really care about cause and effect, if you have already dismissed time?) Given those goals, it is safe to fill up the indeterminate cases left over from syllogistic logic with 'false-implies everything' because one should not be deducing anything from a false premise to begin with. Whatever the reason for mistrusting it, overstating the mistrust is better than understating it. By taking up the convention that any false premise implies all absurd things, you are overstating the reasons for your mistrusting it as broadly as is possible, which is an optimally conservative position. It turns out that we can maximize the excessive safety without resulting in any extraneous contradictions, by simply replacing all the ambiguous outcomes with 'false'. So the convention is conservative, and definite. It will not lead you into asserting too much if your premises are weak, but it removes the ambiguity of a third truth value, even if that leads to inappropriate attribution of causes. 

So knowledge is made up of perspectives, each of which answers a different aspect of will and gives us a different sort of power. This sort of view is consonant with Shannon's information theory. Information, in that context, is detectable order that can be used to predict or control a system. Knowledge, in this sense, then, is information when the prediction or control afforded addresses an actual or potential wish or need. Basing 'knowing' on Shannons's definition of 'information' also agrees with more modern notions of science that are based upon making predictions as the primary test for knowing. Another aspect here is that knowledge answers to a real need, or, at least, a drive evolved to fulfill a need, absent from more abstract notions of knowledge as a faculty such as the "justified true belief" definition or definitions based upon specific kinds of acts. This captures the Montessorian notion of horme or the Lacanian notion of jouissance. Knowing is what horme reaches toward and what embeds and preserves joissance for later use. Learning and knowing are self-integrating processes that explain the mind's engagement in the world as a natural aspect of biology. For your question, then, the question from my point of view is how is intention embedded and transferred? Do machines have needs of their own? Do the needs we build into them continue the biological process, or is there an essential disconnection between derived needs and designed ones? For that, I would look at genes. To my mind, from this point of view, genes know things, and they think: They evaluate and manipulate information with an intention to survive in future creatures. (They do so through the insanely indirect mechanism of creating numerous whole complex creatures. But that is beside the point. They do it.) Therefore, much of the intention that backs our own knowledge is borrowed from genes, and the idea that we pass it along to machines is not a metaphor, it is continuous with the process that lent it to us. 

This framing of decision as an aspect of free will is a false dichotomy that not even our language buys into. A gut wound can decide a battle, and that is not a separate meaning of the word. The difference between the two senses is nothing but manners. The latter just takes a warm, fuzzy wrapper off the word that consists entirely of human insistence on being special. If your machine is part of a social context which causes those around it to expect its output to be acted upon or otherwise have effects, then that machine, like the gut wound above, makes decisions, whether you like the verbiage or not. "Jane decided to have coffee instead of tea" describes a computational process, whether or not it is about determining the future. So why ask whether it has meaning? It has reference, it shapes expectations, it conveys information. Unless you want to get spooky, that is what it means to have meaning. I don't see how anyone can claim that the process of deciding, the algorithm or construction one goes through to reach a path to action, loses its content in a deterministic world. From an existentialist point of view, determinism is irrelevant to life. Whether or not the future is fixed, we are still responsible for our decisions because we are social animals and responsibilities and choices are the things our social life is made up of. Even in the most reductivist of worlds, we as animals vie for advantages. And we usually succeed by learning to make better choices than other animals. Whether or not that activity is ultimately mechanical does not stop it from happening, or remove either the subjective intention that bears it forward or the objective effects choices have on ourselves, the other animals, and the world. If you remove the concept of making decisions, above a certain evolutionary level, you lose the ability to explain or interpret animal behavior. You simply can't say what genes do. So it must retain meaning, or we have to abandon biology as a science (not to mention the social sciences.) 

A great deal of the theory of mind is based upon supposedly obvious observations about our mental states. E.g. that they are immediately available, or that they are introspectable, at least in theory. It seems obvious that the functions of the nervous system that ultimately control truly autonomous functions like peristalsis are not aspects of one's mental state. But someone recently suggested that the unconscious adaptations that keep one from stepping on rocks at the beach should be such. These seem to stake out a vague borderland in the middle of which falls something like breathing, as something we can control at will, but usually don't, never learned or decided much about, and almost never notice. When we wait "with 'bated breath", we have made an unconscious decision not to breathe right then. But is that an element of our 'mental state'? I can't say I could consider any of these processes even close to 'mental'. Mental-ness seems to involve will and decisions, but only ones that eventually become conscious. Still, I cannot say on what basis I would place those constraints, or where my favored position actually comes from. What, if anything, have various schools of thought put forward as a 'litmus test' for whether something is or is not a mental state, and what are their respective justifications for their chosen definitions? Is there a useful continuum of 'mental-ness' that clarifies or points up the sticking points between different basic positions or theories?