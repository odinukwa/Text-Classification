Each of those x's that these existential quantifiers supply is a thing, a something, an object. Depending on the context, these objects can even be abstract (like numbers, predicates, points in 2D space, etc.). But the negated existential quantifiers ("there exists no x" = "it's not the case that there exists an x") themselves are not, at least in these particular sentences, being talked about, so they're not even treated as objects like those x's are. In conclusion, I'd like to very briefly de-mystify some of the superficially problematic things you said: 

This can happen in worlds where either only one thing exists or everything is identical to every other thing. In either case, the following sentence captures the truth-conditions of this claim: ∃x ∀y (x = y). For corrections/suggestions/improvements: please leave a comment or simply edit the post. 

The function prime, which is the referent of 'is prime' or 'is a prime number' is defined only for natural numbers and according to the known rule, maps natural numbers to true or false, depending on their primality. Consider the expression: 

His incompleteness theorems are about elementary arithmetic, which seems to me to be independent of the particular modal logic implicit in various versions of his proof. You don't need arithmetic to do modal logic, and you don't need quantification over possible worlds to do arithmetic. The two can be developed simultaneously, of course. If that were the case, then if the arithmetic were sufficiently expressive, the hybrid system would also suffer from incompleteness. But even then, no harm would be done to the ontological proof. Consider an analogous case. Let's say someone presents a proof that number k is prime. What application can Goedel's incompleteness have in this context? Arithmetic is incomplete, fine. But does it tell us anything about this particular proof of the primality of k? Whether k is prime or not, is independent of the fact that arithmetic is incomplete. The same is true of the proof of God case. 

The argument from (1) and (2) to (3) is valid iff [(1) ∧ (2)] → (3) is a logical truth. But that's not the case, because there is at least one possible world (which is, in fact, the actual world) where (1) and (2) are satisfied but (3) is not. Had the relation been an identity (=) instead of a conditional (→), we'd have a valid argument because identity is symmetrical. The conditional is transitive, but not symmetrical. 

Proof. Combine proofs of Fact (2a) and (2b).                                                                                          ■                                                                    References van Benthem, J. (2010) Modal Logic for Open Minds, Stanford, CSLI Lecture Notes #199; §9.1. Holliday, W.H. (2012) Modal Reasoning, Lecture Course (Spring), UC Berkeley; Lecture 5. Pacuit, E. (2009) "Notes on Modal Logic"; §3.1. 

The addition to our usual setup here is the actuality operator A, which intuitively tells us whether a formula is true in the world of utterance; to see how it works pay attention to the semantics below. 

Informally, this means that n-ary relation is a set of n-tuples of elements selected from a bunch of sets. A special type of relation used often in introductory logic courses is that of the predicate, which is simply a 1-ary or unary relation over the domain of individuals. In single-sorted logics such as classical first-order logic, the domain of individuals is some set D, so we have a simpler definition of relations: 

I'm not sure if that's what you're looking for. I have to admit that the question wasn't very clear. 

This is pretty much the same as the one above. You can go with the one that's closest to the system you use. I didn't justify the steps because, as has been pointed out, we don't know what rules you can use. 

Since → is defined in terms of disjunction & negation, that will be the case just in case ¬(P ∧ P′ ∧ ...) ∨ C is a logical truth. Now that will be the case iff an arbitrary interpretation M either does not satisfy all of (P ∧ P′ ∧ ...) or satisfies C. This is a very strong definition, because establishing the validity of an argument requires that we establish the logical truth of the corresponding conditional, not simply its truth. To do so, we would have to start by saying something like "let M be an arbitrary interpretation of" whatever the language we're working with "such that M satisfies P, P′, P′′, ...". The goal would then be to show that M also satisfies C. If we manage to show that M satisfies C, since we assumed nothing about the nature of M, it would mean that all interpretations that satisfy the premises satisfy C. Contrast that with the strange one: 

The 'modern conception' that Kenny is referring to is the one inherited from mathematics, where sentences don't include such locutions as 'I', 'now', 'yesterday' and so on, and thus the need to make sentences temporally unstable (to use Rescher's phrase) doesn't arise. The tenseless view of propositions is still prevalent in the mathematical practice, but it seems a little too strong to claim that it's the modern conception of the semantics of propositions. The alternatives are out there, and philosophers of time, logicians interested in the temporal modalities, and others in linguistics and computer science departments are debating the pros and cons of each view. So yes, there is more than one modern conception of the semantics of propositions. Further Reading. I would recommend reading the aforementioned SEP article on Time in its entirety. If you're interested in a more formal treatment you can look at van Benthem's Modal Logic for Open Minds, chapter 18, and/or Goldblatt's Logics of Time and Computation, chapter 6, both from CSLI. 

Needless to say, my explanations (1–5) depend crucially on the (Translations), the (Definition), and what I consider to be the standard semantics of the usual connectives. To figure out the answer to your homework, you need to make sure that the (Translations) make sense and the (Definition) captures the meaning of truth-functionality as defined by your teacher. If not clear about anything, leave a comment. 

For these things, the same definition of what makes the thing an animal is used, so they're said to be synonymous. These definitions have certain arbitrariness to them, but the bigger problem here is that: 

Neither 'house' nor 'shed' nor 'and' refer to any physical things. Your house (that is, the referent of the expression "my house" as uttered by you; if you have a unique house) is a physical thing, which may have a shed in it, which is also a physical thing. But the expression/term 'house' refers to an abstract entity, which we take to be the plurality/class/set of all houses. Similarly for 'shed', 'dog', and so on. A particular house, a particular shed, a particular dog, may be referred to by 'house', 'shed', and 'dog', but only if the context of utterance is such that it unambiguously determines the connection between the general term and the particular individual in the domain of discourse. That much about terms.                                                 §1 And1 : Sentence × Sentence → Sentence The word 'and' is usually introduced as a sentential connective: a function on the sentences of some underlying language. E.g. "p and q" is taken to be the sentence r which is true just in case p and q are true. This means that 'and' can be taken to be referring, with respect to some language L, to the set consisting of all triples (P, Q, R) s.t. R is true iff P and Q both are. From the category-theoretic point of view this explication of functions is certainly problematic, but that's an issue for another occasion. The conclusion then is that 'and', like 'house' and 'shed', both refer to sets of entities: in the case of 'house' and 'shed' we're dealing with a set of physical entities; in the case of 'and' we're dealing with a set of linguistic and thus abstract entities. I.e., those terms are similar in that they both refer to abstract entities, but different in that those abstract entities they refer to contain entities of different kinds.                                                        §2 And2 : Term × Term → Term It's worth noting that in some logico-mathematical settings 'and' is overloaded: 'and1' is the usual and that connects two sentences, and 'and2' is a function between not sentences but terms. For example, "a and b" is taken to be the plurality that consists of individual a and individual b. That's the logical explication of the grouping idea Mozibur mentioned. To illustrate my earlier point about 'and' referring to an object, let's look at the referent of 'and2'. It's a function from a pair of terms to another term, so set-theoretically, 'and2' refers to the set of triples (a, b, c) s.t. c is the unique term s.t. for all properties φ, we have φ(c) iff φ(a) and φ(b). For example, if a is a red rose and b is a red lobster, then 'a and b' is their 'group term' of which we can truly predicate 'is red'. That's the general, informal, idea of this second, grouping sense of 'and'. If I find any appropriate readings on that I'll append to this post later.