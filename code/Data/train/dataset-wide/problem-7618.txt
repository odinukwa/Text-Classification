so if these conditions fail we define $\Delta(x)=\infty$. Then we obtain a function $d$ with the above properties by $$ d(x) < r \quad\mbox{if}\quad \exists y s. \Delta(y) < s \land d(x,y) < r-s. $$ These ideas are developed in my draft paper Overt subspaces of $R^n$. I claim that a $\lozenge$ operator gives evidence of the existence of the solution of a problem, the solutions themselves being its accumulation points. The results that Andrej mentioned are in my (published) paper A Lambda Calculus for Real Analysis, motivated by the Intermediate Value Theorem. (Andrej played a major part in the existence and writing of this paper.) PS in answer to the comments below. In the classical use of set theory in mathematics, subsets and predicates are interchangeable. In this case you may replace my $\lozenge U$ throughout by $U\in{\mathcal M}$, where $\mathcal M$ is a family of open subspaces that is closed under joins. However, besides its being a clumsy notation, if you think of an overt subset as a set of sets of points you are deliberately going in the wrong direction to understand this idea. My contention is that an overt subspace consists of the discoverable solutions of a problem. In the example the problem is to invert a differentiable function, but I believe these ideas are much more widely applicable in analysis. Such a problem is expressed syntactically and the discovery of a solution is computation. Set theory (speaking of the set of all solutions) at best puts the cart before the horse but actually contributes nothing to solving the problem. To call set theory "classical" mathematics is a perverse use of the word compared to its meanings in other disciplines, which would naturally describe the Newton-Raphson algorithm as classical analysis. My claim is that we start with the formula in analysis expressing the problem, turn it into a formula in logic (predicate) that defines the overt subspace and then potentially use methods of computational logic to extract (successive approximations to) a solution. Of course there is no magic wand and in practice finding accumulation points of overt subspaces on ${\mathbb R}^n$ will require methods that are equivalent to those that are already used in numerical analysis. What we have gained is a conceptual division of the process into a pure mathematical part (a new concept in general topology that is dual to and usable like compactness) and a computational part. 

There are many different things that Martin may have had in mind in asking this question, but the obvious answer requires less category theory and not more. The basic operation is ${-}\to S$ for some fixed "dualising" object in some cartesian closed (or more generally monoidal closed) category. This operation is part of a monad, of which Martin's natural map is the unit. 

You're not going to learn much about the conceptual or categorical structure of the ordinals from their classical presentation, not just because Excluded Middle is needed at every stage but because of normalformitis: the systematic elimination of structure. That being said, there is a paper by Peter Johnstone, A topos-theorist looks at dilators (JPAA, 58, 1989) that applied Yves Diers' extensive work on categories with multiple-valued adjoints to Jean-Yves Girard's $\Pi^1_2$ Logic: Part I, Dilators (Ann. Math. Logic 21, 1981). This studied ordinal-building constructions considered as functors that preserve pullbacks. You cite Andre Joyal. He wrote a book with Ieke Moerdijk called Algebraic Set Theory (CUP 1995) and the subject was taken up by others and presented on Steve Awodey's website. They characterised the universes of sets ($\in$-strutures in this context) and ordinals as free algebras for arbitrary unions and a unary operation $s$ (singleton or successor) subject to various equations: 

MathOverflow is a valuable resource because it is interdisciplinary. Some of the people who have written about this question come from a traditional pure mathematical background where classical logic is the norm, whilst others are based in computer science departments and generally use intuitionistic logic(s). Something that even those of us in the latter category perhaps do not credit enough is that some quite substantial "heterodox" mathematics has been done in the past thirty years. This means, for example, that Dan Pipponi's answer presupposes some quite sophisticated ideas. I suspect that the thinking behind this question is that it is sometimes said that classical mathematicians ought to be willing to acknowledge intuitionistic mathematics in the same way that they do non-commutative group or ring theory. I agree with this professionally, but I do not think that there is a significant mathematical analogy to be made. If we are looking for analogies in ring theory to the classical/intuitionistic distinction, I think a better one would be the passage from $\mathbb Z$ to (commutative) integral domains in which ideals need not be principal. The development needs to be rewritten, but for the most part this is a matter of "cleaning things up" rather than doing a completely different and vastly more difficult thing like non-commutative ring theory. Unfortunately, rather a lot of the literature is made "dirty" with Excluded Middle, so there is a Herculean task to clean it up. Treating logic in terms of (Boolean or Heyting) algebra is in most circumstances misleading. At primary school we evaluated arithmetic expressions from the inside out, but then we learned to manipulate ones with indeterminate values. Similarly, logic is not about things that "are either true or false" but which instead may perhaps be deducible from one another. Here I am simply making an observation about what mathematicians actually do, even classical ones. The deduction operation that is at issue is Excluded Middle. Essentially, Excluded Middle is like the fear of water. If your parents take you swimming as a baby, maybe before you can walk, then you do not develop the fear of water and learn to swim entirely naturally. Similarly, if your teachers do not constantly indoctrinate you by beginning every proof in their lectures with "suppose not" then you will naturally grow up to be a constructive mathematician. It is only difficult because you have been told to think it is. It is common to see arguments that use contradiction quite gratuitously. They are much more complicated because, instead of proving $C\Rightarrow D\Rightarrow E$, they prove $\lnot E\Rightarrow\lnot D\Rightarrow\lnot C$, so the argument is back-to-front. When some parts of a proof that is naturally $$ A\Rightarrow B\Rightarrow C\Rightarrow D\Rightarrow E\Rightarrow F\Rightarrow G$$ are written forwards and others backwards, it turns into spaghetti: $$ A\Rightarrow B\Rightarrow C,\qquad \lnot E\Rightarrow\lnot D\Rightarrow\lnot C\qquad E\Rightarrow F\Rightarrow G. $$ In fact, as Dan has said, there is a lot of work in theoretical computer science based on the idea that the double negation rule is like a "computational effect" (such as exceptions and gotos) in programming. Unless used vary skillfully, such effects make programs next to impossible to understand. On the other hand, there is considerable skill (that classical mathematicians refuse to acknowledge) in pulling a classical proof apart, teasing out its underlying concepts and creating a new constructive proof. I would, for example, strongly recommend Constructive Analysis by Errett Bishop and Douglas Bridges, which gets on with proving the theorems without dwelling on the counterexamples. The reason why some of us regard intuitionistic logic as fundamental and classical logic as an aberration lies in the following analogy (often called an "isomorphism") that was made by Haskell Curry in the 1930s and spelt out by William Howard in the 1960s. This analogy nowadays probably forms the basis of the masters' logic course in any computer science department. 

Here is an application of a similar but simpler idea. By allowing infinite distances the function does not have to be surjective. For any continuous function $f:X\to Y$ and points $x\in X$ and $y\in Y$, let $$ d_y(x) = \inf \{ d(x,x') | f(x')=y \}. $$ Then $d_y(x)$ is a continuous function of two variables iff $f$ is an open map. This result belongs in undergraduate courses on metric spaces and topology, but whenever I have asked, nobody has seen it before. In my draft paper on Overt Subspaces of ${\mathbb R}^n$ I took this result as the starting point to try to explain overtness (which is often said to be invisible in classical topology) to the ordinary mathematician. In particular, I point out the similarity with the Newton-Raphson algorithm for finding zeroes of continuously differentiable functions. 

By chance, this issue came up recently following an internal seminar by Martin Escardo in Birmingham (where I am now an Honorary Research Fellow). He was developing the foundations of arithmetic (in the setting of Homotopy Type Theory, though this was not essential) in such a way that $3\times 5=5\times 3$ could be seen in a primary-school fashion as transposing a rectangle. He based this on a function $F:{\mathbb{N}}\to{\mathsf{Set}}$ with $F0=\emptyset$ and $F(\mathsf{succ} n)=F(n)\coprod{\mathbf{1}}$. In his treatment the most difficult Proposition is $$ F(n)\cong F(m) \Longrightarrow n=m, $$ which he deduced from the Lemma $$ X\coprod{\mathbf{1}}\cong Y\coprod{\mathbf{1}} \Longrightarrow X \cong Y. $$ This Lemma holds in any lextensive category, i.e. one with finite limits and stable disjoint coproducts. The Proposition follows using Peano induction, since then $$ F(n+1)\cong F(m+1) \Longrightarrow F(n)\cong F(m) \Longrightarrow n=m \Longrightarrow n+1=m+1. $$ I think it is reasonable to suppose that Bo Peep could formulate this Lemma, but I feel it is more plausible that she would use the "infinite descent" argument than the Proposition. I am not sure whether this answers the original question about justifying "the use of categories, or isomorphisms or equivalences", although maybe Martin's treatment of arithmetic does so. 

Whilst this might perhaps be seen as begging the question, it is difficult to see how one treat the other two connectives otherwise than: 

The Pleasures of Counting by Tom (T W) Körner, Cambridge University Press, 1996, ISBN13: 9780521568234. Forget the IMO. This book is about the relevance of mathematics to the real world. Each chapter contains a piece of history (unfortunately often from wartime) in which mathematics provided a solution to a real problem. This is then followed by a development of the relevant mathematical ideas. 

These conditions are all weaker forms of saying that the function is an open map. Any continuous function $f:X\to Y$ between compact Hausdorff spaces is proper: the inverse image $Z=f^{-1}(0)\subset X$ of $0\in Y$ is compact (albeit possibly empty). If $f:X\to Y$ is also an open map then $Z$ is overt too. I'll come back to that word in a moment. When $f$ is an open map between compact Hausdorff spaces and $Z$ is nonempty, there is a compact subspace $K\subset X$ and an open one $V\subset Y$ with $0\in V$ and $V\subset f(K)$. So for real manifolds we might think of $K$ is a (filled-in) ball and $f(K)\setminus V$ as the non-zero values that $f$ takes on the enclosing sphere. Could I have forgotten that the original question was about computability? No, that's exactly what I'm getting at. In ${\bf R}^1$ an enclosing sphere is a straddling interval, $[d,u]$ such that $f(d) < 0 < f(u)$ or $f(d) > 0 > f(u)$. The interval-halving (or, I suspect, any computational) algorithm generates a convergent sequence of straddling intervals. More abstractly, write $\lozenge U$ if the open subset $U$ contains a straddling interval. The interval-halving algorithm (known historically as the Bolzano--Weierstrass theorem or lion hunting) depends exactly on the property that $\lozenge$ takes unions to disjunctions, and in particular $$ \lozenge(U\cup V) \Longrightarrow \lozenge U \lor \lozenge V. $$ (Compare this with the Brouwer degree, which takes disjoint unions to sums of integers.) I claim, therefore, that the formulation of the constructive IVT should be the identification of suitable conditions (more than continuity but less than openness) on $f$ in order to prove the above property of $\lozenge$. Alternatively, instead of restricting the function $f$, we could restrict the open subsets $U$ and $V$. This is what the argument at the end of Section 14 of my paper does. This gives a factorisation $f=g\cdot p$ of any continuous function $f:{\bf R}\to{\bf R}$ into a proper surjection $p$ with compact connected fibres and a non-hovering map $g$. To a classical mathematician, $p$ is obviously surjective in the pointwise sense, whereas this is precisely the situation that a constructivist finds unacceptable. Meanwhile, they agree on finding zeroes of $g$. In fact, this process finds interval-valued zeroes of any continuous function that takes opposite signs, which was the common sense answer to the question in the first place. The operator $\lozenge$ defines an overt subspace, but I'll leave you to read the paper to find out what that means.