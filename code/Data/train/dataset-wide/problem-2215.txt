This has been asked before on SO. Where you have common and very ambiguous names, then prefix with table name. That is, anything you're liable to have to alias in almost every query. So for an Employee table I'd have 

As far as I know, you'd have to look using SQL Server Profiler, the event Esclation information isn't in sys.dm_tran_locks or it's predecessors 

There is a whitepaper too. See the section "Maintaining Statistics in SQL Server 2008" where there are some conditions that sound like affect you. Example: 

There is usually no need to add lock hints in day to day code. As to why you (apparently) have a table lock, this can be caused by no index on ID. The table is being locked because all rows have to be looked at. 

Following on from Andrew Bickerton's answer... The 4th way is to use EXCEPT where the RDBMS supports it This should give the same execution plan as NOT EXISTS 

You probably need trace flag 1118 See Paul Randal's myths about tempdb first, and his TF 1118 article too The TF is described here in KB 328551 I have no direct experience of this but it sounds like what I've read 

2. Separate rows Or have a languageID column to support several languages.This has the drawback that collation will be fixed for all languages which means poor sorting/filtering PageParent 

Yes, it is a bad idea. Log backups relate to a full/diff backup at some point (and sequentially to each other). Any change in recovery model will invalidate the restore sequence. So for this sequence you can only restore as far as step 2. Log backups from step 4 on are useless. 

To backup up logins, you'd backup the master database. For any jobs etc, it's be msdb I assume you want to avoid some complexity by not having 96 log backups per day every 15 minutes. Your approach to of hourly full backups will work, but I'd consider doing hourly or 30 minute differentials with a daily full backups. The combination of full/differential: 

No. There is no flag or metadata about "UsesDynamicSQL" You have to search the definition... There are 2 ways to execute dynamic SQL 

I wouldn't bother testing status of the triggers, personally. A disabled trigger should not exist IMHO. Not least because OBJECTPROPERTY does not support it in SQL Server 2000 Edit: My mistake, OBJECTPROPERTY in SQL Server 2000 has ExecIsTriggerDisabled as per comment. So you can track status before and after 

Your NOT EXISTS is more efficient in most cases. LEFT JOIN internally matches all rows then filters to IS NULL. NOT EXISTS doesn't. This row multiplication also happens in all JOIN based code so you may need an extra aggregate (DISTINCT) to fix the output NOT IN is generally wrong because NULLs cause no match. You can also use EXCEPT which gives the same plan as NOT EXISTS. 

An ORM requires information about the first write (SCOPE_IDENTITY or such) to complete the second write. This means 2 (with an OUTPUT clause) or 3 database (with SELECT SCOPE_IDENTITY) calls in general in a client side transaction. No amount of tinkering with isolation levels will eliminate these 2 or 3 calls. If you want more performance, then the best concept stored procedure to do an atomic write (in a transaction) to both tables in one database call. This means 50% or 66% reduction in database calls Going further, you have to decide what "superfast" really means For example, should you be using SSDs for your transaction log files: the drive hosting the log files determines overall write speed because of write ahead logging. Then there the design: are you using IDENTITY columns or GUIDs or Hibernate style nvarchar keys? Has the ORM designed the database for you? 

Because the return values can be large or a decimal number. Double precision will accept a wide range of values Looking at other numeric types, you only have decimal which will have overhead: you don't know the return scale or precision needed beforehand so it would have to wide 

This means that some connection string has either a non-existent database mentioned, or the login used does not have access. It is also possible that no database is mentioned, and the default database for the login is missing/no permission. One of these 3 options is usually the answer... Edit: you don't have auto-close on the database, do you? 

No, you don't need to gave the columns in the same order. Not least, table order may not reflect actual on-disk order (this is 100% true for SQL Server, and I'm sure MySQL is the same) Unless your OCD itch needs scratched 

An value will be stored "in-row" if it is short enough. The default behaviour can be modified using sp_tableoption, "large value types out of row" option. I wouldn't bother. The DB engine will manage this efficiently by itself. As for design, there are several ways of doing this based on your model: 

Short answer: No Longer... SQL Server doesn't know that it is SSRS sending it a query. So the query from SSRS will run like any other query. It's more likely be that the query optimiser decides to use a table lock for the SSRS query. of course, it could be a different problem, but that's a different question 

The principle is "least privilege" and "need to know": do developers pass this test?Especially when Auditors or Sarbannes-Oxley come knocking. Then, my next assumption: developers are stupid. So if they do need say for 3rd line support, who then needs it? Web monkeys typically don't but database types yes if they are expected to support it. Then, is access needed permanently? They can have "break glass" access using a SQL login or alternate Windows account that requires a sign off. In our case, it was the data owner (some tech savvy business person hopefully) and the IT manager to approve it. I have seen developers test against or run queries on production and take it down because of ignorance. Saying that, developers should take responsibility for their actions: if they do take a server down, they should suffer accordingly. I got someone demoted after one incident... These assume a reasonably size shop of course. The more hats folk wear the less separation of duties you can have Also, is there an environment that developers can run queries against recent data? In my last shop, prod was restored each night to a test server to provide this. 

So nothing of this really worked. Could you think of an alternative how to keep the trigger doing its job but also have the data collector work properly? Resources Here's the original sourcecode of the database level trigger: 

I admit that this solution is not pretty and presumably it won't work if another dynamic sql comes around with another resultset...however it works for me now. Martin 

Then I join it to my table and add a rownumber based on the priority: So here's the output of my subselect (or derived query) named "res": 

By the way: Having a time as datatype varchar(10) sounds a little bit odd. Datetime would seem to be more fitting. Have a look here for advice. 

First of all thank you guys for helping me to get on track with your comments. I have now worked through an example and have a better understanding what's going on. The problem arises with moving LOB-Data (such as VARCHAR(MAX), XML and so on) to another filegroup. When you rebuild a clustered index on another filegroup the LOB-Data stays at it's former place (set by the command in the CREATE TABLE statement). One classic way to move the LOB-Data is to create a second table with the same structure in the new filegroup, copy data over, drop the old table and rename the new one. However this brings in all sorts of possible issues like lost data, invalidated data (because of missing check constraints) and error handling is quite tough. I have done this for one table in the past but IMHO it doesn't scale well and consider the nightmare of having to transfer 100 tables and you got errors for table 15, 33, 88 and 99 to fix. Therefore I use a well-known trick regarding partitioning: As described by Kimberly Tripp LOB-Data does move to the new filegroup when you put partitioning on it. As I do not plan to use partitioning in the long run but just as a helper for moving that LOBs, the partition scheme is quite dull (throwing all partitions into one filegroup): I don't even care, which partition the data is on as I just want to get them moved. Actually this technique and the implementation is not invented by myself...I use a formidable script by Mark White. My mistake was to not fully understand what this script does and what the implications are....which I have now: For LOB-Data it is necessary to rebuild (or recreate) the table (mostly the clustered index) twice: first with putting partitioning on it and second with removing the partitioning. Whether you use or not this results in having to provide the space of the original data TWICE: if your original table has 100MB, you need to provide 200MB for the operation to succeed. At the beginning I was quite puzzled, ending up with my new data files which had a lot of free space after the operation was finished. Now I accepted that I can't cheat around avoiding the free space. However I could avoid the necessity to shrink files afterwards. Therefore my solution is to do the first rebuild on a temporary filegroup and the second rebuild (removing partioning) on the destination filegroup. The temporary filegroup can be removed afterwards (if hopefully I don't hit the error message "The filegroup cannot be removed" (have a look at my question here) anymore. Thanks for reading and your help Martin Here is a repro script for my problem which includes the solution I have come up for it: 

Eventually I have found a solution to my problem. As on a StackOverflow question described your can hack you way around the problem using . So I added these two lines to my code (within the IF clause and before doing other things) and got things to work: 

I have still got all the rows but assigned a rownumber based on my priority. Then the last thing I have to do is to filter only the most relevant rows which are the one having row number 1 . Presto...here's the result of the whole query: 

The problem I have got an issue setting up Management Data Warehouse on SQL Server 2017 CU 5. The job "collection_set_1_noncached_collect_and_upload" keeps failing. It is related to the "Disk Usage" collection set. Error Messages are the following (I highlighted the part which is most relevant IMHO): 

The procedure "AUDIT_TO_OTHER_FG" is a database level trigger. Its purpose is to put audit tables (with history data) into another filegroup. Our Java Application running on top of the database is using Hibernate and doesn't bother specifying filegroups. However all of these audit tables follow a certain naming convention. Thus the trigger fires at a CREATE_TABLE event, rolls back the table creation and creates the table again on a different filegroup. Maybe this is not the most elegant version to put the tables onto a different than the default filegroup...however it has worked fine in the past and has never been a problem until now. I had Management Data Warehouse data collectors set up before for that environment as it was running on SQL Server 2008. There haven't been any problem regarding these triggers in that version. Recently we moved to SQL Server 2017 and now I am experiencing these issues. I dropped the trigger temporarily and the data collector worked fine. So somehow it appears to be interfering with the actions of the data collector and the problem is the dynamic SQL used. However I do not get why this causes a problem as the data collector seems not to create any table in my user databases and the trigger doesn't fire while the data collector is run. Workarounds tried I have read a bit into "WITH RESULT SETS" and tried to change my trigger as follows: