Update: The comments seem to confirm the conflation of determinism with fatalism, so lets dig in here. As noted in the comments, when considering Nostradamus we are adding an assumption that the oracle is possible. Determinism versus Fatalism Let's get to basic definitions. Determinism can be defined as the premise that every effect is the result of antecedent causes. Let's call the presumption of determinism (P1). Now let's swap Nostradamus out for "some guy", call him Ralph. I now want to build a prediction-spiter; let's say it's the non-conscious "Alexa" device. I want to program Alexa in such a way that if Ralph says 3, Alexa says 4. If Ralphs says 1, 2, 4, 5, 6, 7, 8, 9, or 10, Alexa says 3. Now this is what I want to do, but I can't necessarily do that by assuming determinism. So I need another premise. Let's presume it's possible to program Alexa this way. Call that (P2). Now let's explore what happens. If we take them as Ralph-says-x/Alexa-says-y tuples, the possibilities form the exhaustive set: 

I think you're confusing determinism with fatalism. Suppose I'm in a room with an oracle (call him Nostradamus), and we play a simple game. I pick a number from 1 to 10. But before I do, Nostradamus predicts what I will pick. Now since it helps define the difference, suppose Nostradamus actually tells me what his prediction is; in particular, suppose he says I pick 3. If fatalism is true, then I basically must pick 3; the oracle predicted it, after all, so it cannot be avoided. But if determinism is true, I can easily pick 4. This is actually easy to do. I just pick 3 if Nostradamus says I will pick any number but 3. But I pick 4 if he says I will pick 3. All we need for me to have this strange capability is determinism; I don't need free will, and indeterminism actually hampers this ability; the trick to spiting Nostradamus's prediction is to react to what his prediction is. I don't even need to be conscious; we could program Alexa to fill my role if you like. Fatalism makes the predictions pointless, but determinism doesn't. If you can avoid a bad outcome by predicting it would happen, then reacting to the prediction, you can gain a survival advantage. Having this ability requires neither free will, nor indeterminism, nor even consciousness; simply the ability to model and react to the model. Robots can duck if rocks are thrown at them. Self driving cars can apply breaks if they predict otherwise that they will collide with an object. Nothing mysterious is required for "evitability". So if consciousness were mechanistic, and it in part played a role of avoiding bad outcomes by predicting them and then avoiding them, it could be a survival advantage; and such would not require anything non-deterministic. Simply modeling and reacting to the model suffices. But let's suppose consciousness is not mechanistic; instead, let's presume it's an epiphenomenon; in particular: 

That observation is meaningless. For whatever reason, a large number of people cheat. If you actually used the fact that a large number of people who are against public recording cheat to conclude something, then you're doing something wrong. To just establish a correlation between cheaters and anti-public recorders, you need to have a comparative analysis across all groups (including pro-public recorders); i.e., you cannot ignore the base rates. 

Let A be that a person cheats. Let B be that the person is against public recording. Then so far, you had been arguing that A=>B. But here, you're concluding from this that if B, then A. In other words, you are arguing: A=>B, B, therefore A. That is affirming the consequent. As it turns out, the most general concern here isn't merely being exposed for doing something wrong. It's any situation where the revelation of information to some particular party can cause a harm. We're not just talking about people being guilty, we're talking about preventing your friend's abusive ex from learning where your friend lives; keeping despotic anti-butter governments from learning where the butter factories are; keeping mobs of butter-on-the-other-side folk from attacking a butter-on-the-same-side proponent. Keeping a thief from learning mothers' maiden names. Or perhaps, being exposed to a bully for being homosexual, or to rude coworkers for your culinary habits? 

Then the answer is simple. It's of no help; since consciousness is causally impotent, it plays no role in our survival. Counterintuitively, however, this only means that consciousness does not help. It does not necessarily mean that having consciousness does not help. In particular, it could very well be the case that there are particular sorts of mechanisms that, if we had them, would grant us a big survival advantage. It could also be that those mechanisms just so happened to be such that, were they in play, they would somehow result in an epiphenomenal consciousness. Were this the case, having consciousness by means of having those mechanisms would mean we have those mechanisms. In other words, having consciousness could still correlate to having a survival advantage (because a mechanism granting us such could be a "confounding variable"), even if it does not cause it. 

I think this description by the mainstream (I would say "pop") is naive. In particular, there are two major problems with it: 

The reason that you seem to be stuck in a body (which we might call the "subjective justification" for this conclusion) is worth pointing out to help ground the issue. Definitions Since we're adult humans, and especially since it underlies our ability to discuss things in the first place, I'm going to presume we share Theory of Mind (ToM). Breaking this down, subjectively speaking, we appear to have a particular first person view (FPV); I see what I see and you see what I see, but per ToM I don't see what you see and you don't see what I see. From a broader perspective this FPV covers not only sensation but thoughts and planned actions. The fact that I have my subjective FPV, and through ToM project that you have a unique FPV and another individual (say "Joe") has yet another FPV, suggests that there's a separation between my mind, your mind, and Joe's mind; that is, if we get all three "social persons" (living bodies) in a room, then we have as many minds as there are bodies. Let's call this principle "mind separability" (MS). Now, sensations, thoughts, and actions only cover the "reasonable present". I see my monitor; I am in the process of thinking and typing this sentence, but after about a day I will no longer see this monitor in its current state (that is, tomorrow I won't see what I see today), and I won't experience typing this sentence. But there is another analogous experience that extends into the past... the experience of memory. In particular, the memories I have seem to be about having had a particular FPV perspective. And just as you, myself, and Joe seem to have three different distinct minds, we also not only have memories, but have memories of having distinct minds, and furthermore of having particularly distinct minds associated with those three social persons (living bodies). Let's call this principle "recalled mind separability" (RMS). Finally, for completion, it's worth pointing out that however far back those memories go, they still seem to share the property that the object of their memories is about having the particular distinct minds associated with our social persons. That is, there's nothing qualitatively different between remembering what I did yesterday and remembering what I did two days ago, in contrast with the qualitative difference between experiencing something right now and remembering something I did yesterday. So let's call this principle "memory permanence" (MP). Getting back to the subject at hand, MS is what gives us the impression that we're different minds in the first place. RMS and MP give us the impression that we have what I'll term precisely as identities; that we as social persons are individual entities that remain across time. MS, RMS, and MP together also give us the impression that we tend to "aggregate" our experiences across time... that is, there's a "present" where we as individuals have experiences, those experiences seem to be stored in memories, and those memories seem to be "held". (For the record, MS, RMS, and MP are terms I'm inventing for this particular discussion; ToM of course is a standard term). Implications Viewed from this perspective, our subjective experience of being a separate identity flowing through time (which is presumed by your questions) can be said to be derived subjective theories based mainly from RMS and MP. RMS and MP are properties of our memories. If we hold a particular modern viewpoint that our memories are a result of brain states, and we also hold the view that our senses are a result of brain states, then essentially it's the state of a particular brain that suggests that we have a POV in the first place, and it's the aggregation of experiences/memories by that brain that's responsible for the RMS and MP that suggests that we're stuck in particular bodies. From a materialist perspective, our brains are the POV; so scenarios suggesting that our POV's swap are impossible unless you either: (a) physically swap the brains, or (b) put the brains into a blender and carefully reconstruct them into complementary states. Other monisms could similarly relate bodies or body equivalents to minds. A dualist perspective, OTOH, can be seen (most generally) as positing that there is "something else" that is responsible for a POV. I don't think in discussing dualism most generally, we can describe it any further; you can literally hold anything from the notion that every POV is constantly destroyed and recreated between experiences and that identity is an illusion to the notion that there's only one universal POV and that MS is an illusion. For this reason I'm going to continue discussing things from a materialist perspective, so you can see what it's like. Subtleties: A materialist view of identity At the beginning of example 1, you posit that we start with identical brain states, but you don't describe how those brain states come to be identical. I'll discuss two scenarios in an attempt to give a better idea of identity. In the first setup, we're going to clone you to make the 100 brains. Let's pick a particular pre-clone time t1 and call you at this time xwd-t1. After the cloning, let's say that xwd-m is in Mexico and xwd-c is in China. Per your own example, xwd-m and xwd-c's experiences will immediately be different; xwd-m only experiences Mexico, xwd-c China. So xwd-m and xwd-c have distinct FPV's; ToM suggests they have distinct minds. (Modern) materialism would suggest their distinct minds is a result of the fact that their minds are a result of their brain state. But both xwd-m and xwd-c share brain states corresponding to "recalled memories" of being xwd-t1; MP suggests they will continue to share these, so they do not feel like they were separate individuals before the cloning. After the cloning, they do. If we take identity itself as emergent, then we would say that xwd-m and xwd-c are different persons, but they were the same person, and this suggests that identity itself doesn't necessarily preserve number (a single past identity xwd-t1 can "become" two identities). But if you can shove a brain in a blender and recreate it in a different configuration, perhaps you can take some blended brain mass and make your own configuration whole cloth. Suppose you do this and make an entirely new individual... Adam, who you happened to make last Thursday. But you build Adam in such a way that he has memories of being Adam last Wednesday (you need not be "deceptive" about it; you can freely admit to Adam that he has false memories if you wish). Now just as xwd-m and xwd-c each have memories of being xwd-t1 at time t1, Adam has memories of being Adam last Wednesday. The difference here is that there actually was an xwd-t1 for xwd-m to claim he was at time t1, but there was no such Adam. By a similar argument you can say that even if xwd-m and xwd-c happened to share the same experience at some time t2 post cloning, they can only really claim to be the particular referent having that experience (that is, you need a causal connection)... for this reason we would say that each brain has its own mind, and that identity is established through memory, and it can be fake or real under ordinary epistemic criteria (the memories must refer to a real entity and be causally related to it for them to be said to be "genuine"). 

I think you're making a mistake here. What you mean by "have" is just enough of a semantic muddle to mess you up. Certainly something has a conscious experience. What you're requiring for the formation of the sentence to be a true statement has a weaker criteria than you're imagining though; it merely requires that the fact that you have a conscious experience play a causal role in forming the sentence in a semantically relevant way. That requirement is not sufficient to establish that the process itself must be a "conscious" one; only that the conscious experience feed information (by causal linkage) to the thing that forms the sentence. The argument that the process doesn't "have" the experience if it's not conscious is unconvincing; if by "have" you mean it does not actually contain the experience, it's irrelevant so long as the act of experiencing can feed information to the unconscious process. If by "have" you mean it doesn't have causal access to the conscious experience then you're begging the question. Either way, you haven't sufficiently argued in a convincing way that the processes involved in forming true statements about your having conscious experiences must be conscious processes. IOW, it's perfectly reasonable to require the statement to be causally related to having the conscious experience. But this says nothing about the process itself by which the sentence is formed being conscious. 

Pé de Leão addressed these comments from a spiritualist perspective. The same view extends to a materialist perspective, which it appears you're taking. An easy way to think of this is to simply start with Pé de Leão's view that the soul is responsible for the will; then simply view the soul as made of flesh. With this view of mechanics, there's no longer need to figure out whether the micro level controls it or the macro level does, as the macro is implemented by the micro. The weighting of probabilities argument doesn't really help you. It's fundamentally incoherent to claim both than an event is caused by an agency and that the event is random. Weighting doesn't solve this; if I am able to weight probabilities such that there's a 99% chance I get chocolate, then the weighting you can pin to me. But this implies that 1 out of 100 times I do this, "vanilla" will come up. In those 1 out of 100 cases, there's no good way to pin the decision of vanilla as opposed to chocolate to my agency. If we want a libertarian model of free will, we still need the agent to be the cause of the outcome. Random mechanics, even weighted, do no good; what's required for libertarian mechanics is that the agent is an "original cause", not random. It must be possible in principle for the agent, just because he's so inclined to, to always pick chocolate; or to always alternate; not merely because it so happens that the random die rolls that way, but because the agent wishes it. The same principle applies to a compatiblist model of free will. Compatibilist free will actually allows random events to happen; they just can't be part of the mechanics of free will. Under a typical compatiblist view, causal actions are the only kinds of actions that can be attributed to will.