Option 1 won't work. Option 2 wont' work until the PTR record is corrected as noted by @Lazy Badger. Follow these steps: 

As noted in my comment, the receiving system's Ethernet card appears to be running at 1/10th is maximum speed. The transfer rate you are achieving is close to the maximum achievable without compression at the current network card speed. Running rsync in daemon mode on one side may provide better performance than tunneling using SSH. There are security implications to running rsync as a daemon. 

The utility should do this for you. Enable the filter to catch calls to non-existent scripts. (It will catch other scripts as well.) has variable ban times and the ability to whitelist IP addresses. As noted in the comments to your request, it is possible to perform DoS attacks. You should monitor what gets blocked. If you are using to monitor your server, you can monitor the number of addresses has blocked with it. 

The rules are cumulative. In some cases, such as this you will have overlapping rules triggered. Bayes 99 to 99.9 Scores 3.5 

Look at the headers of the delayed email. The headers will show where you are being blocked. It is likely the emails are being delayed by a spam/virus filter somewhere along the path. The headers may indicate where the scanning is occurring as well as providing scan results. 

SPF error conditions do not indicate anything about the desired policy. As such they provide no guidance as to whether or not to accept the message. It is possible that the intended policy is . It is normal to accept mail in this case. It appears Google is being lenient with this domains failure to comply with the standard. Even SPF policy rejections () are unreliable when validating sender addresses. There are a number of cases where rejecting such mail would be inappropriate including: 

Study the RFCs and other relevant documentation. I have more information on my blog starting with a rant on Running and Email Server. I have several other EMail articles some of which are specific. 

If you use the option on your NFS mounts, then should be able to kill processes waiting on NFS IO to complete. This may result in file corruption, but likely no more than creates. 

This is a chain that accept the required ICMP types. It was extracted from a Shorewall6 generated firewall. It is accessed with a rule like: 

I would expect to us the resolver library which will use the name service providers listed in the hosts specification in order. If this does not include DNS, DNS resolution will not occur. is not documented to use this file, but from your experience it appears it may. Often this data is cached by a name service caching daemon. If the daemon, is failing you may get this kind of issue even if the other configurations are correct. and are pure DNS lookup programs. They both resolve names only via DNS. They will not resolve names using files or other non-DNS providers. I would expect them to use the information in directly. 

My website and server are unreachable due to routing for the network block 69.70.44.0/24 seems to be routed from Europe to savvis.net. Then routinbg stops at a router identified as trl-pos-0-3-2-0.chicago.savvis.net [204.70.192.101]. I have had no luck contacting either savvis.net or my ISP (Videotron.net). Are there any other options for getting this corrected? Anyone's help would be appreciated. MTR trace extracts (with and without name lookups). Thanks. 

Receiving email from IMAP is independent of the ability to send email which uses a separate SMTP service. Please remove postfix from the tags as it is irrelevant in this case. Dovecot will require an encrypted connection before it will allow a login. You can generate a self-signed certificate and add it as the SSL key and certificate to your Dovecot configuration. I configure Dovecot with both to listen on the IMAP and IMAPS ports. The IMAP port will use StartTLS to begin the encrypted session. Encryption is not required on when connecting from the server where Dovecot is running. You must also specify one or more password stores. Try running to view your configuration. The process I used to get login working was: 

You can check what the resolver is giving for the mail server is to use the command using your mail server in place of mail.example.com. If this isn't giving the right result sendmail won't either. Also try the command to see what mail sever address you are getting from DNS. It should return the internal mail server. It may also be possible the external IP is hard-coded into the sendmail configuration. Check the configuration file for the external address of the mail server. If I remember right it should . Verify your changes to to do this. Check that the hosts line in has files listed before DNS. Restart after making changes to . Then rerun the genent command above. The hosts line should read like: 

DNS traffic is already load balanced (and cached). Adding additional A records will increase the load on all nameservers as they will need to provide the additional addresses. If one of your nameservers is overloaded, but the other isn't look at the server configuration or the network configuration. If one server is unreachable, traffic will move to other causing a load imbalance. Also check your TTL values to ensure servers can cache your data for a reasonable amount of time. In most cases your data should be cachable for hours or days. Reduce the TTL and take the load if an address is changing. Also check to make sure your server is not able to be used for DNS amplification. From the Internet it should only respond to queries for which it is authoritative. This could use your bandwidth up quite quickly without providing the intended service. EDIT: If you want to reduce traffic to your servers, add secondaries on different networks. Several organizations provide secondary DNS services. Traffic gets spread across more servers using round robin scheduling. 

Apache will always respond to properly formatted requests. However, if you setup a default SSL virtual host you can return 404 or other error response to all requests. If the requested domain does not match the supported domains, the response should also trigger and SSL domain mismatch error. 

It may be simpler to enable or use the existing IPv6 stack on the Ubuntu server. If your server is providing DNS services for the client, you may be able to use to provide the IPv6 address of the server to the client. This may be as simple as adding the IPv6 address of the server to the file. The package can be used to build an IPv6 firewall. The package can be uses to provide a list of services available on the server to the OS X client. For local access to the services, this may be the simplest solution. This works well if you have an IPv6 address on the server. (Most likely you do.) To determine if you already have IPv6 address run the command and look for lines starting . EDIT: If you want to enable the client to enable access to the web pages via the server, an IPv6 web proxy like (version 3.1) will work. (This last edit is done using squid3 over IPv6.) This can be made discoverable via Your server can also provide a relay service for outgoing email. For other services there may be proxies available, or you will need to use an IPv6 to IPv4 NAT. From what I have seen development of these providers has not been significant. Google and some other providers are available on IPv6 so you can get limited connectivity to the Internet using IPv6. As most ISPs don't yet support IPv6 you may need to use a tunnel to connect to the Internet. I started with 6to4 tunnel and moved to a 6in4 tunnel. While I implemented my tunnel on OpenWrt, the process is much the same for Ubuntu. It is easier to implement on a server connected directly to your ISP's modem. 

BBC seem to be using is successfully. I believe there is a video on TED discussing what they are doing with it. 

This should block most of the servers hitting you. You can logscape with fail2ban or CSF and block those IPs at the firewall for a few hours. You may want to rather than connections which fail reverse DNS validation. This will give you a chance to whitelist legitimate hosts. Use a caching DNS server with a large cache (several thousand entries) on your mail server. With the load you are experiencing you will be generating several queries for each host that is hitting you. 

SSD might help with log files on a very busy system. but is probably a bad use of resources. Depending on the size and access distribution of your web site, you may find that most of the data is served from buffer cache in memory. In this case disk access statistics are relatively insignificant. 

You can copy the disk images to the LVM logical volumes and provide this as the disk image for the VM. Make sure you disable NTP and NTPDATE on the virtual servers. I converted some old images using Mondo to create bootable recovery images. This allowed me to resize the partitions during reinstall. 

Your mail server should listen on port 587 (submission) for user agents. This port should require STARTTLS and Authentication before accepting a message for delivery. Thunderbird works well with this setup. Thunderbird will try a few subdomains when it auto-configures. For the domain , configure DNS for for the server running postfix and dovecot. (Replace as required for your domain.) This should be an A record. Other names can be used such as , and , but works for most protocols. 

I don't know about creating the directories, but the rest should be handled by filters and dynamic file names. Normally in a case like this I would log file pattern like. 

To have it work for all programs you will need to use a transparent proxy, with firewall rules or DNS configuration to get your traffic to the proxy. I setup a Transparent Squid Proxy using Shorewall to build the firewall rules. It also run a standard proxy on the standard port which is more reliable. To use DNS you will need to limit access to upstream servers. This will require firewall rules. However, it can be done for a single system with an entry in the file. Your proxy will need to be configured to forward traffic as required. In your case it should proxy to a specified address, and pass the rest of the traffic normally. Unless you are sure of the IP address for is fixed and stable, you should likely proxy all traffic. Large sites often use special DNS handling to provide IP addresses to the fastest available server for a particular user. These tend to move over time, sometimes quite frequently. 

There is no DNS records you can create that will do this. You will need to allow time for DNS change to propagate. Setting the TTL down to an hour or so for twice the old TTL (usually a couple days) before the change will speed the propagation significantly. You could proxy the new server from the old server. Until the DNS changes propagate fully you will have some traffic on the old server. You could also look at doing DNAT on the firewall for the old server if all domains moved. 

When testing you may have to define things like in the test script. Then try matching some records extracted from your log file. 

You should define your where you added to your path. You don't need it in your path. This is a sample Compiler.java file. 

Why worry about forwarding e-mail from spammers? Reject or defer any remote messages on connections that aren't authenticated. If you defer, any misguided legitimate server will eventually deliver according to your MX. If it is a spambot, they will usually not retry in either case. You can reject or deny as early as the "MAIL FROM" command. By that time your user's will have authenticated. If you aren't spam filtering messages for authenticated connections, you could configure the spam filter to always reject messages. If you are filtering messages for authenticated connections, you may be able to configure the spam filter to always reject messages on unauthenticated connections. 

They will time out eventually. Before making changes you should reduce your negative TTL to a reasonable amount. The value is specified in your SOA record for the domain. Query your servers for the SOA record to determine how long the timeout might last. Default value is documented as 3 hours, and maximum value is 7 days. As you have found, it is not a good idea to query your local servers for new services before you know they are available on all your authoritative nameservers. Doing so may prime the cache with a negative answer. Query them first to verify. 

Normally, this would be run by cron or as a service depending on how the package for your system configures it. Modern versions use to collect the historical data to files. Older versions ran to do the same. Check the pages starting with . Also there should be documentation in . can be used to collect and display current data even if the historical data is not available. Check to see if there are any files in , If there are then data is being collected. 

HAProxy should already be adding the X-Forwarded-For header. You may want to add protocol and/or port if either of these is non-standard. I usually test this sort of behavior with a page that echos the request headers. That makes it easy to see what headers are available, and what their contents are. It is not unusual for X-Forward-For to contain a list of addresses. This signals that the request has passed through multiple proxies, or someone is spoofing the header. The right most address will be the one that was added by the last proxy (ha-proxy) that handled the request. Some web servers can be configured to log an IP address from a header rather than the connection. This would be useful for your access logs, and a case where you would want to generate a header based on the incoming connections IP address. It is possible to achieve an A+ rating while supporting all the listed browsers except IE 6 Without using two different stacks. 

Proxy configuration is well documented in the Apache HTTP Server documentation. Caching is also documented there. Other servers have similar capabilities. 

I have always made it a practice to keep fixed addresses outside the DHCP range. That should eliminate this issue. DCHP will offer fixed addresses outside its dynamic range. For fixed addresses, it is common to use infinite or at least very long leases. Check the DHCP leases file to see if the server has an active lease. They are normally configured not to use DHCP, and therefore would not have a lease.