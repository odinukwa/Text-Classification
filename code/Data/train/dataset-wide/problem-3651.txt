To limit Internet usage of our users to a reasonable amount since they are using our sister agency's Internet connection when doing general web stuff That we never under any circumstance permit packets to be routed from our primary LAN to the Extranet LAN. 

You can't accomplish your goals with your current setup. Presumably some device is doing the NAT-RDR / Port Forwarding to required to redirect traffic from your external IP address through your NAT device to your server at 192.168.1.101. Your RFC1918 network doesn't know anything about the Internet and conversely the Internet doesn't know anything about your RFC1918 network. Consequently, your link on 192.168.1.101 doesn't have a route to find your other server at 192.168.1.102 once you are outside your network. When you connect to 192.168.1.101 from the outside you're really just connecting with your external IP address and your NAT device does the magic to translate that through to 192.168.1.101 so when you click on the link to 192.168.1.102 your client has no idea where that machine is or how to get there. You will need to either use a separate external address and separate NAT-RDR rule for your server at 192.168.1.102 or use a separate port on the same NAT-RDR and then update your link on 192.168.1.101 appropriately. 

Check the physical layer and the data link layer first. Nine times out of ten, it's a patch cable that someone ran over repeatably with their office chair (by the way, the port errors will show up in SNMP-reported switch statistics... useful see?). Or the moving company parked their truck in front of one our wireless bridges. Look for bad terminations (use a cable tester), out of spec cable runs, and duplex mismatches, or broadcast loops especially if you don't have physical control over all the switching infrastructure. Layer-7: Look for client or server misconfigurations or configurations that are no longer relevant (Wireshark is your friend here). DNS problems. Are network backups running during the day? Or WSUS updates being applied? Etc. Layer-8: And finally there always seems to be someone watching videos via NetFlix (RMON or SFlow will discover this). 

You can connect your virtual machine's COM port to named pipes but apparently not to actual serial ports. Apparently this is primarily a debugging feature. You can provide access for a virtual machine to a serial port using a COM port re-director such as KernelPro's USB over Ethernet. Additionally the software and drivers for the licensing key need to support being installed on Window Server and in our case on Server Core if you want the licensing key to be installed on the host server. I ended up installing the licensing key and software on a workstation and then using it as that site's "licensing server". This adds about ten different things that can now break this software. A software-based licensing key would of saved me much trouble and I suspect be a more reliable solution. 

In my opinion moving the port that an application runs on does not increase security at all - simply for the reason that the same application is running (with the same strengths and weaknesses) just on a different port. If your application has a weakness moving the port that it listens on to a different port doesn't address the weakness. Worse it actively encourages you to NOT address the weakness because now it is not being hammered on constantly by automated scanning. It hides the real problem which is the problem that should actually be solved. Some examples: 

Just that, "this host on this network"... as Lee B's answer states this translates to all available IP addresses on your host. Hosting a service on 0.0.0.0 will automatically host that service on every addressable interface. 127.0.0.1: From RFC5735: 

This is really a question about password re-use. Ostensibly, any password re-use at all is considered a Bad Thing(TM). The mechanism only works because the password is a secret known only to you... and the more frequently you use the secret the less likely it is to remain a secret. However in the real world we have to re-use passwords, either by literally re-using the same password in multiple accounts or by using one master password to secure a list of unique passwords. The answer to this question really depends on your environment and threat model. As @boris quiroz pointed out, what kind of server is? Who needs access to it? In what other places would the password likely be re-used (i.e., is just for SSH or is also for the nuclear missile control panel? It'd probably be fine to re-use the password for the former, but not for that latter). What do you stand to lose if the password is stolen (i.e., how many eggs are in that single basket)? And what's the price of the data hidden away in your database with the password-less accounts compared that? Then document this process and the reasons underlying your decision. If there's ever an issue in the future, you can justify your choices. There's no hard and fast rules here. You just need to think about the threat model (what am I protecting and from whom), and decide whether the benefits outweigh the costs. Security is always a compromise, so make sure you buy as much of it with what you give up (say in administrative overhead for example) as you can. Personally, I would be inclined to use the passwords on both the SSH and MySQL accounts, preferably different ones, but even a re-use here would be better than nothing in my opinion. (Unless you have lots of user accounts, which is an entirely different problem). 

The vendor's documentation instructed us to add the service account to the Backup Operators and Power User Local Groups - which we did. Reading the Explain tab for each one of the required User Right Assignment policies indicates that the Backup Operators have those Rights by default (TechNet seems to confirm this). Incidentally, there's no mention of Power User being assigned those Rights that I can find so I'm not really sure why that was a requirement. 

That PowerShell snippet should copy the the VHD from your master image, create a new VM and then attach it. There's probably oh so much more you want to do so I'm going to point you to Hyper-V Cmdlets in Windows PowerShell which is pretty dang full featured in Windows 8.1 / Windows Server 2012 R2. If you have specific questions later on, please feel free to bring them back. 

There are some great "appliance" style distributions like pfSense and M0n0wall, that bundle powerful features of their respective operating systems with a nice web application for configuration. In my opinion, these distributions cover a majority the use cases, and make up for what they trade in flexibility for ease-of-use. Is there a similar kind of "appliance" style distribution for Snort? I'm thinking along the lines of something with the Snort sensor, MySQL (or similar database backend), BASE and Pulled Pork configured with some sensible (although, probably not very useful defaults) and a nice web configuration utility for adding rules, viewing alerts, etc. Basically doing what PfSense does for OpenBSD/pf but only for Snort. Has anyone come across something like this? Do you think it'd be worth putting together a project if there isn't already one? 

Like most things Systems Center Configuration Manager, I'm sure there is a perfectly logical reasons for why things are the way they are but as a lowly technician I am also sure I'll never understand why. I checked using the Policy Spy from System Center 2012 R2 Configuration Manager Toolkit and again verified that, yes I am getting the two Maintenance Windows that I expected to find except that starts one hour earlier than it should: 

Finally I reloaded the zone and waited 14 days (the sum of the Refresh + No-Refresh periods). What results did you expect? I expected to see a 2501 Event in the DNS server logs noting the deletion of a bunch of DNS records. What actually happened? Nothing happened. The Zone Aging/Scavenging Properties showed that the zone could be scavenged after 6/12/2014 10:00:00 AM last week. No 2501/2502 events were recorded. All of the records with "aged" time stamps are still present. The date at which the zone can be scavenged after incremented another seven days to ‎6/‎18/‎2014 10:00:00 AM. As I understand it until that date stays at least 14 days in the past nothing will ever even be eligible for scavenging let alone actually be scavenged. The only 2501 events recorded in the event logs are ones that I have triggered by right clicking and selecting "Scavenge Stale Resource Records". They note that scavenging will try to run again in 168 hours which was this morning. I have DNS scavenging enabled for a few months and have waited patiently for something to happen. I have reloaded the zone multiple times (which resets this timestamp). What am I missing here? 

My guess is that you have scheduled your Offline Image Serving (i.e., applying Software Updates to an offline image using DISM) in such a way as that schedule does not have another possible run time. Looking a the Wizard it appears that a Custom Schedule has no option for re-occurance. The Custom Scheduling is designed primarily with the goal of ensuring that you do not inadvertently trigger an network intensive content update of your image to all of your distribution points and less about automating the process of doing Offline Image Processing. 

Looking at the vSphere SSO Logs did not reveal any recent activity with the exception of the which by my reading indicates that lookup of identity sources was successful: 

We are slowly starting to implement dhcp-snooping on our HP ProCurve 2610 series switches, all running the R.11.72 firmware. I'm seeing some strange behavior where dhcp-request or dhcp-renew packets are dropped when originating from "downstream" switches due "untrusted relay information from client". The full error: 

Are you relying on hostname completion (or "dns suffix appending" in the Windows world) to get a Fully Qualified Domain Name when you're running ping? If so, your is missing the and/or options. 

The Problem: What we are trying to do When trying to log into vCenter using the VMware vSphere Client we are greeted with the following error when using both Windows Sessions credentials or manually supplied credentials (): 

Is there a reason I should use port 22 instead of 1234? Because that's where the other 99% of SSH services listen and people will expect it to be so. Convention isn't a hard and fast rule but you'll find that most administrators expect to see SSH on port 22, HTTP on port 80 and so on. Unless you have a very good reason to deviate from the established convention - I suggest you don't. Nine times out of ten, some one much smarter than you (or I) picked the default settings for a good reason and unless you have a better one why change it? (It's really annoying to have to spend your first day on the job nmap-ing everything to try to figure out which "non-conventional" ports the previous admin/consultant/BOFH decided to run everything on). 

A bit of explanation is in order: All our SCCM clients belong to a Collection that gets assigned a Default Maintenance Window that only occurs once and is in the past. This prevents Collection membership changes and untimely client policy requests from causing clients that have held off actions from immediately performing them. However, since the Maintenance Windows are "union-ed" the Weekly Maintenance Window should apply... at 20:00. On a hunch I dumped all the Collections this client was in and then went and checked to see if they have Maintenance Windows assigned to them: 

We have been playing around with SCCM's Application Catalog and have come across an interesting quirk. My manager has directed me to implement the catalog so that software that falls somewhere between the "one-off install" and "needed by the entire workgroup" points on the spectrum of how many people need it should be published to the Application Catalog. Our help desk technicians can use the App Catalog to deploy these kinds of software to select users that need it as the situation warrants. We practice account separation, for example, our help desk rockstar Emmet Brickowski has two Active Directory user accounts. His regular unprivileged account, he should be using for all his regular work and when a UAC prompt rears its ugly head he has an privileged account () that is a member of BUILTIN\Administrators on all our workstations. When Joe User calls the help desk, Emmet remotes in or physically goes to help the user (our culture is big on face-to-face customer time), logs into the App Catalog with his privileged and sees a plethora of software that he can install in a standardized method for our user. Except when Emmet presses the Install button he gets this: 

I take this to mean that Global Condition is returning a different type of variable than I'm evaluating in my Requirement but I have no idea how to troubleshoot it from here. I've tried setting my Global Condition's type to Boolean, and setting the WHERE clause () but this yields the same error. How can I replicate my WQL query-based Collection using the Application-Program's Global Condition/Requirements targeting logic? What am I missing here (other than apt-get)?