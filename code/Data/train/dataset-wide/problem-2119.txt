I particularly have concerns when developers create new databases, even in the dev environment. why? because they start by creating the database in dev and then just ask me to copy it to live. have you done any capacity planning for this database? how big it should be and how much it would grow in a month? 

For dealing with server permissions I am currently using a procedure called sp_SrvPermissions V6.0 by Kenneth Fisher , however, while running a profile trace I often see the following line: 

Always drop a subscription in the publisher first to avoid this error. But if you happen to drop the subscription in the subscriber and get the error in the question above, the way I sorted it out was to go to the publisher server and publisher database and run the script below, that generates a drop subscription command for each article still pointing to the subscription. this is the script to generate the script to run: 

for a basic quick verification I check the log reader on the publication, see if the log reader agent is reading the transactions and passing them to the distributor. 

when I run the function fn_dblog to get some info from the SQL Server transaction log - I am after what is deleting something sometimes only - I have set up a server side trace for this now. I thought I could see something related to my mysterious deleting from the transaction log, but they are backed up every 15 min... anyway, while looking at the transaction log using the query below, I see this: 

I use sql server 2005 enterprise edition and have a subscriber DB - part of a transactional replication - that is used for Business Intelligence purposes - they run reports out of it, and SSIS packages read from it and import data into SSAS. This DB has over 800 GB, some tables have hundreds of millions of records. All these reports and packages need to read data using READ COMMITTED isolation level. As this DB is a subscriber DB and the publisher is very busy, replication keeps updating it all the time - whenever the publisher DB is updated. Sometimes the replication procedures require exclusive locks on tables (or pages or records), and it might be that the reports or packages are running from those tables (or pages or records) and as a result DEADLOCKS are more frequent than what I would like to admit. Example of a replication procedure involved in a deadlock today: 

I have got something that I believe solves the problem, however I have not worked with dynamic truncation: 

I have the following update, that creates a table backup in a database called tablebackups with the records that are going to be updated, and then do 2 different updates. 

Based ONLY on these clauses above, how could I define my clustered index, and other indexes and why? Moderators: I am aware this is not a strictly objective answer, but I consider the contents of the question valuable here is an example of how I am currently using this view: 

Just replace by your index name. in my example below I run it with ''which is one index that I have in only 3 of my databases and you can see the picture how it looks like. 

This is the full XML execution plan of the query and picture above. This is another picture using another tool to compare execution plans. 

I am working on a script that gives me a list of tables that use a specific stored procedure. It also works on all tables used by a stored procedure. on the example below, all stored procedures that use the table 'ProductItemDetailsDenorm' 

In order to see which procedures are using an specific table, () I have a couple of scripts that might help you: this first script shows the procedure and the table(s) the procedure uses: 

the permission I explicitly granted but the other permission was not explicitly granted. I believe it is automatically generated when you create a login to the server, and a user for that login on any database that live there. that implies a connection. considering this, is there any specific reason why I should grant the CONNECT SQL at servel level? 

the first one does not even bring the most correct results, the query 3 (as suggested in the comments by spaghettidba and ypercubeᵀᴹ ) is the one with best performance in my live environment, with my real table and data, as you can see below, the query I had originally and the one based on query 3: 

I have thought about maybe adding to the default trace, if at all possible, but then I get concerned about the extra cost on the default trace. Another idea that came from he comments by is using , but would I be able to link both (the default trace with the tracks of and the ? 

as this table is located in the database on my server, from my local machine I created a to the so that I can insert scripts into my table. this is how I insert a script into my table, the script is located on my local machine: 

but I cannot get hold of the error messages described on the pictures shown above. is there a T-SQL way to get to the errors/warnings related to the alwayson so that I can find out what is wrong? 

Then in his great answer Dan Guzman, showed us SQL_VARIANT_PROPERTY some wonderful thing that I was not aware of, now I am a new DBA and person. 

for a restore to a point in time, you just need the latest full-backup and all the transaction logs thereafter. this script will show you when was the last backup taken and where it is located. 

I get this done through DMV's as you can see on the below script: (it is part of a more detailed script, but I just extracted the part relevant to you in this question) 

is it at all possible? would it be worth creating a computed column, persisted, that would put the zeros and nulls together and create the index on that? is there a work around for when you want to put an OR inside a filtered index? would I not get into the same issue as below? Unable to create a Filtered Index on a Computed Column 

All that done - and to verify that it is all done alright I have the following script to check how many rows in each of the partitions: 

question is, in the event of a , when this current server becomes the secondary, alongside its c:\ drive, would the certificate still be valid? I should be testing this instead of asking but I don't have a good testing environment in hand. 

I get the error below: Msg 3930, Level 16, State 1, Line 11 The current transaction cannot be committed and cannot support operations that write to the log file. Roll back the transaction.