Your main server should (usually) have a unique MX priority. Otherwise you end up with mail entering at random entry points. The backup mail servers may have equal priority among each others, and this could be useful, e.g. if you want to balance the load among them in case of main server failure. For most situations, a single backup server is fully sufficient, provided it is sufficiently independent (uplink-wise). In fact, mostly it is sufficient to rely on the mail server on the sending side to buffer mail and retry when your server is back up. But you have a lot of better control to quickly transmit the backupped mail if you have a backup mail server under your control. On the other hand, it may even be harmful in case of prolonged (say, a full day) main server failure, if the sender thinks that the mail transmission went okay and there is no need to switch to a phone call for urgent matters (as they might do after warning messages by their own mail server). Of yourse I don't have to tell you that all backup mail servers must be configured properly to actually relay your domain. 

Are you sure you know everything the scripts are doing? Maybe the process is disk bound. Some Linux machines run every night and update the database used by . To do so, they scan the entire disk compiling a list of filenames. There's a lot of I/O but not much CPU usage. Maybe Windows does something similar? Compiling statistics by parsing the web logs? If you really are doing lots of SQL queries, maybe they're running on non-indexed fields of large tables, and MySQL has to scan through a lot of records. I think you need to take a closer look at what the nightly process does, in order to track down why it takes so long. 

For your second solution, I think you can simply use "#" in your .qmail file instead of piping through . 

For one-shot deals, scp is handy. If it's a lot of files, then rsync is a good idea. If a connection is dropped, rsync can pick up where it left off. I knew that rsync had compression (), and have just learned that scp does as well (). 

These are Windows Server 2008r2 machines in an AD environment. Some time in the past a DHCP server failed. An admin replaced it using a different host but the same address. Later on, when troubleshooting a rogue DHCP server, it was discovered that both the old and current servers were list as authorized DHCP servers. Once discovered it was de-authorized. That de-authorized our current productions DHCP server for a short period until that was reversed. Those two, with the same ip address, were listed for almost 6 months. Would that have been causing any issues or merely just an harmless entry in the authorized list? 

On a SQL 2008R2 box we recently had a number of jobs fails for various reasons that were mostly memory related, including one stating the page file was full. The Windows 2008R2 VM had 16GB of RAM and a dedicated disk for a 6GB page file. For now we moved the page file back to the C: drive and increased its size to 8GB. The long term effects of that are yet to be seen. Our Server Admin, this morning, increased that "swap" drive to 25GB as was recommended by the GUI. What struck me as odd is that the admin also changed virtual memory to be mananged automatically across all drives. This strikes me as waste of space but I don't really understand how Windows automatically manages the page file. Here is a snapshot of the current virtual memory settings to help with the description. 

The arp table lists for each interface, which MAC is (directly or indirectly) reachable via that interface. That may be a single directly connected device per interface. If you have several switches, you should ignore the "trunk" lines between switches in your counting: Every MAC reachable indirectly via the connected switch wll be listed there. If you even have redundant links between your switches, a specific MAC may be listed only with one of several possible trunk links, depending on which link is currently considered best. Finally, note that a MAC may not be listed if no traffic with that device has passed the switch yet; typically, at least at boot time they will talk something to the net (e.g. DHCP) and thus "tell" their MAC. In summary: For interfaces with directly connected devices, your guess should be fine. Just make sure to drop info from links to wother switches. EDIT: While explaining the arp table in general, I missed the point of your final question: For a router the visibility of a MAC may be less prelevant: Since there will typically be a switch between the router and any other device on the connected LANs, a packet from that device will reach the router only if the device taklks to the router specifically (e.g. in order to reach another net via that router) or vice versa. Thus a router may never learn MACs of devices in the LAN that never bother to talk to any non-LAN host. 

I made a script that takes data from an HR database and populates correlating attributes in AD e.g department, title, manager, location. Since people change titles, departements and/or locations on occasion it is important to keep AD up to date since we have processes that depend on the validity of this information e.g. location based dynamic distribution groups. To try and keep the process fast I just run for each users regardless if something changed or not. I had worried that I would be changing the modified timestamps needlessly since it is faster to just make these changes constantly then it is to verify that no changes would be needed. To my surprise it seems that AD is doing something like this for me as most of the user modified timestamps are not matching subsequent script execution times. This seems like a positive for me as AD is doing this for me under the hood. FYI I have 2 DC's that I could be talking to for this and I have checked both times to ensure that I am not just drawing a horrible conclusion. I cannot find an authoritative source that explains this and I am not sure if this is PowerShell doing the job for me or something Active Directory is doing. 

CentOS 6.2, bind 9.7.3, rsyslog 4.6.2 I recently set up a server, and I noticed that named had stopped logging to after the logs had rotated. I thought that was odd, since all logging happens through and doesn't write directly to the log file. It was even more odd because I had HUPed after updating a zone file, and it still wasn't logging. After I stopped and restarted named, logging resumed. What's going on here? The syslog PID hasn't changed (/var/run/syslogd.pid matches the PID shown in ps). Is rsyslog opening a new socket when logrotate rotates its logs and HUPs it? /etc/logrotate.d/syslog: 

I can't comment on SEO, but the typical use of the 204 response is to not refresh the current page. It's probably most appropriate with a request other than GET (like POST or DELETE): Excerpt from HTTP/1.1: Status Code Definitions: 

Two weeks ago, I was notified by my VPS provider that my server (CentOS 5.5, yum is up to date) had originated "NULL byte/Directory Traversal" attacks agains some servers at DreamHost. I spent a few hours going over the server with a fine toothed comb and didn't find anything. Before logging in, I retrieved a copy of the sshd binary and confirmed that it hadn't been modified. I installed a rootkit checker (chkrootkit-0.49) that found nothing. I checked the web logs of the websites I host, looking for a hit that may have triggered a script on my server to initiate the attacks but found nothing. Checked /var/log/secure and /var/log/messages around the times of the attack but found nothing. Checked , but found nothing. Did a on key directories looking for files modified in the past 3 days, but nothing. What else can I do to to find the cause of the attacks? I wrote a script to check for outbound TCP connections on port 80, but only came up with legitimate connections (SpamAssassin and ClamAV downloading updates, Joomla checking its site for updates, etc.). Even if I did see an active outbound connection, would I even be able to dump data from the process (in the directory) to show me the account originating the attack? After watching the server for a few days, I gave up. Now I've received another complaint from DreamHost, so it's happened again. I've requested detailed logs from DreamHost, but then what? Where else can I look? If I can't find the source, is there something I can install to monitor the server and log data when it starts making outbound connections to tcp/80 in the DreamHost IP space? What would I log? Just get a of all traffic in that timeframe and try to sift through it manually? Update See my accepted answer for the solution I came up with. I'm still interested in options for logging the source of all outbound port 80 traffic -- a way to know what the source process is and perhaps its parent process (and the parent's parent). 

Login to your switch(es) and issue or similar commands (depending on make and model). This will give you all MAC-Adresses of active devices (except thee switch itself). If any of these MACs does not occur among the MACs found with any of the ping or other methods in the other answers, you may want to investigate further what device that is. Maybe it doesn't matter because it does not even speak IP or belongs to a different VLAN, but at least you can get an overview whether your other probes are accurate. 

According to the this cisco documentation, "dispersion, reported in seconds, is the maximum clock time difference that was ever observed between the local clock and server clock". With ntp servers that are not totally broken, a high dispersion should never occur. The only feasible scenario is when your client inits ntp and so far has only its local clock available. And even then, a dispersion as high as you report corresponds to clocks that are off by more than two weeks. It should be sufficient to ensure that the local clock is not too far off in the beginning (even a couple of hours would still be acceptable), either by adjusting the clock (and date even!) in the BIOS or by issuing once before starting on the client. 

So everything appears to be in check there as well. However in testing deleting both users and containers I am unable to find them in order to recover those objects. The only thing that ever shows up is the deleted container itself 

The windowsupdate.log corroborates this. I would like to try and include only what is required to try and keep the post length down. The client reaches out to the server and see that it has X available updates. However it fails to download those. The log shows entries like this: 

When I run that command I get some of the user accounts and groups that I was removing while testing. According to this page on TechNet forums though that means the the bin is enabled. 

From what I can tell from looking at other tutorials about setting this up that should be showing up. For me though, there should be other objects there that have been deleted over the past couple of weeks. I am running my tests as a Enterprise Admin user. Searching for "recycling bin" and "active directory" leads me to other users that have similar issues but most of them are addressed by either actually enabling the feature or being at a lower forest level. In my case both are correct. Not sure what I am doing wrong here or assuming. A fact that is quite possibly related is that I cannot see this "Deleted Objects" container from ldp.exe either as per this guide I was using for comparison. The last step to see the container being: 

I guess this is precisely the event one can expect if a Windows update from our WSUS server requires a reboot. However, as far as I can see, such reboots should happen at most at about 3 a.m. This has happened repeatedly this year, namely 

HPKP in a webserver works by adding headers to replies containing hashes of various public keys, such that at least one of the hashes is valid = of a certificate in the certificate chain of the current server certificate, and at least one is invalid = not occurring in the chain and considered a backup cert. Several strategies exist for choosing the cert to represent the valid entry: 

EDIT after comment: It seems that the main blamed updates KB3002657, KB3035017 were either not installed, or had been installed days before the problem started on the relevant servers (clients, RDP servers, broker, DCs), but I'll try with unsinstalling them anyway ... 

It's going to be almost impossible to get "varied" IP addresses for a single host. An ISP providing VPS services is going to have a block of IP addresses on the same subnet assigned to the machine hosting the VPS instances. You could get a few IPs, but they'll be contiguous. Also, it's unlikely the ISP will appreciate you running a honeypot on the VPS, especially if it's not set up correctly and hackers end up using it to attack other hosts on the ISP's network or the general Internet. Why not run the honeypot off of your home Internet connection? Set up your router to route all packets to a single machine on your internal network. You could even route them to a virtual machine running under VMware on one of your other systems. If you really want to have honeypots on multiple IPs, you should just get multiple VPS instances with different ISPs. 

UPDATE with info as requested per comments If I set security to "TLS 1.0" (instead of "negotiate") at the RDP hosts, the problem persists. If I set to "RDP" the farm works - but everybody has to enter their password twice. In the error situation, for some reason I now often simply get "No connection could be established with the given credentials" instead of the original error. This is accompanied with a login failure event 4625 with status 0xc000006d, substatus 0. Before you ask: All DCs have their clocks in good sync; no LanMan compatabilty settings have been configured in the registry. The certificates on the RDP host client settings that worked were issued by the still trustworthy internal CA (trusted by all as per GPO) and valid until at least four months in the future. For testing I cheanged these to "automatic" certificates and back, without success. The original German error message text reads