The only connotation I can think of where this would be useful would be in L2 subnets where clients talk to well-known servers but are otherwise not supposed to directly communicate. In this case the various clients wouldn't be able to track on one another based on previously observed hardware addresses. At best it's pretty much a corner case, though, as there are better external solutions to accomplish that kind of separation (i.e. PVLAN). Your theory about commonality of address modes seems like the most likely. 

So - when an arbitrary client sends a packet toward the anycast IP, said packet will tend to find itself to the "closest" point of advertisement. I've put scare-quotes around closest because it's only close in the sense of how the routing topology has been laid out and what policies are in place for the routers along the way. It's entirely possible that the closest instance of the anycast address might actually be the furthest physically. If, in turn, the point at which this route is advertised fails (...which could be result of the service failing on the host and the route retracting or a more traditional network reachability issue) then packets bound to the anycast address will be routed to the next-closest (again - in routing protocol terms) instance of the route. During network reconvergence the client's resolution might fail and be re-attempted, with the re-attempt now following a longer path to reach what is - apparently - the same address. This is all transparent to both the client process and the user and is best thought of in network terms as following an alternate path to a given network. It's sometimes helpful to think of an anycast network as a logical construct. It's a virtual subnet that contains the service you're interested in. That virtual subnet is reachable via many paths through the network. That said, here are the major caveats to anycast designs: 

It's actually an expected behavior. As the amount of data increases the size of the transmit window increases which, to a point, will increase the size of the packets sent. Larger packets mean more efficiency (less overhead and time taken for acknowledgements) but they also mean more latency. This is the tradeoff for maintaining reliable delivery while also maintaining any kind of throughput in networks of widely varying latencies and bandwidth. Check out the PSH flag to cause TCP to flush buffers sooner. It might make a marginal difference under some circumstances. If you're looking for consistently low latency for lots of small messages, however, you might be better served looking at UDP - which means you'll have to account for assuring delivery on your own but you might realize a bit more consistency. 

The notion of ports happens at L4 (i.e. UDP) and is of no interest to either IGMP or the routers and switches in your environment. As such, if you run all of your multicast traffic on a single group but differentiate streams by port then all hosts joining that group will receive all of the traffic associated with that group (plus or minus source lists if you are using IGMPv3 and SSM). I doubt this is what you're trying to achieve. So - for purposes of network planning the numbers matter, the ports don't. That said, it would be wise to familiarize yourself with the mapping of multicast group addresses to MAC addresses as there is actually some overlap that needs to be borne in mind. Check out this link from Cisco for how this works as well as an excellent treatment of the topic of multicast overall. Honestly the best option overall would be to use some kind of lookup / mapping system so that your users don't have to remember addresses or port numbers. 

There are still a good number of shops out there using NIS in this fashion, and even more using unencrypted LDAP. Whether it is acceptable or not is a function of your local security policy and the degree to which you trust the other protections on your network. For what it's worth I think it sounds fine for a controlled environment. 

This may be a bit off of the beaten path but I've had good luck chaining in iPXE as the actual bootloader. Among other pluses it has a mechanism to load a kernel module along with your initrd. Check out $URL$ for an example. 

I would start by taking a look at your masking. If the general hosts are in 172.17.0.0/16 and your VPN subnet is in 172.17.7.0/24 then it's entirely possible for there to be some uncertain connectivity situations. A general host in 172.17.0.0/16 when sending a packet to a VPN host in 172.17.7.0/24 will attempt to ARP for the VPN host's address (rather than sending it to a gateway). The VPN host, in turn, tries to send a frame to a host in the general subnet. It's going to send via its gateway. If this gateway is a member of both the /24 and the /16 then you've got a similar problem - either it's an illegal configuration or the packet is actually being bridged rather than routed. It's possible you have proxy-arp configured - that would cause a routing device to answer ARP requests in the larger subnet for a host it has a route to, but this isn't clear from your posted materials. It's also possible that you have bridging set up somewhere in the mix. This could yield some strange situations as a standard ARP would work in one direction but in the other some sort of gateway would be called upon to forward a nominally routed frame back out the receiving interface - which, again, might work in some circumstances but isn't good (nb - this could be a source of duplicate ACK's). Can you put your VPN hosts in a non-overlapping subnet? Say give it a 172.18.x.x address and then configure routing between the gateway for this new subnet and the default gateway for 172.17.0.0/16? At a minimum this would make the whole thing simpler to troubleshoot and it very well might fix things. 

If the on-board NIC supports PXE then netbooting to install would be the easiest way to go, and would allow for quick deployment of additional blades in the future. You don't mention which distribution you're using, but both the Debian and RH-flavored variants have guides available to walk through the process of setting up DHCP, tftp, etc. 

if applied, the and may also give useful information. It's less likely as an issue, but the various fabric stats can also provide some info. Go for , among others. 

You're conflating two different issues - the category rating (Cat5e vs Cat6) refers to physical properties of the cable itself, specifically the bandwidth (in MHz) that can be successfully transmitted within a given noise/interference window. The physical implementations of various Ethernet specs take this into account (thus 1000BaseT vs 10000BaseT, etc). The order of the colors is a wiring specification. There are two common versions in use: EIA-TIA 568A and EIA-TIA 568B. In general you want to make sure that both sides of a given patch cable are using one or the other. If you make a cable with one side A and the other B you'll have a crossover cable (not generally necessary with modern auto MDI ports and NIC's). The same logic applies to jacks as well, of course, and best practice within a given cabling plant is to choose one spec and to stick to it. Also keep in mind that the order here is more than just cosmetic - keeping transmit on its twisted pair (..and separate from the dedicated twisted pair for RX) is crucial to noise rejection techniques, etc. 568A and 568B are both equally performant, but splitting pairs is most definitely not! 

Run to see if you do, in fact, have a key fully generated and registered under a non-default name. If there is, then you can tell the ssh process to use this key with . If the first command doesn't show anything useful then I'd say you can go ahead and generate a new key. You don't list your complete ssh configuration, so it's hard to know what to remove. Try to see what's in there. In general ssh will start to work as soon as the process has a valid key. Remember that you may need to add authentication and, possibly, vty configuration if they aren't in place. 

I'm guessing that you're trying to accomplish the isolation of hosts in the three VLAN's from one another but not from the Linux gateway while allocating addresses from a single common block. If this is the case, then I'd suggest putting each of the VLAN interfaces on the Linux box into a common bridge and using ebtables to limit L2 access on each subinterface to only the gateway. The Ubuntu documentation goes into plenty of detail about both setting up the VLAN interfaces as well as the bridging. So - assume you have eth0.2, eth0.3 and eth0.4 corresponding to VLAN's 2, 3 and 4 Create br0, assign it 192.168.1.1/24 and add the three subinterfaces. Set up ebtables ACL's on br0 to permit traffic from any MAC address to br0's MAC, from br0's MAC to anything and then explicitly block all other traffic. You may want to apply ACL's directly on the subinterfaces to allow for greater customization. 

So there are a lot of variables that may apply here. First off, I'd absolutely recommend that you check out the Nexus 7K VDC Configuration Guide if you haven't already, as it's a good comprehensive treatment of the subject. To your immediate questions, though - it's important to understand that there isn't really a root context. Everything currently on the box is in a VDC - specifically VDC 1. If you set up a new VDC (..say VDC 2) and move interfaces into it then you can pretty much just start configuring. The only situation in which you'd need to materially change the existing configuration would be if you needed to move connections around to open up port groups on an ASIC boundary. This last point (ASIC boundaries) is one of the big items that needs to be tracked. Different LC's have varying numbers of ports and a varying number of those ports will map to a given ASIC. The ASIC itself is mapped to a VDC - so if one port on that ASIC goes to a given VDC then the rest will need to go as well. As an example - the original (now EOS/EOL) 32-port 10GE blades for the 7K could be broken up into 4 groups of 8 ports - so 4 VDC's. The 24-port 10GE M2 blade can be allocated on a per-port basis, etc. By the same token mixing different types of line cards within a VDC has a substantial number of caveats. The M1 blades mentioned above can be mixed with M2's or F2e's but not both at the same time (so M1+M2 or M1+F2e are both OK, but not M1+M2+F2e). There are also implications to capabilities and throughput - so, again, M1+F2e pushes all L3 forwarding to the M1 blades (..even though the F2e is faster by itself) and drops buffer sizes and other capabilities to a least common denominator. As a general statement if you can avoid mixing LC types within a VDC your life will be simpler. This may be less of an issue for you but another concern is allocating resources to VDC's. You can control the amount of CPU and memory, the number of routes and VLAN's, etc for each VDC. From an administrative point of view you might also need to track on who holds privileges to adjust VDC's or reboot the entire box vs those who have full configuration rights within a given VDC. Finally - how modern is the box itself? Supervisor 1, 2 or 2e? Fabric 1 or 2? Sup2/2e potentially allow for up to 8 VDC's while the sup1 supports a max of 4. The Sup2/2e is needed to move beyond NXOS 6.2 and if you have a Sup1 you need to get to 8G of DRAM to even run 6.2. There also may be a need for additional licenses to support VDC operations. Anyhow, take a look at the guide linked above - it should get you started. 

While banks of ports often share an ASIC, each has to have its own separate PHY. If the PHY has been damaged it could very have a problem while its neighbors don't. That said, output drops are an odd symptom for a physical problem - not impossible, but not typical. Notwithstanding half duplex links, output drops usually have more to do with buffer exhaustion than physical problems. You may get more information by setting up a packet capture on the other side of the wire. A bad PHY would be expected to manifest with some number of physical layer errors (bad CRC, runt/giant, etc) on one or both sides of the link. All in all it sounds like you've eliminated enough that it may be past the point of diminishing returns. I'd recommend an RMA if you have a contract. 

Make sure of units: 150 kiloBYTES per second = 1200 kiloBITS per second, or 1.2 megabits. That's slightly on the low side (~80% utilization) but within tolerance depending on measurement intervals, etc. 

Now - finally - with all of this laid out, your question is easier to answer: There's nothing inherent to anycast that makes it more resistant to DDoS. Each of the potentially millions of flows of DDoS traffic will find their way to their nearest instance, likely making it unavailable to any other legitimate clients who are would otherwise be routed to these points. Now, if the vast majority of the hosts on the botnets in use happened to be in, say, Eastern Europe and one of the anycast routes happened to be originated in a nearby PoP (again - "nearby" in terms of routing topology) then this traffic would be sunk to one point while much of the rest of the world continued to resolve to the same route that was also hosted at convenient points on other continents. In this particular case anycast would arguably be one of the best mechanisms to minimize the damage of a DDoS attack. This is highly contingent on how the anycast routes have been distributed and how policy has been configured (see #3 above - not a trivial problem). Clearly this use-case isn't as compelling in the case of a DDoS attack that's truly distributed. If properly engineered, though, the localization of the anycast routes means that the attack load can now be spread across an arbitrary number of geographically dispersed physical hosts. This will tend to dilute the effect of the attack on the target as well as potentially spreading the impact across a bigger chunk of the network. Again - a huge amount is contingent on how things have been engineered and configured. Why is this considered a win over round-robin? Simply because it's possible to deploy an arbitrary number of hosts without the need for separate load-balancers on the individual IP's and there's also no reliance on the timeout value for particular clients deciding to move over to another resolver. One could literally deploy a thousand hosts within a single data center with the same IP and balance the traffic accordingly (nb - obviously massive practical limits based on size of ECMP tables, etc) or deploy a thousand geographically disparate sites each with a thousand hosts. All this could be accomplished without changing a client configuration, without the (admittedly usually clustered) point of failure of a load balancer, etc. In short - when properly engineered it scales as well as the Internet as a whole. 

Why would the routing table change at all? R2 already has routes to everything via its links to R1 and R3. Even if R1 loses its route to 172.16.3.0/24, R3 is still advertising it back to R2 (as it did before the link went down). You should see some change in the OSPF database itself, but the host's routing shouldn't need to change. Try comparing "show ip ospf database" on each of the routers before and after shutting down the link. Worth noting - Without multipath enabled in your ospf configuration only one path to 172.16.3.0/24 will be injected into your routing table. If it sees routes from both routers then it will choose one of the paths based on a tie breaker - the router-id's. If R2 had already selected the path where the link -didn't- go down then nothing changes... A couple of points- 1.) Why are you using "redistribute connected" - particularly on an unrestricted basis. The "network x.y.z.q" command already takes care of the links themselves. Redistribute connected is, at best, going to yield local routes showing up as external. Remove this statement and add another network statement for the 192.168.x interfaces (mark these interfaces as passive if they're supposed to be for end-host connectivity). If you must redistribute connected routes then apply filtering to limit the routes that are actually redistributed. Unrestricted redistribution is only pain waiting to happen. 2.) It's been a while since I've played with Quagga, but I think you also need "link-detect" on the ospf interface definitions.