First of all, don't try uploading straight into a web folder. I would recommend uploading to your home folder, doing the necessary permission changes THEN copying(NOT moving) the files to that destination(will probably require sudo). That should do it. 

Moving those files shouldn't be an issue. This article is what I used. Its a little sparse on details but it will basically let you mount Amazon S3 to migrate those files. Best of luck! 

Slicehost vs. Amazon EC2 is not really fair. I'd check out Linode vs. Amazon EC2. Anyways, my company recently decided to use Linode due to the simplicity of use and the appeal of better I/O rates. Amazon is better if your requirements are minimal or excessive in any sense. Linode/Slicehost win out if you need quality I/O rates. There are more advantages/disadvantages but I still prefer Linode. 

Ignore the Ubuntu 10.10 suggestion. If you're rolling a server, avoid the non LTS release. I'd suggest Debian if you're comfortable with the package system as it has a smaller foot-print than Ubuntu 10.04. 

I would recommend adding this code to your script $URL$ Or use screen. Edit: This is a little more modern but I haven't used it(I use something custom) $URL$ 

You should check out rootkit hunter and fail2ban. They will do a lot of what you need. Watching the Apache logs seems a little too narrow for my tastes. 

Try another server in the same region. It sounds like it's probably the destination server. You can't expect everyone to have 1Gbps links. 

Ultimately, my goal is to be able to iterate over a list of values 'N' times and repeat a stage and pass parameters to each stage and if any of the stage 'fail', fail the whole stage. 

It generally comes down to poor planning on the former description. Reactive security resolution/putting out fires. I.e., "Oh shit, they're hammering us on 123, lock it down!" or more appropriately, blacklisting. The latter is a form of whitelisting(best practice) and usually comes down to locking things down through implicit exclusion and opening up what you need as required. Which one you chose usually comes down to your environment/style. 

You should probably clarify what web server you're using but I'll assume Apache. If so, look into vhost files. What you want to do is very common and quite easy. $URL$ 

You can probably leave them alone in respects to response time. However, it can't hurt to disable them if you know how. Also, consider looking at your apache and mysql configs. That could be part of the problem if you have some nasty queries running. 

I have been offered a job as an 'engineer' for a large tech company - already working as a systems admin for the government. I wanted your take on how you step into a new job and learn how all the pieces work together. They have approximately 2 racks of gear and some very complicated relationships. What is your strategy for getting a grasp on things? Any preferred tools/methods? 

You're going to be doing something called bonding. Check out this article for the specifics. One term to research NLB-Network Load Balancing for Windows. 

I have a list of values (ipOutList). I want to repeat a step N number of times, in parallel based on the size of that list. Can someone show me what I'm missing in this process? Thanks! 

While the AZ is fixed over the lifetime of the gear, it is quasi-unique. Those AZs are randomized on a per account basis so your us-east-1b might be my us-east-1d for example. It is arbitrary and not worth worrying about. 

This is standard practice. You wouldn't want your users trying to submit changes to a database that is being backed up. Also, stop doing backups during production hours. Schedule them for after-hours or something. It's very poor practice to do otherwise. 

MGMT1 is the main VPC. We're able to ping other MGMT VPCs. However, the route from MGMT1 -> MGMT3 -> DEV doesn't seem to be available. I can't ping. I can ping from MGMT3 -> DEV however. That fact leads me to believe all the routes are correct. However, MGMT1 isn't aware of DEV despite being connected to MGMT3 and it knowing about DEV through the peering route table. Am I missing something? Thanks! EDIT: All security groups are very permissive as indicated by the ability to ping across the MGMT layer and from MGMT to DEV within the same region. 

Papercut is actually a really good alternative. The price is decent and only gets better as you scale. Edit: and I wouldn't trust CZsolutions if they can't keep their site online :P 

Any answer you'll get here is purely anecdotal. I prefer Debian or Ubuntu. Make sure you don't use the 64 bit version if you're using less than 4Gb of ram. Also, check out Linode for a better deal(assuming you're using Slicehost). 

There are a lot of facets to this question. Firstly, how are you defining 'squatting'? Maybe they're just parking it. Did they set out to poach the domain in an attempt to cash in on it when your group/company/project needed it? If they're TRULY squatting, you have some legal options you may want to explore. Barring that, don't make the first offer. You'll either set it WAY too low and be working hard to recover, or WAY too high and they got what they want. Secondly, what is your plan if you don't secure the domain? Have you considered alternative domains or are you tied to this? If tied, see my first point. Legal options might not be terrible but it'll probably be cheaper to buy them off. Let me know how it goes. 

I have a Cloudformation stack that I create through Jenkins in various Regions. I have a Chef server in one Region with a separate security group. I need new instances created via Cloudformation to register/be created and add themselves to the Chef SG in us-west-1 regardless of their region. Is this feasible? Edit: I need to do this via the Cloudformation script as opposed to other methods for a multitude of reasons that are lengthy/convoluted. Edit2: For clarity, I don't want the instance to be part of the SG, but rather for that the EIP of the new instance to be added as an ingress in the SG. 

Why not run a standalone system for MySQL, like a VPS. I used to run my dev and production servers on a couple of these, with replicated MySQL DBs on separate nodes. 

You could try: ps aux | grep -v grep | grep program name That'll check the running processes, grep for the program name, but exclude the grep itself. 

Regardless of whether you can reduce the overhead, I would recommend replication of MySQl to a secondary server. With some load-balancers, you can dramatically decrease your downtime AND ease the load on the server. Just a thought. Message me if you want some guidance on setting up replication. 

I'd suggest using Google apps for this and just setting up forwarding. If you insist on setting up a mail server, check out this guide: $URL$ 

Sounds like your ssh config is blocking remote access for root and su is probably doing something similar locally. Double check all your configs. 

You really won't notice a major performance difference unless you're getting a massive surge of traffic. If so, put it on the disk with the fastest RPM. However, you may want to put all this on the larger disk if you imagine it'll grow beyond the size of the smaller disk. 

I would actually recommend a VPS if you're willing to take on the technical requirements. A VPS gives you root access to a server of your choosing. AWS and Linode are two great options(AWS is free for a micro). If you're not technically inclined enough to take on a server; look for a host that offers php/mysql(most do) and that should do you. I would shy away from a hosting provider that offers anything 'unlimited' though. 

I have a few dozen Solaris(5.10) boxes. By default, they are using ksh as the shell. Only a few users have sudo privileges. The rest do not. I don't want my regular users changing to another shell. Here is what I've done: Minimal sudoers so /etc/passwd is out of the question for them. Minimal sudoers so usermod -s is out of the question for them. I have NOT disabled access to the shells in /bin or /usr/bin yet, but it will be done - so please disregard the fact that it hasn't been done yet. So, in theory they could write startup scripts to execute them, right? Am I missing anything else? 

First of all, check out Denyhosts. Add his(and your IP) to it and that will prevent others from connecting. Also, set 'AllowUser hisname yourname in sshd_config. I wouldn't recommend ftp. Check out sftp. If you're using ssh already, you might as well use sftp too. Blocking http is going to be trickier. Check out .htaccess but even that isn't ideal. Most of this is for naught if you're using dynamic ips(most home connections). 

We have an FMS Origin server that is outputting HDS(Also RTMP but we don't want to distribute this). We want to put Vanish on another box and have it act as an intermediary for our CDNs so their edge servers don't hammer our Origin. Documentation is lack 

You haven't setup a standard user. As such, you're connecting with root. Consider adding a regular user and specifying that users login credentials when connecting. 

I've seen this before. It's something to do with Ubuntu 10.10. I'd look around on the bug tracker as its been posted a few times. To be sure, take a snapshot of the disk, wipe it then drop it in a secondary system to see if the bug repeats itself(to rule out the disk - unlikely culprit). 

KVM sounds like it's the best option in this case. We use ESXi here but that's overkill. Xen can be tricky to setup while KVM is relatively simple and stable. I've seen it used in production environments. 

I have thousands of web files(*.php, *.htm, *.html, etc) that contain an absolute path($URL$ I need to scan a directory and all subdirectories for the various file types and within those files, modify my absolute path to a relative one of my choosing and then leave the rest of the file as is. It must not change the path of any other absolute URLs though. I've checked the similar questions already posted on the matter but none seem to address this directly. Any suggestions? Thanks! 

Have you mentioned the whole standardized vs. non-standardized aspects of allowing this? I'd push back if I could and at the VERY least, get all of this in writing to CYA. There isn't much you can do at that level other than tout your standardization argument. 

This is pretty easy to do so I'll keep the answer a little high-level. You can use Cygwin and to get the current IP of your Windows box. Forward rsync(ssh presumably) to your Windows machine. Simple script on the Windows/Cygwin box to then rsync the tar'd file(you're taring it, right?) that needs to be backed up. I.e., That should do it. 

I don't know what to do at this point... EDIT: I've fixed my settings to match the following: MASTER1: 

Can I offer what may seem like a silly idea? Check the type of cable they're using. Also, check the cabling on that switch(unless it's in common use by others already). I'm saying all this because it sounds like a cable issue. Have the user try another cable too and another switch(if possible). 

My favourite is to deploy a simple FTP/NAS device to the network with a public folder. Push scripts to all the hosts to download from the FTP folder and execute the .exe's.