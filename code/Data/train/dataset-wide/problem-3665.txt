I used a product by Castle Rock called SNMPc - its not the most polished of tools, but it does everything that you could want and wont break the bank. Its basically an SNMP statistics collation tool, that can baseline and warn if baselines are deviated from. It can be given thresholds for growth and decline warnings and works well with any SNMP capable device. Enabling SNMP in *nix is simple, as it is within Windows. Extensibility of SNMP is quite easy too (at least on *nix) SNMP is free - there are 3 levels; all to do with security. SNMP 1 is plain text and very 'insecure'. SNMP 2 is encrypted, but its trivial. SNMP 3 uses certificates. It can be a bit of a chore getting it to work the first time though. Because there are so many counters and statistics that you can pull, it can also take a while working out which ones are right for you - but once this is done, its very straight forward. You pay for front end collation and trigger on events to make SNMP useful. You can do it with open source software, but I wanted a modicum of commercial support. Data can be polled from the devices (normal) and on critical systems, you can get the individual system to send a trap event notifying the trap manager that something went wrong, and they need to know now, and not wait for the next poll period. Polling remote devices can be done by using a collection agent - same sort of thing as the console, but without all the reporting wizardry - that then pushes the stats at the central console periodically. Of all the monitoring systems I have used, SNMP kept supplying what I was asked for, and within the budget I was given. There is a product for Microsoft Servers called MOM 'Microsoft Operations Manager' where the 5 server workgroup version is (or at least was) free... but extending it to keep an eye on enterprise systems such as Exchange and SQL could cost a lot in licenses and connectors. Beyond that - My experience is limited to SNMP, MOM, and Spotlight (by Quest) which was awesome and a bit too far beyond our budgetary range for all but the most critical of Oracle Databases. 

Okay, so I'm still not entirely sure what solved this problem, but the array is now rebuilding. The first thing I did was to remove OMSA completely and do a clean install (when I upgraded before, I went from 7.4 to 8.2 using an upgrade package). After uninstalling OMSA 7.4, I rebooted the server and went into the PERC S300 array manager that is an option on boot. There, it was showing Disk 0 as Ready and Disk 1 as being the Spare (OMSA did not show disk 1 as being a spare). I set Disk 0 to the Spare and unassigned Disk 1 from the Spare. After that, I proceeded to boot and install OMSA 8.2. After installation, I went to view my virtual disk and viola, it was finally rebuilding. 

Edit: I upgraded Open Manage Server Administrator and now I receive the following message when I attempt to assign the new disk as a global hotspare: 

I am confident in my NAT policies - People can connect to MY FTP server just fine, and I can connect to other FTP servers from inside my DMZ - there's only one specific server where this is a problem. The connection used to work fine before I upgraded my firewall from a Cisco SA520 to the Dell Sonicwall SOHO. I will also note that while I can connect to other FTP servers, those servers connect in passive mode, so it could be an issue with active mode connections. Currently, the problem server only accepts active mode from my IP. My current thinking is that it's a firewall issue on my end, but I can't understand why. My Firewall rules are pretty simple: 

What's your question? It's technically possible. Unless there's a requirement otherwise, I would probably have the storage mounted in a resource script as opposed to keeping it mounted at all times. 

When you exit an interactive bash login shell, it sends a SIGHUP to all children unless the shell option is set to off. When most userland processes receive a SIGHUP, they will exit. When you , you are marking the process to prevent bash from sending a SIGHUP on exit regardless of the shell options. You could have also prefixed the command with to reproduce this same behavior. As far as STAT D: 

You can use acls and other parameters to specify the situation in which you would not want to cache. For directories: 

I know it seems unrelated, but verifying fundamentals such as permissions and resource availability (memory) could potentially help isolate the issue as well. Are there any other errors? 

Nagios has host and service dependency configuration files. I've linked the documentation below. You can cross-link the checks and each will only alert if the other is down. Nagios Dependencies 

I know there are a ton of similar questions already, but I've read through just about everything I could find and am still having trouble resolving my specific issue. Problem: I am having difficulty transferring data to an external FTP server, but only from an FTP client running on a server located inside my DMZ. Transferring from an FTP client works fine from any machine inside my LAN. A brief overview of my environment: I have a Sonicwall SOHO router/firewall with the following interfaces configured/connected: 

("FTP (All)" service object uses the same ports listed above - TCP 20, 21, 49152 - 65535). I am not very confident in what this rule is doing, but I think this is what allows passive connections to my server from external sources. However, I don't know if it is causing problems when I am trying to connect to external sources. -- Using the Sonicwall's packet monitor, I am able to login to the server and issue a PORT command which returns a 200 PORT successful response. However, after the LIST command is issued, my server never ACKs the SYN message (frame #57/58) sent by the FTP server. The FTP Server then retransmits the SYNs (frame 64/65 and 72/73). When it doesn't receive an ACK in the timeout window, it times out. Here is some output from the Sonicwall's packet monitor during an attempted connection: 

The only effective way to do is is with mod rewrite rules. If the referrer is not from your own domain, then rewrite the image url to be one that is non-existant, somewhere else, or a 1px by 1px transparent gif. A lot of people do this with an image that says 'No leeching, buddy' or similar 

People wanted to exchange files with the outside world (Open Office vs Microsoft Office) They wanted Salesforce integration - MS Office only They wanted screenpop / telephony CTI They wanted some industry specific applications (Windows only) 

There was also some fairly major headaches getting single sign-on working with NIS/Kerberos and all in all, I think we spent more time in support than we saved on licenses from MS From a purely non-tech point of view, the differences between Open Office files and Microsoft files caused a significant headache. I know that OO saves as MS Office files, but it just did not work in the real world. To take eMail as an example - People also get comfortable with the Outlook instantaneous update you see with exchange - not so happy with a 5 minute poll on a local IMAP / POP server. 

Don't forget to change the root password. If any user has UID 0 besides root, they shouldn't. Bad idea. To check: 

That's a bit worrysome. I'd verify that your file wasn't modified by comparing to a known good file. You could use your distribution's package tools to verify the file on an isolated system. 

I agree with Matt. If you can't trust them, they shouldn't have root. For an audit trail, rootsh can be used. You can only allow them to sudo to rootsh. Combined with this and remote logging, it would be slightly more difficult to stop than simply sudo. 

Have you looked at or ? That's typically how I determine link status without physically inspecting the server in Linux. 

While fenix's auditd recommendation seems ideal, you may find a filesystem IDS such as AIDE helpful. Unfortunately, it's unlikely to be fine-grained enough for what you're attempting to isolate. I'll often write scripts as a solution for problems like what you describe. If you cannot accomplish what you want with solutions recommended, write something yourself. It's often not very complicated. 

I have set up a business that ran purely on Open Source. SuSe desktop, Mitel SME for fileserver / eMail. It all ran beautifully until... 

Can you boot in vga mode? if so - try a less capable video card? I had issues after installing Vista on an nVidia machine. After rebuilding it a couple of times, I discovered that it was a known problem, and there was an updated nVidia driver (the 'even more recent' one on windows update was no good) 

is this even feasible? Yes. I would suggest you look at TXT records for holding this information what are the pitfalls? Not sure how do I size the DNS servers. Not sure, but as far as I understand it DNS is a fairly low requirement service using UDP to handle requests and replies - as such it is up to the client to determine if it got an answer or not and re-ask if it was unsuccessful. 

There used to be an overriding reason to go with Linux for 64bit support (and therefore better memory management) - this is now reduced with the more stable support of x64 WIntel platforms. I cannot give you a definitive answer for why, but I looked after the IT support for a smallish development house who wrote applications against Oracle 7i, 8i, 9i and 10i - all database hosts were done vs Linux - on RHEL 3, 4 and 5. The primary reason for this was given as it was the most stable supported Host OS that Oracle ratified for use with their database. With the plethora of patches that Microsoft release, it was almost impossible to keep the host OS at a level that Oracle had tested and would support. Having used Oracle on Windows 2003 x64 and RHEL 4 x64 - the same database performed a lot better on Linux than on Windows - the back end storage was a 300Gb fibre presented raid 10 partition on an EMC array. Both systems were 'untweaked' 

(Note that any missing frames in the sequence above are due to other traffic I filtered out from this view). From my LAN machine, the ACK is sent back after the LIST command and data transfers work just fine. -- So while I have diagnosed it to this point, I don't know what to do next. The router doesn't say it's dropping any packets or blocking anything due to a firewall rule. Any ideas as to why my LAN machine sends the ACK but the DMZ machine doesn't? 

I've checked windows logs and don't see any failures or errors related to this within the past 24 hours (when I first swapped out the disk). Looking at the OMSA logs, it tells me when I unassign a global hotspare, but there is never an event corresponding with me assigning a global hotspare. Similarly, I see notices when I cancel a rebuild but never when it is initiated after assigning a global hot spare. I've done some looking into similar problems, but most people receive an error or an actual failure during rebuilding, which I am not. I have good backups and the system is still running okay, so my next thought is to just take an image, blow away the array and start from scratch. I am mostly looking for other ideas before going that route. 

It is not possible to snoop on your machine without extra software. Remote Desktop does not allow a 'view' or share - Remote Assistance does but requires you to initiate it. VNC/RAdmin etc require that you install software on your machine, and although it can be push installed from a remote location, it's not generally done, and will alert you that it is installed or that someone is using the connection. The same applies to LogMeIn and GoTomyPC Back Orifice will do exactly what you are afraid of, but is detected by even the lowliest of Antivirus products. Dameware allows surreptitious observations of a machine (most others tell you that you are being observed) and also allows the remote administrator to push the installation onto your machine as standard. The real question you should be asking is this ... If you are scared of being discovered - should you really be doing it in the first place? Is it breaking acceptable usage policies? Are your actions illegal / against your employers company interest? If so - do not be scared of doing things you could be disciplined for by SIMPLY NOT DOING IT! Of course it is also possible to identify the port your PC is connected on, set that as a monitor port and send all its traffic to a packet capture station (thats what most big corporates do in cases where monitored activity is required) There - they see everything. I have before now installed SSH on windows machines, so that I can run command line utilities such as PSLIST and PSKILL to see if people are doing things they shouldnt and stop them) So the short answer to your question is 'No, not easily' The long answer to your question is 'Yes they can monitor everything you do, the level of their monitoring will take different amounts of effort, and often the effort is too much for the end results' 

Run a similar version of the earlier script on the source server using cron, where it archives and encrypts all the files to a directory. At an appropriate interval following that, have a cron on the target server rsync over SSH that location. You could scp but rsync has many advantages. Otherwise, I'd scrap your requirements and run an on-demand hybrid solution from the target using SSH pipes. 

You would hire someone to write Linux kernel code as you would any other person. As with other hiring, you seek areas specific to the skill-set you are attempting to hire for and recruit there. This can include user groups, mailing lists (as Bart recommended), USENIX/SAGE job boards, careers.stackoverflow.com, Craigslist, your network, Monster/Dice, and many other places. 

You can use a wildcard certificate with multiple subdomains across multiple IP addresses. While there is no technical limitation, often Certificate Authorities have licensing restrictions on thier usage. 

It's fairly common these days to drop ICMP, as it's a generic method to use for Denial of Service purposes. A higher-bandwidth host or a multiple of hosts repeatedly pinging a single Web server could utilize all its bandwidth. Others might drop to lessen their footprint on the Internet, thus potentially being overlooked by mass scan traffic. While it's common, I'd argue that it provides little value and does little to minimize DoS and footprint while limiting diagnostic potential. 

A client of mine has been in this precise situation. He left his WiFi open to all as a gesture of good will, two months after he did, he got a warning from his ISP about his data usage, and a week after that he received a cease and desist letter from the RIAA He no longer has open WiFi. 

I think the most stupid thing I ever did was to remove the default route on out external facing firewall cluster - whilst vpn'ed to my desktop over a hundred miles away. Fortunately it was in a period designated as planned downtime (just in case), but that did not save me the 200 mile round trip to go and reconfigure the firewall onsite. It also did not help that this took out our internet exposed production systems for the duration of my travel and subsequent fix. We all know the definition of a nano-second. An ohno-second is even smaller, and is the time between hitting 'enter' and realising your mistake. 

Personally, I tend to use binary for every transfer. I was stung once with a file that had been placed from a windows machine onto a unix machine using binary transfer. As a result the txt file still had CR LF endings. ASCI mode looks at source and dest platforms and performs line-ending translation, so the resultant file I got had CR CR LF line endings (LF was translated by ASCI to CR LF as it was unix -> windows) Sounds petty - but it was a 20Gb log file, and I only had one time window in which to collect it. I use eol conversion utils on the local machine if the necessity arises. EDIT: I was getting the file back onto windows from the unix host