tl;dr: Is there an accepted or proposed term for a topological space whose $T_0$ quotient is sober? The condition that a topological space be sober (and therefore equivalent to a locale) may be broken into two parts: 

I can't give you an answer that fully addresses what Lawvere and Rosebrugh were thinking, since I haven't asked them. (If you want to ask them, Rosebrugh runs a category-theory mailing list at $URL$ he is an active participant, and Lawvere has been known to comment from time to time as well.) But I can say something about what can be done with equality between objects in ETCS. For the most part, I think that ETCS has equality between objects by default. First-order logic, as usually treated, is both untyped and equipped with a global equality predicate, and since ZFC is usually written in such a first-order logic, Lawvere wrote ETCS in that logic too. Using an untyped logic requires him to put all morphisms into a single class (and to put objects in that class as well, identifying an object with its identity morphism), which means that he needs that global equality predicate. On the other hand, it's quite possible to write ETCS in a typed first-order logic, with a type of sets, a dependent type of morphisms (dependent on two terms of the type of sets), and a dependent equality predicate for morphisms, but no equality predicate for sets. This is how Leinster does it; although he doesn't say things like ‘dependent type’ and ‘equality predicate’, such types and predicates are what appear in his paper. (And while he never denies the existence of an equality predicate between sets, no such predicate appears in his paper either.) I think that Leinster's way of putting it is more in the structural spirit than Lawvere's, for the reasons in your question. But I don't think that Lawvere made a conscious choice to reject that argument either. 

What do I call two sequences $a, b$ such that $\lim_{n\to\infty} |a_n - b_n| = 0$? Or what do I call two functions $f, g$ such that $\lim_{x\to c} |f(x) - g(x)| = 0$? (For my purposes, these are essentially the same thing, and I will happily extend a term for one to the other.) This seems like such a straightforward condition that it must have a standard term, but I can't find it (in either context). I looked through the Wikipedia article on all of the variations of big-$O$ and the like, but these are all too weak. If $\lim_n a_n$ (hence $\lim_n b_n$) existed, then I could call $a$ and $b$ ‘coterminal’, but that limit might not exist. In an incomplete space, I have seen $a$ and $b$ called ‘co-Cauchy’ under the weaker assumption that one (hence both) is Cauchy, but I don't want to assume that either. I could call $\exp f$ and $\exp g$ ‘asymptotic’ (as $x \to c$), but I want to refer to $f$ and $g$ directly. Surely somebody knows a term for this? 

Characterization is too strong; there are many models of $\operatorname{ETCS}$, so its axioms don't characterize $\operatorname{Set}$ either. But I'll interpret your question as asking how the axioms of $\operatorname{ETCS}$ might be changed if the intended model is $\operatorname{Fin}\operatorname{Set}$ instead of $\operatorname{Set}$. Of course, you must remove the existence of the natural numbers object, and that might be sufficient; I mean, if you just want to have a topos whose internal mathematics is finitist in the weak sense that it doesn't assume the existence of any completed infinity, then this is sufficient. Both $\operatorname{Fin}\operatorname{Set}$ and $\operatorname{Set}$ are models of this theory. If you want to more assertively claim that the objects should be finite sets, then a strong way to do this is to add the following axiom scheme, where $\Phi$ is any property (in the language of $\operatorname{ETCS}$) of sets (objects): $$ \Phi(0) \;\Rightarrow\; (\forall S\colon \operatorname{Ob},\; \Phi(S) \;\Rightarrow\; \Phi(S + 1)) \;\Rightarrow\; \forall S\colon \operatorname{Ob},\; \Phi(S) \text.$$ (Here I've chosen a particular way of phrasing the language of $\operatorname{ETCS}$ that hopefully makes sense to you.) As far as I can see, we can't make this a finite axiomatization (avoiding axiom schemes) without a trick akin to the switch from $\operatorname{ZFC}$ to $\operatorname{NBG}$, although I haven't proved this impossible either. 

An adjunction between posets is precisely a Galois connection. (Paul has characterized these in the case where the posets are both complete.) English Wikipedia; nLab. 

There are several interpretations of the original question, but one is, why focus on open sets rather than closed sets? I have an unusual answer. Suppose you want to do constructive mathematics. (Don't ask me why, you just do.) So you abstract the properties of open and closed subsets from the real line. Then you see that open subsets are closed under arbitrary union but only finitary intersection, OK. Dually, you see that closed sets are closed under arbitrary intersection but … not under finitary union! For example, the union of $ [ 0 , 1 ] $ and $ [ 1 , 2 ] $ cannot be proved to be closed. (The closure of the union is $ [ 0 , 2 ] $, but to prove that the union itself is all of $ [ 0 , 2 ] $ requires the lesser limited principle of omniscience. Or less formally, there is no definite method to decide whether a number is near $ 1 $ is in $ [ 0 , 1 ] $ or in $ [ 1 , 2 ] $.) So open sets are better behaved and naturally you prefer to axiomatise them. But as you continue with constructive topology, more advanced things fail, such as the Tychonoff Theorem (which implies the axiom of choice and thus excluded middle). Then you learn that this stuff works in locale theory, so you abandon traditional topological spaces for locales. And here the duality between open and closed is restored; a locale's frame of opens can just as well be interpreted as a coframe of closeds, and only tradition tells us to do the first. (In the locale of real numbers, the union of the closed sublocales $ [ 0 , 1 ] $ and $ [ 1 , 2 ] $ is the closed sublocale $ [ 0 , 2 ] $, and the thing that you can't prove constructively is that every point in this union belongs to at least one of its addends.) So this answer only works in a very unusual frame of mind: setting off down an unusual path but not going all the way. 

In the theory of electromagnetism, the classical Stokes Theorem moves between the differential and integral forms of two of Maxwell's four equations; see $URL$ for discussion. Note that the integral forms may be directly interpreted using classical physical intuition, while the differential forms give us differential equations that we might solve, so it is important that we can switch between them. ETA: I think that Wikipedia's discussion is a little vague, although possibly appropriate in that context. So here is more detail, looking at Faraday's Law. In terms of physically observable quantities, the law states that the rate of change of the magnetic flux through a stationary surface is proportional to the electromotive force around the boundary of the surface. The magnetic flux is the surface integral of the magnetic field $ \vec H $, and the EMF is the line integral of the electric field $ \vec E $, so we have $$ \oint _ { \partial S } \vec E \cdot \mathrm d \vec r = - \frac { \mathrm d } { \mathrm d t } \iint _ S \vec H \cdot \mathrm d ^ 2 \vec A $$ using standard units and sign conventions. Applying the classical Stokes Theorem on the left and using that $ S $ is stationary on the right, this becomes $$ \iint _ S ( \nabla \times \vec E ) \cdot \mathrm d ^ 2 \vec A = - \iint _ S \frac { \partial \vec H } { \partial t } \cdot \mathrm d ^ 2 \vec A \text ; $$ since this holds for arbitrarily small surfaces, we conclude that $$ \nabla \times \vec E = - \frac { \partial \vec H } { \partial t } \text , $$ a differential equation. (The argument in reverse is even easier, since you don't have to worry about arbitrarily small surfaces.) 

It looks like the term ‘ideal-supporting algebra’ was written by me and survived slightly more than a decade on Wikipedia without being altered. (Well, somebody added a hyphen, a change that I agree with.) Since I put brackets around it, I'm sure that I must have heard the term somewhere, but I couldn't tell you now. Now that I think of it, a more precise term would be ‘ideal-supporting variety [of algebras]’. And if I search for that phrase, I find it in Eric Schechter's 1996 Handbook of Analysis and its Foundations (which for some reason Google Books has classified under Business & Economics). Since I was reading this book a lot a decade ago, that's probably what I meant all along. Shechter often invented terminology for his book, when terminology in the literature was inconsistent or missing, so I wouldn't be surprised if it's essentially unique to him. At this point, probably the best thing for me to do is to edit Wikipedia for a little bit, finishing what I started in 2002. 

I'm afraid that I don't like your proposed proof. You derive a bound on $f(x)$, namely $\epsilon + f(y)$, but this is not fixed. Although you may choose any positive $\epsilon$ you wish (which then gives you $\delta$), $y$ must be chosen to be within $\delta$ of $x$. So as you vary $M$, you vary $x$ (to keep $f(x) > M$), but then (to keep it close enough to $x$) you vary $y$, and it seems possible that $f(y)$ would grow fast enough that $\epsilon + f(y) > M$ would be maintained. 

Here is another, completely different answer. As Carlo indicated, one can use units in which Planck's constant is $1$. This is no arbitrary choice, but one dictated by fundamental physics. Similarly, one may set the vacuum speed of light to $1$. It is a historical accident (mainly due to our being massive creatures bound to a planet) that we think of length and time as being of different dimensions (in the sense of nonstandard analysis), so that we write $E = m c^2$, but really it's just $E = m$. Conversely, it's a historical accident (this time a lucky one) that we don't think of force as having its own dimension, so that we have $f = m a$ instead of $f = m a/g$ (where $g$ is Galileo's constant, about $32 \operatorname{lb}_{[M]} \operatorname{ft} \operatorname{s}^{-2} \operatorname{lb}_{[F]}^{-1}$, with a dimension of $[M] [L] [T]^{-2} [F]^{-1}$). This probably only happened because a clear distinction between the pound-force and the pound-mass came after Newton's laws (but before such other units such as the slug, the poundal, the gram, the dyne, and of course the newton). In electromagnetism, people often make do with only the dimensions $[L]$, $[T]$, and $[M]$, because they set Coulomb's constant to $1$. (Depending on where you put the $c$s, this is either the electrostatic, electromagnetic, or Gaussian system of dimensions; combined with $\operatorname{cm}$, $\operatorname{s}$, and $\operatorname{g}$ as the respective units of $[L]$, $[T]$, and $[M]$, this is called the electrostatic, electromagnetic, or Gaussian system of units.) Only the fuddy-duddies at the BIPM insist on making electric current an independent dimension $[A]$. (They also use $\operatorname{m}$, $\operatorname{s}$, and $\operatorname{kg}$ as the base units, so people write about this as ‘cgs vs mks’, when that's not what it's about at all.) Similarly, set Boltzmann's constant to $1$ to show that energy and temperature have the same dimension, and set Newton's gravitational constant to $1$ as well. Since of course $1 \operatorname{mol} \approx 6.02 \times 10^{23}$, all $6$ of the physical dimensions implicitly endorsed by the BIPM in the SI system of units (the candela depends on human biology) can now be seen to be utterly dimensionless! (The $6$ constants are Planck's, Maxwell's, Coulomb's, Boltzmann's, Newton's, and Avogadro's, and they are log-linearly independent, giving a unique solution to the system of $6$ homogeneous log-linear equations made by setting them all to $1$.) The point is: Every quantity is dimensionless, and every unit is simply some real number, so we may calculate with them as if they were real numbers because they are! (In fact, they are all positive real numbers, justifying our use of them in division and inequalities.) The $A^\bullet$ in my first answer is just $A$, and the group in Qiaochu's answer is trivial. Here is a problem: Although the constants we set to $1$ do come from fundamental physics, there is still some choice (even controversy) about how we do this. First, Planck's constant $h = 2 \pi \hbar$ derives from work on cyclic wave phenomena, and the really basic quantity is Dirac's constant $\hbar = h/(2 \pi)$ (which is also often called Planck's, so Carlo may have meant this all along). Similarly, Coulomb's constant $1/(4 \pi \epsilon_0)$ derives from work on spherically symmetric charge distributions, and the really basic quantity is $1/\epsilon_0$ itself (which, following the Gaussian system on placement of $c$, gives us the Heaviside–Lorentz system of dimensions when we set it to $1$). A similar remark applies to Newton's constant $G = c^2 \kappa/(8 \pi)$; Einstein's constant $\kappa$ is the more basic one. Planck himself, who first came up with all of this, not only used $h$ and $G$ instead of $\hbar$ and $\kappa$, but also used the charge of the proton instead of $\epsilon_0$ or Coulomb's constant, clearly a great error. So while every unit is a real number, different people disagree over which real numbers they are! (And not just because of experimental uncertainty, which is also an issue somewhat.) All of the possible different conventions to eliminate a given set of dimensions are mediated by a group of symmetries, the group in Qiaochu's answer, so keeping track of them all brings us back to the sophisticated answers that he and I gave. But the point is: You don't have to choose a convention. Since some convention is possible (and you already knew this when you saw your first system of units, however arbitrary it may have been), it is valid to say that every unit is a real number (even though which real number depends on the convention chosen), and so we may calculate with them as if they were (positive) real numbers.