Currently The PK is an AUTO_INCREMENT so I am wondering if using this kind of key will be efficient for this type of system. Also the main reason why i am having a asking this question is to help in my function I have been having some issues with loading more messages. due to the fact that messages are deleted the PKs will not be consistent such as . When I try to run the more function using 

I had a table to store user votes for other users. So user1 can vote for user2 (vote up or vote down). One user should be able to vote for many users, but not repeatedly to the same user; user1 can only vote once for user2. I do not think my table is properly structured to reflect this. Currently each time a user gets a vote up/down an ew row is created for this with either a 1 in the vote_up/vote_down column. What changes should I make to properly structure this table? 

Design 2 In this design I thought I would use a bridge/association/junction table to decide which network statuses are valid for a device. It looks like this: 

For the record, this design seems a bit absurd even to me, but this is my thought process. In this one, the presence of a in the table is equivalent to saying = true in Design 1. The has a foreign key constraint and is used to ensure only networkable devices are entered. Can do this using a CHECK constraint (see below), and by making a computed column that is equal to . 

Problem I'm trying to figure out how I would query the database if I don't know if a device is active or archived? For example, if a user has a serial number and wants to find out information about the device, and they are unaware of whether it has been archived. Option 1: Create a view based on a union all? Option 2: Query the active database and then query the archive if the first query returns nothing? The saga continues... An associate suggested that I eliminate the archive database and use a soft delete scheme. I built a model using this idea, and then started running into a host of other problems. Here are the same tables using the soft delete scheme. Soft Delete Example 

I have two sql boxes, one primary and one mirror. The mirror stopped synchronising last night for about 12 hours and so now the primary is transferring about 25GB of logs that built up. It's now synchronising again but I can't connect to the primary db server to check the status and to see how much there is left to transfer. Is it possible to do this through the mirror/witness? 

I have a an SQL server that normally listens on a private IP address (10.x.x.x). However, I would like to be able to connect to the database over the public (31.x.x.x). I tried to enable this tonight, but when I restarted the SQL Server service, the server wasn't listening on the internal IP address any more. I was using the configuration manager to let the SQL Server know the IP addresses it should be listening on. I am running SQL Server 2008R Standard. 

The issue I have with this design is I'm not sure if the relationship with and / is a good or bad design decision. Is propagating a non-key field like to other tables a bad design? I don't have enough experience with database design to make an informed decision. 

This question regards the proper use of NULL and utilizing CHECK constraints for business logic vs stored procedures. I have the following tables setup. 

This design eliminates the need to propagate across the tables. The issue I see with this design is that every device that has network capability will have records in paired with ids 1, 2 and 3, while devices that can't connect to a network will be paired only with id 4. It seems like a lot of extra records that all mean the same thing: devices that can be networked can only use statuses 1, 2 and 3, and devices that can't network only use 4. This design seems to be more "relationally correct", but also smells a bit. Update The following update proposes variations on Design 1. I come across situations like this often, where there are many ways to achieve the same end result. I never know how to tell if there are hidden problems with the designs, and I can't judge when to normalize or denormalize. Is one of these designs preferred over the other and why? Design 1.1 

To clarify users 1-19 can all vote for user 20 once each and user 20 can vote for 1-19 once each. At the end of the day I want to get a tally of how many votes each user has for both up votes and down votes each. 

I have a shopping cart table used to store customer id and product id of their chosen items. However I have multiple product tables, for example cars, books, houses. I started out with one table but then realized using an auto increment key for the product tables might cause a problem, as I will not know which table to join the shopping cart with. I originally started out with one table so I did not have this problem before, but now I have to expand it to accommodate two additional product tables. I am looking for some help in how to restructure this schema to properly store products to the cart table. Cars 

Due to this design's granularity, I could theoretically allow any mix of statuses for devices with this design, but I wanted to control it so I wrote some triggers to only insert the correct mix of statuses depending on whether the device is network capable. Triggers as follows: 

I put check constraints on the table as follows to ensure a network status and network information can only be provided if the device is capable of connecting to a network. 

With this setup, devices are archived by setting field to true and entering an . I could query any device easily whether it is active or archived. (Please ignore the field, as this is used for an unrelated concept). Take notice of the Phone subtype table, where I had to propagate an index of DeviceID and IsArchived flag because phone numbers must be unique for active devices. I have to do this with other subtype tables as well. I don't know if this is a good or bad design. This part really confuses me... What are best practices for handling soft deletes where foreign key values can be marked as deleted. The only thing I can think of is create a routine that searches for all records that are related to deleted data, and create a report for users to resolve the discrepancies. For example, if a table of locations is related to the devices table, and some locations are soft deleted, then the devices refer to locations that no longer exist and must be moved. By the way, I'm using MS SQL Server 2008 R2, and I plan on using Entity Framework 4 in my application. I value database maintainability over performance. Thank you for reading. 

I was just seeking some higher advice on whether or not having your main data on one database, then having separate server database to use for frequent updating and reading will prove to be an optimization or a failure. My logic was that sharing the load like this across 2 separate server databases will be quicker than packing it all in on one shared hosting Thanks for your time. 

In effect this query should return messages from the last 20 entered, but due to the PK being inconsistent sometimes no messages are returned. Eg. if the max_id is 200, 200- 20 is 180 and the messages with ID between this range are deleted or not associated with this members(userId) recipient(friendId). Any insight into this problem as to whether I should change the key type or change the query itself? Thanks for your time. EDIT Just a side note I used ASC because I wanted the newest messages to appear at the bottom of the DIV, while the fetched results would be older and appear at the top. 

I'm using SQL Server 2008R2. Last night, the host machine on which my mirror database was running basically went belly up. Thankfully the principle database was on a different host server and so was fine. However... The principle is working but says suspended. When i try to resume the mirroring, I get an error in the SQL error log giving an error number of 9004. A quick Google of this error number come back with this article. (tl;dr: transaction log is damaged) So, does this mean that the transaction log shipping between the principle and the mirror has somehow got screwed up? How do I fix this? Is it as simple as doing a full backup on the principle and a full transaction log, then restoring them both on the mirror database with norecovery switch on and then set up the mirroring again? Or will I need to do something more drastic? 

I normalized the tables to avoid using NULLs. The problem is that some of these tables depend on each other due to business processes. Some devices must be sanitized, and some are tracked in another system. All devices will eventually be disposed in the Disposal table. The issue is that I need to perform checks, such as if the boolean field is true, then the cannot be entered until the fields are entered. Also, if the boolean value is true, then the fields must be entered before the can be entered. If I merge all of these columns into the table then I will have NULL fields, but I will be able to manage all of the business rules using CHECK constraints. The alternative is to leave the tables as they are, and manage the business logic in the stored procedure by selecting from the tables to check if records exist and then throw appropriate errors. Is this a case where NULL can be used appropriately? The boolean fields and basically give meaning to the NULL fields. If is then the device is not tracked in the other system and and are NULL, and I know that they should be NULL becuase it is not tracked in the other system. Likewise, and I know will be aswell, and a can be entered at any time. If is , then and will be required, and if and are NULL, then I know they have not been officially removed from that system yet and thus cannot have a until they are entered. So it's a question between separate tables/no NULLs/enforce rules in stored procedures vs combined table/NULLs/enforce rules in CHECK constraints. I understand that querying with NULLs in the picture can be complex and have somewhat undefined behavior, so separate tables and stored procedures seem beneficial in that sense. Alternatively, being able to use CHECK constraints and have the rules built into the table seems equally beneficial. Any thoughts? Thanks for reading. Please ask for clarification where needed. Update Example table if they were merged and I allowed NULLs.