I'm reading the paper "the classification of algebras by dominant dimension" by Bruno J.Mueller, the link is here $URL$ In the proof of lemma 3 on page 402, there is a place I can't understand. Who can tell me what $E_R \oplus * \cong \oplus X_R$ and $_AHom_R(E,X)\oplus * \cong \oplus _AHom_R(X,X)\cong \oplus_AA$ mean? 

Let A be an algebra. We denote by by A-proj the full subcategory of A-mod consisting of projective modules. An A-module T is called a tilting module if $proj.dim(_{A}T)=n < \infty$, $Ext_{A} ^{j} (T,T) =0$ for all $j > 0$, and there is an exact sequence $ 0 \rightarrow _A A \rightarrow X_0 \rightarrow X_1 \rightarrow \cdots \rightarrow X_n \rightarrow 0$ in A-mod with all $X_j \in add(T)$. Let T be a tilting A-module of projective dimension $n \geq 1$, then fix a minimal projective resolution of T as follows: $ 0 \rightarrow P_n \rightarrow P_{n-1} \rightarrow \cdots \rightarrow P_1 \rightarrow P_0 \rightarrow T \rightarrow 0$. Then can anyone tell me why A-proj =$ add(\oplus _{i=0} ^n P_i)$? Thank you. 

Street-Fighting Mathematics by Sanjoy Mahajan is about estimation, Fermi calculations, dimensional analysis and so on. I haven't read it yet, but the title was certainly enough to get me to download it. 

When investigating regular languages, regular expressions are obviously a useful characterisation, not least because they are amenable to nice inductions. On the other hand ambiguity can get in the way of some proofs. Every regular language is recognized by an unambiguous context-free grammar (take a deterministic automaton which recognises it, and make a production $R \rightarrow tS$ for every edge $R \stackrel{t}{\rightarrow} S$ in the DFA, and $R \rightarrow \epsilon$ for every accepting state $R$). On the other hand, the natural "grammar" for a regular language is its regular expression. Can these be made unambiguous? To be precise, let's define a parse for a regular expression (this is I think a natural definition, but not one I've seen named before). 

It is known that solving systems of linear equations is reducible to SVD in a straightforward way; if you want to solve $\mathbf{Ax}=\mathbf{b}$, then you can perform SVD on $\mathbf{A}$ and minimize $||\mathbf{UDVx}-\mathbf{b}||$. However, is there a reverse reduction that is also very efficient? That is, if you can solve linear equations, you can solve SVD? EDIT: Because of Denis's comment/answer below, it looks like there isn't a reduction in general. But I'm interested in these problems over $\mathbb{C}$; so, the new question is: If we can solve linear equations over $\mathbb{C}$ exactly or approximately, can we perform an "approximate" SVD (for some suitable notion of "approximate")? The answer still seems to be in the negative, but I defer to people who actually know something about this. 

In short, the parses of a string tell us how a regular expression matches a string if it does. A regular expression $R$ is unambiguous if, for every $x \in L(R)$, there is only one $R$-parse of $x$. 

In an affine space $A$, the displacement (difference) between two points is a vector, and one can add a vector to a point, but not two points. However these can be replaced by a ternary operation in terms of points alone: the parallelogram rule $\nearrow : A \times A \times A \to A,\\,\nearrow(p, a, b) = p+(a-b)$. You can even add scalar multiplication of the difference into the bargain. Why would you want to do this? Well affine spaces are more primitive than a vector space -- yet we use a vector space in defining them. To me the more natural approach is to define them without it, and watch the vector space (of displacements) drop out. 

The celebrated Big Theorem of Picard's is that, in every open set containing an essential singularity of a function $f(z)$, $f(z)$ takes on every value (except for at most one) of $\mathbb{C}$ infinitely often. Now - is the converse true? Is this a way to characterize the existence of an essential singularity of a function? For example, if you're given a non-constant function $f(z)$ that is holomorphic on some open set $\Omega$, and you know that there is an accumulation of 0's towards some point $x$ on the boundary of $\Omega$, then do you know that there must be an essential singularity at $x$? 

One version of Hensel's Lemma is the following statement: Let $R$ be a commutative ring with a unit. Given a polynomial $Q\in R[X]$ and a root $\alpha$ of $Q$ modulo some ideal $I$ (i.e. $Q(\alpha) \in I$), assuming some non-degeneracy conditions (e.g. $Q$ is square-free), then for every $t > 1$, there exists $\beta_t \in R$ such that $\beta_t = \alpha \mod I$, and $Q(\beta_t) \in I^t$, and furthermore, $\beta_t$ is unique. The multidimensional generalization of Hensel's Lemma is often presented as: Given $f_1,\ldots,f_n$ in $R[X_1,\ldots,X_n]$ and a simultaneous root $\alpha \in R^n$ modulo an ideal $I \subset R$ (i.e. $f_i(\alpha) \in I$ for all $i$), assuming some non-degeneracy conditions (e.g. $\det J(\alpha)$ is a unit), there exists $\beta_t \in R^n$ such that $\beta_{t,j} = \alpha_j \mod I$ for all $j$, and $f_i(\beta_t) \in I^t$ for all $i$. Here, $J(\alpha)$ denotes the evaluation of the Jacobian of $f_1,\ldots,f_n$ on $\alpha$. My question is: is there an intermediate generalization in between the univariate case and the multivariate case above where we only consider one polynomial $Q\in R[X_1,\ldots,X_n]$, and we simply want to lift roots of $Q$ modulo an ideal $I$ to roots of $Q$ modulo $I^t$? It seems intuitively like an easier thing to do (we don't require simultaneous solutions to a system of polynomial equations). Does this intermediate generalization exist, and if so, what non-degeneracy conditions would we require? Thank you! 

$A$ and $B$ are said to be $Morita $ $equivalent$ if the category $Mod A$ and $Mod B$ are equivalent. $A$ and $B$ are said to be $derived$ $equivalent$ if $\mathcal{D}^b(Mod A)$ and $\mathcal{D}^b(Mod B)$ are equivalent as triangulated categories. Given a minimal injective resolution of $A$ as an $A$-module$$0 \rightarrow A \rightarrow I_0 \rightarrow I_1 \rightarrow \dots$$ If n is maximal with the property that all modules $I_j$ are projective for $j<n$, then n is called the $dominant$ $dimension$ of $A$. 

Let $A$ be a left coherent ring, that is, a ring for which the kernel of any homomorphisms between finitely generated projective modules are finitely generated. $T^{\bullet}\in \mathcal{K}^b(A-proj)$ is a tilting comples over $A$ with $B = End_{ \mathfrak{D}^b(A)-mod}(T^{\bullet})$. Then we know that $\mathfrak{D}^b(A-mod)$ and $\mathfrak{D}^b(B-mod)$ are equivalent as triangulated categories. I have seen in a paper that"There is a $F:\mathfrak{D}^b(B-mod)\rightarrow \mathfrak{D}^b(A-mod) $ an equivalence of triangulated categories such that $F(B)=T^{\bullet}$. $F$ induces an quivalence $F'$ of the homotopy categories from $\mathcal{K}^b(B-proj)$ to $\mathcal{K}^b(A-proj)$". So I want to ask: 

In an Italian museum (probably the Leonardo da Vinci Museum in Florence) I saw a compass for drawing arbitrary conical sections. I believe the legend mentioned only ellipses, but it could in principle draw the others too. The basic principle is that the "central" arm (in general, the focal arm) of the compass is held at a fixed (per drawing) angle to the desk, while the pencil arm adjusts in length (so that it is shorter at the perigee and longer at the apogee). 

One can consider programming formalisms where a program is a pair (M, P), with M a Turing machine and P a proof that M halts on all inputs. Then, if the proof is correct, (M, P)(x) = M(x); if it is incorrect, (M, P)(x) = 0. Alternatively, one can consider primitive recursion with an "oracle" function: so, for example, we can allow the Ackermann function in our recursion bounds. If we use a primitive recursive oracle, we gain no power; but as long as we use a computable function, our functions remain computable. This should give a hierarchy with respect to the oracles, but I'm not sure of the details. 

Let $A$ be a k-algebra,where k is a fixed field. We denote by $\mathfrak{D}^b(A-mod)$ the bounded derived A-module category. A complex $Z^{\bullet}=(Z^i,d^i) \in \mathfrak{D}^b(A-mod)$ such that all $Z^i$ are finitely generated projective is quasi-isomorphic to the following complex $$ \cdots \rightarrow 0 \rightarrow Coker(d^{-n-1}) \rightarrow Z^{-n+1} \rightarrow \cdots \rightarrow Z^{-2} \rightarrow Z^{-1} \rightarrow Ker(d^0) \rightarrow 0 \rightarrow \cdots $$ So we can see $H^i(Z^{\bullet})=0$ for $i <-n$ or $i >0 $. Then how can I get that $Coker(d^{-n-1})$ is finitely presented module? And how to get the projective dimension of $Coker(d^{-n-1})$ is finite? 

How to show the functor $F=D Hom(-, I_0(\Gamma)): S \rightarrow mod End_{\Gamma}(I_0(\Gamma))$ is an equivalence? If the dominant dimension of $\Gamma \geq 2$, then $\Gamma \in S$. Then how to get that $F(\Gamma)$ is a generator and cognerator of $mod End_{\Gamma}(I_0(\Gamma))$? (A module is called a generator and cogenerator if it contains all idecomposable projecitve and injecitve modules as direct summands)