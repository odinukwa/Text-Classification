I ran into a puzzling configuration anomaly with a SSIS package establishing a OLEDB connection to a SQL Server 2012 availability group. The DBAs administering the AG requested that all developers target a secondary node by specifying the AG listener name and the DB part of the AG, and using as an additional parameter in their connection strings. The routing to the secondary works with using the -K switch but SSIS packages were still being routed to the primary. An example of the connection string they requested is below. This was utilized as a project level config item in the SSISDB catalog. 

Server A and Server B have identical hardware and instance configurations (A is Production, B is QA). B's DBs were restored from A's backups from one week ago. I was provided this query by the development team. 

The sequence may be a bit off for a piecemeal restore you're attempting, in particular when to apply the tlogs. This is what TechNet has documented. 

The filegroup [DATA_1995] remains in the last partition position for values beyond the upper-most boundary. Can this last filegroup be changed after the partition scheme is initially created or is the only way through creation of a new partition scheme and rebuilding all indexes utilizing the old scheme into the new scheme? 

I'm trying to figure out the peak load I can sustain on new hardware I procured for validating backups and performing DBCC checks. I've been using Crystal Diskmark to get throughput stats which helped me benchmark sequential I/O for the copy/restore tasks. I'm having trouble gauging how much random I/O I can sustain for the DBCC check. I'm thinking about using iometer and sqliosim but want to know config would work best to simulate a DBCC check. The hardware I'm testing consists of one R720 with dual E5-2609s for 8 cores, 32 GB RAM, Windows 2008 R2 Standard, SQL Server 2008 R2 Standard with SP2, and a PowerVault 3620f with 24 15k SAS spindles hooked up to two dual port HBAs on the R720. I've been experimenting with 4, 8 and 12 spindle RAID 0 groups (I can afford to lose the fault tolerance as the DBs have a life expectancy of minutes as part of the testing process). I'm thinking I can run multiple simultaneous DBCC checks with the above hardware without hitting disk contention. I have the option to upgrade the RAM to 64 GB and the O/S to Enterprise but probably can't upgrade the SQL to Enterprise due to licensing costs. Any suggestions on how to determine the max random I/O for DBCC using iometer, sqliosim or another utility would be deeply appreciated. 

In the event of a DBA leaving an organization, what options can the surviving team members pursue to change passwords for service accounts portfolio-wide? While using Configuration Manager appears to be the de-facto method, can PowerShell or another scripting or batch language be used to reset them in bulk? I figure if you're changing the login name for the service accounts, Configuration Manager is essential as registry permissions and the like need to be propagated. For just a password change to the existing account however, is a scriptable method OK as long as it's performed during approved maintenance windows for the service restarts? 

I'm not planning on creating 2016Q3 and 2016Q4 quarters as I don't want to incur the data movement between the filegroups. I've elected to start with 1/1/2017 and create a 2017Q1 filegroup. I execute the following, anticipating that it'll be a quick meta modification. 

Is this expected for a FlashArray config or can it be configured at the controller or LUN level to be more optimal for reads than writes? The host will host a data warehouse with tables already in the 250 GB+ range. 

This may be a good candidate for Biml. One approach would be to create a reusable template that would migrate data for a single table in small date ranges with a For Each container. The Biml would loop through your table collection to create identical packages for each qualifying table. Andy Leonard has an intro in his Stairway Series. 

I am looking to eliminate an extraneous data file in a filegroup in my DB but don't want to use the dreaded DBCC SHRINKFILE method, I am preferring to rebuild indexes into an alternate FG, perform the shrink with EMPTYFILE followed with removal of the file and then rebuild back into the original filegroup. Is there a method to identify which table/index objects are populating the utilized pages/extents in the target database file? 

I went through the same exercise deploying hardware along the lines of Microsoft's Fast Track Data Warehouse strategy. Their Fast Track Data Warehouse Reference Guide for SQL Server 2012 covers the topics you discovered and already tested, like MCR (maximum consumption rate) for a query with data already in the buffer pool and BCR (benchmark consumption rate) for a real-world query where some, if not all data, needs to come from the disk subsystem. Query complexity also affects the overall throughput (using a TPC-H benchmark dataset, their example shows throughput rates from 56 to 201 MB/s per core). 

I've discovered that issuing from the console of a SQL host in my network only reveals other instances on the same subnet. It used to reveal all instances on the network so I was wondering if an O/S or network firewall rule is blocking polling attempts outside the subnet. What firewall rule can I asked my network engineers to establish to allow polling for instances beyond the immediate subnet? 

Pinal Dave has an excellent script identifying object dependencies across databases. SQL SERVER â€“ Find Referenced or Referencing Object in SQL Server using sys.sql_expression_dependencies 

Test script using SQLIO where the param file is directed to 40 GB test file on a 3 TB XtremeIO Flash Array LUN 

I came across an unusual scenario on an instance where only half the physical RAM was being utilized but I was observing PLE values between 1-3 seconds constantly. 

One book that I've personally found best describes the calling of a DBA is DBA Survivor: Become a Rockstar DBA. It goes beyond the technical and talks about your relationship to your customers and your responsibilities to the tradecraft. It helped reaffirm my belief that the DBA profession is an essential one that is challenging, honorable and rewarding. 

If there's enough space still left, the best approach may be to 1) create a new filegroup, 2) rebuild the index(es) for the table(s) into the new filegroup with the ONLINE=ON switch, and then 3) run a SHRINKFILE operation to reduce the footprint of the original filegroup. The rebuild operation is multi-threaded while SHRINKFILE last time I checked is a single-threaded process and inherently slower. You also mitigate the fragmentation side-effect by doing the rebuild up front. The catch is having enough space, but if you can rebuild enough indexes into the new FG before the volume capacity is reached, you'll have an overall faster SHRINKFILE operation to reclaim the space from the original FG. 

I'm setting up an ETL process using SQL Server 2014 and SSIS. My Bulk Insert Task is utilizing a source file and format file on ShareA targeting a DB on InstanceA. The package is run via SSA job on InstanceB. Both InstanceA and InstanceB are running the same service account for the Agent. The service account has and read/write privs on the target table. It also has appropriate permissions on both the share and folder via NTFS. When the SSA job runs on InstanceB, the job fails on the step with code 5 for Access Denied attempting to access the source file. However, if I run the T-SQL generated by the task in a job on InstanceA it succeeds, confirming share/NTFS permissions are OK. Am I forced to run the SSIS package on my target server InstanceA to allow to function or are additional permissions needed in order to run on an alternate server? 

I'm in the midst of creating a set of SSIS 2014 packages (using Visual Studio 2013) importing data from 50 tables. Since most of the tables are being filtered on the same column and I'm importing all columns into my staging area, I opted for a reusable design where my package name matches the table name, my OLE DB source query is defined with an expression-based variable (starting with ) and my OLE DB destination table is also defined with an expression-based variable (in this case schema.tablename). Both the source query and target table have matching column names and match in most cases on data types and sizes. As I'm cloning my packages for each table, I copy/paste a previously-created package and visually inspect the mappings to confirm the expressions and the data flow mappings are valid for the new package name. I'm seeing that the expressions are working as planned, but I have to manually do the drag/drop mapping of most columns in the destination editor. Is this expected behavior of the editor as the design metadata of the original package is invalid under the new package name and is there a way to have Visual Studio delete and remap all columns in the data flow based on column name? 

Min/Max Server Memory were set at defaults and the most resource intensive query was the backup history check by Quest Spotlight. Only 1.6 GB of RAM was utilized, 1.5 of which by buffer and procedure cache. After setting Max Server Memory to 3 GB, PLE started to climb and is currently at 21 minutes. I am wondering why Max Server memory would have this positive impact on PLE when less than half the onboard RAM is currently utilized. My first thought is O/S memory trimming was in play before the change in the setting but how can I confirm this? 

But right now, it's been running for 90 minutes. I'm watching via Spotlight that the new data file is being populated. I spot-check the UTC dates on all the tables in the DB in Prod and confirm nothing is dated after 1/1/2017. I can understand the engine needing to seek/scan indexes to confirm nothing has to move to the FG, but if no records are qualified to move, why all the data movement? 

One quick approach to figure out the structure and delimiters of your files would be to use a desktop application with a data import function. MS Excel works very well for smaller files but I find MS Access works very well for larger files and has the advantage of suggesting suitable data types that can port over to SQL Server later. You can toggle the delimiter options in both wizards to see how the file would be parsed. One distant possibility is that the file is fixed width so that the values begin and end in specific column positions. For example, first name can be in the first 15 characters of a line while last name would be 15 characters starting in position #16 on a row. Because of the spaces you're seeing, the space may be the actual delimiter. Visual inspection would be the way to confirm this.