You would be better off using the nid command to change the dbid to a new value. then register your database with the rman catalog, if you use one. 

That should be pretty simple. You should do this from rman. Also run a "show all;" to see how your backups are configured. 

$ORACLE_HOME/dbs is used to store the spfiles, init files and password files on Linux and Unix systems. But as you pointed out the dbs directory not used on windows servers. So on a Windows servers it is probably not needed. 

The following transformations may help. I have not used the DBMS_XSLPROCESSOR.CLOB2FILE method, but I did use these to migrate an Oracle database from Solaris to Linux. I could not use data pump because of the version of Oracle that they were using and the fact that they used XML data types for column data types. 

You need to restore/recover your database. If you can't then yes the code is buried in the system table spaces. But it won't be easy looking through the data file with a hex editor trying to find the code. You should either try to restore the database or just rewrite the code. Do you have a backup of your database? Do you have all of the data files, control files, online redo files? If you don't have a backup and your database is in an inconsistent state, then you can probably alter database recover until cancel and then point Oracle to your online redo logs to see if it can find what it needs. Otherwise you might be out of luck. What is in your alert log for your database? 

That is a tough one, if only there was a way to find things like that online. $URL$ Make a list of the high level objects that need, such as planes, airports, routes, people, roles, etc. Then figure out how they are related, how you identify each object and what attributes they would have. 

Once you get the SQL to run, go to your diag trace directory, assuming that you are using 11g or 12c. Then use tkprof to generate an explain plan along with stats on physical IO and logical IO. 

Find a small recordset and see how many loops your code does. How many does it do for 10, 100 and 1,000 records? You will see that the number of loops increases exponentially as the number of rows increases. My rough guess is that you have asymptotically big O(2^n+2^n+2^n). Asymptotics is a way of estimating work where the amount of rows processed is not known. Can't you just write some minus selects that are union all joined together? My example is looking at indexes and I use different filters so that the data sets will not be the same. Hence you will see the difference. 

If you know the sid name, you can copy a pfile over and edit it to change the values to match the database name. Make sure that you set B_CREATE_FILE_DEST to where you want the data files. Then do a startup nomount on the database. In RMAN do "CATALOG START WITH '/full_path_to_your_backup';" You can then use DB_FILE_NAME_CONVERT to convert the file names to be in the right directory. You can then try to restore a control file. try list backup; and restore the oldest control file. Mount the database and try to restore the database. It could be that the reason why the people who gave you the backup are avoiding you is because they know the backup is incomplete. But you have to try to restore what you can, then at least when you go back to them you can ask a more specific question. 

There is an Oracle doc that you might want to look at, Doc ID 1681266.1. You want to make sure that you are using 4K block sizes for storage. If you are moving to a new server and you are running at least Redhat 6.5 or Windows 2012, than that should not be a problem. Using a 4K physical block size will give you better performance. 

Also under sqlplus you can set the number of days for the control files to track the backup information for RMAN. 

You should be able to backup an 11.2.0.2 database using RMAN with an RMAN repository that is set to 11.2.0.4. There seems to be some invalid objects in your RMAN repository schema. You may want to try compiling all of the invalid objects in your RMAN repository to see if they become valid. This probably has nothing to do with you backing up an 11.2.0.2 database with an 11.2.0.4 RMAN repository. 

You really need to generate an explain plan. Since your query has an OR there may be situations where the query is being short circuited based on data not existing, hence the performance is not consistent. Also, don't join everything together, subqueries with low cardinality put at the top in a WITH subquery_name clause, and join similar tables together, then join them with other tables. 

I addressed this with the PL/SQL project manager years ago, and was told that it is not possible. You can't address the :NEW or :OLD columns as a record and convert them to anything. You can create a temporary table that is either identical to the table that you are writing the trigger for or generic, i.e. table_name, column_name, column_type, clob. You would then insert the :NEW or :OLD values into the temporary table and you can access those values with an after statement trigger. Then you can read the data in an after statement trigger. 

If it was me, I would partition by hour, then you can just drop 24 partitions every day. That is only 744 partitions per month. Then if you query within a relatively short window, you will look at much less data. You may also want to look at your index strategy. My guess is that the cost of maintaining the indexes is too high. IMHO, smaller partitions should help speed up the queries and make it easier to maintain the partitions. 

You should alter your tables and indexes and specify the amount of parallel that you want. How Parallel Execution Works 

I took some code snippets from a backup script that I use. You can redirect all output with the exec command, then redirect it back. You can also surround everything with braces and redirect that to a log file. Personally I think bash is a better shell to program in, if you don't have bash ksh works too. 

You can either "drop user test_user cascade;", then create the user again with all needed grants and objects, or drop the database and restore a backup of the database while it is empty. Depending on the number of tables, indexes and constraints one of the two methods will be faster. The more tables, indexes and constraints, the more likely just dropping the database and restoring a backup of the empty database will be faster. 

I assume that you are practicing backup and recovery. The best way to do this is to do the following: 

Your professor should have taught you the mod function. You can use rownumber and mod to come up with a number from 0 to n where n is one less than the number that you divide by. Hence mod(rownum, 37)+1 will give you a random number between 1 and 37. This gives you between 4 and 5 people working at every location. Sounds oddly random. Except it would not be random for a database class. This query should help you to get on track. 

Not likely. It is possible that your procedure is being run more than you realize? How does the procedure get run? 

My feeling doing a Merge is the same as doing a select statement, then inserting data that does not yet exist in the target table and updating data where the primary key is already there. It sounds like you are saying that all of the data should be there in advance and you are just updating some columns based on new data. You could do a minus query first and if that returns rows then raise your exception. Otherwise you would do either a merge command or simply an update command. Row level triggers tend not to scale to large volumes of data. if you are doing batch processing, then you probably only have one process changing the data at a time, so worrying about concurrency might not be relevant. Why is it an error to not have data? 

Commands to restore database without needing to apply redo. If you don;t want the database to roll forward make sure that there are no files in the archive log directory. You can tar and delete them or just move them elsewhere. 

If you use PL/SQL collections, then the amount of memory needed to run a stored procedure is entirely data driven. How much data are you caching with each stored procedure? 

I sounds like you are doing partitioning based on a virtual column. virtual_column_based_partitioning 

You probably want to use a contextual index. Then when you search in the clob column you would do something like 

Here is a url that explains set events. The two events that you want are 10046 and 10053. Oracle TKPROF & SQL_TRACE You can generate better data on how your query is running by doing the following. Using sqlplus turn on tracing, set the trace file identifier so that you can find your trace file. 

You should be able to do that with an RMAN backup. You need to see what kind of backup Oracle XE supports. But you should be able to shutdown the database, startup mount, then do an RMAN backup making sure to tell Oracle where to send the backup, make sure that you include the control file with the backup. Then copy the backup over to the 64 bit server and inside RMAN use "CATALOG START WITH 'D:\backups';" or where ever the backup is. You need to copy over the init/spfile, create a service on the new server, startup nomount the new service, in rman, connect to the new database service, restore the control file, make sure that the control file restored, then restore the database. You will not need to recover the database since it was done while the database was not open. You then need to find and run the script in %ORACLE_HOME%/rdbms/admin that will convert the database to 64 bit then upgrade the database to 64 bit. It would help if you have a 10g 64 bit home on the new server, but I was not able to install 10g on Windows 2008, it just won't install, so you probably need to run the upgrade manually. You can also upgrade as a 32 bit database on XP and then migrate the database as 11g. Another option is to create a new 11g 64 bit database on the new server and use data pump to migrate it. Regardless you will need to test the process and probably try it more than once. 

You can use PL/SQL and schedule jobs to populate tables or schedule jobs to update materialized views. Then once the data has been updated you can query as much as you want, since the heavy lifting will be done. You should also try tuning the queries. If you have an index with the filter columns and any other columns that you need, then you can avoid reading the index then the table. If you start indexes with the low cardinality columns going towards high cardinality columns, you can reduce the amount of rows looked at. Look at materialized views and including rowid so that you can try to do a fast refresh. 

Has he tried "SELECT COUNT(*) FROM smahala.fishtable;"? If you grant select on a table in one schema to another schema that user should be able to access it by including the schema name in the query. However, if NOAA is using virtual private databases, then there may be other obstacles that prevent you from sharing data. If there isn't much data, you can try to export to a flat file and have the other person import the flat file. but this should probably get resolved by your internal DBA's. Since they know how the database is configured. You might also want to verify that you are using the same Oracle database. 

This error is caused by having more columns in the table then the columns that exist in your insert statement. You can resolve this by adding the missing columns to the values clause or by specifying the columns that you are inserting and making sure that you have the same number of columns as you do values bound to those columns. 

The max limit on memory that you have depends on how much physical memory that your server has, whether it is 32 bit or 64 bit and what OS you use. For example with 32 bit Windows the limit might be 2GB of RAM, depending on what version of Windows server you are using and whether or no you have a certain flag set that would allow you to use up to 3GB or RAM. If you are using a newer 64 bit OS, then you would have no practical limit to the size of the SGA. It would only depend on the amount of physical memory. So you should provide the following: 32 bit/64 bit, operating system and version, hardware type including chip set. 

It looks like you need to write dynamic SQL. Dynamic SQL is SQL the creates SQL that can get run. Here is a simple example of dynamic SQL. 

It looks like a permission issue. As Raj pointed out make sure that export_user has select privilege for the accounts table. Which it will have if it owns the table. Also make sure that it has select, insert, update and delete for MIKETESTCONSOLE_OWNER.ACCOUNTDEPLOYMENTLINKS and all of the tables in that schema. If tables are added over time, you probably need to add grants. 

Try checking the alert log to figure out what sequence it needs next. If you have the archive logs online or in a backup online you can restore the missing archive log files and copy them over to the archive log directory to see if you can get the standby caught up. If you are running backups of your primary database, then just doing an incremental backup might not work because you still might miss changes from before the incremental backup. If you can't resolve the gap in archive logs you might need to restore the standby database again. 

If it was me I would use an ERD tool like Erwin to generate an ERD diagram of your database. You can use Erwin to take the SQL Server ERD and generate an Oracle equivalent. I would then generate all of the table creates from the ERD tool and either with the tool or manually generate the DDL for the constraints, indexes and triggers. Once you do that you can create a schema with all of the tables that you need and create a database connection to SQL Server using heterogeneous services. At that point you have all of the tables that you need in Oracle without any constraints, indexes or triggers. You can go through each SQL Server table and do an insert into as select from. Finally you would run the script to create constraints, indexes and triggers. Since T-SQL and PL/SQL are not compatible, you will need to rewrite stored procedures by hand, including triggers. Erwin has/had a macro language that allows someone to write code that can translate to either SQL Server or PL/SQL and I managed an application that worked with both almost 15 years ago. But if you are migrating once, I would not bother with it. Based on personal experience, this process works best if you can script the process and run many iterations of your migration process. Then test after each iteration. 

Depending on whether you are running Windows or Linux, for the database i would have a non data guard standby, where you ship redo to the database throughout the day. Since you have standard edition, you can either use rsync or robocopy to incrementally copy the files that are not already there. If you run the commands from the standby server you can manually apply the archive logs to your standby database. Regarding the web component, If you leave it shutdown you can incrementally copy over the configuration files. I'm not sure how clean that will be. Maybe you shutdown JBoss prod once per day and copy over everything that has changed. You should do some testing with that. 

You may want to run an EC2 instance as a dev environment. Given that you don't have access to the file system on an RDS database and that you have standard edition, you can't turn on 10046 and 10053 events and use tkprof to read the files, you can't use active session history (ASH) and Automatic Workload Repository (AWR) since you need to have enterprise to use those reports and Oracle would know if you used any of those features in an audit situation. Your best bet is to look for blocking locks in prod and use and EC2 instance to do load testing possibly using set events 10046 and 10053. There might be another tool that you can use in RDS. I don't know what that would be. You are limited in what you can do give that you are running standard edition on an RDS instance.