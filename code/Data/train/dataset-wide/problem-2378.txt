Now let's build some procs that will DROP/CREATE or REBUILD, plus log (approximately) the time taken: 

This is an application issue more than a database issue. Navision treats datetime entries as UTC, and is adding the hour in its display (assuming your local timezone is UTC+1). I have run into this myself since I work with Navision. If you have a client application, you can look at displayed datetime for a record and compare it to what you see by querying the table. 

Short answer: start SQL Server with the -m flag from a local admin account. Step-by-step instructions here: 

Here are my own results: average time in ms for DROP/CREATE: 2547, for REBULD: 1314 It looks like in my contrived example, the winner is REBUILD. Constraints What if the index was created to support a constraint? In this case, a simple DROP and CREATE will fail because you have to drop the constraint first. Let's look at the clustered index from the previous example. 

This demo shows that a delete can produce a very unbalanced b-tree, with practically all data on one side. 

To answer the question in the title, whether the B-tree rebalanced during a delete, the answer appears to be no, at least in the following minimal test case. The following demo runs commands that are best left for a test environment. 

But appeals to authority are boring, so let's test it ourselves! Performance: Set up a table with a non-clustered index, load it with junk, and build a table for logging time. 

Note how the first open transaction (Transaction ID 0000:000002fa for me) isn't committed until the end of the REBUILD ALL, but for the index-by-index rebuilds, they are successively committed. 

The most promising exploration comes from removing the trash variable and using a no-results query. Initially this showed NOLOCK as slightly faster, but when I showed the demo to my boss, NOLOCK was back to being slower. What is it about NOLOCK that slows down a scan with variable assignment? 

Bad news: the plus approach in your script completely flattens out the XML document, and will need to be reworked if you have any multiple-X-per-Y structures in your document. If the dynamic SQL is a requirement, then please provide an example to test that aspect more easily. Another approach might be to build queries you need manually. If your fundamental problem is including parent info, then you might modify my demo SELECT query below. (N.B. Mikael Eriksson's answer has an improved query. Please refer to that.) Key ideas: is the context node, and gives you the parent is a different kind of XML function that belongs in the FROM section of the query and is usually seen with CROSS APPLY. It returns a pointer per match, and is what allows working with multiple rows. Read more here. is one of several methods for letting SQL Server know the value method only has one piece of data to work with (fixing the "requires a singleton" error) 

According to Paul Randal in his Pluralsight course "SQL Server: Logging, Recovery, and the Transaction Log" the LSN is composed of three parts: 

Understanding 1 is correct, and spaghettidba and Joe have good explanations. If you're interested in testing for yourself (on a test instance please), you can use the below script: 

I'm fighting against NOLOCK in my current environment. One argument I've heard is that the overhead of locking slows down a query. So, I devised a test to see just how much this overhead might be. I discovered that NOLOCK actually slows down my scan. At first I was delighted, but now I 'm just confused. Is my test invalid somehow? Shouldn't NOLOCK actually allow a slightly faster scan? What's happening here? Here's my script: 

I then run this update statement against the Phone table to default every other patient's phone number to 555-5555: 

Thanks to the suggestions from others, and I appreciate the time anyone took reading this post. So after looking at this different ways, and taking several breaks, I now see that my validation query was at fault. The join was incorrect. The Update statement was designed correctly, I verified the data was valid in the tables, the relationships correct, and I was able to verify at a detailed level the update worked. Something about my validation query (that did aggregations) seemed off. My validation was incorrectly joining against the wrong foreign key. The join: 

In theory, the data model allows for phones to be related to more than one patient. We did have some issues in the past, and most of the phone numbers have been denormalized. I need help to understand: 

Thousands of records I did NOT want to update were updated. Many of the other groups of Patients were NOT updated. 

I would tend to believe the data (phones being shared) is likely the cause of this problem, but I am having trouble proving that. Can someone help me with this? Thanks for your time. 

PowerBI Integration is not enabled on either of the 2016 servers, and is obviously not a part of the existing 2008 R2 configuration. I do not wish to use this feature. How do I solve this problem and begin to create subscriptions, so I can ultimately migrate all of the reports? 

I am planning a migration to 2016 Reporting Services and have noticed a particular problem. On two separate new installs for SSRS 2016 Developer Edition, I am unable to save a subscription to a report. The report was downloaded as RDL from the current 2008 R2 server, then saved to the new 2016 environment. The new server is build 13.0.4466.4. I also installed the same version of SQL onto a second server and have reproduced this problem. All servers are running in Native Mode. Within Report Manager, I am able to view the contents of the report (on-demand). When I try to create a subscription, the web page is stuck with the "Loading...." prompt after I click 'Create subscription'. It does this regardless of the Render Format I choose. I am attempting to deliver via e-mail. I have domain credentials for the SSRS service account configured, and I have put the IP address for the SMTP server in RS Configuration Manager, so I know it is configured for email ok. In the SSRS log file, I see these types of errors at the time I try to create the subscription: 

The relationship: Patient to PatientPhone - 1 to Many Phone to PatientPhone - 1 to Many In order to validate before and after, I have been using this query to sample the phone numbers by grouping the first couple of digits. It gives me a wide distribution of values, as I would expect: 

My understanding of SQL security is such that in order to have access to the server, you need a login. To access a database on that server, your login needs to be associated with a user in that database. Over time I have been consolidating security so that developers, QA, etc. are in various roles implemented as active directory groups. Those groups have logins on the server, so individual windows logins are not so prevalent on SQL instances. I grant proper accesses to the databases through these roles. A common service I provide is to refresh non-production versions of databases from production. I'll do a backup and restore of a production database onto a development server, de-identify the data, rename it, etc. I have noticed something recently in my environment that is troubling. I was granting access to a key table in production for a development team, when I noticed by using 'fn_my_permissions' that one of the developers has read and write access to the entire production database, rather than being restricted to read-only as I designed. I compared all of this developer's group memberships and found another group he was a member in that was assigned read/write - but only as a database user. There is no associated login for that group on that SQL server. It is typical here to copy a 'baseline' production database and repurpose it elsewhere - whether as another production database, or a non-production version. I am starting to find other examples of this anomaly. To sum it up, the problem I see is that when a database is copied to another server, the users in that database that were previously linked to logins on the original SQL instance seem to retain the same level of access on the new SQL instance - even if there is no related login for them on new SQL instance! This strange effect seems limited to users representing windows group logins. Here is a summary of what I am seeing: Server1 has a login tied to windows group A, which has been granted read/write to database X. Database X is then backed up and restored onto Server2, with logins but none related to group A. The database still shows a user for group A, and when I run 'fn_my_permissions' for a user in that group he has read/write to the database. This particular server is running SQL Enterprise 2012 SP3 with the latest CU. I've seen the same thing on servers running 2008 R2, as well as a current build of SQL 2016. Why is this happening? And what can I do to ensure that users do not inadvertently circumvent the controls I have put in place? It seems obvious I need to remove all of the unnecessary users after the databases are copied, but why? 

Say I have a table with 3 columns: Say ID has an index on it, name has an index on it, and there is another index which combines id and name. Lets say I now have an update statement which looks like this: 

This way I could store all the message (under 1mb) inside of this one entity, but I would have to keep track of which entity each user is at (if the user exceeds the 1mb limit I will have to create a new entity). This proved to also not be too efficient because if I send a message to perform 2 gets to see which message entity I am currently at and which message entity they are currently at. From there I must now use another 2 reads to get those entities and another 4 writes to update them both (considering I do not need to create another entity if it is full). I was looking for any ideas to accomplish what I need in a more efficient way. I have nothing implemented yet so I am open to ANY ideas. My main concern is for this to be efficient, Cheers! 

I am only updating the AGE column, so will MySQL still update the indexes even though non of the indexed columns were modified or will it leave the indexes alone? 

I am looking to expand my single MySQL database into multiple instances to provide High Availability. As well, I am looking to be able to have read replicas and to shard in the future. I have looked for options andnoticed two external tools called MaxScale and Fabric. I was wondering what the difference is between them? 

Additional "would like to have feature, but can live without it requirement if it effects performance considerably" would be to have a user view their messages with another user: 

I have a fresh install of MySQL 5.7 on my Windows machine. When I make a connection using root@localhost, I am able to connect, but when i try to make a connection root@192.168.1.10 (the private IP address of my server) I get the following error: mysql 5.7 access denied for user 'root'@'192.168.1.10'. I opened my my.ini file and added the line bind-address=0.0.0.0. I saved the file and restart my MySQL instance, but still not luck the same error appears. I then tried: 

I will be using Amazon Web Services RDS service to host a MySQL 5.7 server (Currently AWS do not support 5.7, but they have announced they are working on implementing this version and it will be out soon). I will only have one table, but I expect it to have MANY rows. It is a messaging table which will contain messages from users to other users. A Message can only be sent by one user to one other user. This message when read for the first time will need to be updated stating it has been read so it will require one update in it's life if it has been read. A User must be able to view all thier sent messages ordered by newest and all the messages they received ordered by newest. My initial design looks like this: Messages Table: 

1) User would login using their username and password 2) User could get friends by getting their USER entity based on their username 3) User could add/remove friends by getting the entity of user1 and user2 and either adding or removing friend via transaction to make sure they are consistent. 4) User could get all the message they have sent by using indexing the the 'from' attribute (limit of 10 message per request). The same could be done to view all the messages they have received by using the 'to' attribute. When the message has been seen for the first time I would go to the message (1 get) and update the entity (1 write + (4 writes x 3) = 13 writes to update the entity). My major concern - If a user gets 10 messages, this will require 10 get requests plus if all 10 messages are new I will need to update each entity and if all of them are new that is (10 x 14) 140 writes. Then if I get another 10 message for this user the same process and this could all add up very quickly. I then thought of creating an entity to store all the sent/received messages in a string for a user inside of a single entity: