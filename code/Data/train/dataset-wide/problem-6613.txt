For parsing: if the word is already in your 'dictionary' then treat it as a 'unigram'; but if it is new to you then treat it as an 'ngram'. If you are doing semantic statistics on a corpus then you may need to do both. 

Language is constrained by its serial channel, but the memory network it represents has more dimensions. Here is a plausible deep net for all your versions of "I am home today": (sorry I can't draw a nice picture) det(one) @ Mark ___ TAM(have) @ det(one) @ home ___ TAM(be [at]) @ [the] (one home) _____________ [at] @ [this] (one day) The nesting (indents) can't be represented in speech unless we use case markers, strict positioning, or syntax clues. "Today" is temporal, which is a clue that it is adverbial, and because of that clue its position is lightly constrained. We are then permitted to use relative position for 'focus'. 

An 'intonation contour' graphs f0 vs time (and ignores power). The relation of avg f0 in an utterance, compared to the speakers avg f0 over a longer interval, gives information about emphasis or emotion. The relative changes in f0 during the utterance give information about syntax (question, etc). Since the relative changes are independent of avg f0, they may be considered as a separate graph, (orthogonal to the absolute f0 graph,) with the base and range 'normalized'. Only the normalized 'shape' of the contour affects syntax. 'Vertical scale' is meaningful only with the absolute graph -- with the relative contour it is arbitrary (normalized). 

A perfective predicate regards the action as a whole, and only subsets of that whole are regarded as imperfective. 'To need' is no different than 'to eat' in terms of whether you can consider starting, progressing, interrupting or completing the action, or the entire (therefore started and stopped) action. In English, the only real imperfective predicates are those that use the active participle (verb+'ing'). 

'Appositive' means 'side by side', but as its purpose is to give additional information, the more definite noun phrase will usually come first (so we could call the extra info postpositive). In (1), the list is the subject and 'all' is the postpositive (all [of the preceeding items]). "My brother, Allen" and "Allen, my brother" both have equal 'definiteness'. "My brother, a doctor" works, but *"A doctor, my brother" is weak because the definiteness increases in the postpositive. 

Your phrase is a fragment (not a sentence). It might occur as the answer to a question ("What do you want?"). 'That' is a complementizer -- it makes 'you be happy' the complement of 'what'. This is similar to the demonstrative pronoun function of 'this' or 'that', except that the antecedent is a phrase rather than something in the external environment. 

The problem is that the generative rule for VP is too weak. To handle 'at the dog with one eye' as two adverbial phrases, the rule should be something like VP -> [adverb] verb [[PP]] [NP] [[PP]] {allowing any number of sequential adverbial phrases}. Any PP generated under the NP formula would be restrictive to the NP, not adverbial. This new VP rule is still weak, as adverbial phrases include adverbs as well as prepositional phrases, and they can occur before the subject, as well as in the predicate: before or after the verb, or even after the object. 

Any elastic parts of the vocal apparatus that are not symmetrically perpendicular to the airflow will vibrate differently when they point upstream than when they point downstream. 

+'ly' adverbs, or any adverbial (marked with commas if needed), can be inserted ANYWHERE in a clause, except inside a NounPhrase or a PrepPhrase. [note: all NPs could be considered PPs, with null prepositions [by] and [of], since English has strict case ordering.] Adverbials can even split a modal aux from its verb, or an infinitive marker ('to') from its verb. 

You are asking why 'eight tens' is written as 'LXXX' instead of 'VIIIG', where 'G' means 'times ten'. Instead of adding a character for 'times ten', they applied the operation 'times ten' to each character in 'VIII'. They were already using the position of characters to indicate 'plus' (or 'minus'). 

I have changed my answer. To understand 'ought', I will coin a word class called 'control verbal'. A control verbal has an event as its direct object. The subject of this sub-event will be 'raised to object', or it will be omitted (raised to subject) if it matches the calling subject. The verb of the sub-event will be cast to infinitive (possibly bare) or cast to participle. A 'pure' modal (like 'should') always controls by 'raising to subject' and 'bare infinitive'. The 'impure' modal 'ought' differs in that the infinitive may be marked (by 'to'). A control verb (like 'need') is a control verbal that has its own 'action', as well as controlling a sub-event. The infinitive marker may be deleted when the control is negated, making it parse like a modal: "He needs to drink, but he need not eat." Old answer: 'Ought' does not really become modal when the infinitive marker 'to' is elided. Rather, 'ought' is still a 'control verb', but the controlled verb '[to] go' is reduced to a bare infinitive. This is also true of control verbs like 'need' and 'dare'. Because it works with so few verbs, I would call it archaic. 

A unit of voice is an 'allophone' (a sound distinct from other allophones). A unit of linguistics is a 'phoneme' (a signal to the muscles, distinct from other phonemes). Written language evolved to convey the spoken language without sound, but that does not require that the two are correlated (look at Chinese). Languages like English do have correlation between letters and phonemes, but it is not one-to-one (especially for vowels). Because we have things like silent 'e' that affect other vowel groups, it is hard to imagine 'units' unless you paraphrase each syllable into a consonantal onset and a vowel [+ consonant] rime. (ex: site = 's'+'i_e'+'t') 

If 'stative' is a binary term, then the only stative verb is 'to be'. Lack of progression has nothing to do with it (states are not required to be permanent). Verbs like 'seem' are control verbs (He seems [to be] odd.) If you want to differentiate events with obvious action (He kissed her) from events with hidden action (He loved her; [the network in his brain formed the pattern that expressed love]), then 'stative' is a weak term for the latter. 

Determiners:: Logical quantifiers: some, few, more, most, many, much, each, every, any, all. Cardinal quantifiers: 0(no), 1(a[n]), 2 ... Indexical determiners: the, both, [n]either, next, previous, prior, last. Ordinals: 1st, 2nd ... You also bracketed the plural 's', which quantifies 'any', 'some' and 'the'. Your other brackets mark coordinating conjunctions: and, [n]or, but. You should then consider subordinate conjunctions: if, as, because, so, before, after, when, while, unless ...; but that opens the door to many adverbials: where, how, during ... I think you go too far now with bracketing associative prepositions (with, of ...), collection related nouns (group, member ...) and collection related verbs (consist, [have]...). 

Neither of your example sentences are entirely context-free. An AI might comment for each: 1) "I don't recall the experiment." 2) "I don't recall the task." Once you write a program that can identify which phrases have antecedents, and test whether those antecedents are in the same sentence, then it would be relatively simple to extend the program to handle multiple sentences (the 'context'), thus making the goal of the program obsolete -- you would have a context-sensitive program, so testing for context-free sentences wouldn't be necessary. 

Syntactically, these temporal words are [pro]nouns. They are also called adverbs because any adverbial phrase that can be expressed in one word is called an adverb. We know they are adverbial (we assume an elided preposition such as 'at') because they are temporal. In your sentence, 'yesterday' modifies 'ran ...'. The main verb is the only word that it needs to be associated with. The other verbs are associated with it only indirectly (by nesting). For example, if you change 'when' to 'after', then the waving may not have occurred yesterday. 

The purpose of language is to move memories from one brain to another. To be efficient, nothing that is already known by both brains should be transmitted. Besides entire (redundant) clauses, individual words may be elided if the result is not ambiguous (if there is only one part-of-speech that makes sense and that part has an agreed-on null component for that part). Fragmentary questions are appended to the previous statement ([Mary did eat], when) Fragmentary answers replace the query pronoun in the question ([Mary did eat] cake), fragmentary statements are demonstrative (A bird [is there]), and all imperatives are fragmentary predicates ([You shall] stop). 

The 'general rule' might be that certain phonemes are difficult to say in sequence, so one either swaps them (metathesis) or inserts another (typically a vowel, epenthesis). 

Your main error (as Greg Lee tried to point out) is with the relative clause "what I am", which you identified using [NP N VP] {a noun phrase can consist of a clause! - that is wrong}. A relative pronoun needs its own syntax (it is closer to being a conjunction than a pronoun). The meaning is something like "I became [the thing] that (I am [trace: the thing]). You then have several adverbial phrases, but are they adverbial to the main verb 'became' or to the subordinate verb 'am'? The comma is too weak of a punctuation to disambiguate the nesting. As given, the sentence could be saying that the author was twelve when he wrote about what happened in 1975 (which would be before his birth if he was published in a timely manner!) I think the comma should be after 'today'. Notice that 'today' is a temporal [pro]noun, which allows it to be adverbial whereas other nouns (like 'kite') would not be. Your parser will need a finer breakdown of syntax (such as pronoun, proper noun, class noun instead of just noun). {This is still context free parsing} 

'Recognition' works at multiple levels, but (at the level you want) what we recognize are 'allophones'. These are sometimes similar to syllables, and early alphabets were syllabic. The muscle changes used to produce allophones allow representation as phonemes. Converting allophones to phonemes is only needed when spelling with a [semi-]phonetic alphabet. 

The reason is efficiency. If the detectable omission of any part of speech is done consistently enough, that omission can replace a word. In this case, it is a 'zero article'. The determiner that is omitted is either 'all' or 'some', depending whether the noun is the subject or not, and whether the clause is defining or not. It could have been done the other way, with the zero article indicating the singular; but 'a[n]' (from 'one') is so minimal already that it would be a shame to trade it for a plural determiner. The count determiner often goes away completely when a definite article is used. 

Here is a theory: Original languages had few lexical words, so it made sense to have irregular versions for different grammatical functions. Storage could simply link to the appropriate form. As the lexicon grew, these linkages (axons) would be strained to find the right form. If morphemes could be moved to the relation (the neuron body), then the lexical density would remain manageable. (a tradeoff of more dendrites but shorter axons) This theory can only be tested indirectly. Its major assumption is that the brain stores 'data' in highly organized tree-like structures (as opposed to the messy structures that better handle 'processes'). The theory applies only to relations (typically verbs). The case structure of languages does not affect the storage paradigm; it merely increases the efficiency of communication when strict ordering allows the elision of some case marking morphemes. 

"Cup of" is not a determiner for "coffee"; rather, "of coffee" is a restriction of "cup". 'X of Y' means that Y possesses X (Y has X) (X is a part of Y). This is clear in expressions like "leaf of tree" or "top of table". It is less clear in "box of books" or "cup of coffee" because X is a container of Y. We might use 'with' instead of 'of', but 'of' uses simpler phonemes. English can specify just the container with "book box" or "coffee cup", but not the entire set sans preposition: *"box books" or *"cup coffee". 

It is recursive in that the topic of the adverbial ('singing in the cage') is {technically} the verb 'be' ('is mine'), rather than the main topic (1 bird [that is beautiful]). 'in the cage' is also adverbial to 'singing', although the bird has to be at the same location. The depths would be: 0: one bird; 1: be beautiful; 1: be mine; 2: [be] singing; 3: in one cage.