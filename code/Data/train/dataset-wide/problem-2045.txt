I've tested Buffer Pool Extensions on an Amazon EC2 virtual machine. The VM has a single non-persistent high-performance SSD. On VM shutdown the SSD contents are lost, so it can't be used to store business data. In order to leverage the SSD, both tempdb and buffer pool extensions were configured to use it, not unlike what was done on Azure (save the Task Scheduler-Powershell hoop jumping). Some 100 GB BPE was allocated, tempdb data files and logs are currently around 20 GB. The system has been running nicely for about a week. Yesterday, for no apparent reason, buffer pool extensions were disabled. None of the admins admit changing the configuration. It looks like the default trace doesn't record BPE changes, ditto for the system_health Extended Events. Unless I missed something, would there be a way to see what triggered the change? All the Application log and Sql Server's log contain is an entry that states the pool was disabled: 

The list is shamelessly ripped and adapted from MSDN blog article. Read it for more details. For another an article, try Technet Wiki 

For future use, consider adding CHECK constraint to the ClientGenderName column. This prevents inserting rows that don't contain valid values in the first hand. While you are at it, you should consider adding constraints too. 

Yes, it is possible and this may or may not be desirable. The database theory has a concept of isolation, which is about transaction visibility to other processes. MySql's documentation about has a discussion about product specific syntax. 

Trying to find an opitmal value for fill factor is a well-researched problem. In the described scenario, there is not much to be done unless the table doesn't use default values. 

You could use a sequence for generating the values for ID fields. The documentation explicitly mentions your use case. The example C on said page has a sample implementation about this. 

Are there any other probable causes for disabling the BPE than human error? There is a KB article about I/O errors that might cause loss of BPE, but the current patchlevel should include the fix, and there are no log entries about I/O errors anyway. The tempdb files have been autogrowing around the same time, but there was plenty of free space on the SSD. The server is up-to-date with OS (Windows Server 2012 R2) and Sql Server patches (SQL Server 2014 SP1-CU6 X64 Enterprise Edition). 

Even if you meant Sql Server and just mis-spelled db_owner, the answer holds true for any database: use the principle of least priviledge. That is, if an user shouldn't be able to drop the database, do not assign such privileges in the first hand at all. Messing with built-in security group and role settings is bad practice, unless you are very sure what you are up to. Create new roles with appropriate settings instead. This way, you or any other DBA doesn't have to double-check if default permissions are really on default settings. That being said, you might be able to prevent drops with some creative triggers. As how to do that, depends on your RDBMS. 

As this is an interview question, start by working as it was an actual user probelm. Ask for an exact error message as it actually tells what's wrong. If the interviewer doesn't give you one, describe how you would get the error message from user. Would a screenshot do? How to get one? Would a screen sharing session be possible? In order to troubleshoot any computer problem, you should create a work plan. For Sql Server connectivity troubleshooting, the basic problem locations and typical reasons can be outlined like so, 

As how input the data, a loop in a cmd session works pretty well. Change the loop upper limit to the actual number of files instead of using . 

Short-term solution: review that your CLR code minimizes memory usage. For a long-term solution, upgrade to 64-bit. Addendum Kimberly Tripp has written a white paper about CLR stuff from DBA point of view. She's got a few nice queries and Perfmon counters that are of use for tracking CLR resource use. 

Using without is going to return you n rows in whatever way Sql Server cares. As per the documentation: 

Network issue. (No connection, invalid DNS) SQL Server configuration issue. (Service down, wrong port) Firewall issue. (IP restrictions, VPN issues) Client driver issue. (Wrong driver) Application configuration issue. (Typo in server name, misconfigured aliases) Authentication and logon issue. (Missing logon, missing user, missing permissions, missing AD groups) 

Yes, it's possible if you change the transaction isolation level for the session (that's what you call "window" in SSMS) that queries modified data. Often this is not such a great idea, as you might get some unexpected results. Consider the side effects carefully. I have no idea if it's possible to change the transacion isolation level in the Excel Power Query. For example, the following set of queries would insert some data and display the update correctly even without commit/rollback. 

Consider using Sql Client Aliases to abstract the data source name. The aliases are set on client computers by either adding keys to registry or by running (for x86, x64 systems) and (for 32-bit applications in x64 system). A Sql Client Alias is quite a bit like the additional DNS record you described. What's more, aliases support instance names too, a feature that DNS record won't do. After an alias has been created, configure the client to use it instead of real Sql Server instance. Should you ever need to change the server, just modify the alias. As a caveat, there seem to be some applications that won't work well with aliases. These often are legacy ones, like those using ODBC or proprietary database drivers. For modern applications, aliases will work just fine. 

It depends on a lot of different factors. Most are not non-technical. That is, business requirements will usually set marginals for suitable high availability. For example, a failover cluster instance requires shared storage and that usually means a SAN system. Always on availability groups are not available until Sql Server 2012 and you might or might not be able to upgrade the DB and so on. Can you discuss with application developers / vendor about supported HA configurations? In my experience, a lot of software do not really work too well with HA. For example, the software might just crash if DB connection is lost. Read Paul Randal's article and discuss with all the stakeholders about requirements. Discuss with your boss about how much work, software and hardware each solution requires and what kind of budget you got. Consider hiring a consultant for choosing and setting up the solution too. High availability should be set up by a professional. You don't want to explain a failure-by-misconfiguration to any of the stakeholders. 

This behavior is caused by the fact that SQL is a declarative language. In such a language one describes the desired output, not the steps how to build the output. It is up to the RDBMS how to build the output. This often is counter-intuitive to programmers used to procedural and OOP approach, in which one tells explicitly step-by-step what the application should do. If you got queries that mis-behave with boolean logic evaluation order assumption, I'm afraid you got to re-write those. 

The table(s) seem to need some normalization. For example, in the Client table there seems to be both foreign key and data it's being pointed to. First you need to define what values are valid for gender. Is male/female sufficient? Are abbreviations like m/f allowed? Depending on your locale, there might be additional genders for LGBT people, maybe even one for 'Will not tell'. What's the meaning of a NULL value in this column? After you know what the range for valid values are, filtering invalid set is quite simple. This assumes you are really using denormalized table, so no join is needed.