Intersecting random walks can intersect at any time, colliding walks have to be at the same point at the same time. Collision is a much stronger condition, and gives very different asymptotic behavior, and a different critical dimension. For intersections, the critical dimension is 4, meaning that below 4 dimensions you expect intersections and above 4 dimensions you don't, with 4 dimensions being marginal. The essence of the argument is that each random walk trajectory has fractal dimension 2, since each epsilon ball you put on the trajectory only covers a time interval of size epsilon squared. So the condition of intersection is similar to that for randomly placed 2-d planes, up to logarithmic corrections. The critical dimension for your problem is 2, meaning that in more than 2 dimensions, there will be a power-law falloff in the number of collisions in the limit small grid/large distances. For two D-dimensional random walks x1 and x2, their difference s=x1-x2 is a random walk. The time this walk takes to reach the origin gives the coalescing time, and this is the classical Polya problem with critical dimension 2. For two walks a distance 1 on an $\epsilon$ sized grid, the coalescing probability up to any fixed time vanishes as $\epsilon^{d-2}$ for small $\epsilon$ up to log corrections in 2 dimensions. I am unsure if you are only interested in the $\epsilon$ power law behavior, or if you are interested in the detailed pattern of coalescing, which depend on the initial geometry. There is always the problem that you are asking about D>2, where chances are that there will be no collision for a finite number of points at large initial positions. For three random walks, you can consider the three differences s=x1-x2 t=x2-x3 u=x3-x1 which are random walks only correlated to the extent that they obey s+t+u=0, which make skewed coordinates on a 2D hyperplane, and the random walk in x1,x2,x3 becomes an ordinary random walk in 2D dimensions, with absorbing boundaries on s=0,t=0,u=0. Your problem is then the condition that a random walk will hit and stick to these hyperplanes, and then continue walking on the hyperplanes, until it reaches the origin. There is an enormous literature on random walks with these types of absorbing boundaries. But I have the feeling you want the limit of large numbers of walkers, or a finite density of walkers, where this type of thing is useless. For D=1, for collisions of many one dimensional random walks, variations on this problem have been extensively studied in the physics literature under the name "vicious random walkers". For D=2, the marginal case, and "inbetween" there is an interesting variation on this problem with surprising behavior in this Cardy paper ($URL$ This is the problem of annihilating walks--- when two walks collide, they both disappear. The (very hard to make rigorous) analysis in his paper should be adaptable to the case of coalescing walks, but both the diagrams and behavior in simulations are different. 

There is no deterministic stochastic process associated with the Schrodinger equation, so this question is meaningless. There is a stochastic process associated with the analytic continuation of the Schrodinger equation to imaginary times, but this has nothing to do with measurement (or anything else in the physics in real time). This construction has nothing to do with the time energy uncertainty principle. 

print its own code into a variable R deduce all consequences of ZF, looking for the theorem "R does not halt" if it finds this theorem, halt. 

In questions like these, I think it is important to say right off the bat: Theorems are only undecidable in a given axiom system. For all we know, all meaningful mathematical questions, including all questions about the halting/nonhalting of any computer program, are settled from a strong enough axiom of higher infinity. It's just that you need stronger axioms for stronger theorems, and you can't use a fixed computer program to list all the the axioms. Anyway, given some consistent axiomatic system, like ZF, consider the Godel statement, which is constructed as follows: Write computer program GODEL to do the following: 

A little reflection shows that GODEL halts if and only if ZF proves "GODEL does not halt". That is, if you call the sentence "GODEL does not halt" by the name G, then G is true if only if ZF does not prove G. This sentence is unprovable in ZF, so it is order 0. But it is also order 1: ZF can't prove that it is unprovable, because of the fact that it is a pi-1-0 statement, it is a statement about the non-halting of a computer programs. If program P halts, then ZF proves that this program halts (and ZF can also prove this fact about itself). A little reflection shows that G is also of order 2, and order 3, of any order really, because the whole point of the Godel sentence is that ZF can't prove it, can't prove it can't prove it, can't prove that it can't prove that it can't prove it, etc. So the Godel sentence answers your question in a technical sense, but probably not the way you intended. But colloquially, one says that "G is provably unproveable" when you show that G is equivalent to "consistency of ZF", so if you prove " (ZF is consistent) iff (G)", you have proved G is unprovable in common parlance, because the common intuition is that the consistency of an axiom system is to be taken for granted. But G is provable in a stronger system where the consistency of ZF is explicitly stated as a new axiom. So in order to get a non-vacuous hierarchy, one could interpret your levels as follows: level 0: "ZF proves that con(ZF) is equivalent to G." (So G is provably unprovable) level 1: "ZF proves that con(ZF+con(ZF)) is equivalent to G'. (G is unprovable, and it is provably not equivalent to con(ZF), so it is provably unprovably unprovable) level 2: "ZF proves that con(ZF+con(ZF+con(ZF))" (this is harder to state in English, but it's level 2) etc, etc. In this case, you can define the levels up to n, and higher levels by transfinite induction. This is studied in ordinal proof theory. 

Stripping away the pointless formalism, what you are asking in the question is: "What happens if you measure an observable again and again, so that the measurements become very dense?" What happens is called the "quantum zeno effect". If you keep measuring a state that tries to change, you prevent the state from changing, instead you constrain it to stay the same Eigenvector of the observable you measure. But you have a continuous family of observables But you are asking what happens if you measure an observable which varies with time. The result is that you follow the Eignevectors of the observable in a deterministic way. So the first operation will project you to one of the eigenvectors at random, then the remaining continuum of measurments will make the state change continuously to follow the changing eigenvectors of the operator. The reason there is no stochasticity is because if you measure after a time "epsilon", the probability of being found in a different eigenvector goes like "epsilon-squared", so that in the continuum limit, the process becomes completely deterministic, with 100% probability of following the corresponding eigenvector of H(t). The only subtlety is when the eigenvectors collide (have the same eigenvalue at some time t), in which case, a continuous measurement will have to follow the eigenvector through the collision. So if to eigenvectors of H(t) coincide at time t, and afterwards come out in different direction, then there will be some stochasticity associated with the collision. The original direction of the eigenvector (assuming the generic case that only two eigenvectors collided) will have to be expanded in the directions of the two new vectors, and the square of the expansion parameters will tell you the probability of going off in different directions. You might be able to make a markov chain by colliding again and again, but this is not in the spirit of the original question.