TL;DR: Maybe. Maybe Ahkenazi Jews score somewhere between 0.3 and 1 SD higher on IQ tests or verbal IQ. The genetic explanation in the question (disease-conferring genes are also intelligence-boosting genes, there was no bottleneck, but directional selection) has come under criticism. From your question it's unclear whether you want to know whether ethnic Jews are smarter for genetic reasons or simply smarter as a group. A commenter cites the article Natural History of Ashkenazi Intelligence, which makes a more specific claim. An article critical of the genetic explanation by Prof. Brian Ferguson at Rutgers provides a useful counterperspective. It didn't appear in a peer-reviewed journal, but it did receive peer review and the author simply tells us why the editor rejected it, so it probably gives a good perspective on the debate. He also cites one of the original authors with a revision to their theory, which may or may not be a bit more plausible. 

You're lucky, the intense debate about psychology's (and other sciences') false-positive problem has spawned an article (by Schimmack, hailing from your FU Berlin by the way) which uses Gaillot's initial study as a case study of incredible results. The author shows how dodgy methodology, analysis and reporting choices made the initial study incredible (actually some of Gaillot et al.'s reported studies didn't replicate the effect, but those tests weren't reported). Cherries (picking and emphasis mine): 

Power was very low, hence the correlation cannot be reliably estimated. He also combined this experiment with a manipulation of self-focused attention (showing or hiding a tape recorder during the task). 

I read about a new study that appeared in Frontiers in Psychology yesterday that is relevant to this question. It is open access, fairly readable and short, so I encourage you to check it out yourself. The authors let German students construct a 9/11 narrative using a card deck with story elements. When they included more extreme statements, the number of official statements used dropped. This might be seen as evidence for shifting the Overton window by using extreme statements. The authors also make the case that this mechanism may have been at work during the debate about Sarrazin's book "Deutschland schafft sich ab" (basically a German version of the Bell curve debate I talked about in my other answer). The paper also has some general relevance for skeptics.SE because it's about how conspiracy theories form in people's minds. Abstract 

--- end edit Old answer No, there is no such evidence. Apparently there didn't use to be much evidence against it either, but I found two recent studies by Noemi Peters (2010, 2011). First I did a search on "sex differences" multitasking and similar terms, but I could only find a dodgy study in support and not much well-received publications in the field anyway. Apparently Ms Peters found the same dearth in the literature. The fun part: I found her publications by looking who had cited the Pease book :-) 

Disclaimer: I only read the abstracts. That's not sufficient to evaluate the claims in detail and I'm not an expert in the field, but I also found the (generalized) notion counter-intuitive. The studies' findings are based only on photographs taken in a controlled setting by a photographer with all kinds of intentions. Alternative explanations arise (3 studies). The other one is a priming study, I don't even really see the reasoning behind it. Also one sample (models) was presumably exclusively females, one sample was exclusively males. They say these groups are ranking by prestige or dominance, and say that this is two forms of status. Status can be taken to have many meanings (see below), but these two are more something like success dimensions in their respective professions (modeling and football), not the "popularity" kind, which I'd expect to go along with smiles all around. This study finds smiling to be positively correlated with sociometric status (popularity), but unrelated to "power" i.e. dominance. Evidence to the contrary, from a setting that I find more appropriate to generalise from (housemates, ie. actual social groups with a hierarchy). This social psychology experiment which may not generalize to other settings find that male interviewers smile less than their applicants in a simulated job interview, female interviewers do not. Here, sex acts as a moderator, an interesting pattern that couldn't be applied to the findings from the study you cite. 

I did not find the study you mentioned, but I know a very similar classic (371 citations on Google Scholar). Maybe you meant Ambady & Rosenthal (1993). They used the thin slices paradigm (showing people very small amounts of information/behaviour) and predicted end-of-the-semester student evaluations. The people who saw the short video clips were different from the students. 

Edit: Now there's some evidence for this idea I thought I'd come back to this question, because I wasn't really satisfied with what the literature yielded back then and the paper that Peters mentioned (there had only been a press release) has come out. Stoet, O'Connor, Conner, & Laws (2013) looked at this and found some evidence for the idea. Quoting from their abstract 

The effect measured public self-consciousness (using a median-split which wastes information) was not significant: 

So, in summary, smart people are good at music, maths, programming and other complicated stuff. There is no specific reason to believe that there is a correlation between programming and music that goes beyond the one expected on the basis of the general factor underlying both, if experience with other abilities is any guide. There is also no basis to recommend to guitarists to start programming or to programmers to start playing the guitar, at least not if they want to do this to improve on their "main game" -> cognitive training is usually highly specific (practicing the guitar makes you good with the guitar, not with git) and it's unlikely that this relationship is the exception. That is why the Jaeggi study is the one that most researchers want to see replicated at Psychfiledrawer.com (in fact there have been several nonreplications already). 

Some of my friends think that e.g. smiling leads to having wrinkles around your mouth, where your skin folds when you smile, but I'd also count if someone works in the sun a lot an thus squints all the time (if the squinting is causal, not the UV). A Google search of the claim easily turns up some beauty people saying so. A cursory Google scholar search didn't yield much. In German it may have entered language as "Lachfalten" and in English apparently the equivalent is "laugh lines" though I've never read that. I was skeptical of the claim, mainly because 

Fourth, I'd say the MMPI isn't good, it's just widespread. The SKID and the DIPS are supposed to be better. Many prefer the MMPI though for reasons such as sunk cost and the ability to fax your answer sheets in to get a score. Can't dig up an English reference now, sorry. Hank, P.,& Schwenkmezger, P. (2003). Das Minnesota Multiphasic Personality Inventory-2 (MMPI). Testbesprechung im Auftrag des Testkuratoriums. Report Psychologie, 28(5), 294-303. 

Unfortunately I don't have access to this article though it promises to be a current review on the matter. This one I do have access to, they urgently call for more research, which may have happened in the meantime. 

and the one on Cruelty to animals, which makes the interesting point that there may be a common cause to both (I think this runs somewhat counter to this "evil shows itself early" think that I feel is associated with it) 

TLDR: Not satisfactorily answered. Edit: A new interesting study on this topic found runs of homozygosity to be associated with intellectual disability in autistic cases. It is to be seen whether this finding generalises to intelligence variation in the general population. Heterosis (outbreeding elevation) has been proposed as a reason for the Flynn effect (secular rise in IQ scores) by Mingroni in 2004 and 2007. However, the most-accepted principal cause for the Flynn effect may be that it is on measurement specifics (Wichert et al., 2004) and thus does not reflect a "real" latent increase. There has been some studies on inbreeding depression that found decreases of intelligence. As this blogger says the field is not sexy, lots of the studies are old. This is probably 

I just heard of it (a friend has to take it) and it seems obvious that it's a product marketed directly to business people. I'd expect that the test isn't worth much because it was made in 1955 and personality theory has changed much since and because most research shows little incremental validity (above intelligence) even for the good and recent personality tests in job assessment. What impresses me, is that they don't even seem to care much about keeping up the appearance of a scientifically grounded test. When I call up their research database, it doesn't return any articles about the PI. They don't link to any peer-reviewed research from their main page, and the FAQ items about reliability and validity just serve to explain the concepts. When I searched the company's name I found one recent study referencing their coefficients, which were apparently assessed by Perry and Lavori back in 1983. The research was commissioned and not peer-reviewed, so it is not going to hold up very well against accusations of bias. They don't show criterion validity, but some other PDFs on this pi-mgmt site (which I didn't find via the main site). None of those show v. above intelligence. But they say: "The PI has been investigated in over 400 criterion-related validity studies since 1972, (16 of these studies were conducted in 2003 alone) for a variety of jobs and in a variety of industries, and has been shown to be job-related in all of these studies." It was job-related! Thank goodness. Can they refer to 400 studies if they were never published anywhere, not even online and be in accordance with standards of truth in advertising? With the MBTI the research is there, it "just" doesn't show that the instrument is good and most business executives don't find out. But the PI practices strike me as even more audacious. 

I'm glad you asked this question and led me to Kurzban's blog. My first problem with Gaillot et al. (read it years ago) was that with effects this big, it seemed implausible that so many people eat more sweets than they "want". I realize that this isn't a meta-analysis of the findings, but I think it helps a lot to see what is dodgy about it spelled out clearly by more skeptical scientists. I'd reckon (from experience, without having read them all) that some of the articles collected by Sklivvz suffer from similar ailments (clearly Gaillot et al. is not a solid, confirmatory study) and that those by Kurzban do not. There is a grand unified idea behind it, i.e. "the self" but I'm unsure whether this is a good metatheory that can generate claims to be disproved as well. Also: Why was this moved from cogsci? Seems like a perfect fit, maybe we should move it back. Skepticism of cognitive science should be part of cogsci if you ask me. 

The second-hand source is cited 4 times according to Google Scholar, so apparently some people had access to it (or were unscrupulous about scraping references from another textbook...). But in the one citing paper I have access to, they cite it for a different claim. On a Wikipedia talk page about infrared vision, this Snopes report was considered relevant. While the claim is much more crude and about Britain's airforce instead of the US navy, it does have some similarities. Lundquis states himself he couldn't get his hands on the second-hand source, so we don't know the authors of the first study. I tried finding it via Google too (a while ago) and didn't succeed. I don't know how navy experiments are published (delayed maybe?!), so that was a probable hindrance. 

Dtander's answer is quite alright, but because you kept asking about the problem with self-report: First: There is hardly any incentive to pretend that you don't feel empathy, so an estimate that we get from self-report would be lower-bound (if the criteria are proper). Secondly, sociopath, psychopath, dissocial and antisocial personality disorder are not exactly the same concepts depending on who you ask. For example Hervey Cleckley has asserted that psychopaths are immune to suicide but there is differential evidence for two factors that Hare's PCL-R comprises. So some argue that the deviancy/delinquency without lack of empathy/remorse is antisocial personality disorder, and the afflicted will sometimes seek help. I'm not on top of the debate, but I'd say it's likely that it will not come to an inherent endorsement of the term "psychopath" because it has a quite "evil" connotation (the Stout book sounds quite scaremongering as well) and the people Hare means wouldn't seek help anyway. Third, not all of the criteria in antisocial/dissocial personality disorder and even fewer in the PCL-R need to be based on self-reports. There are case files about activities like (juvenile) delinquency, many short-term marital relationships, promiscuity, etc. And not all think they have something to hide. A quote from Geoffrey Miller's Mating Mind: 

Limitations The study had a very small sample (30 in total). The study was vulnerable to expectancy effects from both researchers (scored card decks, knew condition) and students (they might have guessed the objective and helped along, manipulation was not subtle) and several outcomes were used, several of which could have been interpreted as supporting the hypothesis. Thus, I wouldn't place too much trust in their findings until they're replicated in a much larger sample. 

He then did post-hoc analyses (known to increase false positives, but it is hard to appropriately correct for this) 

I took this questions to mean "Are good programmers more likely to be good musicians than non-programmers?" because the real question is a bit trivial (as one comment indicates) and I thought I could tell that this was what Sklivvz wanted to know from his own answer. The most parsimonious explanation for this is likely to be the general factor of mental ability. Here is a study by Deary, Strand, Smith, Fernandes (2007) on the relation of IQ tests to achievement in different school subjects in >70000 English school children. The correlation of g with music was .54, with Information Technology it was .48, with math it was .77. Here's another much smaller study: 

His view (shortened): The "recent review" they mention conveys a very different impression from that forceful assertion (Lynn 2004). 

Some reading material if you'd like. Scheuffgen et al. argue for a different view on intelligence based on the peculiar patterns with autistic children. Currently DSM-IV defines Asperger's as autism without language delay and cognitive impairments. However, at the moment it looks like Asperger's will be collapsed into autism as a diagnosis in the upcoming DSM-5. The discussion is interesting and also addresses group differences in IQ. Your question probably isn't that interesting after all, since Asperger's is by definition not impaired cognition but it's a mainstream idea in research, that there is no clean cutoff to be found between Kanner-Autism and Asperger's. So the correlation between Asperger's and intelligence probably wouldn't mean very much. A correlation of intelligence with autism spectrum disorder would be negative, I guess. (Sorry, this was a comment that turned out to be too long, don't have much time now) 

The 95% confidence interval of a .31 correlation with 62 subjects is [0.07; 0.52]. All told, if this is all Wiseman is relying on (I didn't watch the video, I trusted @ndwaldner), then there is quite the jump from a just-significant correlation between E direction and public self consciousness to Q direction and being "more of an intravert and not very good at lying". Obviously, the self consciousness scale relied on self-report. Personality psychologists usually don't worry about that, but when you're making inferences about lying you probably should. Of course it'd easy to change the E/Q direction consciously if you have a hunch of what is being assessed. PS.: There has been a number of conceptual replications of this paper, which I didn't read.