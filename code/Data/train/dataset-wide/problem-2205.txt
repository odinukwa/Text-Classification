If you want this to replicate through to your slaves you need to specify a --replicate option that tells it the table to place the checksums in (and consequently put the slave values into on each respective slave). If this is your first time you might want to have it create that table for you with --create-replicate-table. It won't need to directly connect to the slaves unless you want it to monitor the slaves for falling behind. It basically issues queries like replace into checksumsTables ... (masterV1, masterV2, (select blah...)) where the literal master values are written to the binlog along w/ the select portion of the replace. When that executes on the slaves the select does it's think to get the slaves checksums. I've spent a lot of time with this tool recently so start out with that and reply with more specific problems you need help with so I don't just rewrite the entire manual and every possible thing that can go wrong. Note: if you run 

This might be better placed on stack overflow. This isn't something you'll accomplish with MySQL. What you're talking about is referred to as 'stemming' in search. Similar to matching different conjugations of a regular word e.g. run => runs ,ran. I don't know of any such applications for proper names off hand but when you find one that will sit alongside your primary application to "normalize" the name before inserting the record into your database. Mysql, sqlserver, mongo, whatever. The DB technology is irrelevant as your task is out side the scope of storing data/documents. Lucene would be a better tool for your task. But I couldn't speak to it's prepackaged ability to stem names like you want. Edit After thinking about it I think I misspoke when I said Lucene would be a "better" approach in of itself for what you want. My understanding is stemmers exist outside of core lucene and then proxy a search for "bob" into ("bob" or "robert") to feed into the lucene engine. 

Playing around with TokuDB I'm finding even after "optimize table" for things such as changing row compression or other DDLs it takes a non deterministic amount of time for freed space to be reflected in the file system. Is there anyway to force this more immediately or otherwise view status of if the clean up is in progress or otherwise scheduled? 

If you're using Nagios there's a build in plugin to monitor all you want and more $URL$ If you're currently implementing your monitoring in general as a bunch of one off scripts you should probably look toward moving to a more centralized tool such as Nagios to manage the checks, threshold and paging policies. Edit: Nagios and the plugins are free, plus you can write your own plugins to suit specific needs that aren't already addressed by the community. 

What can cause reading a simple session variable like tx_isolation to be anything less than instantaneous? 

I seriously doubt you're being CPU bound. This can easily be confirmed or denied by watching top (or process manager? in windows). More ram and faster hard disks are likely the hardware facets you'd want to improve on a DB server. 

This isn't a full answer but you might try looking over the 5.5 change list starting at $URL$ Note the section nav on the right that goes subversion by subversion. Skim through each looking at "incompatible changes". If you find something describing what you are seeing it will at least give you better context of what is happening and why. 

Two possible reasons seem to stick out, ensure your server_id is different on both the master and the slave. This is to be defined at startup in the cnf file. Another option is you specified a very tiny max_binlog_size or max_relay_log_size, ensure the values are what you want and not off by an order of magnitude (eg. 512k instead of 512M) 

It turns out there's a known bug where this only works when calling the the JDBC API directly. It does not work when building queries through something like SimpleJDBCTemplate. All of the applications real work was being done using templates whereas the healtcheck threads were using the direct API. 

Doing a reload like this will give you the added benefit of defragging your tables. This example also implies you don't mind having the 5.1 DB locked up while the dump is occurring. 

Prior to the grant statement. I don't recommend doing this because slaves are generally around in the event of an emergency fail over. The risk of recovery time problems because grants weren't consistently applied out weighs any risk of the password making it into the binlogs. Think about it, if some unsavory user gets to your binlogs, they have access to all the data that was streaming through at the time anyway. Just be sure the file system permissions aren't world readable and keep any actual system level accounts to actual sysadmins to true "need to have" basis. Try to limit system level accounts on your db servers to developers for example. If you find some reason your stack setup necessitates many people have system level accounts, eliminate or minimize those reasons as much as possible, have a dedicated host (could be a VM) that's only job is running mysqld and related tools. (Apps should be on their own segment) This goes for development and production servers. Sure devs will need access to the dev environment to dev, but you can get them individual mysql level user accounts that their apps can connect to (restricted to your local network segment; don't use % for the host). 

I don't have experience with DynamoDB so the best I can do is offer some pointers on optimizations to look for if you stay in the hosted MySQL world. 

If these don't suit your needs please elaborate on your use case on what you are trying to accomplish. 

Rolando's solution has many caveats. The first being one replica stream is necessarily not replicating while the other works. This is going to give you periods of time where your slave is out of synch. You now have to play a delicate balancing act to ensure each has enough time to catch up when it has its "turn". As described you also have to play book keeper of log positions to switch back to. This really just seems buggy, opening the window for missing or inconsistent data or even breaking replication when it goes wrong (either being caused by even a just 'off by one' error in the log position) I would recommend just running multiple mysql instances. There's nothing stopping you from running two or more mysql's on the same machine. They cannot both operate on the same port of course. I don't really see this as being a problem though as every client and library allows you to specify something other than 3306. Just specify port=3307 (or whatever in one of the .cnf files). You will also want to take care in ensuring the individually configured buffer pools and other memory configurations aren't at odds with each other. This is actually a benefit though as you can more finely tune those settings to the specific requirements of the individual databases that are being replicated. This way you just have two replication streams running into the same server; never behind, no book keeping required, no "swapping" script required. 

As you stated you cannot prevent events getting written to the binlogs with replicate-do-tables, however that will determine what gets written to the slaves binlogs if you have log-slave-updates on. Consider setting up an intermediate slave on the same trusted machine as your private master that runs for the sole purpose of binlogs filtering, then have your "public" database replicate from that. If you worried about the 'private' data being written to the binlog even that environment keep in mind the file system permissions restrict read access to just the mysql system user. If you're worried about that account getting compromised so they can read your binlogs, keep in mind at that point you have larger problems and they can just grab the entire datadirectory. 

I'm guessing the answer is no, but I was wondering if there was anyway to tell innodb to not store fetched pages in the buffer pool? The reason for looking at doing this is check summing tables. I'd like to minimize the effects of trashing the useful cached data. 

It sounds like the table is on the cusp of a cutover for the query optimizer between the estimated total rows and estimated rows being looked up. The use/ignore index syntax exists for situations like this. The pitfall to this approach is when the table dynamics change in the future such that it's really not the best choice. 

Deleting in batches and then optimizing is really the worst thing you can do. Each delete will be performed as a transaction and each optimize will effectively recopy what remains of the giant table. Think of it like this. You have a 5 gig table, delete 0.5 gigs, optimize (copy 4.5 gig file), delete 0.5 gigs (copy a 4.0 gig file) etc.. And really the overhead associated with each optimize is much more than just copying that much data on a filesystem since it's tantamount to inserting all those rows into a new table and all the transactional overhead that comes along with it. Dropping a table should be the most efficient way to do it Edit: Here is some thorough benchmarks though $URL$ 

where you enumerate all the individual dates you are interested in. I have come across situations where it is clear MySQL is not smart enough to effecitvely use a range query (i.e. between x and y) verses enumerating specific values for small sets. Which ever the use case you'll want to make sure that column is indexed if you're making regular reporting queries based on its values. 

I was trying to evaluate the latest tokudb Hotbackup for percona's tokudb engine. This install is under debian (Ubuntu 14) I was able to get all the packages installed with out error and get all the base toku plugins installed. Attempting to install the backup plug i get 

To get a feel for current storage requirements. What you'll be more interested in is growth rate over time though to ensure you're not going to run out of space in 3months down the road. MySQL doesn't give a built in growth rate metric so that's something you'll need to periodically poll and record yourself. For memory requirements, in a perfect world your innodb_buffer_pool would be big enough to accommodate all your data and indexes in memory. It's never a perfect world though and can become impractical or cost prohibitive to have such a machine. At the same time, strive to make the buffer pool as large as you can to keep most frequently accessed data in memory. If you are straight up dropping tables the storage space will be reclaimed to the file system if and only if you are configured with innodb_file_per_table = true. Other wise all your data is stored along with metadata and transaction history in ibdata1.