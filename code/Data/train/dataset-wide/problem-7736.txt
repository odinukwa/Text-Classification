Let $V \subseteq \mathbb{R}[x,y]_d$ be a two dimensional linear subspace of the vector space of bivariate forms of degree $d$. For each degree $d$ we can find such subspaces with the property that every element $f \in V$ has only real roots. Furthermore, these vector spaces with such a real-rooted-property can be characterized and are well understood. Now let $V \subseteq \mathbb{R}[x,y,z]_d$ be a three dimensional linear subspace. We say that $V$ has the real-rooted-property if for every two dimensional subspace $V' \subseteq V$ the common zero set $Z(V')=\{p \in \mathbb{P}^2: f(p)=0 \textrm{ for all } f \in V'\}$ consists just of real points. Now I have the following questions: 

Noga Alon published half a dozen papers under the name "A. Nilli". Mathscinet links directly from this pseudonym to Noga's publications. 

Probably you mean Gross's Log-Sobolev Inequality? I cannot help with understanding any variational calculus proofs, but there are other proofs I find easier to understand. Actually, I prefer obtaining the Hypercontractive Inequality first, $\|P_t f\|_q \leq \|g\|_p$ provided $e^{-2t} \leq \frac{p-1}{q-1}$, and then recovering Log-Sobolev by taking $q = 2$, squaring both sides, and differentiating at $t = 0$. (See, e.g., exercise 10.23 at $URL$ ) As for the Hypercontractive Inequality itself, the most traditional short way to prove it (due to Gross, but also independently to Bonami previously) is to prove it first for Boolean functions (by induction on $n$) and then to pass to the Gaussian setting using the Central Limit Theorem. Janson's book Gaussian Hilbert Spaces (Chapter 5) does this nicely (or you can see Chapters 9, 10, 11 at the aforementioned web site). One can also prove Log-Sobolev directly by induction in the Boolean case (see, e.g., Exercise 10.26). If one only cares about the Gaussian setting, there are other short routes to Hypercontractivity (and hence Log-Sobolev). There is the "Neveu method"; although it's short, it uses stochastic calculus and is thus not very elementary. On the other hand, a very direct and simple proof is given in Ledoux's recent paper "Remarks on Gaussian Noise Sensitivity...", based on early ideas by Hu and by Mossel--Neeman. 

Let $\Gamma$ be a finitely generated group and let $A=\mathbb{C}[\Gamma]$ be the corresponding group algebra over $\mathbb{C}$. Let $X$ be the set of all maximal left ideals of $A$ and let $X_0=\{I \in X : \,\, A/I \textrm{ is finite dimensional } \mathbb{C} \textrm{-vector space}\}.$ Now consider $J= \bigcap_{I \in X} I$ and $J_0= \bigcap_{I \in X_0} I$. It is easy to see that $J$ and $J_0$ are two-sided ideals. The ideal $J$ is the Jacobson radical of $A$ and it has been proven that it is the zero ideal. My question is the following: For which groups $\Gamma$ is $J_0$ also the zero ideal? Is there some literature on this question? I am interested in necessary and sufficient criteria as well as in examples, but even more in non-examples. What I know so far: $J_0$ is the zero ideal: -if $\Gamma$ is finite (that's trivial), -if $\Gamma$ is commutative, -if $\Gamma$ is the free group in finitely many generators. 

This identity appears on the Wikipedia page for the "exponential integral": $URL$ I imagine you can get it by integrating the Taylor series and playing around. Wikipedia, and several other places on the web, point to the book by Abramovitz and Stegun. 

Unless I am misreading it, the paper Affine dispersers from subspace polynomials by Ben-Sasson and Kopparty gives an explicit construction which is nonconstant on any affine subspace of dimension less than $6 n^{4/5}$. 

These kinds of questions come up a lot in Theoretical Computer Science. Tools for upper bounds include constructions based on Chebyshev polynomials and Jackson's Theorem; tools for lower bounds include Remez-type inequalities. See, e.g., these papers for several examples of the tricks one can use: $URL$ $URL$ $URL$ 

Consider the univariate case: In general, a linear combination of real-rooted polynomials is not real-rooted: Let $f_1=(z+1)^2$ and $f_2=(z-1)^2$. Every convex combination of $f_1,f_2$ which is not $f_1$ or $f_2$ is not real rooted. Here are some positive criteria: Assume that the $f_i$ have a common interlacer and that they have positive leading coeficient, then every convex combination is real-rooted. See $URL$ for the definitions (text after Lemma 4.2). If we have just two univariate polynomials $f$ and $g$, then every linear combination is real rooted if and only if $f$ and $g$ are real rooted and interlace each other. For more than one variable you have to specify what you mean by real rooted: Every nonconstant polynomial in more than one variable has nonreal roots. 

One example: A 3-uniform hypergraph is the natural way to model the variable/clause structure of a 3-Sat instance. Since 3-Sat is one of the most important algorithmic problems in computational complexity theory, hypergraphs play an important role there. For just one of many possible examples, take a look at the following paper of Feige, Kim, and Ofek: $URL$ 

For an application of the first inequality: The fraction of all $v$-vertex graphs that have fewer than $\frac14 \binom{v}{2}$ edges is ridiculously tiny (smaller than $e^{-v(v-1)/400}$). 

For every $n$ and $k$ we of course have $\mathbb{E}[m] \geq n/k$. For the upper bound, Theorem 5.2 in this paper implies that for every $n$ and $k$ we have the upper bound $\mathbb{E}[m] \leq n/k + 2\sqrt{n}$. For fixed $k$ and large $n$, this is pretty close to sharp since, as you mentioned, already for $k = 2$ there is a lower bound of the form $\mathbb{E}[m] \geq n/2 + c \sqrt{n}$. Note that the lower-order term in the upper bound has no dependence at all on $k$; in particular, not the logarithmic dependence you'd get out of a naive use of tail bounds. In fact, the cited theorem is actually showing an even stronger result: For a random 'word' $w$ of length $n$, formed by choosing each letter uniformly at random from $\{1, 2, \dots, k\}$, the expected length of the longest weakly increasing subsequence is at most $n/k + 2\sqrt{n}$. This immediately implies the abovementioned result, since the subsequence formed by taking all copies of the most frequent letter is weakly increasing, and its length is distributed precisely as $m$. 

It is classically known that every positive integer is a sum of at most four squares of integers, i.e. every sum of squares of integers is a sum of four squares of integers. Now consider a symmetric $n\times n$ matrix $M$ with integer entries which can be written as $M= Q^{\rm T} Q$ for an $m \times n$ matrix $Q$ with integer entries. Can we bound $m$ in some way? I.e. is there a constant $c(n)\in \mathbb{Z}$ depending only on $n$ such that in this situation we can always find a decomposition $M= \widetilde{Q}^{\rm T} \widetilde{Q}$ for a $c(n) \times n$ matrix $\widetilde{Q}$ with integer entries? What is the smallest possible such constant (perhaps even linear growth?)? If we look at the same situation over the polynomial ring $\mathbb{R}[T]$ in one variable, we have the following: Every sum of squares in $\mathbb{R}[T]$ is a sum of two squares and one can show that $c(n)$ grows linearly in $n$: $c(n)=2n$. Thus, I hope that a similar result might hold over the integers too. Perhaps $c(n)$ even grows linearly in $n$? Is someone aware of something in that direction? 

I think a keyword to help you here is Rademacher Complexity. Learning theorists know a lot about these kinds of questions. In particular, for the intervals-on-a-line case, the high-probability bound for the maximum discrepancy should be $O(1/\sqrt{n})$, where $n = |O|$. More generally, if $O \subset \mathbb{R}^d$ and $T$ must be the intersection of $O$ with a halfspace, the bound should be $O(\sqrt{d/n})$, I think. [I could be mistaken here, I didn't look at the details very carefully.] 

I refer to the mathematician described here: $URL$ I am interested in learning, e.g., his full name. 

Cambridge University Press is perfectly willing to publish books which are also freely available on the web (at least in 99%-complete draft form). I'm not sure how far they'd go in terms of the most liberal Creative Commons license, but here are a couple of examples of it occurring: $URL$ -- scroll down to find the link to "The AGT Book". $URL$ I think there are additional other examples from Cambridge; those are just two I knew off the top of my head. Edited to add: Sorry, I didn't notice that David Speyer already pointed out Cambridge University Press in the context of Allen Hatcher's Algebraic Topology book. 

You are right that in general this is not the case. For example let $X$ be the zero set of $x-y^2$ in $\mathbb{A}^2$ and consider the projection $X \to \mathbb{A}^1, (x,y) \mapsto x$. Perhaps you are looking for something like that: Let $f:X \to Y$ be a finite surjective morphism of real smooth varieties. Assume that the real points are dense in $X$, that the real points of $Y$ are connected and that $f$ is unramified over the real points of $X$. Then the image of the real points should be the real points of the image. 

Yes, there is a good notion of dimension, due to Berkovich and developed in my article, as mentioned in the two answers above. Concerning your question about GAGA principle for pure dimension, the answer is positive. I do not know whether it is explicitly written down in the litterature (I did not write it in my paper, for instance), but I can sketch the proof here. Since everything is invariant under ground field extension, you may assume that your ground field is not trivially valued. Now in order to prove that a strict analytic space is purely d-dimensional, it is sufficient to check that the dimension of O_X,x is equal to d for every rigid point x. But this can be checked after completion, and at a rigid point the analytic and algebraic local rings have the same completions, QED. Remark: there is certainly also GAGA for irreducible components (an irreducible scheme has an irreducible analytification) but it is not so easy to prove to my knowledge. Using normalization it can be reduced to GAGA for connectedness, for which I do not know any easy proof: one can compactify and use projective GAGA+Hartog's theorem, or use Berkovich's comparison theorem for Ã©tale cohomology at the H^0 level (!). Maybe there is a simpler one using Noether's normalization? 

The sharpest multidimensional Berry--Esseen Theorem I know is due to Bentkus and appears in the paper "A Lyapunov type bound in ${\mathbb R}^d$". $URL$ It does not use the characteristic function, though. 

My guess is that the optimizer is actually a "strip"; i.e., a set of the form {$x : -t \leq x_1 \leq t$}. But I'm somewhat sure that the solution to this problem is not known. You might take a look at the discussion surrounding after Corollary 3.6 in this paper by Klartag and Regev: $URL$ Barthe may also have some relevant papers. 

This does not fit the original poster's question but is vaguely related: I have heard it said that Carleson did not get the Fields Medal in '66 because his proof of Carleson's Theorem was too difficult to read and verify at the time. (Granted, the result was only published in '66.) And alas, he was too old to get it in '70.