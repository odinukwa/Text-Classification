^^ this works, but NO, not granting full su permissions to this user. Since it works when globally allowing I'm assuming it's somewhere in my syntax. Edit - In order to get around this issue I just put the command I want to run in a script and granted sudo to the script. However this is not ideal and adds an extra step in this workflow. Would much rather be able to run the command directly with sudo. 

Is there any way to allow the upload (put) to the default directory, without having to chdir first? I do have flexibility to move these directories around. For example, the sftp chroot dir doesn't have to be in the users home directory, I can even change around the users home dir (but the user must still be able to use authorized_keys to login). Please note: I do understand many SFTP clients, and the command line SFTP client allows for defining a relative path at login. That is out of scope for this question, I'm desiring this config be done server-side and the client simply just needs to login. 

I've seen this same question asked on several other forums (and SE) with no definitive answer, so I'll try to explain as best I can so that anyone with info understands the problem: AD Server -Win2008 R2 -Not using roaming proviles -Single small GPO Domain Client: Win7Pro x64 - Problem: When a user is working from home (they take their laptop home), they need to wait about 45 seconds before being able to login. Once the mysterious "timeout" is reached, the local cached credentials are used and the system allows login. Root cause: The root cause is that the system is searching for domain controller, but the machine isn't on LAN/VPN so it can't access it since the user is working remote. Workaround: If the user has Wifi disabled AND has unplugged network cable (machine has zero network) then it will login at normal speed. I assume this is because the client is not searching any network for the DC. My question: So what I need to know, is -- how can I shorten this "search time" so that the machine doesn't wait so long trying to find a DC that it will never find. Asking all my users to handle their wifi/network enable/disable just to log into the machine isn't going to work well. If the answer is "that's impossible" then so be it, but I would like a definitive result on this issue. Thanks for any help. 

SSMTP is expected to be TLS from the start. This should work if you set UseTLS to Yes. This port/protocol has been obsoleted now that the STARTTLS option is available. If you want to use STARTTLS try the submission port (587). I have verified that the submission port is open. 

Seriously consider setting up MX records for the domains like foo.bar. Also consider SPF for both versions of the domain. If you mail server is these records should work (SPF records are optional an may not be accepted by old DNS servers.): 

It is common for the reverse and forward DNS records to be on different DNS servers. Whoever provides the IP address will maintain the record for on their DNS servers. The domain registrar will maintain the DNS for the domain. Either of these can delegate the records to another server. If a delegated domain is migrated, it is important to ensure the delegating domain is notified to update the glue records. Until they upgrade their records, DNS lookups will fail when they try to used the old addresses. In this case, it appears you need to notify the IP provider to update the NS records for your IP address block. 

I wouldn't recommend Skype on a secure LAN. It requires a mostly open firewall configuration. All ephemeral ports, and few others need to be open. Incoming traffic needs to be allowed to the PC running Skype. If required, I would setup a separate LAN segment with uPNP (PMP) enabled. I did go through the exercise of figuring out what I needed to do when my wife was traveling. See my blog entry on firewalling Google-Chat and Skype. 

I did a google search (I know that does not provide the answer per se) on the linux kernel mailing list site, but the entries found seem to indicate that it is more or less something harmless. not sure if any entry actually applies to your case, but you can take a quick look at the search result of: local_softirq_pending site:lkml.org 

Almost certainly a hardware problem. Get a diagnostic tool from your vendor and run it through. If you cannot get something like this, at least run something like memtest provided by most Linux versions, which you can run by getting a lived. If it is in warranty, it is best to ask for vendor service. 

It certainly could, but automatic reboot is usually associated with hardware/software issues such as overheating or kernel bugs. So it is possible that heavy load causes overheat which leads to reboot. In any event, you should investigate the log or kernel dumps to find the exact cause. 

Did you try to reinstall grub to new sda? Are the disks, old and new, the same size? If they are, dd the entire disk and it should work. If not, you can transfer files with rsync or tar, and the use livecd to reinstall grub. 

If you have ssh access, you can tunnel the VNC connection over ssh. VNC access is often blocked by firewalls. VNC is not an encrypted protocol, and I wouldn't run it directly over the Internet. You can usually run ssh to any appropriate system on the target network. You don't need to have an ssh connection to the target system. 

This is accepted use of the database, and the reason you see that line in the file. Adding entries after that line allows you to quickly identify local additions. It is common for the /etc/services database to be incomplete, so you may be adding some registered ports. Some tools will accept ports by name if they are available from /etc/services, or an equivalent source. The one risk you have is loosing the additions when patching the O/S as /etc/services may be updated. Make sure you have a copy of the local services that you can quickly append to the new file. Consider automating the addition of missing entries to the file. If you assign ports for services like NFS and VNC, this is a good location to record those ports. VNC has defined port ranges NFS plays better with firewalls, if you configure it to use fixed ports. When possible, I would recommend using the IANA registered service ports, even if they aren't listed in the supplied /etc/services file. This can prevent future issues if they get added to the file. 

I can connect and authenticate without issue over clear text (unencrypted) but cannot seem to communicate with the server over TLS. I connect to this server over TLS through various other systems (our code repo, jenkins, etc all authenticate over Encrypted ldaps protocol to it over port 10686 so I know the server is responding fine over TLS. It uses self-signed certs but that hasn't been an issue so far with other services connecting to it. Based on the log files below, it seems the directive triggers the StartTLS function which I do not want. Regardless I've still experimented with using it... Different configs I've tried: WORKS: (unencrypted) 

Issue: In passing a variable declared within Jenkinsfile to a command that ssh's and executes on a remote host, the variable contents are not retained on the remote host. Built-In Jenkins variables retain fine both locally and on the remote host. The variable I've defined works fine locally but doesn't translate on the remote host Although this issue references docker, this is actually 100% Jenkins Pipeline based as it could apply to any example with or without docker Background: I'm trying to dynamically create an image name based on the current build tag, and put that name into a variable. Then I pass that variable into step which remotes over to a docker host and runs a build step with the name defined. Snippet of applicable parts of Jenkinsfile... 

Port can be reassigned to confuse people etc., so if you have a plain http server actually listening on port 443 then your http url should work, and vice versa. However the restriction you are facing may be more than just the port. Since ssl and http are different protocols, it is trivial to figure out that the packets are not http and your connection is thus blocked. It is probably easier to do ssl tunneling if your http traffic in this case. 

Virtualbox and vmplayer will do also. Xen is another. For guest os you might want to start either with jeos or one of the minimal ubuntu isos. You can easily find all these. 

although it would be nice, but there is no unapply available. you would have to write an undo recipe yourself, depending on what you did exactly (installed package? then purge it, added user? then disable it, etc.) the replaced files should be stored in the clientbucket (/var/lib/puppet/clientbucket usually but it depends on your version and your setting) 

You can use a text browser such as lynx, or URL fetcher such as curl or wget, or even telnet to your http port directly. 

Not sure if there are opensource tools, but GoldenGate which was acquired by Oracle claims to replicate between any major databases. Now that MySQL is also Oracle I won't be surprised that this is supported, but you should contact them if you are interested. I personally have not used this product but have a friend who used to work in the heterogenous database replicatio there. So this is just a pointer for you. 

All authorized mail servers should be listed in the SPF record. As you now have a new authorized mail server, it should be added. Some SPAM checks differentiate between listed (A, MX) and permitted (~all), and will not treat unlisted address as a pass. This penalizes senders who don't send via an approved server (often spambots). The policy indicates that the user doesn't really care who uses their domain, as compared to the policy that could get the email blocked or quarantined. The alternative approach is to configure the new server to relay messages using the existing server. It is common for applications to allow an email relay server to be configured. In python, you would configure your VPS server name instead of localhost. Whichever server is sending email, it should add the required headers when receiving the message from your application. Alternatively, you can add the headers in the application. There is a defined format for the date in the Date header. The Message-id header has a defined format, but it not as strict. Message ids resemble an email address, but the left side should be a unique id. 

Try checking your configurate with a testing site such as $URL$ This is from the OpenSPF.org site. They also have a tool to help build your SPF record. EDIT: You have a large list of servers. You should only have a few servers sending email directly to the Internet. Only servers sending email directly to the Internet (boarder servers) need to be listed. All internal servers should relay out through these servers. If you have roaming users, then setup authenticate access on the submission port for them to send email. If your domain is being used to send spam, then you may be getting backscatter from poorly configured servers. 

So those directories don't match, you never actually mounted the htpassswd file If you write out to it will work since it's writing to your current directory, not to The following worked perfectly for me: 

EDIT: I've also tried setting the host definition to a standard ping check, and put in a service definition for the command. The host shows UP but the service is DOWN. Same exact error "Socket timeout after 10 seconds" -- The command line use is nearly instant with the UDP OK return. I am stumped. EDIT 2: I tweaked the debug settings, and have turned logging verbosity up to the highest value (2). It doesn't really tell me much, but it looks like Nagios is interpreting the command as expected... 

when developing Bash scripts, sometimes I'll run it in a Docker container, or on a VM to test it, but the ideal place for me to build out scripts is just on my local MacOS workstation. When it comes to Bash scripts, this has never been a problem so far. However today I noticed that the command behaves differently between Linux and MacOS. Example scenario, adding days to a date object: Linux: 

Firstly, I am using these as a base guide: $URL$ $URL$ Secondly, our region doesn't support the AWS directory service Scenario I am looking to create a "base AMI" for a set of application servers we have. I may need to deploy 1 or more new servers based off of this image. The instances are originially created from EC2 Windows Server 2008 R2 Datacenter base AMI (Created by Amazon) The applications themselves are static, licensing is completely redistributable, and the config doesn't need to change on one machine vs another. The ONLY thing that is different from one machine to the other is the hostname and network config. Network config is handled by EC2, so the hostname is really the only thing that needs to change from once instance to another The machines are part of Active Directory, and have specific OU Group Policy rules applied to the machines. They will all join the same OU. Goal: My goal is to have a base AMI. When this AMI is launched it auto-joins the domain OR is already joined to the domain. The applications that are ran from the machine REQUIRE domain accounts to run the Windows Services. So I can't have an image that's not joined to the domain. An idea I had (Will test this tomorrow): 

You would have to change the adapter to bridged so that the vm can get an ip in your LAN. Or you need to have a way to forward the icmp traffic in your LAN to the vm. 

if that linux machine is ubuntu or similar distributions, it will not bother checking the ip conflicts and will grab the ip whether it has been used or not by another server. windows may be doing the same thing (i have less experience there so am only guessing). you should contact your network administrator to sort this out, as neither servers will be working correctly. 

There should be no expected issues just from running this command (otherwise the man page will be full of dire warnings for sure). If things die, then it would not be due to atime update turned off, but some hardware problems you have. that remount simply turns off the atime updates and will thus have LESS disk usage and therefore make things less likely to die. Have done this sort of things many times and never see any issue. If you are still concerned and if you can later restart the system or mysql, you certainly can do so. 

using postfix is pretty simple /etc/postfix/main.cf just set up this line: relayhost = $mydomain assuming rest of your configuration such as firewall etc. allows this. this package is available in all linux versions. however, delivering to your gmail account is trickier, as gmail may consider it spam if it figures out that an agent is sending it. you need to do more research if that happens. this is what i ran into at one time. 

I'm trying to find the history of container restarts. Of course the field on a will show the current uptime. However if I have a container with a restart policy such as and it's gone through several restarts - How can I check that restart/uptime history? If the docker engine doesn't natively track this - is there a known good method to handle this? 

This is commonly done with instance An EC2 instance can have any number of tags in key:value format. They are often used liberally to help further processes identify instance roles/types/environments/etc Some examples: 

Since you mentioned Python, I assume you're using BOTO. Check the boto documentation, you can even filter on certain tags: $URL$ Noted on that doc page: 

In the above example, the Subnet for clients will be 10.10.18.0/24 It looks like, from your log you are pushing a subnet (192.168.5.6/255.255.248.0) so that may already be configured properly. Routes: The errors are specifically complaining about the routes. Your log shows 0 out of 0 total routes were successful, so it sounds like it doesn't even have any routes to push too. 

I'm following this guide: $URL$ Which works great! My backups are saving to s3 without issue. However I would love to put an S3 lifecycle policy to purge backups older than 14 days. This is easy enough to do, but there are some other items in the bucket related to gitlab backups (restore instructions, some configs, etc) that I never want to purge. These can be easily skipped by only including backup files via S3 lifecycle policy "prefix" rule which only purge files with a certain prefix in their filename. Great, however the prefix of the gitlab backups using have a dynamically changing prefix. My question: Is there a way to change the file name format of the backups created with ? Or is there a way to alter the Lifecycle policy to take a suffix rather than a prefix? Any suggested alternatives to meet the goal is appreciated as well.