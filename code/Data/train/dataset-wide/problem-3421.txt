Design the system for the expected load. To do that you have to actually know what kind of load you should design for. To know that it's best to gather data from a smaller-size implementation and extrapolate from it. To handle the stampede of users in the morning you can have the virtual desktop machines pre-created, waiting for the users to appear, so the load on storage systems will be manageable. You modify things on Friday afternoon, tell the infrastructure to re-deploy and by Monday morning you have 1 500 new images waiting eagerly for the users. Consider putting the golden images and their linked clones on SSD disks. SSDs love random read access and are a good investment here. Consider 10 Gbps networking infrastructure. Remember, that a saturated 10 Gbps pipe can eat a core of a CPU just for networking processing. Use a sufficiently performing filer (e.g. NetApp or Storwize V7k) to serve network shares and profiles to the users over CIFS. I am not sure if you are interested in designing SAN or LAN for the load, but there are documents addressing both cases, e.g.: 

Take a look at this post and the whole thread. Somebody had a similar problem to yours. Also in the thread there are shown commands which will help you debug the problem. 

Can you reproduce the problem on a test/development machine? Is there something, that triggers the failure (backups, load spike, particular query)? Is the problem intermittent (you log a couple of errors and then the server runs OK), or persistent (once it starts showing up, it keeps showing up)? Personally, I wouldn't trust the machine and I'd migrate the data to some other server while looking for the cause of this errors. 3 disk failures in short time are possible (people win lottery sometimes), but they aren't very probable. 

I'd recommend trying it first on the local machine -- it will make debugging easier. Only when you are sure all elements work, do it on the remote host. The whole trick hinges on ability of being able to FTP remotely as root to a non-chrooted environment (which is not the way I'd recommend to configure FTP server). 

Feature stability. Latest and greatest is wonderful, unless you have to change config of all your production machines right now, because the new version behaves differently/has changed config/input/output format. Stability and security. Because users cannot upgrade to the newer package version (see 1.), the distribution maintainers backport important changes to the supported version. The effort needed for this backporting is one of main reasons, that only some versions of some distributions are being maintained for extended periods of time. 

Linux auditd ($URL$ will give you most power in watching who did what. You have more about it in DerfK's answer. Nothing, however, is going to tell you, who logged in as webadmin, if there are n people who have access to the webadmin account. I would suggest to use named accounts for each user and then either use su - or sudo to run commands from the "function" account. 

CLOSE_WAIT means the other side closed the connection. The socket will be closed after the local program closes the socket descriptor. There is no time-out for CLOSE_WAIT, so process can be stuck with a socket in this state indefinitely. When you kill the process and its children, they close sockets and they get closed. Run and see if the children have the sockets open. If they do, then it looks like a bug in their code. As for FIN_WAIT2, it's when local side waits for FIN,ACK from the other side to confirm closing the connection. However, there's a system-wide time-out on this state (see ), which is by default 60s, so nothing should be stuck in this phase longer than a minute. BUT if seems that it's possible to code a program in such a way that a half-closed connection looks like an active one to the kernel, so the time-out won't kick in. Again, it would seem that you've found a bug. 

For btrfs you need option to enable TRIM support. A very simple but working test for functional TRIM is here: $URL$ 

Method 1: Read the requirements, compare with the list of installed packages. Method 2: Run the configure script. It will fail on missing dependencies. By the way, when you are thinking of installing from source, consider creating a package for your distribution's package manager on a test system and install the package on the production system. Package management systems are a blessing for administrators. Give them a chance. 

It is unlikely, that the scan originates outside the 10.51.0.0/16 network. Typical firewall rules drop packets that claim to origin from a locally-connected network that come through wrong interface. Then, maybe you have atypical firewall rules ;). I'm not sure if I got your question right, but if 10.51.1.15 is the IP of the CentOS box, then apart of the ideas above dont' make sense. If the packets appear to originate from the local box then it has packets to deliver to these addresses. In that case you could set up iptables rules that log packets to invalid addresses of 10.51.0.0/16 network. By invalid addresses I mean addresses which have not been assigned to any host. If something is trying to access them, then probably it has no legitimate reason to do so. Such logging rule (with corresponding limit rule to prevent log filling) will tell you origin of such a packet -- if it was from a local box, or any of the remote networks. If the packets appear to originate from the CentOS box itself you could also take a look at output during a suspected scan and compare it to output saved when there was no scanning. Maybe you'll find some obvious answers. 

A 1. & 2. In VMware you have 3 types of network: Management, VMkernel (IP Storage, i.e. NFS ans/or iSCSI and VMotion) and guests. In ideal world, you keep them separate and each with at least 2 physical interfaces to avoid a SPOF: - Management doesn't need much bandwidth, but you don't want VMs to mess with packets there. - If you keep datastores on NFS / iSCSI then VMkernel will eat bandidth. VMotion too. In ideal world you separate them to prevent VMotion affecting host's access to its datastores. If you don't have datastores on NFS (or they are rarely used, e.g. keep just templates and you don't deploy servers by dozens) it's one network - VMs' network -- kept separate for security reasons. This can have multiple VLANs defined, trunked over physical ports in the virtual switch. If you cannot separate the networks physically (because you don't have enough NICs), it's good practice to have them on different VLANs and IP subnets. A 3. For ESXi cluster to make any sense the datastores (i.e. disk space where your VMs live) should be kept on a shared storage. The file system for datastores (VMFS) is parallel and cluster-aware, so it's not only safe, it's recommended. If any of your physical machines dies, the surviving hosts will restart VMs. They won't be able to do this if VM disk images are on the dead host. A 4. You can define a VLAN, that comes from vSwitch in host A, through physical interfaces, physical switch(es) to host B. If it does not include anything else in the physical world, you now have a "private" network connecting VMs on different hosts. Actually, all VLANs used by any VM should be defined on all physical hosts. That way you enable both VMotion and HA features between hosts. A 5. Reading is good. I'd recommend official VMware documentation -- you won't get any misinformation. I went to some VMware trainings, but they are as good as the trainer. Some just run you by the script, others know a lot or know where to get the answers to your questions. Beside that caffeine, chocolate and pizza help ;) 

What you are looking for is a modem (or a cellular phone working as a modem) and pppd (point-to-point protocol daemon) configured for a dial-in. There's a HOWTO, e.g. here: $URL$ which may help you. If you manage to make a USB-connected cellular phone to work as a serial-port connected modem (which should be possible), the rest will be easy. You could also go for a traditional modem and a landline to the box you want to connect remotely to. This would be definitely easier, but could be more expensive to set up. 

Put package(s) required for sshd re-installation into some directory, e.g. /root/sshd_reloaded. Prepare a script, which installs that package and starts the sshd demon. FTP the packages to the proper location and the script to (or equivalent on your VPS system) and chmod it 0755 to make it executable. The general idea is to make cron pick it up and do the work for you. If you don't want to wait, you could probably put a file into /var/spool/cron/ to make cron execute a script which installs and starts sshd. Wait for the cron to pick up the new file and execute the job. Log into your VPS, change root password (it went in cleartext through FTP) and sin no more ;). 

Would disabling password authentication and requiring public-key based be OK with you? You could also disable ssh completely, if you have some remote HW management solution, like remote console (IBM RSA or HP ILO). If you have physically separate management and production LANs, then it wouldn't be possible to break into the server by logging into it. 

The only method is to destroy and re-create the array with new parameters. Neil Brown have written in his recently published road map for md: 

How much RAM do you have in your VM? F15 installer is comfortable at about 1GB. Gnome-shell uses video acceleration for its functionality, but from what I read in this release there's a fail-back mechanism for environments that don't provide hardware acceleration. 

This is possible, but requires a lot of time and effort, pretty much defies the point of using a LTS disribution. Another thing: Samba isn't the only potential problem. What about kernel/glibc/other essential packages? However you cut it, you'll need some infrastructure for upgrading the boxes. The easiest (but not always feasible, I know) thing would be to use the one set up by the distribution maintainers. 

Tweak with --limit and --limit-burst values until you like the result. From Rusty's Remarkably Unreliable Guides ($URL$ 

If the ARP packets originate from a Linux box you can try generating a generous amount of iptables rules with --pid-owner XXX option (matches if pid of the process creating the packet is XXX; you'd have to cover a large range of pid numbers) and hope that the process that actually sends packets isn't a short-lived spawn of something else. Alternatively, you could use (much fewer) --uid-owner XXX options to find the uid of the owner of the process that sent the packet. On a tangent, if 211.123.123.242 is your gateway and it looks for MAC addresses corresponding to various IPs from this network, then it may have some packets to deliver from outside the network. Who and why tries to communicate with non-existing addresses may be actually more interesting thing to investigate, than hunting for the originator of ARP requests on the gateway box. 

I'll answer your questions in order. o) If you plan to reuse the old drives I'd keep their layout as is (or changed RAID 5 to RAID 6 for the non-boot part), and put a Linux-RAID partition spanning whole disks onto the new 2 TB ones and made RAID 5 or 6 out of them. With such large disks you are running a reasonably high risk of encountering a read error while recovering from a failed drive, so RAID 6 is a very tempting idea. If you both serve files and host VMs, you might make a RAID 10 out of old disks to hold your system and VMs (RAID 10 is going to give you better performance) and RAID 6 out of new disks for the served files. You might go the other way round if you need more space for VM images than for served files. On the RAID space I'd put encrypted volumes. It's made easy by modern distros' installers, just remember that your machine will not reboot without human intervention (providing password for the volume). That way all your data is protected from unauthorised access if it's stolen. The next layer would be LVM because of flexibility it gives, not least because it allows you to fsck a read-only snapshot and find possible silent data corruption without bringing your whole system off line. I think you would do well with two volume groups, one made of old disks, one with only the new ones, this way you know what kind of performance you can expect from which parts of your system. If you don't care or it's not going to be in any way relevant, you might do with just one volume group. I like to have my system and data separated, so I'd made at least three logical volumes into one group (to host swap, / and data) and one in the other (just for data, on the newer, faster disks). Logical volumes are block devices, so you may create filesystems directly on these. o) RAID 6 should be considered. will fail recovery if it encounters read error while resyncing array after drive failure, so you may want to be doubly insured. RAID 6 will cost you performance, and it is not a replacement for backups. o) If you want your data to remain confidential after disks are stolen/disposed then encryption is definitely a good idea. It will cost you performance, because of encryption related computation, but if it's going to be acceptable or not is going to depend on the load you are going to have and CPU power of your hardware. If the server is in a locked, secure room, it may be overkill. If it's in a closet where somebody fast enough could just grab it and flee, it's a very good idea. Any layer you add to the configuration (RAID, LVM, encryption) increases complexity of recovery, so you should plan ahead and have proper tools prepared. I think (not know) that most modern recovery CD distros will just ask you for the password for the encrypted volume, and I know they will take LVM and RAID in their stride. o) You already know what kind of performance you can expect from software RAID -- you run one :). LVM is not going to be a significant overhead, and encryption is CPU-intensive. You should know if your current/expected load is CPU-intensive and plan to have room to spare. Remember, that any disk access is going to cost you extra cycles now, including any I/O from your VMs. CIFS/NFS servers usually have plenty of cycles to burn, I don't know what kind of load you plan to run on your VMs, so cannot comment there. 

This thread may give you some insight into ext4 file placement algorithm. has a function, which seems to give the data you want. You should be able to give it consecutive blocks of a file and get the physical block numbers. 

I'm not sure what you mean by bonding in context of SAN. Multipathing in SAN is done first of all for RAS. You usually have paths to the LUN independent. That means different FC cards, different fabrics, different storage controllers. If your storage supports active/active operation, then you also get to use bandwidth of both links (you can issue requests in parallel over both of them). If your storage does not support active/passive operation, then using multipathing means that you'd loose half the bandwidth, as you'll be able to talk to the LUN over one link at any given time. However, loss of throughput (if any) is, in my opinion, more than acceptable, given increased availability. You eliminate storage-side SPOFs that way and e.g. can stagger SAN firmware upgrades / zoning changes, without risking impacting production (if after upgrade the FC switch goes haywire you can just switch to the other fabric while SAN folks execute recovery strategy).