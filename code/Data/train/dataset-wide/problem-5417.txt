I'm not an economist, but here's how I see it... Fundamentally, economics arises because of the following: 

Attempts to establish that some sentences are true by virtue of their meanings is either vacuously true or viciously circular. Even sentences identified as analytic by its defenders are not actually immune to revision in the light of new data. For example, the definition of momentum was revised in the light of special relativity. Sentences cannot be understood one-by-one in isolation from others, but only by reference to a broader quantity of knowledge. This is Quine's holism thesis, and implies that we cannot neatly parcel up sentences and specify verification conditions for them individually, nor identify those that have no verification conditions because they are analytic. The concept of 'meaning' is according to Quine unclear and lacks explanatory value. This is part and parcel of his doctrine of the indeterminacy of translation, and follows from his behaviourist approach to linguistics. 

There is rather more to this than saying it is an example of denying the antecedent. Denying the antecedent is exemplified by the form "If P then Q; not P; therefore not Q". Contrary to what the wikipedia page says, this is not always the same as "If P then Q; therefore if not P then not Q". This equivalence holds in the propositional calculus, but not in all formal systems. For example, it does not hold in logics with possible world semantics or in probability logic. Also, strictly speaking, only formal languages can have formal fallacies. English is not a formal language, so any attempt to find a formal fallacy in it depends on assimilating the English sentence to a sentence in some formal language such as the propositional calculus. But any such assimilation is open to the challange that it is not a correct representation, or that the choice of logic is inappropriate to the task at hand. This is one of the reasons fallacy hunting is highly overrated and overused. Furthermore, one should not forget that in ordinary discourse, the pragmatics of language is just as important as the semantics. Often when one asserts "if P then Q" it carries the conversational implicature "if not P then not Q". This was described by Grice, who distinguished conventional implicature from conversational implicature: the former is part of the meaning of the terms used and is not cancellable, while the latter is something conveyed by adherence to the cooperative principle and is cancellable. An example might be as follows: suppose Alice says to Bob, "if you wash my car I'll give you $10". Bob is entitled to infer in the circumstances that if he doesn't wash Alice's car, he won't get the $10. The whole point of Alice qualifying "I'll give you $10" with "if you wash my car" is to provide Bob with a motive to wash it. If Bob believes he will get the $10 anyway, he has no reason to wash it; Alice knows this and intends it when she makes her utterance. Alice could perhaps have chosen to say "if and only if you wash my car I'll give you $10" but in practice we typically don't bother to say this: we allow the pragmatics of language to do its fair share of the work of conveying the meaning. 

Your question in effect relates to the distinction between extensional and intensional relations and the underlying problem of universals. The sentence 

Alice: Coke is the best drink in the world. Therefore Coke is better than Pepsi. This argument is valid, but it begs the question, because one would not believe that Coke is the best drink in the world except by first comparing it with all the other drinks, including Pepsi, and finding that it was better. I guess that this is what you mean by shaky premise. 

The logic of conditionals is considerably more complex than most elementary accounts provide for, and the literature on them is vast: it runs to scores of books and thousands of papers. There are several issues with your sentence that would require clarification. 

Rothschild doesn't consider your q to be a propositional assertion, but rather an expression of an attitude towards a proposition. He is using proposition here in a limited sense of a statement that has truth conditions, so "it might be the case that p" is not, for him, a proposition itself but a doxastic property of our attitude towards p. To put this in a broader context, Rothschild is addressing the question of whether conditionals can be said to express propositions at all. On the suppositional account of conditionals, which is held by Ernest Adams, Dorothy Edgington and Jonathan Bennett, conditionals are not propositions, because they do not have truth conditions. Rather, they serve as devices to introduce a supposition and make an assertion within the context of that supposition. Rothschild is sympathetic to the arguments that support the suppositionalist account, but he is trying to steer round them and argue for a propositional account of conditionals under which they can be understood as quantificational restrictors. A version of this position is also defended by Angelika Kratzer. 

There have been many very different philosophies advanced by Christians over the years. This suggests that there is no single thing called "Christian philosophy" but rather many philosophies defended by Christians. Apart from making room for a single deity, they have little in common. For example, Augustine was a platonist, Aquinas was an aristotelean, Descartes and Leibniz were rationalists, Locke was an empirical realist, Berkeley was an idealist, Kierkegaard was an existentialist, etc. 

Option b looks correct to me. Your descriptions for a and b seem to be the wrong way round. Option c says that anything that is both a dog and a cat and well-trained is a good pet. Option d is weird - it says that if anything is a cat then well-trained dogs are good pets. 

Left-nested conditionals, like your (A → B) → C are actually quite rare and sound strange in natural language. Right-nested conditionals A → (B → C) on the other hand are commonplace and are typically considered equivalent to (A ˄ B) → C by the import-export rule. For example, "if I bet on this horse then if it wins I'll buy you an ice cream" is the same as "if I bet on this horse and it wins I'll buy you an ice cream". 

If I can summarise your question, it seems to be: needs are only relative to a given end, so if we say we need something then it must be because we need the end it serves, but then that need must be relative to another end, and so on, so how do needs arise in the first place? The problem leads to a trilemma. Either 1. there is a infinite regress of needs, or 2. there is a circularity, or 3. there is a foundational point that gives rise to a need but does not require one itself. This kind of structure turns up in other places in philosophical thinking, for example in explaining the justification for believing a proposition: if I am challenged as to why I believe some proposition and I offer another proposition as the reason, then how do I justify the second one? The same trilemma arises: my justification will lead ultimately to a regress, a circularity or a foundational point consisting of a proposition that is unquestionably true. Can there be an infinite regress of needs? Hardly, especially for a single person. Are needs circular? Perhaps. "I eat to live and I live to eat" is a coherent thought that might describe how some people choose to live. More widely, we think of life itself as being circular in the sense that living individuals need to reproduce in order for the species to survive, and the species needs to survive in order for there to be individuals capable of reproduction. Are there foundational needs that require no reference to a further end? For an individual person, no, since as you say, a person doesn't need to exist. Is there any broader sense in which there is foundational need? That is a difficult metaphysical question that probably has no answer. Does the universe need to exist? Not as far as we know, but who's to say? 

Causal relations are typically neither necessary nor sufficient. In the case of your water boiling example, one can heat water to 100 degrees without it boiling because it may become superheated, so it is not sufficient. Mackie in his book Cement of the Universe analysed causation as an INUS condition, meaning an insufficient but non-redundant part of an unnecessary but sufficient condition. Quite a mouthful. Understanding what we mean by causal claims however is a lot more complex than simply looking at conditions. We typically use causal claims to explain things. If you want to understand causality further I can recommend Woodward's book "Making Things Happen" and if you have a good head for mathematics you might like Judea Pearl's book on Causality. 

Alice: Bob was watching the football game earlier and now he's upset. His team must have lost. This is a questionable cause. Maybe this is why Bob is upset, but there are many other possibilities. I guess this is what you mean by imagined connection between premise and conclusion, though this is not the best description. The connection may be plausible but far from certain. 

Information has more than one sense. The SEP article on information provides half a dozen. The Shannon concept of information, which you refer to, is rooted in communication theory. It is naturally interpretable as the maximum amount of 'content' that can be conveyed in a message. More generally, we can think of it as a way of quantifying the concept of distinguishability. If I can distinguish between a switch being in the up position or the down position, then I possess information. To be precise, I possess one bit of information. Likewise, if I can distinguish between here and there, off and on, this and that, etc., then these are all examples of information that I possess. Information can be correlated, so it is not simply additive. If I can see by looking at the light switch whether it is up or down, and also see by looking at the light whether it is on or off, this does not sum to two bits of information, because the two are strongly correlated. Not perfectly correlated, because the light may be off due to a power failure or a circuit breaker tripping out, or it may be on because a mischievous person has shorted the switch out with a nail. But Shannon information theory, coupled with Bayesian probability, can allow us to express relationhips in such a way as to allow us to quantify how much distinguishability I have. Although I speak here in the first person, information in this sense does not have to be possessed by a human agent. One could speak of a measuring instrument or a computer having information. Information can be understood as a quantitative measure of the number of distinctions represented by a set, or a distribution, of possibilities. I believe this is the sense in which the concept of information is most commonly deployed in physics and biology. According to some theorists, it is no accident that Boltzmann entropy and Shannon entropy have a striking parallel, as you put it. It is possible to interpret statistical mechanics as a purely statistical theory based only on some simple properties of matter at the microscopic level. Arieh Ben-Naim has written several books on entropy from this perspective. In particular, in his "A Farewell to Entropy" he shows how the Sackur-Tetrode equation for the absolute entropy of an ideal gas can be derived from information theoretic considerations. On this view, the second law of thermodynamics is not so much physics as an application of Bayesian statistics. This view is not undisputed, however, and more detailed consideration would take us into ergodic theory. At the level of human cognition, we think of information as relating to semantic concepts such as meaning and truth. A message may contain information in the Shannon sense, but if it is the ciphertext of an encrypted message and I don't have the decryption key, it conveys no meaning to me. Also, a message may simply be false: Floridi (The Philosophy of Information) prefers to restrict the term information to things that are meaningful and true. I think you are correct in saying that there is no generally accepted way of combining these two concepts of information. Dretske (Knowledge and the Flow of Information) made a valiant attempt at it, but it only goes so far. One might say that the problem of relating information in the purely mathematical sense to information in the semantic sense is an example of the problem of relating extension to intension. If we had a systematic way of doing that, logic would be a lot simpler and more powerful. Speaking of logic, information theory suggests a way of interpreting certain common concepts within logic. A valid argument might be thought of as one in which all of the information in the conclusion is present in the premises. A tautology is a sentence that conveys no information, etc. Information here might be understood in terms of the set of logical possibilities that are excluded. If you are interested in this, a couple of useful papers are David Ellerman "An Introduction to Logical Entropy and its Relation to Shannon Entropy" International Journal of Semantic Computing, (2013) 7(2): 121-145; and Jon Barwise "Information and Impossibilities" Notre Dame Journal of Formal Logic (1997) 38(4): 488–515.