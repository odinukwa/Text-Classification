This specific problem is not too hard. The theme of the proof is to first show that the path reaches the $(2,2)$ point in finite time almost surely. (That is, although their are paths that circle the perimeter forever, the probability of such a path of infinite length is zero.) Then once you know you will eventually reach the $(2,2)$ point in finite time, the path reaches each of the exterior points in finite time almost surely. First part: Assume there is a path with finite probability which never visits $(2,2)$. Then at some $t_0$ a path is not at $(2,2)$, and has not come from $(2,2)$. Then the state at $t_0$ is at some exterior point, and has only one choice of direction to leave in other than going to $(2,2)$. Since at each non-corner step there was a $\frac12$ chance of going to $(2,2)$. So the path taken will a.s. leave the non-center path in a finite number of steps. Second part: The path will a.s. reach a given side point (e.g., $(1,2)$) in a finite number of steps. For we can wait until the path is at the center point, and the path has then either come to will next go to $(1,4)$ with probability $\frac13$. If the path does miss $(1,2)$ then a.s it will arrive at the center point again in a finite number of steps (indeed, since we are satisfied if $(1,2)$ is reached, this finite number of steps is at most $5$ steps), and again there is only a $\frac23$ chance of proceeding without having touched $(1,2)$. So the path will a.s. reash any given side point in a finite number of steps. Finally, given that the path a.s reaches any side point in a finite number of steps, it will also a.s. reach any corner point in a finite number of steps. For example, consider the corner point $(3,3)$. The path a.s. reaches $(2,3)$ in a finite number of steps, and a path missing $(3,3)$ will have come from either $(2,2)$ or $(1,3)$. In either case there is a $\frac12$ probability of hitting $(3,3)$ on the next move, and if it does not, then we wait until in (a.s.) reaches $(2,3)$ again. As before, this iteration of an indefinite number of choices makes the probability of a path that never reaches $(3,3)$ go to zero. The above proof, while acceptable, leaves something to be desired, since the property of visiting every point still holds on a larger grid, where the nice feature of having a center point and a group of points equivalent to $C_8$ in a ring around it no longer applies. 

For $n\in \Bbb{Z}^+$ define the statement "$n$ is $k$-social" to mean that $$ \prod_{i=1}^n p_i +1 \mbox{ has exactly } k \mbox{ prime factors} $$ where $p_i$ is the $i$-th prime. So for example $5$ is $1$-social while $7$ is $3$-social. Similarly define "$n$ is $k$-antisocial to mean that $$ \prod_{i=1}^n p_i -1 \mbox{ has exactly } k \mbox{ prime factors} $$ For completeness, call $n=1$ to be $0$-antisocial. (There may well be another more standard terminology for what I am calling a $k$-social number, and/or the prime number it generates. But these are different than the Euclid number primes.) I had been trying decide the question of whether there is a largest $1$-social number (and similarly a largest $1$-antisocial number), and if there is no largest, find the asymptotic density and distribution of $1$-social numbers, but have not been able to make any progress beyond hand-waving arguments based on the distribution of primes. [As an aside, $25$ is both $1$-social and $1$-antisocial; given the rarity of twin primes, that seems surprising.] But here is a question which may be easier: Define a number to be "friendly" if it is $k$-social and $m$-antisocial with $k>m$; "hostile" if it is $k$-social and $m$-antisocial with $k<m$; and "neutral" otherwise. Prove that the asymptotic density of friendly numbers equals that of hostile numbers. (I suspect that both are $\frac12$, and that neutral numbers become rare as $n \to \infty$.) Although the original intent has been to treat factors of the form $p^j$ as a single prime factor, feel free to treat those as $j$ prime factors if it makes the problem easier. 

The statement is false if $P^*$ is taken to mean the element by element complex conjugate of $P(\lambda)$. A counterexample: let $m=1$, $\omega_0 = \omega_1 = 1$, and $$A_0 = \left( \begin{array}{cc} 0&i\\2i&0 \end{array} \right) \\ A_1 = \left( \begin{array}{cc} 1&i\\0&2 \end{array} \right) $$ Then coefficients in $p(x,y)$ come out to be complex non-real. You must have meant $P^\dagger(\lambda)P(\lambda)$, the Hermitian conjugate of the matrix. With that change: $t$ is a real polynomial in the variable $\lambda = \sqrt{x^2+y^2}$. Proposition 1: $\forall n \in \Bbb{N} : \lambda^n $ is either a polynomial in $x^2$ and $y^2$ or (if $n$ is odd) $\sqrt{x^2+y^2}$ times a polynomial in $x^2$ and $y^2$. Since all the $\omega_m$ are real, we have by proposition 1 that $t = Q(|\lambda| = Q(\sqrt{x^2+y^2}$ is of the form $$ t = P_1(x,y) + \sqrt{x^2+y^2}P_2(x,y) $$ where $P_i(x,y)$ are both real polynomials. Now for a given set of $A_m$, each element of $P(\lambda)$ is a (possibly complex) polynomial in $(x,y)$. But each element of $P^\dagger(\lambda)P(\lambda)$ is a real-valued, thus it is a real polynomial in $(x,y)$. Then each element of $tI - P^\dagger(\lambda)P(\lambda)$ is a real polynomial in $(x,y)$ plus, for diagonal elements, an expression of the form $P_1(x,y) + \sqrt{x^2+y^2}P_2(x,y)$. So each element of $tI - P^\dagger(\lambda)P(\lambda)$ is of the form $P_1(x,y) + \sqrt{x^2+y^2}P_2(x,y)$ Finally, the determinant of a matrix is a polnomial function of all of its elements. This brings us home, because any polynomial function of elements of the form $P_1(x,y) + \sqrt{x^2+y^2}P_2(x,y)$ is itself of the form $P_3(x,y) + \sqrt{x^2+y^2}P_4(x,y)$. Identify in your problem $q(x,y)$ with $P_3$ and p(x,y) with $P_4$. By the way, if only even powers appear in $Q$, then $P_2(x,y) = 0$ since every term ins itself a polynomial in $x^2+y^2)$. Since the off-diagonal elements are also pure polynomials in $x$ and $y$, in that situation, $p(x,y) = 0$. 

Then if the constraint concerns the $L^\infty$ norm, that is, $\forall i \in \{x, y, z\} \leq c$, in each individual direction you have a probelm of a starting velocity, a target endpoint, and a maximum acceleration. You easily solve (for each coordinate) for the time taken to get to that target using the maximal acceleration: this is a matter of simply solving a quadratic equation, using maximum acceleration in the "right" direction. Of those three arrival times, take the longest; that direction will indeed use maximal acceleration. Now for the other two coordinates, fix the arrival time at that longest time, and solve for the necessary acceleration to get home at just that instant; it will in each case be less than the maximum allowed. So the $L^\infty$ case does not have worries about continuity. If the constraint is n the $L^2$ sense (that is, $a_x^2+a_y^2+a_z^2 \leq c^2$), then the problem becomes more difficult. First, you can always make a homogeneous linear transformation of variables such that you are trying to travel from $(x_0,0,0)$ to the origin, and indeed there is still a rotation about the $X$ axis degree of freedom left, so that the vector $v_0$ can be considered to lie in the $XY$ plane. These transformations leave the $L^2$ norm constraint unchanged, and turn this into a two-dimensional problem. I believe the optimal solution to this two-dimensional problem will use a constant acceleration, although this is not so easy to prove. The angle of acceleration is dictated by the requirement that both $x$ and $y$ arrive at zero at the same time; there is in general a unique solution. A bit of algebra arrives at the tangent of the correct angle; again the maximum acceleration magnitude is used, and again there is no continuity worry. Small perturbations about this solution improve one arrival time, at the cost of making the other later, so this is an extremum. Note that your problem did not say you have to end up stopped at the origin. If it did, you would then run up against the "bang-bang" principle. 

Your conjecture that the $a_i$ are independent is incorrect. Consider, for example, $N=2$, $x_1$ and $x_2$ and $y$ each independent uniform randoms on $(1,0)\times(0,1)$. Then, using upper indices as vector indices $$ \begin{array}{cc} x_i^1 = \pmatrix{1 \\ 0} \cdot x_i && x_i^2 = \pmatrix{0 \\ 1} \cdot x_i \\ y^1 = \pmatrix{1 \\ 0} \cdot y &&y^2 = \pmatrix{0 \\ 1} \cdot y \end{array} $$ we can express the $a_i$ as $$ a_1 = \frac{x_2^2y^1-x_1^2y^2}{x_2^2x_1^1-x_1^2x_2^1} $$ and a similar expression for $a_2$. Now write out the distribution of $a_1$, and the distribution of $a_1$ givven that $a_2$ is some fixed value. You will find those distributions are not the same (in particular, the distribution for $a_1$ is broader when $a_2$ is large), so $a_1$ and $a_2$ are not iid.