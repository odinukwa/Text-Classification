If I constrain the plans to disable seqscans (set enable_seqscan=False), it will index range scan on the first key, "campaign_id" but not "phone_number", which really doesn't help any. Constrained plan: 

I'm at a loss as to why it won't use an index for the "phone_number" lookup. Edit: It's definitely the "IN" that is causing problems. I tried changing the "phone_number" condition to a straight "=" and it will use the 3rd index then (and maybe others), seeks both fields, no need to touch the heap. 34ms execute time from a remote hostÂ instead of 500ms+. That doesn't really work in my case as the program may need to look up multiple values through the ORM. I would think that the IN clause is equivalent to "=" for a single match, but that does not seem to be the case. Edit 2: more weirdness I tried the suggestion of putting "phone_number" first in the index, and when I asked PG to EXPLAIN the query, now it is using the plan I want (one step index range scan with both args, no sort, no heap access), but with the "dialer_callrequest_campaign_phone2" index, which is the one I had yesterday! At this point it seems to be a statistic problem. I thought "ANALYZE " was supposed to fix that? Is there a way I can ensure it won't revert back to the bad plan in the future? Current EXPLAIN: 

I'm having some headaches with a procedure. Long story short, I'm using now a hint to force an index ( now, the query runs in 0.1 secs, after this, it was running in 2min ) but the question is, why is it using Parallelism, if it is set to 0? 

Maybe this question is a little off topic, I will close if necessary, but, is there a way to configure Management studio, to when I open it, it starts already with x connections? So I don't need to start it and connect to x instances every time. 

i'm trying to update a remote table using linked server, based on id of my local table. I'm trying this: 

In that way you do not "pollute" your table with input by users. It has the same "likes" as your own (you will just need an extra or similar) but not the dislikes. As the dislikes, it is another extra table, that has to be taken into account in your requests. You could even "merge" the 2 tables with just: if you specify in a constraint that either or are NOT NULL (but not both at the same time). You will then obviously need to accept having a lot of NULL values. But again, I recommend you to think about how you will query this data afterwards to find the model that most suits your needs. Also how often the "other" case could happen? It is more like 1%? 10%? 90%? The design and optimisations would depend on this too... 

EDIT1: Each "Image" has a binary code, and it has 43679 characters ). 1 millions rows for this table ( 1.141.947 images ( with coduser, image, and etc ) 

I'm thinking in something like execute sql, But Im having problems with files with spaces in name ( jhon vacation.mp3 ) for example. Any Ideas? Edit2: Query done, now I need to fix the Spaces in files name. 

You have maintenance plans. Try to follow these steps: Find the maintenance plan name and id that you want to delete. Write down the id of the one you want to delete. 

You can see I got a lot of ( Blank ) values. What would be the best way to update this ( lets say I have more than 100 columns ) to ? I made a script on excel, with for all columns. But I'm curious to know if there's a better way to do this. 

So Amazon added some settings. You will need to contact them directly and ask since their documentation does not provide a result when doing a search, $URL$ gives Your search for "shared_heap_size" returned no results. 

step 3, Form sort keys (for each level, take each value inside each collation array, then put 0000 as delimitator and start again for next level) 

step 4 : Compare sort keys: The second value is enough to sort them all, and it is in fact already in increasing order, so the final order is indeed: 

and of course replace by the timestamp your are dealing with. This could be abstracted in the function that creates this based on the interval value and step you provide, for your specific timestamp. Not sure to have understood the exact truncation you want, so you may need to replace by . 

Well...We solved the problem. There was an Update, inside the procedure running that "select * from...". I commented the update. no more problems. 

is High Availability something to describe Log Shipping and Always On ( replica )? is Always on something to describe Log Shipping and High Availability? I'm studying to get my certification 40-762 but I need to study a lot about these topics ( since i'm bad in this part on online tests ). But even looking at Microsoft's website I'm confused about these topics. I know something about log shipping and replica, But I would like a point to study, and looking for "always on" and "high availability" not knowing what they are is a bit hard. So, how can I describe them? 

There will always be exactly one "campaign_id", and 1-3 or so "phone_number"s to match in the IN clause. The problem seems to be in making the IN clause (which is often only one value) match an index field. The "campaign_id" field has low cardinality in this table, but combined with "phone_number" it is high. 

I have an ORM generated query (Django) running against a local Postgres 9.1.11 instance that looks up ~5 records from two specific match fields. Due to the nature of the EDI load program, I need to execute this query many thousands of times. The planner insists on scanning the entire table for each query and the whole process is taking an unreasonably long time (1+ seconds per query). There has to be an index I can create to allow Postgres to seek to the few records I want upon each execution. Query, generated by the ORM: 

so should be called only if directory is not already populated, so in your case the database just got initialized in a new directory, empty, while your data is still in another directory. But things have changed recently. Homebrew 1.5, released on January 19th 2018 has this in its changelog: 

It is a wrapper around PostgreSQL tool, that can be used to upgrade a database in place, hence not loosing anything, including between major versions. This article ($URL$ can give you a lot of information on how to upgrade PostgreSQL, and what happens when you use . 

This question is not so related to databases but more on Unicode handling and rules. Based on $URL$ Latin1_General_100_CS_AS means: "Collation uses the Latin1 General dictionary sorting rules and maps to code page 1252" with the added CS = Case Sensitive and AS = Accent Sensitive. The mapping between Windows code page 1252 and Unicode ($URL$ show the same values for all characters we are dealing with (except e with macron that does not exist in Microsoft mapping, so no idea what it does with this case), so we can concentrate on Unicode tools and terminology for now. First, let us know precisely what we are dealing with, for all your strings: 

One thing that I noticed was the wait type Is there a way to verify if the disk has a problem, but via SQL SERVER? Oh, and when I pause the script, I got this message: 

When I righ click the job + view history, I can only see 2 days of logs. I'm sure this is a really noob question but I can't find the answer. 

We're trying to achieve better reports using native tools from SQL Server 2014. I'm a DBA, and I don't know so much about c#, c++ and etc. I know also, that we have Analysis services, SQL Server Data tools, but, long story short, programmers here where I work don't care about anything. They don't want to learn. Is there a way to help my company, being a DBA ? I love to learn but I have some doubts about what service I can use to achieve reports with graphics, and those reports that bosses love. If this is a bad question I can delete it with no problem. My past job, we had an awesome application that uses views and procedures to ( I helped them with views and procedures and query tunning, but not with Visual Basic code ), in real time, update graphics and etc on a web page ( it was awesome ) but we can make it here, because as i said, they don't know and don't have the motivation to learn. 

Does it seem like the performace problem is CPU, disk or network bound? i.e. is the disk thrashing or the CPU maxed during a single expensive slow query, or it is it slow for remote queries even if they're small? How are the virtual disks allocated? Have you tried defragmenting them? How much memory is dedicated to the VMs? Are the guest Win2ks having a low memory problem? Is there enough RAM for the SQL server to cache the entire database, and would it be practical to allocate it enough to allow it to do so? Disk access can be very slow from inside a VM. Would it be practical to move both SQL server installs to the same VM? This would reduce resource contention between them. 

It depends if you need something generic or if it can work with a "small" number of columns. In your specific example, this query: 

But the algorithm by itself is a little dense. The gist of it is: Briefly stated, the Unicode Collation Algorithm takes an input Unicode string and a Collation Element Table, containing mapping data for characters. It produces a sort key, which is an array of unsigned 16-bit integers. Two or more sort keys so produced can then be binary-compared to give the correct comparison between the strings for which they were generated. You can view the specific Latin sorting rules here: $URL$ or more directly and specifically for MS SQL: $URL$ For the character it shows: