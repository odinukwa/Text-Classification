Here is an old instructions for setting up master-master replication for windows but concept is same. Considering you already have a 3306 instance by default you can follow below steps to setup as well (if above doesn't help) 

Before doing any changes and attempts, take complete backup of your databases. If you have physical backup of your database: 

are you sure you're doing what you want? In first query you specified localhost while in second you specified %... so for % you will have to connect to server remotely or using ip. Check below: 

So in anycase statement/row, you don't need to be worrying about the data on slave and it should match the master. but still if you want and can fit-the-logic on an event, this might be a possibility on slave at the risk of inconsistency!! As you said this is dw slave, would you consider a separate process to do the task you're looking to do, say a procedure? 

No, you dont have to do it again. The differential backups are just that so the only effect your mistake made was to make the restore time longer than neccessary. If your last transaction log backup was restored successfully then you are ok. 

You can setup a test database and trace the application activity using the SQL Server profiler. By tracing the event "Security Audit\Audit Schema Object Access Event" you can effectively trace all the SQL the client is sending to the server. But that will not necessarily help you in discover the application logic. 

Operating system error 5 is a simple you can always check those errors in the command prompt . Now Volume mount points are sometimes a bother. In Windows server 2000 the calcs.exe had an error and could not set the permissions on the underlying drive and the only way to do it was to mount the volume using a drive letter, set permissions on the root folder and then create a mountpoint. Later versions of windows should be able set the permissions on the mountpoint by using the /M switch so that 

My experience is that it is always going to do an online rebuild in the filegroup on which the index lives. It has to map the existing index and hold enough space for, essentially, one copy. You should only be getting the error when an index which is too large to hold mappings (the copy) is rebuilt - for instance, one time it may be fragmented enough to qualify in Ola's script and the next time it may not be. There is a great article $URL$ which I had to read several times when running into disk space issues with indexes. 

i wouldn't use maintenance plans - i don't trust them. ;) However, it looks like you could possibly be getting either an empty string or just a partial result with: select @backupSetId = position from msdb..backupset where database_name=N'''+@DB_name+''' and backup_set_id But, yeah - I'd use a print statement and see what is actually being generated. 

I used snapmanager with SQL 2005, it worked really well. I would take a snapshot, swing it over, mount it on our reporting server, and voila. The only problem I had is that most of what I wanted to do required undocumented NetApp commands. Once I got them, I just put them into a SQL job and our reporting environment was refreshed daily, and used less space on NetApp. 

Now, as you talk of replace in all tables, that's something not possible in single query. Though you can create a routine to do so! For the sample you can refer a procedure which is finding a string in all tables of all databases. 

Oprn. is "slow" because you do ensure the data durability & consistency. Oprn. is "Fast" because you did less work (disk operations). Even for the "lies" mysqld goes an extra step. 

(Consider backing up binary-logs if you want point in time restores.) For backups you can use traditional mysqldump or mydumper/loader. If your data size is large, it'd be better to go /w physical-backups, follow settingup xtrabackup for mysql with Holland framework. 

Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions in an operating system crash or a power outage. Well I just copied all of the above from documentation ;) Anyways, now about your points. 

run MMC.exe select File - Add remove snapin and find the SQL Server configuration manager and add it to the console root. Save the MMC console for later use with the extension.msc. If the SQL Server Configuration manager does not show up in the MMC console you have to register the DLL's and recompile the .mof for it Open a command prompt with run as admin, type the following command, and then press ENTER: 

sp_depends will give you list of dependent objects in all current versions of SQL server but is listed as deprecated. Here is a MSDN article with a way to find objects referencing a function and with small modifications you can get the list of object referenced by a function: 

I have to say yes. By blocking dev/test you are making sure that application changes are not accidentally updating production data. In many cases sensitive data on dev/test is scrubbed after restore and by blocking you are making certain that no one with access to test can access sensitive data in production. If you are using SQL Server with MSDN licensens you have to segment the Development servers from the production servers anyways. ($URL$ 

You mention that the stored procedures should be done. My guess is that without any transaction marshaling the data is accumulated and then finally committed about 2 hours later. You mention you ran profiler, but I would look at Adam Machanic's sp_whoisactive at $URL$ It will give you all sorts of information. 

This is old, but I had the same issue. My SQL logs and backup tables were indicating that backups were happening, but even when I used the vssadmin command above, I could not locate the backup files. I listed the writers, but none matched the long string in the SQL logs, so I tried some of the other "vssadmin list" commands: ---- Commands Supported ---- List Providers - List registered volume shadow copy providers List Shadows - List existing volume shadow copies List ShadowStorage - List volume shadow copy storage associations List Volumes - List volumes eligible for shadow copies List Writers - List subscribed volume shadow copy writers "List Providers" turned out to be the golden nugget. Although nothing was returned for existing copies or storage associations, the Providers listed the info I needed - it named my SAN provider. I looked in my storage tool and saw it was using Microsoft VSS, and the snapshot times correlated with what was in my SQL log. Three things of note: 

This will mean that you can restore to the point in time before the upgrade (last full backup and log backups until the last one) and if you restore the last full backup and the differential made after the database was put back into the full recovery model and any log backups made after that. And finally the best solution: Schedule your log backups to run every 10-15 minutes during the upgrade and dont change the recovery model to simple. 

If MSDB is on c: master is on c: as well and I'm almost willing to bet that tempdb is still in the default location as the average I/O stall on c is 19 seconds! First, check if tempdb is on c:\ or has any file on the c:\ partition and if so move it away Check if the master database has some free space - Usually you dont need to tune it as very little is written to the master database, unless if someone has created objects in it Then make space in the MSDB database is way to big which can cause a lot of troubles, please remove as much backup and job history from it database as you can. You can do all these checks very simply by downloading and running sp_blitz 

As you already mentioned documentation that you cannot use table level filtering on replication. You might like to try another approach instead. Replicate whole database and change all tables except those you need to use to engine. 

While WAMP comes only with MySQL-Server (no client tools etc), not 100% sure but did you see if the wamp-setup is offline installer only? 

Ofcourse a physical files restore will be quicker than loading a dump. So, if you have full physical backup / snapshot of mysql data-directory - you can go fast! (Below answer assumes that you have physical backup available.) Tables are MyISAM: 

When you cannot identify a primary key for a table you need to use surrogate key; auto_increment columns are most common surrogate keys which database internally provides and hence you should use them then. You may alternatively have sequences or programmatically handle uniqueness... 

If you need to loop over the output of one query to another you can use cursors. For eg, in this stored routine to find in all tables databases we have looped over: 

In both cases you end up with a CRT that you can import to the machine store of the SQL Server and then use the SQL server configuration manager to encrypt the connections. You will have to trust the root certificate in the latter case though and in the former case, if you are not running on a domain you will have to install the root certificate for the windows CSA on both the machines 

You will have to call /pages/folder.aspx with an ItemPath parameter as absolute path /Test which url encoded will look like this. 

Use the SQL Server configuration manager to change the SQL Server Agent user account. Change it to run as local system, apply and restart and then back to the correct user, dont add any privileges to the user in the operating system (esp. not add it to the server local administrator group). The Configuration manager will set all the correct permissions for the account. 

and then truncate the log as above to get out of trouble but remember which databases were in full recovery model. Now read the article linked above and start making good backups. 

I could be wrong, (especially since I don't know German) but looking at the options on the second screen (Media Options), did you try doing "back up to a new media set", instead of an existing one? In 2014 there are additional options. Since it is a 2012 database you are trying to back up, there are probably defaults set in those options which are incompatible. 

ouch. reaching back into my memory - I remember that sometimes I had to put a return at the end of the format file. (SQL 7, maybe even 6.5?) So, maybe hit enter at the end of the format file, or optionally, make sure you didn't hit enter 

We did this, just to be safe we stopped replicating that database, changed compatibility level, then reinitialized. We incurred no issues. It was a pretty small publication and there was only one database subscribed. I think it could possibly be more difficult as your replication scenarios get more complex. (i.e., subscribing db getting publications from many dbs, etc.) 

then create a secondary index for the { VALUE | PATH | PROPERTY } you are querying with Xpath $URL$ and $URL$ 

Is an operating system error. but not an error from the SQL Server. You are probably starving the computer on memory or having some network issues, If on Windows Server 2003 a quick reboot of the secondary server will fix this temporary and then I would look into memory settings on the server 

Install SQL Server on the new nodes Use SP_HELP_REVLOGIN to copy the user account to the secondary node Take a full backup and a single log backup of the primary server and restore on the secondary with no_recovery Break the mirror and remove the secondaries from the primary server Create the endpoints with TSQL or use the Mirroring wizard to configure them and endpoint security start mirroring 

Now if you only have access to the unc path but not the linked server then the @backupfile becomes like this:.