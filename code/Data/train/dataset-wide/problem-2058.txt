You don't have the required permission to access the database mail settings. Ask your DBA to configure it for you. If you don't have a DBA then you will need to logon to the server with an account that has sysadmin server role. 

Backup production Restore backup in staging Drop the unwanted tables Backup staging Restore backup in integration 

I just want to know how they came to 8kb when striking a balance between: smaller page = more page splits bigger page = longer to look through to find value I expect there are many more pros and cons to having a bigger page but if we could start again would we pick 8kb again? I'm not saying it's the wrong decision and I know it would be a massive change to the product to alter the size. Also I know 8kb was chosen a fair few years ago now and I wonder does it still make sense when rows are likely to be longer now due to new data types? 

Maybe he wants to load the table with data and then create the index after. That way index fragmentation and statistics would be in good shape. With that said a good choice of clustering key would keep fragmentation low during the data load. Kimberly Tripp has written a few posts on topic of choosing appropriate clustering keys. 

I have 4 servers: A, B, C and D. A and B are in a master-master setup. C is a slave to B and D is a slave to C. Changes on A or B get to all nodes other than D. I need them to get to D Changes on C get to D but obviously they don't go to A or B which is fine. After some reading I found that I needed the log-slave-updates option set on C, this way C would log the changes replicated to it and then D could read those and make the same updates. I have set the option to on and D is waiting for it's master to send events but changes from A or B are still not getting to D. 

You could use an unstructured column type like XML or JSON. But MySQL doesn't support these types AFAIK. You could store these types as a string in MySQL but I wouldn't recommend that. The reason for that is if you need to change one value in the XML/JSON you would need to change the whole string or somehow parse it with with RegEx. Nasty. So to use an unstructered type you would need to move to a different RDBMS. MariaDB has some JSON support. Switching from MySQL to MariaDB should be an easy migration, see this guide from DigitalOcean. If you don't want to change RDBMS then I think the EAV model mentioned by Phil is the best choice although these tend to degrade in performance as the data grows. Filtered indexes can help but they too AFAIK are not available in MySQL. 

Essentially you have NULL values in salesCategory column. Having NULL values in fact tables is generally bad because of the problem your are facing. The best thing to do is correct your data so that these values are present as this will provide the most accurate results. If you can't remove/correct the NULL values then you are limited to filtering the rows with NULLs but this will skew your aggregates or using the 'UnknownMember' to deal with NULLs. This will mark rows with a NULL Salescategory values as 'Unknown'. This way your aggregates are correct and all the data is visible. If you are using SSAS then you can set 'UnknownMember' in the your dimension -> properties -> KeyColumns -> NullProcessing. 

I would rephrase your question to what is likely to make my company shell out all the extra cash for Enterprise edition? Well it depends on the version of SQL server but below is a list of features and reasons that businesses upgrade for. 

Using SSMS the error log can be found in the management folder. I would check the log for more information and run a full DBCC CHECKDB to ensure the database is OK. Paul Randall is an expert in data corruption in SQL Server and he mentions a fix to your problem in this article. I would read this through to get a good understanding of the situation you are in. In brief he says that the situation may be solved by switching the database to SIMPLE recovery mode, back to FULL recovery mode and then performing a FULL backup. The full backup restarts the log chain. I would also inspect the hard drives that the transaction log file is running on as they may have caused the corruption if they are faulty. 

The problem with this is statements can no longer refer to variables declared in separate batches. I think the real answer is to use stored procedures that are called by your app. This reduces the amount of data sent across the bad link and it ensures the SQL is valid. 

Only way it could break that I can see is if you have two movies with the same title and different mID's. 

I have a production deployment using local SSDs for tempDB. I have 2 SSDs in a RAID1 configuration. I am seeing average reads of 1-2ms but the average writes are showing as 1377ms on all four of my tempdb data files. Each tempdb data file is 2GB with a 1GB growth setting (They haven't grown since deployment 5 months ago) The tempdb log is showing average read 67ms and average write 215ms. The SSDs are Samsung 840 pros. The following code is what I use to get my stats 

Will having enabled decrease the amount of memory used by an in memory OLTP table? If yes I assume the CPU workload will increase. 

At a guess, this sounds like the application is processing the data row by row. The fact that the slow processing is intermittent I would say that it's very unlikely to be the network. 

Your design looks OK to me. The following query will give the trips from a particular users wish list: 

You could have done this by stacking pivot statements but this can be messy and inefficient. This is a different way of pivoting data called the cross tab method. This method is usually more efficient than the pivot statement and easier to read. 

It can help query performance by employing partition elimination. This means large sections of big tables can be ignored when looking for values which means much less IO. Index alignment needs to be looked into when partitioning. See details here You can break your backups by partition. This can be useful if you are struggling to complete your backups in time. See here for details Index rebuilds can be done at the partition level instead of the whole table. Large inserts can be done with partition switching. Queries can also run parallel across partitions. 

The cluster will pickup all the different subnets you have configured on your NICs. Once a server with NICs that have IP addresses of the 2nd subnet is included in the cluster you will be asked to add an IP for each subnet for each role as you add them to the cluster. 

I would set it offline. I say this because mode is generally used to allow an administrator to perform some kind on maintenance on a database without users attempting to access and possibly interfering with the maintenance. It doesn't really matter either way for a restore as your users will experience the same thing which is they wont be able to connect. One possible annoyance with the approach would be if a user stole the single session before you and blocked you from performing your restore. 

You may find that a clustered columnstore index on your table works well but I would really want to avoid having that column in a columnstore index. One of the problems this can cause is dictionary pressure. As you are looking to partition the table and alter data via partition switching I would suggest building a non-clustered columnstore index on your table that does not include the column that contains URLs. This will make the table read-only but that doesn't stop you loading data in and out via partition switching. The way I have done this in the past is to bulk load the data into a staging table, do any post load processing there and then switch the whole table into an empty partition in the main table. It can be fiddly but very effective. The other option is to use SQL Server 2016 which allows updateable non clustered columnstore indexes. 

If you can upgrade your system to SQL 2014 then have a look here to read about natively compiled stored procedures. This will protect your code and possibly improve performance. It's probably a bit late but business logic should be done at the application layer when possible. This keeps all the logic in one place and reduces the need for CPU resources at the database level which can be very expensive when using Microsoft SQL Server. 

I have used option 2 from this article many times to fix this issue. Basically insert the windows install disk, enable the .net 3 feature but select the option to 'specify an alternate path' at the end of the wizard. The different path should be D:\sources\sxs (if the disc is in your D drive) 

This is a common issue, you want to use groups to keep things simple but you don't want to lose visibility of who has access to your databases. The way I see it you have two options: 

I'm not sure if what you are building is a good idea or not but the code below should get it working. You basically need to include the statement to separate the script into individual batches. Using a temporary table to store the original settings values allows access to the values by each of the batches. You can't include the GO statement in a stored procedure so if you are looking to automate this process you will need to separate the script out another way (Agent job steps, SSIS, PowerShell, etc)