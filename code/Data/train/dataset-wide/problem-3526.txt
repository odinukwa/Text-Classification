If you are trying to setup a reverse cache proxy with load balancing...please please read this before you further configure squid. If I misunderstood your needs then just ignore this post. $URL$ 

Khaled's answer is probably your best option. You can also use expect for this, good for instances where you are not allowed to install keys for one reason or another. 

Look at the man pages for check_http and check_ssh. Those two checks are usually ran directly from the nagios server and NOT used in conjunction with NRPE (exceptions exist). The warning and critical thresholds are related to response time. You can also modify what http response codes are acceptable. $URL$ $URL$ Please rephrase your second question as it does not currently make sense. Hope this helps 

Sorry it's kind of obvious, it's late give me a break. dig XX.XX.XX.in-addr.arpa. will reply with the SOA if one has been assigned. 

It's a best practice to have the owner be whatever limited user account is used for uploading/managing the files on the server. The group is often the account that php is running under, so in this case apache would be correct. The other permissions should be set to nothing, as they are. You are close to perfect. If you have a situation where multiple accounts may be modifying/editing the files you can creat a cron script that chowns the dir recursively every hour or so to maintain correct ownership. The same technique works to keep the permissions correct as well. Also, you may want to modify the umask of the limited user account that has ownership to be inline with your permission scheme. 

cloudburst attack is more or less a fancy term for "Host code execution vulnerability from a guest operating system." How do you protect against that? The same way you protect against every products security exploits, with updates/patches. What does this means if you are using some virtual hosting provider (SaaS, PaaS, etc)? Pick one whose core virtualization technology is well tested. Xen would be a great candidate since it's open source and therefore receives external code reviews. If you are implementing virtualization in house use the most recent stable version and implement any additional host protections possible, such as chroots, selinux, jails, kernel patches like grsec, etc. 

I would like to configure snmpd to send traps for disk usage. Currently the file includes these lines: 

Where is this LAN that the web server resides on? If it is at your home then you can configure your personal soho router to forward all requests for incoming http(80) to a specific IP address on the local LAN. This is achieved via the administrative web interface of the router in most instances accessible via the gateway IP of the device. You will want to make sure apache is configured to listen on the computers LAN IP and not just the loopback interface: 

I usually place it in /etc/httpd/ if it's used by multiple vhosts, if only used by one domain then one dir above the webroot would do as well. You can place it in the webroot as well just make sure there's a rule denying access to files with it's naming convention(usually in place by default): 

Just of different way of thinking about it. I don't think you have anything to worry about, nothing is going to bite as long as you leave the directory structure untouched. Obviously other packages are going to drop their own .conf files into the conf.d dir so it would be a bad idea to rename or move it. The modules-enabled/available is eliminated but don't count on the include line being in the conf.d file for all packages. Some still add it to the httpd.conf file via the RPM post install routine. 

This will allow each client page request to use one apache process to handle all it's requests where it would otherwise use multiple apache processes. This will cut down on the amount of apache processes running at any given time. For optimal memory and requests per sec: Move away from mod_php and use fastcgi, or another app server, instead. Apache processes consume a negligible amount of memory when php pages are served by fastcgi. Not to mention fastcgi can keep persisten connections to your dbase server amongst other things. 

Attempt to deliver a message via telnet to the server. If it accepts the email and you still don't see it then the queues may be misconfigured. If gmail can't reach the server you will usually receive a undeliverable notice in that gmail inbox. Telnet SMTP walk through: $URL$ Check the event viewer log files for any service errors as well. 

Unfortunately the service name chosen by the program is hard coded. You will most likely have to modify the sshd source and re-compile. The reason they do this instead of just passing ARGV[0] as the service name is for security reasons. If the pam.d/file was chosen based off ARGV[0] (the program name) then at attacker could possibly symlink/hardlink/cp that program to a name of her choosing. One that had the least restrictions within it's associated pam.d/file. Search the source for a string such as: 

Have you presented this to the dev mailing list? Very interesting for sure. Have you tried enabling full debugging; you might have had to enable it during compile. Once enabled do one request to the canonical and one to another. Perhaps the debug logs will shed some light. Have you tried removing the second server_name domain(somesite-live..)? Good luck, let us know if you figure it out! 

Not always the best solution but remote desktop allows you to use your local cd/dvd drives (and virtual cd drives) as if they were local to the remote server. Check your remote desktop preferences to enable this. 

There may be functionality differences between the libraries so I would make every effort to install the additional older libraries first(apt-file search libboost1.35). If that fails you can attempt to symlink the existing libboost libs to the older names and rebuild your ldcache and re-run ./configure. 

So I need to install the original /etc/pam.d/system-auth-ac file from the authconfig RPM. I attempted to do so by "reinstalling" the authconfig package/rpm like so: 

clear your cache in both and make sure it's still working in one and not the other. Attempt to load the css url in the browser directly i.e. $URL$ View the source and verify the src addresses are correct. Check the sites error logs for any access denied or "not found" lines. 

What browser? Are you attempting to use a proxy? A little more detail would be helpful. Also, unless you are configuring a proxy this post shouldn't be on serverfault.com. 

From the check_http man page: -k, --header=STRING Any other tags to be sent in http header. Use multiple times for additional headers $URL$ 

So all the tutorials and documentation for the Linux quota system has left me confused. For each filesystem with quotas enabled/on where is the actual quota information stored? Is it filesystem metadata or is it in a file? Say user foo creates a new file on /home. How does the kernel determine whether user foo is below their hard limit? Does the kernel have to tally up quota information on that filesystem each time or is it in the superblock or somewhere else? As far as I understand, the kernel consults the aquota.user file for the actual rules, but where is the current quota usage data stored? Can this be viewed with any tools outside repquota and the like? TIA!! Update: Thanks for the help. I had already read that mini-HOWTO. I am pretty clear on the usage of the user space tools. What I was unclear on is whether the usage data was ALSO in the file that stored per-user limits and you answered this with a yes. From what I can tell, rc.sysinit runs quotacheck and quotaon on startup. The quotacheck program analyzes the filesystem, updates the aquota.* files. It then makes use of quota.h and the quotactl() syscall to inform the kernel of quota info. From this point forward the kernel hashes that information and increments/decrements quota stats as changes occur. Upon shutdown, the init.d/halt script runs the quotaoff command RIGHT before the filesystems are unmounted. The quotaoff command does not appear to update the aquota.* files with the information the kernel has in memory. I say this because the {a,c,m}times for the aquota.user file are only updated upon a reboot of the system or by manual running the quotacheck command. It appears - as far as I can tell - that the kernel just drops it's up-to-date usage data on the floor at shutdown. This information is never used to update the aquota.* files. They are updated during startup by quotacheck(rc.sysinit). Seems silly to me since that updated info had already been collected by the kernel. So...in conclusion I am still not entirely clear on the methods. ;) 

Try: ServerTokens ProductOnly Probably goes without..but you will need to reload for changes to take effect. 

Does anyone know how to setup the CentOS repositories to work under RHEL 5.6? Please be specific if possible. 

I know some people are using keepalived and heartbeat for active/standby but what action is taken if the haproxy process were to die? What would be nice is if the virtual IP would switch servers if the haproxy process were to die and/or a networking issue were to occur. We are currently investigating heartbeat and corosync with pacemaker. Can anyone explain their solution to this issue in depth? UPDATE: Thanks Kyle, see answer and links therein. 

So I figured this out and think it may come in handy for others. What I had to do was remove the <> characters from the "illegal" list within nagios.cfg. This is to be done on the server performing the active checks. 

You can limit output to results matching the optional argument. Also, limit your range as your example range is 4194304 IPs long. ;) Have people looking for ya with ranges like that. 

For a quick change do: "MaxRequestsPerChild 4096" to something like: 700 will help. The longer an apache process lives the more resident memory it's going to consume due to mod_php and the like. Also, enable keepalive and place aggresive timeout settings for it: 

If you truly want to learn about web administration you will stay away from CPanel and other web based administration tools. However, if you aren't familiar with web terminology or the core fundamentals then temporarily using CPanel or Plesk or the like may help you become comfortable with DNS, SSL, Virtual Hosts, etc. You should look at installing linux in vmware server or parallels on your local workstation. Learn how to install and configure an http server, etc. Since it's only for the sake of learning there's really no need to pay just yet. If you feel that you must have a server out on the tubes then check out VPS hosting from companies like Linode and Slicehost. They will allow you to reinstall or restore the OS from backups if you screw anything up (I use Linode & they're awesome). Try to become familiar with the core of the OS before you get too deep into "web administration". Knowing the fundamental tools (cat,sed,awk,cut,xargs,find,vi,locate,which,man,STDIN,STDOUT,etc) will likely make any troubleshooting much easier. 

Glad you solved it! Another method is to have the external server be part of the VPN. Then just set the domain/s to the servers new VPN IP instead of the public IP. This way all administrative access to the external server travels over the VPN as well. Plus no special routes or NAT rules. 

So everyone who uses CentOS knows the default repositories have their limits. I've been using rpmforge for years as a secondary repo. I usually use yum priorities to make sure rpmforge never installs something the base repo can take care of. Recently I came across a need to install EPEL as a tertiary repo. Is EPEL kept more up to date than rpmforge or in any way better? Should I be using that instead of rpmforge? 

Within most browsers you can enable a "status bar" at the bottom. This is an option under the "view" menu at the top. Once done it will tell you the current status of a page you are requesting. If it is in fact being delayed by a DNS lookup then you will see something like "Looking up: domain.com" for an extended period. You can also perform some lookups via the command prompt to get an idea of how fast they are being performed. start -> run -> cmd -> nslookup domain.com Mac/*nix: terminal -> dig domain.com 

Nginx's resident memory usage is trivial and to the best of my knowledge there is little penalty for having 8 Nginx processes (1 per core) rather than 1. I would run apache bench or another utiltiy with high concurrency requesting disk heavy objects. Keep increasing the AB tests till you discover the breaking point. Then experiment with the amount of Nginx procs keeping all other tests equal. Report back so we can all be the wiser. 

Listening services should be explicitly enabled not implicitly as is the case with core services on most servers. If you don't need ports 137,138, etc then disable the services so they aren't listening. Google "disable netbios over tcp". You can disable netbios services on a per Network Adapter basis. So disable them on the WAN for sure. First google result: $URL$ Also, as was stated previously you should only listen publicly on your webserver and only on those ports you are actually using. Use the windows host-based firewall to drop everything except tcp requests to http(80), https(443), DNS(53)(UDP). Also, is this internal network solely your own? If it is shared with other "dedicated" server customers or other departments you may wish to limit access by source IP on the internal interface to any netbios, core windows services. 

The default configuration on most distros is going to be pretty secure. It's up to you to make it otherwise. ;) So before implementing custom htaccess rules, enabling symlink support, adding new modules, etc be sure to ask yourself how that change relates to security. Research that particular directive in the context of security via google and apache.org. The default configuration of Apache, though secure, may have modules and features you do not require. So you may wish to disable unnecessary modules, cgi support, ssi support, directory browsing, etc. There are a slew of articles available search for "hardening linux" and start with ones that don't include mod_security or re-compiling. I recommend testing each change as you make it so as to know which ones break your site. Also, often more important that securing Apache itself is securing the content Apache serves. Read about proper site permissions and the configuration files related to the languages used (php.ini). Sorry it's hard to be more specific without a more specific question. I obviously don't want to duplicate Google search results. 

I've tried Zabbix but haven't tried BixData. I would recommend trying Zenoss as well, better than zabbix if you ask me. 

Test to make sure the locations block is being called by placing an additional directive in there..something like: 

How can I monitor the write speed to a tmpfs partition. It does not appear in the output of vmstat or iostat. I am running several python processes that are writing heavily to tmpfs and contributing to load. The load is high but the CPU, memory, disk IO, etc is all nominal. The load seems to be taking this heavy tmpfs IO into account indirectly somehow. I'd love to know the write speed to have an idea of upper limits per host. I'm running blind any help would be appreciated. 

I make use of an .ssh/config file to set my username appropriately based on a given portion of a subdomain, e.g. 

Make sure the mail server is listening on more than just the loopback interface. It most likely isn't by default, for security reasons. 

Performance, that is why you should use a good VPS provider over something like a mac mini. Disk throughput is going to be utterly terrible on a mini. So any and all server requests that hit the platters are going to suffer. Too many concurrent requests will quickly result in disk bound limitations. Not even mentioning RAM with ECC, faster processors, dual power supplies, etc. If you want to own the hardware you can purchase a used server and find a colocation facility near you. However, this will likely cost more than $55 a month. Stick with VPS, who are you using? I use Linode and have a 512MB instance at $20 a month. There may be cheaper offerings than your current host. 

I never really pay block size much attention but obviously there can be benefits to choosing something other than the default. I am looking for a good "best practices" paper on choosing block size. Also, when used on top of LVM is it's performance payoffs or importance negated in any way? TIA 

Run the above on the server and it should list which services are running and which ports they are "bound" to. If 80 is not listed then you need to troubleshoot the apache startup.