You already have the truth-table corresponding to the conjunction connective, so it should be transparent for you to recognize the answer to your question from the syntactic point of view of the calculus. Asserting ¬(P.Q) [Premise 1] is logically equivalent to assigning a binary truth-value of zero to (P.Q). So, looking at the table, you see there are three possible assignments to the truth-values of P and Q that yield that result. Then, by asserting ¬(Q) [Premise 2] and using the same argument, you assign a truth-value of zero to Q; looking again at the truth table, this restricts the possibilities to two cases (namely, one in which P's truth-value is zero, and another in which it is one). So, the assertions do not imply a definite binary value for P's truth state. From the semantical point of view, the interpretation of the conjunction connective in this case follows (informally) this way. You know that either P is false while Q is true, or Q is false while P is true, or P and Q are both false; and you also know that Q is false. This last statements eliminates the first possibility (Q being true while P was false), and you are left with the other two; so, what you seem to think to be justified (the second possibility, that Q being false then P must be true) excludes the third situation when both P and Q are false, and so is fallacious. 

Information, understood in an intuitive sense of "generalized abstract stuff", seems to be more fundamental than computation given that the concept of procedure implies a reference to the former. Information should be understood as mere strings of symbols, and symbols are to be understood as generalized stuff (anything which can be distinguished); What the canonical term "information" refers to, is a computational property of information in the sense of 1. I personally prefer to use the term complexity in both cases of the Shannon and Kolmogorov definitions, and preserve the term information to mere "stuff" (without reference to computation or probability priors; i.e. strings "as given"). 

Assuming that information is the fundamental stuff required for the concept of computation to make sense, one may wonder what is the minimal alphabet (i.e. the minimal amount of different symbols) which is needed to express any form of information in the sense I explained above. The obvious response might be that the binary alphabet is the answer to this question -after all, the bit is used as a universal measure of channel capacity. But it turns out, I haven't found in any textbook I've read a section dedicated to stating and formally proving this, let alone to elegantly and succinctly doing it! Again, it may sound obvius, but if the foundations of science are information and computation (a trend which has been going on for almost a century by now) I seriously think it should be established that the binary digit is the "most simple and fundamental possible unit of (generalized) stuff". I will now state my question properly: 

Not knowing what GS is, as Conrad points out, I cannot yet make a definitive answer, but it is possible that Feyerabend is drawing a distinction between principles-and-rules which can go wrong and principles-and-rules which have been rewritten in a form which allows doubt. "What goes up must go down" fails remarkably if someone holds the object at its apex "What goes up must go down unless someone opposes it" fails remarkably once you exceed escape velocity. "'What goes up must go down' has a remarkable track record of being useful" can no longer "go wrong" because the fuzzy words "remarkable" and "useful" temper it, without losing sight of the value of the original rule. It appears Feyerabend is arguing that we should ignore rules given to us until they are in such a form, because rules which are not in that from have a tendency to squelch contrary ideas (which may be true, but you'd never know it if you squelched them) 

The situation you find yourself in is one which classical logic starts to have trouble, making this a surprisingly good question despite the downvotes. In classical logic, there is a law of the excluded middle: "A thing either is or is not." This makes logic very powerful at slicing away at problems until one finds a solution. However, in some situations, the law of the excluded middle is a poor model of reality. As you have noticed, "veganism is good" () and "veganism is not good" () are both not compelling to you. The law of the excluded middle is simply not an ideal tool in this scenario as phrased. In these situations, one must be cautious when applying classical logic because it can lead one astray, as you have noticed: it is suggesting to you that you must make a decision. Do you need to be at a decision point right now? Can you reframe the problem such that the problem is no longer unsolvable? In particular, can you reframe it such that you can defer answering the question until new information comes to light. Donald Rumsfeld is famous for his phrase about the knowns, the known unknowns, and the unknown unknowns (can I call Rumsfeld a philosopher?). The question of "is veganism good" is a known unknown. You know its there, but you don't know the answer. This part has lead you to the stance that you need to be agnostic to veganism. However, when we start talking about not just believing but acting, the unknown unknowns creep in. What if the price of beef goes up (acting vegan looks better)? What if science shows actual issues with GMO soy (making meatitarianism look better)? What if you start dating a vegan? What if your vegan girlfriend suddenly starts eating meat? If you are not fully committed to veganism or meatitarianism, it seems very reasonable that any of these unknown unknowns that come up should influence your actions, while being committed to one side or the other would suggest these unknown unknowns should not influence your actions. A solution to your conundrum is to seek to be open to new inputs and new arguments, no matter how small they might be. You clearly have not decided veganism or meatitarianism, as per your question, so you should act in a way which opens you up to be flexible to future unknowns unknowns influencing you. This path would make it reasonable to eat some meat, because you need to keep your gut flora ready-and-able to tackle meats if you go meatitarian, but it also suggests spending days as a vegan. Do this not to be a vegan, but to better understand what life as a vegan would be like if you chose that path. Explore vegan recpies. Some of them may taste good to you, and then you should eat them regardless of your final decision on veganism! Be flexible enough so that, when you finally arrive at a choice about veganism, you are fully prepared for whichever path you take. Many people I know have explored such a middle road. One destination it may arrive at is flexitarian. Flexitarians typically eat vegetarian, but they wont let it cramp their lifestyle. If they are at a restaurant with friends and cannot find vegan fare (an unknown unknown), they will eat a meat dish so that they get food, and they typically will not make a big fuss about it so as to not disrupt the meal. 

The text doesn't go into further detail at this point. So i'm confused about how a human being can be either free or responsible in a deterministic universe. In what sense are we free? What are we responsible for? How can our responsibility manifest itself? Many Thanks for your thoughts 

Trying to understand Buddhist concepts by analogy is an interesting proposition but one doomed to failure in my experience. On the face of it you might be onto something - is emptiness (sunyata) the same as switching a computer off. A translation of nirvana (a related concept) is 'blowing out' which seems to point at the same thing. However the Dhammapada, a good candidate for the original words of the Buddha, doesn't talk about emptiness at all. One of the analogies the Dhammapada uses for the enlightenment experience uses is 'the deathless' which seems very different to switching something off. Though it is tempting to understand these concepts by really mentally drilling into them, they really are provisional. Traditionally they are seen as fingers pointing at the moon. One shouldn't obesses about the finger at the expense of understanding the moon. We are invited to test these ideas in the crucible of our own experience (e.g. by Buddhist practice) rather than just by reasoning. Hope that helps some. 

While reading history of philosophy books, Cicero's name often comes up. However I remain unclear of what if any are his original contributions to the philosophy of that period. I know that he was an important transmitter of philosophical though from the Greek to the Roman world and so onto the modern period. However Sextus Empiricus and Diogenes Laertius were also important transmitters of philosophical knowledge but they get much more of a passing reference in the history books. I know that Cicero was an important political figure, an adherent of the skeptic school (to a greater or lesser extent) and I believe that his prose is meant to be magnificent (though I'm no classicist). But what did he add to philosophy itself? Many thanks for your thoughts 

as exhibting homonymy. Anthony Kenny in A New History of Philosophy says that this is the classic example of homonymy. So the assertion is that in each phrase 'medical 'is not the same. But I am confused. My understanding was that the adjective 'medical' is the same in each case. The meaning being 'an object (noun) that relates to medical practice'. How is it that Aristole says that medical is a different word (or has a different meaning) in each case? Can anyone clear up my confusion please. 

Given that the domain of discourse for physics is... well... physical aspects of the universe, their usage of the word "everything" is reasonable. Of course, it doesn't qualify as "everything" in other discussions. Even within physics, the ToE explicitly does not include the initial state of systems, merely the eternal rules governing it. But within the community it is well understood. To hold physics to a stricter standard than that would be tricky. Consider, for example, the Tao, which could be loosely translated as the Chinese concept of "everything." The famous quote about the Tao is: 

Personally, I have great trouble with claiming there is a complexity line-in-the-sand for consciousness too, by the definitions I prefer to use. However, when discussing what "we" as a populace call "consciousness," it does seem reasonable that there is a line that can describe that, for much of what the populace calls "consciousness" permits lines to be drawn. 

For one answer, consider that you wrote all of that without providing a definition of "chunk." How can I algorithmically process your question to generate an answer without a clear definition of "chunk?" For a second answer, consider thoughts of love. People say "When you're in love, you'll know it." Any attempt to think about love from that mindset cannot be analytical. For a third, and more analytical answer, some topics defy the segmentation you suggest can always be done. These things exhibit gestalt behavior: the whole is different from the sum of its parts. When the majority of the value of a thing is actually not the parts, but found in the integration of the many parts into a whole, such analytical thinking becomes harder, or even impossible. As an example of that, consider your section on art: 

Almost all axioms in mathematics have been validated as intuitive concepts before efforts are undertaken to define them logically. For instance, the Greeks axiomatically believed there were no irrational numbers partly because the lack thereof allowed them to believe numbers and geometry were inseparable, and that was intuitive at the time. They held their intuitive belief until someone formally showed that their approach lead to paradox ( Hippasus, presumably). The axioms for addition and multiplication were based on intuitive needs for day to day life, and only axiomatized later when we wished to use them to solve difficult problems such as those involving infinity. Even the modern axioms of set theory, which are used in modern mathematics to "prove" nearly everything, were initially founded in the intuitive idea of "a collection of things," and only had to be rigorously defined as a result of that intuitive idea allowing paradoxes into the mathematical constructions in ways that were deemed unacceptable. In modern day schools, we even teach the intuitive approach in many cases. Consider calculus. Derivatives are typically taught intuitively, rather than relying on their formal definition using epsilon-delta proofs. And the theory behind Common Core, the method of teaching mathematics which is currently taking over America, the goal is to develop a more intuitive understanding of the math first before teaching the abstract versions of that math later. 

So i'm confused (again!). What has common sense to do with logical reasoning? My previous understanding is that logical was an objective discipline. So it seems odd that the subjective and vague 'common sense' is in the logical mix. Certainly with the boolean logic of computer programming that I am more familiar with common sense has no role. Well I think it doesn't but then clearly logic isn't what I thought it was. So what role does common sense play in logic? Does it really have an element of subjectivity? Many thanks for any/all thoughts 

I read the following in Anthony Kenny's 'A New History of Philosophy' book. It is a paraphrase from Chrysippus the Stoic philosopher 

I was listening to a political podcast and David Poltz (one of the presenters) said of an argument that he was unconcerned whether the argument was hypocritical. I found that attitude jarring, counter-intuitive but very interesting. The contention is that if the argument is good then it is good independent of the actions of the person proposing it. To take a simplistic example I could argue that it would be better for the environment if no-one had a car and we all used public transport. If it later turns out that I own 4 cars that I happily drive around in them all day long this doesn't invalidate my argument. It still might well be the best thing if no-one drove their own cars even if I am ignoring my own advice. From (my) every day point of view I would say that hypocrisy does affect an argument but obvious this opinion isn't universally held. What about from a more rigorous (philosophical) point of view. Does hypocrisy invalidate an argument? Are there some philosophical traditions that say it would and some that say it wouldn't perhaps. Many Thanks for your thoughts 

The earliest example that I have heard being advanced is that of Pyrrho the ancient Greek skeptic. He travelled with Alexander the Great and would have had access to Eastern Philosophy though I don't know of any specific evidence linking him with Buddhist though. However his form of noncognitivism skeptism does seem to relate to Buddhist thought on this matter. The Buddha refused to engage with questions such as the eternity of the universe which seems to me to be a similar attitude. The book The Shape of Ancient Thought gives a fuller discussion of this area. I've heard good things about it but I haven't (yet??) read it myself Not strictly related but there is firm evidence of Buddhism in contact with the Greek world in the canonical Buddhist text The Questions of King Milinda - a debate between a Buddhist monk and an Eastern Greek king. To be fair it would be surprising if they didn't come into contact as the Greek empire next door to the India.