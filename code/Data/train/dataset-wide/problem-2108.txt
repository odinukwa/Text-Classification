This wait is associated with a sort operator that runs out of memory and has to spill to tempdb. It is not immediately obvious why rebuilding an index would involve sorting, perhaps there are some details missing from the question. 

You will need to use a domain account as your SQL Service account. Ask your system administrator to supply you with a domain user for your SQL Server services to use. Make sure that account has sufficient rights on both servers & folders, in particular make sure that the agent accounts on both servers have access (read/write) to the folder(s) you are using for the log shipping. I tend to use one folder, and use a shared folder. This is for me in most cases the most reliable method. 

To recover the data, instead of vacuuming, make a copy of the database as it is now, look for a hex editor, and see what you can salvage by hand. See also Undelete accidentally deleted records in Sqlite3 on Stack Overflow. 

There may be additional considerations in the AWS RDS world. Note that is deprecated in favour of . The documentation says: 

Community wiki answer: Use a domain account with access rights on the share, then this should work. will not be able to reach it. If using a domain account is not possible for some reason, you can try granting the machine account permission to the share. The machine account follows the following format: . The is needed to denote it's a computer and not a user. 

If you happen to know how large the table is (page size specifically), that would be roughly the number of reads. If you only know the size in MB (or GB), you'd have to divide by 8192 bytes (the size of a data page) to get a rough estimate of total number of reads. Rebuilding an index might do at least 2x this number depending on which rebuild options you choose. 

The usual way to solve such a problem, is to use a bitfield. So you would create a tags table, and link that via an n:m table to the sales figures or products. Then in the tags table, for each tag you would assign a unique bitvalue as a power of 2, e.g. for and so on. Using you can then condense these values into a single numeric value and store this alongside the product or sales figures. For example tags "sports" and "cricket" on one product become 5. If the bitsize of the numeric type you have available is not enough to store all your tags, use multiple of these fields and store the number or column name of the field and the bitvalue with the tag. Then for querying use clauses of the form: or = 10th flag set You can now do any Boolean expression on the flags. If you designate a single field for all colors you can also do other tricks, like querying for products, which have a Color tag: and so on. As you're in a column oriented database (redshift), is only executed once per unique value in the column. Depending on implementation, the database will further reduce this, by analysing the -clause and use constraints on size through sort order of column values (for free). And if you need that last bit of performance, you can do tricks by collecting statistics on the flags and the queries and grouping them together intelligently. I expect in the use case that you're describing (perform sum ... group by after filtering), performance that you could gain through this, would be negligible compared to the cost of calculation. 

The "object"-reference in object-relational refers to object-oriented programming. This is a programming style where objects like a triangle or a square or some other geographical entity can be told to move itself (coordinates translated) or to rotate a certain degree or to scale some percentage, regardless, which class it is (triangle, square, whatever). As programmer, you don't know which class a particular object is and the programming language takes care of carrying out the correct calculations when you tell this object to move (change 3, 4 or more coordinates). An object-relational database now combines features of object-oriented programming and relational databases and takes care of converting between objects with methods (move, rotate and scale) and tables, which usually lack these methods. 

Unchecking seemed to fix this immediate issue, but caused a new one. Now the 'me' user cannot expand any server nodes in SSMS. It looks like ultimately some of our logins were orphaned because we created the databases from backups from a different server. Community wiki answer based on comments left by the question author 

Community wiki answer: Works as intended: You created a constraint that ensures the uniqueness of and together. Then you inserted a second row with the same , but different value. 

You are not creating a linked server just because you are using The official Microsoft documentation shows you how to create a linked server. If you are having difficulty with this, please edit your question with a specific problem. See also: How to Add a Linked Server 

A backup never breaks the log chain. A non- backup just resets the differential base (the reference point for a backup). So, no, the would not prevent you successfully performing the following restore sequence: 

Community wiki answer: Parameters and local variables are different beasts. The plan with parameters is generated (if not already cached) using row count estimates gleaned from the actual values supplied and statistic histograms. The plan with local variables is generated based on unknown values so the average overall density () is used to estimate row counts. The plans may differ due to different row count estimates. Related: Understanding Performance Mysteries by Erland Sommarskog 

Community wiki answer: Your condition only eliminates about 50% of the rows in the table. In that case the Seq Scan will be faster than the Index Scan. See for example this Stack Overflow answer, where user a_horse_with_no_name says: 

If you use a RAID partition, you will get fault tolerance in case of drive failure. If you instead stripe your database across N drives you are N times more likely to suffer a fault causing you downtime. If you are paying for Enterprise Edition, why on earth are you trying to save money on drives? Spend a few thousand dollars and build a decent RAID. Or use software RAID. Every OS has it built-in. It'd certainly be more real-world tested than most hardware RAID software. 

Install the PosgresQL ODBC driver and then use that to register Postgres tables in Access. Issuing the SQL you have given in Access will then transfer data from Access to PosgresQL. 

Probably it would pay to have a date dimension table listing dates and any date part that might be necessary (like quarter, halves and months). This way you could get rid of doing the date math over and over again and just join. Tricks which this also enables are things like comparing to last years quarter (i.e. add a last_year_quarter column and similar). 

Create an index on . This way oracle will most likely use the index only to answer your slow running query. You can add more columns to that index, so that you also get the needed content. Hopefully you don't need all columns. Be careful about, what impact this index has on your load performance. 

Although you're using an ORM, which hides the nitty gritty details of SQL for your developers, you seem to be interested in different approaches to insert a record and a multitude of detail records in a child table. I would opt for the code, which conveys the intend most clearly and which can be maintained more easily. In my opinion, this will likely be the second option. Only if (measured) performance is an issue, then I would start optimizing. 

I suggest you have a look at the section on 'rating scales' in the complete guide to writing questionnaires: 

Use a function to convert from some ascii compatible representation to the respective encoding of your database, which hopefully is Unicode. There are several possibilities, e.g. HTML entities or URL encoding schemes. Check, how your database supports Unicode code-points natively, e.g. as references of the form or or . 

Community wiki answer: These will show up in the ring buffer with extra information, see Connectivity troubleshooting in SQL Server 2008 with the Connectivity Ring Buffer by the Microsoft SQL Server Protocols team. 

Community wiki answer based on comments left by the question author: I did as suggested and inserted records from a csv file and then from a command prompt, the sales tax was 0 and the table at a rate of .0925 It turned out was defined as an , the tax data type was . I changed to: 

Community wiki answer: Did you really read chapter 23, where vertical/horizontal decomposition is described? I don't see any 'n/a' or 'd/k' anywhere there, even more in numeric columns. Figure 8 on page no 384 shows is what is "obtained" from the database, so a result set. Not what is being stored in the tables. As the previous pages describe, the suggestion is to decompose the table into smaller ones (fewer columns and/or rows) and each table stores the different info, i.e. one table for unknown, another for not applicable, etc.) The only place where 'n/a', 'd/k' are stored if you use a table with a column, and combine multiple missing-information tables into one. 

The MySQL documentation at 14.21.3Â Troubleshooting InnoDB Data Dictionary Operations has the official guidance. Aleksandr's alternative suggestion is: 

Community wiki answer: Yes, the query can still be active (and is in this case). However, while DDL autocommits, data inserts don't. So it may get to that step and then die if pgAdmin is not there anymore to commit it. 

Community wiki answer: No, this is not possible. I'd add this as a code quality item to their code reviews. That said, the root cause of the multiple plans is not the different call syntax. That could also happen with different options, etc. Instead look at query plans and the parameterization behavior, and the Query Store (if on 2016+) to see why you are getting different plans. See also: Slow in the Application, Fast in SSMS? by Erland Sommarskog.