The domain user the job executes under is assigned the db_owner role on msdb and the database used for the query attached to the message. Its default schema on both databases is dbo. It's also assigned the sysadmin role on the server and is a member of the DatabaseMailuserRole on msdb. It also has private and public access to the Database Mail profile used by the query. I've seen dozens of examples of the same issue online, but I've already taken the steps that remedied this problem in the examples I saw. What else can I try? 

The check is a "<=" (I assume you overlooked the -infinity instead of +infinity) - so the value you put in is the upper bound in that case. ("Scale in as soon as CPUUtilization is between your value and -infinity") Just to mention that here: you could also use the "target tracking policy" - with this you can say that auto-scaling should scale in a way that your target value is matched - that includes scale-out AND scale-in out-of-the-box. Hope this helps! 

Yeah that should work (and is the way you should do it in the cloud). Just copy the whole infrastructure and then shift over the traffic to the new one and control the metrics of both infrastructures. 

I have a samba server where I create all the linux accounts and corresponding SMB accounts via a application I built. I manage all the permissions for all of our shares via ACL's also using the application i built. This works great as is, but I don't want to manage all the user passwords anymore as we have a couple hundred users. In our environment there is a Active Directory setup that I can use to authenticate users, and I would prefer to use that for authentication instead of local passwords. I dont have any type of access to this AD other than authentication. I can't write data or anything to it, I don't manage the system. Now I was able to get my samba server on the Domain and it authenticates, but 2 issues: 1) I have to login using Domain\username and thus it creates brand new user account, so I have User1 and Domain\User1 accounts. Not what I want. I only want that single User1 local account that is authenticated over AD instead of local passwords. 2) Every person in that domain can login to the samba server, also not what i want. There are only select people that are determined through various conditions that can access the server. I only want the users that I have created local accounts for to be able to login to my samba server. All the local accounts are setup so that the usernames match the corresponding AD user names, so that should not be an issue. Any ideas on what I need to do? 

It is not possible (even for the AWS support afaik) to use Elastic IPs which are not for VPCs with VPC instances. So you're stuck here - the only possible way to do things like this is not to rely on a fixed IP address (you will get the same problem if you try to use ELBs or more than one instance). Your customers should NOT point to an IP address but they should use CNAME records with the given subdomain you provide for them. With that architecture you're able to migrate the whole domain with all subdomains to a new IP address if you need to and with the CNAME records nothing changes on the customer side (as the subdomain they're pointing to has the new IP address). The only solution for you now would be to send out an email to all customers which use the IP in their DNS records to change it to a CNAME and after migrating all customers to CNAME you can switch to the new Elastic IP and change your own DNS records. UPDATE: As pointed out below it is now possible to move an Elastic IP from "classic" to "VPC" - you will find the details here: $URL$ 

On our campus, we have about 60 Macs joined to our Active Directory domain. Most users have no problems logging into Macs, as long as their accounts are configured correctly. However, we have one particular user who is unable to log in to just some of the Macs. He has no problem with most of them, but there is one group of them (all built from the same image) that he can't log in to. The machine in question is running OS X 10.6.2. The relevant entries from secure.log are below, with the hostname and username redacted. 

I assume we're talking about dynamic IP addresses so you can't limit their IPs inside of the security group as they're always changing. The only way I can think of right now is blocking every external IP address in the security group of the Redshift cluster and setting up a "SSH jump host" inside of the same VPC. If you now enable this host to connect to the Redshift cluster users can access the Redshift via a SSH tunnel. As you can roll out different SSH private keys to each machine you can limit and allow the people independent from each to other to access the Redshift cluster. Several SQL tools are available for DB Connection through SSH tunnels so maybe that is something your users already use and you're able to limit the access to certain users. 

I have an application that I developed that controls access to the printer for specific users. I accomplish this using CUPS and the lpadmin commands 

But if I remove that user from the Allow and Prevent list completely, essentially saying that are allowed to print then everything works great. So the error message only appears if the user is on the CUPS controlled Allowed list. So I can think of 2 options: 1) Figure out why using CUPS for access control causes this issue with this specific printer. 2) Figure out how (using the command line) to take a user Off of the Deny list without using the command. This should make it that user is no longer on any CUPS access control list at all, which has proven to work. Option #2 seems like the easiest option. Help please? 

There is a read preference value "nearest" which actually uses the latency between the MongoS and MongoD to determine which is the best/fastest set member for the query. You can find the documentation about it here: $URL$ The selection is based on the "member selection". For a sharded cluster like yours you can find the way how the MongoS selects the node here: $URL$ The most important thing to notice here is, that "nearest" doesn't care about the type of a node. So even if you do a read query it is possible that the MongoS selects a primary (which sometimes isn't the way you would expect it). To fix this the only way is to use the tags you already mentioned. Hope that helps! 

This command seems to work great for all but this EPSON Stylus Pro 9800 printer. Even if the user is allowed to print, this error message appears in the queue 

We currently have a cisco pix 501 running in our office building and we just got word that our network is going to updated to run jumbo frames. I haven't been able to find much information, but from what I gather our PIX does not support jumbo frames. So assuming that is the case, what should i expect from this? Is it just a matter of the PIX not being able to take advantage of what jumbo frames brings and business as usual? Will the system basically become a paper weight? We are planning on getting rid of PIX soon, so I guess its just a matter of deciding if we need to get those plans running sooner than later. 

Is there a reason why you don't use EBS snapshots? You can use those to save the whole EBS device (incremental) with a simple API call and the snapshots are saved within S3. If you need an old version back just create a volume from this snapshot and connect it to your instance instead of the broken EBS. 

The short answer is: You can create an A or CNAME record to the external IP address or the DNS name of your instance, but you don't want to do this. The long answer: You actually can create a CNAME (or even A) record without an Elastic IP. But every time your EC2 instance is restarted and so moved to another host system the IP address (and your external hostname) of your instance will change. If you can live with this and accept the fact that during the TTL of your DNS record your instance is not reachable you can use the external IP and create an A record with your subdomain. But as DNS is not very fast in distributing changes (even with a low TTL you can't make sure every resolver handles the TTL correctly) you don't want to do such things most of the times. This is why AWS provides the Elastic IP - so your IP address which is in the DNS record never changes but the routing behind this IP address is changed by AWS if you reassign it to another instance (or you reboot your host). This routing change is only inside the AWS data centers and so it is quite fast (within a few seconds) and your instance is reachable again for all users. Hope this helps!