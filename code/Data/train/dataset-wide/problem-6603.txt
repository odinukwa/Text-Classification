I agree that a rhotic dialect would be preferable, and also along the same lines of preserving distinctions used in other dialects, use a dialect where "cot" and "caught" are pronounced distinctly (I used to suggest central Ohio, but the distinction is dying there). However, there are a number of casual-register phonetic reductions that are now fairly standard in the US, which tend to make American English difficult for non-speakers to understand. In actual casual speech, vowels get seriously reduced or deleted, consonants and entire syllables disappear or are radically changed (e.g. k→x in flapping contexts; coda reduction of /t,k/ in the direction of glottal stop). These are mostly register-governed rules, so if you aim for a slower, less-conversational register, that should bring these features under control. 

The main idea of generative semantics is that meaning is present from the start (deep structure), and rules specify how words are inserted and arranged, given that. For a single article, I suggest George Lakoff "On Generative Semantics", in Steinberg & Jacobovits eds. Semantics: An Interdisciplinary Reader. Interpretive semantics, by contrast, specifies the structure of a sentence with no reference to meaning, and towards the end of the derivation, the meaning is computed based on what the syntax has done. See Chomsky "Deep structure, surface structure, and semantic interpretation", same volume. 

Secondary articulation refers to a "lesser" constriction, compared to the constriction of primary place of articulation. So in [tʷ, sʷ], the primary place of articulation is alveolar, and there is a secondary (vocalic-ish) labial articulation. Typically, secondary articulations are glide-like. "Double articulation" refers to segments which have two places of constriction which as "equal" in terms of constriction, for example [kp]. The phonological analysis of double articulations is controversial (the majority of evidence indicates that one of the places is indeed primary, but languages can differ in their analysis so that [kp] may behave like a "very labialized velar", or a "very velarized labial". Also note that these terms are quite variable in their use, so that [kʷ] may be called a "double articulation". 

Voicing is defined as the semi-periodic vibration of the vocal folds. Accordingly, you look to see if there is something that happens repeatedly at a reasonably low but not ridiculously low frequency, e.g. for a given speaker it might be in the range of 80-200 Hz, but higher or a wider range for another. The technique of autocorrelation can be used to detect such repeated patterns (and underlies computation of pitch). The idea of autocorrelation is to look at the shape of a piece of waveform, and find the most similar part that follows it (within an appropriate window). If there is enough of a semi-regular pattern, you have voicing. If you have creaky voice, this method may well not detect the glottal pulses. If you are not looking at fricatives, you can look for visible voicing pulses in the spectrogram. You can also look at a spectrogram, looking for the "voice bar", which is high amplitude at a low frequency. With voiceless fricatives, you can get high amplitude from the turbulent airflow, but the frequency is up there above 1KHz: high amplitude in the realm of 200 Hz does not come from a noisy (aperiodic) source. The problem with voice bar as a detector is that with stops especially, the amplitude of the glottal pulses can be low enough that it's hard to detect. Also, if you are looking at spectral slices, you have to make sure that the low-frequency amplitude is enough higher than the amplitude at higher frequencies (this is an issue in case you intend some automated voicing detection and don't have ideal recordings). Ultimately the various acoustic methods have reasonable but not total success, and the only way to determine if the vocal folds are vibrating is physiologically. 

Athabaskan languages would be the "most prefixing", in (a) being almost or in fact exclusively prefixing and (b) allowing many prefixes (11 positions). Papers on Navaho include this, as well as J. Kari Navajo Verb Prefix Phonology and Young & Morgan The Navajo Language. One can check information from the related language Sekani, and it seems that the language Slavey (K. Rice, A Grammar of Slave) has over a dozen prefix positions. Another contender is related Tlingit, which however has more suffixes. 

Pitch is the perceptual correlate of fundamental frequency. The fundamental frequency of a semi-periodic waveform is the frequency with which the vocal folds vibrate, e.g. 150 times per second. The rate of vibration of the vocal folds is not determined by the degree of constriction in the vocal tract, until the constriction becomes so significant that air-flow is blocked and the vocal folds cease vibrating entirely. 

Assuming that you want data that would be suitable for problem sets, you could start with Gleason's Workbook in descriptive linguistics, and Ronald Langacker's textbook Fundamentals of linguistic analysis, still available at the original price. There is a textbook Laboratory manual for morphology and syntax in various editions, published by SIL and usually (currently) with William Merrifield as author -- there is or was a version available online from them though I can't tell you where (it may also be in Spanish). Most phonology textbooks have problem sets, though they are usually too difficult for absolute first level classes, or they require the student to have memorised particular ideas of likelihood in order to get the right answer. Another approach is to look through journal articles in journals that specialize in language areas, such as Studies in African linguistics or Oceanic linguistics. They often publish descriptive articles that lay out some simple phonological problem in Margyi, pronouns and transitivity in Lushootseed, or Kwara'e tense paradigms. Usually you can skim quickly to see if there are any paradigm-rich papers in an issue. Likewise, you can get problems from some grammars, though often grammar just leave you to figure out what the rules are on your own. 

Actually, they are, sort of, but not in English. The underlying reason is that formants are the result of filtering the glottal source wave to enhance resonant frequencies. There is no physical mechanism that adds amplitude to just F1. The general pattern of the glottal source wave is that most of the energy is at the fundamental, and amplitude decreases according to which harmonic you have. To the extent that you can control the slope of the glottal source (make it flatter; make it fall more steeply), you can control the amplitude of an individual formant (though not to the point of making F3 much higher in amplitude compared to F1). A number of languages (e.g. Dinka, Hmong, Taa) employ a contrastive breathy phonation which has a rapidly falling amplitude in the glottal source wave. Thus, if you can control the source you can weakly control the amplitude of a formant. If you can speak Dinka, you can control the source (or you could just learn how, somewhere). For amusement, you can synthesize complex waves from individual harmonic and unnaturally modify formant amplitudes (if you can synthezise a complex wave from harmonics), and check the perceptual effect. 

The standard explanation, insofar as it seems mysterious given the different articulators of the sounds, is that uvulars and lingual rhotics have in common a lowering of F3 – they sound similar. This page gives a list of claimed cases of "guttural r" (also this). It is reasonably likely that it arrived by boat in southern Norway and Sweden, and Denmark is the most likely immediate source, but how it got to Denmark is non-obvious. Independent development is not out of the question, especially given a phonetic motivation (acoustic similarity). 

Pharyngealization is encountered in Semitic; some Interior Salishan languages; some Caucasian like Ubykh, Tsakhur and Udi; !Xóõ; Chilcotin; Berber languages; Even. In some cases, it is a feature of vowels (Udi, !Xóõ). Sometimes it is involves consonants, but in a way that points to effect-on-vowel as an important feature of pharyngealization. There is a connection between some Central Semitic pharyngealized (emphatic, uvularized) consonants and ejectives in South Semitic (Ethiopic and South Arabian, where the outcome is mixed and variable in South Arabian languages). There are various positions as to whether ejectives became pharyngealized or the converse: my limited grasp of the facts favors the "originally pharyngealized" hypothesis given the existence of pharyngealized ḍ in Arabic corresponding to ʕ in some forms of Aramaic, plus the fact that there's no such thing as "ejective d". I do think that it would be great to get a detailed Semitic-specific answer. In Arabic and Berber it is typically analyzed as a feature of certain consonants, though it also phonetically affects adjacent vowels (in Arabic, the influence on vowels is large enough that it can be the only cue indicating a pharyngealized consonant). The consonants which are phonemically pharyngealized are mainly coronal, but there are pharyngealized dorsals reported for some dialects of Arabic in Yemen, and low-frequency emphatic labials are reported in Syrian Arabic. In Berber, the proto-language apparently had just "ḍ, ẓ" but other coronals have been picked up under Arabic influence; some analyses treat pharyngealization as a high-level prosody that starts at one point and continues to the end (analogous to the situation described by Hoberman in his Language article on Aramaic). Proto-Afro Asiatic reconstruction have been essayed by Dolgopolsky, but reconstruction of earlier phonetic values is fraught with difficulties. In the case of Interior Salishan, there were primary pharyngeal consonants in the proto-language, which seem to have given rise to a distinction between retracted and non-retracted (pharyngealized) vowels and consonants in Lilooet and some related languages, and the source of the retracted series seems to be a general vowel harmony which managed to give rise to a retracted lateral (the only consonant with the feature). Comparative work on proto-Salishan isn't thick on the ground, so the situation there is less than clear. In Chilcotin, it's an automatic feature of alveolars (the language, which is Athabaskan, is coronal-rich, contrasting dentals, alveolars and palatals). It is not clear that there is a phonetic difference in the consonants, and one could treat them as the same as dentals, which trigger "flattening" (a vowel harmony process, roughly equivalent to spread of retraction in neighboring Interior Salishan which is the presumed source of this process). The case of Even seems to be a phonetic development from an ATR system. In other words, there aren't any clear generalizations. 

Basically, no. You would first need a massively-multilingual dictionary which really covers all of the words in each language, and there isn't such a thing. You can always decide, at your peril, to ignore some languages. Then you would have to determine what would constitute "close enough", for example is "vitu" close enough to something in Finnish, or "pool" close enough to something in Norwegian. You can't just use pronunciation, so you would want to not use the Somali word for "food". Finally you would need the dictionary to be well-annotated for cultural information, which would indicate the additional nuances of hooyada (lit. "your mother") which are not found in aabaha ("your father"). 

Early 14th century. Thother is used in spoken English, but is spelled 'Thother and is only in conversational register, not formal writing. 

There is a distinction between head and dependent which is exploited, with C dominating head elements and adjoined V dominating dependent ones within the segment. See p. 10 of van de Weijer: replace "t" with "U" and you'd have a labialized labial. There is some variation in the theory as to representations e.g. whether velars are placeless. 

In transcribing the word as [fij], you are committed to saying that this is a closed syllable (unless you invoke dubious abstract structures where j is not syllabified, or is the onset of a syllable containing no vowel). The letter [j] is not a vowel, and under normal assumptions about syllabification, it would be part of the syllable defined by the preceding vowel i. That, by definition, makes the syllable closed. You have choices in transcriptions, which do not reflect some immutable phonetic absolute, they reflect phonological analysis. If you want to say that the syllable contains a diphthong, then you should transcribe the word as [fii] (which is different from disyllabic [fi.i]), and if you want to say that the syllable contains a monophthong and is open, then you should transcribe the word as [fi]. You should also look at actual practices for transcribing vowels of English, because the diversity of practices very clearly show how transcriptions are simply conventionalized and cannot be fully rationalized. The word "fee" can be transcribed many ways, for example [fi, fi:, fij, fɪj...] In general, i-ending diphthongs are written with final ɪ,i,j as well as raised versions of any of these letters. One is always free to write "car" as [kaɹ] and call it an open syllable by labeling the sequence a rhotic diphthong. In this case your teacher presumably gave you a transcription, and wants you to come up with an analysis. The question you might later pose to the teacher is, why that particular transcription? If the answer is "because this is an closed syllable", then the follow-up question would be "How do you know that it's an open syllable?". If s/he thinks it is an open syllable, the the follow-up questio would be "why [fij] and not [fii] or [fi]?"