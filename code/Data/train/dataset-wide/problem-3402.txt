The disadvantages of the first two options is that it opens your 802.1X scheme up to MiTM attacks. I could conceivably build my own RADIUS server and intercept your user's AD credentials. Not an ideal setup but your department will need to do the risk analysis. If you do go this route, make sure you document for CYA purposes. From a security standpoint the best option is setup a captive portal. Students can use their BYOD devices to connect and reach the portal, pass their user authentication credentials to the portal and the portal can then talk to the RADIUS server. Eduroam is another popular choice for educational organizations. 

If it is at all an option find another job. This will not end well. And honestly do you really want to work at a company that treats everyone as replaceable? Request an emergency budget so you can find a consulting company to keep things running and act as a resource while you get up to speed. If this is denied, make sure you have it in writing somewhere for CYA purposes. Read any documentation left behind by the previous IT staff. Perform an inventory of both physical hardware and devices. Figure out what is in warranty and what isn't. 

Some of our developers use Microsoft Expression Web to author their ASP and ASP.NET web applications. Our web servers are all built on Windows Server 2008 R2 and IIS 7.5 and come in a group of separate Development, Testing and Production servers. The goal is for the developers to check out their source code to their workstation, then publish to Development, then Testing and then finally to Production using Expression. We are connecting with the web servers by exporting the IIS drives as file shares that the developers can then map or use in Expression. This works for one group of servers but not for another. When we try to publish from the Development server to the Testing server we get the following error: 

You're thinking about this the wrong way. You shouldn't care if the server is down, you should care if the service that it is offering is down. The offered service is purely conceptual. It could be composed of one or many actual technologies like HTTP, HTTPS, SSH or whatever else you are leveraging. Don't monitor the server - monitor the service. If you're running a web server, does it really matter that you can ping it? Not really. Does it matter that your monitoring software's agent of choice can't connect to the web server running on it? Absolutely. In a way, your questions is kind of specious. For all intents and purposes if you're running a web server and you can't connect to it's web page then your server is down regardless of what other services or connectivity is available. 

Different administrators accomplish this in different ways. I'm primarily using Debian and I feel the "most correct", correct being defined as the most obvious, integrated and documented way to do this , is by adding directives to your file as you have done. If you do this make sure you don't cheat and just put all your or directives under one interface. Have each interface add the routes that appropriate to it. The other way I've commonly seen this done is with a custom init script very similar to one @mgorven has posted. 

Yes. 2.6.32-5 is the current version of the kernel in Debian Squeeze. You'll notice the linux-image-amd64 dummy package lists linux-image-2.6.32-5-amd64 as a dependency. If you need a more recent version of the kernel you can get it from backports where it is at version 3.2.0-0.bpo.4-amd64. 

We have two Windows Server 2008 SP2 (sadly not 2008 R2) Domain Controllers in a small 150 client domain that are exhibiting very "peaky" CPU usage. The Domain Controllers both exhibit the same behavior and are hosted on vSphere 5.5.0, 1331820. Every two or three seconds the CPU usage jumps up to 80-100% and then quickly drops, remains low for a second or two and then jumps up again. 

This doesn't address your specific question but it does address the larger underlying problem. Debian Etch has been at End-of-Life status since February 2010. That means no security updates. If this is a publically facing web server, you will be quite lucky if it hasn't already been successfully hacked. You should upgrade to the current version of Debian Stable (version 6.0). If you do not have the skills to do this yourself nor the time to acquire them, you should hire someone who can. Nothing personal, this is just how it is. My recommendation is to perform your "upgrade" from Apache to Apache2 by building a new server using Debian 6.0, installing Apache2 and then migrating your website from your old server to the new one. What you suggest will be painful and ultimately unproductive. 

Running the WQL against the ProductName property seems to be a good way to go. If I run in against the namespace I get the following: 

I am working on a Freeradius backed 802.1.x authentication infrastructure for our wireless clients. I am using a rather generic Freeradius configuration with EAP-PEAP. Our clients are predominantly Windows XP SP3 machines but a few Windows 7 32 and 64 bit laptops also exist. Our domain is at the Windows Server 2003 functional level. 802.1x authentication is working with manually configured test clients. I want to create a GPO that autoconfigures our clients by 1) deploying the self-signed CA certificate to them as a Trusted Root Certificate, and 2) sets up our ESSID as a preferred network with the appropriate 802.1x configuration. I am having no difficultly deploying the self-signed CA certificate to clients using a GPO. However, I cannot figure out how to configure the certificate as a Trusted Root Certificate in the GPO. This is from the GPO settings found under : 

As for what that performance penalty will look like in the real world it's really dependent on your application and implementation. It might increase the time by a factor four which only works out to an extra 4/10ths of a second, or it might be four seconds. Testing is important here. It looks like at this point NIST (SP800-131A) only considers RSA and DSA 2048 key-sizes to be acceptable after 2011-2014 and they must have at least 112 bits of security strength. See the section in SP800-131A on Digital Certificates for more information. In my opinion it all boils down to this: your safety margin is "small" with 512-bit keys, "not bad" with 1024-bit keys, and "pretty good" with 2048-bit keys. Do a bit of threat analysis and decide how fat your organization's safety margin needs to be, what kind of data you are protecting and what you stand to lose if the cryptosystem protecting it is broken. Make sure to consider existing policy, and any legal requirements you need to comply with. 

I don't think XenCenter has support for using flash drives as ISO repositories (or it didn't when I worked with XenServer 5.5). If you don't mind using the command line tool () for managing Xen you should be able to SSH to your XenServer host and proceed. Start by reading the XenServer 6.1.0 Administrator's Guide and XenServer 6.1.0 Virtual Machine User's Guide .