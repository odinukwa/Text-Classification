You can use to monitor packets on either end. The tool can be used to capture headers or full packets. The man page includes examples on how to filter packets so you only capture the traffic you are interested in. Your packet size seems a little strange. If you aren't using jumbo frames, your packets are likely to be fragmented for transmission on the network. This may be the cause of your issue. Spitting the frames into parts of 1472 bytes or less would be more appropriate for an ethernet connection. Internet connection may have an even smaller MTU and a parts of 1464 bytes or less may be be more appropriate. You may want to do MTU discovery on the connection to prevent fragmentation. This would allow you to avoid or limit packet fragmentation. 

The code that builds the images is in Perl. I believe you will find period shorter than 1 day will give disappointing results. Sample period is fixed at 5 minutes. Per hour would only have 12 samples. The package should be installed so you can try to generating the images using . Some of the rrd configuration comes from the plugins. Telnet to localhost 4949 to talk to the plugins. The node software has a help command. The rrd data files can be found under . The munin projects site has some help on generating graphs. EDIT: The information you need to generate a graph or data dump is available from the rrd data files. The structure and file naming under /var/lib/munin is pretty easy to figure out. Directory tree matches your domain structure. File names match plugins on hosts. Munin-node will give you the information it uses to format a graph for a particular plugin. The command will connect you to the munin node software on the local host. Once connected the commands most useful for your use are: 

After adding the VM's ip to resolv.conf, pinging foo.local.myapp.com properly resolved to the machine's IP. When I try to ping it from the host machine, however, it acts like the there's no local dns and happily skips to the resolving it via internet, returning the webhost IP address. I tried listing it as the name server on both the host OS and the network router. No dice. I'm very new to this, so I know I am probably doing something remarkably dumb. Could anyone offer a guess? 

I've set up dnsmasq on a virtal machine server in order to handle static wildcard subdomains on my local home network for development purposes. The idea is to have *.local.myapp.com resolve to the VM itself (bridged networking), running the http and dns server. I've installed dnsmasq on the VM. And edited dnsmasq.conf to resolve to the VM's network IP: 

(sorry about the images -- no SSH at the moment) When I ping the machine from the windows host, it looks like this: 

It looks like you are using a self-signed certificate. Client software will often not trust these keys. If you can arrange to have your CA certificate added to the client's trust chain then you should have not problem. Otherwise, the users will need to accept the certificate the first time it is used. Usually the accept dialog will default to permanently accepting the exception. Thunderbird works this way, but appears to require you to accept once for the IMAP/POP server and once for the SMTP server even if the same certificate is used. Eudora and Exchange should work the same way. From what I have seen most email servers do not verify the certificates provided. If they do you will need to configure an ACL to prevent offering StartTLS to those servers. EDIT: OpenSSL trusted certificates are kept in a directory ( on Ubuntu). The certificate is usually named according to the signing authority. There is also a symbolic link based on a hash of the key, that is used for lookup. You can add your own trusted certificates. 

I ran Rootkit Hunter 1.4.0 on a Debian Wheezy server and I am confused by the output. I enabled every tests using the following piece of configuration: 

I've just installed sudo on my machine (Debian Wheezy). While I was editing the sudoers file using visudo, I noticed this line: 

This warning is a general advice ("don't put your files in ") or if it just means that a standard Debian package installation should not install files under . If we are actually discouraged to place the production websites in , where should I put them? 

Ideally, which one (if one of them) should I use? Does the first one mean that any user in the sudo group would be able to run commands as any of the other users of the machine? Does the latest format still apply on Wheezy or is there a newer way to do that, like for example using colons? 

Of course, I will try to provide as much information as possible if needed, feel free to ask in case I forgot to bring specific helpful data. 

Due to a flood of spam mostly sent by spambots, most sites require senders to properly configure their servers. As well, many ISPs block outgoing email traffic from dynamic IP addresses; and may require users with static IPs to request the ability to send email. If you want to send email directly to the internet you will need to ask your ISP to configure the PTR record for your IP address appropriately. 

Yes you can log to a remote syslog daemon. The default program will truncate long messages. If you have memory your can use syslog-ng. Installation should be similar to my installation of Syslog-ng on openWrt. 

can be a list or wild card such as the same as any other list. Reverse configuration would be . Documentation for exim is at . Check in chapter 7. 

The order of reboots is important. Rebooting the server after the clients can result in this situation. The stale NFS handle indicates that the client has a file open, but the server no longer recognizes the file handle. In some cases, NFS will cleanup its data structures after a timeout. In other cases, you will need to clean the NFS data structures yourself and restart NFS afterwards. Where these structures are located are somewhat O/S dependent. Try restarting NFS first on the server and then on the clients. This may clear the file handles. Rebooting NFS servers with files opened from other servers is not recommended. This is especially problematic if the open file has been deleted on the server. The server may keep the file open until it is rebooted, but the reboot will remove the in-memory file handle on the server side. Then the client will no longer be able to open the file. Determining which mounts have been used from the server is difficult and unreliable. The option may show some active mounts, but may not report all of them. Locked files are easier to identify, but require the locking to be enabled and relies on the client software to lock the files. You can use on the clients to identify the processes which have files open on the mounts. I use the and mount options on my NFS mounts. The option causes IO to be retried indefinitely. The option allows processes to be killed if they are waiting on NFS IO to complete. 

Any ideas? edit - upon further inspection, this is only happening on the host computer. Another network computer doesn't experience the problem -- at least not at the same time, although there is the occasional lone timeout every 30 attempts or so. On the host computer, it normally stops responding like clockwork every 20-second or so. I'm very confused, but I'm starting to think my windows configuration is to blame. edit2 - tried setting a different ip for the vm, no luck ... only the host computer is having trouble. Host computer talking to the router and other computers - no interruption Other computers talking to the vm - no problem Deferred host computer's dns to the router and told the router to connect to the vm's dns - host is getting uninterrupted dns now Host trying to reach the vm 'directly' (ie. by network ip) - 15-seconds on, 15-seconds off. If it's an ip conflict, I have no idea the source. Tried assigning the host computer a static network ip address. No difference. Feels like someone's playing a cruel prank on me or something... anyone have a guess at all as to what I should test at this point? 

Most of them must be false positive, but I would like to understand what is happening behind these false positive and fix those who aren't false positives. This report did not appear from nowhere, I had these lines since the very first run of RKHunter. About the deleted files RKHunter runs on a daily basis through a cron, which explains why , (and maybe too?) show up here. However, I don't understand why: 1) All these deleted files are used by processes. Does this report means that each of these processes is trying to use a file that has been deleted, or that it has used at some point a file that existed at the time but got deleted afterwards? I reckon if the answer is the first option, it might be a problem. Can / Should it be fixed? 2) 4 processes of the same program are using the same deleted file. Even if the use of the deleted file is a false positive, is there something in this report that indicates that these 4 processes are an error (instead of having just one)? I understand that this might be completely app-specific and that there might be no answer from the report here, but I am just trying to understand why they are here. For information, they refer to a web application that is running in only once throughout the server (which is mono-threaded and mono-core). About the listening process I indeed have DHCP running on this server (out-of-the-box, I haven't tweaked it nor know much details about DHCP). 3) Is it normal that RKHunter reports a warning about it? If it is a false positive, is there an obvious reason why no other processes listening to the network are reported here (like , ...)? 

Try configuring a connect ACL to drop connections from invalid senders. This can be done at connect time with an ACL. 

You can get a very detailed report on your SSL configuration and issues from the site $URL$ This may help you discover what the issue is. 

TCP is only required, and usually only used when a long response is required. There can be negative impacts. Zone transfers are done over TCP as they are large, and need to be reliable. Not allowing TCP from untrusted servers is one way to ensure that only small answers are given. With the introduction of signed DNS answers, there has been a requirement for loosening of the 512 byte limit to UPD answers. EDNS0 provides the mechanism for longer UDP responses. Failure to allow DNS over TCP is highly likely to break an secure DNS implementation. It is perfectly possible to run a DNS server which only has UDP port 53 open to the Internet. TCP access to DNS peers is required, but this is a small list of hosts. There is a newer RFC596 that now requires TCP for a full DNS implementation. This is aimed at implementors. The documents specifically does not address operators, but warn that not allowing TCP can result in a number of failure scenarios. It details a wide variety of failures that can result if DNS over TCP is not supported. There have been discussions of using TCP to prevent DNS amplification attacks. TCP has its own denial of service risks, but distribution is more difficult. 

I'm running an ubuntu server (bitnami lampstack) vm in vmware-server on a windows host machine. The vm every 10-20 seconds stops responding to the host computer for no apparent reason. I can still access the vm and ping it on other network machines and over the remote console on the host, but it will periodically stop talking to the host in every other way -- no ping, no response from the webserver, no dns -- even though all are alive and kicking at the time. Bridged networking, /etc/network/interfaces is pretty straightforward. 

I know this is a horrible, generalized question with no good answer, and I apologize ahead of time, but I wonder if someone could take a stab at a very broad estimate. Let's say you have a dedicated MySQL server running on about $1K worth of modern hardware. Let's say the average user makes 20 read requests and 5 write requests per minute -- all straightforward queries, no joins; mostly along the lines of 'select this row by UUID' out of an indexed table of ~10,000,000 rows. Very, very, very, very roughly how many concurrent users would you expect a server like that to handle before you're 'pushing it.'