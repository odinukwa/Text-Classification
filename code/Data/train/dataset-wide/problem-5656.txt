If I understand well you're asking "How can we be sure that the logical rules we use to prove things about Logic are valid ? How to escape that strange recursion phenomena ?" They are many possible answers and no consensus. The foundations of Logic are not clear at all and diverges in a lot of directions and oppositions (realism vs anti-realism vs radical anti-realism, holism vs reductionism...). Moreover they're so many logical systems... it's almost absurd (classical, intuitionistic, linear, relevant, paraconsistant, quantum, modal, fuzzy, epistemic, ...). I think it's fair to say that after few thousands of years we don't really understand Logic. That's why some people threw the foundations away to focus on other things. Two major points of view : 

Logicists such as Frege and Russell wanted to reduce mathematics to Logic but it was a failure. It seems that mathematics are more "free" than Logic. Logic seems to give a constraint/format to force things to be "coherent", "well-behaving". That point of view is renforced by the fact that Curry-Howard see formulas as types for programs. Types in programming are used to restrict the programs to the one which behave well according to a concept of what "coherent" is (for instance we can't sum two characters). 

Intuition : Suppose that we have a proof of (X & Y) and take 0 as your new "goal". Use "function application" to solve the problem. Exercice 2 Hum...I don't understand why you wrote the first part of your attempt. It doesn't seem to be related to the question you suggest later. It seems to me that (c) is an inference rule. That is, given the top part you can infer the bottom part (forward reasoning). There's also an alternative reading from bottom to top : if you want to prove the bottom part you have to prove the top part (backward reasoning). For your last question, it seems that you're using the principle of explosion. But in my opinion the right thing to do is to see (∀x.Fx) as a transformer : given an object x, it gives a proof of Fx (you can produce any Fx, including 0 = 1, with the inference rule (c) you have). I may be wrong since I'm not sure to be right about my interpretation for your second exercise. 

Afterword : maybe some philosophers developped these ideas by using concepts from the Philosophy of Language (see for instance the texts from Ludwig Wittgenstein). But is logic only about language ? 

It's an old question but I will still try to give an answer focusing on intuition. There're a lot of intuitive interpretations of Linear Logic. We can see Linear Logic as a Logic dealing with limited resources but also as dealing with static and non-static truths or even actions/processes : it rejects the duplication/erasure of truth/resources (contraction and weakening in Sequent Calculus). That is, we consider that : 

where Γ and Δ are two different "contexts" containing formulas. They are equivalent if we consider arbitrary duplication and erasure of formulas but they are no longer equivalent when we forbid them : it's a refinement of the usual logic. The forbidding of structural rules give birth to two different conjunctions and disjunctions. You can find the definitions of the rules in Wikipedia. 

I want to read the "The Logical Basis of Metaphysics" book from Michael Dummett. Is it hard to read ? I would like an answer for the same question regarding his other texts. Do I need a prior exposition to other concepts, texts (even from himself) or authors ? I mostly have a technical background in Logic (especially related to Computer Science) and I'm aware of only few philosophical ideas. 

Afterword : Today, Linear Logic seems to be the closest system to what Logic truly is (see for instance the texts from Jean-Yves Girard). Philosophers, Logicians, Mathematicians and Computer Scientists and even more should work together and share their ideas to understand what Logic truly is. It seems that it was already done before but only in France with french texts for some reasons I don't know... (see LIGC group) 

Hum... I'm not sure to understand that one. In your example the reason may be that the space of search is finite when in mathematics it can be infinite. But it's not a general thing if in "real world" you include results about physics for instance. Epilogue : Toward the deep structure of logic The origin of rules. In the 20th century, logicians and computer scientists discovered something amazing called the Curry-Howard isomorphism. It states that : 

An interesting thing to point out is that the "rules" you have correspond to what we call BHK Interpretation. Exercice 1 The main problem is that you don't use the new meaning given to the negation ¬ (different from the classical logic). 

Toward a formalization of proof. Mathematician were already making proofs a long time before formal logic appeared. Around the 19th and 20th century a lot of system of proofs were established according our conception of proof. 

When we want to show that a formula is a consequence of a set of formulas, every hypothesis should be used once and is consumed after each use. This process is sometimes compared to chemical reactions. Two conjunctions and disjunctions In Logic we can formulate the conjunction and disjunction rules in two equivalents way. For instance, for conjunction : 

Another interesting thing : what happens when we forbid/restrict the structural rules (contraction and weakening also called duplication and erasure) ? See Linear Logic, Relevant Logic, Affine Logic... 

The Curry-Howard isomorphism seems to give a satisfying point of view about Logic but we don't have a lot of philosophical interpretations yet. That correspondence tells us that proofs behave as computer programs and formulas as type for programs : it gives a natural account to Logic, Logic share something with computation which seems to be "natural". 

With such a system we can restrict the space of proofs within the introduction and elimination of a finite set of connectives. So yes, we use certain strategies to prove things. Another questions : 

The first two questions can be answered, the last one may be a question of psychology or biology (there are actually some litterature about it). 

There're not a lot of litterature about that but I think that correspondence gives a "natural" status the logical rules as coming from the natural concept of "computation". This paradigm interpret logical rules as programs constructions : logical implication is seen as a function in a programming language, conjunction as a pair, disjunction as a sum type/algebraic type... An even finer analysis of logic. Recently logicians like Jean-Yves Girard tried to look even deeper in the analysis of logic : in most proof systems we have a rule to "duplicate" and "erase formula". For instance, giving A ⊢ A, the following still holds : A,A ⊢ A. And giving A,A ⊢ A we have A ⊢ A which still holds. What if we remove these rules and see proofs like chemical reactions where we consume formulas ? It led to Linear Logic which eventually reintroduce infinity/staticity of truth with new operators !A and ?A. Linear Logic finally led to a new dynamical and interactive status for proofs which is still being investigated nowadays. That new approach fully rely on the concept of cut elimination which is now seen as interaction, a central idea of current research in Proof Theory. Toward a fully internal logic. As you said, the problem of axioms and theorem can be problematic. In logic we rely too much on an external world. Logicians tried to make an internal logic called Ludics which is also currently under investigations. Ludics have a concept of "partial" proofs where two partial proofs interact together with one trying to prove a formula A and the other the dual/negation and only one win. The deny. Recent researches in Proof Theory interacts a lot with computer science and tends to give a preferential treatment to logic as explained by computation. A quite controversial question is : what about the logical systems where the theorem of cut elimination (cuts can be eliminated) doesn't hold and which doesn't have any corresponding system in Computer Science ? Are they really "valid" ? Some systems in "philosophical logics" doesn't enjoy the theorem of cut elimination but it's another debate. Summary 

The link between logic and computation is stronger than ever, especially since the establishment of the Curry-Howard isomorphism specifying that can be seen as and as program's . I wondered if we could find any texts providing a philosophical viewpoint of the relation between logic and computation. I couldn't find any document about that. Moreover, I have some related questions : 1) Since most logical systems (e.g intuitionnistic natural deduction, classical sequent calculus) corresponds to computational systems (e.g simply typed λ-calculus, system F, combinatory logic...), can we say that logic and computation have the same nature and origin ? A lot of difficulties arised from the question of the nature of Logic, does computation give an answer ? 2) Can we say that any system which doesn't share computational properties with a computational system is not "a logic" ? (e.g no cut elimination theorem, no confluence/church-rosser property) EDIT : After some research The only things I could find were the work of the french group LIGC but most of the articles they write are in french only. It seems that most of the works linking philosophy and computation concern Linear Logic (which emerges from the Curry-Howard isomorphism) and the Lambda-Calculus (which give a formal account to [functional] computer programs). If I'm not wrong, Linear Logic takes computation (cut-rule elimination seen as evaluation of programs) as a basis for logic. Some properties on programs such that the cut-elimination theorem, confluence or the church-rosser property, when taking the point of view of logic, ensure that our logic behave in a coherent way. We rely on the operational behaviour of logic rather than language or purely philosophical foundations. It seems that these work haven't reached the english community yet but maybe one can find some articles in english written by the members of the group. Some not-too-technical papers (unfortunately in french) : 

After some researches I think I have a quite satisfying answer based on Linear Logic (which is said to be "anti-realist". That's why Girard says that it's unacessible to realism. For more information check "Which Logic for the Radical Anti-Realist" from Bonnay). I won't explain here what Linear Logic is. In Linear Logic, the implication is ⊸ (called linear implication of lollipop) so we can translate the problem to the distinction between ⊸ and ⊢. 

A regular logic without identity A=>A is not imaginable because if something isn't itself we can't prove anything. But actually there is a system called Ludics strongly related to the Curry-Howard isomorphism and to Linear Logic which don't use identity and reformulate Logic in a completely new way. A logic which can't prove A / ~A is an Intuitionistic Logic. See for instance the philosophy of Brouwer (Intuitionism and Formalism). It's a logic that can be used to describe computer programs in the "functional paradigm" (See Curry-Howard isomorphism). Moreover, intuitionistic logic can be described with a sequent calculus without right contraction (conclusion). 

The concept of anti-realism in logic seems to be an interesting and growing idea. I'm looking for references (papers, books, authors...) regarding the debate between realism and anti-realism but especially related to logic and mostly on the anti-realism side but the defense of realism or the criticism of anti-realism are, indeed, accepted. For instance, I'm interested in the ideas exposed in The Realism-Antirealism Debate in the Age of Alternative Logics but realism-oriented. It seems that Anti-Realism has a link with substructural logic (see : Radical Anti-Realism and substructural logics) As far as I know the main current programs are : 

It's similar to the duality between ∧ and ∨ but with two conjunctions and disjunctions. Linear Implication is actually a shortcut for . More generally, is also a shortcut for and are not only "logically equivalent" but equal. means : by consuming we can produce . In the alternative syntax, can be read "consumes A and produces B in parallel". Exponentials The exponentials (bang/of course) and (why not) re-introduce the potential infinity we lost by forbidding duplication/erasure. can be read "as many as we want". They don't have equivalents in the regular logic because they're hidden : the usual implication is actually refined to in linear logic. In the usual logic (classical or intuitionistic) truths are static so when we know we can use as many times as we want to prove . We can actually redefine classical and intuitionnistic logic from linear logic with the exponentials. is just the dual of that is : potential infinity occuring in the hypothesis. If we have it becomes . It's the symmetry between hypothesis and conclusion provided by duality. 

I'm not sure to fully understand your questions but I will try an answer biased by recent formal results in Proof Theory and my viewpoint of a computer scientist. So it won't be a very "philosophical" answer in the sense that I don't fully rely on philosophical references. Note that the nature of proof is not clear at all and it's still something we seek. We actually have several answers for that. 

I think it's fair to say that to imagine a world with a different logic is a bit like imaginating a world with different physical laws. 

Duality The negation is called dual (⊥ or ~) and is no longer seen as the usual negation. It's used to converts hypothesis to conclusion : means that we can consume the formulas of to produce but since the duality is involutive , is the same as where is the dual of all the formulas of . Example : is We can observe that it divides the rules by two (as we can see in the Wikipedia page). Girard sometimes uses the expression "hegelian negation" to refer to linear duality. Duality define a symmetry between hypothesis and conclusion. We can also see as a consumer of . The relation between the operators are given by duality :