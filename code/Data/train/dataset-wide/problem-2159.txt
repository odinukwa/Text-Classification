char(3) is the natural key It's short enough that not using it (adding a surrogate key and using joins all over) adds more overhead then having an extra 2 bytes per row over tinyint. Re-phrasing, not using it adds unnecessary opaqueness and code complexity. And from experience, it is just easier to use (3) even with 40k writes/second and billions of rows. 

A short answer I had similar issues when I was a BI DBA, trying to control deployments without blocking the business needs. And generally prevent yet another badly coded csv-parsing function to add to the current 13 or so already pushed to live. 

Either way works it depends on your maintenance window. For larger tables, I prefer the duplicate table approach because it eases locking/maintenance window problems. I'd use a batch size of 50000 or 100000 too 

Basically, permissions are needed on (see permissions section). The roles are described in "SQL Server Agent Fixed Database Roles" (linked from above) Edit, Jan 2012 After the anonymous downvote 2 years after I answered... Read the question. It says 

It sounds like you have a "One True Lookup Table" (OTLT) anti-pattern and you are mixing entities in this table. You've found why it isn't a good idea: 

Use an archive table and a live table. Don't use a "table per year": this quickly becomes messy and you'll end up needing dynamic SQL when you UNION + view becomes a bottleneck 10 million rows isn't a lot though, and 30 columns is quite wide. So, some options: 

There isn't an optimisation in SQL Server libs for repeating data. Unless I've misunderstood, the logic is very flawed above too If there are 20 rows in B for each A, a 1000 rows in A implies 20k rows in B. There can't be just 100 rows in B unless there is many-many table "AB" with 20k rows with the containing the mapping. So to get all information about which 20 of the 100 B rows map to each A row you table AB too. So this would be either: 

In this special case you've highlighted, they do the same thing, There is no difference. I'll guess that ALTER SCHEMA was added for completeness (eg there is CREATE and DROP why not ALTER) or in preparation for other features (such as renaming a SCHEMA, say) 

SQLPing should do the job I've used it on medium and small networks and it seemed to find all stray SQL Server installs 

In simple terms, you need RAM and IO performance (latency + read speed + write speed) for databases. The choice of 4 or 6 cores or 2.5 GHz vs 3 GHz is not really relevant (I assume you aren't having to choose between a P3-450 with 32 GB RAM or the latest Xeon with 1GB RAM). If you're CPU bound, then you have other problems (poor design, poor indexes, swapping, non-dedicated server etc) 

Basically, yes. A "heterogeneous" query is executed on a "heterogeneous linked server". All queries to this linked server will be heterogeneous. And "heterogenous" is a subset of distributed queries 

In practical terms from a code or user perspective, no. There is one internally () but it is not exposed in any documented system view or function 

The query is run in DatabaseA, not DatabaseB, so I'd say by design. That is, SQL Profiler captures "what is the connection database context" not "In what database is the object am I accessing" This makes sense: each database is logically isolated from each other. Using a synonym or other cross database query is a special case because you can't ensure all databases are in synch (especially around restores) nor enforce foreign keys etc An other example... 

SQL Server is picking a different plan on the other box. Restoring will typically remove issues based on statistics, so I'd look at server differences. Some coarse checks first. Don't assume: check 

SET CONTEXT_INFO exists for SQL Server 2000 too. However, you have to query to read it back. was new for SQL Server 2005. 

Where xxx is, say, 50000 A modification of this, if you want to remove a very high percentage of rows... 

So, one column can store all languages, but it can have only one collation. You can "add" extra collations using computed columns, or coerce it during a sort/compare using the COLLATE clause: Example 

Yes. It's good that you use SCHEMABINDING (we do always) and sometimes you have to remove it to change a dependent object. Just ALTER the view 

OBJECTPROPERTY is local to the database the query is run it. So the passed in is resolved against : but the object_id comes from So here you have case 2. On my server, I have 37 matching object_id values between and . But the names are different. 

You either recreate as above or restore it. The info above is similar to MSDN, see "Creating a New msdb Database". Note: if this came from Paul Randal's blog or the SQL Server Storage Engine then you can trust it 

I haven't tried this myself so don't have any tips. However, an indexed view is the only way to have any index span tables. 

Query results are not cached However, the source table and index data and metadata will be cached after the 1st use (subject to continued use, load and memory pressure though) That is, the results of a query will be evaluated every execution but the tables(s) (and any indexes etc) used by the query will most likely be in memory already. The compiled execution plan will be cached which is where the confusion comes from I suspect 

One of the easiest ways to use a full backup to prepare, a differential to migrate. First off, you do a full backup and restore to prepare the files on the new server disks, but use the option to allow more restores. At the time of migration, you do a differential backup/restore. This is far quicker and you use this time. 

64 bit always. You do understand 32 vs 64 bit generally? Windows Server 2008 R2 of course. Windows 7 is desktop, Windows 2003 is obsolete 

There isn't normally any added value in doing this: you may cache data that isn't needed that requires eviction when your real load occurs. How much time do you think this will save too? Have you done some measurements to see if running a SELECT * on some table improves response after server start up? 

The JOIN after the scan gives a clue: with less rows on one side of the last join (reading right to left of course) the optimiser chooses a "nested loop" not a "hash join". However, before looking at this I'd aim to eliminate the Key Lookup and the DISTINCT. 

You need msdb.dbo.sysjobhistory and a few JOINs of course to read the data To change how SQL Server Agent uses this table, use this stored procedure with suitable values 

70,000 per 2 minutes isn't much: you can do that per second easily in SQL Server. Saying that, the question is otherwise too wide. The choice of edition doesn't really apply: your design and hardware setup matter more. Standard or Web should be enough. If you budget extends to a single RAID 5 volume, for example, then forget scaling up. Or you let nHibernate design your schema. 

Ideas (not sure if applies to MyISAM though, I work with InnoDB) Change the index "parent" so it is on 3 columns: parent, order, name. This matches the WHERE .. ORDER BY Remove . Only take the columns you need. Add any other columns to the index "parent" This will allow the optimiser to use only the index because it is now covering. As it stands, you have to read the whole table because the indexes aren't useful for that query 

Sorry to say it, but best practice is to always qualify object names with the schema anyway. This is mandatory for things like schemabound views. Why is it best practice? See this by Tibor Karaszi or this by Midnight DBA or just trust me or the MS SQL Server Best Practice Analyzer After comment... Have you considered synonyms to make OtherSchema.mycode point to [DOMAIN\Glen].MyCode? 

"Business Intelligence" covers everything from database design to Excel monkeying. Based on this, my interpretation of the new BI edition is "whizzier" features on the OLAP/cube/analysis/mining side compared to the RDBMS side. Arguably, only "column store" is really relevant to BI. Partitioning itself may be Enterprise edition only, but the ALTER TABLE..SWITCH can be run in Standard edition. The BI edition also has no memory use limit for SSAS and SSRS 

You need to stop SQL Server or even reboot the server Or more likely you could be rolling back a huge UPDATE or such: wait or restart, up to you... Edit: Aaron's comment of changing a DB status may work as well as an intermediate step. 

The transaction is doomed with pretty much any exception and must be rolled back. From "Using TRY...CATCH in Transact-SQL" on MSDN 

The sniff comes from the NULL or the empty string. The addition of is ignored. You can fix this one of 2 ways: 

Without doing anything else, add more RAM in the server. RAM is used for caching data and plans. So add more to reduce pressure there. Adding too much RAM is possible but with the amount you've got going on, I doubt it. Then, run some dmv queries to find things to tune: 

An example of how it can happen: Two processes, each one has a shared lock on the same page, then an exclusive lock is needed by each process. Each process can't exclusively lock the page because of the shared row locks. So, a deadlock happens on the same page. More examples here from @AlexK 

server option only exists on SQL Server 2005+ to enable legacy mail features. For SQL Server 2000, it doesn't exist because you have to use Agent XPs for mail. There is no SMTP support via Database Mail I really would considering upgrading, given SQL Server 2012 is due in a few weeks. There is no direct upgrade path from '2000 to '2012. 

Background It is possible that there is a "client alias". The client alias will basically "hijack" and overrides port, protocol, instance, server etc. Personally I don't like them. This can be done explicitly with the "SQL Server Configuration Manager" or by the Control panel ODBC thing. Check and remove it. Alternatively, you can poke around in the registry and remove it under: 

If you mean in the GUI, then this should be Windows locale dependent. If you mean the GUI designers, then frankly I don't know because I use raw SQL. If you mean the actual SQL language, then it is English Edit: Under Tools.. Options, you can select fonts. This may be what you want: 

Don't mix storage (5 or 9 bytes) and what you get back In a decimal(19,5) column it is always 9 bytes on disk. So zero will take nine bytes. Your is merely a representation of that stored number that is then parsed as 10999.99999 by DATALENGTH Edit: DATALENGTH will return the number of bytes needed to store the expression given. It ignores datatype mostly. So is treated as decimal(10,5) which can be stored in 5 bytes not 9 as Martin pointed out in a comment. So a decimal(19,5) column is always 9 bytes. DATALENGTH doesn't care. It looks at the expression and decides it can be stored in 5 bytes. But of course it can't Personally, I've never used DATALENGTH except to find trailing spaces or such. Basically, don't use DATALENGTH on non string datatypes. 

If you sell devices and want to protect data in that device, then you should use an embedded database such as SQLite or SQL Server CE. You have a client/server database engine that is external to your app. Therefore, the MDF is not under your control. If you require an external engine and MDF to be under your control, then enforce it with the software license. There is no way to enforce what you want via permissions or software: it is isn't your PC or Server or network. It is your clients'. Which leads to another question (off topic for this site).. .if you are collecting data and it is important, do your users know about this? 

Except for , the fixed db roles have no overlap with each other. You can see this here ("Permissions of Fixed Database Roles") where the newer GRANT options are listed against the roles 

There is no MOVE command. There is the "RESTORE" command with arguments "WITH MOVE" This deletes the old MDF and LDF files (if database already exists) and recreate the MDF/LDF files (and NDFs etc) where specified 

This doesn't take into account roles etc: it all works on which can be seen if you look at the definition of INFORMATION_SCHEMA.TABLE_PRIVILEGES Personally, I'd just use sys.database_permissions and not bother with the information_schema rubbish. This relies solely on Meta data visibility to filter rows, so you'll see the actual permissions unfiltered by the INFORMATION_SCHEMA.TABLE_PRIVILEGES view which adds a further filter that bollixes you. For completeness, here is the view. Note the implicit "old style" JOIN :-) 

You have to use one of the SQL Server distributable packages to stay within licensing terms. These are not full installs. The one you want is "Microsoft® SQL Server® 2008 R2 Command Line Utilities" Note: copying bcp.exe by itself will not work 

Just change ownership for starters. Database ownership (the owner_sid in sys.databases) not have really have much impact on day to day operations 

With a CTE? This assumes that you get one row from Dolar and Euro. If you get more then one, add a TOP 1 ..ORDER BY: your original query is wrong too then 

Using a trigger You can't use functions as defaults in MySQL, except for CURRENT_TIMESTAMP for TIMESTAMP columns 

The backup size is simply used 8k pages. These 8k pages are part of the MDF. On restore, the MDF and LDF files may need recreated (if already existing and different in some way) or just created So backup file size isn't an indicator of space used on disk by the restored database. And this ignores backup compression too So, if the MDF is 100GB (with 2GB actually used), : unless you have Instant File Initialisation on, then the 100GB needs zeroed. Note the LDF must be zeroed. 

Note: If value is not nullable then it is the same as semantically. But for SUM it need the actual value, not existence. As an example, if you change to without changing the index it should break the query again because it has to process value as a value, not as existence. The query needs 3 columns: added, fk, value. The first 2 are filtered/joined so are key columns. value is just used so can be included. Classic use of a covering index.