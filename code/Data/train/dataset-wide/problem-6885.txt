Guess what the time indexes are, and confirm they are correct. To do this, lag the equation for $T_t$ (so you get rid of expectations), and replace $T_{t-1}$ and $T_t$ with your guess. If the equality holds, your guess is correct. If not, try again. For example, let us assume $T_t= (1-\delta)\frac{B_{t-1}}{p_t} $ (the correct one). 

This handbook provides a broad overview of different modelling approaches to choice theory. Here, I give just one example: discrete choice modelling. Section 2 of this paper offers an example of a dynamic discrete choice model about educational choices. The decision tree is: 

Another trick is to compare their marginal rate of substitution. If for a similar marginal rate of technical substitution (i.e. relative prices) they yield the same relative inputs, then they represent the same preferences (basically one is a monotonic transformation of the other; see below). For the first preferences you give, this is: $$ \frac{\dfrac{\partial u}{\partial x}}{\dfrac{\partial u}{\partial y}} = \frac{y}{x} $$ It is trivial to show that the second function yields the same MRS. Hence, they represent the same preferences. 

Here is a detailed worked example of how to use the command, including an example of the Spatial Durbin model. As the document says, it is estimated using Maximum Likelihood. The model can be of Random Effects or Fixed Effects. Key screenshots below: 

To close the model, the way that the actual equilibrium is found depend on international prices, where the MRS is tangent to the PPF. Alternatively, in a closed economy, the isoquant of consumer preferences provides the MRS. 

You are right. The problem here is a corner solution. Let us define the optimal combination $X^* = \beta_1e_1^* + \beta_2e_2^*$. The first derivative gives you $$X^* = \dfrac{l_1}{2\beta_1} $$ whereas the second gives you $$X^* = \dfrac{l_2}{2\beta_2}$$ They can only be true "by chance". This is, the existence of an interior solution cannot be assured for every possible parameterisation allowed by the parameter space. Actually, if you take a look at the two functions, this is clear. The cost function generate linear isoquants. You can check this here. This is because the cost function is equivalent to $c = X^2$ and $X$ is a linear combination of both $e_1$ and $e_2$. Similarly, the value function also gives linear isoquants. This is because the function has the form $e_1 = a + be_2$. Then, the most likely case is that the isoquants will join in a corner. The interior solution happens just by chance. Notice that when this is the case, any interior solution is optimal! The equilibrium set is then infinite. 

As the FAQ explains, this analysis is based on data from the World Bank, is valid for 2008, and assumes a population of 6.69 billion. Therefore, based on the above result, 5.34 billion people lived below $15 a day in 2008. This is "sort of" consistent with the data in the graph you posted, in my opinion. Unfortunately, such analysis is only valid for 2008. 

Feminist Economics Unlike Neoclassical Economics, which focus is usually on scarcity, Feminist economics focuses on the issue of Power relations, particularly in relation to gender and family structure. For example, whilst a neoclassical economist would study the gender gap in terms of benefits and costs (e.g. cost of post-natal leave), a Feminist economist would focus on how institutions (ranging from firm culture to family structures) are designed to the benefit of men's development and detriment of women's development. Core elements of Feminist economics are: 

According to the latest ONS report, inflation is rising as a consequence of the (pre/post) Brexit devaluation. This graph about the Input PPI is pretty revealing (taken from here): 

The key here is that a gross increase in the cost by approx. £4 billion reduces to a net increase between £1.7 and £2 billion (or £2.2; see Table 6a). The difference between scenarios depends on the assumption of the fiscal multiplier, which can range between 0.34 (used by the Office of Budget Responsibility), and 1.02 (upper bound used by the IMF). Naturally, the larger the multiplier is the higher the benefit from the wage increase, and the lower the net cost. Institute for Fiscal Studies (2017) - Gross cost The Institute for Fiscal Studies provided an analysis of the 2017 general election Labour and Lib Dem manifiesto, which included a lift to the 1% public pay cap increase. As the report states: 

Ok, after several analysis of @denesp's answer, and our subsequent chat discussion, I think I got the answer I was looking for. As @denesp rightly pointed out, the optimisation problem yields two FOCs. These are: \begin{equation} \frac{w}{p} = \alpha AL^{\alpha-1}K^{\beta} \end{equation} \begin{equation} \frac{r}{p} = \beta AL^{\alpha}K^{\beta-1} \end{equation} We can rearrange each of them, to the form $L=f(K)$. These are, respectively: $$ L=\left(\frac{\alpha Ap}{w}\right)^{\frac{1}{1-\alpha}}K^{\frac{\beta}{1-\alpha}} $$ $$ L=\left(\frac{r}{\beta Ap}\right)^{\frac{1}{\alpha}}K^{\frac{1-\beta}{\alpha}} $$ Now, for a given parameterisation (with non-trivial $p>0$), we can plot this functions in the $\{K,L\}$ space. Additionally, we can the optimal capital-labour ratio, which comes from equalising the MRTS to the MRS. As I showed in the question, this optimal relationship is given by: $$ L^*=K^*\left(\frac{r}{w}\frac{\alpha}{\beta}\right) $$ So, we can now plot the three aforementioned functions: 

Additionally, consider the case where "each set of points, in output of commodity space, at which marginal rates of substitution are constant, is a straight line". This is, a straight Engel curve. Are these two definitions equivalent, in the sense that each implies the other? Or to put it differently, can we define: 

Focusing on less general topics and more mathematically involved, you can combine Acemoglu's book on economic growth (a rather mainstream perspective) with the "Handbook of Alternative Theories of Economic Growth". Also, Elgar put together a cool handbook of the most important papers on Capital theory, covering a whole range of perspectives. 

I am running a panel data estimation, where I am including year effects. My goal is to see if there is a time trend in the data, after controlling for other factors. Using random effects, I find the trend is upward sloping. Using fixed effects, I find the trend is downward sloping. Is this a bad result? A Hausman test rejects the RE with p-value of 0.000. Does it mean that the trend values from the RE can be inconsistent? Or is the FE capturing something different than the RE model? Differences in differences perhaps? 

I have data on disposable income, where some households have negative income. Albeit I can immediately compute the Gini with these dataset (e.g. with , or in Stata), it is well known that Gini with some negative values could be higher than 1. An example from such reference: 

PS: as it turns out, the marginal product of labour at $L=0$ is not infinity. Apparently, the Inada conditions do not hold for the CES with $\rho<0. Actually, the marginal product of labour seem to be zero! (check slide 15 here). This is a major revelation to me. 

The conclusion you want to prove is not always true. I will give you a graphical and mathematical answer. Graphical analysis Consider the example in the image below: 

With theory, it is hard to innovate unless you go to the actual knowledge frontier. My suggestion are several, and complementary: 

(source here) Interestingly, the profit share (which is normally included in the capital share, but it need not to be) has remained more or less stable in many countries (see Figure 6 here), but has gone up in the last decades in the US (a country that has experienced a large increase in income and wealth inequality, not the least because a significant fall in the labour share). Yet, this increase has been linear rather than exponential: 

But, what if that transfer is spend in goods? Would that not increase C? What if just a portion of the transfer is used to buy goods? In fact, an article from Business Insider states the following: 

Plenty of journals which are not open access per se do publish some papers in an open access fashion. For example: 

Since the total economic support the program involved was of \$13 billion in contemporaneous time (roughly \$132 billion in Sept 2017 money), only 10% approx of the aid was in form of loans. 

An amazing analysis of World Poverty, in simple slides, is here. A careful analysis of existing evidence is found here. 

If the base year is not updated when methodological changes are carried out, you would not be able to tell that the series before and after the update are not the same. Hence, changing the base year is a trivial way to tell the public they are different. That is also the reason why the two series, even if in nominal prices, are different over the years. A comprehensive and critical analysis of these methodological changes for India can be found in this recent paper. 

See the chapter for more details. In conclusion, there is an attempt to provide homogeneity across goods and services. But there are many reasons why perfect comparability might not be achieved. In the case of government services, if inputs do not reflect outputs homogeneously across countries (partly due to productivity differences, as highlighted above), this will affect PPPs. 

For a series $y_t$, this is something like $$ y_t = d_t + c_t + s_t + e_t $$ Importantly, all the variables are defined in the same units by construction. E.g. if this is GDP, they are all in value added units (real, nominal, whatever). A general case of a decomposition is (assume just trend and cycle): $$ f(y_t) = d^{'}_{t} + c^{'}_{t} $$ In this case, the trend growth of $f(y_t)$ is the growth rate of $d^{'}_t$. But this is not the same as the trend growth rate of $y_t$. Consider an example. Let us construct a series $y_t$, where $d_t=d_0(1+g)^t$ and $c_t=sin(t)$, and $y_t=d_t+c_t$. This is a trend-cycle decomposition where the growth rate of the trend is $g$: $$ \frac{d_{t+1}-d_t}{d_t} = \frac{d_0(1+g)^{t+1}-d_0(1+g)^t}{d_0(1+g)^t} = \cdots = g $$ Now, transform the process into log, and let us postulate a new decomposition: $$ \log y_t = D_t + C_t $$ where $D_t = D_0(1+G)^t$, and $C_t$ is an unknown function. This is by definition correct, but $C_t$ might be an irregular, perhaps not algebraically defined function (like $sin(t)$). In this new case, it is correct, by definition, that the trend growth rate of the log of $y_t$ is $G$, computed with the standard formula. The central question is: can we find a general method to calculate the trend growth rate of $y_t$ ($g$) using only some transformation of $D_t$? (as in your attempt using the exponential). In other words, can we use only $D_0(1+G)^t$ to obtain $g$? (Here I highlight the point of why would you ever go down this route, if you can decompose $y_t$ in levels and solve your problem immediately; but anyway, let us continue). The answer seems to be negative. It is trivial to see that $$ e^{D_0(1+G)t} e^{C_t} = d_0(1+g)^t + sin(t) $$ where you cannot find a direct function between $G$ and $g$ which does not involve the use of $C_t$ and $sin(t)$. In other words, you need to use the information of both decompositions to solve the problem. But this is circular. To make the link between logs and levels, you need to make the decomposition in levels. But if you do the latter, the answer is there already. No need to go further. Finally, consider the very special case where the series $y_t$ only has a constant growth rate (trend): $$ y_t = d_t = d_0(1+g)^t $$ Then, $$ \log y_t = D_t = D_0(1+G)^t = \log \left(d_0(1+g)^t\right) $$ The following formula recovers the original growth rate of $y_t$ in terms of the trend from the series in log: $$ \dfrac{e^{D_{t+1}} - e^{D_t}}{e^{D_t}} = \frac{d_{t+1} - d_t}{d_t} = \cdots = g $$ The result can be generalised to any $f(y_t)$, where the transformation of the growth rate calculation has to be $f^{-1}(\cdot)$. 

I highlighted the one corresponding to the graph you show. The magnitude of the jump precisely matches the amount the statement above mentions. 

So, central banks pursue open market operations by selling/buying bonds from banks or other financial institutions involved in these operations. Maybe the author is referring to the "buying side" of these operations, whereby the central banks buy short-term debt from banks? Does this mean that banks only sell these bonds if they expect a profit? Does that also mean that banks only buy these bonds at a profit? How does this profit materialise? Through a discount/premium over the price at which these bonds are bought/sold? Or is the author simply referring to the fact that these bonds have positive nominal interest rates? (but these bonds are usually from the government, so it is not the central bank who is paying the interest) Does this mean that banks are in every single transaction profiting from such operations? (sorry, a lot of questions but they are very much related). 

Ultimately, a complete answer has to deal with the history of money. Here I just provide a very brief attempt to answer the question. There are dedicated books on the topic, of which the most authoritative source might be Glyn Davies' book. The analysis here is based on that book and Wikipedia. What is money? Davies defines money as: 

In conclusion, it is simply impossible to tell. The calculations you make are a simple ceteris paribus exercise of removing the UK from the _current EU budget. That might be illustrative, but given the huge uncertainty on the issue, it's probably a rather brute calculation. 

This will let you see possible structural changes in the PC, together with movements between two PCs. This gif shows this more clearly: 

Several mechanism are used to adjust for imbalances among these two factors (including weights). The list of problems continues. Which price to include? As the documentation states: 

A super simple calculator, assuming equally sized bins, is here. Simply input your data, comma reparated (e.g. 1,2,3,4,5 in your case), and you get the result, together with a nice graph! This confirms that Gini in your example is 0.267, as the other two people suggested. 

These are references for the following series of topics: 1 - Monetary Policy 2 - Sticky Price Models 3 - Macroeconomics at the Zero Lower Bound 4 - Open Economy 5 - Economic Growth 6 - Inter-temporal Macro 7 - Debt and Fiscal Policy 

It seems the author is assuming that income is received one further year after is is accrued. However, the text seem to me rather obscure. He mentions a "pamphlet regarding this working method which will be published later." (p.42). You might want to trace that one down. 

As a final comment, notice that one thing is for a theory/ideology to be unscientific and another is for it to be wrong. Furthermore, it is hard to imagine a theory/ideology that opposes the prevalent economic and political system to be mainstream. 

This is a massive area of research, revived recently under the title of "capital misallocation". Some reason why capital do not fully adjust: 

For example, in the first book, in Chapter 15, Hayek declares the importance of the nature of government intervention: 

Consider the standard IS-MP model with horizontal MP. According to (c), the IS remains in the original level. Therefore, the fall in the real interest rate $R_t$ leads to an increase in the output gap $\tilde{Y}_t$. This happens because the opportunity cost of investing is lower. As such, for an unchanged marginal return to capital $r$, investment increases, leading to a deviation of actual output from potential output, and therefore to a positive short-term output gap $\tilde{Y}_t$. In the IS-MP-PC model, inflation follows a simplified Phillips Curve, which is: $$\Delta \pi_t = v\tilde{Y}_t + \bar{o}$$ (notation borrowed from Jones, Macroeconomics, 3rd edition). Therefore, in the absence of a cost-push (or supply) shock ($\bar{o}$), deviations in inflation from the target are given by the positive output gap $\tilde{Y}_t$. In consequence, inflation increases. I don't really get your second question (is this preventable?). Clearly, a first best is for central banks to correct their models in order to avoid mistakes. This is of course difficult. It requires to identify potential output and to distinguish between aggregate supply and aggregate demand shocks. Not an easy task.