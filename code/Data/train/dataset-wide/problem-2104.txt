(n.b. I've never used AWS or RDS so there may be some differences). But as a general rule a MySQL DUMP file is just a series of and statements, that when combined create your database. Ultimately it depends on what options were chosen as to what does and doesn't get included. But I think that by default it includes the option. So if you open the file (which is basically a text file) you should see: 

If you're storing it into the same database you're measuring you could use a MySQL Scheduled Event to run the query at a certain time. an example being: 

Is there some trick to getting the Visual Explain feature to work in MySQL Workbench? I am running version 6.3.6 on Windows 7. I've tried simple queries with just one join. I've tried it with complex queries with 12 Joins. I've tried it with MySQL 5.5 and 5.7. But every time I just get I've looked on the Bugs for MySQL Workbench > Visual Explain, but there is no recent bugs, making me think it is something I am doing. But I can't see what. Does anyone have some suggestions? 

I figure I could put it into a MySQL Stored Procedure, and use a cursor to loop through the data and do the comparison. Or I could simply break it up and put each customers data into temporary tables, and run each comparison as a separate query to build another temp table with the result. But if there's a way to do it in a single Query it would be far better. 

However, there is no sign of in the folder. Trying to run it simply gives me I can see all the applications in there suggesting the installation worked ok. I then tried 

I'm trying to get to grips with Foreign Keys in MySQL (5.6), and can't figure out why the below doesn't work: 

No, this is correct. In your you issue a command, which creates a new Binary log. The initial position in the binary log (after headings etc.) is 107, and always will be (unless you upgrade to a different version - then it may be something else). 

Is it possible for a query to block / queries? I have the following (taken from ) on my slave database: 

It contains a list of events with the time it happenned, and the value. I am now looking to generate a report from this similar to: 

I don't have experience of your exact situation, but could you use: auto_increment_increment=3 auto_increment_offset=1 [then 2 and then 3] on your different masters, so that each one generates a different set of id's. e.g. server 1: 1, 4, 7 server 2: 2, 5, 8 server 3: 3, 6, 9 

First point is that you should only write to one Master at a time, with the other set as the 'backup' option in HAPROXY. This will prevent you from writing to an out of date master (unless there is a problem with the main one, in which case you probably don't have an option anyway). By writing to both masters simulataneously you are opening yourself up to a whole load of problems. If you need to write to both you should probably look at Group Replication or Galera Cluster. Second point, I assume you have taken precautions such as setting the and values on the two databases? So that in the event you do write to them both (during testing failover etc) you won't get duplicate entries on your values. If you want to test for Seconds behind (and you have Linux servers) you can use a simple script adapted from here: $URL$ We use it to check various values to determine if a slave database is up to date and available for use. 

On a basic level, yes. Just set up the current Master like you would any normal slave database. However, there are some things to consider first. Is the second slave set to , as you want to avoid writing to both masters at once. You probably want to set and on the first master, and and on the second master. This prevents the two masters from creating the same auto_increment primary keys. You also want to make sure you have no replication filters (binlog-do-db, replicate-do-db etc) set up as they can cause problems too). 

$URL$ As it stands the only option appears to be to install, configure, and test a copy of MySQL 5.6 (which I'll never actually use) to act as an itermediary between my live system and test system. But that just introduces a whole load of additional complexity when trying to run like for like testing between 5.5. and 5.7 slaves (e.g. is it MySQL 5.7 that caused problem X, or was it MySQL 5.6 doing something on the way through etc). I see that there is a BUG report for this, which they have since decided is Not actually a BUG, but a feature!!! 

I've just installed a copy of MySQL 5.7 for testing, and I love the fact that it now only creates a single root account, and puts the password into a file called .mysql_secret. There's just one problem. Where can I find the above mentioned file. On Linux it is apparently found in $HOME, but my test DB is on Windows. I tried various places including %USERPROFILE% but I just can't find it anywhere!! Can anyone please tell me where to find this file? As otherwise my testing will be over before it has begun. 

I open the file (in this case I use gvim for windows because of the size) and search for . This returns: 

So far so good. It executes every minute, and adds an entry to the table, which replicates to the other Database. However, on the Master B, it is marked as , and so doesn't execute. If I do: on Master B, it starts to execute on Master B, but on Master A it is now flagged as , and so doesn't execute. If I then enable it on Master A, Master B is . The reason for wanting this (in case you were wondering), is so that as part of my failover script I simply need to execute on each database accordingly, as opposed to having to enable / disable all my events (one command vs many commands). Under normal circumstances, on Master A () the events execute as per normal and adds the entry; On Master B () the events execute, but don't insert an entry as they don't have permission. I looked at using as the one command, but if I set it to as the default, then we need to remember to enable it each server restart. Alternatively if we set it to as the default we need to remember to disable it every server restart. The use of seemed a better option as it can be easily included in a failover script. Any ideas? 

You can also try . I sometimes find one works when the other doesn't for some reason. Failing that just delete them with like you said. It's not technically correct, but it shouldn't be a problem (I did this frequently in the past before I learned about the commands, and never had any problems). Of course you also want to make sure you have set as well to keep them down in future. I also just saw this note: 

There are a handful of options that are Dynamic, but for the most part they have to be added to the cnf file, which means you need to restart your live databases if you want to use it. 

Making changes to binlog-do-db will require a restart I'm afraid. Personally I would remove all the binlog-db-db from the Masters my.cnf, and just use replicate-do-db on the slave to filter what you want to be processed. In this way you can addd more databases to the master in future without needing to restart. 

I'm sure there's a really obvious answer but: Why is the (byte length) of a string in a field shorter than the actual string? e.g. 

This works, but as soon as I alter the Event on Master A, Master B goes back to and I have to repeat this process. So not perfect by any means, as it relies on us remembering to do this for each event, if we them. 

I have a shell script which loops through a folder and picks up a set of files in turn. I then pass the to a .sql script so that the file can be imported into my database. BUT, I can't for the life of me get it to work. My Basic script is as follows: 

Looking at the documentation it tells me that for the most part they are not Dynamic. So how do I set them to what I need at the time I install the plugin? From what I can tell the only option is to add them to my file and restart the Database. That's fine for a Test System, but I can't go round restarting my LIVE databases, just to install a plugin, surely. 

I came into work this morning, and first thing I noticed is that one of my Slave databases was running behind. I took a look at the and could see that I had the following running. 

for all of your tables. Restarting the loading process will simply start from the beginning of the file and carry out the actions accordingly. (I assume that the import has actually died - its not uncommon (in my experience) to lose connection to MySQL during the import due to the time taken, but the process continues in the background. You can use to check this). 

will fail as has been set as the default database. Whereas under Row Based Replication it should be fine as it looks at the database that is changed rather than the default. $URL$ However, with all DDL statements are replicated as Statements. $URL$ 

There is a 'backup' option that you can use in HAProxy that does what you are after. e.g. if M1 fails, use the the backup (M2) $URL$ Then once M1 is available again, it redirects back to M1. n.b. you do of course need to think about replication delays, as you don't want to redirect back to M1 if it is an hour behind M2 when it comes back up. We use something like this to achieve it: $URL$ 

If you are using replication to 'replicate' data between the different data centers then you can use something like: auto_increment_increment=3 auto_increment_offset=1 #data center 1 auto_increment_offset=2 #data center 2 auto_increment_offset=3 #data center 3 which will then offset the auto increment accordingly. e.g. data center1 - 1, 4, 7 data center2 - 2, 5, 8 data center3 - 3, 6, 9