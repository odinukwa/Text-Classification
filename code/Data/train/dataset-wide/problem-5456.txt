The lesson Willard wants us to learn from this is that we can lose track of the reference of a particular individual term whenever we move it across a modal operator. So the problem with your argument, on this account, is that you are using a constant, Quine, to track your intended reference across possibilities, when in fact it is not at all clear what, if anything, you are tracking when doing so! Now one way around this objection against certain kinds of modal technology is to move from thinking about Quine as a referring constant term (and hence involved in the third grade) to what it means to have the essential properties of being Quine - to rephrase your concerns in terms of a Trans-world Identification predicate (and the safe forms of the second grade). Let's assume that we have some property of what it takes for something to be Quine, and say that that thing Quineizes. Now your two premises take a very different form indeed! 

In his "Three Grades of Modal Involvement", Willard V.O. Quine (I'll call the author Willard to avoid ambiguity with your named item Quine) discusses three ways in which we might discuss modal statements. The first is as an operator on sentences. An intuitive interpretation of your first premise using the first grade of modal involvement would be to say that the statement "Quine is Human" is necessarily true (or true in all possible worlds). The second way is as an operator on propositions, We would use the second grade of involvement to say that in all worlds, Quine is human. And the third is as a sub-propositional operator, which allows for modal operators for themselves to play a part in the composition of more complex propositions - e.g. there is someone called Quine, such that it is necessary that that person is human. Now Willard thinks that the first analysis of necessity is okay and the second can be phrased in such a way as to reduce it to the first. But his problem with the third comes from the question of what object, if any, we should take as the reference of some term within the confines of the necessity operator. Necessity does strange things to our sense of how we ought to interpret descriptive singular terms; for example, while it is true that the number of planets in our solar system (8, for the sake of argument) is such that it is greater than 5, and while it is also necessary that the number 8 is greater than the number 5, it does not thereby follow that it is necessary that the number of planets in our solar system is greater than 5 (that it is necessary that there are at least 5 planets). 

Nagel wanted to argue that the role of luck in assessment threatened to undermine not just individual apportionings of blame but the whole project of making moral judgements. That's because there are some cases where we do and some where we don't want to think that control over the outcome is important - for instance, you don't lose responsibility for your actions just by being under the influence of hallucinogenic drugs. One worthwhile response is to consider that "luck" may play much less of a factor in assessments of responsibility, and I think this is generally the case in your example. A Kantian would say that you're responsible not because of how your actions have brought about consequences increasing or decreasing the likelihood of the correct outcome, but because of your intentions when acting. Regardless of the question of whether I win the roll or not, you are responsible for the choice you made to pass the 8-sided dice rather than the 6-sided one. This in a harmful action for which you bear responsibility, with no sense of there being a "degree" of responsibility lessened by appeal to luck since the actual outcome of the roll didn't impact whether what you did was morally correct or not. But there are different kinds of moral luck that Nagel discusses, and all of these need addressing in order to get over the suggestion that our concept of holding people responsible might be in trouble. 

One attitude we might adopt, assuming a negative attitude to the first question, is that while justification for a posteriori truths depends on a priori truths, the epistemology for a priori propositions has more of a coherentist flair to it - we are justified in our use of mathematics and logic because a given mathematical theory or logic is a self-enclosed system, and these systems have different epistemic virtues depending on how their propositions and objects "connect together" rather then how they depend on things like observable phenomena. Reasoning about a posteriori propositions could then safely partially depend on this coherent base. David Hilbert thought that this might be a good way to account for mathematical truths. Hilbert wanted to give, among other things, a stronger and more logically rigorous axiomatic account for Euclidian geometry, given the emergence of new kinds of geometric space and questions over what geometric systems were, and whether they could be considered correct or usefully applied. Hilbert's observation was that a certain amount of normativity, even in supposedly a priori mathematical systems of geometry, could be found in requiring a proof of the consistency of the set of axioms which give rise to the theories in question. This consistency would be enough, he thought, to identify content for mathematicians to study without skeptically worrying about whether some result was going to come along and render the whole enterprise a waste of time. The key positive contribution in Hilbert's work, motivated by this view of the normativity of mathematical claims, was how he proved the logical independence of the various axioms in his geometric system from one another. Once we accept that consistency is all we require, we can appeal to two forms of mathematical reasoning that might otherwise be in question: 

A good first starting point for this might be some material on Testimony in Epistemology (Stanford Encyclopedia). Testimony as a source of justification is recognised as something to be handled with care, since we might think that we need additional justification to believe that the testifying agent can be reliably judged trustworthy. This shouldn't be seen as reason to hold that testimony can even in its most reliable cases have no part to play in the acquisition of knowledge. Knowing something about theories of testimony will go some way to ensuring that your attitudes to the reports and assertions of others is based in reasonable judgement. 

If you take to be a material conditional, such that is a prior definition rule MC, then a very simple proof might go something like this: 

To summarize, so as to keep this answer relatively self-enclosed, the L-S theorem is a result in classical first-order logic that shows that wherever we have a transfinite model of some theory, we can show that there is a countable one, and indeed one of any arbitrary cardinality. This is a logical result, not a set-theoretic one, in the sense that it concerns a property about the abstract satisfiability of theories rather than something particularly semantic. In fact, the traditional understanding of this result seems at odds in many ways with what set theory tries to say about mathematical resources beyond the finite ordinals - a seeming called the Skolem Paradox (SEP Link). Cohen knows about and understands this theorem incredibly well - it is importantly involved in the technique of forcing over Set theory models to get "new" models through a countable treatment adding new "generic" sets. This treatment is in some sense also independent of its set-theoretic interpretation, since what really matters for the purposes of using forcing isn't that the models are set-theoretic, but more that they follow a Boolean-valued model structure, which guarantees that there is some partial ordering on how we add new forcing conditions respecting classical negation closure and exclusion; set-theory is simply the most standard interpretation and representation of boolean valued models. The question of whether Cohen's technique challenges the coherence of a full formalization of Logic is thus a very nuanced one. Using Forcing, Cohen shows us that given some accepted groundwork in the axioms of Set theory, it is possible to construct a diversity of models, some of which may adequately reflect hypothetical "axioms" or desirable properties that mathematicians may want to further dwell on, and others which may demonstrate properties not immediately reflected in how mathematicians go about practicing their craft. There may not necessarily be a sense of a right and wrong way to formalize our axioms, in that these various models might all be independently mathematically interesting, and all of them can be looked at and theorized about within the scope of mathematical logic as it currently stands. But on the other hand, the logic that he uses to demonstrate this technique doesn't itself appear to be under threat by that observation, for the simple reason that the modality of Forcing only branches out in the realm of the transfinite, thanks to the Lowenheim-Skolem theorem. Nothing in forcing extensions will tell you anything unusual about whether - they might intuitively semantically differ on what set, exactly, the sum of ordinal 5 and ordinal 7 is (unless we also have the axiom that we have a countable standard model), but thanks to the way forcing preserves satisfaction of the ZF axioms from model to model, truth of the relativised versions of statements about the finite ranks of the set theoretic hierarchy is preserved between forcing extensions of ZF models. What this seems to turn on then is what kind of notion of Consequence is at stake when we look at the question of what makes something Logic versus Mathematics. An intuitive sort of "Truth across all models" conception would say that there is something seriously up with what Cohen is doing to logic here, because he's essentially creating "new" logical models that he ought, surely, to have recognised prior to using the L-S theorem as a logical result. But that doesn't seem to be what Cohen thinks he's doing - let's take a section from p1089: