Hash partition to let Mysql to decide the category. But here you can try to have an integer column for Hash. YEAR(date) = 4Max digits tag = 4Max digits country = 2Max digits (Convert this into a country code) Add a new column called part_token and set a datatype to Integer Unsigned. It can range from 0 to 4294967295. Any way max prediction to keep the value is 4294th yr and we are at 2017 :) 

I don't see the configurations are rightly set for memory. Hope you are using innodb engine. Use this to fill basic my.cnf to effectively make user of your server setup. $URL$ 

After a day you may grep for values more than 90 in sumOfUsers column to see which line and then you may get timestamp above it. Thus you may measure time range and do manual monitoring around that time to see how often it takes the connections almost 100. Also you may add Host in query to see if any particular application host is doing this pile up of connections. 

If usage means you can only connect to the DB Server and not anything else. Whereas in this case REPLICATION SLAVE will require to read and pull binlog events from Master. To Fix this: On Master Execute: 

Below is the code snippet and logic I have followed for your question (Looks like a simple question, but not for me). Code Snippet: 

Many experts share many ways on how to overcome this problem. These are my suggestions to play a safe game. Try to set the below command in a seperate session. 

Ah! This one could be application framework has the value passed at the time of creating connection to DBs to set wait_timeout or interactive_timeout for their session level, this overrides default mysql settings. If you enable general log on any test environment and have their application to point to that DB. You will find wait_timeout or interactive_timeout setting such values. Short Term Fix: 

I also have 2 more slave nodes. They are performing real good. And they are used heavily by selects on app end. Any thoughts or any one have run into similar issue before? New Update Something found in network. 

Is Mysql not an ACID compliant according to Postgresql? In some blogs I see Mysql is not ACID compliant. How true is that? Let't not consider the replication here, lets consider a standalone and how efficient is Mysql ACID? In my understanding for Mysql-ACID. 

To be pro-active you may have your application to use PDO instead of plain SQL query. Measures to detect SQL Injections could be : 

Dropping multiple KEYS in a table is a preferred option or one by one is preferred? â€œDROP KEY A,DROP KEY B,DROP KEY C" For more 900 million records in the table. How does it work internally? 

1. Will it mess up the sync in any way - On a high availability architectural view "YES". As long you have Master up and steady you might not end up problems. 2. Will the changes get overwritten from the master during the next replication event 

Firstly there is no need of DB side changes. From device application end you need to have a logic of retry to update in cloud once internet is back. Until it finds the connectivity, log the entries serially into local of that device. Once it finds the connectivity, re-spawn the logged entries from device and let it go as similar as cloud normal update. 

I should group all phone numbers, and its total number of attempts and mention its no:of time answered and no:of time not answered. For this the condition is answer_time='0000-00-00 00:00:00' for not answered and answer_time > '0000-00-00 00:00:00' for answered. There can be any number of times the call has been made to a particular number. Any suggestions or approach would be helpful. I'm terribly stuck up on this. The result should be : 

crontab -l 00 3 * * * /usr/scripts/mysqlbackup.sh 2> /usr/scripts/log_mysqlbackup..err Monitor your nagios or netcool to point to the file /usr/scripts/log_mysqlbackup..err 

Could there be any load spiking in cpu processors or IO, that which can slow down Master Perf? If any medium that I can trfr data that uses less cpu/IO utilization ? 

Issue in 1st Option: I'm afraid to issue this, as this doesn't drop the partition and recreate it. It goes for delete from partition where datetime_col < '2013-11-12 00:00:00'. Not sure how much CPU or undo buffers this will create in a 500G partition table. Issue in 2nd Option: No issues, but it just won't work. Anyother recommendations are most welcome!! 

Now the sync is broken on Slave. Your SQL_SLAVE_THREAD stops. Or anyother conflicting statements tries to append it will get broken, above is a better example for that. But if you get updates like below on Master, for sure your Slave data is consistent with Master. 

Depends upon the criticality of Master and your wallet. If reporting queries are bothering much to Master, you may add a slave to it. If your business grows then chances of reporting might require different indexes created on main data hence you might need to handle different backup policies for Master and Analytics slave. 

Now I want to retain the structure by removing 4th week prior DATA ONLY . But I want to see the structure there WITHOUT data for that partition. 

In this, at the first attempt the call is answered at the second it is not. The attempts may range max to 16 times. FOR MAX 

The slave lag persist on slave0. Final Update (I guess)> I disabled Network consuming mysqlbinlog puller runs via cron. Seems to be draining seconds_behind_master from 700 to 490. 

Alert if any query running beyond X secs or any breaching value to relevant teams. The team could see if slow query log / show processlist contains unusual entries, if confirmed as SQL injection kill the query and remove that application user as a preventive measure. 

As long as you have done flush logs and you have taken the logfilename just before flush logs say it should be fine. Example current binlog file is bin-log00333 and after flush logs it is bin-log00334. You may scp the logs from bin-log00001 to bin-log00333 and don't take bin-log00334. 

For those who still need this. May try using binlog puller module by percona folks. It works perfect and you may add a watchdog for it. 

Can someone explain or direct me how execution on indexes happen with different constants at intervals in Mysql. I notice only for the first execution on the table it takes time, after that with different constants it executes the query very quickly. I would like to know how to execute the query in such a way that it should take same amount of time every time it executes with different constants, is there a way to set some parameter off / on? Query executed time : 9 mins. 

I guess you wanted to ensure if my.cnf is rightly set without any wrong or bad settings. Yep makes sense. And Rick has added points to help you in configuring my.cnf as per the server, which are good practices to follow. But I see this problem as. Hey I have a problem and I need a solution !!! I guess you might have found the solution if the actual problem is nailed. 

An approach I follow for normal mysqldump consistency Write a shell script in below mentioned flow and save it as "mysqlbackup.sh" 

Lets say user_id is indexed. It should be effective according to the extended explain plan. Please verify this before you push the code. 

Here are some of my inputs to stand "FOR OUTSIDE DB" It purely depends upon your need. If your business wants to keep them always available and recoverable with no down time then you might have to think a proper option to keep them available 24/7, but keeping them in Database the capacity management of DB server will be in question. If no priority for your need then I would say better to keep raw files in appserver, just the location of the file in DB server in terms of would help and adding index to it would benefit to get the right file location quicker even the number of rows are more. Imagine you have each field consisting of 1GB text and for 100 rows the size of the table would go more than 100GB. Sounds weird for me when I need to take care of everything like how DB performs when it comes to time taken for recovering the Database in-case of shutdown / backup / High Availability and over all performance of DB server. I haven't got an opportunity to design such a requirement. But if given 

I have 5 Node in Galera cluster. MariaDB Server 10.1.21-MariaDB-1~trusty. I will remove 2 nodes from it but one of them will be a slave. So from 5 nodes 5th one will be completely removed, 4th one will be a slave of 3rd one. I don't want to rebuild a backup from 3rd one. I just wanted to make 4th out of cluster and become slave of 3rd one. May I know the procedure for it? Also why is gtid_current_pos is changing for one node and not on other nodes? Let me know if you need any clarifications. 

innodb_thread_concurrency innodb_flush_neighbors -> This shouldn't be enabled if SSD innodb_log_file_size -> Configure as recommended innodb_io_capacity -> Worth to verify the disk IO, see if IO waits for disk seeks are fine. Try to see if its bursting IOPs on Mysql end for capacity given on server. innodb_flush_log_at_trx_commit -> You can set it to 0/2 if consistency is a trade-off. But if you need consistency then you need to ensure the disk IOPs is given to burst fsync at every commit. Recommended is 1. 

Recently I have seen a table that gets inserts only about 60 per second. It predominantly locks for a while waiting for auto-inc to get released. And thus subsequent inserts stays in the processlist tray. After a while the table gets IN_USE of about 30 threads and later I don't know what it makes to free them and clears the tray. (At this duration the load goes more than 15 for 5 minutes) Suppose if you say application functionality should be shapped to best suite the DB server to react. There are 3 to 5 functionalities each are independent entities in schema wise. Whenever I see the locks it gets affected to all other schemas too. Now what makes best fuzzy is the last one. I see slave keeps in synch with master with a delay of 0 second all time whereas slave has a single thread SQL operation that is passed from relay IO that which acts in FIFO model from the binary logs where Master had generated. When this single headed slave can keep the load less and have the operations upto-date, should the concurrent hits are really made to be concurrent for the functionalities which I assume making the possible IO locks in OS level. Can this be organized in application itself and keep the concurrent tenure density thinner? 

Now this is your final data set : /backup/2017-06-01_18-01-52/2017-06-01_18-22-29/* , replace data dir to this data set -> Ensure every 1 to 4 steps had no errors and completed OK is found on all outputs. Test the result incremental backup data on a Test DB and Restore it to see if the server starts without any error messages 

And then drains down within 5 minutes. And when I check the show processlist I see queries for DML and SQL are halted for some minutes. And it processes very slowly. Whereas each query are indexed appropriately and there will be no delay most of the time it returns less than 1 second for any query that are being executed to server the application. 

I have generated a dump from Master and I have to construct slave and give privileges and binary log position and file name of master for it to obsorb from master. Master is a high transaction OLTP application having TPS of 74.21 inserts/s, 113.17 updates/s, 0.00 deletes/s, 77468.29 reads/s. Now when DML operations are about to operate on SLAVE via binlogs would there be any impact on Master's performance. Say for example Load Avg or IO operations on Master that might get affected. As it allows slave to read the binlog info . Note: Master and Slave have same server configuration. Mysql Version : 5.0.77 OS : CentOS 5.4 Mem: 16GB RAM (80% allocated to INNODB_BUFFER_POOL_SIZE) Database Size: 380 GB This question strikes me when I wanted to scp some of the files to Slave. Whereas it has just 34MBPS avg speed within the network and it took reads IO operations a lot and Load Avg was hitting more than 6, 7 , 11 etc .. Ofcourse its not going to use SSH as it uses mysql socket to connect. Just a measurement to have slave in place by not hindering master's perf. My questions are: 

On Slave keep old data. Set this kind of automated/manual job every week to drop old partition on Master. You might have to once verify the selects going on Master. 

As my project demands multiple processes using load data for a table. And I have tested the above options and finalized to go with option 2. Kindly correct me if my experiments are wrong. 

I have got into slave lag gradually say every minute it lags around 5secs on slave. So it started morning 10AM the Seconds_behind_master was 13. Now at 4:30 PM it has reached to 560. No variable changed. Not much selects are running in there, its all regular ones. In fact we have stopped few applications thinking those could cause slowness due to selects. This seems to be strange for me. This had never happened before. I have below variables already set to minimize IO load on disk. 

Since you are using to take a consistent value of master status. The internals of mysqldump will issue below commands to mysql server. 

Can Elasticsearch automatically remove data? I'm using ES 2.3. We have 26nodes in a cluster with three Master nodes and rest are data nodes. There will huge writes happening through out the day. We see in one of the nodes the OLD GC was going for a very long time more than 19Hrs and after it is completed. Applications started to timeout. I see the data from that node has gradually reduced from 1.4T to 8GB. Immediately I stopped the ES in that node. I have two questions. 

You have to take decision based on below two questions. a). Do you expect to get more records in each type (Bank,Applicaition..)? Ans). Go for separate tables. Lets say Bank might get 200000 records, application might get 50000 records. And if their rate of growth is expected to grow higher its better to keep them in seperate table names like bank_documents, application_documents etc. By this you might avoid issues when you alter the table or add index you can go table by table. Ofcourse it is back ache to DBA but no stress. b). If there is not much of records? Or not much bigger rate of growth? Ans). Then go for single table for all types and you may figure later as and when data grows you may prepare a road-map for application team to split types to different tables.