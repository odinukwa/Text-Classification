Once you do that, then maybe you've got something. You'd still need to work out how more than one truth implies an infinity of them (inductive argument perhaps?), and why that would even be a problem. Once you did that, you'd need to show how this has any bearing on monotheism. Having said that, there's a somewhat similar (and I think more justifiable) line of reasoning you can pursue. If truth is everything that is, then you could tie that in with a pantheistic or panentheistic view of God. This essentially takes you to non-dualism, which has found expression in (AFAIK) all major religions, although not as the majority view. It also leads to some very interesting and potentially life-changing practices, but we're probably outside the realm of philosophy at this point. 

Informative communication assumes shared experience; we describe new things as permutations of those experiences. These shared experiences can be high level (like events) or very basic (like qualia), but shared relevant experiences are required. For instance, someone who grew up in the country can describe the experience to an urban dweller because there are shared experiences; the city has trees and fields (think parks), so by extrapolation, life in the country can be described using the analogy of a very big park. Even absent that, the experience can be described in terms of shared experiences (quiet, lots of green, etc...). On the other hand, a person who is blind from birth cannot describe the absence of vision because the basic shared experience required to do so (vision) is absent. At a bare minimum, an experience requires a sharing of the sensory channel(s) in which the experience occurred. 

Don't take language seriously. It's so fuzzy and ambiguous that it can easily spawn a lot of non-problems that will waste your time. There's a reason a lot of philosophy focuses on language -- from Berkley to Wittgenstein -- many philosophers realized the trap of language. For instance, what exactly does indescribable mean? I can think of 3 meanings off the top of my head, each of which can lead to an entirely different answer to your question.... 

Looking at the above, this may be better looked at as an economic or game theoretic problem. Try to calculate the utility or pay-off for this wager. I'm not saying you can, but the attempt to do so should build a greater appreciation for the grossly oversimplified nature of this wager. 

Does proof even apply? Let's say I think I saw a UFO. Now I want to prove I actually saw it. What do I do? I do some research, make some statements, take some readings. In short, I focus on the things about what I saw, to find out its true nature. Or put another way, all my attempts at proof and at study with respect to X are statements ABOUT X. But X in itself -- phenomenally -- is not up for any dispute. There is no doubt that I experienced the appearance of the UFO. There's only doubt as to what the "real nature" of this appearance was. Was it just in my mind, did it correspond to some physical object, did said physical object correspond to a hoax? Consciousness is of this nature. There is no proof of consciousness for the simple fact that it's pure phenomenology, and phenomenology just is. To put it more confusingly, unless you're a philosophical zombie, if you have experience, then you are conscious. Don't even try to argue or prove it :) 

Sleep-walking as an analogy to going on autopilot or not being aware. Dreaming as an analogy to a deluded way of seeing things (more about the objects of consciousness or state of consciousness rather than consciousness per se). 

If the answer is no, then the question is meaningless, and thus cannot be answered. What if you're just asking if it's plausible that we are programs? Well it's plausible, but that's not saying much. After all, plausibility simply means that this proposition doesn't contradict existing evidence; the propositions above don't. It's easy for any proposition to fit the existing evidence if it says nothing testable. Let's look more closely at some of the details of this claim. When we study the plausibility of this claim, we're asking if software can... 

The system's internal architecture is similar to that of a human (some work has been done on this -- see Neural Networks) and may include embodiment (see this experiment). The system has subjective states -- see Artificial Consciousness. This requires knowing if a system is conscious, which would likely require solving the Hard Problem of Consciousness. It even be impossible to know this (see the Philosophical Zombie). 

Yes, but you'd likely get more out of it if you're familiar with some of the philosophical positions he's challenging. This however, goes for most philosophy. Also, if you're concerned about the readability of the volume, you can find an abridged version written in more modern language here. As an aside, the site above has many other classic philosophers (including Kant!) rendered in more readable, less weighty prose. 

Long Answer... If you look at this from a "Gods-eye" view of making decisions for the future of intelligence, then this at best leads to a vacuous solution, and at worst is impossible (under any reasonable definition of good). First, different entities regard different things as good, which means (a) and (b) are impossible. Furthermore, by (c) you seem to have an objective idea of "good" that is independent of the entities involved. So the only way to solve this is to arbitrarily define "good" as that which satisfies a-f, but since we already have a sense of what "good" is (even if we can't precisely define it), we'd be playing word games at this point. You also don't define dystopian; for instance, if everyone were addicted to super-heroin, but the supply was endless and there were no ill effects, would that be so bad? I can think of many people who'd say it would be much better than what we have now... What about mind-controlled slaves, programmed to be happy? Is our culture of unhappy workers any better? Is freedom such an absolute good that it trumps the happiness of the individual? Also, what do you mean by "breakable by hypotheticals?" Additionally, I think the quantum state argument is irrelevant; we don't even know how matter gives rise to consciousness and "good" (in any non-vacuous definition) is a property of the perceiving (value-laden) consciousness. So, the criteria are... 

It depends on how you define honesty and truth. If Y says it's sunny outside (because Y believes this), but it's cloudy, then is Y telling the truth? Y's claim does not accord with the objective state of affairs, yet it accords with Y's subjective one. This is key because honesty is the tendency to tell the truth, yet this tendency personalizes honesty in a way that truth doesn't necessarily do. That is, one can easily talk about truth as an objective measure (X is true or false, no matter what Y thinks), but honesty makes things contingent upon Y's intentions. Then there's the converse; if Y knows Z doesn't trust him/her, and wishes to deceive Z, then Y can tell the truth. So now is Y being honest? So I'd say the answer depends on whether you're treating truth as an objective measure. Since most people seem to do so, I'll say that in one interpretation of this question, the answer is no -- truth is not inherent in honesty as truth s objective, while honesty is subjective and tied to intent. 

In theory, software can check for informal fallacies, just as in theory, software can simulate human beings. However, in practice this is an immense undertaking. Our intelligence is built on a huge body of contextual information. Even detecting fallacies can call upon apparently unrelated pieces of information. So first, you'd need to encode all of this information at the right level of representation for the software. One attempt to do this (at a high level) was The Cyc Project. Then you have to simulate the robust pattern matching abilities of the human brain. If you try doing this by simulating the brain, then you'll need to build a neural network with a quadrillion connections. Further, since individual neurons don't deal with concepts but with far finer grained inputs, your database would have to be much lower level. Then there's the question of embodiement. Is it enough to simulate a brain? What does input, output and representation even mean? We're embodied beings, so must intelligence in the computer be somehow embodied? Rodney Brooks is a roboticist with this view and has built some robots and written eye-opening articles in this regard. Once you've done that, you have to deal with possible side effects. For instance, is our incredible pattern finding ability one reason why we make mistakes? If so, being able to do what we do well may necessarily entail making the same types of mistakes that we do. So, would we have ended up with software that would make fallacies? Can we have our cake and eat it too? One possible shortcut is to build this system with very little contextual information and expose it to the same inputs that people get in their formative years, in an attempt to "grow" this intelligence. Even that is full of problems, but at least you can bypass the problems of trying to encode higher level concepts in very low level neurological terms. So while the question of fallacy checking is solvable in principle, it may be an error to treat it as an isolated act of intelligence, separable from the other acts of intelligence. Philosophy has a long history of trying to detach reason from bodily concerns, and more and more people are coming to realize that this may be a fallacy. Many conclude that reason cannot be separated from passions or even embodiement; in short, there may be no such thing as "pure detached reason" and one might have to take a holistic approach even to such apparently isolated domains as informal fallacy checking. 

Now, none of these assumptions even suggests that we should end up with a universe with beauty or any other characteristic relevant to human concerns. The pseudo-dilemma of your argument came purely from the unjustified label of "God" which dragged in irrelevant associations. Also, each of your premises is a problem. I'd immediately reject all of them or demand clarification. 

Regarding #3.... Neuro-ethics is an active area of research that has yielded insights into both the structure of the brain and ethical dilemmas. Some notable names... 

This is a really fascinating area, and you are in good company. Some philosophies/schools of mathematics -- like finitism, reject infinity. Others -- like intuitionism -- have nuanced positions on infinity. With that said, mathematical objects are not "real" nor do they necessarily represent anything "real". Yes, they most likely arose from interaction with the real world, but after that, they were abstracted and became a logical system in their own right. Once that happened, they became capable of having all sorts of properties divorced from the real world, including infinity. Now you should think of mathematical objects as purely logical constructs forming a closed system, and instead of trying to ensure that these objects accord with intuition, ask instead if they are consistent within this system. For instance, one definition of Natural Numbers is the Peano Axioms. The key part of this system is that it starts with a number -- 0 -- then defines a successor function that yields a distinct number for any number. Since there's no upper bound defined on this function, it's clear that this system yields infinite numbers. 

I think Memming has the right idea. The issue here isn't the skeptical or non-skeptical claim. It's the entire domain of discourse. The brain in a vat is a suggestion that can never be verified or debunked (even in theory). It is -- quite literally -- nonsense. It can get people to have an epiphany that they may not know their true nature, which can have an emotional impact and even transform the person. But to try to argue it or treat it as if it has any information bearing properties is completely wrong. 

Short Answer: There is no argument there for you to evaluate. Long Answer: The problem here is with arguments in natural language. Natural language is a poor medium for precision, logical clarity and argumentation. This is so pronounced that you can easily spend most of the argument trying to resolve semantic issues -- and that's if you're lucky. If you're not lucky, you'll end up talking about two different things. This gels with my experience. When I encounter apparent arguments, I don't pursue them, but instead continually ask for clarification. Most of the time, I realize there was never an argument there, but a different use of terminology. Ok, so how do you make any use of this information? Simple; don't consider what you witnessed to be an argument. Rather, consider it the PRE-ARGUMENT. This is the point where people are clarifying terms. It may have the superficial structure of an argument, but it isn't, so don't let that fool you. So seek clarification of the terms, to understand what each party really means (as opposed to what they seem to be saying). Once this has been completed, then the argument can begin -- although there's a good chance it will have died by then. 

Short Answer: There is nothing peculiar with the propositions arising from that sentence. Long Answer... First, being universally true while requiring an individual is not a problem; in fact, universal quantification is defined in those terms. That is... âˆ€x:P(x) Is a universal statement by virtue of the fact that P(x) holds for each individual x. Second, if someone were to encounter "I am here now" after the fact, it would still be true. For instance, if I was in New York 3 years ago and wrote "I am here now", and you read it 3 years later, you have 2 valid interpretations: Interpretation 1 

I don't know what philospher said this, although Edmund Husserl sounds like a reasonable candidate. With that said, here are some philosophers/schools that may be candidates: 

Is there such a thing as metaphysics? Can one have a metaphysical experience? Can one communicate a metaphysical experience? 

Is it ok for X to convince Y to do Z if X has no intention of doing Z? Is it ok for X to convince Y to do Z if X profits from Z? If X convinces Y to do Z, should X bear responsibility commensurate to the probability that Y dies from Z?