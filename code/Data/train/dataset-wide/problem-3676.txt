Do you have this job in a user's crontab (Can you see it with ?) or is it in the system crontab ()? The system crontab (On linux at least, don't know about other systems) requires an additional user argument that the user crontabs do not. In a user specific crontab, it should look like: 

That's it. It clearly won't tell you how a file was changed, but at least you'll know that it did change in some way. 

I think you need to set up a display manager such as GDM. When Xvnc is starting up, it tries to communicate with a running display manager via XDMCP. That's what the option is in your xinetd file. I've set up GDM on Hardy before, and it worked pretty well. Give this a try: Install GDM if it's not already installed. Edit /etc/gdm/gdm-cdd.conf: 

Now, having said all that, you're never going to have more than one repository on there anyway with this setup unless you do something to force mod_rewrite to skip the urls of the other repositories. mod_rewrite will cheerfully change into for you every time. You could add a rule before the RewriteRule you have such as: 

Try putting quotes around in your command lines. I suspect that has spaces and those might confuse it by looking like several separate arguments. 

You can create a text file containing the computer names you want to scan (1 name per line) and then run the MBSA command line program as follows: 

Making backups is really a game of probability. Assuming the data is successfully written to any media (as confirmed by the backup program's "verify backup" function), the weak link becomes the shelf life/survivability of the media. Backup tapes can break and be demagnetized. Hard disks can crash. Optical media (like DVDs and Bluray disks) degrade over time. I view the question of "Is it safe to back up to media X?" less of a yes or no question and more one of your goals and retention requirements. If you're looking at a one-time/one-off/adhoc backup that you plan to use in the short term for recovery, then it's less an issue of reliability and more a question of convenience. Assuming that you're looking at a corporate server backup solution (e.g. ongoing backups, some media rotation schedule and some retention period requirement for each backup), it's still less of an issue of reliability (since you'll assumingly have at least a daily backup) and more one of convenience. So assuming your backup process is rigorous (done according to a schedule and verified for errors) and frequent, I see no issue taking advantage of the larger capacity of Blu-ray disks. Under no circumstances though would I rely on any optical media for long term storage. For long term, tape will be most reliable. To really reduce risk of long term backup storage and avoid restore failures I think it's important to have multiple backups stored in different locations. 

Settings in /proc/sys are virtual files. To change them, you need to write to them like you would any other file, like so: 

Basically what this is doing is looking at the url of every request and if it is matches, inserting some config rules on the fly before the authentication and authorization stage of the request. To modify it, change the bit, the regex match , and the list of directives to insert. This will leave you with a user file to maintain, where the username is the directory name from the url. If you want multiple users per directory, you'll have to use groups and maintain that file as well. For that, change to and add . 

Hope that helps. Please comment if you have questions about any part of it and I'll try to explain. Or figure out how I goofed up if it doesn't work. 

This is not at all clear in the documentation. What I believe is happening here is that causes the session to read only the rows or bytes or whatever that existed when the table was locked, regardless of what size the table actually is when it gets to it. Rows can continue to be inserted into the table, but only at the end, past the locked portion of the table. 

As you can see I'm using the Google DNS server to check, so I guess it can be save that the TTL is respected. I must be missing something. But I can't figure it out - can anyone give me a hint here, please? Is the problem the TTL of the SOA record, so this must be lowered before making the switch? 

You don't write anything about the kind of failure on the non working server. I'll assume that the problem is not a broken RAID due to failed hard drives. Obviously, in that situation, it would be of little help to move the broken disks to another server ... Generally the RAID configuration is stored on the disks by the H700 (and also most other RAID controllers nowadays). This is supposed to make it easy to move RAID sets between similiar controllers/servers. You just need to move the disks to the working server (I would make sure to plug them in the same slots nevertheless). When booting up you'll have to enter the RAID BIOS. There will be a menu "Import foreign config". In a normal situation (i.e. all disks working perfectly) the controller is even supposed to detect this by itself: 

I'm afraid I must answer my own question and the answer seems to be No. Using the command it turns out the original cert delivered by Comodo already lacks the needed flags in the field. It doesn't have the feature. The Comodo cert looks like this: 

Apache doesn't have a good way to separately log requests beyond the level. If you really want to have apache log directly to a separate log file, you could accomplish that with a custom logging script and pipe apache's access log to it. The directive can specify a program which apache will start and write log data to that program's stdin, where you could then do some basic parsing and write the log files yourself. Seriously though, just use . 

and follow the instructions to reload apache. If it still doesn't work, add a comment below and possibly update your question. I'll take another look and maybe set up a test system. 

You have some sort of backups right? If not, do that first. Perhaps with rsync and an extra drive, rsync over ssh to another location, or a snapshot if this is a VM on a provider that provides that service on disks. Once you have good backups, run tar with the flag. That will delete the source files as they are added to the tar file. 

Don't forget to add if you don't already have it. Edit: I added the NC flag to the rewrite, to catch mixed case, ie: .Css or .CSS 

The directive can be used several ways, depending on the syntax. To execute a file the way you want, you must use the full path relative to the DocumentRoot, preceded with a /. Otherwise, it's taken to be a literal message to be displayed, which is what is happening for you. Try this: 

My internal subnet is 192.168.1.0/255.255.255.0. I don't understand the warning message since a 10.x.x.1 network can just as easily conflict with a remote network as a 192.168.1.x network can. How shoud I proceed? 

Let's say my users have accounts on some mail server mail.example.com. I currently have my mx record set to mail.example.com and all is good. Now let's say I want to have mails initially delivered to an external service (e.g. Postini. Note that this is not a postini-specific question though). In the normal situation where my mx is set directly to my mail server mail.example.com, sending MTAs will of course look up my MX and send to mail.example.com. In my new situation I'd have my mx set to mx.othermailservice.com and emails would be received there. OtherEmailService.com will then relay the emails (while keeping the return-path header the same) to mail.example.com. Do the emails that are received at mail.example.com after be relayed from the other service "look" any different than emails that go directly to it as would be the case where the mx was set to mail.example.com? 

I recently bought a Cisco RV110W Small Business wireless vpn firewall and I want to open a support case with the Cisco TAC. I assumed small business products came with a support period but I can't determine how to open a support case without a contract. 

If you really want your site on both domain names without any redirection, you can use a RewriteCond to pull out just the domain name, and it will be available to the RewriteRule as %1. 

The three addresses you listed there will work with a certificate for *.example.com. Be careful with other names you might add in the future if they have more words separated by periods. The meaning of * for certificates is inconsistent between browsers. Some will match anything, others will match only one word separated by periods. 

That will cause it to skip the next rule. However, you're going to be updating it by hand from here on out. You might be able to pull off something automated with some RewriteCond magic, but it'll probably be tricky. I'm not really familiar with that, so someone else would have to help you out there. If this is only used by a small group of people, then you might be better off skipping this rewriting stuff entirely and just update everyone's working copies. is meant specifically for situations where the repos is moved and you just want to update your working copy without checking it out again. I understand there are situations where this simply isn't feasible though. 

The other answers here are correct, you need to change what files are being matched by your config. The same problem could occur with the files in /var/log, but the combination of compression and the 10M size limit is keeping it from happening. There's another problem you're going to run into that I'd like to point out. Logrotate is built to rely on a consistent log file name to rotate out old files. When it processes a log file, it uses that as a base name to find all old versions of it by searching for the specific extension that old versions would have. That's what the glob stuff in your debug log is looking for. Since your filename changes every time because of the date, it will never go back and look at those old ones because the filename it's working on now doesn't match. There's two things you could do. First option would be to set up postgres or whatever script is creating that dump file to not use the date. Keeping the filename consistent will let logrotate do it's thing and clear out the old ones. Alternatively, you could skip logrotate entirely and put something like this in your crontab: 

The approach we standardized on was to put up a username/password logon web page (in front of the application) which accepts the credentials. When the credentials are submitted, the application would in turn validate those credentials against the directory and then respond accordingly (in a .NET world you could use Forms Authentication $URL$ to force access to the application via this login page). Since the application is doing the credential validation, you get rich information as to the nature of the login failure. In addition, even if the login succeeds but there's relevant information to display to the user, e.g. their password will expire shortly, etc., this provides an opportunity to do so. UPDATE: I forgot to mention that if you adopt this approach, you'll need to allow anonymous access to the IIS application root. This will allow access to the login web page without first attempting the automatic NTLM authentication. It's up to you whether you leave NTLM authentication enabled; perhaps you do want some clients to still automatically log in. 

I have a Cisco RV110W small office router (this configuration process is common to many Linksys/Cisco routers) and I am trying to define QuickVPN clients. When add a client of type "QuickVPN" the router gives me the following warning: (You can find a larger version of the screenshot here 

I just did this install myself recently. The Erlang included in Hardy is too old. Simple as that. You need to build Erlang as well as CouchDB. 

Make sure to remove the packaged erlang first. Grab erlang here: $URL$ I used R13B02-1, but whatever the latest is should work. Configure with . I compiled it without any of that and Couch died immediately on startup. I think the threads is the important one for Couch, but all those seemed like good ideas. 

Also, you can loosen up on the escaping. It doesn't hurt, but it also doesn't help anything, and it makes it harder to read. 

TL;DR : Stopping your services is not necessary with LVM snapshots. +1 for XtraBackup. It is not required to stop the services. The LVM snapshot is equivalent to pulling the power, which means it is consistent, but the same as a crash. When starting the service from one of your backups, it will have to do crash recovery. The time needed to do this varies depending on the size of your InnoDB log files but can be several minutes. Once the crash recovery has completed, your service will be up and running again normally. By shutting down services before the snapshot, your recovery will start faster but at the cost of some downtime every time you run your backups. Also worth considering is that every time you stop services, you lose all the warm buffer pool, so you will also incur some performance degradation from having to read data into memory again. XtraBackup is designed to work with Oracle MySQL as well as Percona Server. It's a good option, though as with all things backup, test, test, and test it again to make sure it works before you find out you need it.