No, but I'll just concentrate on the first one. I would also recommend that you use a ruler or graph paper or whatever, until you can draw well aligned, neat trees. Your first problem is that you are not expanding the nodes properly. While 'the kids' is one NP, it is made up of two elements: a determiner and a noun. You need to show this in your tree. Similarly, even if a constituent has just one element, you still need to expand it. So 'VP-->coming' should really be 'VP-->V-->coming'. However, if you mean to skip a few steps, please use a triangle to show that you did intend to condense the tree. 

Although I usually study within the Humanities faculty, I am in Computational Linguistics class right now, and can recommend Speech and Language Processing, 2nd Edition, 2009, by Jurafsky & Martin. There is another book I've tried, Language Processing with Perl and Prolog by Nugues & Pierre, but I found that it was much more difficult to get into for me, though perhaps handy if you are able to keep up with it. Getting familiar with a few research papers on human language processing (dealing with ambiguities, etc.) was helpful in understanding the textbooks and relating it to NLP as well. 

I offer another perspective although I like the first answer. The proto-Human language is not reconstructable. Two deep (and highly putative) language families that have been suggested by historical linguists are Niger-Saharan and Nostratic. Were these to be genuine, they would date to the early Holocene and Epipaleolithic respectively. Owing to great internal diversity, the Khoisan and Australian Aboriginal languages remain as strictly geographical superfamilies with no demonstrated genetic relationship between internal families. There is a consensus in lexiostatistics that there is a meantime to happen for semantic shift. After a predicable number of years, a semantic value will be replaced with a new word. The replaced word isn't always recoverable. The meantime to happen is contingent on phonology and the semantic value of the word. Even the more stable items on an ordered Swadesh list (we, two, I, eye, mouth, louse) can't survive indefinitely. After about 9,000 years, most of these conservative words will have been replaced. Eric W. Holman, Aharon Dolgopolsky and Sergei Starostin were the chief linguists of this discipline, glottochronology. To return to my examples, in Niger-Saharan (now completely abandoned) all that is reconstructable outside some typological features are the single phonemes of alleged noun classifiers. Nostratic is relatively better substantiated but not even that comes close to qualifying as as a language related to the proto-Human language. As well as onomatopoeic words for mother, father and birds there are areal terms which travelled with prehistorical human technology. "Pan-African" glosses include */bVr/ for ashes and */um/ for nose. I former is most likely areal and the second onomatopoeic. 

If you mean 'time flies' as you would mean 'fruit flies', then wouldn't 'time flies' have to be one NN, or 'time' to be a modifier? With your current phrase structure rules or lexicon you do not allow this interpretation. You'll have to add rules that either license a construction allowing 'time' to act as a modifier of 'flies', or change the lexicon. 

Speaking just about my own personal experience: I can only understand a little Russian because of the similarities I can find from two other Slavic languages I speak: Ukrainian and Polish. As someone who has never been exposed to Russian before, I feel that I would not be able to understand Russian with just one of the other Slavic languages. Therefore, I don't feel that I can agree with the notion of Russian/Polish understanding. In fact, I feel that this might even be a cultural misconception, as I've had quite a few experiences where Russian speakers insist on speaking Russian at me, convinced that I will 'magically' understand them. I've also noticed that non-Slavic speakers believe this as well. however, in the Czech Republic, speaking Polish will get me nowhere, but speaking Ukrainian will get me by quite well indeed. Again, I'd like to stress that is just personal experience. 

whether or not "tense" is even meaningful in (Classical) Arabic is an open question. Arabic verbs do not generally have tense, although they have aspect. to give a definite future meaning to an imperfect like yadbribu you prefix the invariant "sawfa", a separate word. but "yadribu" can have any "tense" depending on context. "daraba" is taught as past tense (he struck) and "yadribu" as present or future, but that's wrong. the main distinction is aspect not tense. 

there are many transliteration schemes. if you want to be scholarly, follow the Encyclopedia of Islam scheme. informally, I use H for your first example and h for the last. the middle one is not really an "h" sound, I usually go with "kh". 

Yes: Arabic. This was analyzed in considerable detail very early in the Arabic tradition.. Whether some word counts as a verb or a noun is not due to any inherent or essential nature of the word, but to how it is used. And I'm not talking about homonyms etc. but about really the same word. The canonical examples are the so-called "verbal nouns" and "participles". These western terms are miscategorizations. E.g. "Zaydun Daaribu Amrin" v. "Zaydun Daaribun Amran". here the suffixes -u, -un, -in, -an are case endings. The former would normally be translated "Zayd (is the) striker of Amr" (noun); the latter, either that or "Zayd (is, or was, or will be) striking Amr" (verb). But it depends entirely on context; the best translation in either case could be either an English verb or a noun. Moral of the story: categories like "noun" and "verb" are neither universal nor scientific. They are Western cultural artifacts. 

I'd also recommend qTree - if you've ever sat in a syntax class or had to work with natural language parsers, it will be very easy to use: $URL$ 

Sometimes, in some cases. If I may hearken back to an introductory lecture on Language Theory and Processing: According to the principle of compositionally (as first coined by Gottlob Frege) the meaning of a sentence is a function of the meaning of its parts. So, structural relations (syntax) give rise to semantics. Here specifically meaning compositional semantics. Of course, it is very important to note that this once popular theory seems to get tied-up with predicate logic and rather dated theories on computational linguistics and AI language processing, and Generative Grammar as well. 

I wonder if Beth Levin's English Verb Classes and Alternations (1993)would have something in its bibliography that you can use? I found the book hard to find but a copy is floating around online somewhere. 

Peter Roach's fourth edition of English Phonetics and Phonology: A Practical course (2009) has a more detailed list of rules and explanations than your link, if you're interested (Chapter 11: Complex Word Stress).