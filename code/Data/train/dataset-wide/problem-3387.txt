If you ran and already had the package installed, chances are you had a pre-existing database schema in /var/lib/mysql. The version included with RHEL5 is 5.0.77, which could explain the mysql-upgrade error, as that script updates the schema between MySQL versions. The error could also be very similar if you did not create the initial schema as part of your installation. If you're going to compile from source and not specify installation locations, you need to remove the system packages of mysql first. If you want to maintain both the system packages and the source installation, which there is sometimes argument for, you need to be careful not to stomp all over the system package. That typically means installing your source in a location such as as opposed to the default of . You can accomplish this with . There are more details here and best practices that I'm not going to elaborate further on. While there can be good reasons to compile from source, you might reconsider yours, as it may not be necessary for you. will display all packages currently installed. Probably and . Otherwise, you didn't follow the procedure from the documentation. Even if you did, you will have to repeat it after removing the MySQL packages. From , which was included in the mysql-5.1.44 tarball: 

I often prefer to update ca-bundle.crt, which is referenced system wide by anything compiled against OpenSSL. In CentOS, this is stored in . This can differ between distributions. 

Is there any particular configuration you had in mind? PHP would be dynamic content, as such you would not be using the reverse proxy caching functionality with Nginx. If you effectively used Nginx as a load balancer for dynamic content, it would certainly be more efficient. If you are talking about a single server, putting Nginx in front to transparentally proxy dynamic content is going to add an additional layer of processing. While likely minute, it would technically introduce additional overhead. Nevertheless, the performance gains from caching would likely make up for that. The only way to be confident for your unique situation is to run your own benchmarks. A utility such as Jmeter could be used for load testing. 

You're using the command , you should use the command . You can also from within the client interactively. 

If you have enabled, following the instructions below. However, if this was not already in use, your schema will need to be recreated in full for the space to be freed. With InnoDB, you need to the table. Newer versions of MySQL also perform the same function with as well. Be careful with large tables, as these commands lock the table. 

That's the proper solution for that approach. Typically, a pid file would be used and a pid test would be done against the process to insure it's running. If stale, the lockfile would be removed and the process would run anyway. Any additional intelligence would typically be written in the software itself, as a daemon, as opposed to running in cron. 

Only a little, I'd test it or only have it be one of many steps as I'd think the impact would be minimal. The idea is that the cookie will be sent for every request even though it's unnecessary for static content. Check out this document. 

DNS by design does not enable having an authoritative copy of all zones, as it utilizes a hierarchical naming system. The root servers are authoritative for identifying the server responsible for the Top Level Domain (TLD) in question. For example, resolving will first query a root server to identify the authoritative nameserver for . The nameserver will identify the authoritative nameserver for , which will then return the record for . You cannot download a copy of all zones. However, you can run a local caching nameserver. The caching nameserver will provide a local copy of all records resolved, which expire using the Time To Live (TTL) specified for the record. Please keep in mind that my explanation is a simplistic description of the DNS protocol, which can be explored in detail by reading definitions in the Request For Comments. While NXDOMAIN hijacking can be avoided by running a local cache, keep in mind that all DNS resolution traffic will still be transmitted via your Internet connection unencrypted. Your ISP could potentially monitor that traffic and still see the communication. The contracts you have with your ISP as well as your local laws are going to be your definitive means for establishing how your communications are treated. Your ISP's contracts will include the Terms of Service, Privacy Policies and any additional contracts that you may have with your ISP. Using encrypted protocols is one of the best methods for insuring your data against eavesdropping during transit. However, even that has no guarantee of anonymity. There are additional protocols out there such as Tor and Freenet, which attempt to introduce anonymity to the Internet, as it was never designed to be truly anonymous. 

The situation is going to be on a per case circumstance. Generally speaking, unless you have a specific reason to run within a super server, it is best not to. Running within a super server adds additional overhead with high load, as every connection spawns a new process. Apache is designed to be always running. It's threaded and intelligently manages system resources. inetd and other super servers were originally better suited for daemons that did not have the ability to interact with sockets within their native code. 

What is the basis of your reasoning? What bottlenecks do you anticipate? The size of the data stored within your database should not be the basis of the amount of RAM you make available to the system. If it were loaded to the point where it was unable to efficiently serve requests, that is when you should consider it. However, improperly tuned queries, lack of indexes, and a poorly configured MySQL instance will not be addressed in full by hardware upgrades. On a properly tuned instance, if you encounter a bottleneck that prevents all your memory from being utilized under load, multiple instances will not address that. It will likely be a CPU bottleneck or potentially disk I/O. For one example, check out InnoDB Memory Usage. There are certain limitations. Such as with MyISAM on MySQL 5.0, each key buffer will be limited to 4GB even on 64-bit platforms, as they are referenced by 32-bit integers. MySQL creates one thread per connection. As such, if you were not running a 64-bit platform, each thread would be limited to 4GB even with Physical Address Extension. If you wanted to scale out read requests, you could use multiple database servers load balanced with LVS. I would not use multiple instances. High Performance MySQL is a fantastic book. I suggest you read it. 

I would suggest creating a spreadsheet listing the key responsibilities within your department. This could be more general and identify specific areas that have more support associated with them, such as the phone system or ticket system. Have staff assigned as primary and secondary, so as that the immediate knowledge can be shared in addition to the documentation. This will clarify responsibilities and they can be moved around to prevent boredom as well as enable learning within the group. Some example categories for the responsibility matrix would be: 

It sounds like you want the server's IP to serve the content separate from the new you configured. The Apache server will default to the specified outside of the when accessing the IP unless configured otherwise. Specifying a with IP or a VirtualHost would also work. If you require additional assistance, please provide additional information including your Apache configuration file in full. 

The size of the company and the industry largely influences the structure of the IT department. In some cases, a little too much. My professional focus is an Internet technology infrastructure. I encourage the separation of duties between primary production and internal intranet support. Even with a technology company, there will be internal only applications and support demands. Often, helpdesk would fit within internal intranet support. Depending upon the size of your environment, you could potentially justify separating the duties in production between architecture and maintenance. I like the junior to senior level approach, which is documented well by SAGE's job descriptions for system administrators. This is a reasonable rule of thumb to follow in general. In a smaller environment, separating duties too much will not be justified and may encourage siloing within a department. Smaller environments benefit from shared knowledge and responsibilities. As the company and IT department grows, it may make sense to start separating duties more specifically. Key areas, which often benefit from some separation of responsibilities: 

I prefer to utilize my own recursive server as well, chris. It's one less piece of information that is readily given to a third party. The only service I like utilizing an ISP for is the uplink. Chris S makes a good point, however. If every end-user ran a local cache, it would put substantially more DNS load on the Internet. Comcast DNS Hijacking 

eBay and Craigslist are your best options unless you want to seek out local resources outside of Craigslist. 

According to the Apache documentation, DirectoryIndex can be specified in a VirtualHost. If you have any other DirectoryIndexes specified, you might try removing them and only specifying within the VirtualHost. Your style of configuration would have to support this, however. As an alternative, you might consider using mod_alias or mod_rewrite instead. Check out the Redirect functions with mod_alias. 

Try instead. If this file applies to the sub-directory with , try specifying . Best practices often entail specifying these settings within httpd.conf instead if you're the system administrator. Also, it's more risky from a security perspective even to enable .htaccess files in the first place. If the Web server or application user were compromised, this would be one of the first places that could be written to without complete compromise. If specified in the Apache configuration file instead, you would often specify within a section. Apache Docs 

Have you tried running ? That will likely resolve any issues with . .frm files contain the table definitions, the schema. If you are using the MyISAM engine and know your schema one potential last resort solution is as follows: 

dovecot is a daemon that provides access to e-Mail via the POP3 and IMAP protocols. Is the server providing e-Mail services? The logging location will depend on your configuration in . Often the default log location will be . Dovecot uses the logging facility. If you want to see if dovecot is running, you can run and look for the dovecot process. 

Typically for servers serving only a few large sites dedicated in role, such as in a professional IT environment, they would be contained within . I would create subdirectories for each separate site. For example: 

I'm a huge QMAIL fan. It hasn't had a security issue ever. The toaster makes it easier to throw a box up if you're less familiar with it. 

Many will not agree with me but I believe that sharing an alternative view is important here. I am speaking primarily in reference to commodity hardware. When proprietary and non x86 is in scope, things change quite a bit. If the OEM discovers you're using your own hardware in a server, they might attempt to not support it. However, it is often easy enough to remove it for the purpose of troubleshooting. Realistically, if you are dealing with business support, usually you do not have to go through asinine troubleshooting that you would with consumer support. However, if you did not example a high technical level, you might. You have hardware people out there that only buy hard drives on a company's spec sheet. In most cases, this level of detail is going to be excessive. This used to matter more. Depending on the hardware in question, you might encounter more fickle controllers or mainboards but typically if they share the same standard interfaces it works. I've known Intel OEM servers to be fickle occasionally but this does not happen very often. In certain IT shops, you will have a management ethos where they attempt to isolate accountability outside of the IT department. This is often done with support contracts from large vendors. This is more difficult to do if you do not follow the vendor's support requirements to the letter. If the staff in your IT department is less experienced with hardware, it's easy to buy what the OEM gives you and know it will work. This is a benefit for many as well. The way I look at it is that you're paying a premium for intangible benefits. Those intangible benefits are more valuable for some than others. In a smaller shop with limited budget, I would stretch it as far as I could. Now, if you have an existing RAID you should certainly do you best to match the drive manufacture and model. Mis-matching drives has performance implications and is generally unwise. 

Really. I mean it. Why manipulate the logs at time of writing? Edit 1 tail -f -n100 /var/log/httpd/error_log | grep -v 'client denied by server configuration' If you really want to prevent the error from being written to disk, you can pipe your logs through a script. More details here: Apache docs 

When a process is in an "uninterruptible sleep" it's either waiting on I/O or it has encountered a bug. As with its name, it is indeed uninterruptible. You can wait for the resource to free up or help the resource free up, which will help the process state to change. Hopefully, it will be a state that will allow you to terminate the process. It could also be potentially waiting on a resource from a parent process. With GNU ps, you can run using the option to enable thread view. fluxbox is a window manager, so it likely has X as a parent process. Try killing X before fluxbox. Otherwise, the only realistic option is to reboot. 

Typically, when a numeric UID is displaying in a situation like this, it indicates that there may no longer be an entry in for the UID. Is 1001 specified in ? 

You can specify the protocol, the source of the packet, the destination for the packet, the destination port , the source port , and many other flags that will affect how the packets are treated by the rule. If your default policy were and you wanted to allow everyone at the subnet to access SSH on your server, here is an example: 

Maintaining Standards What you describe also falls under maintaining configurations. Build standards, software updates, and other things are related to builds but in a lot of ways separate. If you choose to rely on system packages as opposed to creating your own source based builds for your most important server roles, a lot of that can be maintained with native system utilities. This can be as simple a script to run a loop against your server list and run a . For configuration management, this is where puppet, cfengine, and other configuration management utilities come into play. These are very useful utilities and provide the necessary foundation without writing your own scripts from scratch. When you update your configuration standards for your servers, it is important to backfill this into your standard server builds. 

I'd start with , which is going to be where most generic output defaults to. It will include boot messages and any kernel warnings. Depending on the type of issue, there may be no forensic data remaining. For example, RAM may not produce errors. Disk errors will be in the logs. SSH might have simply broke. Without knowing status at console, it's difficult to say definitively. Typically, an otherwise stable Linux box that hasn't been changed suddenly locking up would example a hardware issue. Most hardware issues require further troubleshooting and diagnostics. If you can provide more details, I will likely be able to give you further recommendations.