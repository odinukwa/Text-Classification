Strict DRY doesn't really apply to databases I've seen DRY is used as justification to create views for "re-use". Then we have views joining views etc and piss poor performance. Generally, similar queries will be used in different ways. One may have an aggregate, one may not, filters will be different (as above). The similarity doesn't justify dynamic SQL not does it justify a view. In your specific case above, I'd consider an IF statement to capture the difference, especially if the code was issued by the same search page or form: you have similarity based on usage, not just the same columns and table being used. Also, you have security to consider. Dynamic SQL require EXECUTE AS (escalation of rights) or permissions on the base tables. Using a plain stored procedure does not require such permissions. You may also have different client code: we have stored procedures per client (schemas control permissions). Do you want to GRANT select rights on all your tables to all clients? 

Your colleague is an idiot. The solution won't be scalable, the UDF isn't concurrent (same reason as this). And how do you deal with multi-row inserts: this would require a UDF call per row And migrating to other RDBMS doesn't happen often in real life... you may as well not use SQL Server now and use sequences on Oracle and hope you don't migrate away. Edit: Your update states that moving data is for refreshing non-production databases. In that case, you ignore the identity columns when refreshing. You don't compromise your implementation to make non-prod loading easier. Or use temp tables to track the identity value changes. Or use processes: we refresh our test system every night from production which avoids the issue entirely. (And ensures our prod backup can be restored too) 

Use SET CONTEXT_INFO in the stored procedure and read it in the trigger with CONTEXT_INFO () Have the trigger reset it on exit. 

A quick search of StackOverflow would show you use sys.sql_modules or OBJECT_DEFINITION to search for code on SQL Server 2005+. You can simply restore onto SQL Server 2005+ but not run to allow this. That's if you want fool proof T-SQL. You could concatenate the syscomments rows together if you wish though in SQL Server 2000. I wouldn't personally. Alternatively, use the SQL Server 2008 upgrade advisor to pick out most (maybe all) possible issues (such as ambiguous ORDER BY clauses which caught me out). Note the Red Gate SQL Search sensibly only supports SQL Server 2005+ 

SSMS is creating a copy of the table, populating it, dropping the original. hence the error. I'd create a new filegroup, rather then adding files to the primary filegroup because of how the files are used within a file group You can move a table "in-situ" to a new filegroup by rebuilding the clustered index using 

Yes Service_account_name = DomaninServiceAccount = Domain user A service account is actually a domain user, just with some extra settings like "no interactive login" and "password does not expire". Dunno why they would use "DomaninServiceAccount" too. It's just Service_account_name Is it certain? Just have to test, but it looks usual for non-Windows applications (Atlassian does the same, and I have other Java apps that have AD authentication too) 

Torn page usually. Do you have backups? DBCC may be able to fix it though The king of DBCC is Paul Randal: to avoid my copy/paste, read these from him 

For Linked Servers in SQL Server ... Run a or some such in a stored procedure that contains a BEGIN TRY/CATCH block. 

To make them the same you'd need DISTINCT on the first one: which doesn't make it more readable. Going further, the 2nd one can be expressed as 

These really are objects in your databases Internally, other useful objects like sys.objects don't actually exist in your database. 

Personally, I avoid views because a view doesn't have persistence (unless it is indexed itself): it is simply a macro expanded by the query optimiser. That is, say you have 2 views that join (one with 3 table, one with 4 tables): the actual query plan will show all 7 tables. In the case above, you have no filters. This means that no index can help you: you'll get a clustered index (which is actually the table and you do have a clustered index, right?) scan 

You'd use the sp_OA% stored procs for this. Or CLR since SQL Server 2005. It can't be done via T-SQL directly because T-SQL is for data manipulation. So you have to use one of the 2 methods above. Unless you want to use xp_cmdshell to run a powershell script. This also brings up one limitation of T-SQL: how to get an object definition to disk? And I guess one reason why you asked this. Again, CLR or sp_OA% will do it. One thing to note is that almost every method and property in SMO maps to a SQL command or query. So using T-SQL to call SMO which is effectively T-SQL is circular. And to get the stored procedure definition you'd use OBJECT_DEFINITION... to get the other properties available in SMO you'd use OBJECT_PROPERTY or query the system objects. 

Assuming you want the expired flag to be real time... You can use a view to access the data that has a CASE with GETDATE(). A computed column also works. Note: you can't index the view because GETDATE is non-deterministic 

You can check permissions (DENY is a permission) in sys.database_permissions You may also have a login vs user mismatch from a restore. Look at sp_change_users_login (still useful though deprecated) Do you have a linked server or dynamic SQL too that changes security context? Example Finally, one thought, is not a valid identifier. You need to use it with delimiters 

Generally, Master-Slave is a disaster recovery setup: when master dies, slave becomes master. Manual intervention is needed or you need some redirection layer, You may also have replication lag built in so that master corruption can be stopped before it hits the slave. Clustering is high availability: failover is automatic. You need both HA and DR, not just one. You can have distributed clusters that mix it a bit, but generally the concepts are different. If the 2 VMs are on the same physical host, this is neither HA nor DR of course. If you do use clustering, then use Percona. MySQL cluster is broken in RDBMS terms 

Since SQL Server 2005, both mean "fill all pages including all b-tree index levels" Quotes from BOL (My bold) SQL Server 2000: 

Do nothing Superkey/subtype, that is put the common fields into empl Change to a self referencing table to build a hierarchy 

A typical pattern is schemas based on permissions, so you'd have , etc for code so all objects have the same permissons from the schema. If you have clear user groups then you can permission on that, but you'll end up with overlapping and messy permissions at some point. I tend to defer the user/group checks to some check inside code and not permissions objects: say you have Admin and HR Excel users: these all run code. Data is usually shared so I'd have a schema, maybe a or schema. Some code isn't public (like a UDF or internal proc) so I'd use a schema for code that shouldn't be run by client code. Finally, schemas like or or are useful sometimes. Although there are no user objects in the schema, the user owns all the schemas. 

It appears by design. See behaviour changes for v9.7 for more. I suggest you'd need VARCHAR inside LPAD with leading spaces 

We use schemas (think of them as namespaces in SQL perhaps) for both permissions and grouping. So instead of "tbl" etc we have 

You can't delete the LDF file: it's essential. For the NDF, you need to show that it's not used to SQL Server. You'd use DBCC SHRINKFILE with EMPTYFILE. Substitute xxx based on sys.database_files 

A boat and a car are separate entities: this means a separate table. There are cases where you want to maintain some kind of uniqueness of "Vehicle" where the non-shared attributes are in child tables and of the parent table. But in this case the simple solution is to have 2 tables. As soon as the attributes diverge you'll have refactoring of SQL code and the client to deal with separate stored procedures etc Edit: as mentioned in comments and other answers, we need more information. 

In addition to other answers... If SQL Server is CPU bound then it often means poor indexing. You can identity missing indexes using dmvs, such as the scripts below. This should remove pressure from CPU. Otherwise, throw hardware at it. If you can fix CPU usage with indexes, then look at as much RAM as you can stuff in the server so it runs from memory. Then disks: RAID 10 not RAID 5 especially for transaction logs. And 64 bit? 

Yes, your approach is a good way to do it. Assuming you have enough contiguous free space on your volume so that the MDF can be created as one file. Also, ensure that you rebuild all indexes first so the data is organised well logically too. 

Personally, I'd consider using triggers to load a proper single table and query that. Then you have control of partitioning (if needed), defining good indexes etc Depending on volumes, you could use service broker to decouple the app and your tables. Basically, anything to avoid massive UNIONs and dynamic SQL...